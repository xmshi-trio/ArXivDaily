{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-02-14T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Distillation of encoder-decoder transformers for sequence labelling. (arXiv:2302.05454v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05454","description":"<p>Driven by encouraging results on a wide range of tasks, the field of NLP is\nexperiencing an accelerated race to develop bigger language models. This race\nfor bigger models has also underscored the need to continue the pursuit of\npractical distillation approaches that can leverage the knowledge acquired by\nthese big models in a compute-efficient manner. Having this goal in mind, we\nbuild on recent work to propose a hallucination-free framework for sequence\ntagging that is especially suited for distillation. We show empirical results\nof new state-of-the-art performance across multiple sequence labelling datasets\nand validate the usefulness of this framework for distilling a large model in a\nfew-shot learning scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farina_M/0/1/0/all/0/1\">Marco Farina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappadopulo_D/0/1/0/all/0/1\">Duccio Pappadopulo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anant Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Leslie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irsoy_O/0/1/0/all/0/1\">Ozan &#x130;rsoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solorio_T/0/1/0/all/0/1\">Thamar Solorio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long-Context Language Decision Transformers and Exponential Tilt for Interactive Text Environments. (arXiv:2302.05507v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05507","description":"<p>Text-based game environments are challenging because agents must deal with\nlong sequences of text, execute compositional actions using text and learn from\nsparse rewards. We address these challenges by proposing Long-Context Language\nDecision Transformers (LLDTs), a framework that is based on long transformer\nlanguage models and decision transformers (DTs). LLDTs extend DTs with 3\ncomponents: (1) exponential tilt to guide the agent towards high obtainable\ngoals, (2) novel goal conditioning methods yielding significantly better\nresults than the traditional return-to-go (sum of all future rewards), and (3)\na model of future observations. Our ablation results show that predicting\nfuture observations improves agent performance. To the best of our knowledge,\nLLDTs are the first to address offline RL with DTs on these challenging games.\nOur experiments show that LLDTs achieve the highest scores among many different\ntypes of agents on some of the most challenging Jericho games, such as\nEnchanter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gontier_N/0/1/0/all/0/1\">Nicolas Gontier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1\">Pau Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laradji_I/0/1/0/all/0/1\">Issam Laradji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazquez_D/0/1/0/all/0/1\">David Vazquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FairPy: A Toolkit for Evaluation of Social Biases and their Mitigation in Large Language Models. (arXiv:2302.05508v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05508","description":"<p>Studies have shown that large pretrained language models exhibit biases\nagainst social groups based on race, gender etc, which they inherit from the\ndatasets they are trained on. Various researchers have proposed mathematical\ntools for quantifying and identifying these biases. There have been methods\nproposed to mitigate such biases. In this paper, we present a comprehensive\nquantitative evaluation of different kinds of biases such as race, gender,\nethnicity, age etc. exhibited by popular pretrained language models such as\nBERT, GPT-2 etc. and also present a toolkit that provides plug-and-play\ninterfaces to connect mathematical tools to identify biases with large\npretrained language models such as BERT, GPT-2 etc. and also present users with\nthe opportunity to test custom models against these metrics. The toolkit also\nallows users to debias existing and custom models using the debiasing\ntechniques proposed so far. The toolkit is available at\nhttps://github.com/HrishikeshVish/Fairpy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Viswanath_H/0/1/0/all/0/1\">Hrishikesh Viswanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NapSS: Paragraph-level Medical Text Simplification via Narrative Prompting and Sentence-matching Summarization. (arXiv:2302.05574v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05574","description":"<p>Accessing medical literature is difficult for laypeople as the content is\nwritten for specialists and contains medical jargon. Automated text\nsimplification methods offer a potential means to address this issue. In this\nwork, we propose a summarize-then-simplify two-stage strategy, which we call\nNapSS, identifying the relevant content to simplify while ensuring that the\noriginal narrative flow is preserved. In this approach, we first generate\nreference summaries via sentence matching between the original and the\nsimplified abstracts. These summaries are then used to train an extractive\nsummarizer, learning the most relevant content to be simplified. Then, to\nensure the narrative consistency of the simplified text, we synthesize\nauxiliary narrative prompts combining key phrases derived from the syntactical\nanalyses of the original text. Our model achieves results significantly better\nthan the seq2seq baseline on an English medical corpus, yielding 3%~4% absolute\nimprovements in terms of lexical similarity, and providing a further 1.1%\nimprovement of SARI score when combined with the baseline. We also highlight\nshortcomings of existing evaluation methods, and introduce new metrics that\ntake into account both lexical and high-level semantic similarity. A human\nevaluation conducted on a random sample of the test set further establishes the\neffectiveness of the proposed approach. Codes and models are released here:\nhttps://github.com/LuJunru/NapSS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Junru Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiazheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pergola_G/0/1/0/all/0/1\">Gabriele Pergola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterizing Attribution and Fluency Tradeoffs for Retrieval-Augmented Large Language Models. (arXiv:2302.05578v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05578","description":"<p>Despite recent progress, it has been difficult to prevent semantic\nhallucinations in generative Large Language Models. One common solution to this\nis augmenting LLMs with a retrieval system and making sure that the generated\noutput is attributable to the retrieved information. Given this new added\nconstraint, it is plausible to expect that the overall quality of the output\nwill be affected, for example, in terms of fluency. Can scaling language models\nhelp?\n</p>\n<p>Here we examine the relationship between fluency and attribution in LLMs\nprompted with retrieved evidence in knowledge-heavy dialog settings. Our\nexperiments were implemented with a set of auto-metrics that are aligned with\nhuman preferences. They were used to evaluate a large set of generations,\nproduced under varying parameters of LLMs and supplied context.\n</p>\n<p>We show that larger models tend to do much better in both fluency and\nattribution, and that (naively) using top-k retrieval versus top-1 retrieval\nimproves attribution but hurts fluency. We next propose a recipe that could\nallow smaller models to both close the gap with larger models and preserve the\nbenefits of top-k retrieval while avoiding its drawbacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aksitov_R/0/1/0/all/0/1\">Renat Aksitov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chung-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reitter_D/0/1/0/all/0/1\">David Reitter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakeri_S/0/1/0/all/0/1\">Siamak Shakeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yunhsuan Sung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASDF: A Differential Testing Framework for Automatic Speech Recognition Systems. (arXiv:2302.05582v1 [eess.AS])","link":"http://arxiv.org/abs/2302.05582","description":"<p>Recent years have witnessed wider adoption of Automated Speech Recognition\n(ASR) techniques in various domains. Consequently, evaluating and enhancing the\nquality of ASR systems is of great importance. This paper proposes ASDF, an\nAutomated Speech Recognition Differential Testing Framework for testing ASR\nsystems. ASDF extends an existing ASR testing tool, the CrossASR++, which\nsynthesizes test cases from a text corpus. However, CrossASR++ fails to make\nuse of the text corpus efficiently and provides limited information on how the\nfailed test cases can improve ASR systems. To address these limitations, our\ntool incorporates two novel features: (1) a text transformation module to boost\nthe number of generated test cases and uncover more errors in ASR systems and\n(2) a phonetic analysis module to identify on which phonemes the ASR system\ntend to produce errors. ASDF generates more high-quality test cases by applying\nvarious text transformation methods (e.g., change tense) to the texts in failed\ntest cases. By doing so, ASDF can utilize a small text corpus to generate a\nlarge number of audio test cases, something which CrossASR++ is not capable of.\nIn addition, ASDF implements more metrics to evaluate the performance of ASR\nsystems from multiple perspectives. ASDF performs phonetic analysis on the\nidentified failed test cases to identify the phonemes that ASR systems tend to\ntranscribe incorrectly, providing useful information for developers to improve\nASR systems. The demonstration video of our tool is made online at\nhttps://www.youtube.com/watch?v=DzVwfc3h9As. The implementation is available at\nhttps://github.com/danielyuenhx/asdf-differential-testing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yuen_D/0/1/0/all/0/1\">Daniel Hao Xian Yuen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pang_A/0/1/0/all/0/1\">Andrew Yong Chen Pang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1\">Zhou Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chong_C/0/1/0/all/0/1\">Chun Yong Chong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lim_M/0/1/0/all/0/1\">Mei Kuan Lim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lo_D/0/1/0/all/0/1\">David Lo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MatKB: Semantic Search for Polycrystalline Materials Synthesis Procedures. (arXiv:2302.05597v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05597","description":"<p>In this paper, we present a novel approach to knowledge extraction and\nretrieval using Natural Language Processing (NLP) techniques for material\nscience. Our goal is to automatically mine structured knowledge from millions\nof research articles in the field of polycrystalline materials and make it\neasily accessible to the broader community. The proposed method leverages NLP\ntechniques such as entity recognition and document classification to extract\nrelevant information and build an extensive knowledge base, from a collection\nof 9.5 Million publications. The resulting knowledge base is integrated into a\nsearch engine, which enables users to search for information about specific\nmaterials, properties, and experiments with greater precision than traditional\nsearch engines like Google. We hope our results can enable material scientists\nquickly locate desired experimental procedures, compare their differences, and\neven inspire them to design new experiments. Our website will be available at\nGithub \\footnote{https://github.com/Xianjun-Yang/PcMSP.git} soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xianjun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1\">Stephen Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petzold_L/0/1/0/all/0/1\">Linda Petzold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion Detection From Social Media Posts. (arXiv:2302.05610v1 [cs.LG])","link":"http://arxiv.org/abs/2302.05610","description":"<p>Over the last few years, social media has evolved into a medium for\nexpressing personal views, emotions, and even business and political proposals,\nrecommendations, and advertisements. We address the topic of identifying\nemotions from text data obtained from social media posts like Twitter in this\nresearch. We have deployed different traditional machine learning techniques\nsuch as Support Vector Machines (SVM), Naive Bayes, Decision Trees, and Random\nForest, as well as deep neural network models such as LSTM, CNN, GRU, BiLSTM,\nBiGRU to classify these tweets into four emotion categories (Fear, Anger, Joy,\nand Sadness). Furthermore, we have constructed a BiLSTM and BiGRU ensemble\nmodel. The evaluation result shows that the deep neural network models(BiGRU,\nto be specific) produce the most promising results compared to traditional\nmachine learning models, with an 87.53 % accuracy rate. The ensemble model\nperforms even better (87.66 %), albeit the difference is not significant. This\nresult will aid in the development of a decision-making tool that visualizes\nemotional fluctuations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md Mahbubur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shova_S/0/1/0/all/0/1\">Shaila Shova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metaphor Detection with Effective Context Denoising. (arXiv:2302.05611v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05611","description":"<p>We propose a novel RoBERTa-based model, RoPPT, which introduces a\ntarget-oriented parse tree structure in metaphor detection. Compared to\nexisting models, RoPPT focuses on semantically relevant information and\nachieves the state-of-the-art on several main metaphor datasets. We also\ncompare our approach against several popular denoising and pruning methods,\ndemonstrating the effectiveness of our approach in context denoising. Our code\nand dataset can be found at https://github.com/MajiBear000/RoPPT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yucheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrault_L/0/1/0/all/0/1\">Lo&#xef;c Barrault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1\">Frank Guerin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Robustness of Discrete Prompts. (arXiv:2302.05619v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05619","description":"<p>Discrete prompts have been used for fine-tuning Pre-trained Language Models\nfor diverse NLP tasks. In particular, automatic methods that generate discrete\nprompts from a small set of training instances have reported superior\nperformance. However, a closer look at the learnt prompts reveals that they\ncontain noisy and counter-intuitive lexical constructs that would not be\nencountered in manually-written prompts. This raises an important yet\nunderstudied question regarding the robustness of automatically learnt discrete\nprompts when used in downstream tasks. To address this question, we conduct a\nsystematic study of the robustness of discrete prompts by applying carefully\ndesigned perturbations into an application using AutoPrompt and then measure\ntheir performance in two Natural Language Inference (NLI) datasets. Our\nexperimental results show that although the discrete prompt-based method\nremains relatively robust against perturbations to NLI inputs, they are highly\nsensitive to other types of perturbations such as shuffling and deletion of\nprompt tokens. Moreover, they generalize poorly across different NLI datasets.\nWe hope our findings will inspire future work on robust discrete prompt\nlearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ishibashi_Y/0/1/0/all/0/1\">Yoichi Ishibashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudoh_K/0/1/0/all/0/1\">Katsuhito Sudoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1\">Satoshi Nakamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dialectograms: Machine Learning Differences between Discursive Communities. (arXiv:2302.05657v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05657","description":"<p>Word embeddings provide an unsupervised way to understand differences in word\nusage between discursive communities. A number of recent papers have focused on\nidentifying words that are used differently by two or more communities. But\nword embeddings are complex, high-dimensional spaces and a focus on identifying\ndifferences only captures a fraction of their richness. Here, we take a step\ntowards leveraging the richness of the full embedding space, by using word\nembeddings to map out how words are used differently. Specifically, we describe\nthe construction of dialectograms, an unsupervised way to visually explore the\ncharacteristic ways in which each community use a focal word. Based on these\ndialectograms, we provide a new measure of the degree to which words are used\ndifferently that overcomes the tendency for existing measures to pick out low\nfrequent or polysemous words. We apply our methods to explore the discourses of\ntwo US political subreddits and show how our methods identify stark affective\npolarisation of politicians and political entities, differences in the\nassessment of proper political action as well as disagreement about whether\ncertain issues require political intervention at all.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Enggaard_T/0/1/0/all/0/1\">Thyge Enggaard</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lohse_A/0/1/0/all/0/1\">August Lohse</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Pedersen_M/0/1/0/all/0/1\">Morten Axel Pedersen</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Lehmann_S/0/1/0/all/0/1\">Sune Lehmann</a> (1 and 3) ((1) Copenhagen Center for Social Data Science, University of Copenhagen, Denmark, (2) Department of Anthropology, University of Copenhagen, Denmark, (3) DTU Compute, Technical University of Denmark, Denmark)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DocILE Benchmark for Document Information Localization and Extraction. (arXiv:2302.05658v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05658","description":"<p>This paper introduces the DocILE benchmark with the largest dataset of\nbusiness documents for the tasks of Key Information Localization and Extraction\nand Line Item Recognition. It contains 6.7k annotated business documents, 100k\nsynthetically generated documents, and nearly~1M unlabeled documents for\nunsupervised pre-training. The dataset has been built with knowledge of domain-\nand task-specific aspects, resulting in the following key features: (i)\nannotations in 55 classes, which surpasses the granularity of previously\npublished key information extraction datasets by a large margin; (ii) Line Item\nRecognition represents a highly practical information extraction task, where\nkey information has to be assigned to items in a table; (iii) documents come\nfrom numerous layouts and the test set includes zero- and few-shot cases as\nwell as layouts commonly seen in the training set. The benchmark comes with\nseveral baselines, including RoBERTa, LayoutLMv3 and DETR-based Table\nTransformer. These baseline models were applied to both tasks of the DocILE\nbenchmark, with results shared in this paper, offering a quick starting point\nfor future work. The dataset and baselines are available at\nhttps://github.com/rossumai/docile.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Simsa_S/0/1/0/all/0/1\">&#x160;t&#x11b;p&#xe1;n &#x160;imsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sulc_M/0/1/0/all/0/1\">Milan &#x160;ulc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uricar_M/0/1/0/all/0/1\">Michal U&#x159;i&#x10d;&#xe1;&#x159;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_Y/0/1/0/all/0/1\">Yash Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamdi_A/0/1/0/all/0/1\">Ahmed Hamdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocian_M/0/1/0/all/0/1\">Mat&#x11b;j Koci&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skalicky_M/0/1/0/all/0/1\">Maty&#xe1;&#x161; Skalick&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Ji&#x159;&#xed; Matas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1\">Antoine Doucet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coustaty_M/0/1/0/all/0/1\">Micka&#xeb;l Coustaty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1\">Dimosthenis Karatzas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counter-GAP: Counterfactual Bias Evaluation through Gendered Ambiguous Pronouns. (arXiv:2302.05674v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05674","description":"<p>Bias-measuring datasets play a critical role in detecting biased behavior of\nlanguage models and in evaluating progress of bias mitigation methods. In this\nwork, we focus on evaluating gender bias through coreference resolution, where\nprevious datasets are either hand-crafted or fail to reliably measure an\nexplicitly defined bias. To overcome these shortcomings, we propose a novel\nmethod to collect diverse, natural, and minimally distant text pairs via\ncounterfactual generation, and construct Counter-GAP, an annotated dataset\nconsisting of 4008 instances grouped into 1002 quadruples. We further identify\na bias cancellation problem in previous group-level metrics on Counter-GAP, and\npropose to use the difference between inconsistency across genders and within\ngenders to measure bias at a quadruple level. Our results show that four\npre-trained language models are significantly more inconsistent across\ndifferent gender groups than within each group, and that a name-based\ncounterfactual data augmentation method is more effective to mitigate such bias\nthan an anonymization-based method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongbin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocijan_V/0/1/0/all/0/1\">Vid Kocijan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Exemplars for In-context Learning. (arXiv:2302.05698v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05698","description":"<p>Large pretrained language models (LMs) have shown impressive In-Context\nLearning (ICL) ability, where the model learns to do an unseen task via a\nprompt consisting of input-output examples as the demonstration, without any\nparameter updates. The performance of ICL is highly dominated by the quality of\nthe selected in-context examples. However, previous selection methods are\nmostly based on simple heuristics, leading to sub-optimal performance. In this\nwork, we formulate in-context example selection as a subset selection problem.\nWe propose CEIL(Compositional Exemplars for In-context Learning), which is\ninstantiated by Determinantal Point Processes (DPPs) to model the interaction\nbetween the given input and in-context examples, and optimized through a\ncarefully-designed contrastive learning objective to obtain preference from\nLMs. We validate CEIL on 12 classification and generation datasets from 7\ndistinct NLP tasks, including sentiment analysis, paraphrase detection, natural\nlanguage inference, commonsense reasoning, open-domain question answering, code\ngeneration, and semantic parsing. Extensive experiments demonstrate not only\nthe state-of-the-art performance but also the transferability and\ncompositionality of CEIL, shedding new light on effective and efficient\nin-context learning. Our code is released at\nhttps://github.com/HKUNLP/icl-ceil.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiacheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiangtao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HateProof: Are Hateful Meme Detection Systems really Robust?. (arXiv:2302.05703v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05703","description":"<p>Exploiting social media to spread hate has tremendously increased over the\nyears. Lately, multi-modal hateful content such as memes has drawn relatively\nmore traction than uni-modal content. Moreover, the availability of implicit\ncontent payloads makes them fairly challenging to be detected by existing\nhateful meme detection systems. In this paper, we present a use case study to\nanalyze such systems' vulnerabilities against external adversarial attacks. We\nfind that even very simple perturbations in uni-modal and multi-modal settings\nperformed by humans with little knowledge about the model can make the existing\ndetection models highly vulnerable. Empirically, we find a noticeable\nperformance drop of as high as 10% in the macro-F1 score for certain attacks.\nAs a remedy, we attempt to boost the model's robustness using contrastive\nlearning as well as an adversarial training-based method - VILLA. Using an\nensemble of the above two approaches, in two of our high resolution datasets,\nwe are able to (re)gain back the performance to a large extent for certain\nattacks. We believe that ours is a first step toward addressing this crucial\nproblem in an adversarial setting and would inspire more such investigations in\nthe future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_P/0/1/0/all/0/1\">Piush Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chawla_P/0/1/0/all/0/1\">Pranit Chawla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_M/0/1/0/all/0/1\">Mithun Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_P/0/1/0/all/0/1\">Punyajoy Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_B/0/1/0/all/0/1\">Binny Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zesch_T/0/1/0/all/0/1\">Torsten Zesch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Animesh Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MTTM: Metamorphic Testing for Textual Content Moderation Software. (arXiv:2302.05706v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05706","description":"<p>The exponential growth of social media platforms such as Twitter and Facebook\nhas revolutionized textual communication and textual content publication in\nhuman society. However, they have been increasingly exploited to propagate\ntoxic content, such as hate speech, malicious advertisement, and pornography,\nwhich can lead to highly negative impacts (e.g., harmful effects on teen mental\nhealth). Researchers and practitioners have been enthusiastically developing\nand extensively deploying textual content moderation software to address this\nproblem. However, we find that malicious users can evade moderation by changing\nonly a few words in the toxic content. Moreover, modern content moderation\nsoftware performance against malicious inputs remains underexplored. To this\nend, we propose MTTM, a Metamorphic Testing framework for Textual content\nModeration software. Specifically, we conduct a pilot study on 2,000 text\nmessages collected from real users and summarize eleven metamorphic relations\nacross three perturbation levels: character, word, and sentence. MTTM employs\nthese metamorphic relations on toxic textual contents to generate test cases,\nwhich are still toxic yet likely to evade moderation. In our evaluation, we\nemploy MTTM to test three commercial textual content moderation software and\ntwo state-of-the-art moderation algorithms against three kinds of toxic\ncontent. The results show that MTTM achieves up to 83.9%, 51%, and 82.5% error\nfinding rates (EFR) when testing commercial moderation software provided by\nGoogle, Baidu, and Huawei, respectively, and it obtains up to 91.2% EFR when\ntesting the state-of-the-art algorithms from the academy. In addition, we\nleverage the test cases generated by MTTM to retrain the model we explored,\nwhich largely improves model robustness (0% to 5.9% EFR) while maintaining the\naccuracy on the original test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jen-tse Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weibin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yizhan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pinjia He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fair Enough: Standardizing Evaluation and Model Selection for Fairness Research in NLP. (arXiv:2302.05711v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05711","description":"<p>Modern NLP systems exhibit a range of biases, which a growing literature on\nmodel debiasing attempts to correct. However current progress is hampered by a\nplurality of definitions of bias, means of quantification, and oftentimes vague\nrelation between debiasing algorithms and theoretical measures of bias. This\npaper seeks to clarify the current situation and plot a course for meaningful\nprogress in fair learning, with two key contributions: (1) making clear\ninter-relations among the current gamut of methods, and their relation to\nfairness theory; and (2) addressing the practical problem of model selection,\nwhich involves a trade-off between fairness and accuracy and has led to\nsystemic issues in fairness research. Putting them together, we make several\nrecommendations to help shape future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xudong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning by Applying: A General Framework for Mathematical Reasoning via Enhancing Explicit Knowledge Learning. (arXiv:2302.05717v1 [cs.AI])","link":"http://arxiv.org/abs/2302.05717","description":"<p>Mathematical reasoning is one of the crucial abilities of general artificial\nintelligence, which requires machines to master mathematical logic and\nknowledge from solving problems. However, existing approaches are not\ntransparent (thus not interpretable) in terms of what knowledge has been\nlearned and applied in the reasoning process. In this paper, we propose a\ngeneral Learning by Applying (LeAp) framework to enhance existing models\n(backbones) in a principled way by explicit knowledge learning. In LeAp, we\nperform knowledge learning in a novel problem-knowledge-expression paradigm,\nwith a Knowledge Encoder to acquire knowledge from problem data and a Knowledge\nDecoder to apply knowledge for expression reasoning. The learned mathematical\nknowledge, including word-word relations and word-operator relations, forms an\nexplicit knowledge graph, which bridges the knowledge \"learning\" and \"applying\"\norganically. Moreover, for problem solving, we design a semantics-enhanced\nmodule and a reasoning-enhanced module that apply knowledge to improve the\nproblem comprehension and symbol reasoning abilities of any backbone,\nrespectively. We theoretically prove the superiority of LeAp's autonomous\nlearning mechanism. Experiments on three real-world datasets show that LeAp\nimproves all backbones' performances, learns accurate knowledge, and achieves a\nmore interpretable reasoning process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiayu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhenya Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">Chengxiang Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthesizing Human Gaze Feedback for Improved NLP Performance. (arXiv:2302.05721v1 [cs.HC])","link":"http://arxiv.org/abs/2302.05721","description":"<p>Integrating human feedback in models can improve the performance of natural\nlanguage processing (NLP) models. Feedback can be either explicit (e.g. ranking\nused in training language models) or implicit (e.g. using human cognitive\nsignals in the form of eyetracking). Prior eye tracking and NLP research reveal\nthat cognitive processes, such as human scanpaths, gleaned from human gaze\npatterns aid in the understanding and performance of NLP models. However, the\ncollection of real eyetracking data for NLP tasks is challenging due to the\nrequirement of expensive and precise equipment coupled with privacy invasion\nissues. To address this challenge, we propose ScanTextGAN, a novel model for\ngenerating human scanpaths over text. We show that ScanTextGAN-generated\nscanpaths can approximate meaningful cognitive signals in human gaze patterns.\nWe include synthetically generated scanpaths in four popular NLP tasks spanning\nsix different datasets as proof of concept and show that the models augmented\nwith generated scanpaths improve the performance of all downstream NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khurana_V/0/1/0/all/0/1\">Varun Khurana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_Y/0/1/0/all/0/1\">Yaman Kumar Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollenstein_N/0/1/0/all/0/1\">Nora Hollenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Rajesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1\">Balaji Krishnamurthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Brief Report on LawGPT 1.0: A Virtual Legal Assistant Based on GPT-3. (arXiv:2302.05729v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05729","description":"<p>LawGPT 1.0 is a virtual legal assistant built on the state-of-the-art\nlanguage model GPT-3, fine-tuned for the legal domain. The system is designed\nto provide legal assistance to users in a conversational manner, helping them\nwith tasks such as answering legal questions, generating legal documents, and\nproviding legal advice. In this paper, we provide a brief overview of LawGPT\n1.0, its architecture, and its performance on a set of legal benchmark tasks.\nPlease note that the detailed information about the model is protected by a\nnon-disclosure agreement (NDA) and cannot be disclosed in this report.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha-Thanh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Divergence-Based Domain Transferability for Zero-Shot Classification. (arXiv:2302.05735v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05735","description":"<p>Transferring learned patterns from pretrained neural language models has been\nshown to significantly improve effectiveness across a variety of language-based\ntasks, meanwhile further tuning on intermediate tasks has been demonstrated to\nprovide additional performance benefits, provided the intermediate task is\nsufficiently related to the target task. However, how to identify related tasks\nis an open problem, and brute-force searching effective task combinations is\nprohibitively expensive. Hence, the question arises, are we able to improve the\neffectiveness and efficiency of tasks with no training examples through\nselective fine-tuning? In this paper, we explore statistical measures that\napproximate the divergence between domain representations as a means to\nestimate whether tuning using one task pair will exhibit performance benefits\nover tuning another. This estimation can then be used to reduce the number of\ntask pairs that need to be tested by eliminating pairs that are unlikely to\nprovide benefits. Through experimentation over 58 tasks and over 6,600 task\npair combinations, we demonstrate that statistical measures can distinguish\neffective task pairs, and the resulting estimates can reduce end-to-end runtime\nby up to 40%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pugantsov_A/0/1/0/all/0/1\">Alexander Pugantsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCreadie_R/0/1/0/all/0/1\">Richard McCreadie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Reparameterized Discrete Diffusion Model for Text Generation. (arXiv:2302.05737v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05737","description":"<p>This work studies discrete diffusion probabilistic models with applications\nto natural language generation. We derive an alternative yet equivalent\nformulation of the sampling from discrete diffusion processes and leverage this\ninsight to develop a family of reparameterized discrete diffusion models. The\nderived generic framework is highly flexible, offers a fresh perspective of the\ngeneration process in discrete diffusion models, and features more effective\ntraining and decoding techniques. We conduct extensive experiments to evaluate\nthe text generation capability of our model, demonstrating significant\nimprovements over existing diffusion models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jianbo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Sign Recognition with Phonology. (arXiv:2302.05759v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05759","description":"<p>We use insights from research on American Sign Language (ASL) phonology to\ntrain models for isolated sign language recognition (ISLR), a step towards\nautomatic sign language understanding. Our key insight is to explicitly\nrecognize the role of phonology in sign production to achieve more accurate\nISLR than existing work which does not consider sign language phonology. We\ntrain ISLR models that take in pose estimations of a signer producing a single\nsign to predict not only the sign but additionally its phonological\ncharacteristics, such as the handshape. These auxiliary predictions lead to a\nnearly 9% absolute gain in sign recognition accuracy on the WLASL benchmark,\nwith consistent improvements in ISLR regardless of the underlying prediction\nmodel architecture. This work has the potential to accelerate linguistic\nresearch in the domain of signed languages and reduce communication barriers\nbetween deaf and hearing people.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kezar_L/0/1/0/all/0/1\">Lee Kezar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sehyr_Z/0/1/0/all/0/1\">Zed Sevcikova Sehyr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Level Generation Through Large Language Models. (arXiv:2302.05817v1 [cs.AI])","link":"http://arxiv.org/abs/2302.05817","description":"<p>Large Language Models (LLMs) are powerful tools, capable of leveraging their\ntraining on natural language to write stories, generate code, and answer\nquestions. But can they generate functional video game levels? Game levels,\nwith their complex functional constraints and spatial relationships in more\nthan one dimension, are very different from the kinds of data an LLM typically\nsees during training. Datasets of game levels are also hard to come by,\npotentially taxing the abilities of these data-hungry models. We investigate\nthe use of LLMs to generate levels for the game Sokoban, finding that LLMs are\nindeed capable of doing so, and that their performance scales dramatically with\ndataset size. We also perform preliminary experiments on controlling LLM level\ngenerators and discuss promising areas for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Todd_G/0/1/0/all/0/1\">Graham Todd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Earle_S/0/1/0/all/0/1\">Sam Earle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasir_M/0/1/0/all/0/1\">Muhammad Umair Nasir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_M/0/1/0/all/0/1\">Michael Cerny Green</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1\">Julian Togelius</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Why is this misleading?\": Detecting News Headline Hallucinations with Explanations. (arXiv:2302.05852v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05852","description":"<p>Automatic headline generation enables users to comprehend ongoing news events\npromptly and has recently become an important task in web mining and natural\nlanguage processing. With the growing need for news headline generation, we\nargue that the hallucination issue, namely the generated headlines being not\nsupported by the original news stories, is a critical challenge for the\ndeployment of this feature in web-scale systems Meanwhile, due to the\ninfrequency of hallucination cases and the requirement of careful reading for\nraters to reach the correct consensus, it is difficult to acquire a large\ndataset for training a model to detect such hallucinations through human\ncuration. In this work, we present a new framework named ExHalder to address\nthis challenge for headline hallucination detection. ExHalder adapts the\nknowledge from public natural language inference datasets into the news domain\nand learns to generate natural language sentences to explain the hallucination\ndetection results. To evaluate the model performance, we carefully collect a\ndataset with more than six thousand labeled &lt;article, headline&gt; pairs.\nExtensive experiments on this dataset and another six public ones demonstrate\nthat ExHalder can identify hallucinated headlines accurately and justifies its\npredictions with human-readable natural language explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiaming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jialu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finnie_D/0/1/0/all/0/1\">Dan Finnie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmati_N/0/1/0/all/0/1\">Negar Rahmati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bendersky_M/0/1/0/all/0/1\">Michael Bendersky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najork_M/0/1/0/all/0/1\">Marc Najork</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Position Matters! Empirical Study of Order Effect in Knowledge-grounded Dialogue. (arXiv:2302.05888v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05888","description":"<p>With the power of large pretrained language models, various research works\nhave integrated knowledge into dialogue systems. The traditional techniques\ntreat knowledge as part of the input sequence for the dialogue system,\nprepending a set of knowledge statements in front of dialogue history. However,\nsuch a mechanism forces knowledge sets to be concatenated in an ordered manner,\nmaking models implicitly pay imbalanced attention to the sets during training.\nIn this paper, we first investigate how the order of the knowledge set can\ninfluence autoregressive dialogue systems' responses. We conduct experiments on\ntwo commonly used dialogue datasets with two types of transformer-based models\nand find that models view the input knowledge unequally. To this end, we\npropose a simple and novel technique to alleviate the order effect by modifying\nthe position embeddings of knowledge input in these models. With the proposed\nposition embedding method, the experimental results show that each knowledge\nstatement is uniformly considered to generate responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hsuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shachi H Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazumder_S/0/1/0/all/0/1\">Sahisnu Mazumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenda Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manuvinakurike_R/0/1/0/all/0/1\">Ramesh Manuvinakurike</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okur_E/0/1/0/all/0/1\">Eda Okur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahay_S/0/1/0/all/0/1\">Saurav Sahay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachman_L/0/1/0/all/0/1\">Lama Nachman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shang-Tse Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TextDefense: Adversarial Text Detection based on Word Importance Entropy. (arXiv:2302.05892v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05892","description":"<p>Currently, natural language processing (NLP) models are wildly used in\nvarious scenarios. However, NLP models, like all deep models, are vulnerable to\nadversarially generated text. Numerous works have been working on mitigating\nthe vulnerability from adversarial attacks. Nevertheless, there is no\ncomprehensive defense in existing works where each work targets a specific\nattack category or suffers from the limitation of computation overhead,\nirresistible to adaptive attack, etc.\n</p>\n<p>In this paper, we exhaustively investigate the adversarial attack algorithms\nin NLP, and our empirical studies have discovered that the attack algorithms\nmainly disrupt the importance distribution of words in a text. A well-trained\nmodel can distinguish subtle importance distribution differences between clean\nand adversarial texts. Based on this intuition, we propose TextDefense, a new\nadversarial example detection framework that utilizes the target model's\ncapability to defend against adversarial attacks while requiring no prior\nknowledge. TextDefense differs from previous approaches, where it utilizes the\ntarget model for detection and thus is attack type agnostic. Our extensive\nexperiments show that TextDefense can be applied to different architectures,\ndatasets, and attack methods and outperforms existing methods. We also discover\nthat the leading factor influencing the performance of TextDefense is the\ntarget model's generalizability. By analyzing the property of the target model\nand the property of the adversarial example, we provide our insights into the\nadversarial attacks in NLP and the principles of our defense method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lujia Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1\">Yuwen Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1\">Chunpeng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yanghe Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discourse Structure Extraction from Pre-Trained and Fine-Tuned Language Models in Dialogues. (arXiv:2302.05895v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05895","description":"<p>Discourse processing suffers from data sparsity, especially for dialogues. As\na result, we explore approaches to build discourse structures for dialogues,\nbased on attention matrices from Pre-trained Language Models (PLMs). We\ninvestigate multiple tasks for fine-tuning and show that the dialogue-tailored\nSentence Ordering task performs best. To locate and exploit discourse\ninformation in PLMs, we propose an unsupervised and a semi-supervised method.\nOur proposals achieve encouraging results on the STAC corpus, with F1 scores of\n57.2 and 59.3 for unsupervised and semi-supervised methods, respectively. When\nrestricted to projective trees, our scores improved to 63.3 and 68.1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chuyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huber_P/0/1/0/all/0/1\">Patrick Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wen Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amblard_M/0/1/0/all/0/1\">Maxime Amblard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braud_C/0/1/0/all/0/1\">Chlo&#xe9; Braud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1\">Giuseppe Carenini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entanglement as a Method to Reduce Uncertainty. (arXiv:2302.05898v1 [q-bio.NC])","link":"http://arxiv.org/abs/2302.05898","description":"<p>In physics, entanglement 'reduces' the entropy of an entity, because the (von\nNeumann) entropy of, e.g., a composite bipartite entity in a pure entangled\nstate is systematically lower than the entropy of the component sub-entities.\nWe show here that this 'genuinely non-classical reduction of entropy as a\nresult of composition' also holds whenever two concepts combine in human\ncognition and, more generally, it is valid in human culture. We exploit these\nresults and make a 'new hypothesis' on the nature of entanglement, namely, the\nproduction of entanglement in the preparation of a composite entity can be seen\nas a 'dynamical process of collaboration between its sub-entities to reduce\nuncertainty', because the composite entity is in a pure state while its\nsub-entities are in a non-pure, or density, state, as a result of the\npreparation. We identify within the nature of this entanglement a mechanism of\ncontextual updating and illustrate the mechanism in the example we analyze. Our\nhypothesis naturally explains the 'non-classical nature' of some quantum\nlogical connectives, as due to Bell-type correlations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Aerts_D/0/1/0/all/0/1\">Diederik Aerts</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Argelles_J/0/1/0/all/0/1\">Jonito Aerts Arg&#xeb;lles</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Beltran_L/0/1/0/all/0/1\">Lester Beltran</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Geriente_S/0/1/0/all/0/1\">Suzette Geriente</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sozzo_S/0/1/0/all/0/1\">Sandro Sozzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Effect of Relative Positional Embeddings on AMR-to-Text Generation with Structural Adapters. (arXiv:2302.05900v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05900","description":"<p>Text generation from Abstract Meaning Representation (AMR) has substantially\nbenefited from the popularized Pretrained Language Models (PLMs). Myriad\napproaches have linearized the input graph as a sequence of tokens to fit the\nPLM tokenization requirements. Nevertheless, this transformation jeopardizes\nthe structural integrity of the graph and is therefore detrimental to its\nresulting representation. To overcome this issue, Ribeiro et al. have recently\nproposed StructAdapt, a structure-aware adapter which injects the input graph\nconnectivity within PLMs using Graph Neural Networks (GNNs). In this paper, we\ninvestigate the influence of Relative Position Embeddings (RPE) on AMR-to-Text,\nand, in parallel, we examine the robustness of StructAdapt. Through ablation\nstudies, graph attack and link prediction, we reveal that RPE might be\npartially encoding input graphs. We suggest further research regarding the role\nof RPE will provide valuable insights for Graph-to-Text generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Montella_S/0/1/0/all/0/1\">Sebastien Montella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasr_A/0/1/0/all/0/1\">Alexis Nasr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinecke_J/0/1/0/all/0/1\">Johannes Heinecke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bechet_F/0/1/0/all/0/1\">Frederic Bechet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rojas_Barahona_L/0/1/0/all/0/1\">Lina M. Rojas-Barahona</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Extended Sequence Tagging Vocabulary for Grammatical Error Correction. (arXiv:2302.05913v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05913","description":"<p>We extend a current sequence-tagging approach to Grammatical Error Correction\n(GEC) by introducing specialised tags for spelling correction and morphological\ninflection using the SymSpell and LemmInflect algorithms. Our approach improves\ngeneralisation: the proposed new tagset allows a smaller number of tags to\ncorrect a larger range of errors. Our results show a performance improvement\nboth overall and in the targeted error categories. We further show that\nensembles trained with our new tagset outperform those trained with the\nbaseline tagset on the public BEA benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mesham_S/0/1/0/all/0/1\">Stuart Mesham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bryant_C/0/1/0/all/0/1\">Christopher Bryant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_M/0/1/0/all/0/1\">Marek Rei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stabilized In-Context Learning with Pre-trained Language Models for Few Shot Dialogue State Tracking. (arXiv:2302.05932v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05932","description":"<p>Prompt-based methods with large pre-trained language models (PLMs) have shown\nimpressive unaided performance across many NLP tasks. These models improve even\nfurther with the addition of a few labeled in-context exemplars to guide output\ngeneration. However, for more complex tasks such as dialogue state tracking\n(DST), designing prompts that reliably convey the desired intent is nontrivial,\nleading to unstable results. Furthermore, building in-context exemplars for\ndialogue tasks is difficult because conversational contexts are long while\nmodel input lengths are relatively short. To overcome these issues we first\nadapt a meta-learning scheme to the dialogue domain which stabilizes the\nability of the model to perform well under various prompts. We additionally\ndesign a novel training method to improve upon vanilla retrieval mechanisms to\nfind ideal in-context examples. Finally, we introduce a saliency model to limit\ndialogue text length, allowing us to include more exemplars per query. In\neffect, we are able to achieve highly competitive results for few-shot DST on\nMultiWOZ.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Derek Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1\">Kun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing the Effectiveness of the Underlying Reasoning Tasks in Multi-hop Question Answering. (arXiv:2302.05963v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05963","description":"<p>To explain the predicted answers and evaluate the reasoning abilities of\nmodels, several studies have utilized underlying reasoning (UR) tasks in\nmulti-hop question answering (QA) datasets. However, it remains an open\nquestion as to how effective UR tasks are for the QA task when training models\non both tasks in an end-to-end manner. In this study, we address this question\nby analyzing the effectiveness of UR tasks (including both sentence-level and\nentity-level tasks) in three aspects: (1) QA performance, (2) reasoning\nshortcuts, and (3) robustness. While the previous models have not been\nexplicitly trained on an entity-level reasoning prediction task, we build a\nmulti-task model that performs three tasks together: sentence-level supporting\nfacts prediction, entity-level reasoning prediction, and answer prediction.\nExperimental results on 2WikiMultiHopQA and HotpotQA-small datasets reveal that\n(1) UR tasks can improve QA performance. Using four debiased datasets that are\nnewly created, we demonstrate that (2) UR tasks are helpful in preventing\nreasoning shortcuts in the multi-hop QA task. However, we find that (3) UR\ntasks do not contribute to improving the robustness of the model on adversarial\nquestions, such as sub-questions and inverted questions. We encourage future\nstudies to investigate the effectiveness of entity-level reasoning in the form\nof natural language questions (e.g., sub-question forms).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ho_X/0/1/0/all/0/1\">Xanh Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh-Khoa Duong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugawara_S/0/1/0/all/0/1\">Saku Sugawara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aizawa_A/0/1/0/all/0/1\">Akiko Aizawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoupling the Skeleton Parsing and Schema Linking for Text-to-SQL. (arXiv:2302.05965v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05965","description":"<p>One of the recent best attempts at Text-to-SQL is the pre-trained language\nmodel. Due to the structural property of the SQL queries, the seq2seq model\ntakes the responsibility of parsing both the schema items (i.e., tables and\ncolumns) and the skeleton (i.e., SQL keywords). Such coupled targets increase\nthe difficulty of parsing the correct SQL queries especially when they involve\nmany schema items and logic operators. This paper proposes a ranking-enhanced\nencoding and skeleton-aware decoding framework to decouple the schema linking\nand the skeleton parsing. Specifically, for a seq2seq encoder-decode model, its\nencoder is injected by the most relevant schema items instead of the whole\nunordered ones, which could alleviate the schema linking effort during SQL\nparsing, and its decoder first generates the skeleton and then the actual SQL\nquery, which could implicitly constrain the SQL parsing. We evaluate our\nproposed framework on Spider and its three robustness variants: Spider-DK,\nSpider-Syn, and Spider-Realistic. The experimental results show that our\nframework delivers promising performance and robustness. Our code is available\nat https://github.com/RUCKBReasoning/RESDSQL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cuiping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MarioGPT: Open-Ended Text2Level Generation through Large Language Models. (arXiv:2302.05981v1 [cs.AI])","link":"http://arxiv.org/abs/2302.05981","description":"<p>Procedural Content Generation (PCG) algorithms provide a technique to\ngenerate complex and diverse environments in an automated way. However, while\ngenerating content with PCG methods is often straightforward, generating\nmeaningful content that reflects specific intentions and constraints remains\nchallenging. Furthermore, many PCG algorithms lack the ability to generate\ncontent in an open-ended manner. Recently, Large Language Models (LLMs) have\nshown to be incredibly effective in many diverse domains. These trained LLMs\ncan be fine-tuned, re-using information and accelerating training for new\ntasks. In this work, we introduce MarioGPT, a fine-tuned GPT2 model trained to\ngenerate tile-based game levels, in our case Super Mario Bros levels. We show\nthat MarioGPT can not only generate diverse levels, but can be text-prompted\nfor controllable level generation, addressing one of the key challenges of\ncurrent PCG techniques. As far as we know, MarioGPT is the first text-to-level\nmodel. We also combine MarioGPT with novelty search, enabling it to generate\ndiverse levels with varying play-style dynamics (i.e. player paths). This\ncombination allows for the open-ended generation of an increasingly diverse\nrange of content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sudhakaran_S/0/1/0/all/0/1\">Shyam Sudhakaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Duque_M/0/1/0/all/0/1\">Miguel Gonz&#xe1;lez-Duque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glanois_C/0/1/0/all/0/1\">Claire Glanois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freiberger_M/0/1/0/all/0/1\">Matthias Freiberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najarro_E/0/1/0/all/0/1\">Elias Najarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risi_S/0/1/0/all/0/1\">Sebastian Risi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASR Bundestag: A Large-Scale political debate dataset in German. (arXiv:2302.06008v1 [cs.CL])","link":"http://arxiv.org/abs/2302.06008","description":"<p>We present ASR Bundestag, a dataset for automatic speech recognition in\nGerman, consisting of 610 hours of aligned audio-transcript pairs for\nsupervised training as well as 1,038 hours of unlabeled audio snippets for\nself-supervised learning, based on raw audio data and transcriptions from\nplenary sessions and committee meetings of the German parliament. In addition,\nwe discuss utilized approaches for the automated creation of speech datasets\nand assess the quality of the resulting dataset based on evaluations and\nfinetuning of a pre-trained state of the art model. We make the dataset\npublicly available, including all subsets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wirth_J/0/1/0/all/0/1\">Johannes Wirth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peinl_R/0/1/0/all/0/1\">Ren&#xe9; Peinl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey on Publicly Available Sinhala Natural Language Processing Tools and Research. (arXiv:1906.02358v17 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1906.02358","description":"<p>Sinhala is the native language of the Sinhalese people who make up the\nlargest ethnic group of Sri Lanka. The language belongs to the globe-spanning\nlanguage tree, Indo-European. However, due to poverty in both linguistic and\neconomic capital, Sinhala, in the perspective of Natural Language Processing\ntools and research, remains a resource-poor language which has neither the\neconomic drive its cousin English has nor the sheer push of the law of numbers\na language such as Chinese has. A number of research groups from Sri Lanka have\nnoticed this dearth and the resultant dire need for proper tools and research\nfor Sinhala natural language processing. However, due to various reasons, these\nattempts seem to lack coordination and awareness of each other. The objective\nof this paper is to fill that gap of a comprehensive literature survey of the\npublicly available Sinhala natural language tools and research so that the\nresearchers working in this field can better utilize contributions of their\npeers. As such, we shall be uploading this paper to arXiv and perpetually\nupdate it periodically to reflect the advances made in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">Nisansa de Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The emojification of sentiment on social media: Collection and analysis of a longitudinal Twitter sentiment dataset. (arXiv:2108.13898v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2108.13898","description":"<p>Social media, as a means for computer-mediated communication, has been\nextensively used to study the sentiment expressed by users around events or\ntopics. There is however a gap in the longitudinal study of how sentiment\nevolved in social media over the years. To fill this gap, we develop TM-Senti,\na new large-scale, distantly supervised Twitter sentiment dataset with over 184\nmillion tweets and covering a time period of over seven years. We describe and\nassess our methodology to put together a large-scale, emoticon- and emoji-based\nlabelled sentiment analysis dataset, along with an analysis of the resulting\ndataset. Our analysis highlights interesting temporal changes, among others in\nthe increasing use of emojis over emoticons. We publicly release the dataset\nfor further research in tasks including sentiment analysis and text\nclassification of tweets. The dataset can be fully rehydrated including tweet\nmetadata and without missing tweets thanks to the archive of tweets publicly\navailable on the Internet Archive, which the dataset is based on.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenjie Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alkhalifa_R/0/1/0/all/0/1\">Rabab Alkhalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Multi-modal Summarization. (arXiv:2109.05199v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05199","description":"<p>The new era of technology has brought us to the point where it is convenient\nfor people to share their opinions over an abundance of platforms. These\nplatforms have a provision for the users to express themselves in multiple\nforms of representations, including text, images, videos, and audio. This,\nhowever, makes it difficult for users to obtain all the key information about a\ntopic, making the task of automatic multi-modal summarization (MMS) essential.\nIn this paper, we present a comprehensive survey of the existing research in\nthe area of MMS, covering various modalities like text, image, audio, and\nvideo. Apart from highlighting the different evaluation metrics and datasets\nused for the MMS task, our work also discusses the current challenges and\nfuture directions in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jangra_A/0/1/0/all/0/1\">Anubhav Jangra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Sourajit Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1\">Adam Jatowt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sriparna Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasanuzzaman_M/0/1/0/all/0/1\">Mohammad Hasanuzzaman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers. (arXiv:2112.09237v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.09237","description":"<p>Building natural language inference (NLI) benchmarks that are both\nchallenging for modern techniques, and free from shortcut biases is difficult.\nChief among these biases is \"single sentence label leakage,\" where\nannotator-introduced spurious correlations yield datasets where the logical\nrelation between (premise, hypothesis) pairs can be accurately predicted from\nonly a single sentence, something that should in principle be impossible. We\ndemonstrate that despite efforts to reduce this leakage, it persists in modern\ndatasets that have been introduced since its 2018 discovery. To enable future\namelioration efforts, introduce a novel model-driven technique, the progressive\nevaluation of cluster outliers (PECO) which enables both the objective\nmeasurement of leakage, and the automated detection of subpopulations in the\ndata which maximally exhibit it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenda Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The NLP Task Effectiveness of Long-Range Transformers. (arXiv:2202.07856v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.07856","description":"<p>Transformer models cannot easily scale to long sequences due to their O(N^2)\ntime and space complexity. This has led to Transformer variants seeking to\nlower computational complexity, such as Longformer and Performer. While such\nmodels have theoretically greater efficiency, their effectiveness on real NLP\ntasks has not been well studied. We benchmark 7 variants of Transformer models\non 5 difficult NLP tasks and 7 datasets. We design experiments to isolate the\neffect of pretraining and hyperparameter settings, to focus on their capacity\nfor long-range attention. Moreover, we present various methods to investigate\nattention behaviors to illuminate model details beyond metric scores. We find\nthat the modified attention in long-range transformers has advantages on\ncontent selection and query-guided decoding, but they come with previously\nunrecognized drawbacks such as insufficient attention to distant tokens and\naccumulated approximation error.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_G/0/1/0/all/0/1\">Guanghui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yukun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"USCORE: An Effective Approach to Fully Unsupervised Evaluation Metrics for Machine Translation. (arXiv:2202.10062v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.10062","description":"<p>The vast majority of evaluation metrics for machine translation are\nsupervised, i.e., (i) are trained on human scores, (ii) assume the existence of\nreference translations, or (iii) leverage parallel data. This hinders their\napplicability to cases where such supervision signals are not available. In\nthis work, we develop fully unsupervised evaluation metrics. To do so, we\nleverage similarities and synergies between evaluation metric induction,\nparallel corpus mining, and MT systems. In particular, we use an unsupervised\nevaluation metric to mine pseudo-parallel data, which we use to remap deficient\nunderlying vector spaces (in an iterative manner) and to induce an unsupervised\nMT system, which then provides pseudo-references as an additional component in\nthe metric. Finally, we also induce unsupervised multilingual sentence\nembeddings from pseudo-parallel data. We show that our fully unsupervised\nmetrics are effective, i.e., they beat supervised competitors on 4 out of our 5\nevaluation datasets. We make our code publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belouadi_J/0/1/0/all/0/1\">Jonas Belouadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A survey in Adversarial Defences and Robustness in NLP. (arXiv:2203.06414v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.06414","description":"<p>In recent years, it has been seen that deep neural networks are lacking\nrobustness and are vulnerable in case of adversarial perturbations in input\ndata. Strong adversarial attacks are proposed by various authors for tasks\nunder computer vision and Natural Language Processing (NLP). As a\ncounter-effort, several defense mechanisms are also proposed to save these\nnetworks from failing. Defending the neural networks from adversarial attacks\nhas its own importance, where the goal is to ensure that the model's prediction\ndoesn't change if input data is perturbed. Numerous methods for adversarial\ndefense in NLP are proposed of late, for different NLP tasks such as text\nclassification, named entity recognition, natural language inferencing, etc.\nSome of these methods are not just used for defending neural networks from\nadversarial attacks, but also used as a regularization mechanism during\ntraining, saving the model from overfitting. The proposed survey is an attempt\nto review different methods proposed for adversarial defenses in NLP in recent\nyears by proposing a novel taxonomy. This survey also highlights the fragility\nof the advanced deep neural networks in NLP and the challenges in defending\nthem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_S/0/1/0/all/0/1\">Shreya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doddapaneni_S/0/1/0/all/0/1\">Sumanth Doddapaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M.Khapra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravindran_B/0/1/0/all/0/1\">Balaraman Ravindran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of Fake News Detection with Knowledge-Enhanced Language Models. (arXiv:2204.00458v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.00458","description":"<p>Recent advances in fake news detection have exploited the success of\nlarge-scale pre-trained language models (PLMs). The predominant\nstate-of-the-art approaches are based on fine-tuning PLMs on labelled fake news\ndatasets. However, large-scale PLMs are generally not trained on structured\nfactual data and hence may not possess priors that are grounded in factually\naccurate knowledge. The use of existing knowledge bases (KBs) with rich\nhuman-curated factual information has thus the potential to make fake news\ndetection more effective and robust. In this paper, we investigate the impact\nof knowledge integration into PLMs for fake news detection. We study several\nstate-of-the-art approaches for knowledge integration, mostly using Wikidata as\nKB, on two popular fake news datasets - LIAR, a politics-based dataset, and\nCOVID-19, a dataset of messages posted on social media relating to the COVID-19\npandemic. Our experiments show that knowledge-enhanced models can significantly\nimprove fake news detection on LIAR where the KB is relevant and up-to-date.\nThe mixed results on COVID-19 highlight the reliance on stylistic features and\nthe importance of domain-specific and current KBs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Whitehouse_C/0/1/0/all/0/1\">Chenxi Whitehouse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weyde_T/0/1/0/all/0/1\">Tillman Weyde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhyastha_P/0/1/0/all/0/1\">Pranava Madhyastha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komninos_N/0/1/0/all/0/1\">Nikos Komninos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-specific Compression for Multi-task Language Models using Attribution-based Pruning. (arXiv:2205.04157v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.04157","description":"<p>Multi-task language models show outstanding performance for various natural\nlanguage understanding tasks with only a single model. However, these language\nmodels utilize an unnecessarily large number of model parameters, even when\nused only for a specific task. This paper proposes a novel training-free\ncompression method for multi-task language models using a pruning method.\nSpecifically, we use an attribution method to determine which neurons are\nessential for performing a specific task. We task-specifically prune\nunimportant neurons and leave only task-specific parameters. Furthermore, we\nextend our method to be applicable in low-resource and unsupervised settings.\nSince our compression method is training-free, it uses few computing resources\nand does not destroy the pre-trained knowledge of language models. Experimental\nresults on the six widely-used datasets show that our proposed pruning method\nsignificantly outperforms baseline pruning methods. In addition, we demonstrate\nthat our method preserves performance even in an unseen domain setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1\">Nakyeong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Yunah Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwanhee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Seohyeong Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1\">Kyomin Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BanglaNLG and BanglaT5: Benchmarks and Resources for Evaluating Low-Resource Natural Language Generation in Bangla. (arXiv:2205.11081v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11081","description":"<p>This work presents BanglaNLG, a comprehensive benchmark for evaluating\nnatural language generation (NLG) models in Bangla, a widely spoken yet\nlow-resource language. We aggregate six challenging conditional text generation\ntasks under the BanglaNLG benchmark, introducing a new dataset on dialogue\ngeneration in the process. Furthermore, using a clean corpus of 27.5 GB of\nBangla data, we pretrain BanglaT5, a sequence-to-sequence Transformer language\nmodel for Bangla. BanglaT5 achieves state-of-the-art performance in all of\nthese tasks, outperforming several multilingual models by up to 9% absolute\ngain and 32% relative gain. We are making the new dialogue dataset and the\nBanglaT5 model publicly available at https://github.com/csebuetnlp/BanglaNLG in\nthe hope of advancing future research on Bangla NLG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1\">Abhik Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_T/0/1/0/all/0/1\">Tahmid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahriyar_R/0/1/0/all/0/1\">Rifat Shahriyar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summarize and Generate to Back-translate: Unsupervised Translation of Programming Languages. (arXiv:2205.11116v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11116","description":"<p>Back-translation is widely known for its effectiveness in neural machine\ntranslation when there is little to no parallel data. In this approach, a\nsource-to-target model is coupled with a target-to-source model trained in\nparallel. The target-to-source model generates noisy sources, while the\nsource-to-target model is trained to reconstruct the targets and vice versa.\nRecent developments of multilingual pre-trained sequence-to-sequence models for\nprogramming languages have been very effective for a broad spectrum of\ndownstream software engineering tasks. Hence, training them to build\nprogramming language translation systems via back-translation is compelling.\nHowever, these models cannot be further trained via back-translation since they\nlearn to output sequences in the same language as the inputs during\npre-training. As an alternative, we propose performing back-translation via\ncode summarization and generation. In code summarization, a model learns to\ngenerate natural language (NL) summaries given code snippets. In code\ngeneration, the model learns to do the opposite. Therefore, target-to-source\ngeneration in back-translation can be viewed as a target-to-NL-to-source\ngeneration. We show that our proposed approach performs competitively with\nstate-of-the-art methods. We have made the code publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Saikat Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_B/0/1/0/all/0/1\">Baishakhi Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Workflow Discovery from Dialogues in the Low Data Regime. (arXiv:2205.11690v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11690","description":"<p>Text-based dialogues are now widely used to solve real-world problems. In\ncases where solution strategies are already known, they can sometimes be\ncodified into workflows and used to guide humans or artificial agents through\nthe task of helping clients. We introduce a new problem formulation that we\ncall Workflow Discovery (WD) in which we are interested in the situation where\na formal workflow may not yet exist. Still, we wish to discover the set of\nactions that have been taken to resolve a particular problem. We also examine a\nsequence-to-sequence (Seq2Seq) approach for this novel task. We present\nexperiments where we extract workflows from dialogues in the Action-Based\nConversations Dataset (ABCD). Since the ABCD dialogues follow known workflows\nto guide agents, we can evaluate our ability to extract such workflows using\nground truth sequences of actions. We propose and evaluate an approach that\nconditions models on the set of possible actions, and we show that using this\nstrategy, we can improve WD performance. Our conditioning approach also\nimproves zero-shot and few-shot WD performance when transferring learned models\nto unseen domains within and across datasets. Further, on ABCD a modified\nvariant of our Seq2Seq method achieves state-of-the-art performance on related\nbut different problems of Action State Tracking (AST) and Cascading Dialogue\nSuccess (CDS) across many evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hattami_A/0/1/0/all/0/1\">Amine El Hattami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raimondo_S/0/1/0/all/0/1\">Stefania Raimondo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laradji_I/0/1/0/all/0/1\">Issam Laradji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazquez_D/0/1/0/all/0/1\">David Vazquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1\">Pau Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Chris Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Reranking for Multi-hop QA via Language Model Prompting. (arXiv:2205.12650v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12650","description":"<p>We study few-shot reranking for multi-hop QA (MQA) with open-domain\nquestions. To alleviate the need for a large number of labeled\nquestion-document pairs for retriever training, we propose PromptRank, which\nrelies on large language models prompting for multi-hop path reranking.\nPromptRank first constructs an instruction-based prompt that includes a\ncandidate document path and then computes the relevance score between a given\nquestion and the path based on the conditional likelihood of the question given\nthe path prompt according to a language model. PromptRank yields strong\nretrieval performance on HotpotQA with only 128 training examples compared to\nstate-of-the-art methods trained on thousands of examples -- 73.6 recall@10 by\nPromptRank vs. 77.8 by PathRetriever and 77.5 by multi-hop dense retrieval.\nCode available at https://github.com/mukhal/PromptRank\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalifa_M/0/1/0/all/0/1\">Muhammad Khalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Logeswaran_L/0/1/0/all/0/1\">Lajanugen Logeswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Moontae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Task-Oriented Dialogue Systems with Response Selection as an Auxiliary Task. (arXiv:2208.07097v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.07097","description":"<p>The adoption of pre-trained language models in task-oriented dialogue systems\nhas resulted in significant enhancements of their text generation abilities.\nHowever, these architectures are slow to use because of the large number of\ntrainable parameters and can sometimes fail to generate diverse responses. To\naddress these limitations, we propose two models with auxiliary tasks for\nresponse selection - (1) distinguishing distractors from ground truth responses\nand (2) distinguishing synthetic responses from ground truth labels. They\nachieve state-of-the-art results on the MultiWOZ 2.1 dataset with combined\nscores of 107.5 and 108.3 and outperform a baseline with three times more\nparameters. We publish reproducible code and checkpoints and discuss the\neffects of applying auxiliary tasks to T5-based architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cholakov_R/0/1/0/all/0/1\">Radostin Cholakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolev_T/0/1/0/all/0/1\">Todor Kolev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A new hazard event classification model via deep learning and multifractal. (arXiv:2209.05263v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.05263","description":"<p>Hazard and operability analysis (HAZOP) is the paradigm of industrial safety\nthat can reveal the hazards of process from its node deviations, consequences,\ncauses, measures and suggestions, and such hazards can be considered as hazard\nevents (HaE). The classification research on HaE has much irreplaceable\npragmatic values. In this paper, we present a novel deep learning model termed\nDLF through multifractal to explore HaE classification where the motivation is\nthat HaE can be naturally regarded as a kind of time series. Specifically,\nfirst HaE is vectorized to get HaE time series by employing BERT. Then, a new\nmultifractal analysis method termed HmF-DFA is proposed to win HaE fractal\nseries by analyzing HaE time series. Finally, a new hierarchical gating neural\nnetwork (HGNN) is designed to process HaE fractal series to accomplish the\nclassification of HaE from three aspects: severity, possibility and risk. We\ntake HAZOP reports of 18 processes as cases, and launch the experiments on this\nbasis. Results demonstrate that compared with other classifiers, DLF classifier\nperforms better under metrics of precision, recall and F1-score, especially for\nthe severity aspect. Also, HmF-DFA and HGNN effectively promote HaE\nclassification. Our HaE classification system can serve application incentives\nto experts, engineers, employees, and other enterprises. We hope our research\ncan contribute added support to the daily practice in industrial safety.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Ming Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Dong Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dimensions of Interpersonal Dynamics in Text: Group Membership and Fine-grained Interpersonal Emotion. (arXiv:2209.06687v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.06687","description":"<p>Current studies of bias in NLP rely mainly on identifying (unwanted or\nnegative) bias towards a specific demographic group. While this has led to\nprogress recognizing and mitigating negative bias, and having a clear notion of\nthe targeted group is necessary, it is not always practical. In this work we\nextrapolate to a broader notion of bias, rooted in social science and\npsychology literature. We move towards predicting interpersonal group\nrelationship (IGR) - modeling the relationship between the speaker and the\ntarget in an utterance - using fine-grained interpersonal emotions as an\nanchor. We build and release a dataset of English tweets by US Congress members\nannotated for interpersonal emotion -- the first of its kind, and 'found\nsupervision' for IGR labels; our analyses show that subtle emotional signals\nare indicative of different biases. While humans can perform better than chance\nat identifying IGR given an utterance, we show that neural models perform much\nbetter; furthermore, a shared encoding between IGR and interpersonal perceived\nemotion enabled performance gains in both tasks. Data and code for this paper\nare available at https://github.com/venkatasg/interpersonal-bias\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Govindarajan_V/0/1/0/all/0/1\">Venkata S Govindarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atwell_K/0/1/0/all/0/1\">Katherine Atwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinno_B/0/1/0/all/0/1\">Barea Sinno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaver_D/0/1/0/all/0/1\">David I. Beaver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Transformer Memorization Recall Through Idioms. (arXiv:2210.03588v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03588","description":"<p>To produce accurate predictions, language models (LMs) must balance between\ngeneralization and memorization. Yet, little is known about the mechanism by\nwhich transformer LMs employ their memorization capacity. When does a model\ndecide to output a memorized phrase, and how is this phrase then retrieved from\nmemory? In this work, we offer the first methodological framework for probing\nand characterizing recall of memorized sequences in transformer LMs. First, we\nlay out criteria for detecting model inputs that trigger memory recall, and\npropose idioms as inputs that typically fulfill these criteria. Next, we\nconstruct a dataset of English idioms and use it to compare model behavior on\nmemorized vs. non-memorized inputs. Specifically, we analyze the internal\nprediction construction process by interpreting the model's hidden\nrepresentations as a gradual refinement of the output probability distribution.\nWe find that across different model sizes and architectures, memorized\npredictions are a two-step process: early layers promote the predicted token to\nthe top of the output distribution, and upper layers increase model confidence.\nThis suggests that memorized information is stored and retrieved in the early\nlayers of the network. Last, we demonstrate the utility of our methodology\nbeyond idioms in memorized factual statements. Overall, our work makes a first\nstep towards understanding memory recall, and provides a methodological basis\nfor future studies of transformer memorization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haviv_A/0/1/0/all/0/1\">Adi Haviv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1\">Ido Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gidron_J/0/1/0/all/0/1\">Jacob Gidron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_R/0/1/0/all/0/1\">Roei Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CHARD: Clinical Health-Aware Reasoning Across Dimensions for Text Generation Models. (arXiv:2210.04191v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04191","description":"<p>We motivate and introduce CHARD: Clinical Health-Aware Reasoning across\nDimensions, to investigate the capability of text generation models to act as\nimplicit clinical knowledge bases and generate free-flow textual explanations\nabout various health-related conditions across several dimensions. We collect\nand present an associated dataset, CHARDat, consisting of explanations about 52\nhealth conditions across three clinical dimensions. We conduct extensive\nexperiments using BART and T5 along with data augmentation, and perform\nautomatic, human, and qualitative analyses. We show that while our models can\nperform decently, CHARD is very challenging with strong potential for further\nexploration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Steven Y. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khetan_V/0/1/0/all/0/1\">Vivek Khetan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sacaleanu_B/0/1/0/all/0/1\">Bogdan Sacaleanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gershman_A/0/1/0/all/0/1\">Anatole Gershman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustification of Multilingual Language Models to Real-world Noise in Crosslingual Zero-shot Settings with Robust Contrastive Pretraining. (arXiv:2210.04782v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04782","description":"<p>Advances in neural modeling have achieved state-of-the-art (SOTA) results on\npublic natural language processing (NLP) benchmarks, at times surpassing human\nperformance. However, there is a gap between public benchmarks and real-world\napplications where noise, such as typographical or grammatical mistakes, is\nabundant and can result in degraded performance. Unfortunately, works which\nevaluate the robustness of neural models on noisy data and propose\nimprovements, are limited to the English language. Upon analyzing noise in\ndifferent languages, we observe that noise types vary greatly across languages.\nThus, existing investigations do not generalize trivially to multilingual\nsettings. To benchmark the performance of pretrained multilingual language\nmodels, we construct noisy datasets covering five languages and four NLP tasks\nand observe a clear gap in the performance between clean and noisy data in the\nzero-shot cross-lingual setting. After investigating several ways to boost the\nrobustness of multilingual models in this setting, we propose Robust\nContrastive Pretraining (RCP). RCP combines data augmentation with a\ncontrastive loss term at the pretraining stage and achieves large improvements\non noisy (and original test data) across two sentence-level (+3.2%) and two\nsequence-labeling (+10 F1-score) multilingual classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stickland_A/0/1/0/all/0/1\">Asa Cooper Stickland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Sailik Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krone_J/0/1/0/all/0/1\">Jason Krone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_S/0/1/0/all/0/1\">Saab Mansour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vote'n'Rank: Revision of Benchmarking with Social Choice Theory. (arXiv:2210.05769v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.05769","description":"<p>The development of state-of-the-art systems in different applied areas of\nmachine learning (ML) is driven by benchmarks, which have shaped the paradigm\nof evaluating generalisation capabilities from multiple perspectives. Although\nthe paradigm is shifting towards more fine-grained evaluation across diverse\ntasks, the delicate question of how to aggregate the performances has received\nparticular interest in the community. In general, benchmarks follow the\nunspoken utilitarian principles, where the systems are ranked based on their\nmean average score over task-specific metrics. Such aggregation procedure has\nbeen viewed as a sub-optimal evaluation protocol, which may have created the\nillusion of progress. This paper proposes Vote'n'Rank, a framework for ranking\nsystems in multi-task benchmarks under the principles of the social choice\ntheory. We demonstrate that our approach can be efficiently utilised to draw\nnew insights on benchmarking in several ML sub-fields and identify the\nbest-performing systems in research and development case studies. The\nVote'n'Rank's procedures are more robust than the mean average while being able\nto handle missing performance scores and determine conditions under which the\nsystem becomes the winner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rofin_M/0/1/0/all/0/1\">Mark Rofin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikhailov_V/0/1/0/all/0/1\">Vladislav Mikhailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florinskiy_M/0/1/0/all/0/1\">Mikhail Florinskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kravchenko_A/0/1/0/all/0/1\">Andrey Kravchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tutubalina_E/0/1/0/all/0/1\">Elena Tutubalina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shavrina_T/0/1/0/all/0/1\">Tatiana Shavrina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karabekyan_D/0/1/0/all/0/1\">Daniel Karabekyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Continual Learning for Text Classification via Selective Inter-client Transfer. (arXiv:2210.06101v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06101","description":"<p>In this work, we combine the two paradigms: Federated Learning (FL) and\nContinual Learning (CL) for text classification task in cloud-edge continuum.\nThe objective of Federated Continual Learning (FCL) is to improve deep learning\nmodels over life time at each client by (relevant and efficient) knowledge\ntransfer without sharing data. Here, we address challenges in minimizing\ninter-client interference while knowledge sharing due to heterogeneous tasks\nacross clients in FCL setup. In doing so, we propose a novel framework,\nFederated Selective Inter-client Transfer (FedSeIT) which selectively combines\nmodel parameters of foreign clients. To further maximize knowledge transfer, we\nassess domain overlap and select informative tasks from the sequence of\nhistorical tasks at each foreign client while preserving privacy. Evaluating\nagainst the baselines, we show improved performance, a gain of (average) 12.4\\%\nin text classification over a sequence of tasks using five datasets from\ndiverse domains. To the best of our knowledge, this is the first work that\napplies FCL to NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_Y/0/1/0/all/0/1\">Yatin Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rai_P/0/1/0/all/0/1\">Pranav Rai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubert_M/0/1/0/all/0/1\">Matthias Schubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Pankaj Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Pretrained Language Models (Yet) Reason Deductively?. (arXiv:2210.06442v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06442","description":"<p>Acquiring factual knowledge with Pretrained Language Models (PLMs) has\nattracted increasing attention, showing promising performance in many\nknowledge-intensive tasks. Their good performance has led the community to\nbelieve that the models do possess a modicum of reasoning competence rather\nthan merely memorising the knowledge. In this paper, we conduct a comprehensive\nevaluation of the learnable deductive (also known as explicit) reasoning\ncapability of PLMs. Through a series of controlled experiments, we posit two\nmain findings. (i) PLMs inadequately generalise learned logic rules and perform\ninconsistently against simple adversarial surface form edits. (ii) While the\ndeductive reasoning fine-tuning of PLMs does improve their performance on\nreasoning over unseen knowledge facts, it results in catastrophically\nforgetting the previously learnt knowledge. Our main results suggest that PLMs\ncannot yet perform reliable deductive reasoning, demonstrating the importance\nof controlled examinations and probing of PLMs' reasoning abilities; we reach\nbeyond (misleading) task performance, revealing that PLMs are still far from\nhuman-level reasoning capabilities, even for simple deductive tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhangdie Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zaiqiao Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity Tracking via Effective Use of Multi-Task Learning Model and Mention-guided Decoding. (arXiv:2210.06444v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06444","description":"<p>Cross-task knowledge transfer via multi-task learning has recently made\nremarkable progress in general NLP tasks. However, entity tracking on the\nprocedural text has not benefited from such knowledge transfer because of its\ndistinct formulation, i.e., tracking the event flow while following structural\nconstraints. State-of-the-art entity tracking approaches either design\ncomplicated model architectures or rely on task-specific pre-training to\nachieve good results. To this end, we propose MeeT, a Multi-task\nlearning-enabled entity Tracking approach, which utilizes knowledge gained from\ngeneral domain tasks to improve entity tracking. Specifically, MeeT first\nfine-tunes T5, a pre-trained multi-task learning model, with entity\ntracking-specialized QA formats, and then employs our customized decoding\nstrategy to satisfy the structural constraints. MeeT achieves state-of-the-art\nperformances on two popular entity tracking datasets, even though it does not\nrequire any task-specific architecture design or pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Janvijay Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_F/0/1/0/all/0/1\">Fan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhen Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Document-level Information Extraction via Imitation Learning. (arXiv:2210.06600v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06600","description":"<p>We present a novel iterative extraction model, IterX, for extracting complex\nrelations, or templates, i.e., N-tuples representing a mapping from named slots\nto spans of text within a document. Documents may feature zero or more\ninstances of a template of any given type, and the task of template extraction\nentails identifying the templates in a document and extracting each template's\nslot values. Our imitation learning approach casts the problem as a Markov\ndecision process (MDP), and relieves the need to use predefined template orders\nto train an extractor. It leads to state-of-the-art results on two established\nbenchmarks -- 4-ary relation extraction on SciREX and template extraction on\nMUC-4 -- as well as a strong baseline on the new BETTER Granular task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunmo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gantt_W/0/1/0/all/0/1\">William Gantt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1\">Weiwei Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tongfei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1\">Aaron Steven White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shortcomings of Question Answering Based Factuality Frameworks for Error Localization. (arXiv:2210.06748v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06748","description":"<p>Despite recent progress in abstractive summarization, models often generate\nsummaries with factual errors. Numerous approaches to detect these errors have\nbeen proposed, the most popular of which are question answering (QA)-based\nfactuality metrics. These have been shown to work well at predicting\nsummary-level factuality and have potential to localize errors within\nsummaries, but this latter capability has not been systematically evaluated in\npast research. In this paper, we conduct the first such analysis and find that,\ncontrary to our expectations, QA-based frameworks fail to correctly identify\nerror spans in generated summaries and are outperformed by trivial exact match\nbaselines. Our analysis reveals a major reason for such poor localization:\nquestions generated by the QG module often inherit errors from non-factual\nsummaries which are then propagated further into downstream modules. Moreover,\neven human-in-the-loop question generation cannot easily offset these problems.\nOur experiments conclusively show that there exist fundamental issues with\nlocalization using the QA framework which cannot be fixed solely by stronger QA\nand QG models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamoi_R/0/1/0/all/0/1\">Ryo Kamoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_T/0/1/0/all/0/1\">Tanya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Closed-book Question Generation via Contrastive Learning. (arXiv:2210.06781v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06781","description":"<p>Question Generation (QG) is a fundamental NLP task for many downstream\napplications. Recent studies on open-book QG, where supportive answer-context\npairs are provided to models, have achieved promising progress. However,\ngenerating natural questions under a more practical closed-book setting that\nlacks these supporting documents still remains a challenge. In this work, we\npropose a new QG model for this closed-book setting that is designed to better\nunderstand the semantics of long-form abstractive answers and store more\ninformation in its parameters through contrastive learning and an answer\nreconstruction module. Through experiments, we validate the proposed QG model\non both public datasets and a new WikiCQA dataset. Empirical results show that\nthe proposed QG model outperforms baselines in both automatic evaluation and\nhuman evaluation. In addition, we show how to leverage the proposed model to\nimprove existing question-answering systems. These results further indicate the\neffectiveness of our QG model for enhancing closed-book question-answering\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiangjue Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiaying Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caverlee_J/0/1/0/all/0/1\">James Caverlee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bootstrapping Multilingual Semantic Parsers using Large Language Models. (arXiv:2210.07313v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07313","description":"<p>Despite cross-lingual generalization demonstrated by pre-trained multilingual\nmodels, the translate-train paradigm of transferring English datasets across\nmultiple languages remains to be a key mechanism for training task-specific\nmultilingual models. However, for many low-resource languages, the availability\nof a reliable translation service entails significant amounts of costly\nhuman-annotated translation pairs. Further, translation services may continue\nto be brittle due to domain mismatch between task-specific input text and\ngeneral-purpose text used for training translation models. For multilingual\nsemantic parsing, we demonstrate the effectiveness and flexibility offered by\nlarge language models (LLMs) for translating English datasets into several\nlanguages via few-shot prompting. Through extensive comparisons on two public\ndatasets, MTOP and MASSIVE, spanning 50 languages and several domains, we show\nthat our method of translating data using LLMs outperforms a strong\ntranslate-train baseline on 41 out of 50 languages. We study the key design\nchoices that enable more effective multilingual data translation via prompted\nLLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Awasthi_A/0/1/0/all/0/1\">Abhijeet Awasthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1\">Nitish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samanta_B/0/1/0/all/0/1\">Bidisha Samanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dave_S/0/1/0/all/0/1\">Shachi Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1\">Sunita Sarawagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mind the Labels: Describing Relations in Knowledge Graphs With Pretrained Models. (arXiv:2210.07373v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07373","description":"<p>Pretrained language models (PLMs) for data-to-text (D2T) generation can use\nhuman-readable data labels such as column headings, keys, or relation names to\ngeneralize to out-of-domain examples. However, the models are well-known in\nproducing semantically inaccurate outputs if these labels are ambiguous or\nincomplete, which is often the case in D2T datasets. In this paper, we expose\nthis issue on the task of descibing a relation between two entities. For our\nexperiments, we collect a novel dataset for verbalizing a diverse set of 1,522\nunique relations from three large-scale knowledge graphs (Wikidata, DBPedia,\nYAGO). We find that although PLMs for D2T generation expectedly fail on unclear\ncases, models trained with a large variety of relation labels are surprisingly\nrobust in verbalizing novel, unseen relations. We argue that using data with a\ndiverse set of clear and meaningful labels is key to training D2T generation\nsystems capable of generalizing to novel domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kasner_Z/0/1/0/all/0/1\">Zden&#x11b;k Kasner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konstas_I/0/1/0/all/0/1\">Ioannis Konstas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1\">Ond&#x159;ej Du&#x161;ek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Behavior Cloned Transformers are Neurosymbolic Reasoners. (arXiv:2210.07382v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07382","description":"<p>In this work, we explore techniques for augmenting interactive agents with\ninformation from symbolic modules, much like humans use tools like calculators\nand GPS systems to assist with arithmetic and navigation. We test our agent's\nabilities in text games -- challenging benchmarks for evaluating the multi-step\nreasoning abilities of game agents in grounded, language-based environments.\nOur experimental study indicates that injecting the actions from these symbolic\nmodules into the action space of a behavior cloned transformer agent increases\nperformance on four text game benchmarks that test arithmetic, navigation,\nsorting, and common sense reasoning by an average of 22%, allowing an agent to\nreach the highest possible performance on unseen games. This action injection\ntechnique is easily extended to new agents, environments, and symbolic modules.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruoyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jansen_P/0/1/0/all/0/1\">Peter Jansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cote_M/0/1/0/all/0/1\">Marc-Alexandre C&#xf4;t&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SQA3D: Situated Question Answering in 3D Scenes. (arXiv:2210.07474v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.07474","description":"<p>We propose a new task to benchmark scene understanding of embodied agents:\nSituated Question Answering in 3D Scenes (SQA3D). Given a scene context (e.g.,\n3D scan), SQA3D requires the tested agent to first understand its situation\n(position, orientation, etc.) in the 3D scene as described by text, then reason\nabout its surrounding environment and answer a question under that situation.\nBased upon 650 scenes from ScanNet, we provide a dataset centered around 6.8k\nunique situations, along with 20.4k descriptions and 33.4k diverse reasoning\nquestions for these situations. These questions examine a wide spectrum of\nreasoning capabilities for an intelligent agent, ranging from spatial relation\ncomprehension to commonsense understanding, navigation, and multi-hop\nreasoning. SQA3D imposes a significant challenge to current multi-modal\nespecially 3D reasoning models. We evaluate various state-of-the-art approaches\nand find that the best one only achieves an overall score of 47.20%, while\namateur human participants can reach 90.06%. We believe SQA3D could facilitate\nfuture embodied AI research with stronger situation understanding and reasoning\ncapability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yong_S/0/1/0/all/0/1\">Silong Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zilong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yitao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyuan Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConEntail: An Entailment-based Framework for Universal Zero and Few Shot Classification with Supervised Contrastive Pretraining. (arXiv:2210.07587v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07587","description":"<p>A universal classification model aims to generalize to diverse classification\ntasks in both zero and few shot settings. A promising way toward universal\nclassification is to cast heterogeneous data formats into a dataset-agnostic\n\"meta-task\" (e.g., textual entailment, question answering) then pretrain a\nmodel on the combined meta dataset. The existing work is either pretrained on\nspecific subsets of classification tasks, or pretrained on both classification\nand generation data but the model could not fulfill its potential in\nuniversality and reliability. These also leave a massive amount of annotated\ndata under-exploited. To fill these gaps, we propose ConEntail, a new framework\nfor universal zero and few shot classification with supervised contrastive\npretraining. Our unified meta-task for classification is based on nested\nentailment. It can be interpreted as \"Does sentence a entails [sentence b\nentails label c]\". This formulation enables us to make better use of 57\nannotated classification datasets for supervised contrastive pretraining and\nuniversal evaluation. In this way, ConEntail helps the model (1) absorb\nknowledge from different datasets, and (2) gain consistent performance gain\nwith more pretraining data. In experiments, we compare our model with\ndiscriminative and generative models pretrained on the same dataset. The\nresults confirm that our framework effectively exploits existing annotated data\nand consistently outperforms baselines in both zero (9.4% average improvement)\nand few shot settings (3.5% average improvement).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ranran Haoran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1\">Aysa Xuemo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Cultural Commonsense Knowledge at Scale. (arXiv:2210.07763v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07763","description":"<p>Structured knowledge is important for many AI applications. Commonsense\nknowledge, which is crucial for robust human-centric AI, is covered by a small\nnumber of structured knowledge projects. However, they lack knowledge about\nhuman traits and behaviors conditioned on socio-cultural contexts, which is\ncrucial for situative AI. This paper presents CANDLE, an end-to-end methodology\nfor extracting high-quality cultural commonsense knowledge (CCSK) at scale.\nCANDLE extracts CCSK assertions from a huge web corpus and organizes them into\ncoherent clusters, for 3 domains of subjects (geography, religion, occupation)\nand several cultural facets (food, drinks, clothing, traditions, rituals,\nbehaviors). CANDLE includes judicious techniques for classification-based\nfiltering and scoring of interestingness. Experimental evaluations show the\nsuperiority of the CANDLE CCSK collection over prior works, and an extrinsic\nuse case demonstrates the benefits of CCSK for the GPT-3 language model. Code\nand data can be accessed at https://candle.mpi-inf.mpg.de/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan-Phong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1\">Simon Razniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varde_A/0/1/0/all/0/1\">Aparna Varde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EntityCS: Improving Zero-Shot Cross-lingual Transfer with Entity-Centric Code Switching. (arXiv:2210.12540v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12540","description":"<p>Accurate alignment between languages is fundamental for improving\ncross-lingual pre-trained language models (XLMs). Motivated by the natural\nphenomenon of code-switching (CS) in multilingual speakers, CS has been used as\nan effective data augmentation method that offers language alignment at the\nword- or phrase-level, in contrast to sentence-level via parallel instances.\nExisting approaches either use dictionaries or parallel sentences with word\nalignment to generate CS data by randomly switching words in a sentence.\nHowever, such methods can be suboptimal as dictionaries disregard semantics,\nand syntax might become invalid after random word switching. In this work, we\npropose EntityCS, a method that focuses on Entity-level Code-Switching to\ncapture fine-grained cross-lingual semantics without corrupting syntax. We use\nWikidata and English Wikipedia to construct an entity-centric CS corpus by\nswitching entities to their counterparts in other languages. We further propose\nentity-oriented masking strategies during intermediate model training on the\nEntityCS corpus for improving entity prediction. Evaluation of the trained\nmodels on four entity-centric downstream tasks shows consistent improvements\nover the baseline with a notable increase of 10% in Fact Retrieval. We release\nthe corpus and models to assist research on code-switching and enriching XLMs\nwith external knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Whitehouse_C/0/1/0/all/0/1\">Chenxi Whitehouse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christopoulou_F/0/1/0/all/0/1\">Fenia Christopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iacobacci_I/0/1/0/all/0/1\">Ignacio Iacobacci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not Just Plain Text! Fuel Document-Level Relation Extraction with Explicit Syntax Refinement and Subsentence Modeling. (arXiv:2211.05343v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05343","description":"<p>Document-level relation extraction (DocRE) aims to identify semantic labels\namong entities within a single document. One major challenge of DocRE is to dig\ndecisive details regarding a specific entity pair from long text. However, in\nmany cases, only a fraction of text carries required information, even in the\nmanually labeled supporting evidence. To better capture and exploit instructive\ninformation, we propose a novel expLicit syntAx Refinement and Subsentence\nmOdeliNg based framework (LARSON). By introducing extra syntactic information,\nLARSON can model subsentences of arbitrary granularity and efficiently screen\ninstructive ones. Moreover, we incorporate refined syntax into text\nrepresentations which further improves the performance of LARSON. Experimental\nresults on three benchmark datasets (DocRED, CDR, and GDA) demonstrate that\nLARSON significantly outperforms existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1\">Zhichao Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiuxing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Narrative Information and the Distillation of Stories. (arXiv:2211.12423v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.12423","description":"<p>The act of telling stories is a fundamental part of what it means to be\nhuman. This work introduces the concept of narrative information, which we\ndefine to be the overlap in information space between a story and the items\nthat compose the story. Using contrastive learning methods, we show how modern\nartificial neural networks can be leveraged to distill stories and extract a\nrepresentation of the narrative information. We then demonstrate how\nevolutionary algorithms can leverage this to extract a set of narrative\ntemplates and how these templates -- in tandem with a novel curve-fitting\nalgorithm we introduce -- can reorder music albums to automatically induce\nstories in them. In the process of doing so, we give strong statistical\nevidence that these narrative information templates are present in existing\nalbums. While we experiment only with music albums here, the premises of our\nwork extend to any form of (largely) independent media.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ashley_D/0/1/0/all/0/1\">Dylan R. Ashley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herrmann_V/0/1/0/all/0/1\">Vincent Herrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friggstad_Z/0/1/0/all/0/1\">Zachary Friggstad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1\">J&#xfc;rgen Schmidhuber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explicit Knowledge Transfer for Weakly-Supervised Code Generation. (arXiv:2211.16740v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.16740","description":"<p>Large language models (LLMs) can acquire strong code-generation capabilities\nthrough few-shot learning. In contrast, supervised fine-tuning is still needed\nfor smaller models to achieve good performance. Such fine-tuning demands a\nlarge number of task-specific NL-code pairs, which are expensive to obtain. In\nthis paper, we attempt to transfer the code generation ability of an LLM to a\nsmaller model with the aid of weakly-supervised data. More specifically, we\npropose explicit knowledge transfer (EKT), which uses the few-shot capabilities\nof a teacher LLM to create NL-code pairs that we then filter for correctness\nand fine-tune the student on. We evaluate EKT on the task of generating code\nsolutions to math word problems from the GSM8k dataset. We find that EKT not\nonly yields better performance than training with expert iteration, but also\noutperforms knowledge distillation, another form of knowledge transfer. A\nGPT-Neo 1.3B model trained using EKT with a GPT-J teacher achieves a 12.4%\npass@100 on GSM8k, while the same student and teacher trained with knowledge\ndistillation yield only a 3.7% pass@100. We also show that it is possible for a\nstudent model to outperform the teacher using EKT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azerbayev_Z/0/1/0/all/0/1\">Zhangir Azerbayev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1\">Ansong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoelkopf_H/0/1/0/all/0/1\">Hailey Schoelkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"APOLLO: An Optimized Training Approach for Long-form Numerical Reasoning. (arXiv:2212.07249v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.07249","description":"<p>Long-form numerical reasoning in financial analysis aims to generate a\nreasoning program to calculate the correct answer for a given question.\nPrevious work followed a retriever-generator framework, where the retriever\nselects key facts from a long-form document, and the generator generates a\nreasoning program based on retrieved facts. However, they treated all facts\nequally without considering the different contributions of facts with and\nwithout numbers. Meanwhile, the program consistency were ignored under\nsupervised training, resulting in lower training accuracy and diversity. To\nsolve these problems, we proposed APOLLO to improve the long-form numerical\nreasoning framework. For the retriever, we adopt a number-aware negative\nsampling strategy to enable the retriever to be more discriminative on key\nnumerical facts. For the generator, we design consistency-based reinforcement\nlearning and target program augmentation strategy based on the consistency of\nprogram execution results. Experimental results on the FinQA and ConvFinQA\nleaderboard verify the effectiveness of our proposed method, achieving the new\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiashuo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jian Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models. (arXiv:2212.08037v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08037","description":"<p>Large language models (LLMs) have shown impressive results while requiring\nlittle or no direct supervision. Further, there is mounting evidence that LLMs\nmay have potential in information-seeking scenarios. We believe the ability of\nan LLM to attribute the text that it generates is likely to be crucial in this\nsetting. We formulate and study Attributed QA as a key first step in the\ndevelopment of attributed LLMs. We propose a reproducible evaluation framework\nfor the task and benchmark a broad set of architectures. We take human\nannotations as a gold standard and show that a correlated automatic metric is\nsuitable for development. Our experimental work gives concrete answers to two\nkey questions (How to measure attribution?, and How well do current\nstate-of-the-art methods perform on attribution?), and give some hints as to\nhow to address a third (How to build LLMs with attribution?).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bohnet_B/0/1/0/all/0/1\">Bernd Bohnet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verga_P/0/1/0/all/0/1\">Pat Verga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aharoni_R/0/1/0/all/0/1\">Roee Aharoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andor_D/0/1/0/all/0/1\">Daniel Andor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_L/0/1/0/all/0/1\">Livio Baldini Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciaramita_M/0/1/0/all/0/1\">Massimiliano Ciaramita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1\">Jacob Eisenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganchev_K/0/1/0/all/0/1\">Kuzman Ganchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1\">Jonathan Herzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_K/0/1/0/all/0/1\">Kai Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwiatkowski_T/0/1/0/all/0/1\">Tom Kwiatkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Ji Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jianmo Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saralegui_L/0/1/0/all/0/1\">Lierni Sestorain Saralegui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1\">Michael Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Dipanjan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrov_S/0/1/0/all/0/1\">Slav Petrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webster_K/0/1/0/all/0/1\">Kellie Webster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AnyTOD: A Programmable Task-Oriented Dialog System. (arXiv:2212.09939v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09939","description":"<p>We propose AnyTOD, an end-to-end, zero-shot task-oriented dialog (TOD) system\ncapable of handling unseen tasks without task-specific training. We view TOD as\na program executed by a language model (LM), where program logic and ontology\nis provided by a designer as a schema. To enable generalization to unseen\nschemas and programs without prior training, AnyTOD adopts a neuro-symbolic\napproach. A neural LM keeps track of events occurring during a conversation and\na symbolic program implementing the dialog policy is executed to recommend next\nactions AnyTOD should take. This approach drastically reduces data annotation\nand model training requirements, addressing the enduring challenge of rapidly\nadapting a TOD system to unseen tasks and domains. We demonstrate\nstate-of-the-art results on STAR, ABCD and SGD benchmarks. We also demonstrate\nstrong zero-shot transfer ability in low-resource settings, such as zero-shot\non MultiWOZ. In addition, we release STARv2, an updated version of the STAR\ndataset with richer annotations, for benchmarking zero-shot end-to-end TOD\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jeffrey Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Raghav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Harrison Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1\">Abhinav Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingqiu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltau_H/0/1/0/all/0/1\">Hagen Soltau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafran_I/0/1/0/all/0/1\">Izhak Shafran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hungry Hungry Hippos: Towards Language Modeling with State Space Models. (arXiv:2212.14052v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2212.14052","description":"<p>State space models (SSMs) have demonstrated state-of-the-art sequence\nmodeling performance in some modalities, but underperform attention in language\nmodeling. Moreover, despite scaling nearly linearly in sequence length instead\nof quadratically, SSMs are still slower than Transformers due to poor hardware\nutilization. In this paper, we make progress on understanding the expressivity\ngap between SSMs and attention in language modeling, and on reducing the\nhardware barrier between SSMs and attention. First, we use synthetic language\nmodeling tasks to understand the gap between SSMs and attention. We find that\nexisting SSMs struggle with two capabilities: recalling earlier tokens in the\nsequence and comparing tokens across the sequence. To understand the impact on\nlanguage modeling, we propose a new SSM layer, H3, that is explicitly designed\nfor these abilities. H3 matches attention on the synthetic languages and comes\nwithin 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid\n125M-parameter H3-attention model that retains two attention layers\nsurprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to\nimprove the efficiency of training SSMs on modern hardware, we propose\nFlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on\nsequences up to 8K, and introduces a novel state passing algorithm that\nexploits the recurrent properties of SSMs to scale to longer sequences.\nFlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows\nhybrid language models to generate text 2.4$\\times$ faster than Transformers.\nUsing FlashConv, we scale hybrid H3-attention language models up to 2.7B\nparameters on the Pile and find promising initial results, achieving lower\nperplexity than Transformers and outperforming Transformers in zero- and\nfew-shot learning on a majority of tasks in the SuperGLUE benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dao_T/0/1/0/all/0/1\">Tri Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1\">Daniel Y. Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saab_K/0/1/0/all/0/1\">Khaled K. Saab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_A/0/1/0/all/0/1\">Armin W. Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudra_A/0/1/0/all/0/1\">Atri Rudra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.10405","description":"<p>Recently decades have witnessed the empirical success of framing Knowledge\nGraph (KG) embeddings via language models. However, language model-based KG\nembeddings are usually deployed as static artifacts, which are challenging to\nmodify without re-training after deployment. To address this issue, we propose\na new task of editing language model-based KG embeddings in this paper. The\nproposed task aims to enable data-efficient and fast updates to KG embeddings\nwithout damaging the performance of the rest. We build four new datasets:\nE-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge\nediting baselines demonstrating the limited ability of previous models to\nhandle the proposed challenging task. We further propose a simple yet strong\nbaseline dubbed KGEditor, which utilizes additional parametric layers of the\nhyper network to edit/add facts. Comprehensive experimental results demonstrate\nthat KGEditor can perform better when updating specific facts while not\naffecting the rest with low training resources. Code and datasets will be\navailable in https://github.com/zjunlp/PromptKG/tree/main/deltaKG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1\">Bozhong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zelin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI vs. Human -- Differentiation Analysis of Scientific Content Generation. (arXiv:2301.10416v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.10416","description":"<p>Recent neural language models have taken a significant step forward in\nproducing remarkably controllable, fluent, and grammatical text. Although\nstudies have found that AI-generated text is not distinguishable from\nhuman-written text for crowd-sourcing workers, there still exist errors in\nAI-generated text which are even subtler and harder to spot. We primarily focus\non the scenario in which scientific AI writing assistant is deeply involved.\nFirst, we construct a feature description framework to distinguish between\nAI-generated text and human-written text from syntax, semantics, and pragmatics\nbased on the human evaluation. Then we utilize the features, i.e., writing\nstyle, coherence, consistency, and argument logistics, from the proposed\nframework to analyze two types of content. Finally, we adopt several publicly\navailable methods to investigate the gap of between AI-generated scientific\ntext and human-written scientific text by AI-generated scientific text\ndetection models. The results suggest that while AI has the potential to\ngenerate scientific content that is as accurate as human-written content, there\nis still a gap in terms of depth and overall quality. The AI-generated\nscientific content is more likely to contain errors in factual issues. We find\nthat there exists a \"writing style\" gap between AI-generated scientific text\nand human-written scientific text. Based on the analysis result, we summarize a\nseries of model-agnostic and distribution-agnostic features for detection tasks\nin other domains. Findings in this paper contribute to guiding the optimization\nof AI models to produce high-quality content and addressing related ethical and\nsecurity concerns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yongqiang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_F/0/1/0/all/0/1\">Fan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1\">Qikai Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaozhong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering. (arXiv:2301.10799v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.10799","description":"<p>The field of visual question answering (VQA) has recently seen a surge in\nresearch focused on providing explanations for predicted answers. However,\ncurrent systems mostly rely on separate models to predict answers and generate\nexplanations, leading to less grounded and frequently inconsistent results. To\naddress this, we propose a multitask learning approach towards a Unified Model\nfor Answer and Explanation generation (UMAE). Our approach involves the\naddition of artificial prompt tokens to training data and fine-tuning a\nmultimodal encoder-decoder model on a variety of VQA-related tasks. In our\nexperiments, UMAE models surpass the prior state-of-the-art answer accuracy on\nA-OKVQA by 10~15%, show competitive results on OK-VQA, achieve new\nstate-of-the-art explanation scores on A-OKVQA and VCR, and demonstrate\npromising out-of-domain performance on VQA-X.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Whitehouse_C/0/1/0/all/0/1\">Chenxi Whitehouse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weyde_T/0/1/0/all/0/1\">Tillman Weyde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhyastha_P/0/1/0/all/0/1\">Pranava Madhyastha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Dynamic Prompting for Response Generation in Task-oriented Dialog Systems. (arXiv:2301.13268v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.13268","description":"<p>Response generation is one of the critical components in task-oriented dialog\nsystems. Existing studies have shown that large pre-trained language models can\nbe adapted to this task. The typical paradigm of adapting such extremely large\nlanguage models would be by fine-tuning on the downstream tasks which is not\nonly time-consuming but also involves significant resources and access to\nfine-tuning data. Prompting (Schick and Sch\\\"utze, 2020) has been an\nalternative to fine-tuning in many NLP tasks. In our work, we explore the idea\nof using prompting for response generation in task-oriented dialog systems.\nSpecifically, we propose an approach that performs contextual dynamic prompting\nwhere the prompts are learnt from dialog contexts. We aim to distill useful\nprompting signals from the dialog context. On experiments with MultiWOZ 2.2\ndataset (Zang et al., 2020), we show that contextual dynamic prompts improve\nresponse generation in terms of combined score (Mehri et al., 2019) by 3\nabsolute points, and a massive 20 points when dialog states are incorporated.\nFurthermore, human annotation on these conversations found that agents which\nincorporate context were preferred over agents with vanilla prefix-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Swamy_S/0/1/0/all/0/1\">Sandesh Swamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabari_N/0/1/0/all/0/1\">Narges Tabari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chacha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangadharaiah_R/0/1/0/all/0/1\">Rashmi Gangadharaiah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Transfer of Article-aware Legal Outcome Classification for European Court of Human Rights Cases. (arXiv:2302.00609v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.00609","description":"<p>In this paper, we cast Legal Judgment Prediction on European Court of Human\nRights cases into an article-aware classification task, where the case outcome\nis classified from a combined input of case facts and convention articles. This\nconfiguration facilitates the model learning some legal reasoning ability in\nmapping article text to specific case fact text. It also provides an\nopportunity to evaluate the model's ability to generalize to zero-shot settings\nwhen asked to classify the case outcome with respect to articles not seen\nduring training. We devise zero-shot experiments and apply domain adaptation\nmethods based on domain discrimination and Wasserstein distance. Our results\ndemonstrate that the article-aware architecture outperforms straightforward\nfact classification. We also find that domain adaptation methods improve\nzero-shot transfer performance, with article relatedness and encoder\npre-training influencing the effect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santosh_T/0/1/0/all/0/1\">T.Y.S.S Santosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichim_O/0/1/0/all/0/1\">Oana Ichim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grabmair_M/0/1/0/all/0/1\">Matthias Grabmair</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Task Dependency and Contrastive Learning for Case Outcome Classification on European Court of Human Rights Cases. (arXiv:2302.00768v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.00768","description":"<p>We report on an experiment in case outcome classification on European Court\nof Human Rights cases where our model first learns to identify the convention\narticles allegedly violated by the state from case facts descriptions, and\nsubsequently uses that information to classify whether the court finds a\nviolation of those articles. We assess the dependency between these two tasks\nat the feature and outcome level. Furthermore, we leverage a hierarchical\ncontrastive loss to pull together article-specific representations of cases at\nthe higher level, leading to distinctive article clusters. The cases in each\narticle cluster are further pulled closer based on their outcome, leading to\nsub-clusters of cases with similar outcomes. Our experiment results demonstrate\nthat, given a static pre-trained encoder, our models produce a small but\nconsistent improvement in classification performance over single-task and joint\nmodels without contrastive loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santosh_T/0/1/0/all/0/1\">T.Y.S.S Santosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blas_M/0/1/0/all/0/1\">Marcel Perez San Blas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kemper_P/0/1/0/all/0/1\">Phillip Kemper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grabmair_M/0/1/0/all/0/1\">Matthias Grabmair</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CTE: A Dataset for Contextualized Table Extraction. (arXiv:2302.01451v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.01451","description":"<p>Relevant information in documents is often summarized in tables, helping the\nreader to identify useful facts. Most benchmark datasets support either\ndocument layout analysis or table understanding, but lack in providing data to\napply both tasks in a unified way. We define the task of Contextualized Table\nExtraction (CTE), which aims to extract and define the structure of tables\nconsidering the textual context of the document. The dataset comprises 75k\nfully annotated pages of scientific papers, including more than 35k tables.\nData are gathered from PubMed Central, merging the information provided by\nannotations in the PubTables-1M and PubLayNet datasets. The dataset can support\nCTE and adds new classes to the original ones. The generated annotations can be\nused to develop end-to-end pipelines for various tasks, including document\nlayout analysis, table detection, structure recognition, and functional\nanalysis. We formally define CTE and evaluation metrics, showing which subtasks\ncan be tackled, describing advantages, limitations, and future works of this\ncollection of data. Annotations and code will be accessible a\nhttps://github.com/AILab-UniFI/cte-dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gemelli_A/0/1/0/all/0/1\">Andrea Gemelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vivoli_E/0/1/0/all/0/1\">Emanuele Vivoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marinai_S/0/1/0/all/0/1\">Simone Marinai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Theory of Mind May Have Spontaneously Emerged in Large Language Models. (arXiv:2302.02083v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.02083","description":"<p>Theory of mind (ToM), or the ability to impute unobservable mental states to\nothers, is central to human social interactions, communication, empathy,\nself-consciousness, and morality. We administer classic false-belief tasks,\nwidely used to test ToM in humans, to several language models, without any\nexamples or pre-training. Our results show that models published before 2022\nshow virtually no ability to solve ToM tasks. Yet, the January 2022 version of\nGPT-3 (davinci-002) solved 70% of ToM tasks, a performance comparable with that\nof seven-year-old children. Moreover, its November 2022 version (davinci-003),\nsolved 93% of ToM tasks, a performance comparable with that of nine-year-old\nchildren. These findings suggest that ToM-like ability (thus far considered to\nbe uniquely human) may have spontaneously emerged as a byproduct of language\nmodels' improving language skills.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kosinski_M/0/1/0/all/0/1\">Michal Kosinski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nationality Bias in Text Generation. (arXiv:2302.02463v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.02463","description":"<p>Little attention is placed on analyzing nationality bias in language models,\nespecially when nationality is highly used as a factor in increasing the\nperformance of social NLP models. This paper examines how a text generation\nmodel, GPT-2, accentuates pre-existing societal biases about country-based\ndemonyms. We generate stories using GPT-2 for various nationalities and use\nsensitivity analysis to explore how the number of internet users and the\ncountry's economic status impacts the sentiment of the stories. To reduce the\npropagation of biases through large language models (LLM), we explore the\ndebiasing method of adversarial triggering. Our results show that GPT-2\ndemonstrates significant bias against countries with lower internet users, and\nadversarial triggering effectively reduces the same.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venkit_P/0/1/0/all/0/1\">Pranav Narayanan Venkit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gautam_S/0/1/0/all/0/1\">Sanjana Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panchanadikar_R/0/1/0/all/0/1\">Ruchi Panchanadikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ting-Hao/0/1/0/all/0/1\">Ting-Hao</a> (Kenneth) <a href=\"http://arxiv.org/find/cs/1/au:+Huang/0/1/0/all/0/1\">Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1\">Shomir Wilson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clinical BioBERT Hyperparameter Optimization using Genetic Algorithm. (arXiv:2302.03822v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2302.03822","description":"<p>Clinical factors account only for a small portion, about 10-30%, of the\ncontrollable factors that affect an individual's health outcomes. The remaining\nfactors include where a person was born and raised, where he/she pursued their\neducation, what their work and family environment is like, etc. These factors\nare collectively referred to as Social Determinants of Health (SDoH). The\nmajority of SDoH data is recorded in unstructured clinical notes by physicians\nand practitioners. Recording SDoH data in a structured manner (in an EHR) could\ngreatly benefit from a dedicated ontology of SDoH terms. Our research focuses\non extracting sentences from clinical notes, making use of such an SDoH\nontology (called SOHO) to provide appropriate concepts. We utilize recent\nadvancements in Deep Learning to optimize the hyperparameters of a Clinical\nBioBERT model for SDoH text. A genetic algorithm-based hyperparameter tuning\nregimen was implemented to identify optimal parameter settings. To implement a\ncomplete classifier, we pipelined Clinical BioBERT with two subsequent linear\nlayers and two dropout layers. The output predicts whether a text fragment\ndescribes an SDoH issue of the patient. We compared the AdamW, Adafactor, and\nLAMB optimizers. In our experiments, AdamW outperformed the others in terms of\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kollapally_N/0/1/0/all/0/1\">Navya Martin Kollapally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geller_J/0/1/0/all/0/1\">James Geller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPTScore: Evaluate as You Desire. (arXiv:2302.04166v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.04166","description":"<p>Generative Artificial Intelligence (AI) has enabled the development of\nsophisticated models that are capable of producing high-caliber text, images,\nand other outputs through the utilization of large pre-trained models.\nNevertheless, assessing the quality of the generation is an even more arduous\ntask than the generation itself, and this issue has not been given adequate\nconsideration recently. This paper proposes a novel evaluation framework,\nGPTScore, which utilizes the emergent abilities (e.g., zero-shot instruction)\nof generative pre-trained models to score generated texts. There are 19\npre-trained models explored in this paper, ranging in size from 80M (e.g.,\nFLAN-T5-small) to 175B (e.g., GPT3). Experimental results on four text\ngeneration tasks, 22 evaluation aspects, and corresponding 37 datasets\ndemonstrate that this approach can effectively allow us to achieve what one\ndesires to evaluate for texts simply by natural language instructions. This\nnature helps us overcome several long-standing challenges in text\nevaluation--how to achieve customized, multi-faceted evaluation without the\nneed for annotated samples. We make our code publicly available at\nhttps://github.com/jinlanfu/GPTScore.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1\">See-Kiong Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengbao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals. (arXiv:2302.04449v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.04449","description":"<p>High sample complexity has long been a challenge for RL. On the other hand,\nhumans learn to perform tasks not only from interaction or demonstrations, but\nalso by reading unstructured text documents, e.g., instruction manuals.\nInstruction manuals and wiki pages are among the most abundant data that could\ninform agents of valuable features and policies or task-specific environmental\ndynamics and reward structures. Therefore, we hypothesize that the ability to\nutilize human-written instruction manuals to assist learning policies for\nspecific tasks should lead to a more efficient and better-performing agent.\n</p>\n<p>We propose the Read and Reward framework. Read and Reward speeds up RL\nalgorithms on Atari games by reading manuals released by the Atari game\ndevelopers. Our framework consists of a QA Extraction module that extracts and\nsummarizes relevant information from the manual and a Reasoning module that\nevaluates object-agent interactions based on information from the manual.\nAuxiliary reward is then provided to a standard A2C RL agent, when interaction\nis detected. When assisted by our design, A2C improves on 4 games in the Atari\nenvironment with sparse rewards, and requires 1000x less training frames\ncompared to the previous SOTA Agent 57 on Skiing, the hardest game in Atari.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yewen Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azaria_A/0/1/0/all/0/1\">Amos Azaria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_T/0/1/0/all/0/1\">Tom M. Mitchell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge is a Region in Weight Space for Fine-tuned Language Models. (arXiv:2302.04863v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.04863","description":"<p>Research on neural networks has largely focused on understanding a single\nmodel trained on a single dataset. However, relatively little is known about\nthe relationships between different models, especially those trained or tested\non different datasets. We address this by studying how the weight space and\nunderlying loss landscape of different models are interconnected.\n</p>\n<p>Specifically, we demonstrate that fine-tuned models that were optimized for\nhigh performance, reside in well-defined regions in weight space, and vice\nversa -- that any model that resides anywhere in those regions also has high\nperformance. Specifically, we show that language models that have been\nfine-tuned on the same dataset form a tight cluster in the weight space and\nthat models fine-tuned on different datasets from the same underlying task form\na looser cluster. Moreover, traversing around the region between the models\nreaches new models that perform comparably or even better than models found via\nfine-tuning, even on tasks that the original models were not fine-tuned on.\n</p>\n<p>Our findings provide insight into the relationships between models,\ndemonstrating that a model positioned between two similar models can acquire\nthe knowledge of both. We leverage this finding and design a method to pick a\nbetter model for efficient fine-tuning. Specifically, we show that starting\nfrom the center of the region is as good or better than the pre-trained model\nin 11 of 12 datasets and improves accuracy by 3.06 on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gueta_A/0/1/0/all/0/1\">Almog Gueta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venezian_E/0/1/0/all/0/1\">Elad Venezian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1\">Yoav Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Song of Ice and Fire: Analyzing Textual Autotelic Agents in ScienceWorld. (arXiv:2302.05244v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2302.05244","description":"<p>Building open-ended agents that can autonomously discover a diversity of\nbehaviours is one of the long-standing goals of artificial intelligence. This\nchallenge can be studied in the framework of autotelic RL agents, i.e. agents\nthat learn by selecting and pursuing their own goals, self-organizing a\nlearning curriculum. Recent work identified language has a key dimension of\nautotelic learning, in particular because it enables abstract goal sampling and\nguidance from social peers for hindsight relabelling. Within this perspective,\nwe study the following open scientific questions: What is the impact of\nhindsight feedback from a social peer (e.g. selective vs. exhaustive)? How can\nthe agent learn from very rare language goal examples in its experience replay?\nHow can multiple forms of exploration be combined, and take advantage of easier\ngoals as stepping stones to reach harder ones? To address these questions, we\nuse ScienceWorld, a textual environment with rich abstract and combinatorial\nphysics. We show the importance of selectivity from the social peer's feedback;\nthat experience replay needs to over-sample examples of rare goals; and that\nfollowing self-generated goal sequences where the agent's competence is\nintermediate leads to significant improvements in final performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teodorescu_L/0/1/0/all/0/1\">Laetitia Teodorescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_E/0/1/0/all/0/1\">Eric Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cote_M/0/1/0/all/0/1\">Marc-Alexandre C&#xf4;t&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-02-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}}]}]}