{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-04-11T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Bridging Nations: Quantifying the Role of Multilinguals in Communication on Social Media. (arXiv:2304.03797v1 [cs.SI])","link":"http://arxiv.org/abs/2304.03797","description":"<p>Social media enables the rapid spread of many kinds of information, from\nmemes to social movements. However, little is known about how information\ncrosses linguistic boundaries. We apply causal inference techniques on the\nEuropean Twitter network to quantify multilingual users' structural role and\ncommunication influence in cross-lingual information exchange. Overall,\nmultilinguals play an essential role; posting in multiple languages increases\nbetweenness centrality by 13%, and having a multilingual network neighbor\nincreases monolinguals' odds of sharing domains and hashtags from another\nlanguage 16-fold and 4-fold, respectively. We further show that multilinguals\nhave a greater impact on diffusing information less accessible to their\nmonolingual compatriots, such as information from far-away countries and\ncontent about regional politics, nascent social movements, and job\nopportunities. By highlighting information exchange across borders, this work\nsheds light on a crucial component of how information and ideas spread around\nthe world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mendelsohn_J/0/1/0/all/0/1\">Julia Mendelsohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sayan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budak_C/0/1/0/all/0/1\">Ceren Budak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why think step-by-step? Reasoning emerges from the locality of experience. (arXiv:2304.03843v1 [cs.AI])","link":"http://arxiv.org/abs/2304.03843","description":"<p>Humans have a powerful and mysterious capacity to reason. By working through\na series of purely mental steps, we can make inferences we would not be capable\nof making directly -- despite that fact that we get no additional data from the\nworld. Similarly, large language models can perform better at complex tasks\nthrough chain-of-thought reasoning, where they generate intermediate steps\nbefore answering a question. We use language models to investigate the\nquestions of when and why reasoning is helpful, testing the hypothesis that\nreasoning is effective when training data consisting of local clusters of\nvariables that influence each other strongly. These training conditions enable\nthe chaining of accurate local inferences in order to estimate relationships\nbetween variables that were not seen together in training. We train an\nautoregressive transformer on samples from joint distributions defined by Bayes\nnets, but only include a subset of all the variables in each sample. We compare\nlanguage models' ability to match conditional probabilities both with and\nwithout intermediate reasoning steps, finding that intermediate steps help only\nwhen the training data is locally structured with respect to dependencies\nbetween variables. Furthermore, intermediate variables need to be relevant to\nthe relationship between observed information and target inferences. Our\nresults illustrate how the statistical structure of training data drives the\neffectiveness of reasoning step by step.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prystawski_B/0/1/0/all/0/1\">Ben Prystawski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah D. Goodman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factify 2: A Multimodal Fake News and Satire News Dataset. (arXiv:2304.03897v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03897","description":"<p>The internet gives the world an open platform to express their views and\nshare their stories. While this is very valuable, it makes fake news one of our\nsociety's most pressing problems. Manual fact checking process is time\nconsuming, which makes it challenging to disprove misleading assertions before\nthey cause significant harm. This is he driving interest in automatic fact or\nclaim verification. Some of the existing datasets aim to support development of\nautomating fact-checking techniques, however, most of them are text based.\nMulti-modal fact verification has received relatively scant attention. In this\npaper, we provide a multi-modal fact-checking dataset called FACTIFY 2,\nimproving Factify 1 by using new data sources and adding satire articles.\nFactify 2 has 50,000 new data instances. Similar to FACTIFY 1.0, we have three\nbroad categories - support, no-evidence, and refute, with sub-categories based\non the entailment of visual and textual data. We also provide a BERT and Vison\nTransformer based baseline, which acheives 65% F1 score in the test set. The\nbaseline codes and the dataset will be made available at\nhttps://github.com/surya1701/Factify-2.0.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suryavardan_S/0/1/0/all/0/1\">S Suryavardan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shreyash Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwa_P/0/1/0/all/0/1\">Parth Patwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_M/0/1/0/all/0/1\">Megha Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rani_A/0/1/0/all/0/1\">Anku Rani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reganti_A/0/1/0/all/0/1\">Aishwarya Reganti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Amitava Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinnakotla_M/0/1/0/all/0/1\">Manoj Chinnakotla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Srijan Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Short Text Matching Model Enhanced with Knowledge via Contrastive Learning. (arXiv:2304.03898v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03898","description":"<p>In recent years, short Text Matching tasks have been widely applied in the\nfields ofadvertising search and recommendation. The difficulty lies in the lack\nof semantic information and word ambiguity caused by the short length of the\ntext. Previous works have introduced complement sentences or knowledge bases to\nprovide additional feature information. However, these methods have not fully\ninteracted between the original sentence and the complement sentence, and have\nnot considered the noise issue that may arise from the introduction of external\nknowledge bases. Therefore, this paper proposes a short Text Matching model\nthat combines contrastive learning and external knowledge. The model uses a\ngenerative model to generate corresponding complement sentences and uses the\ncontrastive learning method to guide the model to obtain more semantically\nmeaningful encoding of the original sentence. In addition, to avoid noise, we\nuse keywords as the main semantics of the original sentence to retrieve\ncorresponding knowledge words in the knowledge base, and construct a knowledge\ngraph. The graph encoding model is used to integrate the knowledge base\ninformation into the model. Our designed model achieves state-of-the-art\nperformance on two publicly available Chinese Text Matching datasets,\ndemonstrating the effectiveness of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1\">Qiqiang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1\">Mengmeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_H/0/1/0/all/0/1\">Hanjie Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shaohua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangzheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yanlong Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study and Improvement for Speech Emotion Recognition. (arXiv:2304.03899v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03899","description":"<p>Multimodal speech emotion recognition aims to detect speakers' emotions from\naudio and text. Prior works mainly focus on exploiting advanced networks to\nmodel and fuse different modality information to facilitate performance, while\nneglecting the effect of different fusion strategies on emotion recognition. In\nthis work, we consider a simple yet important problem: how to fuse audio and\ntext modality information is more helpful for this multimodal task. Further, we\npropose a multimodal emotion recognition model improved by perspective loss.\nEmpirical results show our method obtained new state-of-the-art results on the\nIEMOCAP dataset. The in-depth analysis explains why the improved model can\nachieve improvements and outperforms baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yizhe Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xinyu Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Code Explanations Created by Students and Large Language Models. (arXiv:2304.03938v1 [cs.CY])","link":"http://arxiv.org/abs/2304.03938","description":"<p>Reasoning about code and explaining its purpose are fundamental skills for\ncomputer scientists. There has been extensive research in the field of\ncomputing education on the relationship between a student's ability to explain\ncode and other skills such as writing and tracing code. In particular, the\nability to describe at a high-level of abstraction how code will behave over\nall possible inputs correlates strongly with code writing skills. However,\ndeveloping the expertise to comprehend and explain code accurately and\nsuccinctly is a challenge for many students. Existing pedagogical approaches\nthat scaffold the ability to explain code, such as producing exemplar code\nexplanations on demand, do not currently scale well to large classrooms. The\nrecent emergence of powerful large language models (LLMs) may offer a solution.\nIn this paper, we explore the potential of LLMs in generating explanations that\ncan serve as examples to scaffold students' ability to understand and explain\ncode. To evaluate LLM-created explanations, we compare them with explanations\ncreated by students in a large course ($n \\approx 1000$) with respect to\naccuracy, understandability and length. We find that LLM-created explanations,\nwhich can be produced automatically on demand, are rated as being significantly\neasier to understand and more accurate summaries of code than student-created\nexplanations. We discuss the significance of this finding, and suggest how such\nmodels can be incorporated into introductory programming education.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leinonen_J/0/1/0/all/0/1\">Juho Leinonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denny_P/0/1/0/all/0/1\">Paul Denny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacNeil_S/0/1/0/all/0/1\">Stephen MacNeil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarsa_S/0/1/0/all/0/1\">Sami Sarsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernstein_S/0/1/0/all/0/1\">Seth Bernstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joanne Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_A/0/1/0/all/0/1\">Andrew Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellas_A/0/1/0/all/0/1\">Arto Hellas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MphayaNER: Named Entity Recognition for Tshivenda. (arXiv:2304.03952v1 [cs.CL])","link":"http://arxiv.org/abs/2304.03952","description":"<p>Named Entity Recognition (NER) plays a vital role in various Natural Language\nProcessing tasks such as information retrieval, text classification, and\nquestion answering. However, NER can be challenging, especially in low-resource\nlanguages with limited annotated datasets and tools. This paper adds to the\neffort of addressing these challenges by introducing MphayaNER, the first\nTshivenda NER corpus in the news domain. We establish NER baselines by\n\\textit{fine-tuning} state-of-the-art models on MphayaNER. The study also\nexplores zero-shot transfer between Tshivenda and other related Bantu\nlanguages, with chiShona and Kiswahili showing the best results. Augmenting\nMphayaNER with chiShona data was also found to improve model performance\nsignificantly. Both MphayaNER and the baseline models are made publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mbuvha_R/0/1/0/all/0/1\">Rendani Mbuvha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David I. Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutavhatsindi_T/0/1/0/all/0/1\">Tendani Mutavhatsindi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakhuhu_T/0/1/0/all/0/1\">Tshimangadzo Rakhuhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mauda_A/0/1/0/all/0/1\">Aluwani Mauda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maumela_T/0/1/0/all/0/1\">Tshifhiwa Joshua Maumela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masindi_A/0/1/0/all/0/1\">Andisani Masindi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rananga_S/0/1/0/all/0/1\">Seani Rananga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1\">Vukosi Marivate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marwala_T/0/1/0/all/0/1\">Tshilidzi Marwala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WikiGoldSK: Annotated Dataset, Baselines and Few-Shot Learning Experiments for Slovak Named Entity Recognition. (arXiv:2304.04026v1 [cs.CL])","link":"http://arxiv.org/abs/2304.04026","description":"<p>Named Entity Recognition (NER) is a fundamental NLP tasks with a wide range\nof practical applications. The performance of state-of-the-art NER methods\ndepends on high quality manually anotated datasets which still do not exist for\nsome languages. In this work we aim to remedy this situation in Slovak by\nintroducing WikiGoldSK, the first sizable human labelled Slovak NER dataset. We\nbenchmark it by evaluating state-of-the-art multilingual Pretrained Language\nModels and comparing it to the existing silver-standard Slovak NER dataset. We\nalso conduct few-shot experiments and show that training on a sliver-standard\ndataset yields better results. To enable future work that can be based on\nSlovak NER, we release the dataset, code, as well as the trained models\npublicly under permissible licensing terms at\nhttps://github.com/NaiveNeuron/WikiGoldSK.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suba_D/0/1/0/all/0/1\">D&#xe1;vid &#x160;uba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suppa_M/0/1/0/all/0/1\">Marek &#x160;uppa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kubik_J/0/1/0/all/0/1\">Jozef Kub&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamerlik_E/0/1/0/all/0/1\">Endre Hamerlik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takac_M/0/1/0/all/0/1\">Martin Tak&#xe1;&#x10d;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bipol: A Novel Multi-Axes Bias Evaluation Metric with Explainability for NLP. (arXiv:2304.04029v1 [cs.CL])","link":"http://arxiv.org/abs/2304.04029","description":"<p>We introduce bipol, a new metric with explainability, for estimating social\nbias in text data. Harmful bias is prevalent in many online sources of data\nthat are used for training machine learning (ML) models. In a step to address\nthis challenge we create a novel metric that involves a two-step process:\ncorpus-level evaluation based on model classification and sentence-level\nevaluation based on (sensitive) term frequency (TF). After creating new models\nto detect bias along multiple axes using SotA architectures, we evaluate two\npopular NLP datasets (COPA and SQUAD). As additional contribution, we created a\nlarge dataset (with almost 2 million labelled samples) for training models in\nbias detection and make it publicly available. We also make public our codes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alkhaled_L/0/1/0/all/0/1\">Lama Alkhaled</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adewumi_T/0/1/0/all/0/1\">Tosin Adewumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabry_S/0/1/0/all/0/1\">Sana Sabah Sabry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder. (arXiv:2304.04052v1 [cs.CL])","link":"http://arxiv.org/abs/2304.04052","description":"<p>The sequence-to-sequence (seq2seq) task aims at generating the target\nsequence based on the given input source sequence. Traditionally, most of the\nseq2seq task is resolved by the Encoder-Decoder framework which requires an\nencoder to encode the source sequence and a decoder to generate the target\ntext. Recently, a bunch of new approaches have emerged that apply decoder-only\nlanguage models directly to the seq2seq task. Despite the significant\nadvancements in applying language models to the seq2seq task, there is still a\nlack of thorough analysis on the effectiveness of the decoder-only language\nmodel architecture. This paper aims to address this gap by conducting a\ndetailed comparison between the encoder-decoder architecture and the\ndecoder-only language model framework through the analysis of a regularized\nencoder-decoder structure. This structure is designed to replicate all\nbehaviors in the classical decoder-only language model but has an encoder and a\ndecoder making it easier to be compared with the classical encoder-decoder\nstructure. Based on the analysis, we unveil the attention degeneration problem\nin the language model, namely, as the generation step number grows, less and\nless attention is focused on the source sequence. To give a quantitative\nunderstanding of this problem, we conduct a theoretical sensitivity analysis of\nthe attention output with respect to the source input. Grounded on our\nanalysis, we propose a novel partial attention language model to solve the\nattention degeneration problem. Experimental results on machine translation,\nsummarization, and data-to-text generation tasks support our analysis and\ndemonstrate the effectiveness of our proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zihao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+So_A/0/1/0/all/0/1\">Anthony Man-Cho So</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengding Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"tmn at SemEval-2023 Task 9: Multilingual Tweet Intimacy Detection using XLM-T, Google Translate, and Ensemble Learning. (arXiv:2304.04054v1 [cs.CL])","link":"http://arxiv.org/abs/2304.04054","description":"<p>The paper describes a transformer-based system designed for SemEval-2023 Task\n9: Multilingual Tweet Intimacy Analysis. The purpose of the task was to predict\nthe intimacy of tweets in a range from 1 (not intimate at all) to 5 (very\nintimate). The official training set for the competition consisted of tweets in\nsix languages (English, Spanish, Italian, Portuguese, French, and Chinese). The\ntest set included the given six languages as well as external data with four\nlanguages not presented in the training set (Hindi, Arabic, Dutch, and Korean).\nWe presented a solution based on an ensemble of XLM-T, a multilingual RoBERTa\nmodel adapted to the Twitter domain. To improve the performance of unseen\nlanguages, each tweet was supplemented by its English translation. We explored\nthe effectiveness of translated data for the languages seen in fine-tuning\ncompared to unseen languages and estimated strategies for using translated data\nin transformer-based models. Our solution ranked 4th on the leaderboard while\nachieving an overall Pearson's r of 0.599 over the test set. The proposed\nsystem improves up to 0.088 Pearson's r over a score averaged across all 45\nsubmissions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glazkova_A/0/1/0/all/0/1\">Anna Glazkova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Multi Labeled Bengali Toxic Comments Classification using Deep Learning. (arXiv:2304.04087v1 [cs.CL])","link":"http://arxiv.org/abs/2304.04087","description":"<p>This paper presents a deep learning-based pipeline for categorizing Bengali\ntoxic comments, in which at first a binary classification model is used to\ndetermine whether a comment is toxic or not, and then a multi-label classifier\nis employed to determine which toxicity type the comment belongs to. For this\npurpose, we have prepared a manually labeled dataset consisting of 16,073\ninstances among which 8,488 are Toxic and any toxic comment may correspond to\none or more of the six toxic categories - vulgar, hate, religious, threat,\ntroll, and insult simultaneously. Long Short Term Memory (LSTM) with BERT\nEmbedding achieved 89.42% accuracy for the binary classification task while as\na multi-label classifier, a combination of Convolutional Neural Network and\nBi-directional Long Short Term Memory (CNN-BiLSTM) with attention mechanism\nachieved 78.92% accuracy and 0.86 as weighted F1-score. To explain the\npredictions and interpret the word feature importance during classification by\nthe proposed models, we utilized Local Interpretable Model-Agnostic\nExplanations (LIME) framework. We have made our dataset public and can be\naccessed at -\nhttps://github.com/deepu099cse/Multi-Labeled-Bengali-Toxic-Comments-Classification\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belal_T/0/1/0/all/0/1\">Tanveer Ahmed Belal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahariar_G/0/1/0/all/0/1\">G. M. Shahariar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Md. Hasanul Kabir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding. (arXiv:2304.04099v1 [cs.IR])","link":"http://arxiv.org/abs/2304.04099","description":"<p>Unsupervised discovery of stories with correlated news articles in real-time\nhelps people digest massive news streams without expensive human annotations. A\ncommon approach of the existing studies for unsupervised online story discovery\nis to represent news articles with symbolic- or graph-based embedding and\nincrementally cluster them into stories. Recent large language models are\nexpected to improve the embedding further, but a straightforward adoption of\nthe models by indiscriminately encoding all information in articles is\nineffective to deal with text-rich and evolving news streams. In this work, we\npropose a novel thematic embedding with an off-the-shelf pretrained sentence\nencoder to dynamically represent articles and stories by considering their\nshared temporal themes. To realize the idea for unsupervised online story\ndiscovery, a scalable framework USTORY is introduced with two main techniques,\ntheme- and time-aware dynamic embedding and novelty-aware adaptive clustering,\nfueled by lightweight story summaries. A thorough evaluation with real news\ndata sets demonstrates that USTORY achieves higher story discovery performances\nthan baselines while being robust and scalable to various streaming settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Susik Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongha Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-class Categorization of Reasons behind Mental Disturbance in Long Texts. (arXiv:2304.04118v1 [cs.CL])","link":"http://arxiv.org/abs/2304.04118","description":"<p>Motivated with recent advances in inferring users' mental state in social\nmedia posts, we identify and formulate the problem of finding causal indicators\nbehind mental illness in self-reported text. In the past, we witness the\npresence of rule-based studies for causal explanation analysis on curated\nFacebook data. The investigation on transformer-based model for multi-class\ncausal categorization in Reddit posts point to a problem of using long-text\nwhich contains as many as 4000 words. Developing end-to-end transformer-based\nmodels subject to the limitation of maximum-length in a given instance. To\nhandle this problem, we use Longformer and deploy its encoding on\ntransformer-based classifier. The experimental results show that Longformer\nachieves new state-of-the-art results on M-CAMS, a publicly available dataset\nwith 62\\% F1-score. Cause-specific analysis and ablation study prove the\neffectiveness of Longformer. We believe our work facilitates causal analysis of\ndepression and suicide risk on social media data, and shows potential for\napplication on other mental health conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_M/0/1/0/all/0/1\">Muskan Garg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Graph Convolutional Network for Text Classification. (arXiv:2304.04152v1 [cs.CL])","link":"http://arxiv.org/abs/2304.04152","description":"<p>Graph convolutional network (GCN) has been successfully applied to capture\nglobal non-consecutive and long-distance semantic information for text\nclassification. However, while GCN-based methods have shown promising results\nin offline evaluations, they commonly follow a seen-token-seen-document\nparadigm by constructing a fixed document-token graph and cannot make\ninferences on new documents. It is a challenge to deploy them in online systems\nto infer steaming text data. In this work, we present a continual GCN model\n(ContGCN) to generalize inferences from observed documents to unobserved\ndocuments. Concretely, we propose a new all-token-any-document paradigm to\ndynamically update the document-token graph in every batch during both the\ntraining and testing phases of an online system. Moreover, we design an\noccurrence memory module and a self-supervised contrastive learning objective\nto update ContGCN in a label-free manner. A 3-month A/B test on Huawei public\nopinion analysis system shows ContGCN achieves 8.86% performance gain compared\nwith state-of-the-art methods. Offline experiments on five public datasets also\nshow ContGCN can improve inference quality. The source code will be released at\nhttps://github.com/Jyonn/ContGCN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tiandeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qijiong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Ming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jiandong Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An investigation of speaker independent phrase break models in End-to-End TTS systems. (arXiv:2304.04157v1 [eess.AS])","link":"http://arxiv.org/abs/2304.04157","description":"<p>This paper presents our work on phrase break prediction in the context of\nend-to-end TTS systems, motivated by the following questions: (i) Is there any\nutility in incorporating an explicit phrasing model in an end-to-end TTS\nsystem?, and (ii) How do you evaluate the effectiveness of a phrasing model in\nan end-to-end TTS system? In particular, the utility and effectiveness of\nphrase break prediction models are evaluated in in the context of childrens\nstory synthesis, using listener comprehension. We show by means of perceptual\nlistening evaluations that there is a clear preference for stories synthesized\nafter predicting the location of phrase breaks using a trained phrasing model,\nover stories directly synthesized without predicting the location of phrase\nbreaks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vadapalli_A/0/1/0/all/0/1\">Anandaswarup Vadapalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Similarity-Aware Multimodal Prompt Learning for Fake News Detection. (arXiv:2304.04187v1 [cs.CL])","link":"http://arxiv.org/abs/2304.04187","description":"<p>The standard paradigm for fake news detection mainly utilizes text\ninformation to model the truthfulness of news. However, the discourse of online\nfake news is typically subtle and it requires expert knowledge to use textual\ninformation to debunk fake news. Recently, studies focusing on multimodal fake\nnews detection have outperformed text-only methods. Recent approaches utilizing\nthe pre-trained model to extract unimodal features, or fine-tuning the\npre-trained model directly, have become a new paradigm for detecting fake news.\nAgain, this paradigm either requires a large number of training instances, or\nupdates the entire set of pre-trained model parameters, making real-world fake\nnews detection impractical. Furthermore, traditional multimodal methods fuse\nthe cross-modal features directly without considering that the uncorrelated\nsemantic representation might inject noise into the multimodal features. This\npaper proposes a Similarity-Aware Multimodal Prompt Learning (SAMPLE)\nframework. First, we incorporate prompt learning into multimodal fake news\ndetection. Prompt learning, which only tunes prompts with a frozen language\nmodel, can reduce memory usage significantly and achieve comparable\nperformances, compared with fine-tuning. We analyse three prompt templates with\na soft verbalizer to detect fake news. In addition, we introduce the\nsimilarity-aware fusing method to adaptively fuse the intensity of multimodal\nrepresentation and mitigate the noise injection via uncorrelated cross-modal\nfeatures. For evaluation, SAMPLE surpasses the F1 and the accuracies of\nprevious works on two benchmark multimodal datasets, demonstrating the\neffectiveness of the proposed method in detecting fake news. In addition,\nSAMPLE also is superior to other approaches regardless of few-shot and\ndata-rich settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Ye Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaomin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yimin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaoman Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingyi Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maynard_D/0/1/0/all/0/1\">Diana Maynard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Team QUST at SemEval-2023 Task 3: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting Online News Genre, Framing and Persuasion Techniques. (arXiv:2304.04190v1 [cs.CL])","link":"http://arxiv.org/abs/2304.04190","description":"<p>This paper describes the participation of team QUST in the SemEval2023 task\n3. The monolingual models are first evaluated with the under-sampling of the\nmajority classes in the early stage of the task. Then, the pre-trained\nmultilingual model is fine-tuned with a combination of the class weights and\nthe sample weights. Two different fine-tuning strategies, the task-agnostic and\nthe task-dependent, are further investigated. All experiments are conducted\nunder the 10-fold cross-validation, the multilingual approaches are superior to\nthe monolingual ones. The submitted system achieves the second best in Italian\nand Spanish (zero-shot) in subtask-1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Ye Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extractive Summarization via ChatGPT for Faithful Summary Generation. (arXiv:2304.04193v1 [cs.CL])","link":"http://arxiv.org/abs/2304.04193","description":"<p>Extractive summarization is a crucial task in natural language processing\nthat aims to condense long documents into shorter versions by directly\nextracting sentences. The recent introduction of ChatGPT has attracted\nsignificant interest in the NLP community due to its remarkable performance on\na wide range of downstream tasks. However, concerns regarding factuality and\nfaithfulness have hindered its practical applications for summarization\nsystems. This paper first presents a thorough evaluation of ChatGPT's\nperformance on extractive summarization and compares it with traditional\nfine-tuning methods on various benchmark datasets. Our experimental analysis\nreveals that ChatGPT's extractive summarization performance is still inferior\nto existing supervised systems in terms of ROUGE scores. In addition, we\nexplore the effectiveness of in-context learning and chain-of-thought reasoning\nfor enhancing its performance. Furthermore, we find that applying an\nextract-then-generate pipeline with ChatGPT yields significant performance\nimprovements over abstractive baselines in terms of summary faithfulness. These\nobservations highlight potential directions for enhancing ChatGPT's\ncapabilities for faithful text summarization tasks using two-stage approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RISC: Generating Realistic Synthetic Bilingual Insurance Contract. (arXiv:2304.04212v1 [cs.CL])","link":"http://arxiv.org/abs/2304.04212","description":"<p>This paper presents RISC, an open-source Python package data generator\n(https://github.com/GRAAL-Research/risc). RISC generates look-alike automobile\ninsurance contracts based on the Quebec regulatory insurance form in French and\nEnglish. Insurance contracts are 90 to 100 pages long and use complex legal and\ninsurance-specific vocabulary for a layperson. Hence, they are a much more\ncomplex class of documents than those in traditional NLP corpora. Therefore, we\nintroduce RISCBAC, a Realistic Insurance Synthetic Bilingual Automobile\nContract dataset based on the mandatory Quebec car insurance contract. The\ndataset comprises 10,000 French and English unannotated insurance contracts.\nRISCBAC enables NLP research for unsupervised automatic summarisation, question\nanswering, text simplification, machine translation and more. Moreover, it can\nbe further automatically annotated as a dataset for supervised tasks such as\nNER\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beauchemin_D/0/1/0/all/0/1\">David Beauchemin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khoury_R/0/1/0/all/0/1\">Richard Khoury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Editable User Profiles for Controllable Text Recommendation. (arXiv:2304.04250v1 [cs.IR])","link":"http://arxiv.org/abs/2304.04250","description":"<p>Methods for making high-quality recommendations often rely on learning latent\nrepresentations from interaction data. These methods, while performant, do not\nprovide ready mechanisms for users to control the recommendation they receive.\nOur work tackles this problem by proposing LACE, a novel concept value\nbottleneck model for controllable text recommendations. LACE represents each\nuser with a succinct set of human-readable concepts through retrieval given\nuser-interacted documents and learns personalized representations of the\nconcepts based on user documents. This concept based user profile is then\nleveraged to make recommendations. The design of our model affords control over\nthe recommendations through a number of intuitive interactions with a\ntransparent user profile. We first establish the quality of recommendations\nobtained from LACE in an offline evaluation on three recommendation tasks\nspanning six datasets in warm-start, cold-start, and zero-shot setups. Next, we\nvalidate the controllability of LACE under simulated user interactions.\nFinally, we implement LACE in an interactive controllable recommender system\nand conduct a user study to demonstrate that users are able to improve the\nquality of recommendations they receive through interactions with an editable\nuser profile.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mysore_S/0/1/0/all/0/1\">Sheshera Mysore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jasim_M/0/1/0/all/0/1\">Mahmood Jasim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1\">Hamed Zamani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Preliminary Evaluation of ChatGPT for Zero-shot Dialogue Understanding. (arXiv:2304.04256v1 [cs.CL])","link":"http://arxiv.org/abs/2304.04256","description":"<p>Zero-shot dialogue understanding aims to enable dialogue to track the user's\nneeds without any training data, which has gained increasing attention. In this\nwork, we investigate the understanding ability of ChatGPT for zero-shot\ndialogue understanding tasks including spoken language understanding (SLU) and\ndialogue state tracking (DST). Experimental results on four popular benchmarks\nreveal the great potential of ChatGPT for zero-shot dialogue understanding. In\naddition, extensive analysis shows that ChatGPT benefits from the multi-turn\ninteractive prompt in the DST task but struggles to perform slot filling for\nSLU. Finally, we summarize several unexpected behaviors of ChatGPT in dialogue\nunderstanding tasks, hoping to provide some insights for future research on\nbuilding zero-shot dialogue understanding systems with Large Language Models\n(LLMs).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Wenbo Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiguang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Libo Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FrenchMedMCQA: A French Multiple-Choice Question Answering Dataset for Medical domain. (arXiv:2304.04280v1 [cs.CL])","link":"http://arxiv.org/abs/2304.04280","description":"<p>This paper introduces FrenchMedMCQA, the first publicly available\nMultiple-Choice Question Answering (MCQA) dataset in French for medical domain.\nIt is composed of 3,105 questions taken from real exams of the French medical\nspecialization diploma in pharmacy, mixing single and multiple answers. Each\ninstance of the dataset contains an identifier, a question, five possible\nanswers and their manual correction(s). We also propose first baseline models\nto automatically process this MCQA task in order to report on the current\nperformances and to highlight the difficulty of the task. A detailed analysis\nof the results showed that it is necessary to have representations adapted to\nthe medical domain or to the MCQA task: in our case, English specialized models\nyielded better results than generic French ones, even though FrenchMedMCQA is\nin French. Corpus, models and tools are available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Labrak_Y/0/1/0/all/0/1\">Yanis Labrak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazoge_A/0/1/0/all/0/1\">Adrien Bazoge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dufour_R/0/1/0/all/0/1\">Richard Dufour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouvier_M/0/1/0/all/0/1\">Mickael Rouvier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morin_E/0/1/0/all/0/1\">Emmanuel Morin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daille_B/0/1/0/all/0/1\">B&#xe9;atrice Daille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gourraud_P/0/1/0/all/0/1\">Pierre-Antoine Gourraud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes. (arXiv:2304.04321v1 [cs.AI])","link":"http://arxiv.org/abs/2304.04321","description":"<p>Understanding the continuous states of objects is essential for task learning\nand planning in the real world. However, most existing task learning benchmarks\nassume discrete(e.g., binary) object goal states, which poses challenges for\nthe learning of complex tasks and transferring learned policy from simulated\nenvironments to the real world. Furthermore, state discretization limits a\nrobot's ability to follow human instructions based on the grounding of actions\nand states. To tackle these challenges, we present ARNOLD, a benchmark that\nevaluates language-grounded task learning with continuous states in realistic\n3D scenes. ARNOLD is comprised of 8 language-conditioned tasks that involve\nunderstanding object states and learning policies for continuous goals. To\npromote language-instructed learning, we provide expert demonstrations with\ntemplate-generated language descriptions. We assess task performance by\nutilizing the latest language-conditioned policy learning models. Our results\nindicate that current models for language-conditioned manipulations continue to\nexperience significant challenges in novel goal-state generalizations, scene\ngeneralizations, and object generalizations. These findings highlight the need\nto develop new algorithms that address this gap and underscore the potential\nfor further research in this area. See our project page at:\nhttps://arnold-benchmark.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Ran Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiangyong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yizhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_H/0/1/0/all/0/1\">Haoran Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiaofeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qingyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_W/0/1/0/all/0/1\">Wensi Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Ziheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terzopoulos_D/0/1/0/all/0/1\">Demetri Terzopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_B/0/1/0/all/0/1\">Baoxiong Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyuan Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study. (arXiv:2304.04339v1 [cs.CL])","link":"http://arxiv.org/abs/2304.04339","description":"<p>Recently, ChatGPT has drawn great attention from both the research community\nand the public. We are particularly curious about whether it can serve as a\nuniversal sentiment analyzer. To this end, in this work, we provide a\npreliminary evaluation of ChatGPT on the understanding of opinions, sentiments,\nand emotions contained in the text. Specifically, we evaluate it in four\nsettings, including standard evaluation, polarity shift evaluation, open-domain\nevaluation, and sentiment inference evaluation. The above evaluation involves\n18 benchmark datasets and 5 representative sentiment analysis tasks, and we\ncompare ChatGPT with fine-tuned BERT and corresponding state-of-the-art (SOTA)\nmodels on end-task. Moreover, we also conduct human evaluation and present some\nqualitative case studies to gain a deep comprehension of its sentiment analysis\ncapabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zengzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qiming Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zixiang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_R/0/1/0/all/0/1\">Rui Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebBrain: Learning to Generate Factually Correct Articles for Queries by Grounding on Large Web Corpus. (arXiv:2304.04358v1 [cs.CL])","link":"http://arxiv.org/abs/2304.04358","description":"<p>In this paper, we introduce a new NLP task -- generating short factual\narticles with references for queries by mining supporting evidence from the\nWeb. In this task, called WebBrain, the ultimate goal is to generate a fluent,\ninformative, and factually-correct short article (e.g., a Wikipedia article)\nfor a factual query unseen in Wikipedia. To enable experiments on WebBrain, we\nconstruct a large-scale dataset WebBrain-Raw by extracting English Wikipedia\narticles and their crawlable Wikipedia references. WebBrain-Raw is ten times\nlarger than the previous biggest peer dataset, which can greatly benefit the\nresearch community. From WebBrain-Raw, we construct two task-specific datasets:\nWebBrain-R and WebBrain-G, which are used to train in-domain retriever and\ngenerator, respectively. Besides, we empirically analyze the performances of\nthe current state-of-the-art NLP techniques on WebBrain and introduce a new\nframework ReGen, which enhances the generation factualness by improved evidence\nretrieval and task-specific pre-training for generation. Experiment results\nshow that ReGen outperforms all baselines in both automatic and human\nevaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">Hongjing Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yutao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1\">Haoqi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_R/0/1/0/all/0/1\">Ruofei Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])","link":"http://arxiv.org/abs/2304.04370","description":"<p>Human intelligence has the remarkable ability to assemble basic skills into\ncomplex ones so as to solve complex tasks. This ability is equally important\nfor Artificial Intelligence (AI), and thus, we assert that in addition to the\ndevelopment of large, comprehensive intelligent models, it is equally crucial\nto equip such models with the capability to harness various domain-specific\nexpert models for complex task-solving in the pursuit of Artificial General\nIntelligence (AGI). Recent developments in Large Language Models (LLMs) have\ndemonstrated remarkable learning and reasoning abilities, making them promising\nas a controller to select, synthesize, and execute external models to solve\ncomplex tasks. In this project, we develop OpenAGI, an open-source AGI research\nplatform, specifically designed to offer complex, multi-step tasks and\naccompanied by task-specific datasets, evaluation metrics, and a diverse range\nof extensible models. OpenAGI formulates complex tasks as natural language\nqueries, serving as input to the LLM. The LLM subsequently selects,\nsynthesizes, and executes models provided by OpenAGI to address the task.\nFurthermore, we propose a Reinforcement Learning from Task Feedback (RLTF)\nmechanism, which uses the task-solving result as feedback to improve the LLM's\ntask-solving ability. Thus, the LLM is responsible for synthesizing various\nexternal models for solving complex tasks, while RLTF provides feedback to\nimprove its task-solving ability, enabling a feedback loop for self-improving\nAI. We believe that the paradigm of LLMs operating various expert models for\ncomplex task-solving is a promising approach towards AGI. To facilitate the\ncommunity's long-term improvement and evaluation of AGI's ability, we\nopen-source the code, benchmark, and evaluation methods of the OpenAGI project\nat https://github.com/agiresearch/OpenAGI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wenyue Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jianchao Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Juntao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Factual Inconsistency Problem in Abstractive Text Summarization: A Survey. (arXiv:2104.14839v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.14839","description":"<p>Recently, various neural encoder-decoder models pioneered by Seq2Seq\nframework have been proposed to achieve the goal of generating more abstractive\nsummaries by learning to map input text to output text. At a high level, such\nneural models can freely generate summaries without any constraint on the words\nor phrases used. Moreover, their format is closer to human-edited summaries and\noutput is more readable and fluent. However, the neural model's abstraction\nability is a double-edged sword. A commonly observed problem with the generated\nsummaries is the distortion or fabrication of factual information in the\narticle. This inconsistency between the original text and the summary has\ncaused various concerns over its applicability, and the previous evaluation\nmethods of text summarization are not suitable for this issue. In response to\nthe above problems, the current research direction is predominantly divided\ninto two categories, one is to design fact-aware evaluation metrics to select\noutputs without factual inconsistency errors, and the other is to develop new\nsummarization systems towards factual consistency. In this survey, we focus on\npresenting a comprehensive review of these fact-specific evaluation methods and\ntext summarization models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yichong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiachong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiaocheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InCoder: A Generative Model for Code Infilling and Synthesis. (arXiv:2204.05999v3 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2204.05999","description":"<p>Code is seldom written in a single left-to-right pass and is instead\nrepeatedly edited and refined. We introduce InCoder, a unified generative model\nthat can perform program synthesis (via left-to-right generation) as well as\nediting (via infilling). InCoder is trained to generate code files from a large\ncorpus of permissively licensed code, where regions of code have been randomly\nmasked and moved to the end of each file, allowing code infilling with\nbidirectional context. Our model is the first generative model that is able to\ndirectly perform zero-shot code infilling, which we evaluate on challenging\ntasks such as type inference, comment generation, and variable re-naming. We\nfind that the ability to condition on bidirectional context substantially\nimproves performance on these tasks, while still performing comparably on\nstandard program synthesis benchmarks in comparison to left-to-right only\nmodels pretrained at similar scale. The InCoder models and code are publicly\nreleased. https://sites.google.com/view/incoder-code-models\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1\">Daniel Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jessy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sida Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1\">Eric Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1\">Freda Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">Ruiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change). (arXiv:2206.10498v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.10498","description":"<p>Recent advances in large language models (LLMs) have transformed the field of\nnatural language processing (NLP). From GPT-3 to PaLM, the state-of-the-art\nperformance on natural language tasks is being pushed forward with every new\nlarge language model. Along with natural language abilities, there has been a\nsignificant interest in understanding whether such models exhibit reasoning\ncapabilities with the use of reasoning benchmarks. However, even though results\nare seemingly positive, these benchmarks prove to be simplistic in nature and\nthe performance of LLMs on these benchmarks cannot be used as evidence to\nsupport, many a times outlandish, claims being made about LLMs' reasoning\ncapabilities. Further, these only represent a very limited set of simple\nreasoning tasks and we need to look at more sophisticated reasoning problems if\nwe are to measure the true limits of such LLM-based systems. Motivated by this,\nwe propose an extensible assessment framework to test the capabilities of LLMs\non reasoning about actions and change, a central aspect of human intelligence.\nWe provide multiple test cases that are more involved than any of the\npreviously established benchmarks and each test case evaluates a different\naspect of reasoning about actions and change. Results on GPT-3 (davinci),\nInstruct-GPT3 (text-davinci-002) and BLOOM (176B), showcase subpar performance\non such reasoning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valmeekam_K/0/1/0/all/0/1\">Karthik Valmeekam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olmo_A/0/1/0/all/0/1\">Alberto Olmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sreedharan_S/0/1/0/all/0/1\">Sarath Sreedharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kambhampati_S/0/1/0/all/0/1\">Subbarao Kambhampati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M-SpeechCLIP: Leveraging Large-Scale, Pre-Trained Models for Multilingual Speech to Image Retrieval. (arXiv:2211.01180v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.01180","description":"<p>This work investigates the use of large-scale, English-only pre-trained\nmodels (CLIP and HuBERT) for multilingual image-speech retrieval. For\nnon-English image-speech retrieval, we outperform the current state-of-the-art\nperformance by a wide margin both when training separate models for each\nlanguage, and with a single model which processes speech in all three\nlanguages. We identify key differences in model behavior and performance\nbetween English and non-English settings, attributable to the English-only\npre-training of CLIP and HuBERT, and investigate how fine-tuning the\npre-trained models impacts these differences. Finally, we show that our models\ncan be used for mono- and cross-lingual speech-text retrieval and cross-lingual\nspeech-speech retrieval, despite never having seen any parallel speech-text or\nspeech-speech data during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berry_L/0/1/0/all/0/1\">Layne Berry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shih_Y/0/1/0/all/0/1\">Yi-Jen Shih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsuan-Fu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transformer-Based Substitute Recommendation Model Incorporating Weakly Supervised Customer Behavior Data. (arXiv:2211.02533v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2211.02533","description":"<p>The substitute-based recommendation is widely used in E-commerce to provide\nbetter alternatives to customers. However, existing research typically uses the\ncustomer behavior signals like co-view and view-but-purchase-another to capture\nthe substitute relationship. Despite its intuitive soundness, we find that such\nan approach might ignore the functionality and characteristics of products. In\nthis paper, we adapt substitute recommendation into language matching problem\nby taking product title description as model input to consider product\nfunctionality. We design a new transformation method to de-noise the signals\nderived from production data. In addition, we consider multilingual support\nfrom the engineering point of view. Our proposed end-to-end transformer-based\nmodel achieves both successes from offline and online experiments. The proposed\nmodel has been deployed in a large-scale E-commerce website for 11 marketplaces\nin 6 languages. Our proposed model is demonstrated to increase revenue by 19%\nbased on an online A/B experiment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wenting Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Haoyang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xingjian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neppalli_N/0/1/0/all/0/1\">Naveen Neppalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discharge Summary Hospital Course Summarisation of In Patient Electronic Health Record Text with Clinical Concept Guided Deep Pre-Trained Transformer Models. (arXiv:2211.07126v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07126","description":"<p>Brief Hospital Course (BHC) summaries are succinct summaries of an entire\nhospital encounter, embedded within discharge summaries, written by senior\nclinicians responsible for the overall care of a patient. Methods to\nautomatically produce summaries from inpatient documentation would be\ninvaluable in reducing clinician manual burden of summarising documents under\nhigh time-pressure to admit and discharge patients. Automatically producing\nthese summaries from the inpatient course, is a complex, multi-document\nsummarisation task, as source notes are written from various perspectives (e.g.\nnursing, doctor, radiology), during the course of the hospitalisation. We\ndemonstrate a range of methods for BHC summarisation demonstrating the\nperformance of deep learning summarisation models across extractive and\nabstractive summarisation scenarios. We also test a novel ensemble extractive\nand abstractive summarisation model that incorporates a medical concept\nontology (SNOMED) as a clinical guidance signal and shows superior performance\nin 2 real-world clinical data sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Searle_T/0/1/0/all/0/1\">Thomas Searle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_Z/0/1/0/all/0/1\">Zina Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teo_J/0/1/0/all/0/1\">James Teo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobson_R/0/1/0/all/0/1\">Richard Dobson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STAGE: Span Tagging and Greedy Inference Scheme for Aspect Sentiment Triplet Extraction. (arXiv:2211.15003v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.15003","description":"<p>Aspect Sentiment Triplet Extraction (ASTE) has become an emerging task in\nsentiment analysis research, aiming to extract triplets of the aspect term, its\ncorresponding opinion term, and its associated sentiment polarity from a given\nsentence. Recently, many neural networks based models with different tagging\nschemes have been proposed, but almost all of them have their limitations:\nheavily relying on 1) prior assumption that each word is only associated with a\nsingle role (e.g., aspect term, or opinion term, etc. ) and 2) word-level\ninteractions and treating each opinion/aspect as a set of independent words.\nHence, they perform poorly on the complex ASTE task, such as a word associated\nwith multiple roles or an aspect/opinion term with multiple words. Hence, we\npropose a novel approach, Span TAgging and Greedy infErence (STAGE), to extract\nsentiment triplets in span-level, where each span may consist of multiple words\nand play different roles simultaneously. To this end, this paper formulates the\nASTE task as a multi-class span classification problem. Specifically, STAGE\ngenerates more accurate aspect sentiment triplet extractions via exploring\nspan-level information and constraints, which consists of two components,\nnamely, span tagging scheme and greedy inference strategy. The former tag all\npossible candidate spans based on a newly-defined tagging set. The latter\nretrieves the aspect/opinion term with the maximum length from the candidate\nsentiment snippet to output sentiment triplets. Furthermore, we propose a\nsimple but effective model based on the STAGE, which outperforms the\nstate-of-the-arts by a large margin on four widely-used datasets. Moreover, our\nSTAGE can be easily generalized to other pair/triplet extraction tasks, which\nalso demonstrates the superiority of the proposed scheme STAGE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shuo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yuanyuan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1\">Rui Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dangyang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lifelong Embedding Learning and Transfer for Growing Knowledge Graphs. (arXiv:2211.15845v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.15845","description":"<p>Existing knowledge graph (KG) embedding models have primarily focused on\nstatic KGs. However, real-world KGs do not remain static, but rather evolve and\ngrow in tandem with the development of KG applications. Consequently, new facts\nand previously unseen entities and relations continually emerge, necessitating\nan embedding model that can quickly learn and transfer new knowledge through\ngrowth. Motivated by this, we delve into an expanding field of KG embedding in\nthis paper, i.e., lifelong KG embedding. We consider knowledge transfer and\nretention of the learning on growing snapshots of a KG without having to learn\nembeddings from scratch. The proposed model includes a masked KG autoencoder\nfor embedding learning and update, with an embedding transfer strategy to\ninject the learned knowledge into the new entity and relation embeddings, and\nan embedding regularization method to avoid catastrophic forgetting. To\ninvestigate the impacts of different aspects of KG growth, we construct four\ndatasets to evaluate the performance of lifelong KG embedding. Experimental\nresults show that the proposed model outperforms the state-of-the-art inductive\nand lifelong embedding baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yuanning Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zequn Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenqiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yiqiao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kexin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MiLMo:Minority Multilingual Pre-trained Language Model. (arXiv:2212.01779v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.01779","description":"<p>Pre-trained language models are trained on large-scale unsupervised data, and\nthey can fine-turn the model only on small-scale labeled datasets, and achieve\ngood results. Multilingual pre-trained language models can be trained on\nmultiple languages, and the model can understand multiple languages at the same\ntime. At present, the search on pre-trained models mainly focuses on rich\nresources, while there is relatively little research on low-resource languages\nsuch as minority languages, and the public multilingual pre-trained language\nmodel can not work well for minority languages. Therefore, this paper\nconstructs a multilingual pre-trained model named MiLMo that performs better on\nminority language tasks, including Mongolian, Tibetan, Uyghur, Kazakh and\nKorean. To solve the problem of scarcity of datasets on minority languages and\nverify the effectiveness of the MiLMo model, this paper constructs a minority\nmultilingual text classification dataset named MiTC, and trains a word2vec\nmodel for each language. By comparing the word2vec model and the pre-trained\nmodel in the text classification task, this paper provides an optimal scheme\nfor the downstream task research of minority languages. The final experimental\nresults show that the performance of the pre-trained model is better than that\nof the word2vec model, and it has achieved the best results in minority\nmultilingual text classification. The multilingual pre-trained model MiLMo,\nmultilingual word2vec model and multilingual text classification dataset MiTC\nare published on <a href=\"http://milmo.cmli-nlp.com/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Junjie Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hanru Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinhe Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_W/0/1/0/all/0/1\">Wugedele Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaobing Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How would Stance Detection Techniques Evolve after the Launch of ChatGPT?. (arXiv:2212.14548v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.14548","description":"<p>Stance detection refers to the task of extracting the standpoint (Favor,\nAgainst or Neither) towards a target in given texts. Such research gains\nincreasing attention with the proliferation of social media contents. The\nconventional framework of handling stance detection is converting it into text\nclassification tasks. Deep learning models have already replaced rule-based\nmodels and traditional machine learning models in solving such problems.\nCurrent deep neural networks are facing two main challenges which are\ninsufficient labeled data and information in social media posts and the\nunexplainable nature of deep learning models. A new pre-trained language model\nchatGPT was launched on Nov 30, 2022. For the stance detection tasks, our\nexperiments show that ChatGPT can achieve SOTA or similar performance for\ncommonly used datasets including SemEval-2016 and P-Stance. At the same time,\nChatGPT can provide explanation for its own prediction, which is beyond the\ncapability of any existing model. The explanations for the cases it cannot\nprovide classification results are especially useful. ChatGPT has the potential\nto be the best AI model for stance detection tasks in NLP, or at least change\nthe research paradigm of this field. ChatGPT also opens up the possibility of\nbuilding explanatory AI for stance detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">Daijun Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Liwen Jing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.01313","description":"<p>This work provides a formalization of Knowledge Graphs (KGs) as a new class\nof graphs that we denote doubly exchangeable attributed graphs, where node and\npairwise (joint 2-node) representations must be equivariant to permutations of\nboth node ids and edge (&amp; node) attributes (relations &amp; node features).\nDouble-permutation equivariant KG representations open a new research direction\nin KGs. We show that this equivariance imposes a structural representation of\nrelations that allows neural networks to perform complex logical reasoning\ntasks in KGs. Finally, we introduce a general blueprint for such equivariant\nrepresentations and test a simple GNN-based double-permutation equivariant\nneural architecture that achieve state-of-the-art Hits@10 test accuracy in the\nWN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately\nperform logical reasoning tasks that no existing methods can perform, to the\nbest of our knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yangze Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_B/0/1/0/all/0/1\">Bruno Ribeiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RESDSQL: Decoupling Schema Linking and Skeleton Parsing for Text-to-SQL. (arXiv:2302.05965v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.05965","description":"<p>One of the recent best attempts at Text-to-SQL is the pre-trained language\nmodel. Due to the structural property of the SQL queries, the seq2seq model\ntakes the responsibility of parsing both the schema items (i.e., tables and\ncolumns) and the skeleton (i.e., SQL keywords). Such coupled targets increase\nthe difficulty of parsing the correct SQL queries especially when they involve\nmany schema items and logic operators. This paper proposes a ranking-enhanced\nencoding and skeleton-aware decoding framework to decouple the schema linking\nand the skeleton parsing. Specifically, for a seq2seq encoder-decode model, its\nencoder is injected by the most relevant schema items instead of the whole\nunordered ones, which could alleviate the schema linking effort during SQL\nparsing, and its decoder first generates the skeleton and then the actual SQL\nquery, which could implicitly constrain the SQL parsing. We evaluate our\nproposed framework on Spider and its three robustness variants: Spider-DK,\nSpider-Syn, and Spider-Realistic. The experimental results show that our\nframework delivers promising performance and robustness. Our code is available\nat https://github.com/RUCKBReasoning/RESDSQL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cuiping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guiding Large Language Models via Directional Stimulus Prompting. (arXiv:2302.11520v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.11520","description":"<p>We introduce a new framework, Directional Stimulus Prompting, that uses a\ntuneable language model (LM) to provide guidance for the black-box frozen large\nlanguage model (LLM) on downstream tasks. Unlike prior work that manually or\nautomatically finds the optimal prompt for each task, we train a policy LM to\ngenerate discrete tokens as directional stimulus of each input, which is a\nhint/cue such as keywords of an article for summarization. The directional\nstimulus is then combined with the original input and fed into the LLM to guide\nits generation toward the desired target. The policy LM can be trained through\n1) supervised learning from annotated data and 2) reinforcement learning from\noffline and online rewards to explore directional stimulus that better aligns\nLLMs with human preferences. This framework is flexibly applicable to various\nLMs and tasks. To verify its effectiveness, we apply our framework to\nsummarization and dialogue response generation tasks. Experimental results\ndemonstrate that it can significantly improve LLMs' performance with a small\ncollection of training data: a T5 (780M) trained with 2,000 samples from the\nCNN/Daily Mail dataset improves Codex (175B)'s performance by 9.0% in ROUGE-Avg\nscores; only 80 dialogues can boost the combined score by 39.7%, achieving\ncomparable or even better performance than some fully trained models on the\nMultiWOZ dataset. We have made our code publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1\">Michel Galley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xifeng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Language Models with Language Feedback at Scale. (arXiv:2303.16755v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.16755","description":"<p>Pretrained language models often generate outputs that are not in line with\nhuman preferences, such as harmful text or factually incorrect summaries.\nRecent work approaches the above issues by learning from a simple form of human\nfeedback: comparisons between pairs of model-generated outputs. However,\ncomparison feedback only conveys limited information about human preferences.\nIn this paper, we introduce Imitation learning from Language Feedback (ILF), a\nnew approach that utilizes more informative language feedback. ILF consists of\nthree steps that are applied iteratively: first, conditioning the language\nmodel on the input, an initial LM output, and feedback to generate refinements.\nSecond, selecting the refinement incorporating the most feedback. Third,\nfinetuning the language model to maximize the likelihood of the chosen\nrefinement given the input. We show theoretically that ILF can be viewed as\nBayesian Inference, similar to Reinforcement Learning from human feedback. We\nevaluate ILF's effectiveness on a carefully-controlled toy task and a realistic\nsummarization task. Our experiments demonstrate that large language models\naccurately incorporate feedback and that finetuning with ILF scales well with\nthe dataset size, even outperforming finetuning on human summaries. Learning\nfrom both language and comparison feedback outperforms learning from each\nalone, achieving human-level summarization performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scheurer_J/0/1/0/all/0/1\">J&#xe9;r&#xe9;my Scheurer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campos_J/0/1/0/all/0/1\">Jon Ander Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korbak_T/0/1/0/all/0/1\">Tomasz Korbak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1\">Jun Shern Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Angelica Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Large Language Models. (arXiv:2303.18223v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.18223","description":"<p>Language is essentially a complex, intricate system of human expressions\ngoverned by grammatical rules. It poses a significant challenge to develop\ncapable AI algorithms for comprehending and grasping a language. As a major\napproach, language modeling has been widely studied for language understanding\nand generation in the past two decades, evolving from statistical language\nmodels to neural language models. Recently, pre-trained language models (PLMs)\nhave been proposed by pre-training Transformer models over large-scale corpora,\nshowing strong capabilities in solving various NLP tasks. Since researchers\nhave found that model scaling can lead to performance improvement, they further\nstudy the scaling effect by increasing the model size to an even larger size.\nInterestingly, when the parameter scale exceeds a certain level, these enlarged\nlanguage models not only achieve a significant performance improvement but also\nshow some special abilities that are not present in small-scale language\nmodels. To discriminate the difference in parameter scale, the research\ncommunity has coined the term large language models (LLM) for the PLMs of\nsignificant size. Recently, the research on LLMs has been largely advanced by\nboth academia and industry, and a remarkable progress is the launch of ChatGPT,\nwhich has attracted widespread attention from society. The technical evolution\nof LLMs has been making an important impact on the entire AI community, which\nwould revolutionize the way how we develop and use AI algorithms. In this\nsurvey, we review the recent advances of LLMs by introducing the background,\nkey findings, and mainstream techniques. In particular, we focus on four major\naspects of LLMs, namely pre-training, adaptation tuning, utilization, and\ncapacity evaluation. Besides, we also summarize the available resources for\ndeveloping LLMs and discuss the remaining issues for future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yupeng Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1\">Yingqian Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Beichen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zican Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yifan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yushuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhipeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jinhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1\">Ruiyang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xinyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zikang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Creativity of Large Language Models. (arXiv:2304.00008v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2304.00008","description":"<p>Large Language Models (LLMs) are revolutionizing several areas of Artificial\nIntelligence. One of the most remarkable applications is creative writing,\ne.g., poetry or storytelling: the generated outputs are often of astonishing\nquality. However, a natural question arises: can LLMs be really considered\ncreative? In this article we firstly analyze the development of LLMs under the\nlens of creativity theories, investigating the key open questions and\nchallenges. Then, we discuss a set of \"easy\" and \"hard\" problems in machine\ncreativity, presenting them in relation to LLMs. Finally, we examine the\nsocietal impact of these technologies with a particular focus on the creative\nindustries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Franceschelli_G/0/1/0/all/0/1\">Giorgio Franceschelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musolesi_M/0/1/0/all/0/1\">Mirco Musolesi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: A Preliminary Empirical Study. (arXiv:2304.00723v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.00723","description":"<p>Evaluating the quality of generated text is a challenging task in natural\nlanguage processing. This difficulty arises from the inherent complexity and\ndiversity of text. Recently, OpenAI's ChatGPT, a powerful large language model\n(LLM), has garnered significant attention due to its impressive performance in\nvarious tasks. Therefore, we present this report to investigate the\neffectiveness of LLMs, especially ChatGPT, and explore ways to optimize their\nuse in assessing text quality. We compared three kinds of reference-free\nevaluation methods based on ChatGPT or similar LLMs. The experimental results\nprove that ChatGPT is capable to evaluate text quality effectively from various\nperspectives without reference and demonstrates superior performance than most\nexisting automatic metrics. In particular, the Explicit Score, which utilizes\nChatGPT to generate a numeric score measuring text quality, is the most\neffective and reliable method among the three exploited approaches. However,\ndirectly comparing the quality of two texts using ChatGPT may lead to\nsuboptimal results. We hope this report will provide valuable insights into\nselecting appropriate methods for evaluating text quality with LLMs such as\nChatGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Learning Based Multilingual Emoji Prediction In Clean and Attack Scenarios. (arXiv:2304.01005v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01005","description":"<p>Federated learning is a growing field in the machine learning community due\nto its decentralized and private design. Model training in federated learning\nis distributed over multiple clients giving access to lots of client data while\nmaintaining privacy. Then, a server aggregates the training done on these\nmultiple clients without access to their data, which could be emojis widely\nused in any social media service and instant messaging platforms to express\nusers' sentiments. This paper proposes federated learning-based multilingual\nemoji prediction in both clean and attack scenarios. Emoji prediction data have\nbeen crawled from both Twitter and SemEval emoji datasets. This data is used to\ntrain and evaluate different transformer model sizes including a sparsely\nactivated transformer with either the assumption of clean data in all clients\nor poisoned data via label flipping attack in some clients. Experimental\nresults on these models show that federated learning in either clean or\nattacked scenarios performs similarly to centralized training in multilingual\nemoji prediction on seen and unseen languages under different data sources and\ndistributions. Our trained transformers perform better than other techniques on\nthe SemEval emoji dataset in addition to the privacy as well as distributed\nbenefits of federated learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gamal_K/0/1/0/all/0/1\">Karim Gamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaber_A/0/1/0/all/0/1\">Ahmed Gaber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amer_H/0/1/0/all/0/1\">Hossam Amer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Contrastive Transfer Framework with Propagation Structure for Boosting Low-Resource Rumor Detection. (arXiv:2304.01492v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01492","description":"<p>The truth is significantly hampered by massive rumors that spread along with\nbreaking news or popular topics. Since there is sufficient corpus gathered from\nthe same domain for model training, existing rumor detection algorithms show\npromising performance on yesterday's news. However, due to a lack of\nsubstantial training data and prior expert knowledge, they are poor at spotting\nrumors concerning unforeseen events, especially those propagated in different\nlanguages (i.e., low-resource regimes). In this paper, we propose a unified\ncontrastive transfer framework to detect rumors by adapting the features\nlearned from well-resourced rumor data to that of the low-resourced with only\nfew-shot annotations. More specifically, we first represent rumor circulated on\nsocial media as an undirected topology for enhancing the interaction of user\nopinions, and then train a Multi-scale Graph Convolutional Network via a\nunified contrastive paradigm to mine effective clues simultaneously from post\nsemantics and propagation structure. Our model explicitly breaks the barriers\nof the domain and/or language issues, via language alignment and a novel\ndomain-adaptive contrastive learning mechanism. To well-generalize the\nrepresentation learning using a small set of annotated target events, we reveal\nthat rumor-indicative signal is closely correlated with the uniformity of the\ndistribution of these events. We design a target-wise contrastive training\nmechanism with three event-level data augmentation strategies, capable of\nunifying the representations by distinguishing target events. Extensive\nexperiments conducted on four low-resource datasets collected from real-world\nmicroblog platforms demonstrate that our framework achieves much better\nperformance than state-of-the-art methods and exhibits a superior capacity for\ndetecting rumors at early stages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongzhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhiwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Mingfei Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models. (arXiv:2304.01852v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01852","description":"<p>This paper presents a comprehensive survey of ChatGPT and GPT-4,\nstate-of-the-art large language models (LLM) from the GPT series, and their\nprospective applications across diverse domains. Indeed, key innovations such\nas large-scale pre-training that captures knowledge across the entire world\nwide web, instruction fine-tuning and Reinforcement Learning from Human\nFeedback (RLHF) have played significant roles in enhancing LLMs' adaptability\nand performance. We performed an in-depth analysis of 194 relevant papers on\narXiv, encompassing trend analysis, word cloud representation, and distribution\nanalysis across various application domains. The findings reveal a significant\nand increasing interest in ChatGPT/GPT-4 research, predominantly centered on\ndirect natural language processing applications, while also demonstrating\nconsiderable potential in areas ranging from education and history to\nmathematics, medicine, and physics. This study endeavors to furnish insights\ninto ChatGPT's capabilities, potential implications, ethical concerns, and\noffer direction for future advancements in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tianle Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Siyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiayue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuanyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jiaming Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Antong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mengshen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dajiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiang_N/0/1/0/all/0/1\">Ning Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dingang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_B/0/1/0/all/0/1\">Bao Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT. (arXiv:2304.02213v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.02213","description":"<p>Data has growing significance in exploring cutting-edge materials, and the\nnumber of datasets has been generated either by hand or automated approaches.\nHowever, the materials science field struggles to effectively utilize the\nabundance of generated data, especially in applied disciplines where materials\nare evaluated based on device performance rather than their properties. This\narticle presents a new NLP task called structured information inference (SII)\nto address the complexities of information extraction at the device level in\nmaterials science. We accomplished this task by tuning GPT-3 on an existing\nperovskite solar cell FAIR (Findable, Accessible, Interoperable, Reusable)\ndataset with 91.8% F1-score and we updated the dataset with all related\nscientific papers up to now. The produced data is formatted and normalized,\nenabling its direct utilization as input in subsequent data analysis. This\nfeature will enable materials scientists to develop their own models by\nselecting high-quality review papers within their domain. Furthermore, we\ndesigned experiments to predict solar cells' electrical performance and design\nmaterials or devices with target parameters through LLM. We obtained comparable\nperformance with traditional machine learning methods without feature\nselection, demonstrating the potential of LLMs to learn scientific knowledge\nand design new materials like a materials scientist.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yuwei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yufei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linghu_Q/0/1/0/all/0/1\">Qingyuan Linghu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaozhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kit_C/0/1/0/all/0/1\">Chunyu Kit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grazian_C/0/1/0/all/0/1\">Clara Grazian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoex_B/0/1/0/all/0/1\">Bram Hoex</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noise-Robust Dense Retrieval via Contrastive Alignment Post Training. (arXiv:2304.03401v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2304.03401","description":"<p>The success of contextual word representations and advances in neural\ninformation retrieval have made dense vector-based retrieval a standard\napproach for passage and document ranking. While effective and efficient,\ndual-encoders are brittle to variations in query distributions and noisy\nqueries. Data augmentation can make models more robust but introduces overhead\nto training set generation and requires retraining and index regeneration. We\npresent Contrastive Alignment POst Training (CAPOT), a highly efficient\nfinetuning method that improves model robustness without requiring index\nregeneration, the training set optimization, or alteration. CAPOT enables\nrobust retrieval by freezing the document encoder while the query encoder\nlearns to align noisy queries with their unaltered root. We evaluate CAPOT\nnoisy variants of MSMARCO, Natural Questions, and Trivia QA passage retrieval,\nfinding CAPOT has a similar impact as data augmentation with none of its\noverhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Campos_D/0/1/0/all/0/1\">Daniel Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magnani_A/0/1/0/all/0/1\">Alessandro Magnani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Catalogue Generation for Literature Review: A Benchmark. (arXiv:2304.03512v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.03512","description":"<p>Multi-document scientific summarization can extract and organize important\ninformation from an abundant collection of papers, arousing widespread\nattention recently. However, existing efforts focus on producing lengthy\noverviews lacking a clear and logical hierarchy. To alleviate this problem, we\npresent an atomic and challenging task named Hierarchical Catalogue Generation\nfor Literature Review (HiCatGLR), which aims to generate a hierarchical\ncatalogue for a review paper given various references. We carefully construct a\nnovel English Hierarchical Catalogues of Literature Reviews Dataset (HiCaD)\nwith 13.8k literature review catalogues and 120k reference papers, where we\nbenchmark diverse experiments via the end-to-end and pipeline methods. To\naccurately assess the model performance, we design evaluation metrics for\nsimilarity to ground truth from semantics and structure. Besides, our extensive\nanalyses verify the high quality of our dataset and the effectiveness of our\nevaluation metrics. Furthermore, we discuss potential directions for this task\nto motivate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiaocheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiachong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yingsheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-04-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}