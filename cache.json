{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-10-06T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English ChatGPT Responses. (arXiv:2310.03031v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03031","description":"<p>With the introduction of ChatGPT, OpenAI made large language models (LLM)\naccessible to users with limited IT expertise. However, users with no\nbackground in natural language processing (NLP) might lack a proper\nunderstanding of LLMs. Thus the awareness of their inherent limitations, and\ntherefore will take the systems' output at face value. In this paper, we\nsystematically analyse prompts and the generated responses to identify possible\nproblematic issues with a special focus on gender biases, which users need to\nbe aware of when processing the system's output. We explore how ChatGPT reacts\nin English and German if prompted to answer from a female, male, or neutral\nperspective. In an in-depth investigation, we examine selected prompts and\nanalyse to what extent responses differ if the system is prompted several times\nin an identical way. On this basis, we show that ChatGPT is indeed useful for\nhelping non-IT users draft texts for their daily work. However, it is\nabsolutely crucial to thoroughly check the system's responses for biases as\nwell as for syntactic and grammatical mistakes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Urchs_S/0/1/0/all/0/1\">Stefanie Urchs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thurner_V/0/1/0/all/0/1\">Veronika Thurner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assenmacher_M/0/1/0/all/0/1\">Matthias A&#xdf;enmacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heumann_C/0/1/0/all/0/1\">Christian Heumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiemichen_S/0/1/0/all/0/1\">Stephanie Thiemichen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How FaR Are Large Language Models From Agents with Theory-of-Mind?. (arXiv:2310.03051v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03051","description":"<p>\"Thinking is for Doing.\" Humans can infer other people's mental states from\nobservations--an ability called Theory-of-Mind (ToM)--and subsequently act\npragmatically on those inferences. Existing question answering benchmarks such\nas ToMi ask models questions to make inferences about beliefs of characters in\na story, but do not test whether models can then use these inferences to guide\ntheir actions. We propose a new evaluation paradigm for large language models\n(LLMs): Thinking for Doing (T4D), which requires models to connect inferences\nabout others' mental states to actions in social scenarios. Experiments on T4D\ndemonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking\ncharacters' beliefs in stories, but they struggle to translate this capability\ninto strategic action. Our analysis reveals the core challenge for LLMs lies in\nidentifying the implicit inferences about mental states without being\nexplicitly asked about as in ToMi, that lead to choosing the correct action in\nT4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee\nand Reflect (FaR), which provides a reasoning structure that encourages LLMs to\nanticipate future challenges and reason about potential actions. FaR boosts\nGPT-4's performance from 50% to 71% on T4D, outperforming other prompting\nmethods such as Chain-of-Thought and Self-Ask. Moreover, FaR generalizes to\ndiverse out-of-distribution story structures and scenarios that also require\nToM inferences to choose an action, consistently outperforming other methods\nincluding few-shot in-context learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potharaju_S/0/1/0/all/0/1\">Srividya Pranavi Potharaju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aditya Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKee_K/0/1/0/all/0/1\">Kevin R. McKee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1\">Aida Nematzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1\">Shyam Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faruqui_M/0/1/0/all/0/1\">Manaal Faruqui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Knowledge-Critical Subnetworks in Pretrained Language Models. (arXiv:2310.03084v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03084","description":"<p>Pretrained language models (LMs) encode implicit representations of knowledge\nin their parameters. However, localizing these representations and\ndisentangling them from each other remains an open problem. In this work, we\ninvestigate whether pretrained language models contain various\nknowledge-critical subnetworks: particular sparse computational subgraphs\nresponsible for encoding specific knowledge the model has memorized. We propose\na multi-objective differentiable weight masking scheme to discover these\nsubnetworks and show that we can use them to precisely remove specific\nknowledge from models while minimizing adverse effects on the behavior of the\noriginal language model. We demonstrate our method on multiple GPT2 variants,\nuncovering highly sparse subnetworks (98%+) that are solely responsible for\nspecific collections of relational knowledge. When these subnetworks are\nremoved, the remaining network maintains most of its initial capacity (modeling\nlanguage and other memorized relational knowledge) but struggles to express the\nremoved knowledge, and suffers performance drops on examples needing this\nremoved knowledge on downstream tasks after finetuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bayazit_D/0/1/0/all/0/1\">Deniz Bayazit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foroutan_N/0/1/0/all/0/1\">Negar Foroutan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1\">Gail Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning. (arXiv:2310.03094v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03094","description":"<p>Large language models (LLMs) such as GPT-4 have exhibited remarkable\nperformance in a variety of tasks, but this strong performance often comes with\nthe high expense of using paid API services. In this paper, we are motivated to\nstudy building an LLM cascade to save the cost of using LLMs, particularly for\nperforming reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline\nfollows the intuition that simpler questions can be addressed by a weaker but\nmore affordable LLM, whereas only the challenging questions necessitate the\nstronger and more expensive LLM. To realize this decision-making, we consider\nthe \"answer consistency\" of the weaker LLM as a signal of the question\ndifficulty and propose several methods for the answer sampling and consistency\nchecking, including one leveraging a mixture of two thought representations\n(i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six\nreasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and\nstronger LLMs, respectively, we demonstrate that our proposed LLM cascades can\nachieve performance comparable to using solely the stronger LLM but require\nonly 40% of its cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_M/0/1/0/all/0/1\">Murong Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Liang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Ziyu Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaTool Benchmark: Deciding Whether to Use Tools and Which to Use. (arXiv:2310.03128v1 [cs.SE])","link":"http://arxiv.org/abs/2310.03128","description":"<p>Large language models (LLMs) have garnered significant attention due to their\nimpressive natural language processing (NLP) capabilities. Recently, many\nstudies have focused on the tool utilization ability of LLMs. They primarily\ninvestigated how LLMs effectively collaborate with given specific tools.\nHowever, in scenarios where LLMs serve as intelligent agents, as seen in\napplications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate\ndecision-making processes that involve deciding whether to employ a tool and\nselecting the most suitable tool(s) from a collection of available tools to\nfulfill user requests. Therefore, in this paper, we introduce MetaTool, a\nbenchmark designed to evaluate whether LLMs have tool usage awareness and can\ncorrectly choose tools. Specifically, we create a dataset called ToolE within\nthe benchmark. This dataset contains various types of user queries in the form\nof prompts that trigger LLMs to use tools, including both single-tool and\nmulti-tool scenarios. Subsequently, we set the tasks for both tool usage\nawareness and tool selection. We define four subtasks from different\nperspectives in tool selection, including tool selection with similar choices,\ntool selection in specific scenarios, tool selection with possible reliability\nissues, and multi-tool selection. We conduct experiments involving nine popular\nLLMs and find that the majority of them still struggle to effectively select\ntools, highlighting the existing gaps between LLMs and genuine intelligent\nagents. However, through the error analysis, we found there is still\nsignificant room for improvement. Finally, we conclude with insights for tool\ndevelopers that follow ChatGPT to provide detailed descriptions that can\nenhance the tool selection performance of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiawen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chenrui Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Siyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qihui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yao Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1\">Neil Zhenqiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$\\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program Synthesis. (arXiv:2310.03173v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03173","description":"<p>Program synthesis aims to create accurate, executable code from natural\nlanguage descriptions. This field has leveraged the power of reinforcement\nlearning (RL) in conjunction with large language models (LLMs), significantly\nenhancing code generation capabilities. This integration focuses on directly\noptimizing functional correctness, transcending conventional supervised losses.\nWhile current literature predominantly favors policy-based algorithms,\nattributes of program synthesis suggest a natural compatibility with\nvalue-based methods. This stems from rich collection of off-policy programs\ndeveloped by human programmers, and the straightforward verification of\ngenerated programs through automated unit testing (i.e. easily obtainable\nrewards in RL language). Diverging from the predominant use of policy-based\nalgorithms, our work explores the applicability of value-based approaches,\nleading to the development of our $\\mathcal{B}$-Coder (pronounced Bellman\ncoder). Yet, training value-based methods presents challenges due to the\nenormous search space inherent to program synthesis. To this end, we propose an\ninitialization protocol for RL agents utilizing pre-trained LMs and a\nconservative Bellman operator to reduce training complexities. Moreover, we\ndemonstrate how to leverage the learned value functions as a dual strategy to\npost-process generated programs. Our empirical evaluations demonstrated\n$\\mathcal{B}$-Coder's capability in achieving state-of-the-art performance\ncompared with policy-based methods. Remarkably, this achievement is reached\nwith minimal reward engineering effort, highlighting the effectiveness of\nvalue-based RL, independent of reward designs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zishun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1\">Yunzhe Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust and Interpretable Medical Image Classifiers via Concept Bottleneck Models. (arXiv:2310.03182v1 [cs.CV])","link":"http://arxiv.org/abs/2310.03182","description":"<p>Medical image classification is a critical problem for healthcare, with the\npotential to alleviate the workload of doctors and facilitate diagnoses of\npatients. However, two challenges arise when deploying deep learning models to\nreal-world healthcare applications. First, neural models tend to learn spurious\ncorrelations instead of desired features, which could fall short when\ngeneralizing to new domains (e.g., patients with different ages). Second, these\nblack-box models lack interpretability. When making diagnostic predictions, it\nis important to understand why a model makes a decision for trustworthy and\nsafety considerations. In this paper, to address these two limitations, we\npropose a new paradigm to build robust and interpretable medical image\nclassifiers with natural language concepts. Specifically, we first query\nclinical concepts from GPT-4, then transform latent image features into\nexplicit concepts with a vision-language model. We systematically evaluate our\nmethod on eight medical image classification datasets to verify its\neffectiveness. On challenging datasets with strong confounding factors, our\nmethod can mitigate spurious correlations thus substantially outperform\nstandard visual encoders and other baselines. Finally, we show how\nclassification with a small number of concepts brings a level of\ninterpretability for understanding model decisions through case studies in real\nmedical data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_A/0/1/0/all/0/1\">An Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiwu Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zexue He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_P/0/1/0/all/0/1\">Petros Karypis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chengyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gentili_A/0/1/0/all/0/1\">Amilcare Gentili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Chun-Nan Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs Between Groundedness and Human Preference. (arXiv:2310.03184v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03184","description":"<p>For middle-school math students, interactive question-answering (QA) with\ntutors is an effective way to learn. The flexibility and emergent capabilities\nof generative large language models (LLMs) has led to a surge of interest in\nautomating portions of the tutoring process - including interactive QA to\nsupport conceptual discussion of mathematical concepts. However, LLM responses\nto math questions can be incorrect or mismatched to the educational context -\nsuch as being misaligned with a school's curriculum. One potential solution is\nretrieval-augmented generation (RAG), which involves incorporating a vetted\nexternal knowledge source in the LLM prompt to increase response quality. In\nthis paper, we designed prompts that retrieve and use content from a\nhigh-quality open-source math textbook to generate responses to real student\nquestions. We evaluate the efficacy of this RAG system for middle-school\nalgebra and geometry QA by administering a multi-condition survey, finding that\nhumans prefer responses generated using RAG, but not when responses are too\ngrounded in the textbook content. We argue that while RAG is able to improve\nresponse quality, designers of math QA systems must consider trade-offs between\ngenerating responses preferred by students and responses closely matched to\nspecific educational resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levonian_Z/0/1/0/all/0/1\">Zachary Levonian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenglu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wangda Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gade_A/0/1/0/all/0/1\">Anoushka Gade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henkel_O/0/1/0/all/0/1\">Owen Henkel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Postle_M/0/1/0/all/0/1\">Millie-Ellen Postle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_W/0/1/0/all/0/1\">Wanli Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Rise of Open Science: Tracking the Evolution and Perceived Value of Data and Methods Link-Sharing Practices. (arXiv:2310.03193v1 [cs.DL])","link":"http://arxiv.org/abs/2310.03193","description":"<p>In recent years, funding agencies and journals increasingly advocate for open\nscience practices (e.g. data and method sharing) to improve the transparency,\naccess, and reproducibility of science. However, quantifying these practices at\nscale has proven difficult. In this work, we leverage a large-scale dataset of\n1.1M papers from arXiv that are representative of the fields of physics, math,\nand computer science to analyze the adoption of data and method link-sharing\npractices over time and their impact on article reception. To identify links to\ndata and methods, we train a neural text classification model to automatically\nclassify URL types based on contextual mentions in papers. We find evidence\nthat the practice of link-sharing to methods and data is spreading as more\npapers include such URLs over time. Reproducibility efforts may also be\nspreading because the same links are being increasingly reused across papers\n(especially in computer science); and these links are increasingly concentrated\nwithin fewer web domains (e.g. Github) over time. Lastly, articles that share\ndata and method links receive increased recognition in terms of citation count,\nwith a stronger effect when the shared links are active (rather than defunct).\nTogether, these findings demonstrate the increased spread and perceived value\nof data and method sharing practices in open science.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Hancheng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodge_J/0/1/0/all/0/1\">Jesse Dodge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McFarland_D/0/1/0/all/0/1\">Daniel A. McFarland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lucy Lu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Language Models Employ the Socratic Method? Experiments with Code Debugging. (arXiv:2310.03210v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03210","description":"<p>When employing the Socratic method of teaching, instructors guide students\ntoward solving a problem on their own rather than providing the solution\ndirectly. While this strategy can substantially improve learning outcomes, it\nis usually time-consuming and cognitively demanding. Automated Socratic\nconversational agents can augment human instruction and provide the necessary\nscale, however their development is hampered by the lack of suitable data for\ntraining and evaluation. In this paper, we introduce a manually created dataset\nof multi-turn Socratic advice that is aimed at helping a novice programmer fix\nbuggy solutions to simple computational problems. The dataset is then used for\nbenchmarking the Socratic debugging abilities of a number of language models,\nranging from fine-tuning the instruction-based text-to-text transformer Flan-T5\nto zero-shot and chain of thought prompting of the much larger GPT-4. The code\nand datasets are made freely available for research at the link below.\nhttps://github.com/taisazero/socratic-debugging-benchmark\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Al_Hossami_E/0/1/0/all/0/1\">Erfan Al-Hossami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bunescu_R/0/1/0/all/0/1\">Razvan Bunescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">Justin Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teehan_R/0/1/0/all/0/1\">Ryan Teehan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Performance of Multimodal Language Models. (arXiv:2310.03211v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03211","description":"<p>Instruction-tuned large language models (LLMs) have demonstrated promising\nzero-shot generalization capabilities across various downstream tasks. Recent\nresearch has introduced multimodal capabilities to LLMs by integrating\nindependently pretrained vision encoders through model grafting. These\nmultimodal variants undergo instruction tuning, similar to LLMs, enabling\neffective zero-shot generalization for multimodal tasks. This study conducts a\ncomparative analysis of different multimodal instruction tuning approaches and\nevaluates their performance across a range of tasks, including complex\nreasoning, conversation, image captioning, multiple-choice questions (MCQs),\nand binary classification. Through rigorous benchmarking and ablation\nexperiments, we reveal key insights for guiding architectural choices when\nincorporating multimodal capabilities into LLMs. However, current approaches\nhave limitations; they do not sufficiently address the need for a diverse\nmultimodal instruction dataset, which is crucial for enhancing task\ngeneralization. Additionally, they overlook issues related to truthfulness and\nfactuality when generating responses. These findings illuminate current\nmethodological constraints in adapting language models for image comprehension\nand provide valuable guidance for researchers and practitioners seeking to\nharness multimodal versions of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_U/0/1/0/all/0/1\">Utsav Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bas_E/0/1/0/all/0/1\">Erhan Bas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation. (arXiv:2310.03214v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03214","description":"<p>Most large language models (LLMs) are trained once and never updated; thus,\nthey lack the ability to dynamically adapt to our ever-changing world. In this\nwork, we perform a detailed study of the factuality of LLM-generated text in\nthe context of answering questions that test current world knowledge.\nSpecifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a\ndiverse range of question and answer types, including questions that require\nfast-changing world knowledge as well as questions with false premises that\nneed to be debunked. We benchmark a diverse array of both closed and\nopen-source LLMs under a two-mode evaluation procedure that allows us to\nmeasure both correctness and hallucination. Through human evaluations involving\nmore than 50K judgments, we shed light on limitations of these models and\ndemonstrate significant room for improvement: for instance, all models\n(regardless of model size) struggle on questions that involve fast-changing\nknowledge and false premises. Motivated by these results, we present\nFreshPrompt, a simple few-shot prompting method that substantially boosts the\nperformance of an LLM on FreshQA by incorporating relevant and up-to-date\ninformation retrieved from a search engine into the prompt. Our experiments\nshow that FreshPrompt outperforms both competing search engine-augmented\nprompting methods such as Self-Ask (Press et al., 2022) as well as commercial\nsystems such as Perplexity.AI. Further analysis of FreshPrompt reveals that\nboth the number of retrieved evidences and their order play a key role in\ninfluencing the correctness of LLM-generated answers. Additionally, instructing\nthe LLM to generate concise and direct answers helps reduce hallucination\ncompared to encouraging more verbose answers. To facilitate future work, we\nrelease FreshQA at github.com/freshllms/freshqa and commit to updating it at\nregular intervals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tu Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jerry Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tar_C/0/1/0/all/0/1\">Chris Tar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yun-Hsuan Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luong_T/0/1/0/all/0/1\">Thang Luong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Representations of First-person Pronouns for Prediction of Depression Symptom Severity. (arXiv:2310.03232v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03232","description":"<p>Prior work has shown that analyzing the use of first-person singular pronouns\ncan provide insight into individuals' mental status, especially depression\nsymptom severity. These findings were generated by counting frequencies of\nfirst-person singular pronouns in text data. However, counting doesn't capture\nhow these pronouns are used. Recent advances in neural language modeling have\nleveraged methods generating contextual embeddings. In this study, we sought to\nutilize the embeddings of first-person pronouns obtained from contextualized\nlanguage representation models to capture ways these pronouns are used, to\nanalyze mental status. De-identified text messages sent during online\npsychotherapy with weekly assessment of depression severity were used for\nevaluation. Results indicate the advantage of contextualized first-person\npronoun embeddings over standard classification token embeddings and\nfrequency-based pronoun analysis results in predicting depression symptom\nseverity. This suggests contextual representations of first-person pronouns can\nenhance the predictive utility of language used by people with depression\nsymptoms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xinyang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burkhardt_H/0/1/0/all/0/1\">Hannah A Burkhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arean_P/0/1/0/all/0/1\">Patricia A Are&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hull_T/0/1/0/all/0/1\">Thomas D Hull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1\">Trevor Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning. (arXiv:2310.03249v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03249","description":"<p>Large language models (LLMs) have achieved remarkable success across a wide\nspectrum of tasks; however, they still face limitations in scenarios that\ndemand long-term planning and spatial reasoning. To facilitate this line of\nresearch, in this work, we propose a new benchmark, termed $\\textbf{P}$ath\n$\\textbf{P}$lanning from $\\textbf{N}$atural $\\textbf{L}$anguage\n($\\textbf{PPNL}$). Our benchmark evaluates LLMs' spatial-temporal reasoning by\nformulating ''path planning'' tasks that require an LLM to navigate to target\nlocations while avoiding obstacles and adhering to constraints. Leveraging this\nbenchmark, we systematically investigate LLMs including GPT-4 via different\nfew-shot prompting methodologies and BART and T5 of various sizes via\nfine-tuning. Our experimental results show the promise of few-shot GPT-4 in\nspatial reasoning, when it is prompted to reason and act interleavedly,\nalthough it still fails to make long-term temporal reasoning. In contrast,\nwhile fine-tuned LLMs achieved impressive results on in-distribution reasoning\ntasks, they struggled to generalize to larger environments or environments with\nmore obstacles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aghzal_M/0/1/0/all/0/1\">Mohamed Aghzal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plaku_E/0/1/0/all/0/1\">Erion Plaku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Ziyu Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unlock Predictable Scaling from Emergent Abilities. (arXiv:2310.03262v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03262","description":"<p>The scientific scale-up of large language models (LLMs) necessitates a\ncomprehensive understanding of their scaling properties. However, the existing\nliterature on the scaling properties only yields an incomplete answer:\noptimization loss decreases predictably as the model size increases, in line\nwith established scaling law; yet no scaling law for task has been established\nand the task performances are far from predictable during scaling. Task\nperformances typically show minor gains on small models until they improve\ndramatically once models exceed a size threshold, exemplifying the ``emergent\nabilities''. In this study, we discover that small models, although they\nexhibit minor performance, demonstrate critical and consistent task performance\nimprovements that are not captured by conventional evaluation strategies due to\ninsufficient measurement resolution. To measure such improvements, we introduce\nPassUntil, an evaluation strategy through massive sampling in the decoding\nphase. We conduct quantitative investigations into the scaling law of task\nperformance. Firstly, a strict task scaling law is identified, enhancing the\npredictability of task performances. Remarkably, we are able to predict the\nperformance of the 2.4B model on code generation with merely 0.05\\% deviation\nbefore training starts. Secondly, underpinned by PassUntil, we observe concrete\nevidence of emergent abilities and ascertain that they are not in conflict with\nthe continuity of performance improvement. Their semblance to break-through is\nthat their scaling curve cannot be fitted by standard scaling law function. We\nthen introduce a mathematical definition for the emergent abilities. Through\nthe definition, we refute a prevalent ``multi-step reasoning hypothesis''\nregarding the genesis of emergent abilities and propose a new hypothesis with a\nsatisfying fit to the observed scaling curve.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengding Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinrong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chaoqun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zebin Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Guoyang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InstructProtein: Aligning Human and Protein Language via Knowledge Instruction. (arXiv:2310.03269v1 [q-bio.BM])","link":"http://arxiv.org/abs/2310.03269","description":"<p>Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, but they fall short in comprehending biological sequences\nsuch as proteins. To address this challenge, we propose InstructProtein, an\ninnovative LLM that possesses bidirectional generation capabilities in both\nhuman and protein languages: (i) taking a protein sequence as input to predict\nits textual function description and (ii) using natural language to prompt\nprotein sequence generation. To achieve this, we first pre-train an LLM on both\nprotein and natural language corpora, enabling it to comprehend individual\nlanguages. Then supervised instruction tuning is employed to facilitate the\nalignment of these two distinct languages. Herein, we introduce a knowledge\ngraph-based instruction generation framework to construct a high-quality\ninstruction dataset, addressing annotation imbalance and instruction deficits\nin existing protein-text corpus. In particular, the instructions inherit the\nstructural relations between proteins and function annotations in knowledge\ngraphs, which empowers our model to engage in the causal modeling of protein\nfunctions, akin to the chain-of-thought processes in natural languages.\nExtensive experiments on bidirectional protein-text generation tasks show that\nInstructProtein outperforms state-of-the-art LLMs by large margins. Moreover,\nInstructProtein serves as a pioneering step towards text-based protein function\nprediction and sequence design, effectively bridging the gap between protein\nand human language understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_Z/0/1/0/all/0/1\">Zeyuan Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ding_K/0/1/0/all/0/1\">Keyan Ding</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Qin_M/0/1/0/all/0/1\">Ming Qin</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiang Zhuang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Li_X/0/1/0/all/0/1\">Xiaotong Li</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Formalism and Approach for Improving Robustness of Large Language Models Using Risk-Adjusted Confidence Scores. (arXiv:2310.03283v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03283","description":"<p>Large Language Models (LLMs), such as ChatGPT, have achieved impressive\nmilestones in natural language processing (NLP). Despite their impressive\nperformance, the models are known to pose important risks. As these models are\ndeployed in real-world applications, a systematic understanding of different\nrisks posed by these models on tasks such as natural language inference (NLI),\nis much needed. In this paper, we define and formalize two distinct types of\nrisk: decision risk and composite risk. We also propose a risk-centric\nevaluation framework, and four novel metrics, for assessing LLMs on these risks\nin both in-domain and out-of-domain settings. Finally, we propose a\nrisk-adjusted calibration method called DwD for helping LLMs minimize these\nrisks in an overall NLI architecture. Detailed experiments, using four NLI\nbenchmarks, three baselines and two LLMs, including ChatGPT, show both the\npractical utility of the evaluation framework, and the efficacy of DwD in\nreducing decision and composite risk. For instance, when using DwD, an\nunderlying LLM is able to address an extra 20.1% of low-risk inference tasks\n(but which the LLM erroneously deems high-risk without risk adjustment) and\nskip a further 19.8% of high-risk tasks, which would have been answered\nincorrectly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1\">Ke Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kejriwal_M/0/1/0/all/0/1\">Mayank Kejriwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Dialogue Response Generation Agent for Large Language Models by Asking Questions to Detect User's Intentions. (arXiv:2310.03293v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03293","description":"<p>Large Language Models (LLMs), such as ChatGPT, have recently been applied to\nvarious NLP tasks due to its open-domain generation capabilities. However,\nthere are two issues with applying LLMs to dialogue tasks. 1. During the\ndialogue process, users may have implicit intentions that might be overlooked\nby LLMs. Consequently, generated responses couldn't align with the user's\nintentions. 2. It is unlikely for LLMs to encompass all fields comprehensively.\nIn certain specific domains, their knowledge may be incomplete, and LLMs cannot\nupdate the latest knowledge in real-time. To tackle these issues, we propose a\nframework~\\emph{using LLM to \\textbf{E}nhance dialogue response generation by\nasking questions to \\textbf{D}etect user's \\textbf{I}mplicit\nin\\textbf{T}entions} (\\textbf{EDIT}). Firstly, EDIT generates open questions\nrelated to the dialogue context as the potential user's intention; Then, EDIT\nanswers those questions by interacting with LLMs and searching in\ndomain-specific knowledge bases respectively, and use LLMs to choose the proper\nanswers to questions as extra knowledge; Finally, EDIT enhances response\ngeneration by explicitly integrating those extra knowledge. Besides, previous\nquestion generation works only focus on asking questions with answers in\ncontext. In order to ask open questions, we construct a Context-Open-Question\n(COQ) dataset. On two task-oriented dialogue tasks (Wizard of Wikipedia and\nHoll-E), EDIT outperformed other LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Siwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiangqing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_R/0/1/0/all/0/1\">Rui Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Personalized Story Evaluation. (arXiv:2310.03304v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03304","description":"<p>While large language models (LLMs) have shown impressive results for more\nobjective tasks such as QA and retrieval, it remains nontrivial to evaluate\ntheir performance on open-ended text generation for reasons including (1) data\ncontamination; (2) multi-dimensional evaluation criteria; and (3)\nsubjectiveness stemming from reviewers' personal preferences. To address such\nissues, we propose to model personalization in an uncontaminated open-ended\ngeneration assessment. We create two new datasets Per-MPST and Per-DOC for\npersonalized story evaluation, by re-purposing existing datasets with proper\nanonymization and new personalized labels. We further develop a personalized\nstory evaluation model PERSE to infer reviewer preferences and provide a\npersonalized evaluation. Specifically, given a few exemplary reviews from a\nparticular reviewer, PERSE predicts either a detailed review or fine-grained\ncomparison in several aspects (such as interestingness and surprise) for that\nreviewer on a new text input. Experimental results show that PERSE outperforms\nGPT-4 by 15.8% on Kendall correlation of story ratings, and by 13.7% on\npairwise preference prediction accuracy. Both datasets and code will be\nreleased at https://github.com/dqwang122/PerSE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Danqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hanlin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaomeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1\">Andrew Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning. (arXiv:2310.03309v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03309","description":"<p>Exploiting large language models (LLMs) to tackle deductive reasoning has\ngarnered growing attention. It still remains highly challenging to achieve\nsatisfactory results in complex deductive problems, characterized by plenty of\npremises (i.e., facts or rules) entailing intricate relationships among\nentities and requiring multi-hop reasoning. One intuitive solution is to\ndecompose the original task into smaller sub-tasks, and then chain the multiple\ncasual reasoning steps together in a forward (e.g., Selection-Inference) or\nbackward (e.g., LAMBADA) direction. However, these techniques inevitably\nnecessitate a large number of overall stages, leading to computationally\nexpensive operations and a higher possibility of making misleading steps. In\naddition to stage-by-stage decomposition, we draw inspiration from another\naspect of human problem-solving. Humans tend to distill the most relevant\ninformation and organize their thoughts systematically (e.g., creating mind\nmaps), which assists them in answering questions or drawing conclusions\nprecisely and quickly. In light of this, we propose a novel reasoning approach\nnamed Concise and Organized Perception (COP). COP carefully analyzes the given\nstatements to efficiently identify the most pertinent information while\neliminating redundancy. It then prompts the LLMs in a more organized form that\nadapts to the model's inference process. By perceiving concise and organized\nproofs, the deductive reasoning abilities of LLMs can be better elicited, and\nthe risk of acquiring errors caused by excessive reasoning stages is mitigated.\nFurthermore, our approach can be combined with the aforementioned ones to\nfurther boost their performance. Extensive experimental results on three\npopular deductive benchmarks (i.e., ProofWriter, PrOntoQA and PrOntoQA-OOD)\nshow that COP significantly outperforms previous state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shaotian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chen Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jieping Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reformulating Domain Adaptation of Large Language Models as Adapt-Retrieve-Revise. (arXiv:2310.03328v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03328","description":"<p>While large language models (LLMs) like GPT-4 have recently demonstrated\nastonishing zero-shot capabilities in general domain tasks, they often generate\ncontent with hallucinations in specific domains such as Chinese law, hindering\ntheir application in these areas. This is typically due to the absence of\ntraining data that encompasses such a specific domain, preventing GPT-4 from\nacquiring in-domain knowledge. A pressing challenge is that it's not plausible\nto continue training LLMs of such scale on in-domain data.\n</p>\n<p>This paper introduces a simple and effective domain adaptation framework for\nGPT-4 by reformulating generation as an \\textbf{adapt-retrieve-revise} process.\nThe initial step is to \\textbf{adapt} an affordable 7B LLM to the target domain\nby continuing learning on in-domain data. When solving a task, we leverage the\nadapted LLM to generate a draft answer given a task query. Then, the draft\nanswer will be used to \\textbf{retrieve} supporting evidence candidates from an\nexternal in-domain knowledge base. Finally, the draft answer and retrieved\nevidence are concatenated into a whole prompt to let GPT-4 assess the evidence\nand \\textbf{revise} the draft answer to generate the final answer.\n</p>\n<p>Our proposal combines the advantages of the efficiency of adapting a smaller\n7B model with the evidence-assessing capability of GPT-4 and effectively\nprevents GPT-4 from generating hallucinatory content. In the zero-shot setting\nof four Chinese legal tasks, our method improves accuracy by 33.3\\% compared to\nthe direct generation by GPT-4. When compared to two stronger retrieval-based\nbaselines, our method outperforms them by 15.4\\% and 23.9\\%. Our code will be\nreleased\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+wan_Z/0/1/0/all/0/1\">Zhen wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yating Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yexiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1\">Fei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurohashi_S/0/1/0/all/0/1\">Sadao Kurohashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Hallucinations in Chinese Large Language Models. (arXiv:2310.03368v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03368","description":"<p>In this paper, we establish a benchmark named HalluQA (Chinese Hallucination\nQuestion-Answering) to measure the hallucination phenomenon in Chinese large\nlanguage models. HalluQA contains 450 meticulously designed adversarial\nquestions, spanning multiple domains, and takes into account Chinese historical\nculture, customs, and social phenomena. During the construction of HalluQA, we\nconsider two types of hallucinations: imitative falsehoods and factual errors,\nand we construct adversarial samples based on GLM-130B and ChatGPT. For\nevaluation, we design an automated evaluation method using GPT-4 to judge\nwhether a model output is hallucinated. We conduct extensive experiments on 24\nlarge language models, including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk\nand etc. Out of the 24 models, 18 achieved non-hallucination rates lower than\n50%. This indicates that HalluQA is highly challenging. We analyze the primary\ntypes of hallucinations in different types of models and their causes.\nAdditionally, we discuss which types of hallucinations should be prioritized\nfor different types of models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1\">Qinyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mozhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Mianqiu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhangyue Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Procedural Text Mining with Large Language Models. (arXiv:2310.03376v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03376","description":"<p>Recent advancements in the field of Natural Language Processing, particularly\nthe development of large-scale language models that are pretrained on vast\namounts of knowledge, are creating novel opportunities within the realm of\nKnowledge Engineering. In this paper, we investigate the usage of large\nlanguage models (LLMs) in both zero-shot and in-context learning settings to\ntackle the problem of extracting procedures from unstructured PDF text in an\nincremental question-answering fashion. In particular, we leverage the current\nstate-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model,\naccompanied by two variations of in-context learning that involve an ontology\nwith definitions of procedures and steps and a limited number of samples of\nfew-shot learning. The findings highlight both the promise of this approach and\nthe value of the in-context learning customisations. These modifications have\nthe potential to significantly address the challenge of obtaining sufficient\ntraining data, a hurdle often encountered in deep learning-based Natural\nLanguage Processing techniques for procedure extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rula_A/0/1/0/all/0/1\">Anisa Rula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DSouza_J/0/1/0/all/0/1\">Jennifer D&#x27;Souza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM Based Multi-Document Summarization Exploiting Main-Event Biased Monotone Submodular Content Extraction. (arXiv:2310.03414v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03414","description":"<p>Multi-document summarization is a challenging task due to its inherent\nsubjective bias, highlighted by the low inter-annotator ROUGE-1 score of 0.4\namong DUC-2004 reference summaries. In this work, we aim to enhance the\nobjectivity of news summarization by focusing on the main event of a group of\nrelated news documents and presenting it coherently with sufficient context.\nOur primary objective is to succinctly report the main event, ensuring that the\nsummary remains objective and informative. To achieve this, we employ an\nextract-rewrite approach that incorporates a main-event biased\nmonotone-submodular function for content selection. This enables us to extract\nthe most crucial information related to the main event from the document\ncluster. To ensure coherence, we utilize a fine-tuned Language Model (LLM) for\nrewriting the extracted content into a coherent text. The evaluation using\nobjective metrics and human evaluators confirms the effectiveness of our\napproach, as it surpasses potential baselines, demonstrating excellence in both\ncontent coverage, coherence, and informativeness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurisinkel_L/0/1/0/all/0/1\">Litton J Kurisinkel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Language Model Pruning for Automatic Speech Recognition. (arXiv:2310.03424v1 [cs.LG])","link":"http://arxiv.org/abs/2310.03424","description":"<p>We study model pruning methods applied to Transformer-based neural network\nlanguage models for automatic speech recognition. We explore three aspects of\nthe pruning frame work, namely criterion, method and scheduler, analyzing their\ncontribution in terms of accuracy and inference speed. To the best of our\nknowledge, such in-depth analyses on large-scale recognition systems has not\nbeen reported in the literature. In addition, we propose a variant of low-rank\napproximation suitable for incrementally compressing models, and delivering\nmultiple models with varied target sizes. Among other results, we show that a)\ndata-driven pruning outperforms magnitude-driven in several scenarios; b)\nincremental pruning achieves higher accuracy compared to one-shot pruning,\nespecially when targeting smaller sizes; and c) low-rank approximation presents\nthe best trade-off between size reduction and inference speed-up for moderate\ncompression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Emili_L/0/1/0/all/0/1\">Leonardo Emili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraga_Silva_T/0/1/0/all/0/1\">Thiago Fraga-Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pusateri_E/0/1/0/all/0/1\">Ernest Pusateri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nussbaum_Thom_M/0/1/0/all/0/1\">Markus Nu&#xdf;baum-Thom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oualil_Y/0/1/0/all/0/1\">Youssef Oualil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The North System for Formosa Speech Recognition Challenge 2023. (arXiv:2310.03443v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03443","description":"<p>This report provides a concise overview of the proposed North system, which\naims to achieve automatic word/syllable recognition for Taiwanese Hakka\n(Sixian). The report outlines three key components of the system: the\nacquisition, composition, and utilization of the training data; the\narchitecture of the model; and the hardware specifications and operational\nstatistics. The demonstration of the system can be found at\nhttps://asrvm.iis.sinica.edu.tw/hakka_sixian.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kai-Chen Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Shin Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Multi-document Summarization: Coverage & Coherence Intuitive Policy with Large Language Model Based Rewards. (arXiv:2310.03473v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03473","description":"<p>Memory-efficient large language models are good at refining text input for\nbetter readability. However, controllability is a matter of concern when it\ncomes to text generation tasks with long inputs, such as multi-document\nsummarization. In this work, we investigate for a generic controllable approach\nfor multi-document summarization that leverages the capabilities of LLMs to\nrefine the text. In particular, we train a controllable content extraction\nscheme to extract the text that will be refined by an LLM. The scheme is\ndesigned with a novel coverage and coherence intuitive policy, which is duly\nrewarded by a passively trained LLM. Our approach yields competitive results in\nthe evaluation using ROUGE metrics and outperforms potential baselines in\ncoherence, as per human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurisinkel_L/0/1/0/all/0/1\">Litton J Kurisinkel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+chen_N/0/1/0/all/0/1\">Nancy F chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tik-to-Tok: Translating Language Models One Token at a Time: An Embedding Initialization Strategy for Efficient Language Adaptation. (arXiv:2310.03477v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03477","description":"<p>Training monolingual language models for low and mid-resource languages is\nmade challenging by limited and often inadequate pretraining data. In this\nstudy, we propose a novel model conversion strategy to address this issue,\nadapting high-resources monolingual language models to a new target language.\nBy generalizing over a word translation dictionary encompassing both the source\nand target languages, we map tokens from the target tokenizer to semantically\nsimilar tokens from the source language tokenizer. This one-to-many token\nmapping improves tremendously the initialization of the embedding table for the\ntarget language. We conduct experiments to convert high-resource models to mid-\nand low-resource languages, namely Dutch and Frisian. These converted models\nachieve a new state-of-the-art performance on these languages across all sorts\nof downstream tasks. By reducing significantly the amount of data and time\nrequired for training state-of-the-art models, our novel model conversion\nstrategy has the potential to benefit many languages worldwide.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Remy_F/0/1/0/all/0/1\">Fran&#xe7;ois Remy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delobelle_P/0/1/0/all/0/1\">Pieter Delobelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berendt_B/0/1/0/all/0/1\">Bettina Berendt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demuynck_K/0/1/0/all/0/1\">Kris Demuynck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1\">Thomas Demeester</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust and Generalizable Training: An Empirical Study of Noisy Slot Filling for Input Perturbations. (arXiv:2310.03518v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03518","description":"<p>In real dialogue scenarios, as there are unknown input noises in the\nutterances, existing supervised slot filling models often perform poorly in\npractical applications. Even though there are some studies on noise-robust\nmodels, these works are only evaluated on rule-based synthetic datasets, which\nis limiting, making it difficult to promote the research of noise-robust\nmethods. In this paper, we introduce a noise robustness evaluation dataset\nnamed Noise-SF for slot filling task. The proposed dataset contains five types\nof human-annotated noise, and all those noises are exactly existed in real\nextensive robust-training methods of slot filling into the proposed framework.\nBy conducting exhaustive empirical evaluation experiments on Noise-SF, we find\nthat baseline models have poor performance in robustness evaluation, and the\nproposed framework can effectively improve the robustness of models. Based on\nthe empirical experimental results, we make some forward-looking suggestions to\nfuel the research in this direction. Our dataset Noise-SF will be released at\nhttps://github.com/dongguanting/Noise-SF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guanting Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiaoshuai Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zechen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhengyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Shanglin Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jinzheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Keqing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bo Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiran Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Redefining Digital Health Interfaces with Large Language Models. (arXiv:2310.03560v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03560","description":"<p>Digital health tools have the potential to significantly improve the delivery\nof healthcare services. However, their use remains comparatively limited due,\nin part, to challenges surrounding usability and trust. Recently, Large\nLanguage Models (LLMs) have emerged as general-purpose models with the ability\nto process complex information and produce human-quality text, presenting a\nwealth of potential applications in healthcare. Directly applying LLMs in\nclinical settings is not straightforward, with LLMs susceptible to providing\ninconsistent or nonsensical answers. We demonstrate how LLMs can utilize\nexternal tools to provide a novel interface between clinicians and digital\ntechnologies. This enhances the utility and practical impact of digital\nhealthcare tools and AI models while addressing current issues with using LLM\nin clinical settings such as hallucinations. We illustrate our approach with\nexamples from cardiovascular disease and diabetes risk prediction, highlighting\nthe benefit compared to traditional interfaces for digital tools.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imrie_F/0/1/0/all/0/1\">Fergus Imrie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rauba_P/0/1/0/all/0/1\">Paulius Rauba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1\">Mihaela van der Schaar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLEVRER-Humans: Describing Physical and Causal Events the Human Way. (arXiv:2310.03635v1 [cs.AI])","link":"http://arxiv.org/abs/2310.03635","description":"<p>Building machines that can reason about physical events and their causal\nrelationships is crucial for flexible interaction with the physical world.\nHowever, most existing physical and causal reasoning benchmarks are exclusively\nbased on synthetically generated events and synthetic natural language\ndescriptions of causal relationships. This design brings up two issues. First,\nthere is a lack of diversity in both event types and natural language\ndescriptions; second, causal relationships based on manually-defined heuristics\nare different from human judgments. To address both shortcomings, we present\nthe CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of\nphysical events with human labels. We employ two techniques to improve data\ncollection efficiency: first, a novel iterative event cloze task to elicit a\nnew representation of events in videos, which we term Causal Event Graphs\n(CEGs); second, a data augmentation technique based on neural language\ngenerative models. We convert the collected CEGs into questions and answers to\nbe consistent with prior work. Finally, we study a collection of baseline\napproaches for CLEVRER-Humans question-answering, highlighting the great\nchallenges set forth by our benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jiayuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuelin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xikun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah D. Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Self-Supervised Speech Representations for Indigenous American Languages. (arXiv:2310.03639v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03639","description":"<p>The application of self-supervision to speech representation learning has\ngarnered significant interest in recent years, due to its scalability to large\namounts of unlabeled data. However, much progress, both in terms of\npre-training and downstream evaluation, has remained concentrated in\nmonolingual models that only consider English. Few models consider other\nlanguages, and even fewer consider indigenous ones. In our submission to the\nNew Language Track of the ASRU 2023 ML-SUPERB Challenge, we present an ASR\ncorpus for Quechua, an indigenous South American Language. We benchmark the\nefficacy of large SSL models on Quechua, along with 6 other indigenous\nlanguages such as Guarani and Bribri, on low-resource ASR. Our results show\nsurprisingly strong performance by state-of-the-art SSL models, showing the\npotential generalizability of large-scale models to real-world data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chih-Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">William Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zevallos_R/0/1/0/all/0/1\">Rodolfo Zevallos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_J/0/1/0/all/0/1\">John Ortega</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TRAM: Bridging Trust Regions and Sharpness Aware Minimization. (arXiv:2310.03646v1 [cs.LG])","link":"http://arxiv.org/abs/2310.03646","description":"<p>By reducing the curvature of the loss surface in the parameter space,\nSharpness-aware minimization (SAM) yields widespread robustness improvement\nunder domain transfer. Instead of focusing on parameters, however, this work\nconsiders the transferability of representations as the optimization target for\nout-of-domain generalization in a fine-tuning setup. To encourage the retention\nof transferable representations, we consider trust region-based fine-tuning\nmethods, which exploit task-specific skills without forgetting task-agnostic\nrepresentations from pre-training. We unify parameter- and representation-space\nsmoothing approaches by using trust region bounds to inform SAM-style\nregularizers on both of these optimization surfaces. We propose Trust Region\nAware Minimization (TRAM), a fine-tuning algorithm that optimizes for flat\nminima and smooth, informative representations without forgetting pre-trained\nstructure. We find that TRAM outperforms both sharpness-aware and trust\nregion-based optimization methods on cross-domain language modeling and\ncross-lingual transfer, where robustness to domain transfer and representation\ngenerality are critical for success. TRAM establishes a new standard in\ntraining generalizable models with minimal additional computation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sherborne_T/0/1/0/all/0/1\">Tom Sherborne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1\">Naomi Saphra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasigi_P/0/1/0/all/0/1\">Pradeep Dasigi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MapperGPT: Large Language Models for Linking and Mapping Entities. (arXiv:2310.03666v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03666","description":"<p>Aligning terminological resources, including ontologies, controlled\nvocabularies, taxonomies, and value sets is a critical part of data integration\nin many domains such as healthcare, chemistry, and biomedical research. Entity\nmapping is the process of determining correspondences between entities across\nthese resources, such as gene identifiers, disease concepts, or chemical entity\nidentifiers. Many tools have been developed to compute such mappings based on\ncommon structural features and lexical information such as labels and synonyms.\nLexical approaches in particular often provide very high recall, but low\nprecision, due to lexical ambiguity. As a consequence of this, mapping efforts\noften resort to a labor intensive manual mapping refinement through a human\ncurator.\n</p>\n<p>Large Language Models (LLMs), such as the ones employed by ChatGPT, have\ngeneralizable abilities to perform a wide range of tasks, including\nquestion-answering and information extraction. Here we present MapperGPT, an\napproach that uses LLMs to review and refine mapping relationships as a\npost-processing step, in concert with existing high-recall methods that are\nbased on lexical and structural heuristics.\n</p>\n<p>We evaluated MapperGPT on a series of alignment tasks from different domains,\nincluding anatomy, developmental biology, and renal diseases. We devised a\ncollection of tasks that are designed to be particularly challenging for\nlexical methods. We show that when used in combination with high-recall\nmethods, MapperGPT can provide a substantial improvement in accuracy, beating\nstate-of-the-art (SOTA) methods such as LogMap.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matentzoglu_N/0/1/0/all/0/1\">Nicolas Matentzoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caufield_J/0/1/0/all/0/1\">J. Harry Caufield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_H/0/1/0/all/0/1\">Harshad B. Hegde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reese_J/0/1/0/all/0/1\">Justin T. Reese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moxon_S/0/1/0/all/0/1\">Sierra Moxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyeongsik Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harris_N/0/1/0/all/0/1\">Nomi L. Harris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haendel_M/0/1/0/all/0/1\">Melissa A Haendel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mungall_C/0/1/0/all/0/1\">Christopher J. Mungall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction. (arXiv:2310.03668v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03668","description":"<p>Large Language Models (LLMs) combined with instruction tuning have made\nsignificant progress when generalizing to unseen tasks. However, they have been\nless successful in Information Extraction (IE), lagging behind task-specific\nmodels. Typically, IE tasks are characterized by complex annotation guidelines\nwhich describe the task and give examples to humans. Previous attempts to\nleverage such information have failed, even with the largest models, as they\nare not able to follow the guidelines out-of-the-box. In this paper we propose\nGoLLIE (Guideline-following Large Language Model for IE), a model able to\nimprove zero-shot results on unseen IE tasks by virtue of being fine-tuned to\ncomply with annotation guidelines. Comprehensive evaluation empirically\ndemonstrates that GoLLIE is able to generalize to and follow unseen guidelines,\noutperforming previous attempts at zero-shot information extraction. The\nablation study shows that detailed guidelines is key for good results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sainz_O/0/1/0/all/0/1\">Oscar Sainz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Ferrero_I/0/1/0/all/0/1\">Iker Garc&#xed;a-Ferrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agerri_R/0/1/0/all/0/1\">Rodrigo Agerri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacalle_O/0/1/0/all/0/1\">Oier Lopez de Lacalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigau_G/0/1/0/all/0/1\">German Rigau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1\">Eneko Agirre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers. (arXiv:2310.03686v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03686","description":"<p>In recent years, many interpretability methods have been proposed to help\ninterpret the internal states of Transformer-models, at different levels of\nprecision and complexity. Here, to analyze encoder-decoder Transformers, we\npropose a simple, new method: DecoderLens. Inspired by the LogitLens (for\ndecoder-only Transformers), this method involves allowing the decoder to\ncross-attend representations of intermediate encoder layers instead of using\nthe final encoder output, as is normally done in encoder-decoder models. The\nmethod thus maps previously uninterpretable vector representations to\nhuman-interpretable sequences of words or symbols. We report results from the\nDecoderLens applied to models trained on question answering, logical reasoning,\nspeech recognition and machine translation. The DecoderLens reveals several\nspecific subtasks that are solved at low or intermediate layers, shedding new\nlight on the information flow inside the encoder component of this important\nclass of models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Langedijk_A/0/1/0/all/0/1\">Anna Langedijk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohebbi_H/0/1/0/all/0/1\">Hosein Mohebbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarti_G/0/1/0/all/0/1\">Gabriele Sarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuidema_W/0/1/0/all/0/1\">Willem Zuidema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jumelet_J/0/1/0/all/0/1\">Jaap Jumelet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!. (arXiv:2310.03693v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03693","description":"<p>Optimizing large language models (LLMs) for downstream use cases often\ninvolves the customization of pre-trained LLMs through further fine-tuning.\nMeta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5\nTurbo on custom datasets also encourage this practice. But, what are the safety\ncosts associated with such custom fine-tuning? We note that while existing\nsafety alignment infrastructures can restrict harmful behaviors of LLMs at\ninference time, they do not cover safety risks when fine-tuning privileges are\nextended to end-users. Our red teaming studies find that the safety alignment\nof LLMs can be compromised by fine-tuning with only a few adversarially\ndesigned training examples. For instance, we jailbreak GPT-3.5 Turbo's safety\nguardrails by fine-tuning it on only 10 such examples at a cost of less than\n$0.20 via OpenAI's APIs, making the model responsive to nearly any harmful\ninstructions. Disconcertingly, our research also reveals that, even without\nmalicious intent, simply fine-tuning with benign and commonly used datasets can\nalso inadvertently degrade the safety alignment of LLMs, though to a lesser\nextent. These findings suggest that fine-tuning aligned LLMs introduces new\nsafety risks that current safety infrastructures fall short of addressing --\neven if a model's initial safety alignment is impeccable, it is not necessarily\nto be maintained after custom fine-tuning. We outline and critically analyze\npotential mitigations and advocate for further research efforts toward\nreinforcing safety protocols for the custom fine-tuning of aligned LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiangyu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tinghao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoxi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1\">Prateek Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_P/0/1/0/all/0/1\">Peter Henderson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Agent Instructs Large Language Models to be General Zero-Shot Reasoners. (arXiv:2310.03710v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03710","description":"<p>We introduce a method to improve the zero-shot reasoning abilities of large\nlanguage models on general language understanding tasks. Specifically, we build\nan autonomous agent to instruct the reasoning process of large language models.\nWe show this approach further unleashes the zero-shot reasoning abilities of\nlarge language models to more tasks. We study the performance of our method on\na wide set of datasets spanning generation, classification, and reasoning. We\nshow that our method generalizes to most tasks and obtains state-of-the-art\nzero-shot performance on 20 of the 29 datasets that we evaluate. For instance,\nour method boosts the performance of state-of-the-art large language models by\na large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and\nGPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement\nin reasoning is striking, with an average increase of 10.5%. With our method,\nLlama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Crispino_N/0/1/0/all/0/1\">Nicholas Crispino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montgomery_K/0/1/0/all/0/1\">Kyle Montgomery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_F/0/1/0/all/0/1\">Fankun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenguang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines. (arXiv:2310.03714v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03714","description":"<p>The ML community is rapidly exploring techniques for prompting language\nmodels (LMs) and for stacking them into pipelines that solve complex tasks.\nUnfortunately, existing LM pipelines are typically implemented using hard-coded\n\"prompt templates\", i.e. lengthy strings discovered via trial and error. Toward\na more systematic approach for developing and optimizing LM pipelines, we\nintroduce DSPy, a programming model that abstracts LM pipelines as text\ntransformation graphs, i.e. imperative computational graphs where LMs are\ninvoked through declarative modules. DSPy modules are parameterized, meaning\nthey can learn (by creating and collecting demonstrations) how to apply\ncompositions of prompting, finetuning, augmentation, and reasoning techniques.\nWe design a compiler that will optimize any DSPy pipeline to maximize a given\nmetric. We conduct two case studies, showing that succinct DSPy programs can\nexpress and optimize sophisticated LM pipelines that reason about math word\nproblems, tackle multi-hop retrieval, answer complex questions, and control\nagent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and\nllama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot\nprompting (generally by over 25% and 65%, respectively) and pipelines with\nexpert-created demonstrations (by up to 5-46% and 16-40%, respectively). On top\nof that, DSPy programs compiled to open and relatively small LMs like\n770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely\non expert-written prompt chains for proprietary GPT-3.5. DSPy is available at\nhttps://github.com/stanfordnlp/dspy\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1\">Omar Khattab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhvi_A/0/1/0/all/0/1\">Arnav Singhvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_P/0/1/0/all/0/1\">Paridhi Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santhanam_K/0/1/0/all/0/1\">Keshav Santhanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vardhamanan_S/0/1/0/all/0/1\">Sri Vardhamanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haq_S/0/1/0/all/0/1\">Saiful Haq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Ashutosh Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_T/0/1/0/all/0/1\">Thomas T. Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moazam_H/0/1/0/all/0/1\">Hanna Moazam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_H/0/1/0/all/0/1\">Heather Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Long Way to Go: Investigating Length Correlations in RLHF. (arXiv:2310.03716v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03716","description":"<p>Great successes have been reported using Reinforcement Learning from Human\nFeedback (RLHF) to align large language models. Open-source preference datasets\nand reward models have enabled wider experimentation beyond generic chat\nsettings, particularly to make systems more \"helpful\" for tasks like web\nquestion answering, summarization, and multi-turn dialogue. When optimizing for\nhelpfulness, RLHF has been consistently observed to drive models to produce\nlonger outputs. This paper demonstrates that optimizing for response length is\na significant factor behind RLHF's reported improvements in these settings.\nFirst, we study the relationship between reward and length for reward models\ntrained on three open-source preference datasets for helpfulness. Here, length\ncorrelates strongly with reward, and improvements in reward score are driven in\nlarge part by shifting the distribution over output lengths. We then explore\ninterventions during both RL and reward model learning to see if we can achieve\nthe same downstream improvements as RLHF without increasing length. While our\ninterventions mitigate length increases, they aren't uniformly effective across\nsettings. Furthermore, we find that even running RLHF with a reward based\nsolely on length can reproduce most of the downstream improvements over the\ninitial policy model, showing that reward models in these settings have a long\nway to go.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singhal_P/0/1/0/all/0/1\">Prasann Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_T/0/1/0/all/0/1\">Tanya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiacheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modular Speech-to-Text Translation for Zero-Shot Cross-Modal Transfer. (arXiv:2310.03724v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03724","description":"<p>Recent research has shown that independently trained encoders and decoders,\ncombined through a shared fixed-size representation, can achieve competitive\nperformance in speech-to-text translation. In this work, we show that this type\nof approach can be further improved with multilingual training. We observe\nsignificant improvements in zero-shot cross-modal speech translation, even\noutperforming a supervised approach based on XLSR for several languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duquenne_P/0/1/0/all/0/1\">Paul-Ambroise Duquenne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwenk_H/0/1/0/all/0/1\">Holger Schwenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Beno&#xee;t Sagot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning. (arXiv:2310.03731v1 [cs.CL])","link":"http://arxiv.org/abs/2310.03731","description":"<p>The recently released GPT-4 Code Interpreter has demonstrated remarkable\nproficiency in solving challenging math problems, primarily attributed to its\nability to seamlessly reason with natural language, generate code, execute\ncode, and continue reasoning based on the execution output. In this paper, we\npresent a method to fine-tune open-source language models, enabling them to use\ncode for modeling and deriving math equations and, consequently, enhancing\ntheir mathematical reasoning abilities. We propose a method of generating novel\nand high-quality datasets with math problems and their code-based solutions,\nreferred to as MathCodeInstruct. Each solution interleaves natural language,\ncode, and execution results. We also introduce a customized supervised\nfine-tuning and inference approach. This approach yields the MathCoder models,\na family of models capable of generating code-based solutions for solving\nchallenging math problems. Impressively, the MathCoder models achieve\nstate-of-the-art scores among open-source LLMs on the MATH (45.2%) and GSM8K\n(83.9%) datasets, substantially outperforming other open-source alternatives.\nNotably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K\nand MATH but also outperforms GPT-4 on the competition-level MATH dataset. The\ndataset and models will be released at https://github.com/mathllm/MathCoder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Ke Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Houxing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aojun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zimu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Sichun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weikang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linqi Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_M/0/1/0/all/0/1\">Mingjie Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text as Environment: A Deep Reinforcement Learning Text Readability Assessment Model. (arXiv:1912.05957v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1912.05957","description":"<p>Evaluating the readability of a text can significantly facilitate the precise\nexpression of information in written form. The formulation of text readability\nassessment involves the identification of meaningful properties of the text\nregardless of its length. Sophisticated features and models are used to\nevaluate the comprehensibility of texts accurately. Despite this, the problem\nof assessing texts' readability efficiently remains relatively untouched. The\nefficiency of state-of-the-art text readability assessment models can be\nfurther improved using deep reinforcement learning models. Using a hard\nattention-based active inference technique, the proposed approach makes\nefficient use of input text and computational resources. Through the use of\nsemi-supervised signals, the reinforcement learning model uses the minimum\namount of text in order to determine text's readability. A comparison of the\nmodel on Weebit and Cambridge Exams with state-of-the-art models, such as the\nBERT text readability model, shows that it is capable of achieving\nstate-of-the-art accuracy with a significantly smaller amount of input text\nthan other models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_H/0/1/0/all/0/1\">Hamid Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khasteh_S/0/1/0/all/0/1\">Seyed Hossein Khasteh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Human Sentence Processing with Left-Corner Recurrent Neural Network Grammars. (arXiv:2109.04939v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04939","description":"<p>In computational linguistics, it has been shown that hierarchical structures\nmake language models (LMs) more human-like. However, the previous literature\nhas been agnostic about a parsing strategy of the hierarchical models. In this\npaper, we investigated whether hierarchical structures make LMs more\nhuman-like, and if so, which parsing strategy is most cognitively plausible. In\norder to address this question, we evaluated three LMs against human reading\ntimes in Japanese with head-final left-branching structures: Long Short-Term\nMemory (LSTM) as a sequential model and Recurrent Neural Network Grammars\n(RNNGs) with top-down and left-corner parsing strategies as hierarchical\nmodels. Our computational modeling demonstrated that left-corner RNNGs\noutperformed top-down RNNGs and LSTM, suggesting that hierarchical and\nleft-corner architectures are more cognitively plausible than top-down or\nsequential architectures. In addition, the relationships between the cognitive\nplausibility and (i) perplexity, (ii) parsing, and (iii) beam size will also be\ndiscussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoshida_R/0/1/0/all/0/1\">Ryo Yoshida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noji_H/0/1/0/all/0/1\">Hiroshi Noji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oseki_Y/0/1/0/all/0/1\">Yohei Oseki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training. (arXiv:2110.14883v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.14883","description":"<p>The success of Transformer models has pushed the deep learning model scale to\nbillions of parameters. Due to the limited memory resource of a single GPU,\nHowever, the best practice for choosing the optimal parallel strategy is still\nlacking, since it requires domain expertise in both deep learning and parallel\ncomputing.\n</p>\n<p>The Colossal-AI system addressed the above challenge by introducing a unified\ninterface to scale your sequential code of model training to distributed\nenvironments. It supports parallel training methods such as data, pipeline,\ntensor, and sequence parallelism, as well as heterogeneous training methods\nintegrated with zero redundancy optimizer. Compared to the baseline system,\nColossal-AI can achieve up to 2.76 times training speedup on large-scale\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shenggui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_Z/0/1/0/all/0/1\">Zhengda Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jiarui Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haichen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Diffusion Energy-Based Model for Interpretable Text Modeling. (arXiv:2206.05895v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.05895","description":"<p>Latent space Energy-Based Models (EBMs), also known as energy-based priors,\nhave drawn growing interests in generative modeling. Fueled by its flexibility\nin the formulation and strong modeling power of the latent space, recent works\nbuilt upon it have made interesting attempts aiming at the interpretability of\ntext modeling. However, latent space EBMs also inherit some flaws from EBMs in\ndata space; the degenerate MCMC sampling quality in practice can lead to poor\ngeneration quality and instability in training, especially on data with complex\nlatent structures. Inspired by the recent efforts that leverage diffusion\nrecovery likelihood learning as a cure for the sampling issue, we introduce a\nnovel symbiosis between the diffusion models and latent space EBMs in a\nvariational learning framework, coined as the latent diffusion energy-based\nmodel. We develop a geometric clustering-based regularization jointly with the\ninformation bottleneck to further improve the quality of the learned latent\nspace. Experiments on several challenging tasks demonstrate the superior\nperformance of our model on interpretable text modeling over strong\ncounterparts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Peiyu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sirui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_B/0/1/0/all/0/1\">Baoxiong Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruiqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yixin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-stage LLM Fine-tuning with Less Specialization and More Generalization. (arXiv:2211.00635v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.00635","description":"<p>Pretrained large language models (LLMs) are general purpose problem solvers\napplicable to a diverse set of tasks with prompts. They can be further improved\ntowards a specific task by fine-tuning on a specialized dataset. However,\nfine-tuning usually makes the model narrowly specialized on this dataset with\nreduced general in-context learning performances, which is undesirable whenever\nthe fine-tuned model needs to handle additional tasks where no fine-tuning data\nis available. In this work, we first demonstrate that fine-tuning on a single\ntask indeed decreases LLMs' general in-context learning performance. We\ndiscover one important cause of such forgetting, format specialization, where\nthe model overfits to the format of the fine-tuned task. We further show that\nformat specialization happens at the very beginning of fine-tuning. To solve\nthis problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet\neffective two-stage fine-tuning framework that reduces format specialization\nand improves generalization. ProMoT offloads task-specific format learning into\nadditional and removable parameters by first doing prompt tuning and then\nfine-tuning the model itself with this soft prompt attached. With experiments\non several fine-tuning tasks and 8 in-context evaluation tasks, we show that\nProMoT achieves comparable performance on fine-tuned tasks to standard\nfine-tuning, but with much less loss of in-context learning performances across\na board range of out-of-domain evaluation tasks. More importantly, ProMoT can\neven enhance generalization on in-context learning tasks that are semantically\nrelated to the fine-tuned task, e.g. ProMoT on En-Fr translation significantly\nimproves performance on other language pairs, and ProMoT on NLI improves\nperformance on summarization. Experiments also show that ProMoT can improve the\ngeneralization performance of multi-task training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1\">Si Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Daliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasik_M/0/1/0/all/0/1\">Michal Lukasik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Felix Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1\">Inderjit S Dhillon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-scale investigation of weakly-supervised deep learning for the fine-grained semantic indexing of biomedical literature. (arXiv:2301.09350v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.09350","description":"<p>Objective: Semantic indexing of biomedical literature is usually done at the\nlevel of MeSH descriptors with several related but distinct biomedical concepts\noften grouped together and treated as a single topic. This study proposes a new\nmethod for the automated refinement of subject annotations at the level of MeSH\nconcepts. Methods: Lacking labelled data, we rely on weak supervision based on\nconcept occurrence in the abstract of an article, which is also enhanced by\ndictionary-based heuristics. In addition, we investigate deep learning\napproaches, making design choices to tackle the particular challenges of this\ntask. The new method is evaluated on a large-scale retrospective scenario,\nbased on concepts that have been promoted to descriptors. Results: In our\nexperiments concept occurrence was the strongest heuristic achieving a macro-F1\nscore of about 0.63 across several labels. The proposed method improved it\nfurther by more than 4pp. Conclusion: The results suggest that concept\noccurrence is a strong heuristic for refining the coarse-grained labels at the\nlevel of MeSH concepts and the proposed method improves it further.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nentidis_A/0/1/0/all/0/1\">Anastasios Nentidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzopoulos_T/0/1/0/all/0/1\">Thomas Chatzopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krithara_A/0/1/0/all/0/1\">Anastasia Krithara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1\">Grigorios Tsoumakas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paliouras_G/0/1/0/all/0/1\">Georgios Paliouras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Inferential Reproducibility of Machine Learning Research. (arXiv:2302.04054v6 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.04054","description":"<p>Reliability of machine learning evaluation -- the consistency of observed\nevaluation scores across replicated model training runs -- is affected by\nseveral sources of nondeterminism which can be regarded as measurement noise.\nCurrent tendencies to remove noise in order to enforce reproducibility of\nresearch results neglect inherent nondeterminism at the implementation level\nand disregard crucial interaction effects between algorithmic noise factors and\ndata properties. This limits the scope of conclusions that can be drawn from\nsuch experiments. Instead of removing noise, we propose to incorporate several\nsources of variance, including their interaction with data properties, into an\nanalysis of significance and reliability of machine learning evaluation, with\nthe aim to draw inferences beyond particular instances of trained models. We\nshow how to use linear mixed effects models (LMEMs) to analyze performance\nevaluation scores, and to conduct statistical inference with a generalized\nlikelihood ratio test (GLRT). This allows us to incorporate arbitrary sources\nof noise like meta-parameter variations into statistical significance testing,\nand to assess performance differences conditional on data properties.\nFurthermore, a variance component analysis (VCA) enables the analysis of the\ncontribution of noise sources to overall variance and the computation of a\nreliability coefficient by the ratio of substantial to total variance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagmann_M/0/1/0/all/0/1\">Michael Hagmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meier_P/0/1/0/all/0/1\">Philipp Meier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1\">Stefan Riezler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GOAL: A Challenging Knowledge-grounded Video Captioning Benchmark for Real-time Soccer Commentary Generation. (arXiv:2303.14655v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2303.14655","description":"<p>Despite the recent emergence of video captioning models, how to generate\nvivid, fine-grained video descriptions based on the background knowledge (i.e.,\nlong and informative commentary about the domain-specific scenes with\nappropriate reasoning) is still far from being solved, which however has great\napplications such as automatic sports narrative. In this paper, we present\nGOAL, a benchmark of over 8.9k soccer video clips, 22k sentences, and 42k\nknowledge triples for proposing a challenging new task setting as\nKnowledge-grounded Video Captioning (KGVC). Moreover, we conduct experimental\nadaption of existing methods to show the difficulty and potential directions\nfor solving this valuable and applicable task. Our data and code are available\nat https://github.com/THU-KEG/goal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Ji Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jifan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_T/0/1/0/all/0/1\">Teng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1\">Kunyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yifan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_X/0/1/0/all/0/1\">Xinyu Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Weidong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yu Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teaching Large Language Models to Self-Debug. (arXiv:2304.05128v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.05128","description":"<p>Large language models (LLMs) have achieved impressive performance on code\ngeneration. However, for complex programming tasks, generating the correct\nsolution in one go becomes challenging, thus some prior works have designed\nprogram repair approaches to improve code generation performance. In this work,\nwe propose Self-Debugging, which teaches a large language model to debug its\npredicted program via few-shot demonstrations. In particular, we demonstrate\nthat Self-Debugging can teach the large language model to perform rubber duck\ndebugging; i.e., without any human feedback on the code correctness or error\nmessages, the model is able to identify its mistakes by investigating the\nexecution results and explaining the generated code in natural language.\nSelf-Debugging achieves the state-of-the-art performance on several code\ngeneration benchmarks, including the Spider dataset for text-to-SQL generation,\nTransCoder for C++-to-Python translation, and MBPP for text-to-Python\ngeneration. On the Spider benchmark where there are no unit tests to verify the\ncorrectness of predictions, Self-Debugging with code explanation consistently\nimproves the baseline by 2-3%, and improves the prediction accuracy on problems\nof the hardest level by 9%. On TransCoder and MBPP where unit tests are\navailable, Self-Debugging improves the baseline accuracy by up to 12%.\nMeanwhile, by leveraging feedback messages and reusing failed predictions,\nSelf-Debugging notably improves sample efficiency, and can match or outperform\nbaseline models that generate more than 10x candidate programs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Maxwell Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scharli_N/0/1/0/all/0/1\">Nathanael Sch&#xe4;rli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MedAlpaca -- An Open-Source Collection of Medical Conversational AI Models and Training Data. (arXiv:2304.08247v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.08247","description":"<p>As large language models (LLMs) like OpenAI's GPT series continue to make\nstrides, we witness the emergence of artificial intelligence applications in an\never-expanding range of fields. In medicine, these LLMs hold considerable\npromise for improving medical workflows, diagnostics, patient care, and\neducation. Yet, there is an urgent need for open-source models that can be\ndeployed on-premises to safeguard patient privacy. In our work, we present an\ninnovative dataset consisting of over 160,000 entries, specifically crafted to\nfine-tune LLMs for effective medical applications. We investigate the impact of\nfine-tuning these datasets on publicly accessible pre-trained LLMs, and\nsubsequently, we juxtapose the performance of pre-trained-only models against\nthe fine-tuned models concerning the examinations that future medical doctors\nmust pass to achieve certification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tianyu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_L/0/1/0/all/0/1\">Lisa C. Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papaioannou_J/0/1/0/all/0/1\">Jens-Michalis Papaioannou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grundmann_P/0/1/0/all/0/1\">Paul Grundmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberhauser_T/0/1/0/all/0/1\">Tom Oberhauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loser_A/0/1/0/all/0/1\">Alexander L&#xf6;ser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truhn_D/0/1/0/all/0/1\">Daniel Truhn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bressem_K/0/1/0/all/0/1\">Keno K. Bressem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Emergent In-Context Learning as Kernel Regression. (arXiv:2305.12766v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.12766","description":"<p>Large language models (LLMs) have initiated a paradigm shift in transfer\nlearning. In contrast to the classic pretraining-then-finetuning procedure, in\norder to use LLMs for downstream prediction tasks, one only needs to provide a\nfew demonstrations, known as in-context examples, without adding more or\nupdating existing model parameters. This in-context learning (ICL) capability\nof LLMs is intriguing, and it is not yet fully understood how pretrained LLMs\nacquire such capabilities. In this paper, we investigate the reason why a\ntransformer-based language model can accomplish in-context learning after\npre-training on a general language corpus by proposing one hypothesis that LLMs\ncan simulate kernel regression with internal representations when faced with\nin-context examples. More concretely, we first prove that Bayesian inference on\nin-context prompts can be asymptotically understood as kernel regression $\\hat\ny = \\sum_i y_i K(x, x_i)/\\sum_i K(x, x_i)$ as the number of in-context\ndemonstrations grows. Then, we empirically investigate the in-context behaviors\nof language models. We find that during ICL, the attention and hidden features\nin LLMs match the behaviors of a kernel regression. Finally, our theory\nprovides insights into multiple phenomena observed in the ICL field: why\nretrieving demonstrative samples similar to test samples can help, why ICL\nperformance is sensitive to the output formats, and why ICL accuracy benefits\nfrom selecting in-distribution and representative samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Han Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physics of Language Models: Part 1, Context-Free Grammar. (arXiv:2305.13673v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13673","description":"<p>We design controlled experiments to study HOW generative language models,\nlike GPT, learn context-free grammars (CFGs) -- diverse language systems with a\ntree-like structure capturing many aspects of natural languages, programs, and\nlogics. CFGs are as hard as pushdown automata, and can be ambiguous so that\nverifying if a string satisfies the rules requires dynamic programming. We\nconstruct synthetic data and demonstrate that even for difficult (long and\nambiguous) CFGs, pre-trained transformers can learn to generate sentences with\nnear-perfect accuracy and impressive diversity.\n</p>\n<p>More importantly, we delve into the physical principles behind how\ntransformers learns CFGs. We discover that the hidden states within the\ntransformer implicitly and precisely encode the CFG structure (such as putting\ntree node information exactly on the subtree boundary), and learn to form\n\"boundary to boundary\" attentions resembling dynamic programming. We also cover\nsome extension of CFGs as well as the robustness aspect of transformers against\ngrammar mistakes. Overall, our research provides a comprehensive and empirical\nunderstanding of how transformers learn CFGs, and reveals the physical\nmechanisms utilized by transformers to capture the structure and rules of\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1\">Zeyuan Allen-Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BA-SOT: Boundary-Aware Serialized Output Training for Multi-Talker ASR. (arXiv:2305.13716v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2305.13716","description":"<p>The recently proposed serialized output training (SOT) simplifies\nmulti-talker automatic speech recognition (ASR) by generating speaker\ntranscriptions separated by a special token. However, frequent speaker changes\ncan make speaker change prediction difficult. To address this, we propose\nboundary-aware serialized output training (BA-SOT), which explicitly\nincorporates boundary knowledge into the decoder via a speaker change detection\ntask and boundary constraint loss. We also introduce a two-stage connectionist\ntemporal classification (CTC) strategy that incorporates token-level SOT CTC to\nrestore temporal context information. Besides typical character error rate\n(CER), we introduce utterance-dependent character error rate (UD-CER) to\nfurther measure the precision of speaker change prediction. Compared to\noriginal SOT, BA-SOT reduces CER/UD-CER by 5.1%/14.0%, and leveraging a\npre-trained ASR model for BA-SOT model initialization further reduces\nCER/UD-CER by 8.4%/19.9%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuhao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Pengcheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Annotation Imputation to Individualize Predictions: Initial Studies on Distribution Dynamics and Model Predictions. (arXiv:2305.15070v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.15070","description":"<p>Annotating data via crowdsourcing is time-consuming and expensive. Due to\nthese costs, dataset creators often have each annotator label only a small\nsubset of the data. This leads to sparse datasets with examples that are marked\nby few annotators. The downside of this process is that if an annotator doesn't\nget to label a particular example, their perspective on it is missed. This is\nespecially concerning for subjective NLP datasets where there is no single\ncorrect label: people may have different valid opinions. Thus, we propose using\nimputation methods to generate the opinions of all annotators for all examples,\ncreating a dataset that does not leave out any annotator's view. We then train\nand prompt models, using data from the imputed dataset, to make predictions\nabout the distribution of responses and individual annotations.\n</p>\n<p>In our analysis of the results, we found that the choice of imputation method\nsignificantly impacts soft label changes and distribution. While the imputation\nintroduces noise in the prediction of the original dataset, it has shown\npotential in enhancing shots for prompts, particularly for low-response-rate\nannotators. We have made all of our code and data publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lowmanstone_L/0/1/0/all/0/1\">London Lowmanstone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_R/0/1/0/all/0/1\">Ruyuan Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owan_R/0/1/0/all/0/1\">Risako Owan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jaehyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers. (arXiv:2305.17455v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.17455","description":"<p>Recent vision-language models have achieved tremendous progress far beyond\nwhat we ever expected. However, their computational costs are also dramatically\ngrowing with rapid development, especially for the large models. It makes model\nacceleration exceedingly critical in a scenario of limited resources. Although\nextensively studied for unimodal models, the acceleration for multimodal\nmodels, especially the vision-language Transformers, is relatively\nunder-explored. To pursue more efficient and accessible vision-language\nTransformers, this paper introduces \\textbf{Cross}-\\textbf{G}uided\n\\textbf{E}nsemble of \\textbf{T}okens (\\textbf{\\emph{CrossGET}}), a universal\nacceleration framework for vision-language Transformers. This framework\nadaptively combines tokens through real-time, cross-modal guidance, thereby\nachieving substantial acceleration while keeping high performance.\n\\textit{CrossGET} has two key innovations: 1) \\textit{Cross-Guided Matching and\nEnsemble}. \\textit{CrossGET} incorporates cross-modal guided token matching and\nensemble to exploit cross-modal information effectively, only introducing\ncross-modal tokens with negligible extra parameters. 2) \\textit{Complete-Graph\nSoft Matching}. In contrast to the existing bipartite soft matching approach,\n\\textit{CrossGET} introduces a complete-graph soft matching policy to achieve\nmore reliable token-matching results while maintaining parallelizability and\nhigh efficiency. Extensive experiments are conducted on various vision-language\ntasks, including image-text retrieval, visual reasoning, image captioning, and\nvisual question answering. Performance on both classic multimodal architectures\nand emerging multimodal LLMs demonstrate the effectiveness and versatility of\nthe proposed \\textit{CrossGET} framework. The code will be at\n\\url{https://github.com/sdc17/CrossGET}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Dachuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chaofan Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Anyi Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhendong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting the Role of Language Priors in Vision-Language Models. (arXiv:2306.01879v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2306.01879","description":"<p>Vision-language models (VLMs) are impactful in part because they can be\napplied to a variety of visual understanding tasks in a zero-shot fashion,\nwithout any fine-tuning. We study $\\textit{generative VLMs}$ that are trained\nfor next-word generation given an image. We explore their zero-shot performance\non the illustrative task of image-text retrieval across 8 popular\nvision-language benchmarks. Our first observation is that they can be\nrepurposed for discriminative tasks (such as image-text retrieval) by simply\ncomputing the match score of generating a particular text string given an\nimage. We call this probabilistic score the $\\textit{Visual Generative\nPre-Training Score}$ (VisualGPTScore). While the VisualGPTScore produces\nnear-perfect accuracy on some retrieval benchmarks, it yields poor accuracy on\nothers. We analyze this behavior through a probabilistic lens, pointing out\nthat some benchmarks inadvertently capture unnatural language distributions by\ncreating adversarial but unlikely text captions. In fact, we demonstrate that\neven a \"blind\" language model that ignores any image evidence can sometimes\noutperform all prior art, reminiscent of similar challenges faced by the\nvisual-question answering (VQA) community many years ago. We derive a\nprobabilistic post-processing scheme that controls for the amount of linguistic\nbias in generative VLMs at test time without having to retrain or fine-tune the\nmodel. We show that the VisualGPTScore, when appropriately debiased, is a\nstrong zero-shot baseline for vision-language understanding, oftentimes\nproducing state-of-the-art accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhiqiu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyue Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SqueezeLLM: Dense-and-Sparse Quantization. (arXiv:2306.07629v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.07629","description":"<p>Generative Large Language Models (LLMs) have demonstrated remarkable results\nfor a wide range of tasks. However, deploying these models for inference has\nbeen a significant challenge due to their unprecedented resource requirements.\nThis has forced existing deployment frameworks to use multi-GPU inference\npipelines, which are often complex and costly, or to use smaller and less\nperformant models. In this work, we demonstrate that the main bottleneck for\ngenerative inference with LLMs is memory bandwidth, rather than compute,\nspecifically for single batch inference. While quantization has emerged as a\npromising solution by representing model weights with reduced precision,\nprevious efforts have often resulted in notable performance degradation. To\naddress this, we introduce SqueezeLLM, a post-training quantization framework\nthat not only enables lossless compression to ultra-low precisions of up to\n3-bit, but also achieves higher quantization performance under the same memory\nconstraint. Our framework incorporates two novel ideas: (i) sensitivity-based\nnon-uniform quantization, which searches for the optimal bit precision\nassignment based on second-order information; and (ii) the Dense-and-Sparse\ndecomposition that stores outliers and sensitive weight values in an efficient\nsparse format. When applied to the LLaMA models, our 3-bit quantization\nsignificantly reduces the perplexity gap from the FP16 baseline by up to 2.1x\nas compared to the state-of-the-art methods with the same memory requirement.\nFurthermore, when deployed on an A6000 GPU, our quantized models achieve up to\n2.3x speedup compared to the baseline. Our code is open-sourced and available\nonline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooper_C/0/1/0/all/0/1\">Coleman Hooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1\">Amir Gholami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiuyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Overview of Large Language Models. (arXiv:2307.06435v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.06435","description":"<p>Large Language Models (LLMs) have recently demonstrated remarkable\ncapabilities in natural language processing tasks and beyond. This success of\nLLMs has led to a large influx of research contributions in this direction.\nThese works encompass diverse topics such as architectural innovations of the\nunderlying neural networks, context length improvements, model alignment,\ntraining datasets, benchmarking, efficiency and more. With the rapid\ndevelopment of techniques and regular breakthroughs in LLM research, it has\nbecome considerably challenging to perceive the bigger picture of the advances\nin this direction. Considering the rapidly emerging plethora of literature on\nLLMs, it is imperative that the research community is able to benefit from a\nconcise yet comprehensive overview of the recent developments in this field.\nThis article provides that overview to the research community. It not only\nfocuses on a systematic treatment of the existing literature on a broad range\nof LLM related concept, but also pays special attention to providing\ncomprehensive summaries with extensive details about the individual existing\nmodels, datasets and major insights. We also pay heed to aligning our overview\nwith the emerging outlook of this research direction by accounting for the\nother recently materializing reviews of the broader research direction of LLMs.\nOur self-contained comprehensive overview of LLMs discusses relevant background\nconcepts along with covering the advanced topics at the frontier of this\nresearch direction. This review article is intended to not only provide a\nsystematic survey, but also a quick comprehensive reference for the researchers\nand practitioners to draw insights from extensive informative summaries of the\nexisting works to advance the LLM research direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naveed_H/0/1/0/all/0/1\">Humza Naveed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Asad Ullah Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Shi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saqib_M/0/1/0/all/0/1\">Muhammad Saqib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usman_M/0/1/0/all/0/1\">Muhammad Usman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1\">Naveed Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. (arXiv:2308.00436v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2308.00436","description":"<p>The recent progress in large language models (LLMs), especially the invention\nof chain-of-thought prompting, has made it possible to automatically answer\nquestions by stepwise reasoning. However, when faced with more complicated\nproblems that require non-linear thinking, even the strongest LLMs make\nmistakes. To address this, we explore whether LLMs are able to recognize errors\nin their own step-by-step reasoning, without resorting to external resources.\nTo this end, we propose SelfCheck, a general-purpose zero-shot verification\nschema for recognizing such errors. We then use the results of these checks to\nimprove question-answering performance by conducting weighted voting on\nmultiple solutions to the question. We test SelfCheck on three datasets (GSM8K,\nMathQA, and MATH) and find that it successfully recognizes errors and, in turn,\nincreases final answer accuracies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miao_N/0/1/0/all/0/1\">Ning Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rainforth_T/0/1/0/all/0/1\">Tom Rainforth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging Emotion Role Labeling and Appraisal-based Emotion Analysis. (arXiv:2309.02092v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.02092","description":"<p>The term emotion analysis in text subsumes various natural language\nprocessing tasks which have in common the goal to enable computers to\nunderstand emotions. Most popular is emotion classification in which one or\nmultiple emotions are assigned to a predefined textual unit. While such setting\nis appropriate to identify the reader's or author's emotion, emotion role\nlabeling adds the perspective of mentioned entities and extracts text spans\nthat correspond to the emotion cause. The underlying emotion theories agree on\none important point; that an emotion is caused by some internal or external\nevent and comprises several subcomponents, including the subjective feeling and\na cognitive evaluation. We therefore argue that emotions and events are related\nin two ways. (1) Emotions are events; and this perspective is the fundament in\nNLP for emotion role labeling. (2) Emotions are caused by events; a perspective\nthat is made explicit with research how to incorporate psychological appraisal\ntheories in NLP models to interpret events. These two research directions, role\nlabeling and (event-focused) emotion classification, have by and large been\ntackled separately. We contributed to both directions with the projects SEAT\n(Structured Multi-Domain Emotion Analysis from Text) and CEAT (Computational\nEvent Evaluation based on Appraisal Theories for Emotion Analysis), both funded\nby the German Research Foundation. In this paper, we consolidate the findings\nand discuss open research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Evaluating Framework for Natural Language Understanding in AI Systems: Language Acquisition as a Core for Future Metrics. (arXiv:2309.11981v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.11981","description":"<p>In the burgeoning field of artificial intelligence (AI), the unprecedented\nprogress of large language models (LLMs) in natural language processing (NLP)\noffers an opportunity to revisit the entire approach of traditional metrics of\nmachine intelligence, both in form and content. As the realm of machine\ncognitive evaluation has already reached Imitation, the next step is an\nefficient Language Acquisition and Understanding. Our paper proposes a paradigm\nshift from the established Turing Test towards an all-embracing framework that\nhinges on language acquisition, taking inspiration from the recent advancements\nin LLMs. The present contribution is deeply tributary of the excellent work\nfrom various disciplines, point out the need to keep interdisciplinary bridges\nopen, and delineates a more robust and sustainable approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vera_P/0/1/0/all/0/1\">Patricio Vera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moya_P/0/1/0/all/0/1\">Pedro Moya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barraza_L/0/1/0/all/0/1\">Lisa Barraza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AnglE-optimized Text Embeddings. (arXiv:2309.12871v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.12871","description":"<p>High-quality text embedding is pivotal in improving semantic textual\nsimilarity (STS) tasks, which are crucial components in Large Language Model\n(LLM) applications. However, a common challenge existing text embedding models\nface is the problem of vanishing gradients, primarily due to their reliance on\nthe cosine function in the optimization objective, which has saturation zones.\nTo address this issue, this paper proposes a novel angle-optimized text\nembedding model called AnglE. The core idea of AnglE is to introduce angle\noptimization in a complex space. This novel approach effectively mitigates the\nadverse effects of the saturation zone in the cosine function, which can impede\ngradient and hinder optimization processes. To set up a comprehensive STS\nevaluation, we experimented on existing short-text STS datasets and a newly\ncollected long-text STS dataset from GitHub Issues. Furthermore, we examine\ndomain-specific STS scenarios with limited labeled data and explore how AnglE\nworks with LLM-annotated data. Extensive experiments were conducted on various\ntasks including short-text STS, long-text STS, and domain-specific STS tasks.\nThe results show that AnglE outperforms the state-of-the-art (SOTA) STS models\nthat ignore the cosine saturation zone. These findings demonstrate the ability\nof AnglE to generate high-quality text embeddings and the usefulness of angle\noptimization in STS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLPBench: Evaluating Large Language Models on Solving NLP Problems. (arXiv:2309.15630v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.15630","description":"<p>Recent developments in large language models (LLMs) have shown promise in\nenhancing the capabilities of natural language processing (NLP). Despite these\nsuccesses, there remains a dearth of research dedicated to the NLP\nproblem-solving abilities of LLMs. To fill the gap in this area, we present a\nunique benchmarking dataset, NLPBench, comprising 378 college-level NLP\nquestions spanning various NLP topics sourced from Yale University's prior\nfinal exams. NLPBench includes questions with context, in which multiple\nsub-questions share the same public information, and diverse question types,\nincluding multiple choice, short answer, and math. Our evaluation, centered on\nLLMs such as GPT-3.5/4, PaLM-2, and LLAMA-2, incorporates advanced prompting\nstrategies like the chain-of-thought (CoT) and tree-of-thought (ToT). Our study\nreveals that the effectiveness of the advanced prompting strategies can be\ninconsistent, occasionally damaging LLM performance, especially in smaller\nmodels like the LLAMA-2 (13b). Furthermore, our manual assessment illuminated\nspecific shortcomings in LLMs' scientific problem-solving skills, with\nweaknesses in logical decomposition and reasoning notably affecting results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linxin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jieyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lechao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pengyuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Large Language Models for Qualitative Analysis can Introduce Serious Bias. (arXiv:2309.17147v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.17147","description":"<p>Large Language Models (LLMs) are quickly becoming ubiquitous, but the\nimplications for social science research are not yet well understood. This\npaper asks whether LLMs can help us analyse large-N qualitative data from\nopen-ended interviews, with an application to transcripts of interviews with\nRohingya refugees in Cox's Bazaar, Bangladesh. We find that a great deal of\ncaution is needed in using LLMs to annotate text as there is a risk of\nintroducing biases that can lead to misleading inferences. We here mean bias in\nthe technical sense, that the errors that LLMs make in annotating interview\ntranscripts are not random with respect to the characteristics of the interview\nsubjects. Training simpler supervised models on high-quality human annotations\nwith flexible coding leads to less measurement error and bias than LLM\nannotations. Therefore, given that some high quality annotations are necessary\nin order to asses whether an LLM introduces bias, we argue that it is probably\npreferable to train a bespoke model on these annotations than it is to use an\nLLM for annotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ashwin_J/0/1/0/all/0/1\">Julian Ashwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhabra_A/0/1/0/all/0/1\">Aditya Chhabra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_V/0/1/0/all/0/1\">Vijayendra Rao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DyVal: Graph-informed Dynamic Evaluation of Large Language Models. (arXiv:2309.17167v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2309.17167","description":"<p>Large language models (LLMs) have achieved remarkable performance in various\nevaluation benchmarks. However, concerns about their performance are raised on\npotential data contamination in their considerable volume of training corpus.\nMoreover, the static nature and fixed complexity of current benchmarks may\ninadequately gauge the advancing capabilities of LLMs. In this paper, we\nintroduce DyVal, a novel, general, and flexible evaluation protocol for dynamic\nevaluation of LLMs. Based on our proposed dynamic evaluation framework, we\nbuild graph-informed DyVal by leveraging the structural advantage of directed\nacyclic graphs to dynamically generate evaluation samples with controllable\ncomplexities. DyVal generates challenging evaluation sets on reasoning tasks\nincluding mathematics, logical reasoning, and algorithm problems. We evaluate\nvarious LLMs ranging from Flan-T5-large to ChatGPT and GPT4. Experiments\ndemonstrate that LLMs perform worse in DyVal-generated evaluation samples with\ndifferent complexities, emphasizing the significance of dynamic evaluation. We\nalso analyze the failure cases and results of different prompting methods.\nMoreover, DyVal-generated samples are not only evaluation sets, but also\nhelpful data for fine-tuning to improve the performance of LLMs on existing\nbenchmarks. We hope that DyVal can shed light on the future evaluation research\nof LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kaijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1\">Neil Zhenqiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TADIS: Steering Models for Deep-Thinking about Demonstration Examples. (arXiv:2310.00901v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.00901","description":"<p>Instruction tuning has been demonstrated that could significantly improve the\nzero-shot generalization capability to unseen tasks by an apparent margin. By\nincorporating additional context (e.g., task definition, examples) during the\nfine-tuning process, Large Language Models (LLMs) achieved much higher\nperformance than before. However, recent work reported that delusive task\nexamples can achieve almost the same performance as correct task examples,\nindicating the input-label correspondence is less important than previously\nthought. Intrigued by this counter-intuitive observation, we suspect models\nhave the same illusion of competence as humans. Therefore, we propose a novel\nmethod called TADIS that steers LLMs for \"Deep-Thinking'' about demonstration\nexamples instead of merely seeing. To alleviate the illusion of competence of\nmodels, we first ask the model to verify the correctness of shown examples.\nThen, using the verification results as conditions to elicit models for a\nbetter answer. Our experimental results show that TADIS consistently\noutperforms competitive baselines on in-domain and out-domain tasks (improving\n2.79 and 4.03 average ROUGLE-L on out-domain and in-domain datasets,\nrespectively). Despite the presence of generated examples (not all of the\nthinking labels are accurate), TADIS can notably enhance performance in\nzero-shot and few-shot settings. This also suggests that our approach can be\nadopted on a large scale to improve the instruction following capabilities of\nmodels without any manual labor. Moreover, we construct three types of thinking\nlabels with different model sizes and find that small models learn from the\nformat of TADIS but larger models can be steered for \"Deep-Thinking''.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1\">Tianci Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanhua Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Borges and AI. (arXiv:2310.01425v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.01425","description":"<p>Many believe that Large Language Models (LLMs) open the era of Artificial\nIntelligence (AI). Some see opportunities while others see dangers. Yet both\nproponents and opponents grasp AI through the imagery popularised by science\nfiction. Will the machine become sentient and rebel against its creators? Will\nwe experience a paperclip apocalypse? Before answering such questions, we\nshould first ask whether this mental imagery provides a good description of the\nphenomenon at hand. Understanding weather patterns through the moods of the\ngods only goes so far. The present paper instead advocates understanding LLMs\nand their connection to AI through the imagery of Jorge Luis Borges, a master\nof 20th century literature, forerunner of magical realism, and precursor to\npostmodern literature. This exercise leads to a new perspective that\nilluminates the relation between language modelling and artificial\nintelligence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bottou_L/0/1/0/all/0/1\">L&#xe9;on Bottou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ring Attention with Blockwise Transformers for Near-Infinite Context. (arXiv:2310.01889v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.01889","description":"<p>Transformers have emerged as the architecture of choice for many\nstate-of-the-art AI models, showcasing exceptional performance across a wide\nrange of AI applications. However, the memory demands imposed by Transformers\nlimit their ability to handle long sequences, thereby creating challenges for\ntasks involving extended sequences or long-term dependencies. We present a\ndistinct approach, Ring Attention, which leverages blockwise computation of\nself-attention to distribute long sequences across multiple devices while\nconcurrently overlapping the communication of key-value blocks with the\ncomputation of blockwise attention. By processing longer input sequences while\nmaintaining memory efficiency, Ring Attention enables training and inference of\nsequences that are device count times longer than those of prior\nmemory-efficient Transformers, effectively eliminating the memory constraints\nimposed by individual devices. Extensive experiments on language modeling tasks\ndemonstrate the effectiveness of Ring Attention in allowing large sequence\ninput size and improving performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the definition of toxicity in NLP. (arXiv:2310.02357v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.02357","description":"<p>The fundamental problem in toxicity detection task lies in the fact that the\ntoxicity is ill-defined. This causes us to rely on subjective and vague data in\nmodels' training, which results in non-robust and non-accurate results: garbage\nin - garbage out.\n</p>\n<p>This work suggests a new, stress-level-based definition of toxicity designed\nto be objective and context-aware. On par with it, we also describe possible\nways of applying this new definition to dataset creation and model training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berezin_S/0/1/0/all/0/1\">Sergey Berezin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farahbakhsh_R/0/1/0/all/0/1\">Reza Farahbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crespi_N/0/1/0/all/0/1\">Noel Crespi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LC-Score: Reference-less estimation of Text Comprehension Difficulty. (arXiv:2310.02754v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.02754","description":"<p>Being able to read and understand written text is critical in a digital era.\nHowever, studies shows that a large fraction of the population experiences\ncomprehension issues. In this context, further initiatives in accessibility are\nrequired to improve the audience text comprehension. However, writers are\nhardly assisted nor encouraged to produce easy-to-understand content. Moreover,\nAutomatic Text Simplification (ATS) model development suffers from the lack of\nmetric to accurately estimate comprehension difficulty We present\n\\textsc{LC-Score}, a simple approach for training text comprehension metric for\nany French text without reference \\ie predicting how easy to understand a given\ntext is on a $[0, 100]$ scale. Our objective with this scale is to\nquantitatively capture the extend to which a text suits to the \\textit{Langage\nClair} (LC, \\textit{Clear Language}) guidelines, a French initiative closely\nrelated to English Plain Language. We explore two approaches: (i) using\nlinguistically motivated indicators used to train statistical models, and (ii)\nneural learning directly from text leveraging pre-trained language models. We\nintroduce a simple proxy task for comprehension difficulty training as a\nclassification task. To evaluate our models, we run two distinct human\nannotation experiments, and find that both approaches (indicator based and\nneural) outperforms commonly used readability and comprehension metrics such as\nFKGL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tardy_P/0/1/0/all/0/1\">Paul Tardy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roze_C/0/1/0/all/0/1\">Charlotte Roze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poupet_P/0/1/0/all/0/1\">Paul Poupet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning. (arXiv:2310.02954v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.02954","description":"<p>Recent advances in natural language processing, primarily propelled by Large\nLanguage Models (LLMs), have showcased their remarkable capabilities grounded\nin in-context learning. A promising avenue for guiding LLMs in intricate\nreasoning tasks involves the utilization of intermediate reasoning steps within\nthe Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge lies\nin the effective selection of exemplars for facilitating in-context learning.\nIn this study, we introduce a framework that leverages Dual Queries and\nLow-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplars\nfor in-context learning. Dual Queries first query LLM to obtain LLM-generated\nknowledge such as CoT, then query the retriever to obtain the final exemplars\nvia both question and the knowledge. Moreover, for the second query, LoRe\nemploys dimensionality reduction techniques to refine exemplar selection,\nensuring close alignment with the input question's knowledge. Through extensive\nexperiments, we demonstrate that DQ-LoRe significantly outperforms prior\nstate-of-the-art methods in the automatic selection of exemplars for GPT-4,\nenhancing performance from 92.5\\% to 94.2\\%. Our comprehensive analysis further\nreveals that DQ-LoRe consistently outperforms retrieval-based approaches in\nterms of both performance and adaptability, especially in scenarios\ncharacterized by distribution shifts. DQ-LoRe pushes the boundaries of\nin-context learning and opens up new avenues for addressing complex reasoning\nchallenges. We will release the code soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jiong Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chuanyang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhijiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yichun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhicheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qingxing Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haiming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiongwei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jing Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction. (arXiv:2306.15724v3 [cs.RO] CROSS LISTED)","link":"http://arxiv.org/abs/2306.15724","description":"<p>The ability to detect and analyze failed executions automatically is crucial\nfor an explainable and robust robotic system. Recently, Large Language Models\n(LLMs) have demonstrated strong reasoning abilities on textual inputs. To\nleverage the power of LLMs for robot failure explanation, we introduce REFLECT,\na framework which queries LLM for failure reasoning based on a hierarchical\nsummary of robot past experiences generated from multisensory observations. The\nfailure explanation can further guide a language-based planner to correct the\nfailure and complete the task. To systematically evaluate the framework, we\ncreate the RoboFail dataset with a variety of tasks and failure scenarios. We\ndemonstrate that the LLM-based framework is able to generate informative\nfailure explanations that assist successful correction planning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zeyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahety_A/0/1/0/all/0/1\">Arpit Bahety</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuran Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study on Fertility Proposals Using Multi-Grained Topic Analysis Methods. (arXiv:2307.10025v2 [cs.HC] CROSS LISTED)","link":"http://arxiv.org/abs/2307.10025","description":"<p>Fertility issues are closely related to population security, in 60 years\nChina's population for the first time in a negative growth trend, the change of\nfertility policy is of great concern to the community. 2023 \"two sessions\"\nproposal \"suggests that the country in the form of legislation, the birth of\nthe registration of the cancellation of the marriage restriction\" This topic\nwas once a hot topic on the Internet, and \"unbundling\" the relationship between\nbirth registration and marriage has become the focus of social debate. In this\npaper, we adopt co-occurrence semantic analysis, topic analysis and sentiment\nanalysis to conduct multi-granularity semantic analysis of microblog comments.\nIt is found that the discussion on the proposal of \"removing marriage\nrestrictions from birth registration\" involves the individual, society and the\nstate at three dimensions, and is detailed into social issues such as personal\nbehaviour, social ethics and law, and national policy, with people's sentiment\ninclined to be negative in most of the topics. Based on this, eight proposals\nwere made to provide a reference for governmental decision making and to form a\nreference method for researching public opinion on political issues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yulin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of AI Generated Text Detection Tools. (arXiv:2310.01423v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2310.01423","description":"<p>Since ChatGPT has emerged as a major AIGC model, providing high-quality\nresponses across a wide range of applications (including software development\nand maintenance), it has attracted much interest from many individuals. ChatGPT\nhas great promise, but there are serious problems that might arise from its\nmisuse, especially in the realms of education and public safety. Several AIGC\ndetectors are available, and they have all been tested on genuine text.\nHowever, more study is needed to see how effective they are for multi-domain\nChatGPT material. This study aims to fill this need by creating a multi-domain\ndataset for testing the state-of-the-art APIs and tools for detecting\nartificially generated information used by universities and other research\ninstitutions. A large dataset consisting of articles, abstracts, stories, news,\nand product reviews was created for this study. The second step is to use the\nnewly created dataset to put six tools through their paces. Six different\nartificial intelligence (AI) text identification systems, including \"GPTkit,\"\n\"GPTZero,\" \"Originality,\" \"Sapling,\" \"Writer,\" and \"Zylalab,\" have accuracy\nrates between 55.29 and 97.0%. Although all the tools fared well in the\nevaluations, originality was particularly effective across the board.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akram_A/0/1/0/all/0/1\">Arslan Akram</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-10-05T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}