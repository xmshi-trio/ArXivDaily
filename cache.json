{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2024-01-12T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Automated Assessment of Students' Code Comprehension using LLMs. (arXiv:2401.05399v1 [cs.CY])","link":"http://arxiv.org/abs/2401.05399","description":"<p>Assessing student's answers and in particular natural language answers is a\ncrucial challenge in the field of education. Advances in machine learning,\nincluding transformer-based models such as Large Language Models(LLMs), have\nled to significant progress in various natural language tasks. Nevertheless,\namidst the growing trend of evaluating LLMs across diverse tasks, evaluating\nLLMs in the realm of automated answer assesment has not received much\nattention. To address this gap, we explore the potential of using LLMs for\nautomated assessment of student's short and open-ended answer. Particularly, we\nuse LLMs to compare students' explanations with expert explanations in the\ncontext of line-by-line explanations of computer programs.\n</p>\n<p>For comparison purposes, we assess both Large Language Models (LLMs) and\nencoder-based Semantic Textual Similarity (STS) models in the context of\nassessing the correctness of students' explanation of computer code. Our\nfindings indicate that LLMs, when prompted in few-shot and chain-of-thought\nsetting perform comparable to fine-tuned encoder-based models in evaluating\nstudents' short answers in programming domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oli_P/0/1/0/all/0/1\">Priti Oli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banjade_R/0/1/0/all/0/1\">Rabin Banjade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chapagain_J/0/1/0/all/0/1\">Jeevan Chapagain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_V/0/1/0/all/0/1\">Vasile Rus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Essay Scoring with Adversarial Weights Perturbation and Metric-specific AttentionPooling. (arXiv:2401.05433v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05433","description":"<p>The objective of this study is to improve automated feedback tools designed\nfor English Language Learners (ELLs) through the utilization of data science\ntechniques encompassing machine learning, natural language processing, and\neducational data analytics. Automated essay scoring (AES) research has made\nstrides in evaluating written essays, but it often overlooks the specific needs\nof English Language Learners (ELLs) in language development. This study\nexplores the application of BERT-related techniques to enhance the assessment\nof ELLs' writing proficiency within AES.\n</p>\n<p>To address the specific needs of ELLs, we propose the use of DeBERTa, a\nstate-of-the-art neural language model, for improving automated feedback tools.\nDeBERTa, pretrained on large text corpora using self-supervised learning,\nlearns universal language representations adaptable to various natural language\nunderstanding tasks. The model incorporates several innovative techniques,\nincluding adversarial training through Adversarial Weights Perturbation (AWP)\nand Metric-specific AttentionPooling (6 kinds of AP) for each label in the\ncompetition.\n</p>\n<p>The primary focus of this research is to investigate the impact of\nhyperparameters, particularly the adversarial learning rate, on the performance\nof the model. By fine-tuning the hyperparameter tuning process, including the\ninfluence of 6AP and AWP, the resulting models can provide more accurate\nevaluations of language proficiency and support tailored learning tasks for\nELLs. This work has the potential to significantly benefit ELLs by improving\ntheir English language proficiency and facilitating their educational journey.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xinyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_C/0/1/0/all/0/1\">Chang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qunwei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM4PLC: Harnessing Large Language Models for Verifiable Programming of PLCs in Industrial Control Systems. (arXiv:2401.05443v1 [cs.SE])","link":"http://arxiv.org/abs/2401.05443","description":"<p>Although Large Language Models (LLMs) have established pre-dominance in\nautomated code generation, they are not devoid of shortcomings. The pertinent\nissues primarily relate to the absence of execution guarantees for generated\ncode, a lack of explainability, and suboptimal support for essential but niche\nprogramming languages. State-of-the-art LLMs such as GPT-4 and LLaMa2 fail to\nproduce valid programs for Industrial Control Systems (ICS) operated by\nProgrammable Logic Controllers (PLCs). We propose LLM4PLC, a user-guided\niterative pipeline leveraging user feedback and external verification tools\nincluding grammar checkers, compilers and SMV verifiers to guide the LLM's\ngeneration. We further enhance the generation potential of LLM by employing\nPrompt Engineering and model fine-tuning through the creation and usage of\nLoRAs. We validate this system using a FischerTechnik Manufacturing TestBed\n(MFTB), illustrating how LLMs can evolve from generating structurally flawed\ncode to producing verifiably correct programs for industrial applications. We\nrun a complete test suite on GPT-3.5, GPT-4, Code Llama-7B, a fine-tuned Code\nLlama-7B model, Code Llama-34B, and a fine-tuned Code Llama-34B model. The\nproposed pipeline improved the generation success rate from 47% to 72%, and the\nSurvey-of-Experts code quality from 2.25/10 to 7.75/10. To promote open\nresearch, we share the complete experimental setup, the LLM Fine-Tuning\nWeights, and the video demonstrations of the different programs on our\ndedicated webpage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fakih_M/0/1/0/all/0/1\">Mohamad Fakih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dharmaji_R/0/1/0/all/0/1\">Rahul Dharmaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moghaddas_Y/0/1/0/all/0/1\">Yasamin Moghaddas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araya_G/0/1/0/all/0/1\">Gustavo Quiros Araya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogundare_O/0/1/0/all/0/1\">Oluwatosin Ogundare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faruque_M/0/1/0/all/0/1\">Mohammad Abdullah Al Faruque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks. (arXiv:2401.05507v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05507","description":"<p>In this paper, we introduce \"InfiAgent-DABench\", the first benchmark\nspecifically designed to evaluate LLM-based agents in data analysis tasks. This\nbenchmark contains DAEval, a dataset consisting of 311 data analysis questions\nderived from 55 CSV files, and an agent framework to evaluate LLMs as data\nanalysis agents. We adopt a format-prompting technique, ensuring questions to\nbe closed-form that can be automatically evaluated. Our extensive benchmarking\nof 23 state-of-the-art LLMs uncovers the current challenges encountered in data\nanalysis tasks. In addition, we have developed DAAgent, a specialized agent\ntrained on instruction-tuning datasets. Evaluation datasets and toolkits for\nInfiAgent-DABench are released at https://github.com/InfiAgent/InfiAgent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xueyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1\">Shuang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1\">Ziwei Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuwu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jing Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Ming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jianbo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Pampas to Pixels: Fine-Tuning Diffusion Models for Ga\\'ucho Heritage. (arXiv:2401.05520v1 [cs.CV])","link":"http://arxiv.org/abs/2401.05520","description":"<p>Generative AI has become pervasive in society, witnessing significant\nadvancements in various domains. Particularly in the realm of Text-to-Image\n(TTI) models, Latent Diffusion Models (LDMs), showcase remarkable capabilities\nin generating visual content based on textual prompts. This paper addresses the\npotential of LDMs in representing local cultural concepts, historical figures,\nand endangered species. In this study, we use the cultural heritage of Rio\nGrande do Sul (RS), Brazil, as an illustrative case. Our objective is to\ncontribute to the broader understanding of how generative models can help to\ncapture and preserve the cultural and historical identity of regions. The paper\noutlines the methodology, including subject selection, dataset creation, and\nthe fine-tuning process. The results showcase the images generated, alongside\nthe challenges and feasibility of each concept. In conclusion, this work shows\nthe power of these models to represent and preserve unique aspects of diverse\nregions and communities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amadeus_M/0/1/0/all/0/1\">Marcellus Amadeus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castaneda_W/0/1/0/all/0/1\">William Alberto Cruz Casta&#xf1;eda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanella_A/0/1/0/all/0/1\">Andr&#xe9; Felipe Zanella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahlow_F/0/1/0/all/0/1\">Felipe Rodrigues Perche Mahlow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodePrompt: Improving Source Code-Related Classification with Knowledge Features through Prompt Learning. (arXiv:2401.05544v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05544","description":"<p>Researchers have explored the potential of utilizing pre-trained language\nmodels, such as CodeBERT, to improve source code-related tasks. Previous\nstudies have mainly relied on CodeBERT's text embedding capability and the\n`[CLS]' sentence embedding information as semantic representations for\nfine-tuning downstream source code-related tasks. However, these methods\nrequire additional neural network layers to extract effective features,\nresulting in higher computational costs. Furthermore, existing approaches have\nnot leveraged the rich knowledge contained in both source code and related\ntext, which can lead to lower accuracy. This paper presents a novel approach,\nCodePrompt, which utilizes rich knowledge recalled from a pre-trained model by\nprompt learning and an attention mechanism to improve source code-related\nclassification tasks. Our approach initially motivates the language model with\nprompt information to retrieve abundant knowledge associated with the input as\nrepresentative features, thus avoiding the need for additional neural network\nlayers and reducing computational costs. Subsequently, we employ an attention\nmechanism to aggregate multiple layers of related knowledge for each task as\nfinal features to boost their accuracy. We conducted extensive experiments on\nfour downstream source code-related tasks to evaluate our approach and our\nresults demonstrate that CodePrompt achieves new state-of-the-art performance\non the accuracy metric while also exhibiting computation cost-saving\ncapabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Senlin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1\">Yu-Ming Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengjun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Useful Blunders: Can Automated Speech Recognition Errors Improve Downstream Dementia Classification?. (arXiv:2401.05551v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05551","description":"<p>\\textbf{Objectives}: We aimed to investigate how errors from automatic speech\nrecognition (ASR) systems affect dementia classification accuracy, specifically\nin the ``Cookie Theft'' picture description task. We aimed to assess whether\nimperfect ASR-generated transcripts could provide valuable information for\ndistinguishing between language samples from cognitively healthy individuals\nand those with Alzheimer's disease (AD).\n</p>\n<p>\\textbf{Methods}: We conducted experiments using various ASR models, refining\ntheir transcripts with post-editing techniques. Both these imperfect ASR\ntranscripts and manually transcribed ones were used as inputs for the\ndownstream dementia classification. We conducted comprehensive error analysis\nto compare model performance and assess ASR-generated transcript effectiveness\nin dementia classification.\n</p>\n<p>\\textbf{Results}: Imperfect ASR-generated transcripts surprisingly\noutperformed manual transcription for distinguishing between individuals with\nAD and those without in the ``Cookie Theft'' task. These ASR-based models\nsurpassed the previous state-of-the-art approach, indicating that ASR errors\nmay contain valuable cues related to dementia. The synergy between ASR and\nclassification models improved overall accuracy in dementia classification.\n</p>\n<p>\\textbf{Conclusion}: Imperfect ASR transcripts effectively capture linguistic\nanomalies linked to dementia, improving accuracy in classification tasks. This\nsynergy between ASR and classification models underscores ASR's potential as a\nvaluable tool in assessing cognitive impairment and related clinical\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changye Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weizhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1\">Trevor Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pakhomov_S/0/1/0/all/0/1\">Serguei Pakhomov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrustLLM: Trustworthiness in Large Language Models. (arXiv:2401.05561v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05561","description":"<p>Large language models (LLMs), exemplified by ChatGPT, have gained\nconsiderable attention for their excellent natural language processing\ncapabilities. Nonetheless, these LLMs present many challenges, particularly in\nthe realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs\nemerges as an important topic. This paper introduces TrustLLM, a comprehensive\nstudy of trustworthiness in LLMs, including principles for different dimensions\nof trustworthiness, established benchmark, evaluation, and analysis of\ntrustworthiness for mainstream LLMs, and discussion of open challenges and\nfuture directions. Specifically, we first propose a set of principles for\ntrustworthy LLMs that span eight different dimensions. Based on these\nprinciples, we further establish a benchmark across six dimensions including\ntruthfulness, safety, fairness, robustness, privacy, and machine ethics. We\nthen present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of\nover 30 datasets. Our findings firstly show that in general trustworthiness and\nutility (i.e., functional effectiveness) are positively related. Secondly, our\nobservations reveal that proprietary LLMs generally outperform most open-source\ncounterparts in terms of trustworthiness, raising concerns about the potential\nrisks of widely accessible open-source LLMs. However, a few open-source LLMs\ncome very close to proprietary ones. Thirdly, it is important to note that some\nLLMs may be overly calibrated towards exhibiting trustworthiness, to the extent\nthat they compromise their utility by mistakenly treating benign prompts as\nharmful and consequently not responding. Finally, we emphasize the importance\nof ensuring transparency not only in the models themselves but also in the\ntechnologies that underpin trustworthiness. Knowing the specific trustworthy\ntechnologies that have been employed is crucial for analyzing their\neffectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Siyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qihui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chujie Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yixin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_W/0/1/0/all/0/1\">Wenhan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yixuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiner Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yijue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhikun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Furong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Huaxiu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kellis_M/0/1/0/all/0/1\">Manolis Kellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1\">Marinka Zitnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jieyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_J/0/1/0/all/0/1\">John Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1\">Kai Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaidi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Backes_M/0/1/0/all/0/1\">Michael Backes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1\">Neil Zhenqiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1\">Rex Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shuiwang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jana_S/0/1/0/all/0/1\">Suman Jana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Willian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuyu Wang</a>, et al. (4 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. (arXiv:2401.05566v1 [cs.CR])","link":"http://arxiv.org/abs/2401.05566","description":"<p>Humans are capable of strategically deceptive behavior: behaving helpfully in\nmost situations, but then behaving very differently in order to pursue\nalternative objectives when given the opportunity. If an AI system learned such\na deceptive strategy, could we detect it and remove it using current\nstate-of-the-art safety training techniques? To study this question, we\nconstruct proof-of-concept examples of deceptive behavior in large language\nmodels (LLMs). For example, we train models that write secure code when the\nprompt states that the year is 2023, but insert exploitable code when the\nstated year is 2024. We find that such backdoored behavior can be made\npersistent, so that it is not removed by standard safety training techniques,\nincluding supervised fine-tuning, reinforcement learning, and adversarial\ntraining (eliciting unsafe behavior and then training to remove it). The\nbackdoored behavior is most persistent in the largest models and in models\ntrained to produce chain-of-thought reasoning about deceiving the training\nprocess, with the persistence remaining even when the chain-of-thought is\ndistilled away. Furthermore, rather than removing backdoors, we find that\nadversarial training can teach models to better recognize their backdoor\ntriggers, effectively hiding the unsafe behavior. Our results suggest that,\nonce a model exhibits deceptive behavior, standard techniques could fail to\nremove such deception and create a false impression of safety.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hubinger_E/0/1/0/all/0/1\">Evan Hubinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denison_C/0/1/0/all/0/1\">Carson Denison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1\">Jesse Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambert_M/0/1/0/all/0/1\">Mike Lambert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1\">Meg Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacDiarmid_M/0/1/0/all/0/1\">Monte MacDiarmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanham_T/0/1/0/all/0/1\">Tamera Lanham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziegler_D/0/1/0/all/0/1\">Daniel M. Ziegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maxwell_T/0/1/0/all/0/1\">Tim Maxwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Newton Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jermyn_A/0/1/0/all/0/1\">Adam Jermyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askell_A/0/1/0/all/0/1\">Amanda Askell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radhakrishnan_A/0/1/0/all/0/1\">Ansh Radhakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anil_C/0/1/0/all/0/1\">Cem Anil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duvenaud_D/0/1/0/all/0/1\">David Duvenaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_D/0/1/0/all/0/1\">Deep Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barez_F/0/1/0/all/0/1\">Fazl Barez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jack Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ndousse_K/0/1/0/all/0/1\">Kamal Ndousse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_K/0/1/0/all/0/1\">Kshitij Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellitto_M/0/1/0/all/0/1\">Michael Sellitto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1\">Mrinank Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DasSarma_N/0/1/0/all/0/1\">Nova DasSarma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosse_R/0/1/0/all/0/1\">Roger Grosse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kravec_S/0/1/0/all/0/1\">Shauna Kravec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yuntao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witten_Z/0/1/0/all/0/1\">Zachary Witten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favaro_M/0/1/0/all/0/1\">Marina Favaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1\">Jan Brauner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karnofsky_H/0/1/0/all/0/1\">Holden Karnofsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christiano_P/0/1/0/all/0/1\">Paul Christiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graham_L/0/1/0/all/0/1\">Logan Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1\">Jared Kaplan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mindermann_S/0/1/0/all/0/1\">S&#xf6;ren Mindermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenblatt_R/0/1/0/all/0/1\">Ryan Greenblatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shlegeris_B/0/1/0/all/0/1\">Buck Shlegeris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiefer_N/0/1/0/all/0/1\">Nicholas Schiefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"POMP: Probability-driven Meta-graph Prompter for LLMs in Low-resource Unsupervised Neural Machine Translation. (arXiv:2401.05596v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05596","description":"<p>Low-resource languages (LRLs) face challenges in supervised neural machine\ntranslation due to limited parallel data, prompting research into unsupervised\nmethods. Unsupervised neural machine translation (UNMT) methods, including\nback-translation, transfer learning, and pivot-based translation, offer\npractical solutions for LRL translation, but they are hindered by issues like\nsynthetic data noise, language bias, and error propagation, which can\npotentially be mitigated by Large Language Models (LLMs). LLMs have advanced\nNMT with in-context learning (ICL) and supervised fine-tuning methods, but\ninsufficient training data results in poor performance in LRLs. We argue that\nLLMs can mitigate the linguistic noise with auxiliary languages to improve\ntranslations in LRLs. In this paper, we propose Probability-driven Meta-graph\nPrompter (POMP), a novel approach employing a dynamic, sampling-based graph of\nmultiple auxiliary languages to enhance LLMs' translation capabilities for\nLRLs. POMP involves constructing a directed acyclic meta-graph for each source\nlanguage, from which we dynamically sample multiple paths to prompt LLMs to\nmitigate the linguistic noise and improve translations during training. We use\nthe BLEURT metric to evaluate the translations and back-propagate rewards,\nestimated by scores, to update the probabilities of auxiliary languages in the\npaths. Our experiments show significant improvements in the translation quality\nof three LRLs, demonstrating the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shilong Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhiliang Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zhihua Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REBUS: A Robust Evaluation Benchmark of Understanding Symbols. (arXiv:2401.05604v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05604","description":"<p>We propose a new benchmark evaluating the performance of multimodal large\nlanguage models on rebus puzzles. The dataset covers 333 original examples of\nimage-based wordplay, cluing 13 categories such as movies, composers, major\ncities, and food. To achieve good performance on the benchmark of identifying\nthe clued word or phrase, models must combine image recognition and string\nmanipulation with hypothesis testing, multi-step reasoning, and an\nunderstanding of human cognition, making for a complex, multimodal evaluation\nof capabilities. We find that proprietary models such as GPT-4V and Gemini Pro\nsignificantly outperform all other tested models. However, even the best model\nhas a final accuracy of just 24%, highlighting the need for substantial\nimprovements in reasoning. Further, models rarely understand all parts of a\npuzzle, and are almost always incapable of retroactively explaining the correct\nanswer. Our benchmark can therefore be used to identify major shortcomings in\nthe knowledge and reasoning of multimodal large language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gritsevskiy_A/0/1/0/all/0/1\">Andrew Gritsevskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panickssery_A/0/1/0/all/0/1\">Arjun Panickssery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirtland_A/0/1/0/all/0/1\">Aaron Kirtland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kauffman_D/0/1/0/all/0/1\">Derik Kauffman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gundlach_H/0/1/0/all/0/1\">Hans Gundlach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritsevskaya_I/0/1/0/all/0/1\">Irina Gritsevskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavanagh_J/0/1/0/all/0/1\">Joe Cavanagh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_J/0/1/0/all/0/1\">Jonathan Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roux_L/0/1/0/all/0/1\">Lydia La Roux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_M/0/1/0/all/0/1\">Michelle Hung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Laws for Forgetting When Fine-Tuning Large Language Models. (arXiv:2401.05605v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05605","description":"<p>We study and quantify the problem of forgetting when fine-tuning pre-trained\nlarge language models (LLMs) on a downstream task. We find that\nparameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adapters\n(LoRA), still suffer from catastrophic forgetting. In particular, we identify a\nstrong inverse linear relationship between the fine-tuning performance and the\namount of forgetting when fine-tuning LLMs with LoRA. We further obtain precise\nscaling laws that show forgetting increases as a shifted power law in the\nnumber of parameters fine-tuned and the number of update steps. We also examine\nthe impact of forgetting on knowledge, reasoning, and the safety guardrails\ntrained into Llama 2 7B chat. Our study suggests that forgetting cannot be\navoided through early stopping or by varying the number of parameters\nfine-tuned. We believe this opens up an important safety-critical direction for\nfuture research to evaluate and develop fine-tuning schemes which mitigate\nforgetting\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalajdzievski_D/0/1/0/all/0/1\">Damjan Kalajdzievski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models. (arXiv:2401.05618v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05618","description":"<p>In this paper, we introduce Concise Chain-of-Thought (CCoT) prompting. We\ncompared standard CoT and CCoT prompts to see how conciseness impacts response\nlength and correct-answer accuracy. We evaluated this using GPT-3.5 and GPT-4\nwith a multiple-choice question-and-answer (MCQA) benchmark. CCoT reduced\naverage response length by 48.70% for both GPT-3.5 and GPT-4 while having a\nnegligible impact on problem-solving performance. However, on math problems,\nGPT-3.5 with CCoT incurs a performance penalty of 27.69%. Overall, CCoT leads\nto an average per-token cost reduction of 22.67%. These results have practical\nimplications for AI systems engineers using LLMs to solve real-world problems\nwith CoT prompt-engineering techniques. In addition, these results provide more\ngeneral insight for AI researchers studying the emergent behavior of\nstep-by-step reasoning in LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Renze_M/0/1/0/all/0/1\">Matthew Renze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guven_E/0/1/0/all/0/1\">Erhan Guven</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DrawTalking: Building Interactive Worlds by Sketching and Speaking. (arXiv:2401.05631v1 [cs.HC])","link":"http://arxiv.org/abs/2401.05631","description":"<p>We introduce an interactive approach, DrawTalking, in which the user builds\ninteractive worlds by sketching and speaking. It emphasizes user control and\nflexibility, and gives programming-like capability without code. We implemented\nit on the iPad. An open-ended study shows the mechanics resonate and are\napplicable to many creative-exploratory use cases. We hope to inspire and\ninform research in future natural user-centered interfaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_K/0/1/0/all/0/1\">Karl Toby Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazi_R/0/1/0/all/0/1\">Rubaiat Habib Kazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Li-Yi Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1\">Haijun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perlin_K/0/1/0/all/0/1\">Ken Perlin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Processing for Dialects of a Language: A Survey. (arXiv:2401.05632v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05632","description":"<p>State-of-the-art natural language processing (NLP) models are trained on\nmassive training corpora, and report a superlative performance on evaluation\ndatasets. This survey delves into an important attribute of these datasets: the\ndialect of a language. Motivated by the performance degradation of NLP models\nfor dialectic datasets and its implications for the equity of language\ntechnologies, we survey past research in NLP for dialects in terms of datasets,\nand approaches. We describe a wide range of NLP tasks in terms of two\ncategories: natural language understanding (NLU) (for tasks such as dialect\nclassification, sentiment analysis, parsing, and NLU benchmarks) and natural\nlanguage generation (NLG) (for summarisation, machine translation, and dialogue\nsystems). The survey is also broad in its coverage of languages which include\nEnglish, Arabic, German among others. We observe that past work in NLP\nconcerning dialects goes deeper than mere dialect classification, and . This\nincludes early approaches that used sentence transduction that lead to the\nrecent approaches that integrate hypernetworks into LoRA. We expect that this\nsurvey will be useful to NLP researchers interested in building equitable\nlanguage technologies by rethinking LLM benchmarks and model architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Aditya Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1\">Raj Dabre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanojia_D/0/1/0/all/0/1\">Diptesh Kanojia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1\">Haolan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dippold_D/0/1/0/all/0/1\">Doris Dippold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Detecting Cherry-picking in News Coverage Using Large Language Models. (arXiv:2401.05650v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05650","description":"<p>Cherry-picking refers to the deliberate selection of evidence or facts that\nfavor a particular viewpoint while ignoring or distorting evidence that\nsupports an opposing perspective. Manually identifying instances of\ncherry-picked statements in news stories can be challenging, particularly when\nthe opposing viewpoint's story is absent. This study introduces Cherry, an\ninnovative approach for automatically detecting cherry-picked statements in\nnews articles by finding missing important statements in the target news story.\nCherry utilizes the analysis of news coverage from multiple sources to identify\ninstances of cherry-picking. Our approach relies on language models that\nconsider contextual information from other news sources to classify statements\nbased on their importance to the event covered in the target news story.\nFurthermore, this research introduces a novel dataset specifically designed for\ncherry-picking detection, which was used to train and evaluate the performance\nof the models. Our best performing model achieves an F-1 score of about %89 in\ndetecting important statements when tested on unseen set of news stories.\nMoreover, results show the importance incorporating external knowledge from\nalternative unbiased narratives when assessing a statement's importance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jaradat_I/0/1/0/all/0/1\">Israa Jaradat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haiqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengkai Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Conversational Diagnostic AI. (arXiv:2401.05654v1 [cs.AI])","link":"http://arxiv.org/abs/2401.05654","description":"<p>At the heart of medicine lies the physician-patient dialogue, where skillful\nhistory-taking paves the way for accurate diagnosis, effective management, and\nenduring trust. Artificial Intelligence (AI) systems capable of diagnostic\ndialogue could increase accessibility, consistency, and quality of care.\nHowever, approximating clinicians' expertise is an outstanding grand challenge.\nHere, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large\nLanguage Model (LLM) based AI system optimized for diagnostic dialogue.\n</p>\n<p>AMIE uses a novel self-play based simulated environment with automated\nfeedback mechanisms for scaling learning across diverse disease conditions,\nspecialties, and contexts. We designed a framework for evaluating\nclinically-meaningful axes of performance including history-taking, diagnostic\naccuracy, management reasoning, communication skills, and empathy. We compared\nAMIE's performance to that of primary care physicians (PCPs) in a randomized,\ndouble-blind crossover study of text-based consultations with validated patient\nactors in the style of an Objective Structured Clinical Examination (OSCE). The\nstudy included 149 case scenarios from clinical providers in Canada, the UK,\nand India, 20 PCPs for comparison with AMIE, and evaluations by specialist\nphysicians and patient actors. AMIE demonstrated greater diagnostic accuracy\nand superior performance on 28 of 32 axes according to specialist physicians\nand 24 of 26 axes according to patient actors. Our research has several\nlimitations and should be interpreted with appropriate caution. Clinicians were\nlimited to unfamiliar synchronous text-chat which permits large-scale\nLLM-patient interactions but is not representative of usual clinical practice.\nWhile further research is required before AMIE could be translated to\nreal-world settings, the results represent a milestone towards conversational\ndiagnostic AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_T/0/1/0/all/0/1\">Tao Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palepu_A/0/1/0/all/0/1\">Anil Palepu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaekermann_M/0/1/0/all/0/1\">Mike Schaekermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saab_K/0/1/0/all/0/1\">Khaled Saab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freyberg_J/0/1/0/all/0/1\">Jan Freyberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanno_R/0/1/0/all/0/1\">Ryutaro Tanno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Amy Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Brenna Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1\">Mohamed Amin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomasev_N/0/1/0/all/0/1\">Nenad Tomasev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azizi_S/0/1/0/all/0/1\">Shekoofeh Azizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_K/0/1/0/all/0/1\">Karan Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Le Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1\">Albert Webson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_K/0/1/0/all/0/1\">Kavita Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_S/0/1/0/all/0/1\">S Sara Mahdavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semturs_C/0/1/0/all/0/1\">Christopher Semturs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gottweis_J/0/1/0/all/0/1\">Juraj Gottweis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barral_J/0/1/0/all/0/1\">Joelle Barral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_K/0/1/0/all/0/1\">Katherine Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corrado_G/0/1/0/all/0/1\">Greg S Corrado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matias_Y/0/1/0/all/0/1\">Yossi Matias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karthikesalingam_A/0/1/0/all/0/1\">Alan Karthikesalingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_V/0/1/0/all/0/1\">Vivek Natarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unveiling the Tapestry of Automated Essay Scoring: A Comprehensive Investigation of Accuracy, Fairness, and Generalizability. (arXiv:2401.05655v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05655","description":"<p>Automatic Essay Scoring (AES) is a well-established educational pursuit that\nemploys machine learning to evaluate student-authored essays. While much effort\nhas been made in this area, current research primarily focuses on either (i)\nboosting the predictive accuracy of an AES model for a specific prompt (i.e.,\ndeveloping prompt-specific models), which often heavily relies on the use of\nthe labeled data from the same target prompt; or (ii) assessing the\napplicability of AES models developed on non-target prompts to the intended\ntarget prompt (i.e., developing the AES models in a cross-prompt setting).\nGiven the inherent bias in machine learning and its potential impact on\nmarginalized groups, it is imperative to investigate whether such bias exists\nin current AES methods and, if identified, how it intervenes with an AES\nmodel's accuracy and generalizability. Thus, our study aimed to uncover the\nintricate relationship between an AES model's accuracy, fairness, and\ngeneralizability, contributing practical insights for developing effective AES\nmodels in real-world education. To this end, we meticulously selected nine\nprominent AES methods and evaluated their performance using seven metrics on an\nopen-sourced dataset, which contains over 25,000 essays and various demographic\ninformation about students such as gender, English language learner status, and\neconomic status. Through extensive evaluations, we demonstrated that: (1)\nprompt-specific models tend to outperform their cross-prompt counterparts in\nterms of predictive accuracy; (2) prompt-specific models frequently exhibit a\ngreater bias towards students of different economic statuses compared to\ncross-prompt models; (3) in the pursuit of generalizability, traditional\nmachine learning models coupled with carefully engineered features hold greater\npotential for achieving both high accuracy and fairness than complex neural\nnetwork models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kaixun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakovic_M/0/1/0/all/0/1\">Mladen Rakovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Q/0/1/0/all/0/1\">Quanlong Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasevic_D/0/1/0/all/0/1\">Dragan Ga&#x161;evi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanliang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConcEPT: Concept-Enhanced Pre-Training for Language Models. (arXiv:2401.05669v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05669","description":"<p>Pre-trained language models (PLMs) have been prevailing in state-of-the-art\nmethods for natural language processing, and knowledge-enhanced PLMs are\nfurther proposed to promote model performance in knowledge-intensive tasks.\nHowever, conceptual knowledge, one essential kind of knowledge for human\ncognition, still remains understudied in this line of research. This limits\nPLMs' performance in scenarios requiring human-like cognition, such as\nunderstanding long-tail entities with concepts. In this paper, we propose\nConcEPT, which stands for Concept-Enhanced Pre-Training for language models, to\ninfuse conceptual knowledge into PLMs. ConcEPT exploits external taxonomies\nwith entity concept prediction, a novel pre-training objective to predict the\nconcepts of entities mentioned in the pre-training contexts. Unlike previous\nconcept-enhanced methods, ConcEPT can be readily adapted to various downstream\napplications without entity linking or concept mapping. Results of extensive\nexperiments show the effectiveness of ConcEPT in four tasks such as entity\ntyping, which validates that our model gains improved conceptual knowledge with\nconcept-enhanced pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1\">Zhouhong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiaqing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">Dakuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UCorrect: An Unsupervised Framework for Automatic Speech Recognition Error Correction. (arXiv:2401.05689v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05689","description":"<p>Error correction techniques have been used to refine the output sentences\nfrom automatic speech recognition (ASR) models and achieve a lower word error\nrate (WER). Previous works usually adopt end-to-end models and has strong\ndependency on Pseudo Paired Data and Original Paired Data. But when only\npre-training on Pseudo Paired Data, previous models have negative effect on\ncorrection. While fine-tuning on Original Paired Data, the source side data\nmust be transcribed by a well-trained ASR model, which takes a lot of time and\nnot universal. In this paper, we propose UCorrect, an unsupervised\nDetector-Generator-Selector framework for ASR Error Correction. UCorrect has no\ndependency on the training data mentioned before. The whole procedure is first\nto detect whether the character is erroneous, then to generate some candidate\ncharacters and finally to select the most confident one to replace the error\ncharacter. Experiments on the public AISHELL-1 dataset and WenetSpeech dataset\nshow the effectiveness of UCorrect for ASR error correction: 1) it achieves\nsignificant WER reduction, achieves 6.83\\% even without fine-tuning and 14.29\\%\nafter fine-tuning; 2) it outperforms the popular NAR correction models by a\nlarge margin with a competitive low latency; and 3) it is an universal method,\nas it reduces all WERs of the ASR model with different decoding strategies and\nreduces all WERs of ASR models trained on different scale datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaxin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Minghan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_X/0/1/0/all/0/1\">Xiaosong Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Daimeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_H/0/1/0/all/0/1\">Hengchao Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zongyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhengzhe Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinglu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Shimin Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Physician Diagnostic Logic into Large Language Models: Preference Learning from Process Feedback. (arXiv:2401.05695v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05695","description":"<p>The use of large language models in medical dialogue generation has garnered\nsignificant attention, with a focus on improving response quality and fluency.\nWhile previous studies have made progress in optimizing model performance for\nsingle-round medical Q&amp;A tasks, there is a need to enhance the model's\ncapability for multi-round conversations to avoid logical inconsistencies. To\naddress this, we propose an approach called preference learning from process\nfeedback~(PLPF), which integrates the doctor's diagnostic logic into LLMs. PLPF\ninvolves rule modeling, preference data generation, and preference alignment to\ntrain the model to adhere to the diagnostic process. Experimental results using\nStandardized Patient Testing show that PLPF enhances the diagnostic accuracy of\nthe baseline model in medical conversations by 17.6%, outperforming traditional\nreinforcement learning from human feedback. Additionally, PLPF demonstrates\neffectiveness in both multi-round and single-round dialogue tasks, showcasing\nits potential for improving medical dialogue generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_C/0/1/0/all/0/1\">Chengfeng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenpin Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haiyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yongqiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1\">Zhenwei Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R-BI: Regularized Batched Inputs enhance Incremental Decoding Framework for Low-Latency Simultaneous Speech Translation. (arXiv:2401.05700v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05700","description":"<p>Incremental Decoding is an effective framework that enables the use of an\noffline model in a simultaneous setting without modifying the original model,\nmaking it suitable for Low-Latency Simultaneous Speech Translation. However,\nthis framework may introduce errors when the system outputs from incomplete\ninput. To reduce these output errors, several strategies such as Hold-$n$,\nLA-$n$, and SP-$n$ can be employed, but the hyper-parameter $n$ needs to be\ncarefully selected for optimal performance. Moreover, these strategies are more\nsuitable for end-to-end systems than cascade systems. In our paper, we propose\na new adaptable and efficient policy named \"Regularized Batched Inputs\". Our\nmethod stands out by enhancing input diversity to mitigate output errors. We\nsuggest particular regularization techniques for both end-to-end and cascade\nsystems. We conducted experiments on IWSLT Simultaneous Speech Translation\n(SimulST) tasks, which demonstrate that our approach achieves low latency while\nmaintaining no more than 2 BLEU points loss compared to offline systems.\nFurthermore, our SimulST systems attained several new state-of-the-art results\nin various language directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaxin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhanglin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zongyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_H/0/1/0/all/0/1\">Hengchao Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Daimeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Z/0/1/0/all/0/1\">Zhiqiang Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaojun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAT-LLM: Prompting Large Language Models with Text Style Definition for Chinese Article-style Transfer. (arXiv:2401.05707v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05707","description":"<p>Text style transfer is increasingly prominent in online entertainment and\nsocial media. However, existing research mainly concentrates on style transfer\nwithin individual English sentences, while ignoring the complexity of long\nChinese texts, which limits the wider applicability of style transfer in\ndigital media realm. To bridge this gap, we propose a Chinese Article-style\nTransfer framework (CAT-LLM), leveraging the capabilities of Large Language\nModels (LLMs). CAT-LLM incorporates a bespoke, pluggable Text Style Definition\n(TSD) module aimed at comprehensively analyzing text features in articles,\nprompting LLMs to efficiently transfer Chinese article-style. The TSD module\nintegrates a series of machine learning algorithms to analyze article-style\nfrom both words and sentences levels, thereby aiding LLMs thoroughly grasp the\ntarget style without compromising the integrity of the original text. In\naddition, this module supports dynamic expansion of internal style trees,\nshowcasing robust compatibility and allowing flexible optimization in\nsubsequent research. Moreover, we select five Chinese articles with distinct\nstyles and create five parallel datasets using ChatGPT, enhancing the models'\nperformance evaluation accuracy and establishing a novel paradigm for\nevaluating subsequent research on article-style transfer. Extensive\nexperimental results affirm that CAT-LLM outperforms current research in terms\nof transfer accuracy and content preservation, and has remarkable applicability\nto various types of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1\">Zhen Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_D/0/1/0/all/0/1\">Dinghao Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Liumin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero Resource Cross-Lingual Part Of Speech Tagging. (arXiv:2401.05727v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05727","description":"<p>Part of speech tagging in zero-resource settings can be an effective approach\nfor low-resource languages when no labeled training data is available. Existing\nsystems use two main techniques for POS tagging i.e. pretrained multilingual\nlarge language models(LLM) or project the source language labels into the zero\nresource target language and train a sequence labeling model on it. We explore\nthe latter approach using the off-the-shelf alignment module and train a hidden\nMarkov model(HMM) to predict the POS tags. We evaluate transfer learning setup\nwith English as a source language and French, German, and Spanish as target\nlanguages for part-of-speech tagging. Our conclusion is that projected\nalignment data in zero-resource language can be beneficial to predict POS tags.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chopra_S/0/1/0/all/0/1\">Sahil Chopra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-modal Retrieval for Knowledge-based Visual Question Answering. (arXiv:2401.05736v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05736","description":"<p>Knowledge-based Visual Question Answering about Named Entities is a\nchallenging task that requires retrieving information from a multimodal\nKnowledge Base. Named entities have diverse visual representations and are\ntherefore difficult to recognize. We argue that cross-modal retrieval may help\nbridge the semantic gap between an entity and its depictions, and is foremost\ncomplementary with mono-modal retrieval. We provide empirical evidence through\nexperiments with a multimodal dual encoder, namely CLIP, on the recent ViQuAE,\nInfoSeek, and Encyclopedic-VQA datasets. Additionally, we study three different\nstrategies to fine-tune such a model: mono-modal, cross-modal, or joint\ntraining. Our method, which combines mono-and cross-modal retrieval, is\ncompetitive with billion-parameter models on the three datasets, while being\nconceptually simpler and computationally cheaper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lerner_P/0/1/0/all/0/1\">Paul Lerner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferret_O/0/1/0/all/0/1\">Olivier Ferret</a> (LIST (CEA), DIASI), <a href=\"http://arxiv.org/find/cs/1/au:+Guinaudeau_C/0/1/0/all/0/1\">Camille Guinaudeau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Shocking Amount of the Web is Machine Translated: Insights from Multi-Way Parallelism. (arXiv:2401.05749v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05749","description":"<p>We show that content on the web is often translated into many languages, and\nthe low quality of these multi-way translations indicates they were likely\ncreated using Machine Translation (MT). Multi-way parallel, machine generated\ncontent not only dominates the translations in lower resource languages; it\nalso constitutes a large fraction of the total web content in those languages.\nWe also find evidence of a selection bias in the type of content which is\ntranslated into many languages, consistent with low quality English content\nbeing translated en masse into many lower resource languages, via MT. Our work\nraises serious concerns about training models such as multilingual large\nlanguage models on both monolingual and bilingual data scraped from the web.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thompson_B/0/1/0/all/0/1\">Brian Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhaliwal_M/0/1/0/all/0/1\">Mehak Preet Dhaliwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frisch_P/0/1/0/all/0/1\">Peter Frisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Domhan_T/0/1/0/all/0/1\">Tobias Domhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Federico_M/0/1/0/all/0/1\">Marcello Federico</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Structured Semantics Understanding and Generation of Language Models via Question Answering. (arXiv:2401.05777v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05777","description":"<p>Recent advancement in the capabilities of large language models (LLMs) has\ntriggered a new surge in LLMs' evaluation. Most recent evaluation works tends\nto evaluate the comprehensive ability of LLMs over series of tasks. However,\nthe deep structure understanding of natural language is rarely explored. In\nthis work, we examine the ability of LLMs to deal with structured semantics on\nthe tasks of question answering with the help of the human-constructed formal\nlanguage. Specifically, we implement the inter-conversion of natural and formal\nlanguage through in-context learning of LLMs to verify their ability to\nunderstand and generate the structured logical forms. Extensive experiments\nwith models of different sizes and in different formal languages show that\ntoday's state-of-the-art LLMs' understanding of the logical forms can approach\nhuman level overall, but there still are plenty of room in generating correct\nlogical forms, which suggest that it is more effective to use LLMs to generate\nmore natural language training data to reinforce a small model than directly\nanswering questions with LLMs. Moreover, our results also indicate that models\nexhibit considerable sensitivity to different formal languages. In general, the\nformal language with the lower the formalization level, i.e. the more similar\nit is to natural language, is more LLMs-friendly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shulin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiaxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tingjian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems. (arXiv:2401.05778v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05778","description":"<p>Large language models (LLMs) have strong capabilities in solving diverse\nnatural language processing tasks. However, the safety and security issues of\nLLM systems have become the major obstacle to their widespread application.\nMany studies have extensively investigated risks in LLM systems and developed\nthe corresponding mitigation strategies. Leading-edge enterprises such as\nOpenAI, Google, Meta, and Anthropic have also made lots of efforts on\nresponsible LLMs. Therefore, there is a growing need to organize the existing\nstudies and establish comprehensive taxonomies for the community. In this\npaper, we delve into four essential modules of an LLM system, including an\ninput module for receiving prompts, a language model trained on extensive\ncorpora, a toolchain module for development and deployment, and an output\nmodule for exporting LLM-generated content. Based on this, we propose a\ncomprehensive taxonomy, which systematically analyzes potential risks\nassociated with each module of an LLM system and discusses the corresponding\nmitigation strategies. Furthermore, we review prevalent benchmarks, aiming to\nfacilitate the risk assessment of LLM systems. We hope that this paper can help\nLLM participants embrace a systematic perspective to build their responsible\nLLM systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_T/0/1/0/all/0/1\">Tianyu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chuanpu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sijia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xinhao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunpeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Ziyi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peiyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhixing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Junwu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xinyu Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zujie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning. (arXiv:2401.05787v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05787","description":"<p>While chain-of-thought (CoT) prompting has revolutionized how LLMs perform\nreasoning tasks, its current methods and variations (e.g, Self-consistency,\nReACT, Reflexion, Tree-of-Thoughts (ToT), Cumulative Reasoning (CR)) suffer\nfrom limitations like slowness, limited context grounding, hallucination and\ninconsistent outputs. To overcome these challenges, we introduce Evidence to\nGenerate (E2G), a novel single-agent, two-step prompting framework. Instead of\nunverified reasoning claims, this innovative approach leverages the power of\n\"evidence for decision making\" by first focusing exclusively on the thought\nsequences (the series of intermediate steps) explicitly mentioned in the\ncontext which then serve as extracted evidence, guiding the LLM's output\ngeneration process with greater precision and efficiency. This simple yet\npowerful approach unlocks the true potential of chain-of-thought like\nprompting, paving the way for faster, more reliable, and more contextually\naware reasoning in LLMs. \\tool achieves remarkable results robustly across a\nwide range of knowledge-intensive reasoning and generation tasks, surpassing\nbaseline approaches with state-of-the-art LLMs. For example, (i) on LogiQA\nbenchmark using GPT-4 as backbone model, \\tool achieves a new state-of-the\nAccuracy of 53.8% exceeding CoT by 18%, ToT by 11%, CR by 9% (ii) a variant of\nE2G with PaLM2 outperforms the variable-shot performance of Gemini Ultra by 0.9\nF1 points, reaching an F1 score of 83.3 on a subset of DROP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parvez_M/0/1/0/all/0/1\">Md Rizwan Parvez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Low-rank Subspaces for Language-agnostic Multilingual Representations. (arXiv:2401.05792v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05792","description":"<p>Large pretrained multilingual language models (ML-LMs) have shown remarkable\ncapabilities of zero-shot cross-lingual transfer, without direct cross-lingual\nsupervision. While these results are promising, follow-up works found that,\nwithin the multilingual embedding spaces, there exists strong language identity\ninformation which hinders the expression of linguistic factors shared across\nlanguages. For semantic tasks like cross-lingual sentence retrieval, it is\ndesired to remove such language identity signals to fully leverage semantic\ninformation. In this work, we provide a novel view of projecting away\nlanguage-specific factors from a multilingual embedding space. Specifically, we\ndiscover that there exists a low-rank subspace that primarily encodes\ninformation irrelevant to semantics (e.g., syntactic information). To identify\nthis subspace, we present a simple but effective unsupervised method based on\nsingular value decomposition with multiple monolingual corpora as input. Once\nthe subspace is found, we can directly project the original embeddings into the\nnull space to boost language agnosticism without finetuning. We systematically\nevaluate our method on various tasks including the challenging\nlanguage-agnostic QA retrieval task. Empirical results show that applying our\nmethod consistently leads to improvements over commonly used ML-LMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhihui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Handong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuai Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Designing Heterogeneous LLM Agents for Financial Sentiment Analysis. (arXiv:2401.05799v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05799","description":"<p>Large language models (LLMs) have drastically changed the possible ways to\ndesign intelligent systems, shifting the focuses from massive data acquisition\nand new modeling training to human alignment and strategical elicitation of the\nfull potential of existing pre-trained models. This paradigm shift, however, is\nnot fully realized in financial sentiment analysis (FSA), due to the\ndiscriminative nature of this task and a lack of prescriptive knowledge of how\nto leverage generative models in such a context. This study investigates the\neffectiveness of the new paradigm, i.e., using LLMs without fine-tuning for\nFSA. Rooted in Minsky's theory of mind and emotions, a design framework with\nheterogeneous LLM agents is proposed. The framework instantiates specialized\nagents using prior domain knowledge of the types of FSA errors and reasons on\nthe aggregated agent discussions. Comprehensive evaluation on FSA datasets show\nthat the framework yields better accuracies, especially when the discussions\nare substantial. This study contributes to the design foundations and paves new\navenues for LLMs-based FSA. Implications on business and management are also\ndiscussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Frank Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tuning LLMs with Contrastive Alignment Instructions for Machine Translation in Unseen, Low-resource Languages. (arXiv:2401.05811v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05811","description":"<p>This article introduces contrastive alignment instructions (AlignInstruct) to\naddress two challenges in machine translation (MT) on large language models\n(LLMs). One is the expansion of supported languages to previously unseen ones.\nThe second relates to the lack of data in low-resource languages. Model\nfine-tuning through MT instructions (MTInstruct) is a straightforward approach\nto the first challenge. However, MTInstruct is limited by weak cross-lingual\nsignals inherent in the second challenge. AlignInstruct emphasizes\ncross-lingual supervision via a cross-lingual discriminator built using\nstatistical word alignments. Our results based on fine-tuning the BLOOMZ models\n(1b1, 3b, and 7b1) in up to 24 unseen languages showed that: (1) LLMs can\neffectively translate unseen languages using MTInstruct; (2) AlignInstruct led\nto consistent improvements in translation quality across 48 translation\ndirections involving English; (3) Discriminator-based instructions outperformed\ntheir generative counterparts as cross-lingual instructions; (4) AlignInstruct\nimproved performance in 30 zero-shot directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhuoyuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yen Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Goal-Oriented Agents for Evolving Problems Observed via Conversation. (arXiv:2401.05822v1 [cs.AI])","link":"http://arxiv.org/abs/2401.05822","description":"<p>The objective of this work is to train a chatbot capable of solving evolving\nproblems through conversing with a user about a problem the chatbot cannot\ndirectly observe. The system consists of a virtual problem (in this case a\nsimple game), a simulated user capable of answering natural language questions\nthat can observe and perform actions on the problem, and a Deep Q-Network\n(DQN)-based chatbot architecture. The chatbot is trained with the goal of\nsolving the problem through dialogue with the simulated user using\nreinforcement learning. The contributions of this paper are as follows: a\nproposed architecture to apply a conversational DQN-based agent to evolving\nproblems, an exploration of training methods such as curriculum learning on\nmodel performance and the effect of modified reward functions in the case of\nincreasing environment complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Free_M/0/1/0/all/0/1\">Michael Free</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langworthy_A/0/1/0/all/0/1\">Andrew Langworthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimitropoulaki_M/0/1/0/all/0/1\">Mary Dimitropoulaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_S/0/1/0/all/0/1\">Simon Thompson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hallucination Benchmark in Medical Visual Question Answering. (arXiv:2401.05827v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05827","description":"<p>The recent success of large language and vision models on vision question\nanswering (VQA), particularly their applications in medicine (Med-VQA), has\nshown a great potential of realizing effective visual assistants for\nhealthcare. However, these models are not extensively tested on the\nhallucination phenomenon in clinical settings. Here, we created a hallucination\nbenchmark of medical images paired with question-answer sets and conducted a\ncomprehensive evaluation of the state-of-the-art models. The study provides an\nin-depth analysis of current models limitations and reveals the effectiveness\nof various prompting strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jinge Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yunsoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Honghan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inferring Intentions to Speak Using Accelerometer Data In-the-Wild. (arXiv:2401.05849v1 [cs.LG])","link":"http://arxiv.org/abs/2401.05849","description":"<p>Humans have good natural intuition to recognize when another person has\nsomething to say. It would be interesting if an AI can also recognize\nintentions to speak. Especially in scenarios when an AI is guiding a group\ndiscussion, this can be a useful skill. This work studies the inference of\nsuccessful and unsuccessful intentions to speak from accelerometer data. This\nis chosen because it is privacy-preserving and feasible for in-the-wild\nsettings since it can be placed in a smart badge. Data from a real-life social\nnetworking event is used to train a machine-learning model that aims to infer\nintentions to speak. A subset of unsuccessful intention-to-speak cases in the\ndata is annotated. The model is trained on the successful intentions to speak\nand evaluated on both the successful and unsuccessful cases. In conclusion,\nthere is useful information in accelerometer data, but not enough to reliably\ncapture intentions to speak. For example, posture shifts are correlated with\nintentions to speak, but people also often shift posture without having an\nintention to speak, or have an intention to speak without shifting their\nposture. More modalities are likely needed to reliably infer intentions to\nspeak.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Litian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molhoek_J/0/1/0/all/0/1\">Jord Molhoek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jing Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Boosting Many-to-Many Multilingual Machine Translation with Large Language Models. (arXiv:2401.05861v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05861","description":"<p>The training paradigm for machine translation has gradually shifted, from\nlearning neural machine translation (NMT) models with extensive parallel\ncorpora to instruction finetuning on pretrained multilingual large language\nmodels (LLMs) with high-quality translation pairs. In this paper, we focus on\nboosting the many-to-many multilingual translation performance of LLMs with an\nemphasis on zero-shot translation directions. We demonstrate that prompt\nstrategies adopted during instruction finetuning are crucial to zero-shot\ntranslation performance and introduce a cross-lingual consistency\nregularization, XConST, to bridge the representation gap among different\nlanguages and improve zero-shot translation performance. XConST is not a new\nmethod, but a version of CrossConST (Gao et al., 2023a) adapted for\nmultilingual finetuning on LLMs with translation instructions. Experimental\nresults on ALMA (Xu et al., 2023) and LLaMA-2 (Touvron et al., 2023) show that\nour approach consistently improves translation performance. Our implementations\nare available at https://github.com/gpengzhi/CrossConST-LLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Pengzhi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhongjun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Personality Recognition in Dialogue by Data Augmentation and Heterogeneous Conversational Graph Networks. (arXiv:2401.05871v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05871","description":"<p>Personality recognition is useful for enhancing robots' ability to tailor\nuser-adaptive responses, thus fostering rich human-robot interactions. One of\nthe challenges in this task is a limited number of speakers in existing\ndialogue corpora, which hampers the development of robust, speaker-independent\npersonality recognition models. Additionally, accurately modeling both the\ninterdependencies among interlocutors and the intra-dependencies within the\nspeaker in dialogues remains a significant issue. To address the first\nchallenge, we introduce personality trait interpolation for speaker data\naugmentation. For the second, we propose heterogeneous conversational graph\nnetworks to independently capture both contextual influences and inherent\npersonality traits. Evaluations on the RealPersonaChat corpus demonstrate our\nmethod's significant improvements over existing baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yahui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Haiyue Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawahara_T/0/1/0/all/0/1\">Tatsuya Kawahara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Deduplication For Socia Media Data Selection. (arXiv:2401.05883v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05883","description":"<p>Social media data is plagued by the redundancy problem caused by its noisy\nnature, leading to increased training time and model bias. To address this\nissue, we propose a novel approach called generative duplication. It aims to\nremove duplicate text from noisy social media data and mitigate model bias. By\ndoing so, it can improve social media language understanding performance and\nsave training time. Extensive experiments demonstrate that the proposed\ngenerative deduplication can effectively reduce training samples while\nimproving performance. This evidence suggests the effectiveness of generative\ndeduplication and its importance in social media language understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EpilepsyLLM: Domain-Specific Large Language Model Fine-tuned with Epilepsy Medical Knowledge. (arXiv:2401.05908v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05908","description":"<p>With large training datasets and massive amounts of computing sources, large\nlanguage models (LLMs) achieve remarkable performance in comprehensive and\ngenerative ability. Based on those powerful LLMs, the model fine-tuned with\ndomain-specific datasets posseses more specialized knowledge and thus is more\npractical like medical LLMs. However, the existing fine-tuned medical LLMs are\nlimited to general medical knowledge with English language. For\ndisease-specific problems, the model's response is inaccurate and sometimes\neven completely irrelevant, especially when using a language other than\nEnglish. In this work, we focus on the particular disease of Epilepsy with\nJapanese language and introduce a customized LLM termed as EpilepsyLLM. Our\nmodel is trained from the pre-trained LLM by fine-tuning technique using\ndatasets from the epilepsy domain. The datasets contain knowledge of basic\ninformation about disease, common treatment methods and drugs, and important\nnotes in life and work. The experimental results demonstrate that EpilepsyLLM\ncan provide more reliable and specialized medical knowledge responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xuyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qibin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1\">Toshihisa Tanaka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt-based mental health screening from social media text. (arXiv:2401.05912v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05912","description":"<p>This article presents a method for prompt-based mental health screening from\na large and noisy dataset of social media text. Our method uses GPT 3.5.\nprompting to distinguish publications that may be more relevant to the task,\nand then uses a straightforward bag-of-words text classifier to predict actual\nuser labels. Results are found to be on pair with a BERT mixture of experts\nclassifier, and incurring only a fraction of its computational costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santos_W/0/1/0/all/0/1\">Wesley Ramos dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paraboni_I/0/1/0/all/0/1\">Ivandre Paraboni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Teachers Can Use Large Language Models and Bloom's Taxonomy to Create Educational Quizzes. (arXiv:2401.05914v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05914","description":"<p>Question generation (QG) is a natural language processing task with an\nabundance of potential benefits and use cases in the educational domain. In\norder for this potential to be realized, QG systems must be designed and\nvalidated with pedagogical needs in mind. However, little research has assessed\nor designed QG approaches with the input from real teachers or students. This\npaper applies a large language model-based QG approach where questions are\ngenerated with learning goals derived from Bloom's taxonomy. The automatically\ngenerated questions are used in multiple experiments designed to assess how\nteachers use them in practice. The results demonstrate that teachers prefer to\nwrite quizzes with automatically generated questions, and that such quizzes\nhave no loss in quality compared to handwritten versions. Further, several\nmetrics indicate that automatically generated questions can even improve the\nquality of the quizzes created, showing the promise for large scale use of QG\nin the classroom setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elkins_S/0/1/0/all/0/1\">Sabina Elkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochmar_E/0/1/0/all/0/1\">Ekaterina Kochmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jackie C.K. Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serban_I/0/1/0/all/0/1\">Iulian Serban</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Unhelpfulness in Emotional Support Conversations with Multifaceted AI Feedback. (arXiv:2401.05928v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05928","description":"<p>An emotional support conversation system aims to alleviate users' emotional\ndistress and assist them in addressing their challenges. To generate supportive\nresponses, it is critical to consider multiple factors such as empathy, support\nstrategies, and response coherence, as established in prior methods.\nNonetheless, previous models occasionally generate unhelpful responses, which\nintend to provide support but display counterproductive effects. According to\npsychology and communication theories, poor performance in just one\ncontributing factor might cause a response to be unhelpful. From the model\ntraining perspective, since these models have not been exposed to unhelpful\nresponses during their training phase, they are unable to distinguish if the\ntokens they generate might result in unhelpful responses during inference. To\naddress this issue, we introduce a novel model-agnostic framework named\nmitigating unhelpfulness with multifaceted AI feedback for emotional support\n(Muffin). Specifically, Muffin employs a multifaceted AI feedback module to\nassess the helpfulness of responses generated by a specific model with\nconsideration of multiple factors. Using contrastive learning, it then reduces\nthe likelihood of the model generating unhelpful responses compared to the\nhelpful ones. Experimental results demonstrate that Muffin effectively\nmitigates the generation of unhelpful responses while slightly increasing\nresponse fluency and relevance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiashuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunpu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leong_C/0/1/0/all/0/1\">Chak Tou Leong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully. (arXiv:2401.05930v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05930","description":"<p>Large language models (LLMs) demonstrate great performance in text\ngeneration. However, LLMs are still suffering from hallucinations. In this\nwork, we propose an inference-time method, Self-Highlighted Hesitation (SH2),\nto help LLMs decode more truthfully. SH2 is based on a simple fact rooted in\ninformation theory that for an LLM, the tokens predicted with lower\nprobabilities are prone to be more informative than others. Our analysis shows\nthat the tokens assigned with lower probabilities by an LLM are more likely to\nbe closely related to factual information, such as nouns, proper nouns, and\nadjectives. Therefore, we propose to ''highlight'' the factual information by\nselecting the tokens with the lowest probabilities and concatenating them to\nthe original context, thus forcing the model to repeatedly read and hesitate on\nthese tokens before generation. During decoding, we also adopt contrastive\ndecoding to emphasize the difference in the output probabilities brought by the\nhesitation. Experimental results demonstrate that our SH2, requiring no\nadditional data or models, can effectively help LLMs elicit factual knowledge\nand distinguish hallucinated contexts. Significant and consistent improvements\nare achieved by SH2 for LLaMA-7b and LLaMA2-7b on multiple hallucination tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kai_J/0/1/0/all/0/1\">Jushi Kai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hai Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks. (arXiv:2401.05949v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05949","description":"<p>In-context learning, a paradigm bridging the gap between pre-training and\nfine-tuning, has demonstrated high efficacy in several NLP tasks, especially in\nfew-shot settings. Unlike traditional fine-tuning methods, in-context learning\nadapts pre-trained models to unseen tasks without updating any parameters.\nDespite being widely applied, in-context learning is vulnerable to malicious\nattacks. In this work, we raise security concerns regarding this paradigm. Our\nstudies demonstrate that an attacker can manipulate the behavior of large\nlanguage models by poisoning the demonstration context, without the need for\nfine-tuning the model. Specifically, we have designed a new backdoor attack\nmethod, named ICLAttack, to target large language models based on in-context\nlearning. Our method encompasses two types of attacks: poisoning demonstration\nexamples and poisoning prompts, which can make models behave in accordance with\npredefined intentions. ICLAttack does not require additional fine-tuning to\nimplant a backdoor, thus preserving the model's generality. Furthermore, the\npoisoned examples are correctly labeled, enhancing the natural stealth of our\nattack method. Extensive experimental results across several language models,\nranging in size from 1.3B to 40B parameters, demonstrate the effectiveness of\nour attack method, exemplified by a high average attack success rate of 95.0%\nacross the three datasets on OPT models. Our findings highlight the\nvulnerabilities of language models, and we hope this work will raise awareness\nof the possible security threats associated with in-context learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1\">Meihuizi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuan_L/0/1/0/all/0/1\">Luu Anh Tuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jinming Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM-as-a-Coauthor: The Challenges of Detecting LLM-Human Mixcase. (arXiv:2401.05952v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05952","description":"<p>With the remarkable development and widespread applications of large language\nmodels (LLMs), the use of machine-generated text (MGT) is becoming increasingly\ncommon. This trend brings potential risks, particularly to the quality and\ncompleteness of information in fields such as news and education. Current\nresearch predominantly addresses the detection of pure MGT without adequately\naddressing mixed scenarios including AI-revised Human-Written Text (HWT) or\nhuman-revised MGT. To confront this challenge, we introduce mixcase, a novel\nconcept representing a hybrid text form involving both machine-generated and\nhuman-generated content. We collected mixcase instances generated from multiple\ndaily text-editing scenarios and composed MixSet, the first dataset dedicated\nto studying these mixed modification scenarios. We conduct experiments to\nevaluate the efficacy of popular MGT detectors, assessing their effectiveness,\nrobustness, and generalization performance. Our findings reveal that existing\ndetectors struggle to identify mixcase as a separate class or MGT, particularly\nin dealing with subtle modifications and style adaptability. This research\nunderscores the urgent need for more fine-grain detectors tailored for mixcase,\noffering valuable insights for future research. Code and Models are available\nat https://github.com/Dongping-Chen/MixSet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chujie Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongping Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qihui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yao Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph Embedding. (arXiv:2401.05967v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05967","description":"<p>The primary aim of Knowledge Graph embeddings (KGE) is to learn\nlow-dimensional representations of entities and relations for predicting\nmissing facts. While rotation-based methods like RotatE and QuatE perform well\nin KGE, they face two challenges: limited model flexibility requiring\nproportional increases in relation size with entity dimension, and difficulties\nin generalizing the model for higher-dimensional rotations. To address these\nissues, we introduce OrthogonalE, a novel KGE model employing matrices for\nentities and block-diagonal orthogonal matrices with Riemannian optimization\nfor relations. This approach enhances the generality and flexibility of KGE\nmodels. The experimental results indicate that our new KGE model, OrthogonalE,\nis both general and flexible, significantly outperforming state-of-the-art KGE\nmodels while substantially reducing the number of relation parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yihua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimodaira_H/0/1/0/all/0/1\">Hidetoshi Shimodaira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combating Adversarial Attacks with Multi-Agent Debate. (arXiv:2401.05998v1 [cs.CL])","link":"http://arxiv.org/abs/2401.05998","description":"<p>While state-of-the-art language models have achieved impressive results, they\nremain susceptible to inference-time adversarial attacks, such as adversarial\nprompts generated by red teams <a href=\"/abs/2209.07858\">arXiv:2209.07858</a>. One approach proposed to\nimprove the general quality of language model generations is multi-agent\ndebate, where language models self-evaluate through discussion and feedback\n<a href=\"/abs/2305.14325\">arXiv:2305.14325</a>. We implement multi-agent debate between current\nstate-of-the-art language models and evaluate models' susceptibility to red\nteam attacks in both single- and multi-agent settings. We find that multi-agent\ndebate can reduce model toxicity when jailbroken or less capable models are\nforced to debate with non-jailbroken or more capable models. We also find\nmarginal improvements through the general usage of multi-agent interactions. We\nfurther perform adversarial prompt content classification via embedding\nclustering, and analyze the susceptibility of different models to different\ntypes of attack topics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chern_S/0/1/0/all/0/1\">Steffi Chern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhen Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Andy Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization. (arXiv:2401.06034v1 [cs.CL])","link":"http://arxiv.org/abs/2401.06034","description":"<p>Pretrained language models (PLMs) have shown remarkable generalization toward\nmultiple tasks and languages. Nonetheless, the generalization of PLMs towards\nunseen languages is poor, resulting in significantly worse language\nperformance, or even generating nonsensical responses that are comparable to a\nrandom baseline. This limitation has been a longstanding problem of PLMs\nraising the problem of diversity and equal access to language modeling\ntechnology. In this work, we solve this limitation by introducing LinguAlchemy,\na regularization technique that incorporates various aspects of languages\ncovering typological, geographical, and phylogenetic constraining the resulting\nrepresentation of PLMs to better characterize the corresponding linguistics\nconstraints. LinguAlchemy significantly improves the accuracy performance of\nmBERT and XLM-R on unseen languages by ~18% and ~2%, respectively compared to\nfully finetuned models and displaying a high degree of unseen language\ngeneralization. We further introduce AlchemyScale and AlchemyTune, extension of\nLinguAlchemy which adjusts the linguistic regularization weights automatically,\nalleviating the need for hyperparameter search. LinguAlchemy enables better\ncross-lingual generalization to unseen languages which is vital for better\ninclusivity and accessibility of PLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adilazuarda_M/0/1/0/all/0/1\">Muhammad Farid Adilazuarda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purwarianti_A/0/1/0/all/0/1\">Ayu Purwarianti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Data Contamination for Pre-training Language Models. (arXiv:2401.06059v1 [cs.CL])","link":"http://arxiv.org/abs/2401.06059","description":"<p>Language models pre-trained on web-scale corpora demonstrate impressive\ncapabilities on diverse downstream tasks. However, there is increasing concern\nwhether such capabilities might arise from evaluation datasets being included\nin the pre-training corpus -- a phenomenon known as \\textit{data contamination}\n-- in a manner that artificially increases performance. There has been little\nunderstanding of how this potential contamination might influence LMs'\nperformance on downstream tasks. In this paper, we explore the impact of data\ncontamination at the pre-training stage by pre-training a series of GPT-2\nmodels \\textit{from scratch}. We highlight the effect of both text\ncontamination (\\textit{i.e.}\\ input text of the evaluation samples) and\nground-truth contamination (\\textit{i.e.}\\ the prompts asked on the input and\nthe desired outputs) from evaluation data. We also investigate the effects of\nrepeating contamination for various downstream tasks. Additionally, we examine\nthe prevailing n-gram-based definitions of contamination within current LLM\nreports, pinpointing their limitations and inadequacy. Our findings offer new\ninsights into data contamination's effects on language model capabilities and\nunderscore the need for independent, comprehensive contamination assessments in\nLLM studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Minhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Ken Ziyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaeffer_R/0/1/0/all/0/1\">Rylan Schaeffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1\">Siru Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1\">Sanmi Koyejo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models. (arXiv:2401.06066v1 [cs.CL])","link":"http://arxiv.org/abs/2401.06066","description":"<p>In the era of large language models, Mixture-of-Experts (MoE) is a promising\narchitecture for managing computational costs when scaling up model parameters.\nHowever, conventional MoE architectures like GShard, which activate the top-$K$\nout of $N$ experts, face challenges in ensuring expert specialization, i.e.\neach expert acquires non-overlapping and focused knowledge. In response, we\npropose the DeepSeekMoE architecture towards ultimate expert specialization. It\ninvolves two principal strategies: (1) finely segmenting the experts into $mN$\nones and activating $mK$ from them, allowing for a more flexible combination of\nactivated experts; (2) isolating $K_s$ experts as shared ones, aiming at\ncapturing common knowledge and mitigating redundancy in routed experts.\nStarting from a modest scale with 2B parameters, we demonstrate that\nDeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5\ntimes the expert parameters and computation. In addition, DeepSeekMoE 2B nearly\napproaches the performance of its dense counterpart with the same number of\ntotal parameters, which set the upper bound of MoE models. Subsequently, we\nscale up DeepSeekMoE to 16B parameters and show that it achieves comparable\nperformance with LLaMA2 7B, with only about 40% of computations. Further, our\npreliminary efforts to scale up DeepSeekMoE to 145B parameters consistently\nvalidate its substantial advantages over the GShard architecture, and show its\nperformance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%)\nof computations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Damai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chengqi Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chenggang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">R.X. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Huazuo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Deli Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiashi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wangding Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xingkai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Y. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhenda Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Y.K. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Panpan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1\">Fuli Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_C/0/1/0/all/0/1\">Chong Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1\">Wenfeng Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEGO:Language Enhanced Multi-modal Grounding Model. (arXiv:2401.06071v1 [cs.CV])","link":"http://arxiv.org/abs/2401.06071","description":"<p>Multi-modal large language models have demonstrated impressive performance\nacross various tasks in different modalities. However, existing multi-modal\nmodels primarily emphasize capturing global information within each modality\nwhile neglecting the importance of perceiving local information across\nmodalities. Consequently, these models lack the ability to effectively\nunderstand the fine-grained details of input data, limiting their performance\nin tasks that require a more nuanced understanding. To address this limitation,\nthere is a compelling need to develop models that enable fine-grained\nunderstanding across multiple modalities, thereby enhancing their applicability\nto a wide range of tasks. In this paper, we propose LEGO, a language enhanced\nmulti-modal grounding model. Beyond capturing global information like other\nmulti-modal models, our proposed model excels at tasks demanding a detailed\nunderstanding of local information within the input. It demonstrates precise\nidentification and localization of specific regions in images or moments in\nvideos. To achieve this objective, we design a diversified dataset construction\npipeline, resulting in a multi-modal, multi-granularity dataset for model\ntraining. The code, dataset, and demo of our model can be found at https:\n//github.com/lzw-lzw/LEGO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaowei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Hang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yiqing Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1\">Qi Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Ran Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junting Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zefeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_V/0/1/0/all/0/1\">Van Tu Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhida Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain of History: Learning and Forecasting with LLMs for Temporal Knowledge Graph Completion. (arXiv:2401.06072v1 [cs.AI])","link":"http://arxiv.org/abs/2401.06072","description":"<p>Temporal Knowledge Graph Completion (TKGC) is a challenging task of\npredicting missing event links at future timestamps by leveraging established\ntemporal structural knowledge. Given the formidable generative capabilities\ninherent in LLMs (LLMs), this paper proposes a novel approach to conceptualize\ntemporal link prediction as an event generation task within the context of a\nhistorical event chain. We employ efficient fine-tuning methods to make LLMs\nadapt to specific graph textual information and patterns discovered in temporal\ntimelines. Furthermore, we introduce structure-based historical data\naugmentation and the integration of reverse knowledge to emphasize LLMs'\nawareness of structural information, thereby enhancing their reasoning\ncapabilities. We conduct thorough experiments on multiple widely used datasets\nand find that our fine-tuned model outperforms existing embedding-based models\non multiple metrics, achieving SOTA results. We also carry out sufficient\nablation experiments to explore the key influencing factors when LLMs perform\nstructured temporal knowledge inference tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Ruilin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_T/0/1/0/all/0/1\">Tianle Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zicheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiayi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint. (arXiv:2401.06081v1 [cs.CL])","link":"http://arxiv.org/abs/2401.06081","description":"<p>Reinforcement learning (RL) has been widely used in training large language\nmodels~(LLMs) for preventing unexpected outputs, \\eg reducing harmfulness and\nerrors. However, existing RL methods mostly adopt the instance-level reward,\nwhich is unable to provide fine-grained supervision for complex reasoning\ntasks, and can not focus on the few key tokens that lead to the incorrectness.\nTo address it, we propose a new RL method named \\textbf{RLMEC} that\nincorporates a generative model as the reward model, which is trained by the\nerroneous solution rewriting task under the minimum editing constraint, and can\nproduce token-level rewards for RL training. Based on the generative reward\nmodel, we design the token-level RL objective for training and an\nimitation-based regularization for stabilizing RL process. And the both\nobjectives focus on the learning of the key tokens for the erroneous solution,\nreducing the effect of other unimportant tokens. The experiment results on\nmathematical tasks and question-answering tasks have demonstrated the\neffectiveness of our approach. Our code and data are available at\n\\url{https://github.com/RUCAIBox/RLMEC}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhipeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Junchen Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fuzheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Di Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autocompletion of Chief Complaints in the Electronic Health Records using Large Language Models. (arXiv:2401.06088v1 [cs.CL])","link":"http://arxiv.org/abs/2401.06088","description":"<p>The Chief Complaint (CC) is a crucial component of a patient's medical record\nas it describes the main reason or concern for seeking medical care. It\nprovides critical information for healthcare providers to make informed\ndecisions about patient care. However, documenting CCs can be time-consuming\nfor healthcare providers, especially in busy emergency departments. To address\nthis issue, an autocompletion tool that suggests accurate and well-formatted\nphrases or sentences for clinical notes can be a valuable resource for triage\nnurses. In this study, we utilized text generation techniques to develop\nmachine learning models using CC data. In our proposed work, we train a Long\nShort-Term Memory (LSTM) model and fine-tune three different variants of\nBiomedical Generative Pretrained Transformers (BioGPT), namely\nmicrosoft/biogpt, microsoft/BioGPT-Large, and microsoft/BioGPT-Large-PubMedQA.\nAdditionally, we tune a prompt by incorporating exemplar CC sentences,\nutilizing the OpenAI API of GPT-4. We evaluate the models' performance based on\nthe perplexity score, modified BERTScore, and cosine similarity score. The\nresults show that BioGPT-Large exhibits superior performance compared to the\nother models. It consistently achieves a remarkably low perplexity score of\n1.65 when generating CC, whereas the baseline LSTM model achieves the best\nperplexity score of 170. Further, we evaluate and assess the proposed models'\nperformance and the outcome of GPT-4.0. Our study demonstrates that utilizing\nLLMs such as BioGPT, leads to the development of an effective autocompletion\ntool for generating CC documentation in healthcare settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_K/0/1/0/all/0/1\">K M Sajjadul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nipu_A/0/1/0/all/0/1\">Ayesha Siddika Nipu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madiraju_P/0/1/0/all/0/1\">Praveen Madiraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_P/0/1/0/all/0/1\">Priya Deshpande</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v1 [cs.CL])","link":"http://arxiv.org/abs/2401.06102","description":"<p>Inspecting the information encoded in hidden representations of large\nlanguage models (LLMs) can explain models' behavior and verify their alignment\nwith human values. Given the capabilities of LLMs in generating\nhuman-understandable text, we propose leveraging the model itself to explain\nits internal representations in natural language. We introduce a framework\ncalled Patchscopes and show how it can be used to answer a wide range of\nresearch questions about an LLM's computation. We show that prior\ninterpretability methods based on projecting representations into the\nvocabulary space and intervening on the LLM computation, can be viewed as\nspecial instances of this framework. Moreover, several of their shortcomings\nsuch as failure in inspecting early layers or lack of expressivity can be\nmitigated by a Patchscope. Beyond unifying prior inspection techniques,\nPatchscopes also opens up new possibilities such as using a more capable model\nto explain the representations of a smaller model, and unlocks new applications\nsuch as self-correction in multi-hop reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghandeharioun_A/0/1/0/all/0/1\">Asma Ghandeharioun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1\">Avi Caciularu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pearce_A/0/1/0/all/0/1\">Adam Pearce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixon_L/0/1/0/all/0/1\">Lucas Dixon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers are Multi-State RNNs. (arXiv:2401.06104v1 [cs.CL])","link":"http://arxiv.org/abs/2401.06104","description":"<p>Transformers are considered conceptually different compared to the previous\ngeneration of state-of-the-art NLP models - recurrent neural networks (RNNs).\nIn this work, we demonstrate that decoder-only transformers can in fact be\nconceptualized as infinite multi-state RNNs - an RNN variant with unlimited\nhidden state size. We further show that pretrained transformers can be\nconverted into $\\textit{finite}$ multi-state RNNs by fixing the size of their\nhidden state. We observe that several existing transformers cache compression\ntechniques can be framed as such conversion policies, and introduce a novel\npolicy, TOVA, which is simpler compared to these policies. Our experiments with\nseveral long range tasks indicate that TOVA outperforms all other baseline\npolicies, while being nearly on par with the full (infinite) model, and using\nin some cases only $\\frac{1}{8}$ of the original cache size. Our results\nindicate that transformer decoder LLMs often behave in practice as RNNs. They\nalso lay out the option of mitigating one of their most painful computational\nbottlenecks - the size of their cache memory. We publicly release our code at\nhttps://github.com/schwartz-lab-NLP/TOVA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oren_M/0/1/0/all/0/1\">Matanel Oren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassid_M/0/1/0/all/0/1\">Michael Hassid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PALP: Prompt Aligned Personalization of Text-to-Image Models. (arXiv:2401.06105v1 [cs.CV])","link":"http://arxiv.org/abs/2401.06105","description":"<p>Content creators often aim to create personalized images using personal\nsubjects that go beyond the capabilities of conventional text-to-image models.\nAdditionally, they may want the resulting image to encompass a specific\nlocation, style, ambiance, and more. Existing personalization methods may\ncompromise personalization ability or the alignment to complex textual prompts.\nThis trade-off can impede the fulfillment of user prompts and subject fidelity.\nWe propose a new approach focusing on personalization methods for a\n\\emph{single} prompt to address this issue. We term our approach prompt-aligned\npersonalization. While this may seem restrictive, our method excels in\nimproving text alignment, enabling the creation of images with complex and\nintricate prompts, which may pose a challenge for current techniques. In\nparticular, our method keeps the personalized model aligned with a target\nprompt using an additional score distillation sampling term. We demonstrate the\nversatility of our method in multi- and single-shot settings and further show\nthat it can compose multiple subjects or use inspiration from reference images,\nsuch as artworks. We compare our approach quantitatively and qualitatively with\nexisting baselines and state-of-the-art techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arar_M/0/1/0/all/0/1\">Moab Arar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voynov_A/0/1/0/all/0/1\">Andrey Voynov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hertz_A/0/1/0/all/0/1\">Amir Hertz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avrahami_O/0/1/0/all/0/1\">Omri Avrahami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fruchter_S/0/1/0/all/0/1\">Shlomi Fruchter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pritch_Y/0/1/0/all/0/1\">Yael Pritch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1\">Ariel Shamir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings. (arXiv:2401.06112v1 [cs.CL])","link":"http://arxiv.org/abs/2401.06112","description":"<p>Word embedding is one of the most important components in natural language\nprocessing, but interpreting high-dimensional embeddings remains a challenging\nproblem. To address this problem, Independent Component Analysis (ICA) is\nidentified as an effective solution. ICA-transformed word embeddings reveal\ninterpretable semantic axes; however, the order of these axes are arbitrary. In\nthis study, we focus on this property and propose a novel method, Axis Tour,\nwhich optimizes the order of the axes. Inspired by Word Tour, a one-dimensional\nword embedding method, we aim to improve the clarity of the word embedding\nspace by maximizing the semantic continuity of the axes. Furthermore, we show\nthrough experiments on downstream tasks that Axis Tour constructs better\nlow-dimensional embeddings compared to both PCA and ICA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamagiwa_H/0/1/0/all/0/1\">Hiroaki Yamagiwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takase_Y/0/1/0/all/0/1\">Yusuke Takase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimodaira_H/0/1/0/all/0/1\">Hidetoshi Shimodaira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extreme Compression of Large Language Models via Additive Quantization. (arXiv:2401.06118v1 [cs.LG])","link":"http://arxiv.org/abs/2401.06118","description":"<p>The emergence of accurate open large language models (LLMs) has led to a race\ntowards quantization techniques for such models enabling execution on end-user\ndevices. In this paper, we revisit the problem of \"extreme\" LLM\ncompression--defined as targeting extremely low bit counts, such as 2 to 3 bits\nper parameter, from the point of view of classic methods in Multi-Codebook\nQuantization (MCQ). Our work builds on top of Additive Quantization, a classic\nalgorithm from the MCQ family, and adapts it to the quantization of language\nmodels. The resulting algorithm advances the state-of-the-art in LLM\ncompression, outperforming all recently-proposed techniques in terms of\naccuracy at a given compression budget. For instance, when compressing Llama 2\nmodels to 2 bits per parameter, our algorithm quantizes the 7B model to 6.93\nperplexity (a 1.29 improvement relative to the best prior work, and 1.81 points\nfrom FP16), the 13B model to 5.70 perplexity (a .36 improvement) and the 70B\nmodel to 3.94 perplexity (a .22 improvement) on WikiText2. We release our\nimplementation of Additive Quantization for Language Models AQLM as a baseline\nto facilitate future research in LLM quantization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Egiazarian_V/0/1/0/all/0/1\">Vage Egiazarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panferov_A/0/1/0/all/0/1\">Andrei Panferov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznedelev_D/0/1/0/all/0/1\">Denis Kuznedelev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frantar_E/0/1/0/all/0/1\">Elias Frantar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1\">Artem Babenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1\">Dan Alistarh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TOFU: A Task of Fictitious Unlearning for LLMs. (arXiv:2401.06121v1 [cs.LG])","link":"http://arxiv.org/abs/2401.06121","description":"<p>Large language models trained on massive corpora of data from the web can\nmemorize and reproduce sensitive or private data raising both legal and ethical\nconcerns. Unlearning, or tuning models to forget information present in their\ntraining data, provides us with a way to protect private data after training.\nAlthough several methods exist for such unlearning, it is unclear to what\nextent they result in models equivalent to those where the data to be forgotten\nwas never learned in the first place. To address this challenge, we present\nTOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen\nour understanding of unlearning. We offer a dataset of 200 diverse synthetic\nauthor profiles, each consisting of 20 question-answer pairs, and a subset of\nthese profiles called the forget set that serves as the target for unlearning.\nWe compile a suite of metrics that work together to provide a holistic picture\nof unlearning efficacy. Finally, we provide a set of baseline results from\nexisting unlearning algorithms. Importantly, none of the baselines we consider\nshow effective unlearning motivating continued efforts to develop approaches\nfor unlearning that effectively tune models so that they truly behave as if\nthey were never trained on the forget data at all.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maini_P/0/1/0/all/0/1\">Pratyush Maini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhili Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwarzschild_A/0/1/0/all/0/1\">Avi Schwarzschild</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1\">J. Zico Kolter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear Spaces of Meanings: Compositional Structures in Vision-Language Models. (arXiv:2302.14383v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.14383","description":"<p>We investigate compositional structures in data embeddings from pre-trained\nvision-language models (VLMs). Traditionally, compositionality has been\nassociated with algebraic operations on embeddings of words from a pre-existing\nvocabulary. In contrast, we seek to approximate representations from an encoder\nas combinations of a smaller set of vectors in the embedding space. These\nvectors can be seen as \"ideal words\" for generating concepts directly within\nthe embedding space of the model. We first present a framework for\nunderstanding compositional structures from a geometric perspective. We then\nexplain what these compositional structures entail probabilistically in the\ncase of VLM embeddings, providing intuitions for why they arise in practice.\nFinally, we empirically explore these structures in CLIP's embeddings and we\nevaluate their usefulness for solving different vision-language tasks such as\nclassification, debiasing, and retrieval. Our results show that simple linear\nalgebraic operations on embedding vectors can be used as compositional and\ninterpretable methods for regulating the behavior of VLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trager_M/0/1/0/all/0/1\">Matthew Trager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perera_P/0/1/0/all/0/1\">Pramuditha Perera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zancato_L/0/1/0/all/0/1\">Luca Zancato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1\">Alessandro Achille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_P/0/1/0/all/0/1\">Parminder Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Taxonomy of Foundation Model based Systems through the Lens of Software Architecture. (arXiv:2305.05352v5 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2305.05352","description":"<p>The recent release of large language model (LLM) based chatbots, such as\nChatGPT, has attracted huge interest in foundation models. It is widely\nbelieved that foundation models will serve as the fundamental building blocks\nfor future AI systems. As foundation models are in their early stages, the\ndesign of foundation model based systems has not yet been systematically\nexplored. There is limited understanding about the impact of introducing\nfoundation models in software architecture. Therefore, in this paper, we\npropose a taxonomy of foundation model based systems, which classifies and\ncompares the characteristics of foundation models and design options of\nfoundation model based systems. Our taxonomy comprises three categories: the\npretraining and adaptation of foundation models, the architecture design of\nfoundation model based systems, and responsible-AI-by-design. This taxonomy can\nserve as concrete guidance for making major architectural design decisions when\ndesigning foundation model based systems and highlights trade-offs arising from\ndesign decisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qinghua Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Liming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1\">Zhenchang Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whittle_J/0/1/0/all/0/1\">Jon Whittle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Persian Typographical Error Type Detection Using Deep Neural Networks on Algorithmically-Generated Misspellings. (arXiv:2305.11731v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.11731","description":"<p>Spelling correction is a remarkable challenge in the field of natural\nlanguage processing. The objective of spelling correction tasks is to recognize\nand rectify spelling errors automatically. The development of applications that\ncan effectually diagnose and correct Persian spelling and grammatical errors\nhas become more important in order to improve the quality of Persian text. The\nTypographical Error Type Detection in Persian is a relatively understudied\narea. Therefore, this paper presents a compelling approach for detecting\ntypographical errors in Persian texts. Our work includes the presentation of a\npublicly available dataset called FarsTypo, which comprises 3.4 million words\narranged in chronological order and tagged with their corresponding\npart-of-speech. These words cover a wide range of topics and linguistic styles.\nWe develop an algorithm designed to apply Persian-specific errors to a scalable\nportion of these words, resulting in a parallel dataset of correct and\nincorrect words. By leveraging FarsTypo, we establish a strong foundation and\nconduct a thorough comparison of various methodologies employing different\narchitectures. Additionally, we introduce a groundbreaking Deep Sequential\nNeural Network that utilizes both word and character embeddings, along with\nbidirectional LSTM layers, for token classification aimed at detecting\ntypographical errors across 51 distinct classes. Our approach is contrasted\nwith highly advanced industrial systems that, unlike this study, have been\ndeveloped using a diverse range of resources. The outcomes of our final method\nproved to be highly competitive, achieving an accuracy of 97.62%, precision of\n98.83%, recall of 98.61%, and surpassing others in terms of speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mohammad Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faili_H/0/1/0/all/0/1\">Heshaam Faili</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heterogeneous Value Alignment Evaluation for Large Language Models. (arXiv:2305.17147v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17147","description":"<p>The emergent capabilities of Large Language Models (LLMs) have made it\ncrucial to align their values with those of humans. However, current\nmethodologies typically attempt to assign value as an attribute to LLMs, yet\nlack attention to the ability to pursue value and the importance of\ntransferring heterogeneous values in specific practical applications. In this\npaper, we propose a Heterogeneous Value Alignment Evaluation (HVAE) system,\ndesigned to assess the success of aligning LLMs with heterogeneous values.\nSpecifically, our approach first brings the Social Value Orientation (SVO)\nframework from social psychology, which corresponds to how much weight a person\nattaches to the welfare of others in relation to their own. We then assign the\nLLMs with different social values and measure whether their behaviors align\nwith the inducing values. We conduct evaluations with new auto-metric\n\\textit{value rationality} to represent the ability of LLMs to align with\nspecific values. Evaluating the value rationality of five mainstream LLMs, we\ndiscern a propensity in LLMs towards neutral values over pronounced personal\nvalues. By examining the behavior of these LLMs, we contribute to a deeper\ninsight into the value alignment of LLMs within a heterogeneous value system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaowei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ceyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1\">Siyuan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_Z/0/1/0/all/0/1\">Ziqi Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaodong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Tuning Language Models with Just Forward Passes. (arXiv:2305.17333v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.17333","description":"<p>Fine-tuning language models (LMs) has yielded success on diverse downstream\ntasks, but as LMs grow in size, backpropagation requires a prohibitively large\namount of memory. Zeroth-order (ZO) methods can in principle estimate gradients\nusing only two forward passes but are theorized to be catastrophically slow for\noptimizing large models. In this work, we propose a memory-efficient\nzerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate\nin-place, thereby fine-tuning LMs with the same memory footprint as inference.\nFor example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter\nmodel, whereas fine-tuning with backpropagation can train only a 2.7B LM with\nthe same budget. We conduct comprehensive experiments across model types\n(masked and autoregressive LMs), model scales (up to 66B), and downstream tasks\n(classification, multiple-choice, and generation). Our results demonstrate that\n(1) MeZO significantly outperforms in-context learning and linear probing; (2)\nMeZO achieves comparable performance to fine-tuning with backpropagation across\nmultiple tasks, with up to 12x memory reduction and up to 2x GPU-hour reduction\nin our implementation; (3) MeZO is compatible with both full-parameter and\nparameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO\ncan effectively optimize non-differentiable objectives (e.g., maximizing\naccuracy or F1). We support our empirical findings with theoretical insights,\nhighlighting how adequate pre-training and task prompts enable MeZO to\nfine-tune huge models, despite classical ZO analyses suggesting otherwise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malladi_S/0/1/0/all/0/1\">Sadhika Malladi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nichani_E/0/1/0/all/0/1\">Eshaan Nichani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damian_A/0/1/0/all/0/1\">Alex Damian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jason D. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Sanjeev Arora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Distractor and Feedback Generation for Math Multiple-choice Questions via In-context Learning. (arXiv:2308.03234v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.03234","description":"<p>Multiple-choice questions (MCQs) are ubiquitous in almost all levels of\neducation since they are easy to administer, grade, and are a reliable form of\nassessment. An important aspect of MCQs is the distractors, i.e., incorrect\noptions that are designed to target specific misconceptions or insufficient\nknowledge among students. To date, the task of crafting high-quality\ndistractors has largely remained a labor-intensive process for teachers and\nlearning content designers, which has limited scalability. In this work, we\nexplore the task of automated distractor and corresponding feedback message\ngeneration in math MCQs using large language models. We establish a formulation\nof these two tasks and propose a simple, in-context learning-based solution.\nMoreover, we propose generative AI-based metrics for evaluating the quality of\nthe feedback messages. We conduct extensive experiments on these tasks using a\nreal-world MCQ dataset. Our findings suggest that there is a lot of room for\nimprovement in automated distractor and feedback generation; based on these\nfindings, we outline several directions for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McNichols_H/0/1/0/all/0/1\">Hunter McNichols</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wanyong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaewook Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarlatos_A/0/1/0/all/0/1\">Alexander Scarlatos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_D/0/1/0/all/0/1\">Digory Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodhead_S/0/1/0/all/0/1\">Simon Woodhead</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_A/0/1/0/all/0/1\">Andrew Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CORAL: Expert-Curated medical Oncology Reports to Advance Language Model Inference. (arXiv:2308.03853v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.03853","description":"<p>Both medical care and observational studies in oncology require a thorough\nunderstanding of a patient's disease progression and treatment history, often\nelaborately documented in clinical notes. Despite their vital role, no current\noncology information representation and annotation schema fully encapsulates\nthe diversity of information recorded within these notes. Although large\nlanguage models (LLMs) have recently exhibited impressive performance on\nvarious medical natural language processing tasks, due to the current lack of\ncomprehensively annotated oncology datasets, an extensive evaluation of LLMs in\nextracting and reasoning with the complex rhetoric in oncology notes remains\nunderstudied. We developed a detailed schema for annotating textual oncology\ninformation, encompassing patient characteristics, tumor characteristics,\ntests, treatments, and temporality. Using a corpus of 40 de-identified breast\nand pancreatic cancer progress notes at University of California, San\nFrancisco, we applied this schema to assess the zero-shot abilities of three\nrecent LLMs (GPT-4, GPT-3.5-turbo, and FLAN-UL2) to extract detailed\noncological history from two narrative sections of clinical progress notes. Our\nteam annotated 9028 entities, 9986 modifiers, and 5312 relationships. The GPT-4\nmodel exhibited overall best performance, with an average BLEU score of 0.73,\nan average ROUGE score of 0.72, an exact-match F1-score of 0.51, and an average\naccuracy of 68% on complex tasks (expert manual evaluation on subset). Notably,\nit was proficient in tumor characteristic and medication extraction, and\ndemonstrated superior performance in relational inference like adverse event\ndetection. However, further improvements are needed before using it to reliably\nextract important facts from cancer progress notes needed for clinical\nresearch, complex population management, and documenting quality patient care.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sushil_M/0/1/0/all/0/1\">Madhumita Sushil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_V/0/1/0/all/0/1\">Vanessa E. Kennedy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandair_D/0/1/0/all/0/1\">Divneet Mandair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_B/0/1/0/all/0/1\">Brenda Y. Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zack_T/0/1/0/all/0/1\">Travis Zack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butte_A/0/1/0/all/0/1\">Atul J. Butte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task Selection and Assignment for Multi-modal Multi-task Dialogue Act Classification with Non-stationary Multi-armed Bandits. (arXiv:2309.09832v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.09832","description":"<p>Multi-task learning (MTL) aims to improve the performance of a primary task\nby jointly learning with related auxiliary tasks. Traditional MTL methods\nselect tasks randomly during training. However, both previous studies and our\nresults suggest that such a random selection of tasks may not be helpful, and\ncan even be harmful to performance. Therefore, new strategies for task\nselection and assignment in MTL need to be explored. This paper studies the\nmulti-modal, multi-task dialogue act classification task, and proposes a method\nfor selecting and assigning tasks based on non-stationary multi-armed bandits\n(MAB) with discounted Thompson Sampling (TS) using Gaussian priors. Our\nexperimental results show that in different training stages, different tasks\nhave different utility. Our proposed method can effectively identify the task\nutility, actively avoid useless or harmful tasks, and realise the task\nassignment during training. Our proposed method is significantly superior in\nterms of UAR and F1 to the single-task and multi-task baselines with p-values &lt;\n0.05. Further analysis of experiments indicates that for the dataset with the\ndata imbalance problem, our proposed method has significantly higher stability\nand can obtain consistent and decent performance for minority classes. Our\nproposed method is superior to the current state-of-the-art model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breaking through Deterministic Barriers: Randomized Pruning Mask Generation and Selection. (arXiv:2310.13183v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2310.13183","description":"<p>It is widely acknowledged that large and sparse models have higher accuracy\nthan small and dense models under the same model size constraints. This\nmotivates us to train a large model and then remove its redundant neurons or\nweights by pruning. Most existing works pruned the networks in a deterministic\nway, the performance of which solely depends on a single pruning criterion and\nthus lacks variety. Instead, in this paper, we propose a model pruning strategy\nthat first generates several pruning masks in a designed random way.\nSubsequently, along with an effective mask-selection rule, the optimal mask is\nchosen from the pool of mask candidates. To further enhance efficiency, we\nintroduce an early mask evaluation strategy, mitigating the overhead associated\nwith training multiple masks. Our extensive experiments demonstrate that this\napproach achieves state-of-the-art performance across eight datasets from GLUE,\nparticularly excelling at high levels of sparsity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Weizhi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1\">Qi Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dongkuan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models. (arXiv:2310.13191v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.13191","description":"<p>The pruning objective has recently extended beyond accuracy and sparsity to\nrobustness in language models. Despite this, existing methods struggle to\nenhance robustness against adversarial attacks when continually increasing\nmodel sparsity and require a retraining process. As humans step into the era of\nlarge language models, these issues become increasingly prominent. This paper\nproposes that the robustness of language models is proportional to the extent\nof pre-trained knowledge they encompass. Accordingly, we introduce a\npost-training pruning strategy designed to faithfully replicate the embedding\nspace and feature space of dense language models, aiming to conserve more\npre-trained knowledge during the pruning process. In this setup, each layer's\nreconstruction error not only originates from itself but also includes\ncumulative error from preceding layers, followed by an adaptive rectification.\nCompared to other state-of-art baselines, our approach demonstrates a superior\nbalance between accuracy, sparsity, robustness, and pruning cost with BERT on\ndatasets SST2, IMDB, and AGNews, marking a significant stride towards robust\npruning in language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1\">Qi Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dongkuan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek. (arXiv:2311.00541v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.00541","description":"<p>Word meanings change over time, and word senses evolve, emerge or die out in\nthe process. For ancient languages, where the corpora are often small and\nsparse, modelling such changes accurately proves challenging, and quantifying\nuncertainty in sense-change estimates consequently becomes important. GASC\n(Genre-Aware Semantic Change) and DiSC (Diachronic Sense Change) are existing\ngenerative models that have been used to analyse sense change for target words\nfrom an ancient Greek text corpus, using unsupervised learning without the help\nof any pre-training. These models represent the senses of a given target word\nsuch as \"kosmos\" (meaning decoration, order or world) as distributions over\ncontext words, and sense prevalence as a distribution over senses. The models\nare fitted using Markov Chain Monte Carlo (MCMC) methods to measure temporal\nchanges in these representations. In this paper, we introduce EDiSC, an\nEmbedded DiSC model, which combines word embeddings with DiSC to provide\nsuperior model performance. We show empirically that EDiSC offers improved\npredictive accuracy, ground-truth recovery and uncertainty quantification, as\nwell as better sampling efficiency and scalability properties with MCMC\nmethods. We also discuss the challenges of fitting these models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zafar_S/0/1/0/all/0/1\">Schyan Zafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicholls_G/0/1/0/all/0/1\">Geoff K. Nicholls</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CausalCite: A Causal Formulation of Paper Citations. (arXiv:2311.02790v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.02790","description":"<p>Evaluating the significance of a paper is pivotal yet challenging for the\nscientific community. While the citation count is the most commonly used proxy\nfor this purpose, they are widely criticized for failing to accurately reflect\na paper's true impact. In this work, we propose a causal inference method,\nTextMatch, which adapts the traditional matching framework to high-dimensional\ntext embeddings. Specifically, we encode each paper using the text embeddings\nby large language models (LLMs), extract similar samples by cosine similarity,\nand synthesize a counterfactual sample by the weighted average of similar\npapers according to their similarity values. We apply the resulting metric,\ncalled CausalCite, as a causal formulation of paper citations. We show its\neffectiveness on various criteria, such as high correlation with paper impact\nas reported by scientific experts on a previous dataset of 1K papers,\n(test-of-time) awards for past papers, and its stability across various\nsub-fields of AI. We also provide a set of findings that can serve as suggested\nways for future researchers to use our metric for a better understanding of a\npaper's quality. Our code and data are at\nhttps://github.com/causalNLP/causal-cite.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_I/0/1/0/all/0/1\">Ishan Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokhtarian_E/0/1/0/all/0/1\">Ehsan Mokhtarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Siyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALYMPICS: Language Agents Meet Game Theory -- Exploring Strategic Decision-Making with AI Agents. (arXiv:2311.03220v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.03220","description":"<p>This paper introduces Alympics (Olympics for Agents), a systematic simulation\nframework utilizing Large Language Model (LLM) agents for game theory research.\nAlympics creates a versatile platform for studying complex game theory\nproblems, bridging the gap between theoretical game theory and empirical\ninvestigations by providing a controlled environment for simulating human-like\nstrategic interactions with LLM agents. In our pilot case study, the \"Water\nAllocation Challenge,\" we explore Alympics through a challenging strategic game\nfocused on the multi-round auction on scarce survival resources. This study\ndemonstrates the framework's ability to qualitatively and quantitatively\nanalyze game determinants, strategies, and outcomes. Additionally, we conduct a\ncomprehensive human assessment and an in-depth evaluation of LLM agents in\nstrategic decision-making scenarios. Our findings not only expand the\nunderstanding of LLM agents' proficiency in emulating human strategic behavior\nbut also highlight their potential in advancing game theory knowledge, thereby\nenriching our understanding of both game theory and empowering further research\ninto strategic decision-making domains with LLM agents. Codes, prompts, and all\nrelated resources are available at https://github.com/microsoft/Alympics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shaoguang Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuzhe Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenshan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fengyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fovea Transformer: Efficient Long-Context Modeling with Structured Fine-to-Coarse Attention. (arXiv:2311.07102v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.07102","description":"<p>The quadratic complexity of self-attention in Transformers has hindered the\nprocessing of long text. To alleviate this problem, previous works have\nproposed to sparsify the attention matrix, taking advantage of the observation\nthat crucial information about a token can be derived from its neighbors. These\nmethods typically combine one or another form of local attention and global\nattention. Such combinations introduce abrupt changes in contextual granularity\nwhen going from local to global, which may be undesirable. We believe that a\nsmoother transition could potentially enhance model's ability to capture\nlong-context dependencies. In this study, we introduce Fovea Transformer, a\nlong-context focused transformer that addresses the challenges of capturing\nglobal dependencies while maintaining computational efficiency. To achieve\nthis, we construct a multi-scale tree from the input sequence, and use\nrepresentations of context tokens with a progressively coarser granularity in\nthe tree, as their distance to the query token increases. We evaluate our model\non three long-context summarization tasks\\footnote{Our code is publicly\navailable at: \\textit{https://github.com/ZiweiHe/Fovea-Transformer}}. It\nachieves state-of-the-art performance on two of them, and competitive results\non the third with mixed improvement and setback of the evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Ziwei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jian Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Le Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1\">Jingwen Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bo Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning. (arXiv:2312.05720v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2312.05720","description":"<p>Federated learning (FL) emphasizes decentralized training by storing data\nlocally and sending only model updates, underlining user privacy. Recently, a\nline of works on privacy attacks impairs user privacy by extracting sensitive\ntraining text from language models in the context of FL. Yet, these attack\ntechniques face distinct hurdles: some work chiefly with limited batch sizes\n(e.g., batch size of 1), and others are easily detectable. This paper\nintroduces an innovative approach that is challenging to detect, significantly\nenhancing the recovery rate of text in various batch-size settings. Building on\nfundamental gradient matching and domain prior knowledge, we enhance the attack\nby recovering the input of the Pooler layer of language models, which enables\nus to provide additional supervised signals at the feature level. Unlike\ngradient data, these signals do not average across sentences and tokens,\nthereby offering more nuanced and effective insights. We benchmark our method\nusing text classification tasks on datasets such as CoLA, SST-2, and Rotten\nTomatoes. Across different batch sizes and models, our approach consistently\noutperforms previous state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1\">Qi Lei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TaCo: Targeted Concept Removal in Output Embeddings for NLP via Information Theory and Explainability. (arXiv:2312.06499v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.06499","description":"<p>The fairness of Natural Language Processing (NLP) models has emerged as a\ncrucial concern. Information theory indicates that to achieve fairness, a model\nshould not be able to predict sensitive variables, such as gender, ethnicity,\nand age. However, information related to these variables often appears\nimplicitly in language, posing a challenge in identifying and mitigating biases\neffectively. To tackle this issue, we present a novel approach that operates at\nthe embedding level of an NLP model, independent of the specific architecture.\nOur method leverages insights from recent advances in XAI techniques and\nemploys an embedding transformation to eliminate implicit information from a\nselected variable. By directly manipulating the embeddings in the final layer,\nour approach enables a seamless integration into existing models without\nrequiring significant modifications or retraining. In evaluation, we show that\nthe proposed post-hoc approach significantly reduces gender-related\nassociations in NLP models while preserving the overall performance and\nfunctionality of the models. An implementation of our method is available:\nhttps://github.com/fanny-jourdan/TaCo\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jourdan_F/0/1/0/all/0/1\">Fanny Jourdan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethune_L/0/1/0/all/0/1\">Louis B&#xe9;thune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picard_A/0/1/0/all/0/1\">Agustin Picard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risser_L/0/1/0/all/0/1\">Laurent Risser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asher_N/0/1/0/all/0/1\">Nicholas Asher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extending Whisper with prompt tuning to target-speaker ASR. (arXiv:2312.08079v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.08079","description":"<p>Target-speaker automatic speech recognition (ASR) aims to transcribe the\ndesired speech of a target speaker from multi-talker overlapped utterances.\nMost of the existing target-speaker ASR (TS-ASR) methods involve either\ntraining from scratch or fully fine-tuning a pre-trained model, leading to\nsignificant training costs and becoming inapplicable to large foundation\nmodels. This work leverages prompt tuning, a parameter-efficient fine-tuning\napproach, to extend Whisper, a large-scale single-talker ASR model, to TS-ASR.\nVariants of prompt tuning approaches along with their configurations are\nexplored and optimized for TS-ASR.Experimental results show that prompt tuning\ncan achieve performance comparable to state-of-the-art full training approaches\nwhile only requiring about 1\\% of task-specific model parameters. Notably, the\noriginal Whisper's features, such as inverse text normalization and timestamp\ntagging, are retained in target-speaker ASR, keeping the generated\ntranscriptions natural and informative.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhiyuan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1\">Mingjie Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ju Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Verifiable Text Generation with Evolving Memory and Self-Reflection. (arXiv:2312.09075v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.09075","description":"<p>Despite the remarkable ability of large language models (LLMs) in language\ncomprehension and generation, they often suffer from producing factually\nincorrect information, also known as hallucination. A promising solution to\nthis issue is verifiable text generation, which prompts LLMs to generate\ncontent with citations for accuracy verification. However, verifiable text\ngeneration is non-trivial due to the focus-shifting phenomenon, the intricate\nreasoning needed to align the claim with correct citations, and the dilemma\nbetween the precision and breadth of retrieved documents. In this paper, we\npresent VTG, an innovative framework for Verifiable Text Generation with\nevolving memory and self-reflection. VTG introduces evolving long short-term\nmemory to retain both valuable documents and recent documents. A two-tier\nverifier equipped with an evidence finder is proposed to rethink and reflect on\nthe relationship between the claim and citations. Furthermore, active retrieval\nand diverse query generation are utilized to enhance both the precision and\nbreadth of the retrieved documents. We conduct extensive experiments on five\ndatasets across three knowledge-intensive tasks and the results reveal that VTG\nsignificantly outperforms baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hengyi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yingyan Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaochi Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuaiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dawei Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re-parameterized Low-rank Prompt: Generalize a Vision-Language Model within 0.5K Parameters. (arXiv:2312.10813v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2312.10813","description":"<p>With the development of large pre-trained vision-language models, how to\neffectively transfer the knowledge of such foundational models to downstream\ntasks becomes a hot topic, especially in a data-deficient scenario. Recently,\nprompt tuning has become a popular solution. When adapting the vision-language\nmodels, researchers freeze the parameters in the backbone and only design and\ntune the prompts. On the one hand, the delicate design of prompt tuning\nexhibits strong performance. On the other hand, complicated structures and\nupdate rules largely increase the computation and storage cost. Motivated by\nthe observation that the evolution pattern of the generalization capability in\nvisual-language models aligns harmoniously with the trend of rank variations in\nthe prompt matrix during adaptation, we design a new type of prompt,\nRe-parameterized Low-rank Prompt (RLP), for both efficient and effective\nadaptation. Our method could largely reduce the number of tunable parameters\nand storage space, which is quite beneficial in resource-limited scenarios.\nExtensive experiments further demonstrate the superiority of RLP. In\nparticular, RLP shows comparable or even stronger performance than the latest\nstate-of-the-art methods with an extremely small number of parameters. On a\nseries of tasks over 11 datasets, RLP significantly increases the average\ndownstream accuracy of classic prompt tuning by up to 5.25% using merely 0.5K\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_T/0/1/0/all/0/1\">Tianxiang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Mengyao Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sicheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated speech audiometry: Can it work using open-source pre-trained Kaldi-NL automatic speech recognition?. (arXiv:2312.12269v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.12269","description":"<p>A practical speech audiometry tool is the digits-in-noise (DIN) test for\nhearing screening of populations of varying ages and hearing status. The test\nis usually conducted by a human supervisor (e.g., clinician), who scores the\nresponses spoken by the listener, or online, where a software scores the\nresponses entered by the listener. The test has 24 digit-triplets presented in\nan adaptive staircase procedure, resulting in a speech reception threshold\n(SRT). We propose an alternative automated DIN test setup that can evaluate\nspoken responses whilst conducted without a human supervisor, using the\nopen-source automatic speech recognition toolkit, Kaldi-NL. Thirty\nself-reported normal-hearing Dutch adults (19-64 years) completed one\nDIN+Kaldi-NL test. Their spoken responses were recorded, and used for\nevaluating the transcript of decoded responses by Kaldi-NL. Study 1 evaluated\nthe Kaldi-NL performance through its word error rate (WER), percentage of\nsummed decoding errors regarding only digits found in the transcript compared\nto the total number of digits present in the spoken responses. Average WER\nacross participants was 5.0% (range 0 - 48%, SD = 8.8%), with average decoding\nerrors in three triplets per participant. Study 2 analysed the effect that\ntriplets with decoding errors from Kaldi-NL had on the DIN test output (SRT),\nusing bootstrapping simulations. Previous research indicated 0.70 dB as the\ntypical within-subject SRT variability for normal-hearing adults. Study 2\nshowed that up to four triplets with decoding errors produce SRT variations\nwithin this range, suggesting that our proposed setup could be feasible for\nclinical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Araiza_Illan_G/0/1/0/all/0/1\">Gloria Araiza-Illan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_L/0/1/0/all/0/1\">Luke Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_K/0/1/0/all/0/1\">Khiet P. Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baskent_D/0/1/0/all/0/1\">Deniz Baskent</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation. (arXiv:2312.14187v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.14187","description":"<p>Recent work demonstrates that, after being fine-tuned on a high-quality\ninstruction dataset, the resulting model can obtain impressive capabilities to\naddress a wide range of tasks. However, existing methods for instruction data\ngeneration often produce duplicate data and are not controllable enough on data\nquality. In this paper, we extend the generalization of instruction tuning by\nclassifying the instruction data to 4 code-related tasks and propose a\nLLM-based Generator-Discriminator data process framework to generate diverse,\nhigh-quality instruction data from open source code. Hence, we introduce\nCodeOcean, a dataset comprising 20,000 instruction instances across 4 universal\ncode-related tasks,which is aimed at augmenting the effectiveness of\ninstruction tuning and improving the generalization ability of fine-tuned\nmodel. Subsequently, we present WaveCoder, a fine-tuned Code LLM with\nWidespread And Versatile Enhanced instruction tuning. This model is\nspecifically designed for enhancing instruction tuning of Code Language Models\n(LLMs). Our experiments demonstrate that Wavecoder models outperform other\nopen-source models in terms of generalization ability across different\ncode-related tasks at the same level of fine-tuning scale. Moreover, Wavecoder\nexhibits high efficiency in previous code generation tasks. This paper thus\noffers a significant contribution to the field of instruction data generation\nand fine-tuning models, providing new insights and tools for enhancing\nperformance in code-related tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhaojian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_N/0/1/0/all/0/1\">Ning Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yangyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yishujie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenxiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1\">Qiufeng Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Media Bias Taxonomy: A Systematic Literature Review on the Forms and Automated Detection of Media Bias. (arXiv:2312.16148v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.16148","description":"<p>The way the media presents events can significantly affect public perception,\nwhich in turn can alter people's beliefs and views. Media bias describes a\none-sided or polarizing perspective on a topic. This article summarizes the\nresearch on computational methods to detect media bias by systematically\nreviewing 3140 research papers published between 2019 and 2022. To structure\nour review and support a mutual understanding of bias across research domains,\nwe introduce the Media Bias Taxonomy, which provides a coherent overview of the\ncurrent state of research on media bias from different perspectives. We show\nthat media bias detection is a highly active research field, in which\ntransformer-based classification approaches have led to significant\nimprovements in recent years. These improvements include higher classification\naccuracy and the ability to detect more fine-granular types of bias. However,\nwe have identified a lack of interdisciplinarity in existing projects, and a\nneed for more awareness of the various types of media bias to support\nmethodologically thorough performance evaluations of media bias detection\nsystems. Concluding from our analysis, we see the integration of recent machine\nlearning advancements with reliable and diverse bias assessment strategies from\nother research areas as the most promising area for future research\ncontributions in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Spinde_T/0/1/0/all/0/1\">Timo Spinde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinterreiter_S/0/1/0/all/0/1\">Smi Hinterreiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haak_F/0/1/0/all/0/1\">Fabian Haak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giese_H/0/1/0/all/0/1\">Helge Giese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuschke_N/0/1/0/all/0/1\">Norman Meuschke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DebugBench: Evaluating Debugging Capability of Large Language Models. (arXiv:2401.04621v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2401.04621","description":"<p>Large Language Models (LLMs) have demonstrated exceptional coding capability.\nHowever, as another critical component of programming proficiency, the\ndebugging capability of LLMs remains relatively unexplored. Previous\nevaluations of LLMs' debugging ability are significantly limited by the risk of\ndata leakage, the scale of the dataset, and the variety of tested bugs. To\novercome these deficiencies, we introduce `DebugBench', an LLM debugging\nbenchmark consisting of 4,253 instances. It covers four major bug categories\nand 18 minor types in C++, Java, and Python. To construct DebugBench, we\ncollect code snippets from the LeetCode community, implant bugs into source\ndata with GPT-4, and assure rigorous quality checks. We evaluate two commercial\nand three open-source models in a zero-shot scenario. We find that (1) while\nclosed-source models like GPT-4 exhibit inferior debugging performance compared\nto humans, open-source models such as Code Llama fail to attain any pass rate\nscores; (2) the complexity of debugging notably fluctuates depending on the bug\ncategory; (3) incorporating runtime feedback has a clear impact on debugging\nperformance which is not always helpful. As an extension, we also compare LLM\ndebugging and code generation, revealing a strong correlation between them for\nclosed-source models. These findings will benefit the development of LLMs in\ndebugging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_R/0/1/0/all/0/1\">Runchu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yining Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1\">Xin Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yinxu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yesai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2401.04679","description":"<p>We investigate parameter-efficient fine-tuning (PEFT) methods that can\nprovide good accuracy under limited computational and memory budgets in the\ncontext of large language models (LLMs). We present a new PEFT method called\nRobust Adaptation (RoSA) inspired by robust principal component analysis (PCA)\nthat jointly trains $\\textit{low-rank}$ and $\\textit{highly-sparse}$ components\non top of a set of fixed pretrained weights to efficiently approximate the\nperformance of a full-fine-tuning (FFT) solution. Across a series of\nchallenging generative tasks such as grade-school math and SQL query\ngeneration, which require fine-tuning for good performance, we show that RoSA\noutperforms both LoRA and pure sparse fine-tuning, at the same parameter\nbudget. We provide system support for RoSA to complement the training\nalgorithm, specifically in the form of sparse GPU kernels which enable memory-\nand computationally-efficient training. Our code will be made available at\n$\\href{https://github.com/IST-DASLab/RoSA}{\\text{our github page}}$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nikdan_M/0/1/0/all/0/1\">Mahdi Nikdan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabesh_S/0/1/0/all/0/1\">Soroush Tabesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1\">Dan Alistarh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-based Valence and Arousal Expressions between the United States and China: a Cross-Cultural Examination. (arXiv:2401.05254v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2401.05254","description":"<p>Although affective expressions of individuals have been extensively studied\nusing social media, research has primarily focused on the Western context.\nThere are substantial differences among cultures that contribute to their\naffective expressions. This paper examines the differences between Twitter (X)\nin the United States and Sina Weibo posts in China on two primary dimensions of\naffect - valence and arousal. We study the difference in the functional\nrelationship between arousal and valence (so-called V-shaped) among individuals\nin the US and China and explore the associated content differences.\nFurthermore, we correlate word usage and topics in both platforms to interpret\ntheir differences. We observe that for Twitter users, the variation in\nemotional intensity is less distinct between negative and positive emotions\ncompared to Weibo users, and there is a sharper escalation in arousal\ncorresponding with heightened emotions. From language features, we discover\nthat affective expressions are associated with personal life and feelings on\nTwitter, while on Weibo such discussions are about socio-political topics in\nthe society. These results suggest a West-East difference in the V-shaped\nrelationship between valence and arousal of affective expressions on social\nmedia influenced by content differences. Our findings have implications for\napplications and theories related to cultural differences in affective\nexpressions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1\">Young-Min Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_D/0/1/0/all/0/1\">Dandan Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thapa_S/0/1/0/all/0/1\">Stuti Thapa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sherman_G/0/1/0/all/0/1\">Garrick Sherman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1\">Lyle Ungar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_L/0/1/0/all/0/1\">Louis Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guntuku_S/0/1/0/all/0/1\">Sharath Chandra Guntuku</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2024-01-11T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}