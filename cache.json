{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-10-13T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Transformers generalize differently from information stored in context vs in weights. (arXiv:2210.05675v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05675","description":"<p>Transformer models can use two fundamentally different kinds of information:\ninformation stored in weights during training, and information provided\n``in-context'' at inference time. In this work, we show that transformers\nexhibit different inductive biases in how they represent and generalize from\nthe information in these two sources. In particular, we characterize whether\nthey generalize via parsimonious rules (rule-based generalization) or via\ndirect comparison with observed examples (exemplar-based generalization). This\nis of important practical consequence, as it informs whether to encode\ninformation in weights or in context, depending on how we want models to use\nthat information. In transformers trained on controlled stimuli, we find that\ngeneralization from weights is more rule-based whereas generalization from\ncontext is largely exemplar-based. In contrast, we find that in transformers\npre-trained on natural language, in-context learning is significantly\nrule-based, with larger models showing more rule-basedness. We hypothesise that\nrule-based generalization from in-context information might be an emergent\nconsequence of large-scale training on language, which has sparse rule-like\nstructure. Using controlled stimuli, we verify that transformers pretrained on\ndata containing sparse rule-like structure exhibit more rule-based\ngeneralization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">Stephanie C.Y. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_I/0/1/0/all/0/1\">Ishita Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junkyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumaran_D/0/1/0/all/0/1\">Dharshan Kumaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampinen_A/0/1/0/all/0/1\">Andrew K. Lampinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shapley Head Pruning: Identifying and Removing Interference in Multilingual Transformers. (arXiv:2210.05709v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05709","description":"<p>Multilingual transformer-based models demonstrate remarkable zero and\nfew-shot transfer across languages by learning and reusing language-agnostic\nfeatures. However, as a fixed-size model acquires more languages, its\nperformance across all languages degrades, a phenomenon termed interference.\nOften attributed to limited model capacity, interference is commonly addressed\nby adding additional parameters despite evidence that transformer-based models\nare overparameterized. In this work, we show that it is possible to reduce\ninterference by instead identifying and pruning language-specific parameters.\nFirst, we use Shapley Values, a credit allocation metric from coalitional game\ntheory, to identify attention heads that introduce interference. Then, we show\nthat removing identified attention heads from a fixed model improves\nperformance for a target language on both sentence classification and\nstructural prediction, seeing gains as large as 24.7\\%. Finally, we provide\ninsights on language-agnostic and language-specific attention heads using\nattention visualization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Held_W/0/1/0/all/0/1\">William Held</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Language Maps for Robot Navigation. (arXiv:2210.05714v1 [cs.RO])","link":"http://arxiv.org/abs/2210.05714","description":"<p>Grounding language to the visual observations of a navigating agent can be\nperformed using off-the-shelf visual-language models pretrained on\nInternet-scale data (e.g., image captions). While this is useful for matching\nimages to natural language descriptions of object goals, it remains disjoint\nfrom the process of mapping the environment, so that it lacks the spatial\nprecision of classic geometric maps. To address this problem, we propose\nVLMaps, a spatial map representation that directly fuses pretrained\nvisual-language features with a 3D reconstruction of the physical world. VLMaps\ncan be autonomously built from video feed on robots using standard exploration\napproaches and enables natural language indexing of the map without additional\nlabeled data. Specifically, when combined with large language models (LLMs),\nVLMaps can be used to (i) translate natural language commands into a sequence\nof open-vocabulary navigation goals (which, beyond prior work, can be spatial\nby construction, e.g., \"in between the sofa and TV\" or \"three meters to the\nright of the chair\") directly localized in the map, and (ii) can be shared\namong multiple robots with different embodiments to generate new obstacle maps\non-the-fly (by using a list of obstacle categories). Extensive experiments\ncarried out in simulated and real world environments show that VLMaps enable\nnavigation according to more complex language instructions than existing\nmethods. Videos are available at https:vlmaps.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chenguang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mees_O/0/1/0/all/0/1\">Oier Mees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Andy Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1\">Wolfram Burgard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relational Embeddings for Language Independent Stance Detection. (arXiv:2210.05715v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05715","description":"<p>The large majority of the research performed on stance detection has been\nfocused on developing more or less sophisticated text classification systems,\neven when many benchmarks are based on social network data such as Twitter.\nThis paper aims to take on the stance detection task by placing the emphasis\nnot so much on the text itself but on the interaction data available on social\nnetworks. More specifically, we propose a new method to leverage social\ninformation such as friends and retweets by generating relational embeddings,\nnamely, dense vector representations of interaction pairs. Our method can be\napplied to any language and target without any manual tuning. Our experiments\non seven publicly available datasets and four different languages show that\ncombining our relational embeddings with textual methods helps to substantially\nimprove performance, obtaining best results for six out of seven evaluation\nsettings, outperforming strong baselines based on large pre-trained language\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Landa_J/0/1/0/all/0/1\">Joseba Fernandez de Landa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agerri_R/0/1/0/all/0/1\">Rodrigo Agerri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Text Representations under Tight Annotation Budgets: Measuring Structural Alignment. (arXiv:2210.05721v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05721","description":"<p>Annotating large collections of textual data can be time consuming and\nexpensive. That is why the ability to train models with limited annotation\nbudgets is of great importance. In this context, it has been shown that under\ntight annotation budgets the choice of data representation is key. The goal of\nthis paper is to better understand why this is so. With this goal in mind, we\npropose a metric that measures the extent to which a given representation is\nstructurally aligned with a task. We conduct experiments on several text\nclassification datasets testing a variety of models and representations. Using\nour proposed metric we show that an efficient representation for a task (i.e.\none that enables learning from few samples) is a representation that induces a\ngood alignment between latent input structure and class structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Gutierrez_C/0/1/0/all/0/1\">C&#xe9;sar Gonz&#xe1;lez-Guti&#xe9;rrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Primadhanty_A/0/1/0/all/0/1\">Audi Primadhanty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cazzaro_F/0/1/0/all/0/1\">Francesco Cazzaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quattoni_A/0/1/0/all/0/1\">Ariadna Quattoni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring and Improving Semantic Diversity of Dialogue Generation. (arXiv:2210.05725v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05725","description":"<p>Response diversity has become an important criterion for evaluating the\nquality of open-domain dialogue generation models. However, current evaluation\nmetrics for response diversity often fail to capture the semantic diversity of\ngenerated responses, as they mainly consider lexical aspects of the generated\nresponses. In this paper, we introduce a new automatic evaluation metric to\nmeasure the semantic diversity of generated responses. Through human\nevaluation, we demonstrate that our proposed metric captures human judgments on\nresponse diversity better than existing lexical-level diversity metrics.\nFurthermore, motivated by analyzing an existing dialogue dataset, we propose a\nsimple yet effective learning method that improves the semantic diversity of\ngenerated responses. Our learning method weights training samples based on the\nsemantic distribution of the training set. We show that our learning method\nimproves response diversity and coherency better than other baseline methods\nthrough automatic and human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Seungju Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Beomsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Buru Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Speech Recognition of Low-Resource Languages Based on Chukchi. (arXiv:2210.05726v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05726","description":"<p>The following paper presents a project focused on the research and creation\nof a new Automatic Speech Recognition (ASR) based in the Chukchi language.\nThere is no one complete corpus of the Chukchi language, so most of the work\nconsisted in collecting audio and texts in the Chukchi language from open\nsources and processing them. We managed to collect 21:34:23 hours of audio\nrecordings and 112,719 sentences (or 2,068,273 words) of text in the Chukchi\nlanguage. The XLSR model was trained on the obtained data, which showed good\nresults even with a small amount of data. Besides the fact that the Chukchi\nlanguage is a low-resource language, it is also polysynthetic, which\nsignificantly complicates any automatic processing. Thus, the usual WER metric\nfor evaluating ASR becomes less indicative for a polysynthetic language.\nHowever, the CER metric showed good results. The question of metrics for\npolysynthetic languages remains open.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Safonova_A/0/1/0/all/0/1\">Anastasia Safonova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yudina_T/0/1/0/all/0/1\">Tatiana Yudina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadimanov_E/0/1/0/all/0/1\">Emil Nadimanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davenport_C/0/1/0/all/0/1\">Cydnie Davenport</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Streaming Punctuation for Long-form Dictation with Transformers. (arXiv:2210.05756v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05756","description":"<p>While speech recognition Word Error Rate (WER) has reached human parity for\nEnglish, long-form dictation scenarios still suffer from segmentation and\npunctuation problems resulting from irregular pausing patterns or slow\nspeakers. Transformer sequence tagging models are effective at capturing long\nbi-directional context, which is crucial for automatic punctuation. A typical\nAutomatic Speech Recognition (ASR) production system, however, is constrained\nby real-time requirements, making it hard to incorporate the right context when\nmaking punctuation decisions. In this paper, we propose a streaming approach\nfor punctuation or re-punctuation of ASR output using dynamic decoding windows\nand measure its impact on punctuation and segmentation accuracy in a variety of\nscenarios. The new system tackles over-segmentation issues, improving\nsegmentation F0.5-score by 13.9%. Streaming punctuation achieves an average\nBLEU-score gain of 0.66 for the downstream task of Machine Translation (MT).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Behre_P/0/1/0/all/0/1\">Piyush Behre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Sharman Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varadharajan_P/0/1/0/all/0/1\">Padma Varadharajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuangyu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoupled Context Processing for Context Augmented Language Modeling. (arXiv:2210.05758v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05758","description":"<p>Language models can be augmented with a context retriever to incorporate\nknowledge from large external databases. By leveraging retrieved context, the\nneural network does not have to memorize the massive amount of world knowledge\nwithin its internal parameters, leading to better parameter efficiency,\ninterpretability and modularity. In this paper we examined a simple yet\neffective architecture for incorporating external context into language models\nbased on decoupled Encoder Decoder architecture. We showed that such a simple\narchitecture achieves competitive results on auto-regressive language modeling\nand open domain question answering tasks. We also analyzed the behavior of the\nproposed model which performs grounded context transfer. Finally we discussed\nthe computational implications of such retrieval augmented models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zonglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1\">Ruiqi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Propagators of Disinformation on Twitter Using Quantitative Discursive Analysis. (arXiv:2210.05760v1 [cs.SI])","link":"http://arxiv.org/abs/2210.05760","description":"<p>Efforts by foreign actors to influence public opinion have gained\nconsiderable attention because of their potential to impact democratic\nelections. Thus, the ability to identify and counter sources of disinformation\nis increasingly becoming a top priority for government entities in order to\nprotect the integrity of democratic processes. This study presents a method of\nidentifying Russian disinformation bots on Twitter using centering resonance\nanalysis and Clauset-Newman-Moore community detection. The data reflect a\nsignificant degree of discursive dissimilarity between known Russian\ndisinformation bots and a control set of Twitter users during the timeframe of\nthe 2016 U.S. Presidential Election. The data also demonstrate statistically\nsignificant classification capabilities (MCC = 0.9070) based on community\nclustering. The prediction algorithm is very effective at identifying true\npositives (bots), but is not able to resolve true negatives (non-bots) because\nof the lack of discursive similarity between control users. This leads to a\nhighly sensitive means of identifying propagators of disinformation with a high\ndegree of discursive similarity on Twitter, with implications for limiting the\nspread of disinformation that could impact democratic processes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bailey_M/0/1/0/all/0/1\">Mark M. Bailey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vote'n'Rank: Revision of Benchmarking with Social Choice Theory. (arXiv:2210.05769v1 [cs.LG])","link":"http://arxiv.org/abs/2210.05769","description":"<p>The development of state-of-the-art systems in different applied areas of\nmachine learning (ML) is driven by benchmarks, which have shaped the paradigm\nof evaluating generalisation capabilities from multiple perspectives. Although\nthe paradigm is shifting towards more fine-grained evaluation across diverse\ntasks, the delicate question of how to aggregate the performances has received\nparticular interest in the community. In general, benchmarks follow the\nunspoken utilitarian principles, where the systems are ranked based on their\nmean average score over task-specific metrics. Such aggregation procedure has\nbeen viewed as a sub-optimal evaluation protocol, which may have created the\nillusion of progress. This paper proposes Vote'n'Rank, a framework for ranking\nsystems in multi-task benchmarks under the principles of the social choice\ntheory. We demonstrate that our approach can be efficiently utilised to draw\nnew insights on benchmarking in several ML sub-fields and identify the\nbest-performing systems in research and development case studies. The\nVote'n'Rank's procedures are more robust than the mean average while being able\nto handle missing performance scores and determine conditions under which the\nsystem becomes the winner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rofin_M/0/1/0/all/0/1\">Mark Rofin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikhailov_V/0/1/0/all/0/1\">Vladislav Mikhailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florinskiy_M/0/1/0/all/0/1\">Mikhail Florinskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kravchenko_A/0/1/0/all/0/1\">Andrey Kravchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tutubalina_E/0/1/0/all/0/1\">Elena Tutubalina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shavrina_T/0/1/0/all/0/1\">Tatiana Shavrina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karabekyan_D/0/1/0/all/0/1\">Daniel Karabekyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applying FrameNet to Chinese(Poetry). (arXiv:2210.05772v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05772","description":"<p>FrameNet( Fillmore and Baker [2009] ) is well-known for its wide use for\nknowledge representation in the form of inheritance-based ontologies and\nlexica( Trott et al. [2020] ). Although FrameNet is usually applied to\nlanguages like English, Spanish and Italian, there are still plenty of FrameNet\ndata sets available for other languages like Chinese, which differs\nsignificantly from those languages based on Latin alphabets. In this paper, the\ntranslation from ancient Chinese Poetry to modern Chinese will be first\nconducted to further apply the Chinese FrameNet(CFN, provided by Shanxi\nUniversity). Afterwards, the translation from modern Chinese will be conducted\nas well for the comparison between the applications of CFN and English\nFrameNet. Finally, the overall comparison will be draw between CFN to modern\nChinese and English FrameNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zirong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bil-DOS: A Bi-lingual Dialogue Ordering System (for Subway). (arXiv:2210.05773v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05773","description":"<p>Due to the unfamiliarity to particular words(or proper nouns) for\ningredients, non-native English speakers can be extremely confused about the\nordering process in restaurants like Subway. Thus, We developed a dialogue\nsystem, which supports Chinese(Mandarin)1 and English2 at the same time. In\nother words, users can switch arbitrarily between Chinese(Mandarin) and English\nas the conversation is being conducted. This system is specifically designed\nfor Subway ordering3. In BilDOS, we designed a Discriminator module to tell the\nlanguage is being used in inputted user utterance, a Translator module to\ntranslate used language into English if it is not English, and a Dialogue\nManager module to detect the intention within inputted user utterances, handle\noutlier inputs by throwing clarification requests, map detected Intention and\ndetailed Keyword4 into a particular intention class, locate the current\nordering process, continue to give queries to finish the order, conclude the\norder details once the order is completed, activate the evaluation process when\nthe conversation is done.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zirong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Haotian Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual Speaker Identification Using Distant Supervision. (arXiv:2210.05780v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05780","description":"<p>Speaker identification, determining which character said each utterance in\nliterary text, benefits many downstream tasks. Most existing approaches use\nexpert-defined rules or rule-based features to directly approach this task, but\nthese approaches come with significant drawbacks, such as lack of contextual\nreasoning and poor cross-lingual generalization. In this work, we propose a\nspeaker identification framework that addresses these issues. We first extract\nlarge-scale distant supervision signals in English via general-purpose tools\nand heuristics, and then apply these weakly-labeled instances with a focus on\nencouraging contextual reasoning to train a cross-lingual language model. We\nshow that the resulting model outperforms previous state-of-the-art methods on\ntwo English speaker identification benchmarks by up to 9% in accuracy and 5%\nwith only distant supervision, as well as two Chinese speaker identification\ndatasets by up to 4.7%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Ben Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Up Deliberation for Multilingual ASR. (arXiv:2210.05785v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05785","description":"<p>Multilingual end-to-end automatic speech recognition models are attractive\ndue to its simplicity in training and deployment. Recent work on large-scale\ntraining of such models has shown promising results compared to monolingual\nmodels. However, the work often focuses on multilingual models themselves in a\nsingle-pass setup. In this work, we investigate second-pass deliberation for\nmultilingual speech recognition. Our proposed deliberation is multilingual,\ni.e., the text encoder encodes hypothesis text from multiple languages, and the\ndecoder attends to multilingual text and audio. We investigate scaling the\ndeliberation text encoder and decoder, and compare scaling the deliberation\ndecoder and the first-pass cascaded encoder. We show that deliberation improves\nthe average WER on 9 languages by 4% relative compared to the single-pass\nmodel. By increasing the size of the deliberation up to 1B parameters, the\naverage WER improvement increases to 9%, with up to 14% for certain languages.\nOur deliberation rescorer is based on transformer layers and can be\nparallelized during rescoring.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Ke Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Learning with Joint Fine-Tuning for Multimodal Sentiment Analysis. (arXiv:2210.05790v1 [cs.LG])","link":"http://arxiv.org/abs/2210.05790","description":"<p>Most existing methods focus on sentiment analysis of textual data. However,\nrecently there has been a massive use of images and videos on social platforms,\nmotivating sentiment analysis from other modalities. Current studies show that\nexploring other modalities (e.g., images) increases sentiment analysis\nperformance. State-of-the-art multimodal models, such as CLIP and VisualBERT,\nare pre-trained on datasets with the text paired with images. Although the\nresults obtained by these models are promising, pre-training and sentiment\nanalysis fine-tuning tasks of these models are computationally expensive. This\npaper introduces a transfer learning approach using joint fine-tuning for\nsentiment analysis. Our proposal achieved competitive results using a more\nstraightforward alternative fine-tuning strategy that leverages different\npre-trained unimodal models and efficiently combines them in a multimodal\nspace. Moreover, our proposal allows flexibility when incorporating any\npre-trained model for texts and images during the joint fine-tuning stage,\nbeing especially interesting for sentiment classification in low-resource\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toledo_G/0/1/0/all/0/1\">Guilherme Louren&#xe7;o de Toledo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcacini_R/0/1/0/all/0/1\">Ricardo Marcondes Marcacini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparison of Soft and Hard Target RNN-T Distillation for Large-scale ASR. (arXiv:2210.05793v1 [cs.LG])","link":"http://arxiv.org/abs/2210.05793","description":"<p>Knowledge distillation is an effective machine learning technique to transfer\nknowledge from a teacher model to a smaller student model, especially with\nunlabeled data. In this paper, we focus on knowledge distillation for the RNN-T\nmodel, which is widely used in state-of-the-art (SoTA) automatic speech\nrecognition (ASR). Specifically, we compared using soft and hard target\ndistillation to train large-scaleRNN-T models on the LibriSpeech/LibriLight\npublic dataset (60k hours) and our in-house data (600k hours). We found that\nhard tar-gets are more effective when the teacher and student have different\narchitecture, such as large teacher and small streaming student. On the other\nhand, soft target distillation works better in self-training scenario like\niterative large teacher training. For a large model with0.6B weights, we\nachieve a new SoTA word error rate (WER) on LibriSpeech (8% relative\nimprovement on dev-other) using Noisy Student Training with soft target\ndistillation. It also allows our production teacher to adapt new data domain\ncontinuously.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1\">Dongseong Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustify Transformers with Robust Kernel Density Estimation. (arXiv:2210.05794v1 [cs.LG])","link":"http://arxiv.org/abs/2210.05794","description":"<p>Recent advances in Transformer architecture have empowered its empirical\nsuccess in various tasks across different domains. However, existing works\nmainly focus on improving the standard accuracy and computational cost, without\nconsidering the robustness of contaminated samples. Existing work has shown\nthat the self-attention mechanism, which is the center of the Transformer\narchitecture, can be viewed as a non-parametric estimator based on the\nwell-known kernel density estimation (KDE). This motivates us to leverage the\nrobust kernel density estimation (RKDE) in the self-attention mechanism, to\nalleviate the issue of the contamination of data by down-weighting the weight\nof bad samples in the estimation process. The modified self-attention mechanism\ncan be incorporated into different Transformer variants. Empirical results on\nlanguage modeling and image classification tasks demonstrate the effectiveness\nof this approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1\">Tongzheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tan Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_J/0/1/0/all/0/1\">Joydeep Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1\">Nhat Ho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Underspecification in Scene Description-to-Depiction Tasks. (arXiv:2210.05815v1 [cs.CV])","link":"http://arxiv.org/abs/2210.05815","description":"<p>Questions regarding implicitness, ambiguity and underspecification are\ncrucial for understanding the task validity and ethical concerns of multimodal\nimage+text systems, yet have received little attention to date. This position\npaper maps out a conceptual framework to address this gap, focusing on systems\nwhich generate images depicting scenes from scene descriptions. In doing so, we\naccount for how texts and images convey meaning differently. We outline a set\nof core challenges concerning textual and visual ambiguity, as well as risks\nthat may be amplified by ambiguous and underspecified elements. We propose and\ndiscuss strategies for addressing these challenges, including generating\nvisually ambiguous images, and generating a set of diverse images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hutchinson_B/0/1/0/all/0/1\">Ben Hutchinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1\">Vinodkumar Prabhakaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Social-Group-Agnostic Word Embedding Debiasing via the Stereotype Content Model. (arXiv:2210.05831v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05831","description":"<p>Existing word embedding debiasing methods require social-group-specific word\npairs (e.g., \"man\"-\"woman\") for each social attribute (e.g., gender), which\ncannot be used to mitigate bias for other social groups, making these methods\nimpractical or costly to incorporate understudied social groups in debiasing.\nWe propose that the Stereotype Content Model (SCM), a theoretical framework\ndeveloped in social psychology for understanding the content of stereotypes,\nwhich structures stereotype content along two psychological dimensions -\n\"warmth\" and \"competence\" - can help debiasing efforts to become\nsocial-group-agnostic by capturing the underlying connection between bias and\nstereotypes. Using only pairs of terms for warmth (e.g., \"genuine\"-\"fake\") and\ncompetence (e.g.,\"smart\"-\"stupid\"), we perform debiasing with established\nmethods and find that, across gender, race, and age, SCM-based debiasing\nperforms comparably to group-specific debiasing\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Omrani_A/0/1/0/all/0/1\">Ali Omrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_B/0/1/0/all/0/1\">Brendan Kennedy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atari_M/0/1/0/all/0/1\">Mohammad Atari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Morteza Dehghani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP also Understands Text: Prompting CLIP for Phrase Understanding. (arXiv:2210.05836v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05836","description":"<p>Contrastive Language-Image Pretraining (CLIP) efficiently learns visual\nconcepts by pre-training with natural language supervision. CLIP and its visual\nencoder have been explored on various vision and language tasks and achieve\nstrong zero-shot or transfer learning performance. However, the application of\nits text encoder solely for text understanding has been less explored. In this\npaper, we find that the text encoder of CLIP actually demonstrates strong\nability for phrase understanding, and can even significantly outperform popular\nlanguage models such as BERT with a properly designed prompt. Extensive\nexperiments validate the effectiveness of our method across different datasets\nand domains on entity clustering and entity set expansion tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_A/0/1/0/all/0/1\">An Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiacheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanrong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SEAL : Interactive Tool for Systematic Error Analysis and Labeling. (arXiv:2210.05839v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05839","description":"<p>With the advent of Transformers, large language models (LLMs) have saturated\nwell-known NLP benchmarks and leaderboards with high aggregate performance.\nHowever, many times these models systematically fail on tail data or rare\ngroups not obvious in aggregate evaluation. Identifying such problematic data\ngroups is even more challenging when there are no explicit labels (e.g.,\nethnicity, gender, etc.) and further compounded for NLP datasets due to the\nlack of visual features to characterize failure modes (e.g., Asian males,\nanimals indoors, waterbirds on land, etc.). This paper introduces an\ninteractive Systematic Error Analysis and Labeling (\\seal) tool that uses a\ntwo-step approach to first identify high error slices of data and then, in the\nsecond step, introduce methods to give human-understandable semantics to those\nunderperforming slices. We explore a variety of methods for coming up with\ncoherent semantics for the error groups using language models for semantic\nlabeling and a text-to-image model for generating visual features. SEAL toolkit\nand demo screencast is available at https://huggingface.co/spaces/nazneen/seal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1\">Nazneen Rajani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1\">Weixin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lingjiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1\">Meg Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MedJEx: A Medical Jargon Extraction Model with Wiki's Hyperlink Span and Contextualized Masked Language Model Score. (arXiv:2210.05875v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05875","description":"<p>This paper proposes a new natural language processing (NLP) application for\nidentifying medical jargon terms potentially difficult for patients to\ncomprehend from electronic health record (EHR) notes. We first present a novel\nand publicly available dataset with expert-annotated medical jargon terms from\n18K+ EHR note sentences ($MedJ$). Then, we introduce a novel medical jargon\nextraction ($MedJEx$) model which has been shown to outperform existing\nstate-of-the-art NLP models. First, MedJEx improved the overall performance\nwhen it was trained on an auxiliary Wikipedia hyperlink span dataset, where\nhyperlink spans provide additional Wikipedia articles to explain the spans (or\nterms), and then fine-tuned on the annotated MedJ data. Secondly, we found that\na contextualized masked language model score was beneficial for detecting\ndomain-specific unfamiliar jargon terms. Moreover, our results show that\ntraining on the auxiliary Wikipedia hyperlink span datasets improved six out of\neight biomedical named entity recognition benchmark datasets. Both MedJ and\nMedJEx are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1\">Sunjae Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zonghai Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_H/0/1/0/all/0/1\">Harmon S. Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_D/0/1/0/all/0/1\">David A. Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corner_B/0/1/0/all/0/1\">Brian Corner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AD-DROP: Attribution-Driven Dropout for Robust Language Model Fine-Tuning. (arXiv:2210.05883v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05883","description":"<p>Fine-tuning large pre-trained language models on downstream tasks is apt to\nsuffer from overfitting when limited training data is available. While dropout\nproves to be an effective antidote by randomly dropping a proportion of units,\nexisting research has not examined its effect on the self-attention mechanism.\nIn this paper, we investigate this problem through self-attention attribution\nand find that dropping attention positions with low attribution scores can\naccelerate training and increase the risk of overfitting. Motivated by this\nobservation, we propose Attribution-Driven Dropout (AD-DROP), which randomly\ndiscards some high-attribution positions to encourage the model to make\npredictions by relying more on low-attribution positions to reduce overfitting.\nWe also develop a cross-tuning strategy to alternate fine-tuning and AD-DROP to\navoid dropping high-attribution positions excessively. Extensive experiments on\nvarious benchmarks show that AD-DROP yields consistent improvements over\nbaselines. Analysis further confirms that AD-DROP serves as a strategic\nregularizer to prevent overfitting during fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jinghao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1\">Xiaojun Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_S/0/1/0/all/0/1\">Shaoliang Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perplexity from PLM Is Unreliable for Evaluating Text Quality. (arXiv:2210.05892v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05892","description":"<p>Recently, amounts of works utilize perplexity~(PPL) to evaluate the quality\nof the generated text. They suppose that if the value of PPL is smaller, the\nquality(i.e. fluency) of the text to be evaluated is better. However, we find\nthat the PPL referee is unqualified and it cannot evaluate the generated text\nfairly for the following reasons: (i) The PPL of short text is larger than long\ntext, which goes against common sense, (ii) The repeated text span could damage\nthe performance of PPL, and (iii) The punctuation marks could affect the\nperformance of PPL heavily. Experiments show that the PPL is unreliable for\nevaluating the quality of given text. Last, we discuss the key problems with\nevaluating text quality using language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yequan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiawen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xuying Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Prompting for Implicit Intent Prediction and Recommendation with Commonsense Reasoning. (arXiv:2210.05901v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05901","description":"<p>Intelligent virtual assistants are currently designed to perform tasks or\nservices explicitly mentioned by users, so multiple related domains or tasks\nneed to be performed one by one through a long conversation with many explicit\nintents. Instead, human assistants are capable of reasoning (multiple) implicit\nintents based on user utterances via commonsense knowledge, reducing complex\ninteractions and improving practicality. Therefore, this paper proposes a\nframework of multi-domain dialogue systems, which can automatically infer\nimplicit intents based on user utterances and then perform zero-shot prompting\nusing a large pre-trained language model to trigger suitable single\ntask-oriented bots. The proposed framework is demonstrated effective to realize\nimplicit intents and recommend associated bots in a zero-shot manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1\">Hui-Chi Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun-Nung Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discourse Analysis via Questions and Answers: Parsing Dependency Structures of Questions Under Discussion. (arXiv:2210.05905v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05905","description":"<p>Automatic discourse processing, which can help understand how sentences\nconnect to each other, is bottlenecked by data: current discourse formalisms\npose highly demanding annotation tasks involving large taxonomies of discourse\nrelations, making them inaccessible to lay annotators. This work instead adopts\nthe linguistic framework of Questions Under Discussion (QUD) for discourse\nanalysis and seeks to derive QUD structures automatically. QUD views each\nsentence as an answer to a question triggered in prior context; thus, we\ncharacterize relationships between sentences as free-form questions, in\ncontrast to exhaustive fine-grained taxonomies. We develop the\nfirst-of-its-kind QUD parser that derives a dependency structure of questions\nover full documents, trained using a large question-answering dataset DCQA\nannotated in a manner consistent with the QUD framework. Importantly, data\ncollection is easily crowdsourced using DCQA's paradigm. We show that this\nleads to a parser attaining strong performance according to human evaluation.\nWe illustrate how our QUD structure is distinct from RST trees, and demonstrate\nthe utility of QUD analysis in the context of document simplification. Our\nfindings show that QUD parsing is an appealing alternative for automatic\ndiscourse processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ko_W/0/1/0/all/0/1\">Wei-Jen Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yating Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalton_C/0/1/0/all/0/1\">Cutter Dalton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivas_D/0/1/0/all/0/1\">Dananjay Srinivas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hate-CLIPper: Multimodal Hateful Meme Classification based on Cross-modal Interaction of CLIP Features. (arXiv:2210.05916v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05916","description":"<p>Hateful memes are a growing menace on social media. While the image and its\ncorresponding text in a meme are related, they do not necessarily convey the\nsame meaning when viewed individually. Hence, detecting hateful memes requires\ncareful consideration of both visual and textual information. Multimodal\npre-training can be beneficial for this task because it effectively captures\nthe relationship between the image and the text by representing them in a\nsimilar feature space. Furthermore, it is essential to model the interactions\nbetween the image and text features through intermediate fusion. Most existing\nmethods either employ multimodal pre-training or intermediate fusion, but not\nboth. In this work, we propose the Hate-CLIPper architecture, which explicitly\nmodels the cross-modal interactions between the image and text representations\nobtained using Contrastive Language-Image Pre-training (CLIP) encoders via a\nfeature interaction matrix (FIM). A simple classifier based on the FIM\nrepresentation is able to achieve state-of-the-art performance on the Hateful\nMemes Challenge (HMC) dataset with an AUROC of 85.8, which even surpasses the\nhuman performance of 82.65. Experiments on other meme datasets such as\nPropaganda Memes and TamilMemes also demonstrate the generalizability of the\nproposed approach. Finally, we analyze the interpretability of the FIM\nrepresentation and show that cross-modal interactions can indeed facilitate the\nlearning of meaningful concepts. The code for this work is available at\nhttps://github.com/gokulkarthik/hateclipper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1\">Gokul Karthik Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nanadakumar_K/0/1/0/all/0/1\">Karthik Nanadakumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Step out of KG: Knowledge Graph Completion via Knowledgeable Retrieval and Reading Comprehension. (arXiv:2210.05921v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05921","description":"<p>Knowledge graphs, as the cornerstone of many AI applications, usually face\nserious incompleteness problems. In recent years, there have been many efforts\nto study automatic knowledge graph completion (KGC), most of which use existing\nknowledge to infer new knowledge. However, in our experiments, we find that not\nall relations can be obtained by inference, which constrains the performance of\nexisting models. To alleviate this problem, we propose a new model based on\ninformation retrieval and reading comprehension, namely IR4KGC. Specifically,\nwe pre-train a knowledge-based information retrieval module that can retrieve\ndocuments related to the triples to be completed. Then, the retrieved documents\nare handed over to the reading comprehension module to generate the predicted\nanswers. In experiments, we find that our model can well solve relations that\ncannot be inferred from existing knowledge, and achieve good results on KGC\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1\">Xin Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zijun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_K/0/1/0/all/0/1\">Kaisheng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Knowledge from Language Models for Video-based Action Anticipation. (arXiv:2210.05991v1 [cs.CV])","link":"http://arxiv.org/abs/2210.05991","description":"<p>Anticipating future actions in a video is useful for many autonomous and\nassistive technologies. Prior action anticipation work mostly treats this as a\nvision modality problem, where the models learn the task information primarily\nfrom the video features in the target action anticipation datasets. In this\nwork, we propose a method to make use of the text-modality that is available\nduring the training, to bring in complementary information that is not present\nin the target action anticipation datasets. In particular, we leverage\npre-trained language models to build a text-modality teacher that is able to\npredict future actions based on text labels of the past actions extracted from\nthe input video. To further adapt the teacher to the target domain (cooking),\nwe also pretrain the teacher on textual instructions from a recipes dataset\n(Recipe1M). Then, we distill the knowledge gained by the text-modality teacher\ninto a vision-modality student to further improve it's performance. We\nempirically evaluate this simple cross-modal distillation strategy on two video\ndatasets EGTEA-GAZE+ and EPIC-KITCHEN 55. Distilling this text-modality\nknowledge into a strong vision model (Anticipative Vision Transformer) yields\nconsistent gains across both datasets, 3.5% relative improvement on top1 class\nmean recall for EGTEA-GAZE+, 7.2% on top5 many-shot class mean recall for\nEPIC-KITCHEN 55 and achieves new state-of-the-results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sayontan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_T/0/1/0/all/0/1\">Tanvi Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoai_M/0/1/0/all/0/1\">Minh Hoai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Graph-Based Text Representations with Character and Word Level N-grams. (arXiv:2210.05999v1 [cs.CL])","link":"http://arxiv.org/abs/2210.05999","description":"<p>Graph-based text representation focuses on how text documents are represented\nas graphs for exploiting dependency information between tokens and documents\nwithin a corpus. Despite the increasing interest in graph representation\nlearning, there is limited research in exploring new ways for graph-based text\nrepresentation, which is important in downstream natural language processing\ntasks. In this paper, we first propose a new heterogeneous word-character text\ngraph that combines word and character n-gram nodes together with document\nnodes, allowing us to better learn dependencies among these entities.\nAdditionally, we propose two new graph-based neural models, WCTextGCN and\nWCTextGAT, for modeling our proposed text graph. Extensive experiments in text\nclassification and automatic text summarization benchmarks demonstrate that our\nproposed models consistently outperform competitive baselines and\nstate-of-the-art graph-based models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Autoregressive Machine Translation with Translation Memories. (arXiv:2210.06020v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06020","description":"<p>Non-autoregressive machine translation (NAT) has recently made great\nprogress. However, most works to date have focused on standard translation\ntasks, even though some edit-based NAT models, such as the Levenshtein\nTransformer (LevT), seem well suited to translate with a Translation Memory\n(TM). This is the scenario considered here. We first analyze the vanilla LevT\nmodel and explain why it does not do well in this setting. We then propose a\nnew variant, TM-LevT, and show how to effectively train this model. By\nmodifying the data presentation and introducing an extra deletion operation, we\nobtain performance that are on par with an autoregressive approach, while\nreducing the decoding load. We also show that incorporating TMs during training\ndispenses to use knowledge distillation, a well-known trick used to mitigate\nthe multimodality issue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jitao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crego_J/0/1/0/all/0/1\">Josep Crego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yvon_F/0/1/0/all/0/1\">Fran&#xe7;ois Yvon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lbl2Vec: An Embedding-Based Approach for Unsupervised Document Retrieval on Predefined Topics. (arXiv:2210.06023v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06023","description":"<p>In this paper, we consider the task of retrieving documents with predefined\ntopics from an unlabeled document dataset using an unsupervised approach. The\nproposed unsupervised approach requires only a small number of keywords\ndescribing the respective topics and no labeled document. Existing approaches\neither heavily relied on a large amount of additionally encoded world knowledge\nor on term-document frequencies. Contrariwise, we introduce a method that\nlearns jointly embedded document and word vectors solely from the unlabeled\ndocument dataset in order to find documents that are semantically similar to\nthe topics described by the keywords. The proposed method requires almost no\ntext preprocessing but is simultaneously effective at retrieving relevant\ndocuments with high probability. When successively retrieving documents on\ndifferent predefined topics from publicly available and commonly used datasets,\nwe achieved an average area under the receiver operating characteristic curve\nvalue of 0.95 on one dataset and 0.92 on another. Further, our method can be\nused for multiclass document classification, without the need to assign labels\nto the dataset in advance. Compared with an unsupervised classification\nbaseline, we increased F1 scores from 76.6 to 82.7 and from 61.0 to 75.1 on the\nrespective datasets. For easy replication of our approach, we make the\ndeveloped Lbl2Vec code publicly available as a ready-to-use tool under the\n3-Clause BSD license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schopf_T/0/1/0/all/0/1\">Tim Schopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braun_D/0/1/0/all/0/1\">Daniel Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthes_F/0/1/0/all/0/1\">Florian Matthes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Granularity Cross-modal Alignment for Generalized Medical Visual Representation Learning. (arXiv:2210.06044v1 [cs.CV])","link":"http://arxiv.org/abs/2210.06044","description":"<p>Learning medical visual representations directly from paired radiology\nreports has become an emerging topic in representation learning. However,\nexisting medical image-text joint learning methods are limited by instance or\nlocal supervision analysis, ignoring disease-level semantic correspondences. In\nthis paper, we present a novel Multi-Granularity Cross-modal Alignment (MGCA)\nframework for generalized medical visual representation learning by harnessing\nthe naturally exhibited semantic correspondences between medical image and\nradiology reports at three different levels, i.e., pathological region-level,\ninstance-level, and disease-level. Specifically, we first incorporate the\ninstance-wise alignment module by maximizing the agreement between image-report\npairs. Further, for token-wise alignment, we introduce a bidirectional\ncross-attention strategy to explicitly learn the matching between fine-grained\nvisual tokens and text tokens, followed by contrastive learning to align them.\nMore important, to leverage the high-level inter-subject relationship semantic\n(e.g., disease) correspondences, we design a novel cross-modal disease-level\nalignment paradigm to enforce the cross-modal cluster assignment consistency.\nExtensive experimental results on seven downstream medical image datasets\ncovering image classification, object detection, and semantic segmentation\ntasks demonstrate the stable and superior performance of our framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fuying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuyin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shujun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vardhanabhuti_V/0/1/0/all/0/1\">Varut Vardhanabhuti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lequan Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Massive Multilingual Pre-Trained Language Models Towards Real Zero-Shot Neural Machine Translation in Clinical Domain. (arXiv:2210.06068v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06068","description":"<p>Massively multilingual pre-trained language models (MMPLMs) are developed in\nrecent years demonstrating superpowers and the pre-knowledge they acquire for\ndownstream tasks. In this work, we investigate whether MMPLMs can be applied to\nzero-shot machine translation (MT) toward entirely new language pairs and new\ndomains. We carry out an experimental investigation using Meta-AI's MMPLMs\n\"wmt21-dense-24-wide-en-X and X-en (WMT21fb)\" which were pre-trained on 7\nlanguage pairs and 14 translation directions including English to Czech,\nGerman, Hausa, Icelandic, Japanese, Russian, and Chinese, and opposite\ndirection. We fine-tune these MMPLMs towards English-Spanish language pair\nwhich did not exist at all in their original pre-trained corpora both\nimplicitly and explicitly. We prepare carefully aligned clinical domain data\nfor this fine-tuning, which is different from their original mixed domain\nknowledge as well. Our experimental result shows that the fine-tuning is very\nsuccessful using just 250k well-aligned in-domain EN-ES pairs/sentences for\nthree sub-task translation tests: clinical cases, clinical terms, and ontology\nconcepts. It achieves very close evaluation scores to another MMPLM NLLB from\nMeta-AI, which included Spanish as a high-resource setting in the pre-training.\nTo the best of our knowledge, this is the first work on using MMPLMs towards\nreal zero-shot NMT successfully for totally unseen languages during\npre-training, and also the first in clinical domain for such a study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lifeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erofeev_G/0/1/0/all/0/1\">Gleb Erofeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorokina_I/0/1/0/all/0/1\">Irina Sorokina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gladkoff_S/0/1/0/all/0/1\">Serge Gladkoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1\">Goran Nenadic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summary on the ISCSLP 2022 Chinese-English Code-Switching ASR Challenge. (arXiv:2210.06091v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06091","description":"<p>Code-switching automatic speech recognition becomes one of the most\nchallenging and the most valuable scenarios of automatic speech recognition,\ndue to the code-switching phenomenon between multilingual language and the\nfrequent occurrence of code-switching phenomenon in daily life. The ISCSLP 2022\nChinese-English Code-Switching Automatic Speech Recognition (CSASR) Challenge\naims to promote the development of code-switching automatic speech recognition.\nThe ISCSLP 2022 CSASR challenge provided two training sets, TAL_CSASR corpus\nand MagicData-RAMC corpus, a development and a test set for participants, which\nare used for CSASR model training and evaluation. Along with the challenge, we\nalso provide the baseline system performance for reference. As a result, more\nthan 40 teams participated in this challenge, and the winner team achieved\n16.70% Mixture Error Rate (MER) performance on the test set and has achieved\n9.8% MER absolute improvement compared with the baseline system. In this paper,\nwe will describe the datasets, the associated baselines system and the\nrequirements, and summarize the CSASR challenge results and major techniques\nand tricks used in the submitted systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shuhao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengfei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_i/0/1/0/all/0/1\">infeng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei-Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Runyan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gaofeng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yonghong Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Continual Learning for Text Classification via Selective Inter-client Transfer. (arXiv:2210.06101v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06101","description":"<p>In this work, we combine the two paradigms: Federated Learning (FL) and\nContinual Learning (CL) for text classification task in cloud-edge continuum.\nThe objective of Federated Continual Learning (FCL) is to improve deep learning\nmodels over life time at each client by (relevant and efficient) knowledge\ntransfer without sharing data. Here, we address challenges in minimizing\ninter-client interference while knowledge sharing due to heterogeneous tasks\nacross clients in FCL setup. In doing so, we propose a novel framework,\nFederated Selective Inter-client Transfer (FedSeIT) which selectively combines\nmodel parameters of foreign clients. To further maximize knowledge transfer, we\nassess domain overlap and select informative tasks from the sequence of\nhistorical tasks at each foreign client while preserving privacy. Evaluating\nagainst the baselines, we show improved performance, a gain of (average) 12.4\\%\nin text classification over a sequence of tasks using five datasets from\ndiverse domains. To the best of our knowledge, this is the first work that\napplies FCL to NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_Y/0/1/0/all/0/1\">Yatin Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rai_P/0/1/0/all/0/1\">Pranav Rai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubert_M/0/1/0/all/0/1\">Matthias Schubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Pankaj Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EduQG: A Multi-format Multiple Choice Dataset for the Educational Domain. (arXiv:2210.06104v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06104","description":"<p>We introduce a high-quality dataset that contains 3,397 samples comprising\n(i) multiple choice questions, (ii) answers (including distractors), and (iii)\ntheir source documents, from the educational domain. Each question is phrased\nin two forms, normal and close. Correct answers are linked to source documents\nwith sentence-level annotations. Thus, our versatile dataset can be used for\nboth question and distractor generation, as well as to explore new challenges\nsuch as question format conversion. Furthermore, 903 questions are accompanied\nby their cognitive complexity level as per Bloom's taxonomy. All questions have\nbeen generated by educational experts rather than crowd workers to ensure they\nare maintaining educational and learning standards. Our analysis and\nexperiments suggest distinguishable differences between our dataset and\ncommonly used ones for question generation for educational purposes. We believe\nthis new dataset can serve as a valuable resource for research and evaluation\nin the educational domain. The dataset and baselines will be released to\nsupport further research in question generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hadifar_A/0/1/0/all/0/1\">Amir Hadifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitew_S/0/1/0/all/0/1\">Semere Kiros Bitew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deleu_J/0/1/0/all/0/1\">Johannes Deleu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Develder_C/0/1/0/all/0/1\">Chris Develder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1\">Thomas Demeester</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Data Augmentation for Translation Suggestion. (arXiv:2210.06138v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06138","description":"<p>Translation suggestion (TS) models are used to automatically provide\nalternative suggestions for incorrect spans in sentences generated by machine\ntranslation. This paper introduces the system used in our submission to the\nWMT'22 Translation Suggestion shared task. Our system is based on the ensemble\nof different translation architectures, including Transformer, SA-Transformer,\nand DynamicConv. We use three strategies to construct synthetic data from\nparallel corpora to compensate for the lack of supervised data. In addition, we\nintroduce a multi-phase pre-training strategy, adding an additional\npre-training phase with in-domain data. We rank second and third on the\nEnglish-German and English-Chinese bidirectional tasks, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongxiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1\">Siyu Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Annotating Norwegian Language Varieties on Twitter for Part-of-Speech. (arXiv:2210.06150v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06150","description":"<p>Norwegian Twitter data poses an interesting challenge for Natural Language\nProcessing (NLP) tasks. These texts are difficult for models trained on\nstandardized text in one of the two Norwegian written forms (Bokm{\\aa}l and\nNynorsk), as they contain both the typical variation of social media text, as\nwell as a large amount of dialectal variety. In this paper we present a novel\nNorwegian Twitter dataset annotated with POS-tags. We show that models trained\non Universal Dependency (UD) data perform worse when evaluated against this\ndataset, and that models trained on Bokm{\\aa}l generally perform better than\nthose trained on Nynorsk. We also see that performance on dialectal tweets is\ncomparable to the written standards for some models. Finally we perform a\ndetailed analysis of the errors that models commonly make on this data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maehlum_P/0/1/0/all/0/1\">Petter M&#xe6;hlum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaasen_A/0/1/0/all/0/1\">Andre K&#xe5;sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Touileb_S/0/1/0/all/0/1\">Samia Touileb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_J/0/1/0/all/0/1\">Jeremy Barnes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding. (arXiv:2210.06155v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06155","description":"<p>Recent years have witnessed the rise and success of pre-training techniques\nin visually-rich document understanding. However, most existing methods lack\nthe systematic mining and utilization of layout-centered knowledge, leading to\nsub-optimal performances. In this paper, we propose ERNIE-Layout, a novel\ndocument pre-training solution with layout knowledge enhancement in the whole\nworkflow, to learn better representations that combine the features from text,\nlayout, and image. Specifically, we first rearrange input sequences in the\nserialization stage, and then present a correlative pre-training task, reading\norder prediction, to learn the proper reading order of documents. To improve\nthe layout awareness of the model, we integrate a spatial-aware disentangled\nattention into the multi-modal transformer and a replaced regions prediction\ntask into the pre-training phase. Experimental results show that ERNIE-Layout\nachieves superior performance on various downstream tasks, setting new\nstate-of-the-art on key information extraction, document image classification,\nand document question answering datasets. The code and models are publicly\navailable at\n<a href=\"http://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/ernie-layout.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1\">Qiming Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yinxu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenjin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1\">Bin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhengjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Teng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Weichong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yongfeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shikun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1\">Hao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focusing on Context is NICE: Improving Overshadowed Entity Disambiguation. (arXiv:2210.06164v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06164","description":"<p>Entity disambiguation (ED) is the task of mapping an ambiguous entity mention\nto the corresponding entry in a structured knowledge base. Previous research\nshowed that entity overshadowing is a significant challenge for existing ED\nmodels: when presented with an ambiguous entity mention, the models are much\nmore likely to rank a more frequent yet less contextually relevant entity at\nthe top. Here, we present NICE, an iterative approach that uses entity type\ninformation to leverage context and avoid over-relying on the frequency-based\nprior. Our experiments show that NICE achieves the best performance results on\nthe overshadowed entities while still performing competitively on the frequent\nentities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Provatorova_V/0/1/0/all/0/1\">Vera Provatorova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tedeschi_S/0/1/0/all/0/1\">Simone Tedeschi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakulenko_S/0/1/0/all/0/1\">Svitlana Vakulenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navigli_R/0/1/0/all/0/1\">Roberto Navigli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanoulas_E/0/1/0/all/0/1\">Evangelos Kanoulas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VCSE: Time-Domain Visual-Contextual Speaker Extraction Network. (arXiv:2210.06177v1 [cs.CV])","link":"http://arxiv.org/abs/2210.06177","description":"<p>Speaker extraction seeks to extract the target speech in a multi-talker\nscenario given an auxiliary reference. Such reference can be auditory, i.e., a\npre-recorded speech, visual, i.e., lip movements, or contextual, i.e., phonetic\nsequence. References in different modalities provide distinct and complementary\ninformation that could be fused to form top-down attention on the target\nspeaker. Previous studies have introduced visual and contextual modalities in a\nsingle model. In this paper, we propose a two-stage time-domain\nvisual-contextual speaker extraction network named VCSE, which incorporates\nvisual and self-enrolled contextual cues stage by stage to take full advantage\nof every modality. In the first stage, we pre-extract a target speech with\nvisual cues and estimate the underlying phonetic sequence. In the second stage,\nwe refine the pre-extracted target speech with the self-enrolled contextual\ncues. Experimental results on the real-world Lip Reading Sentences 3 (LRS3)\ndatabase demonstrate that our proposed VCSE network consistently outperforms\nother state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_M/0/1/0/all/0/1\">Meng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zexu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longbiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_J/0/1/0/all/0/1\">Jianwu Dang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SilverAlign: MT-Based Silver Data Algorithm For Evaluating Word Alignment. (arXiv:2210.06207v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06207","description":"<p>Word alignments are essential for a variety of NLP tasks. Therefore, choosing\nthe best approaches for their creation is crucial. However, the scarce\navailability of gold evaluation data makes the choice difficult. We propose\nSilverAlign, a new method to automatically create silver data for the\nevaluation of word aligners by exploiting machine translation and minimal\npairs. We show that performance on our silver data correlates well with gold\nbenchmarks for 9 language pairs, making our approach a valid resource for\nevaluation of different domains and languages when gold data are not available.\nThis addresses the important scenario of missing gold data alignments for\nlow-resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koksal_A/0/1/0/all/0/1\">Abdullatif K&#xf6;ksal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Severini_S/0/1/0/all/0/1\">Silvia Severini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pruning Pre-trained Language Models Without Fine-Tuning. (arXiv:2210.06210v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06210","description":"<p>To overcome the overparameterized problem in Pre-trained Language Models\n(PLMs), pruning is widely used as a simple and straightforward compression\nmethod by directly removing unimportant weights. Previous first-order methods\nsuccessfully compress PLMs to extremely high sparsity with little performance\ndrop. These methods, such as movement pruning, use first-order information to\nprune PLMs while fine-tuning the remaining weights. In this work, we argue\nfine-tuning is redundant for first-order pruning, since first-order pruning is\nsufficient to converge PLMs to downstream tasks without fine-tuning. Under this\nmotivation, we propose Static Model Pruning (SMP), which only uses first-order\npruning to adapt PLMs to downstream tasks while achieving the target sparsity\nlevel. In addition, we also design a new masking function and training\nobjective to further improve SMP. Extensive experiments at various sparsity\nlevels show SMP has significant improvements over first-order and zero-order\nmethods. Unlike previous first-order methods, SMP is also applicable to low\nsparsity and outperforms zero-order methods. Meanwhile, SMP is more parameter\nefficient than other methods due to it does not require fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Ting Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Deqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards visually prompted keyword localisation for zero-resource spoken languages. (arXiv:2210.06229v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06229","description":"<p>Imagine being able to show a system a visual depiction of a keyword and\nfinding spoken utterances that contain this keyword from a zero-resource speech\ncorpus. We formalise this task and call it visually prompted keyword\nlocalisation (VPKL): given an image of a keyword, detect and predict where in\nan utterance the keyword occurs. To do VPKL, we propose a speech-vision model\nwith a novel localising attention mechanism which we train with a new keyword\nsampling scheme. We show that these innovations give improvements in VPKL over\nan existing speech-vision model. We also compare to a visual bag-of-words (BoW)\nmodel where images are automatically tagged with visual labels and paired with\nunlabelled speech. Although this visual BoW can be queried directly with a\nwritten keyword (while our's takes image queries), our new model still\noutperforms the visual BoW in both detection and localisation, giving a 16%\nrelative improvement in localisation F1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nortje_L/0/1/0/all/0/1\">Leanne Nortje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1\">Herman Kamper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quasi-symbolic explanatory NLI via disentanglement: A geometrical examination. (arXiv:2210.06230v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06230","description":"<p>Disentangling the encodings of neural models is a fundamental aspect for\nimproving interpretability, semantic control, and understanding downstream task\nperformance in Natural Language Processing. The connection points between\ndisentanglement and downstream tasks, however, remains underexplored from a\nexplanatory standpoint. This work presents a methodology for assessment of\ngeometrical properties of the resulting latent space w.r.t. vector operations\nand semantic disentanglement in quantitative and qualitative terms, based on a\nVAE-based supervised framework. Empirical results indicate that the\nrole-contents of explanations, such as \\textit{ARG0-animal}, are disentangled\nin the latent space, which provides us a chance for controlling the explanation\ngeneration by manipulating the traversal of vector over latent space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_D/0/1/0/all/0/1\">Danilo S. Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratt_Hartmann_I/0/1/0/all/0/1\">Ian Pratt-Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andr&#xe9; Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A context-aware knowledge transferring strategy for CTC-based ASR. (arXiv:2210.06244v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06244","description":"<p>Non-autoregressive automatic speech recognition (ASR) modeling has received\nincreasing attention recently because of its fast decoding speed and superior\nperformance. Among representatives, methods based on the connectionist temporal\nclassification (CTC) are still a dominating stream. However, the theoretically\ninherent flaw, the assumption of independence between tokens, creates a\nperformance barrier for the school of works. To mitigate the challenge, we\npropose a context-aware knowledge transferring strategy, consisting of a\nknowledge transferring module and a context-aware training strategy, for\nCTC-based ASR. The former is designed to distill linguistic information from a\npre-trained language model, and the latter is framed to modulate the\nlimitations caused by the conditional independence assumption. As a result, a\nknowledge-injected context-aware CTC-based ASR built upon the wav2vec2.0 is\npresented in this paper. A series of experiments on the AISHELL-1 and AISHELL-2\ndatasets demonstrate the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Ke-Han Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kuan-Yu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Back to the Future: On Potential Histories in NLP. (arXiv:2210.06245v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06245","description":"<p>Machine learning and NLP require the construction of datasets to train and\nfine-tune models. In this context, previous work has demonstrated the\nsensitivity of these data sets. For instance, potential societal biases in this\ndata are likely to be encoded and to be amplified in the models we deploy. In\nthis work, we draw from developments in the field of history and take a novel\nperspective on these problems: considering datasets and models through the lens\nof historical fiction surfaces their political nature, and affords\nre-configuring how we view the past, such that marginalized discourses are\nsurfaced. Building on such insights, we argue that contemporary methods for\nmachine learning are prejudiced towards dominant and hegemonic histories.\nEmploying the example of neopronouns, we show that by surfacing marginalized\nhistories within contemporary conditions, we can create models that better\nrepresent the lived realities of traditionally marginalized and excluded\ncommunities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Talat_Z/0/1/0/all/0/1\">Zeerak Talat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CIKQA: Learning Commonsense Inference with a Unified Knowledge-in-the-loop QA Paradigm. (arXiv:2210.06246v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06246","description":"<p>Recently, the community has achieved substantial progress on many commonsense\nreasoning benchmarks. However, it is still unclear what is learned from the\ntraining process: the knowledge, inference capability, or both? We argue that\ndue to the large scale of commonsense knowledge, it is infeasible to annotate a\nlarge enough training set for each task to cover all commonsense for learning.\nThus we should separate the commonsense knowledge acquisition and inference\nover commonsense knowledge as two separate tasks. In this work, we focus on\ninvestigating models' commonsense inference capabilities from two perspectives:\n(1) Whether models can know if the knowledge they have is enough to solve the\ntask; (2) Whether models can develop commonsense inference capabilities that\ngeneralize across commonsense tasks. We first align commonsense tasks with\nrelevant knowledge from commonsense knowledge bases and ask humans to annotate\nwhether the knowledge is enough or not. Then, we convert different commonsense\ntasks into a unified question answering format to evaluate models'\ngeneralization capabilities. We name the benchmark as Commonsense Inference\nwith Knowledge-in-the-loop Question Answering (CIKQA).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yintong Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1\">Yanai Elazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot On-the-Fly Event Schema Induction. (arXiv:2210.06254v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06254","description":"<p>What are the events involved in a pandemic outbreak? What steps should be\ntaken when planning a wedding? The answers to these questions can be found by\ncollecting many documents on the complex event of interest, extracting relevant\ninformation, and analyzing it. We present a new approach in which large\nlanguage models are utilized to generate source documents that allow\npredicting, given a high-level event definition, the specific events,\narguments, and relations between them to construct a schema that describes the\ncomplex event in its entirety. Using our model, complete schemas on any topic\ncan be generated on-the-fly without any manual data collection, i.e., in a\nzero-shot manner. Moreover, we develop efficient methods to extract pertinent\ninformation from texts and demonstrate in a series of experiments that these\nschemas are considered to be more complete than human-curated ones in the\nmajority of examined scenarios. Finally, we show that this framework is\ncomparable in performance with previous supervised schema induction methods\nthat rely on collecting real texts while being more general and flexible\nwithout the need for a predefined ontology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dror_R/0/1/0/all/0/1\">Rotem Dror</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task Compass: Scaling Multi-task Pre-training with Task Prefix. (arXiv:2210.06277v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06277","description":"<p>Leveraging task-aware annotated data as supervised signals to assist with\nself-supervised learning on large-scale unlabeled data has become a new trend\nin pre-training language models. Existing studies show that multi-task learning\nwith large-scale supervised tasks suffers from negative effects across tasks.\nTo tackle the challenge, we propose a task prefix guided multi-task\npre-training framework to explore the relationships among tasks. We conduct\nextensive experiments on 40 datasets, which show that our model can not only\nserve as the strong foundation backbone for a wide range of tasks but also be\nfeasible as a probing tool for analyzing task relationships. The task\nrelationships reflected by the prefixes align transfer learning performance\nbetween tasks. They also suggest directions for data augmentation with\ncomplementary tasks, which help our model achieve human-parity results on\ncommonsense reasoning leaderboards. Code is available at\nhttps://github.com/cooelf/CompassMTL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuwei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TwiRGCN: Temporally Weighted Graph Convolution for Question Answering over Temporal Knowledge Graphs. (arXiv:2210.06281v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06281","description":"<p>Recent years have witnessed much interest in temporal reasoning over\nknowledge graphs (KG) for complex question answering (QA), but there remains a\nsubstantial gap in human capabilities. We explore how to generalize relational\ngraph convolutional networks (RGCN) for temporal KGQA. Specifically, we propose\na novel, intuitive and interpretable scheme to modulate the messages passed\nthrough a KG edge during convolution, based on the relevance of its associated\ntime period to the question. We also introduce a gating device to predict if\nthe answer to a complex temporal question is likely to be a KG entity or time\nand use this prediction to guide our scoring mechanism. We evaluate the\nresulting system, which we call TwiRGCN, on TimeQuestions, a recently released,\nchallenging dataset for multi-hop complex temporal QA. We show that TwiRGCN\nsignificantly outperforms state-of-the-art systems on this dataset across\ndiverse question types. Notably, TwiRGCN improves accuracy by 9--10 percentage\npoints for the most difficult ordinal and implicit question types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Aditya Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_A/0/1/0/all/0/1\">Apoorv Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_C/0/1/0/all/0/1\">Chitrank Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazemi_S/0/1/0/all/0/1\">Seyed Mehran Kazemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1\">Soumen Chakrabarti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Generalized and Explainable Long-Range Context Representation for Dialogue Systems. (arXiv:2210.06282v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06282","description":"<p>Context representation is crucial to both dialogue understanding and\ngeneration. Recently, the most popular method for dialog context representation\nis to concatenate the last-$k$ previous utterances as context and use a large\ntransformer-based model to generate the next response. However, this method may\nnot be ideal for conversations containing long-range dependencies. In this\nwork, we propose DialoGX, a novel encoder-decoder based framework for\nconversational response generation with a generalized and explainable context\nrepresentation that can look beyond the last-$k$ utterances. Hence the method\nis adaptive to conversations with long-range dependencies. Our proposed\nsolution is based on two key ideas: a) computing a dynamic representation of\nthe entire context, and b) finding the previous utterances that are relevant\nfor generating the next response. Instead of last-$k$ utterances, DialoGX uses\nthe concatenation of the dynamic context vector and encoding of the most\nrelevant utterances as input which enables it to represent conversations of any\nlength in a compact and generalized fashion. We conduct our experiments on\nDailyDialog, a popular open-domain chit-chat dataset. DialoGX achieves\ncomparable performance with the state-of-the-art models on the automated\nmetrics. We also justify our context representation through the lens of\npsycholinguistics and show that the relevance score of previous utterances\nagrees well with human cognition which makes DialoGX explainable as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1\">Suvodip Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desarkar_M/0/1/0/all/0/1\">Maunendra Sankar Desarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srijith_P/0/1/0/all/0/1\">P. K. Srijith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Changing the Representation: Examining Language Representation for Neural Sign Language Production. (arXiv:2210.06312v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06312","description":"<p>Neural Sign Language Production (SLP) aims to automatically translate from\nspoken language sentences to sign language videos. Historically the SLP task\nhas been broken into two steps; Firstly, translating from a spoken language\nsentence to a gloss sequence and secondly, producing a sign language video\ngiven a sequence of glosses. In this paper we apply Natural Language Processing\ntechniques to the first step of the SLP pipeline. We use language models such\nas BERT and Word2Vec to create better sentence level embeddings, and apply\nseveral tokenization techniques, demonstrating how these improve performance on\nthe low resource translation task of Text to Gloss. We introduce Text to\nHamNoSys (T2H) translation, and show the advantages of using a phonetic\nrepresentation for sign language translation rather than a sign level gloss\nrepresentation. Furthermore, we use HamNoSys to extract the hand shape of a\nsign and use this as additional supervision during training, further increasing\nthe performance on T2H. Assembling best practise, we achieve a BLEU-4 score of\n26.99 on the MineDGS dataset and 25.09 on PHOENIX14T, two new state-of-the-art\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Walsh_H/0/1/0/all/0/1\">Harry Walsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saunders_B/0/1/0/all/0/1\">Ben Saunders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1\">Richard Bowden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Models are Parsimonious Learners: Activation Sparsity in Trained Transformers. (arXiv:2210.06313v1 [cs.LG])","link":"http://arxiv.org/abs/2210.06313","description":"<p>This paper studies the curious phenomenon for machine learning models with\nTransformer architectures that their activation maps are sparse. By activation\nmap we refer to the intermediate output of the multi-layer perceptrons (MLPs)\nafter a ReLU activation function, and by \"sparse\" we mean that on average very\nfew entries (e.g., 3.0% for T5-Base and 6.3% for ViT-B16) are nonzero for each\ninput to MLP. Moreover, larger Transformers with more layers and wider MLP\nhidden dimensions are sparser as measured by the percentage of nonzero entries.\nThrough extensive experiments we demonstrate that the emergence of sparsity is\na prevalent phenomenon that occurs for both natural language processing and\nvision tasks, on both training and evaluation data, for Transformers of various\nconfigurations, at layers of all depth levels, as well as for other\narchitectures including MLP-mixers and 2-layer MLPs. We show that sparsity also\nemerges using training datasets with random labels, or with random inputs, or\nwith infinite amount of data, demonstrating that sparsity is not a result of a\nspecific family of datasets. We discuss how sparsity immediately implies a way\nto significantly reduce the FLOP count and improve efficiency for Transformers.\nMoreover, we demonstrate perhaps surprisingly that enforcing an even sparser\nactivation via Top-k thresholding with a small value of k brings a collection\nof desired but missing properties for Transformers, namely less sensitivity to\nnoisy training data, more robustness to input corruptions, and better\ncalibration for their prediction confidence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zonglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chong You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1\">Srinadh Bhojanapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Daliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_A/0/1/0/all/0/1\">Ankit Singh Rawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddi_S/0/1/0/all/0/1\">Sashank J. Reddi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_K/0/1/0/all/0/1\">Ke Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chern_F/0/1/0/all/0/1\">Felix Chern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Felix Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1\">Ruiqi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SQuId: Measuring Speech Naturalness in Many Languages. (arXiv:2210.06324v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06324","description":"<p>Much of text-to-speech research relies on human evaluation, which incurs\nheavy costs and slows down the development process. The problem is particularly\nacute in heavily multilingual applications, where recruiting and polling judges\ncan take weeks. We introduce SQuId (Speech Quality Identification), a\nmultilingual naturalness prediction model trained on over a million ratings and\ntested in 65 locales-the largest effort of this type to date. The main insight\nis that training one model on many locales consistently outperforms mono-locale\nbaselines. We present our task, the model, and show that it outperforms a\ncompetitive baseline based on w2v-BERT and VoiceMOS by 50.0%. We then\ndemonstrate the effectiveness of cross-locale transfer during fine-tuning and\nhighlight its effect on zero-shot locales, i.e., locales for which there is no\nfine-tuning data. Through a series of analyses, we highlight the role of\nnon-linguistic effects such as sound artifacts in cross-locale transfer.\nFinally, we present the effect of our design decision, e.g., model size,\npre-training diversity, and language rebalancing with several ablation\nexperiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sellam_T/0/1/0/all/0/1\">Thibault Sellam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camp_J/0/1/0/all/0/1\">Joshua Camp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mackinnon_D/0/1/0/all/0/1\">Diana Mackinnon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_A/0/1/0/all/0/1\">Ankur P. Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riesa_J/0/1/0/all/0/1\">Jason Riesa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RedHOT: A Corpus of Annotated Medical Questions, Experiences, and Claims on Social Media. (arXiv:2210.06331v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06331","description":"<p>We present Reddit Health Online Talk (RedHOT), a corpus of 22,000 richly\nannotated social media posts from Reddit spanning 24 health conditions.\nAnnotations include demarcations of spans corresponding to medical claims,\npersonal experiences, and questions. We collect additional granular annotations\non identified claims. Specifically, we mark snippets that describe patient\nPopulations, Interventions, and Outcomes (PIO elements) within these. Using\nthis corpus, we introduce the task of retrieving trustworthy evidence relevant\nto a given claim made on social media. We propose a new method to automatically\nderive (noisy) supervision for this task which we use to train a dense\nretrieval model; this outperforms baseline models. Manual evaluation of\nretrieval results performed by medical doctors indicate that while our system\nperformance is promising, there is considerable room for improvement. Collected\nannotations (and scripts to assemble the dataset), are available at\nhttps://github.com/sominw/redhot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wadhwa_S/0/1/0/all/0/1\">Somin Wadhwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khetan_V/0/1/0/all/0/1\">Vivek Khetan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amir_S/0/1/0/all/0/1\">Silvio Amir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron Wallace</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Text Detection: Systemic Literature Review. (arXiv:2210.06336v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06336","description":"<p>Within the text analysis and processing fields, generated text attacks have\nbeen made easier to create than ever before. To combat these attacks open\nsourcing models and datasets have become a major trend to create automated\ndetection algorithms in defense of authenticity. For this purpose, synthetic\ntext detection has become an increasingly viable topic of research. This review\nis written for the purpose of creating a snapshot of the state of current\nliterature and easing the barrier to entry for future authors. Towards that\ngoal, we identified few research trends and challenges in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_J/0/1/0/all/0/1\">Jesus Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alsmadi_I/0/1/0/all/0/1\">Izzat Alsmadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Radiology Report Generation Systems by Removing Hallucinated References to Non-existent Priors. (arXiv:2210.06340v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06340","description":"<p>Current deep learning models trained to generate radiology reports from chest\nradiographs are capable of producing clinically accurate, clear, and actionable\ntext that can advance patient care. However, such systems all succumb to the\nsame problem: making hallucinated references to non-existent prior reports.\nSuch hallucinations occur because these models are trained on datasets of\nreal-world patient reports that inherently refer to priors. To this end, we\npropose two methods to remove references to priors in radiology reports: (1) a\nGPT-3-based few-shot approach to rewrite medical reports without references to\npriors; and (2) a BioBERT-based token classification approach to directly\nremove words referring to priors. We use the aforementioned approaches to\nmodify MIMIC-CXR, a publicly available dataset of chest X-rays and their\nassociated free-text radiology reports; we then retrain CXR-RePaiR, a radiology\nreport generation system, on the adapted MIMIC-CXR dataset. We find that our\nre-trained model--which we call CXR-ReDonE--outperforms previous report\ngeneration methods on clinical metrics, achieving an average BERTScore of\n0.2351 (2.57% absolute improvement). We expect our approach to be broadly\nvaluable in enabling current radiology report generation systems to be more\ndirectly integrated into clinical pipelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_V/0/1/0/all/0/1\">Vignav Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_N/0/1/0/all/0/1\">Nathan Andrew Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TaskMix: Data Augmentation for Meta-Learning of Spoken Intent Understanding. (arXiv:2210.06341v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06341","description":"<p>Meta-Learning has emerged as a research direction to better transfer\nknowledge from related tasks to unseen but related tasks. However,\nMeta-Learning requires many training tasks to learn representations that\ntransfer well to unseen tasks; otherwise, it leads to overfitting, and the\nperformance degenerates to worse than Multi-task Learning. We show that a\nstate-of-the-art data augmentation method worsens this problem of overfitting\nwhen the task diversity is low. We propose a simple method, TaskMix, which\nsynthesizes new tasks by linearly interpolating existing tasks. We compare\nTaskMix against many baselines on an in-house multilingual intent\nclassification dataset of N-Best ASR hypotheses derived from real-life\nhuman-machine telephony utterances and two datasets derived from MTOP. We show\nthat TaskMix outperforms baselines, alleviates overfitting when task diversity\nis low, and does not degrade performance even when it is high.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1\">Surya Kant Sahu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variational Open-Domain Question Answering. (arXiv:2210.06345v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06345","description":"<p>We introduce the Variational Open-Domain (VOD) framework for end-to-end\ntraining and evaluation of retrieval-augmented models (open-domain question\nanswering and language modelling). We show that the R\\'enyi variational bound,\na lower bound to the task marginal likelihood, can be exploited to aid\noptimization and use importance sampling to estimate the task log-likelihood\nlower bound and its gradients using samples drawn from an auxiliary retriever\n(approximate posterior). The framework can be used to train modern\nretrieval-augmented systems end-to-end using tractable and consistent estimates\nof the R\\'enyi variational bound and its gradients. We demonstrate the\nframework's versatility by training reader-retriever BERT-based models on\nmultiple-choice medical exam questions (MedMCQA and USMLE). We registered a new\nstate-of-the-art for both datasets (MedMCQA: $62.9$\\%, USMLE: $55.0$\\%). Last,\nwe show that the retriever part of the learned reader-retriever model trained\non the medical board exam questions can be used in search engines for a medical\nknowledge base.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lievin_V/0/1/0/all/0/1\">Valentin Li&#xe9;vin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motzfeldt_A/0/1/0/all/0/1\">Andreas Geert Motzfeldt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jensen_I/0/1/0/all/0/1\">Ida Riis Jensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1\">Ole Winther</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting the clinical citation count of biomedical papers using multilayer perceptron neural network. (arXiv:2210.06346v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06346","description":"<p>The number of clinical citations received from clinical guidelines or\nclinical trials has been considered as one of the most appropriate indicators\nfor quantifying the clinical impact of biomedical papers. Therefore, the early\nprediction of the clinical citation count of biomedical papers is critical to\nscientific activities in biomedicine, such as research evaluation, resource\nallocation, and clinical translation. In this study, we designed a four-layer\nmultilayer perceptron neural network (MPNN) model to predict the clinical\ncitation count of biomedical papers in the future by using 9,822,620 biomedical\npapers published from 1985 to 2005. We extracted ninety-one paper features from\nthree dimensions as the input of the model, including twenty-one features in\nthe paper dimension, thirty-five in the reference dimension, and thirty-five in\nthe citing paper dimension. In each dimension, the features can be classified\ninto three categories, i.e., the citation-related features, the clinical\ntranslation-related features, and the topic-related features. Besides, in the\npaper dimension, we also considered the features that have previously been\ndemonstrated to be related to the citation counts of research papers. The\nresults showed that the proposed MPNN model outperformed the other five\nbaseline models, and the features in the reference dimension were the most\nimportant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xuli Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1\">Qikai Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context Generation Improves Open Domain Question Answering. (arXiv:2210.06349v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06349","description":"<p>Closed-book question answering (QA) requires a model to directly answer an\nopen-domain question without access to any external knowledge. Prior work on\nclosed-book QA either directly finetunes or prompts a pretrained language model\n(LM) to leverage the stored knowledge. However, they do not fully exploit the\nparameterized knowledge. To address this issue, we propose a two-stage,\nclosed-book QA framework which employs a coarse-to-fine approach to extract\nrelevant knowledge and answer a question. Our approach first generates a\nrelated context for a given question by prompting a pretrained LM. We then\nprompt the same LM for answer prediction using the generated context and the\nquestion. Additionally, to eliminate failure caused by context uncertainty, we\nmarginalize over generated contexts. Experimental results on three QA\nbenchmarks show that our method significantly outperforms previous closed-book\nQA methods (e.g. exact matching 68.6% vs. 55.3%), and is on par with open-book\nmethods that exploit external knowledge sources (e.g. 68.6% vs. 68.0%). Our\nmethod is able to better exploit the stored knowledge in pretrained LMs without\nadding extra learnable parameters or needing finetuning, and paves the way for\nhybrid models that integrate pretrained LMs with external knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwary_M/0/1/0/all/0/1\">Mostofa Patwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhumoye_S/0/1/0/all/0/1\">Shrimai Prabhumoye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prenger_R/0/1/0/all/0/1\">Ryan Prenger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Keyword Based Approach to Understanding the Overpenalization of Marginalized Groups by English Marginal Abuse Models on Twitter. (arXiv:2210.06351v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06351","description":"<p>Harmful content detection models tend to have higher false positive rates for\ncontent from marginalized groups. In the context of marginal abuse modeling on\nTwitter, such disproportionate penalization poses the risk of reduced\nvisibility, where marginalized communities lose the opportunity to voice their\nopinion on the platform. Current approaches to algorithmic harm mitigation, and\nbias detection for NLP models are often very ad hoc and subject to human bias.\nWe make two main contributions in this paper. First, we design a novel\nmethodology, which provides a principled approach to detecting and measuring\nthe severity of potential harms associated with a text-based model. Second, we\napply our methodology to audit Twitter's English marginal abuse model, which is\nused for removing amplification eligibility of marginally abusive content.\nWithout utilizing demographic labels or dialect classifiers, we are still able\nto detect and measure the severity of issues related to the over-penalization\nof the speech of marginalized communities, such as the use of reclaimed speech,\ncounterspeech, and identity related terms. In order to mitigate the associated\nharms, we experiment with adding additional true negative examples and find\nthat doing so provides improvements to our fairness metrics without large\ndegradations in model performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yee_K/0/1/0/all/0/1\">Kyra Yee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebag_A/0/1/0/all/0/1\">Alice Schoenauer Sebag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Redfield_O/0/1/0/all/0/1\">Olivia Redfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_E/0/1/0/all/0/1\">Emily Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eck_M/0/1/0/all/0/1\">Matthias Eck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belli_L/0/1/0/all/0/1\">Luca Belli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Russian Web Tables: A Public Corpus of Web Tables for Russian Language Based on Wikipedia. (arXiv:2210.06353v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06353","description":"<p>Corpora that contain tabular data such as WebTables are a vital resource for\nthe academic community. Essentially, they are the backbone of any modern\nresearch in information management. They are used for various tasks of data\nextraction, knowledge base construction, question answering, column semantic\ntype detection and many other. Such corpora are useful not only as a source of\ndata, but also as a base for building test datasets. So far, there were no such\ncorpora for the Russian language and this seriously hindered research in the\naforementioned areas.\n</p>\n<p>In this paper, we present the first corpus of Web tables created specifically\nout of Russian language material. It was built via a special toolkit we have\ndeveloped to crawl the Russian Wikipedia. Both the corpus and the toolkit are\nopen-source and publicly available. Finally, we present a short study that\ndescribes Russian Wikipedia tables and their statistics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fedorov_P/0/1/0/all/0/1\">Platon Fedorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mironov_A/0/1/0/all/0/1\">Alexey Mironov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chernishev_G/0/1/0/all/0/1\">George Chernishev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-to-Audio Grounding Based Novel Metric for Evaluating Audio Caption Similarity. (arXiv:2210.06354v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06354","description":"<p>Automatic Audio Captioning (AAC) refers to the task of translating an audio\nsample into a natural language (NL) text that describes the audio events,\nsource of the events and their relationships. Unlike NL text generation tasks,\nwhich rely on metrics like BLEU, ROUGE, METEOR based on lexical semantics for\nevaluation, the AAC evaluation metric requires an ability to map NL text\n(phrases) that correspond to similar sounds in addition lexical semantics.\nCurrent metrics used for evaluation of AAC tasks lack an understanding of the\nperceived properties of sound represented by text. In this paper, wepropose a\nnovel metric based on Text-to-Audio Grounding (TAG), which is, useful for\nevaluating cross modal tasks like AAC. Experiments on publicly available AAC\ndata-set shows our evaluation metric to perform better compared to existing\nmetrics used in NL text and image captioning literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhosale_S/0/1/0/all/0/1\">Swapnil Bhosale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_R/0/1/0/all/0/1\">Rupayan Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopparapu_S/0/1/0/all/0/1\">Sunil Kumar Kopparapu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extractive Question Answering on Queries in Hindi and Tamil. (arXiv:2210.06356v1 [cs.CL])","link":"http://arxiv.org/abs/2210.06356","description":"<p>Indic languages like Hindi and Tamil are underrepresented in the natural\nlanguage processing (NLP) field compared to languages like English. Due to this\nunderrepresentation, performance on NLP tasks (such as search algorithms) in\nIndic languages are inferior to their English counterparts. This difference\ndisproportionately affects those who come from lower socioeconomic statuses\nbecause they consume the most Internet content in local languages. The goal of\nthis project is to build an NLP model that performs better than pre-existing\nmodels for the task of extractive question-answering (QA) on a public dataset\nin Hindi and Tamil. Extractive QA is an NLP task where answers to questions are\nextracted from a corresponding body of text. To build the best solution, we\nused three different models. The first model is an unmodified cross-lingual\nversion of the NLP model RoBERTa, known as XLM-RoBERTa, that is pretrained on\n100 languages. The second model is based on the pretrained RoBERTa model with\nan extra classification head for the question answering, but we used a custom\nIndic tokenizer, then optimized hyperparameters and fine tuned on the Indic\ndataset. The third model is based on XLM-RoBERTa, but with extra finetuning and\ntraining on the Indic dataset. We hypothesize the third model will perform best\nbecause of the variety of languages the XLM-RoBERTa model has been pretrained\non and the additional finetuning on the Indic dataset. This hypothesis was\nproven wrong because the paired RoBERTa models performed the best as the\ntraining data used was most specific to the task performed as opposed to the\nXLM-RoBERTa models which had much data that was not in either Hindi or Tamil.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thirumala_A/0/1/0/all/0/1\">Adhitya Thirumala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferracane_E/0/1/0/all/0/1\">Elisa Ferracane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Question Answering Modeling Improvements Hold Across Benchmarks?. (arXiv:2102.01065v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.01065","description":"<p>Do question answering (QA) modeling improvements (e.g., choice of\narchitecture and training procedure) hold across the diverse landscape of QA\nbenchmarks? To study this question, we introduce the notion of concurrence --\ntwo benchmarks have high concurrence on a set of modeling approaches if they\nrank the modeling approaches similarly. We measure the concurrence between 32\nQA benchmarks on a set of 20 diverse modeling approaches and find that\nhuman-constructed benchmarks have high concurrence amongst themselves, even if\ntheir passage and question distributions are very different. Surprisingly, even\ndownsampled human-constructed benchmarks (i.e., collecting less data) and\nprogrammatically-generated benchmarks (e.g., cloze-formatted examples) have\nhigh concurrence with human-constructed benchmarks. These results indicate\nthat, despite years of intense community focus on a small number of benchmarks,\nthe modeling improvements studied hold broadly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nelson F. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tony Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Oh My Mistake!: Toward Realistic Dialogue State Tracking including Turnback Utterances. (arXiv:2108.12637v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12637","description":"<p>The primary purpose of dialogue state tracking (DST), a critical component of\nan end-to-end conversational system, is to build a model that responds well to\nreal-world situations. Although we often change our minds from time to time\nduring ordinary conversations, current benchmark datasets do not adequately\nreflect such occurrences and instead consist of over-simplified conversations,\nin which no one changes their mind during a conversation. As the main question\ninspiring the present study, \"Are current benchmark datasets sufficiently\ndiverse to handle casual conversations in which one changes their mind after a\ncertain topic is over?\" We found that the answer is \"No\" because DST models\ncannot refer to previous user preferences when template-based turnback\nutterances are injected into the dataset. Even in the the simplest\nmind-changing (turnback) scenario, the performance of DST models significantly\ndegenerated. However, we found that this performance degeneration can be\nrecovered when the turnback scenarios are explicitly designed in the training\nset, implying that the problem is not with the DST models but rather with the\nconstruction of the benchmark dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Takyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yukyung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1\">Hoonsang Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_P/0/1/0/all/0/1\">Pilsung Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bang_J/0/1/0/all/0/1\">Junseong Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Misuk Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Can Be Strong Differentially Private Learners. (arXiv:2110.05679v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.05679","description":"<p>Differentially Private (DP) learning has seen limited success for building\nlarge deep learning models of text, and straightforward attempts at applying\nDifferentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have\nresulted in large performance drops and high computational overhead. We show\nthat this performance drop can be mitigated with (1) the use of large\npretrained language models; (2) non-standard hyperparameters that suit DP\noptimization; and (3) fine-tuning objectives which are aligned with the\npretraining procedure. With the above, we obtain NLP models that outperform\nstate-of-the-art DP-trained models under the same privacy budget and strong\nnon-private baselines -- by directly fine-tuning pretrained models with DP\noptimization on moderately-sized corpora. To address the computational\nchallenge of running DP-SGD with large Transformers, we propose a memory saving\ntechnique that allows clipping in DP-SGD to run without instantiating\nper-example gradients for any linear layer in the model. The technique enables\nprivately training Transformers with almost the same memory cost as non-private\ntraining at a modest run-time overhead. Contrary to conventional wisdom that DP\noptimization fails at learning high-dimensional models (due to noise that\nscales with dimension) empirical results reveal that private learning with\npretrained language models doesn't tend to suffer from dimension-dependent\nperformance degradation. Code to reproduce results can be found at\nhttps://github.com/lxuechen/private-transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuechen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1\">Florian Tram&#xe8;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples. (arXiv:2111.10962v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.10962","description":"<p>Knowledge-enhanced language representation learning has shown promising\nresults across various knowledge-intensive NLP tasks. However, prior methods\nare limited in efficient utilization of multilingual knowledge graph (KG) data\nfor language model (LM) pretraining. They often train LMs with KGs in indirect\nways, relying on extra entity/relation embeddings to facilitate knowledge\ninjection. In this work, we explore methods to make better use of the\nmultilingual annotation and language agnostic property of KG triples, and\npresent novel knowledge based multilingual language models (KMLMs) trained\ndirectly on the knowledge triples. We first generate a large amount of\nmultilingual synthetic sentences using the Wikidata KG triples. Then based on\nthe intra- and inter-sentence structures of the generated data, we design\npretraining tasks to enable the LMs to not only memorize the factual knowledge\nbut also learn useful logical patterns. Our pretrained KMLMs demonstrate\nsignificant performance improvements on a wide range of knowledge-intensive\ncross-lingual tasks, including named entity recognition (NER), factual\nknowledge retrieval, relation classification, and a newly designed logical\nreasoning task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruidan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers. (arXiv:2112.09237v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.09237","description":"<p>Building natural language inference (NLI) benchmarks that are both\nchallenging for modern techniques, and free from cheating feature biases is\ndifficult. Chief among these biases is single sentence label leakage, where\nannotator-introduced spurious correlations yield datasets where the logical\nrelation between (premise, hypothesis) pairs can be accurately predicted from\nonly a single sentence, something that should in principle be impossible. We\ndemonstrate that despite efforts to reduce this leakage, it persists in modern\ndatasets that have been introduced since its 2018 discovery. To enable future\namelioration efforts, introduce a novel model-driven technique, the progressive\nevaluation of cluster outliers (PECO) which enables both the objective\nmeasurement of leakage, and the automated detection of subpopulations in the\ndata which maximally exhibit it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenda Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Attention Network for Stock Movements Prediction. (arXiv:2112.13593v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.13593","description":"<p>Stock prices move as piece-wise trending fluctuation rather than a purely\nrandom walk. Traditionally, the prediction of future stock movements is based\non the historical trading record. Nowadays, with the development of social\nmedia, many active participants in the market choose to publicize their\nstrategies, which provides a window to glimpse over the whole market's attitude\ntowards future movements by extracting the semantics behind social media.\nHowever, social media contains conflicting information and cannot replace\nhistorical records completely. In this work, we propose a multi-modality\nattention network to reduce conflicts and integrate semantic and numeric\nfeatures to predict future stock movements comprehensively. Specifically, we\nfirst extract semantic information from social media and estimate their\ncredibility based on posters' identity and public reputation. Then we\nincorporate the semantic from online posts and numeric features from historical\nrecords to make the trading strategy. Experimental results show that our\napproach outperforms previous methods by a significant margin in both\nprediction accuracy (61.20\\%) and trading profits (9.13\\%). It demonstrates\nthat our method improves the performance of stock movements prediction and\ninforms future research on multi-modality fusion towards stock prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shwai He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shi Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCROLLS: Standardized CompaRison Over Long Language Sequences. (arXiv:2201.03533v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.03533","description":"<p>NLP benchmarks have largely focused on short texts, such as sentences and\nparagraphs, even though long texts comprise a considerable amount of natural\nlanguage in the wild. We introduce SCROLLS, a suite of tasks that require\nreasoning over long texts. We examine existing long-text datasets, and handpick\nones where the text is naturally long, while prioritizing tasks that involve\nsynthesizing information across the input. SCROLLS contains summarization,\nquestion answering, and natural language inference tasks, covering multiple\ndomains, including literature, science, business, and entertainment. Initial\nbaselines, including Longformer Encoder-Decoder, indicate that there is ample\nroom for improvement on SCROLLS. We make all datasets available in a unified\ntext-to-text format and host a live leaderboard to facilitate research on model\narchitecture and pretraining methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaham_U/0/1/0/all/0/1\">Uri Shaham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segal_E/0/1/0/all/0/1\">Elad Segal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivgi_M/0/1/0/all/0/1\">Maor Ivgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efrat_A/0/1/0/all/0/1\">Avia Efrat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoran_O/0/1/0/all/0/1\">Ori Yoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haviv_A/0/1/0/all/0/1\">Adi Haviv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ankit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Wenhan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Training Data with Language Models: Towards Zero-Shot Language Understanding. (arXiv:2202.04538v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.04538","description":"<p>Pretrained language models (PLMs) have demonstrated remarkable performance in\nvarious natural language processing tasks: Unidirectional PLMs (e.g., GPT) are\nwell known for their superior text generation capabilities; bidirectional PLMs\n(e.g., BERT) have been the prominent choice for natural language understanding\n(NLU) tasks. While both types of models have achieved promising few-shot\nlearning performance, their potential for zero-shot learning has been\nunderexplored. In this paper, we present a simple approach that uses both types\nof PLMs for fully zero-shot learning of NLU tasks without requiring any\ntask-specific data: A unidirectional PLM generates class-conditioned texts\nguided by prompts, which are used as the training data for fine-tuning a\nbidirectional PLM. With quality training data selected based on the generation\nprobability and regularization techniques (label smoothing and temporal\nensembling) applied to the fine-tuning stage for better generalization and\nstability, our approach demonstrates strong performance across seven\nclassification tasks of the GLUE benchmark (e.g., 72.3/73.8 on MNLI-m/mm and\n92.8 on SST-2), significantly outperforming zero-shot prompting methods and\nachieving even comparable results to strong few-shot approaches using 32\ntraining samples per class.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual CheckList: Generation and Evaluation. (arXiv:2203.12865v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12865","description":"<p>Multilingual evaluation benchmarks usually contain limited high-resource\nlanguages and do not test models for specific linguistic capabilities.\nCheckList is a template-based evaluation approach that tests models for\nspecific capabilities. The CheckList template creation process requires native\nspeakers, posing a challenge in scaling to hundreds of languages. In this work,\nwe explore multiple approaches to generate Multilingual CheckLists. We device\nan algorithm - Template Extraction Algorithm (TEA) for automatically extracting\ntarget language CheckList templates from machine translated instances of a\nsource language templates. We compare the TEA CheckLists with CheckLists\ncreated with different levels of human intervention. We further introduce\nmetrics along the dimensions of cost, diversity, utility, and correctness to\ncompare the CheckLists. We thoroughly analyze different approaches to creating\nCheckLists in Hindi. Furthermore, we experiment with 9 more different\nlanguages. We find that TEA followed by human verification is ideal for scaling\nChecklist-based evaluation to multiple languages while TEA gives a good\nestimates of model performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+K_K/0/1/0/all/0/1\">Karthikeyan K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_S/0/1/0/all/0/1\">Shaily Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Pankaj Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aditya_S/0/1/0/all/0/1\">Somak Aditya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dandapat_S/0/1/0/all/0/1\">Sandipan Dandapat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitaram_S/0/1/0/all/0/1\">Sunayana Sitaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Representation Collapse of Sparse Mixture of Experts. (arXiv:2204.09179v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.09179","description":"<p>Sparse mixture of experts provides larger model capacity while requiring a\nconstant computational overhead. It employs the routing mechanism to distribute\ninput tokens to the best-matched experts according to their hidden\nrepresentations. However, learning such a routing mechanism encourages token\nclustering around expert centroids, implying a trend toward representation\ncollapse. In this work, we propose to estimate the routing scores between\ntokens and experts on a low-dimensional hypersphere. We conduct extensive\nexperiments on cross-lingual language model pre-training and fine-tuning on\ndownstream tasks. Experimental results across seven multilingual benchmarks\nshow that our method achieves consistent gains. We also present a comprehensive\nanalysis on the representation and routing behaviors of our models. Our method\nalleviates the representation collapse issue and achieves more consistent\nrouting than the baseline mixture-of-experts methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zewen Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Damai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patra_B/0/1/0/all/0/1\">Barun Patra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1\">Saksham Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_P/0/1/0/all/0/1\">Payal Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Revise References for Faithful Summarization. (arXiv:2204.10290v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10290","description":"<p>In real-world scenarios with naturally occurring datasets, reference\nsummaries are noisy and may contain information that cannot be inferred from\nthe source text. On large news corpora, removing low quality samples has been\nshown to reduce model hallucinations. Yet, for smaller, and/or noisier corpora,\nfiltering is detrimental to performance. To improve reference quality while\nretaining all data, we propose a new approach: to selectively re-write\nunsupported reference sentences to better reflect source data. We automatically\ngenerate a synthetic dataset of positive and negative revisions by corrupting\nsupported sentences and learn to revise reference sentences with contrastive\nlearning. The intensity of revisions is treated as a controllable attribute so\nthat, at inference, diverse candidates can be over-generated-then-rescored to\nbalance faithfulness and abstraction. To test our methods, we extract noisy\nreferences from publicly available MIMIC-III discharge summaries for the task\nof hospital-course summarization, and vary the data on which models are\ntrained. According to metrics and human evaluation, models trained on revised\nclinical references are much more faithful, informative, and fluent than models\ntrained on original or filtered data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adams_G/0/1/0/all/0/1\">Griffin Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shing_H/0/1/0/all/0/1\">Han-Chin Shing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winestock_C/0/1/0/all/0/1\">Christopher Winestock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhadad_N/0/1/0/all/0/1\">No&#xe9;mie Elhadad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech. (arXiv:2205.07211v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2205.07211","description":"<p>Style transfer for out-of-domain (OOD) speech synthesis aims to generate\nspeech samples with unseen style (e.g., speaker identity, emotion, and prosody)\nderived from an acoustic reference, while facing the following challenges: 1)\nThe highly dynamic style features in expressive voice are difficult to model\nand transfer; and 2) the TTS models should be robust enough to handle diverse\nOOD conditions that differ from the source data. This paper proposes\nGenerSpeech, a text-to-speech model towards high-fidelity zero-shot style\ntransfer of OOD custom voice. GenerSpeech decomposes the speech variation into\nthe style-agnostic and style-specific parts by introducing two components: 1) a\nmulti-level style adaptor to efficiently model a large range of style\nconditions, including global speaker and emotion characteristics, and the local\n(utterance, phoneme, and word-level) fine-grained prosodic representations; and\n2) a generalizable content adaptor with Mix-Style Layer Normalization to\neliminate style information in the linguistic content representation and thus\nimprove model generalization. Our evaluations on zero-shot style transfer\ndemonstrate that GenerSpeech surpasses the state-of-the-art models in terms of\naudio quality and style similarity. The extension studies to adaptive style\ntransfer further show that GenerSpeech performs robustly in the few-shot data\nsetting. Audio samples are available at \\url{https://GenerSpeech.github.io/}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_R/0/1/0/all/0/1\">Rongjie Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jinglin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_C/0/1/0/all/0/1\">Chenye Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Rule Induction for Efficient and Interpretable Semi-Supervised Learning. (arXiv:2205.09067v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09067","description":"<p>Semi-supervised learning has shown promise in allowing NLP models to\ngeneralize from small amounts of labeled data. Meanwhile, pretrained\ntransformer models act as black-box correlation engines that are difficult to\nexplain and sometimes behave unreliably. In this paper, we propose tackling\nboth of these challenges via Automatic Rule Induction (ARI), a simple and\ngeneral-purpose framework for the automatic discovery and integration of\nsymbolic rules into pretrained transformer models. First, we extract weak\nsymbolic rules from low-capacity machine learning models trained on small\namounts of labeled data. Next, we use an attention mechanism to integrate these\nrules into high-capacity pretrained transformer models. Last, the\nrule-augmented system becomes part of a self-training framework to boost\nsupervision signal on unlabeled data. These steps can be layered beneath a\nvariety of existing weak supervision and semi-supervised NLP algorithms in\norder to improve performance and interpretability. Experiments across nine\nsequence classification and relation extraction tasks suggest that ARI can\nimprove state-of-the-art methods with no manual effort and minimal\ncomputational overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1\">Reid Pryzant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-training with Two-phase Self-augmentation for Few-shot Dialogue Generation. (arXiv:2205.09661v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09661","description":"<p>In task-oriented dialogue systems, response generation from meaning\nrepresentations (MRs) often suffers from limited training examples, due to the\nhigh cost of annotating MR-to-Text pairs. Previous works on self-training\nleverage fine-tuned conversational models to automatically generate\npseudo-labeled MR-to-Text pairs for further fine-tuning. However, some\nself-augmented data may be noisy or uninformative for the model to learn from.\nIn this work, we propose a two-phase self-augmentation procedure to generate\nhigh-quality pseudo-labeled MR-to-Text pairs: the first phase selects the most\ninformative MRs based on model's prediction uncertainty; with the selected MRs,\nthe second phase generates accurate responses by aggregating multiple perturbed\nlatent representations from each MR. Empirical experiments on two benchmark\ndatasets, FewShotWOZ and FewShotSGD, show that our method generally outperforms\nexisting self-training methods on both automatic and human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wanyu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Label Errors using Pre-Trained Language Models. (arXiv:2205.12702v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12702","description":"<p>For identifying label errors in natural language datasets, we show that large\npre-trained language models are extremely capable: simply hand-verifying data\npoints in descending order of out-of-distribution model loss significantly\noutperforms more complex mechanisms proposed in previous work.\n</p>\n<p>We also contribute a novel method for producing human-originated label noise\nusing existing crowdsourced datasets, apply it to SNLI and TweetNLP, and show\nthat the resulting errors have similar characteristics to real label errors. We\npresent evidence that pre-training provides limited robustness to this more\nrealistic form of label noise.\n</p>\n<p>Finally, we use crowdsourced verification to evaluate performance on IMDB,\nAmazon Reviews, and Recon, and find that pre-trained models detect errors with\n9-36% higher absolute Area Under the Precision-Recall Curve compared to\nexisting models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chong_D/0/1/0/all/0/1\">Derek Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jenny Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Dense Graph Do You Need for Self-Attention?. (arXiv:2205.14014v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.14014","description":"<p>Transformers have made progress in miscellaneous tasks, but suffer from\nquadratic computational and memory complexities. Recent works propose sparse\nTransformers with attention on sparse graphs to reduce complexity and remain\nstrong performance. While effective, the crucial parts of how dense a graph\nneeds to be to perform well are not fully explored. In this paper, we propose\nNormalized Information Payload (NIP), a graph scoring function measuring\ninformation transfer on graph, which provides an analysis tool for trade-offs\nbetween performance and complexity. Guided by this theoretical analysis, we\npresent Hypercube Transformer, a sparse Transformer that models token\ninteractions in a hypercube and shows comparable or even better results with\nvanilla Transformer while yielding $O(N\\log N)$ complexity with sequence length\n$N$. Experiments on tasks requiring various sequence lengths lay validation for\nour graph function well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chu-Tak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qipeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhangyue Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yunhua Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CEBaB: Estimating the Causal Effects of Real-World Concepts on NLP Model Behavior. (arXiv:2205.14140v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.14140","description":"<p>The increasing size and complexity of modern ML systems has improved their\npredictive capabilities but made their behavior harder to explain. Many\ntechniques for model explanation have been developed in response, but we lack\nclear criteria for assessing these techniques. In this paper, we cast model\nexplanation as the causal inference problem of estimating causal effects of\nreal-world concepts on the output behavior of ML models given actual input\ndata. We introduce CEBaB, a new benchmark dataset for assessing concept-based\nexplanation methods in Natural Language Processing (NLP). CEBaB consists of\nshort restaurant reviews with human-generated counterfactual reviews in which\nan aspect (food, noise, ambiance, service) of the dining experience was\nmodified. Original and counterfactual reviews are annotated with\nmultiply-validated sentiment ratings at the aspect-level and review-level. The\nrich structure of CEBaB allows us to go beyond input features to study the\neffects of abstract, real-world concepts on model behavior. We use CEBaB to\ncompare the quality of a range of concept-based explanation methods covering\ndifferent assumptions and conceptions of the problem, and we seek to establish\nnatural metrics for comparative assessments of these methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abraham_E/0/1/0/all/0/1\">Eldar David Abraham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DOosterlinck_K/0/1/0/all/0/1\">Karel D&#x27;Oosterlinck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1\">Amir Feder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gat_Y/0/1/0/all/0/1\">Yair Ori Gat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Atticus Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhengxuan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UTTS: Unsupervised TTS with Conditional Disentangled Sequential Variational Auto-encoder. (arXiv:2206.02512v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2206.02512","description":"<p>In this paper, we propose a novel unsupervised text-to-speech (UTTS)\nframework which does not require text-audio pairs for the TTS acoustic modeling\n(AM). UTTS is a multi-speaker speech synthesizer that supports zero-shot voice\ncloning, it is developed from a perspective of disentangled speech\nrepresentation learning. The framework offers a flexible choice of a speaker's\nduration model, timbre feature (identity) and content for TTS inference. We\nleverage recent advancements in self-supervised speech representation learning\nas well as speech synthesis front-end techniques for system development.\nSpecifically, we employ our recently formulated Conditional Disentangled\nSequential Variational Auto-encoder (C-DSVAE) as the backbone UTTS AM, which\noffers well-structured content representations given unsupervised alignment\n(UA) as condition during training. For UTTS inference, we utilize a lexicon to\nmap input text to the phoneme sequence, which is expanded to the frame-level\nforced alignment (FA) with a speaker-dependent duration model. Then, we develop\nan alignment mapping module that converts FA to UA. Finally, the C-DSVAE,\nserving as the self-supervised TTS AM, takes the predicted UA and a target\nspeaker embedding to generate the mel spectrogram, which is ultimately\nconverted to waveform with a neural vocoder. We show how our method enables\nspeech synthesis without using a paired TTS corpus. Experiments demonstrate\nthat UTTS can synthesize speech of high naturalness and intelligibility\nmeasured by human and objective evaluations. Audio samples are available at our\ndemo page https://neurtts.github.io/utts_demo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lian_J/0/1/0/all/0/1\">Jiachen Lian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chunlei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anumanchipalli_G/0/1/0/all/0/1\">Gopala Krishna Anumanchipalli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STable: Table Generation Framework for Encoder-Decoder Models. (arXiv:2206.04045v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.04045","description":"<p>The output structure of database-like tables, consisting of values structured\nin horizontal rows and vertical columns identifiable by name, can cover a wide\nrange of NLP tasks. Following this constatation, we propose a framework for\ntext-to-table neural models applicable to problems such as extraction of line\nitems, joint entity and relation extraction, or knowledge base population. The\npermutation-based decoder of our proposal is a generalized sequential method\nthat comprehends information from all cells in the table. The training\nmaximizes the expected log-likelihood for a table's content across all random\npermutations of the factorization order. During the content inference, we\nexploit the model's ability to generate cells in any order by searching over\npossible orderings to maximize the model's confidence and avoid substantial\nerror accumulation, which other sequential models are prone to. Experiments\ndemonstrate a high practical value of the framework, which establishes\nstate-of-the-art results on several challenging datasets, outperforming\nprevious solutions by up to 15%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pietruszka_M/0/1/0/all/0/1\">Micha&#x142; Pietruszka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turski_M/0/1/0/all/0/1\">Micha&#x142; Turski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borchmann_L/0/1/0/all/0/1\">&#x141;ukasz Borchmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwojak_T/0/1/0/all/0/1\">Tomasz Dwojak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palka_G/0/1/0/all/0/1\">Gabriela Pa&#x142;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szyndler_K/0/1/0/all/0/1\">Karolina Szyndler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurkiewicz_D/0/1/0/all/0/1\">Dawid Jurkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garncarek_L/0/1/0/all/0/1\">&#x141;ukasz Garncarek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLIPv2: Unifying Localization and Vision-Language Understanding. (arXiv:2206.05836v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.05836","description":"<p>We present GLIPv2, a grounded VL understanding model, that serves both\nlocalization tasks (e.g., object detection, instance segmentation) and\nVision-Language (VL) understanding tasks (e.g., VQA, image captioning). GLIPv2\nelegantly unifies localization pre-training and Vision-Language Pre-training\n(VLP) with three pre-training tasks: phrase grounding as a VL reformulation of\nthe detection task, region-word contrastive learning as a novel region-word\nlevel contrastive learning task, and the masked language modeling. This\nunification not only simplifies the previous multi-stage VLP procedure but also\nachieves mutual benefits between localization and understanding tasks.\nExperimental results show that a single GLIPv2 model (all model weights are\nshared) achieves near SoTA performance on various localization and\nunderstanding tasks. The model also shows (1) strong zero-shot and few-shot\nadaption performance on open-vocabulary object detection tasks and (2) superior\ngrounding capability on VL understanding tasks. Code will be released at\nhttps://github.com/microsoft/GLIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haotian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models. (arXiv:2208.11445v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.11445","description":"<p>The ability to extrapolate, i.e., to make predictions on sequences that are\nlonger than those presented as training examples, is a challenging problem for\ncurrent deep learning models. Recent work shows that this limitation persists\nin state-of-the-art Transformer-based models. Most solutions to this problem\nuse specific architectures or training methods that do not generalize to other\ntasks. We demonstrate that large language models can succeed in extrapolation\nwithout modifying their architecture or training procedure. Our experimental\nresults show that generating step-by-step rationales and introducing marker\ntokens are both required for effective extrapolation. First, we induce a\nlanguage model to produce step-by-step rationales before outputting the answer\nto effectively communicate the task to the model. However, as sequences become\nlonger, we find that current models struggle to keep track of token positions.\nTo address this issue, we interleave output tokens with markup tokens that act\nas explicit positional and counting symbols. Our findings show how these two\ncomplementary approaches enable remarkable sequence extrapolation and highlight\na limitation of current architectures to effectively generalize without\nexplicit surface form guidance. Code available at\nhttps://github.com/MirelleB/induced-rationales-markup-tokens\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bueno_M/0/1/0/all/0/1\">Mirelle Bueno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemmel_C/0/1/0/all/0/1\">Carlos Gemmel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalton_J/0/1/0/all/0/1\">Jeffrey Dalton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotufo_R/0/1/0/all/0/1\">Roberto Lotufo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WeLM: A Well-Read Pre-trained Language Model for Chinese. (arXiv:2209.10372v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.10372","description":"<p>Large Language Models pre-trained with self-supervised learning have\ndemonstrated impressive zero-shot generalization capabilities on a wide\nspectrum of tasks. In this work, we present WeLM: a well-read pre-trained\nlanguage model for Chinese that is able to seamlessly perform different types\nof tasks with zero or few-shot demonstrations. WeLM is trained with 10B\nparameters by \"reading\" a curated high-quality corpus covering a wide range of\ntopics. We show that WeLM is equipped with broad knowledge on various domains\nand languages. On 18 monolingual (Chinese) tasks, WeLM can significantly\noutperform existing pre-trained models with similar sizes and match the\nperformance of models up to 25 times larger. WeLM also exhibits strong\ncapabilities in multi-lingual and code-switching understanding, outperforming\nexisting multilingual language models pre-trained on 30 languages. Furthermore,\nWe collected human-written prompts for a large set of supervised datasets in\nChinese and fine-tuned WeLM with multi-prompted training. The resulting model\ncan attain strong generalization on unseen types of tasks and outperform the\nunsupervised WeLM in zero-shot learning. Finally, we demonstrate that WeLM has\nbasic skills at explaining and calibrating the decisions from itself, which can\nbe promising directions for future research. Our models can be applied from\nhttps://welm.weixin.qq.com/docs/api/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hui Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Houjin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zilin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re-contextualizing Fairness in NLP: The Case of India. (arXiv:2209.12226v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.12226","description":"<p>Recent research has revealed undesirable bi-ases in NLP data and models.\nHowever, theseefforts focus of social disparities in West, andare not directly\nportable to other geo-culturalcontexts. In this paper, we focus on NLP\nfair-ness in the context of India. We start witha brief account of the\nprominent axes of so-cial disparities in India. We build resourcesfor fairness\nevaluation in the Indian contextand use them to demonstrate prediction bi-ases\nalong some of the axes. We then delvedeeper into social stereotypes for Region\nandReligion, demonstrating its prevalence in cor-pora and models. Finally, we\noutline a holis-tic research agenda to re-contextualize NLPfairness research\nfor the Indian context, ac-counting for Indiansocietal context,\nbridgingtechnologicalgaps in NLP capabilities and re-sources, and adapting to\nIndian culturalvalues.While we focus on India, this framework canbe generalized\nto other geo-cultural contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_S/0/1/0/all/0/1\">Shaily Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dave_S/0/1/0/all/0/1\">Shachi Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1\">Vinodkumar Prabhakaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting Operations Research with Auto-Formulation of Optimization Models from Problem Descriptions. (arXiv:2209.15565v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.15565","description":"<p>We describe an augmented intelligence system for simplifying and enhancing\nthe modeling experience for operations research. Using this system, the user\nreceives a suggested formulation of an optimization problem based on its\ndescription. To facilitate this process, we build an intuitive user interface\nsystem that enables the users to validate and edit the suggestions. We\ninvestigate controlled generation techniques to obtain an automatic suggestion\nof formulation. Then, we evaluate their effectiveness with a newly created\ndataset of linear programming problems drawn from various application domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramamonjison_R/0/1/0/all/0/1\">Rindranirina Ramamonjison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haley Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Timothy T. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shiqi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rengan_V/0/1/0/all/0/1\">Vishnu Rengan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1\">Amin Banitalebi-Dehkordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zirui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformer Based Model for Describing a Set of Images as a Story. (arXiv:2210.02762v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.02762","description":"<p>Visual Story-Telling is the process of forming a multi-sentence story from a\nset of images. Appropriately including visual variation and contextual\ninformation captured inside the input images is one of the most challenging\naspects of visual storytelling. Consequently, stories developed from a set of\nimages often lack cohesiveness, relevance, and semantic relationship. In this\npaper, we propose a novel Vision Transformer Based Model for describing a set\nof images as a story. The proposed method extracts the distinct features of the\ninput images using a Vision Transformer (ViT). Firstly, input images are\ndivided into 16X16 patches and bundled into a linear projection of flattened\npatches. The transformation from a single image to multiple image patches\ncaptures the visual variety of the input visual patterns. These features are\nused as input to a Bidirectional-LSTM which is part of the sequence encoder.\nThis captures the past and future image context of all image patches. Then, an\nattention mechanism is implemented and used to increase the discriminatory\ncapacity of the data fed into the language model, i.e. a Mogrifier-LSTM. The\nperformance of our proposed model is evaluated using the Visual Story-Telling\ndataset (VIST), and the results show that our model outperforms the current\nstate of the art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malakan_Z/0/1/0/all/0/1\">Zainy M. Malakan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_G/0/1/0/all/0/1\">Ghulam Mubashar Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are All Steps Equally Important? Benchmarking Essentiality Detection of Events. (arXiv:2210.04074v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04074","description":"<p>Natural language often describes events in different granularities, such that\nmore coarse-grained (goal) events can often be decomposed into fine-grained\nsequences of (step) events. A critical but overlooked challenge in\nunderstanding an event process lies in the fact that the step events are not\nequally important to the central goal. In this paper, we seek to fill this gap\nby studying how well current models can understand the essentiality of\ndifferent step events towards a goal event. As discussed by cognitive studies,\nsuch an ability enables the machine to mimic human's commonsense reasoning\nabout preconditions and necessary efforts of daily-life tasks. Our work\ncontributes with a high-quality corpus of (goal, step) pairs from a community\nguideline website WikiHow, where the steps are manually annotated with their\nessentiality w.r.t. the goal. The high IAA indicates that humans have a\nconsistent understanding of the events. Despite evaluating various statistical\nand massive pre-trained NLU models, we observe that existing SOTA models all\nperform drastically behind humans, indicating the need for future investigation\nof this crucial yet challenging task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yueguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yuqian Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Copy the Teacher: Data and Model Challenges in Embodied Dialogue. (arXiv:2210.04443v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.04443","description":"<p>Embodied dialogue instruction following requires an agent to complete a\ncomplex sequence of tasks from a natural language exchange. The recent\nintroduction of benchmarks (Padmakumar et al., 2022) raises the question of how\nbest to train and evaluate models for this multi-turn, multi-agent,\nlong-horizon task. This paper contributes to that conversation, by arguing that\nimitation learning (IL) and related low-level metrics are actually misleading\nand do not align with the goals of embodied dialogue research and may hinder\nprogress. We provide empirical comparisons of metrics, analysis of three\nmodels, and make suggestions for how the field might best progress. First, we\nobserve that models trained with IL take spurious actions during evaluation.\nSecond, we find that existing models fail to ground query utterances, which are\nessential for task completion. Third, we argue evaluation should focus on\nhigher-level semantic goals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">So Yeon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YFACC: A Yor\\`ub\\'a speech-image dataset for cross-lingual keyword localisation through visual grounding. (arXiv:2210.04600v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04600","description":"<p>Visually grounded speech (VGS) models are trained on images paired with\nunlabelled spoken captions. Such models could be used to build speech systems\nin settings where it is impossible to get labelled data, e.g. for documenting\nunwritten languages. However, most VGS studies are in English or other\nhigh-resource languages. This paper attempts to address this shortcoming. We\ncollect and release a new single-speaker dataset of audio captions for 6k\nFlickr images in Yor\\`ub\\'a -- a real low-resource language spoken in Nigeria.\nWe train an attention-based VGS model where images are automatically tagged\nwith English visual labels and paired with Yor\\`ub\\'a utterances. This enables\ncross-lingual keyword localisation: a written English query is detected and\nlocated in Yor\\`ub\\'a speech. To quantify the effect of the smaller dataset, we\ncompare to English systems trained on similar and more data. We hope that this\nnew dataset will stimulate research in the use of VGS models for real\nlow-resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Olaleye_K/0/1/0/all/0/1\">Kayode Olaleye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oneata_D/0/1/0/all/0/1\">Dan Oneata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1\">Herman Kamper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Readability Controllable Biomedical Document Summarization. (arXiv:2210.04705v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04705","description":"<p>Different from general documents, it is recognised that the ease with which\npeople can understand a biomedical text is eminently varied, owing to the\nhighly technical nature of biomedical documents and the variance of readers'\ndomain knowledge. However, existing biomedical document summarization systems\nhave paid little attention to readability control, leaving users with summaries\nthat are incompatible with their levels of expertise. In recognition of this\nurgent demand, we introduce a new task of readability controllable\nsummarization for biomedical documents, which aims to recognise users'\nreadability demands and generate summaries that better suit their needs:\ntechnical summaries for experts and plain language summaries (PLS) for laymen.\nTo establish this task, we construct a corpus consisting of biomedical papers\nwith technical summaries and PLSs written by the authors, and benchmark\nmultiple advanced controllable abstractive and extractive summarization models\nbased on pre-trained language models (PLMs) with prevalent controlling and\ngeneration techniques. Moreover, we propose a novel masked language model (MLM)\nbased metric and its variant to effectively evaluate the readability\ndiscrepancy between lay and technical summaries. Experimental results from\nautomated and human evaluations show that though current control techniques\nallow for a certain degree of readability adjustment during generation, the\nperformance of existing controllable summarization methods is far from\ndesirable in this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zheheng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qianqian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1\">Sophia Ananiadou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Neural Referential Form Selectors on a Realistic Multilingual Dataset. (arXiv:2210.04828v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04828","description":"<p>Previous work on Neural Referring Expression Generation (REG) all uses\nWebNLG, an English dataset that has been shown to reflect a very limited range\nof referring expression (RE) use. To tackle this issue, we build a dataset\nbased on the OntoNotes corpus that contains a broader range of RE use in both\nEnglish and Chinese (a language that uses zero pronouns). We build neural\nReferential Form Selection (RFS) models accordingly, assess them on the dataset\nand conduct probing experiments. The experiments suggest that, compared to\nWebNLG, OntoNotes is better for assessing REG/RFS models. We compare English\nand Chinese RFS and confirm that, in line with linguistic theories, Chinese RFS\ndepends more on discourse context than English.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Same_F/0/1/0/all/0/1\">Fahime Same</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deemter_K/0/1/0/all/0/1\">Kees van Deemter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting or Guessing? Improving Faithfulness of Event Temporal Relation Extraction. (arXiv:2210.04992v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04992","description":"<p>In this paper, we seek to improve the faithfulness of TempRel extraction\nmodels from two perspectives. The first perspective is to extract genuinely\nbased on contextual description. To achieve this, we propose to conduct\ncounterfactual analysis to attenuate the effects of two significant types of\ntraining biases: the event trigger bias and the frequent label bias. We also\nadd tense information into event representations to explicitly place an\nemphasis on the contextual description. The second perspective is to provide\nproper uncertainty estimation and abstain from extraction when no relation is\ndescribed in the text. By parameterization of Dirichlet Prior over the\nmodel-predicted categorical distribution, we improve the model estimates of the\ncorrectness likelihood and make TempRel predictions more selective. We also\nemploy temperature scaling to recalibrate the model confidence measure after\nbias mitigation. Through experimental analysis on MATRES, MATRES-DS, and\nTDDiscourse, we demonstrate that our model extracts TempRel and timelines more\nfaithfully compared to SOTA methods, especially under distribution shifts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yuqian Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1\">Jacob R. Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PatternRank: Leveraging Pretrained Language Models and Part of Speech for Unsupervised Keyphrase Extraction. (arXiv:2210.05245v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05245","description":"<p>Keyphrase extraction is the process of automatically selecting a small set of\nmost relevant phrases from a given text. Supervised keyphrase extraction\napproaches need large amounts of labeled training data and perform poorly\noutside the domain of the training data. In this paper, we present PatternRank,\nwhich leverages pretrained language models and part-of-speech for unsupervised\nkeyphrase extraction from single documents. Our experiments show PatternRank\nachieves higher precision, recall and F1-scores than previous state-of-the-art\napproaches. In addition, we present the KeyphraseVectorizers package, which\nallows easy modification of part-of-speech patterns for candidate keyphrase\nselection, and hence adaptation of our approach to any domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schopf_T/0/1/0/all/0/1\">Tim Schopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klimek_S/0/1/0/all/0/1\">Simon Klimek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthes_F/0/1/0/all/0/1\">Florian Matthes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting and Advancing Chinese Natural Language Understanding with Accelerated Heterogeneous Knowledge Pre-training. (arXiv:2210.05287v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05287","description":"<p>Recently, knowledge-enhanced pre-trained language models (KEPLMs) improve\ncontext-aware representations via learning from structured relations in\nknowledge graphs, and/or linguistic knowledge from syntactic or dependency\nanalysis. Unlike English, there is a lack of high-performing open-source\nChinese KEPLMs in the natural language processing (NLP) community to support\nvarious language understanding applications. In this paper, we revisit and\nadvance the development of Chinese natural language understanding with a series\nof novel Chinese KEPLMs released in various parameter sizes, namely CKBERT\n(Chinese knowledge-enhanced BERT).Specifically, both relational and linguistic\nknowledge is effectively injected into CKBERT based on two novel pre-training\ntasks, i.e., linguistic-aware masked language modeling and contrastive\nmulti-hop relation modeling. Based on the above two pre-training paradigms and\nour in-house implemented TorchAccelerator, we have pre-trained base (110M),\nlarge (345M) and huge (1.3B) versions of CKBERT efficiently on GPU clusters.\nExperiments demonstrate that CKBERT outperforms strong baselines for Chinese\nover various benchmark NLP tasks and in terms of different model sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Taolin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Junwei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Ang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinghui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofeng He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19-related Nepali Tweets Classification in a Low Resource Setting. (arXiv:2210.05425v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2210.05425","description":"<p>Billions of people across the globe have been using social media platforms in\ntheir local languages to voice their opinions about the various topics related\nto the COVID-19 pandemic. Several organizations, including the World Health\nOrganization, have developed automated social media analysis tools that\nclassify COVID-19-related tweets into various topics. However, these tools that\nhelp combat the pandemic are limited to very few languages, making several\ncountries unable to take their benefit. While multi-lingual or low-resource\nlanguage-specific tools are being developed, they still need to expand their\ncoverage, such as for the Nepali language. In this paper, we identify the eight\nmost common COVID-19 discussion topics among the Twitter community using the\nNepali language, set up an online platform to automatically gather Nepali\ntweets containing the COVID-19-related keywords, classify the tweets into the\neight topics, and visualize the results across the period in a web-based\ndashboard. We compare the performance of two state-of-the-art multi-lingual\nlanguage models for Nepali tweet classification, one generic (mBERT) and the\nother Nepali language family-specific model (MuRIL). Our results show that the\nmodels' relative performance depends on the data size, with MuRIL doing better\nfor a larger dataset. The annotated data, models, and the web-based dashboard\nare open-sourced at https://github.com/naamiinepal/covid-tweet-classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adhikari_R/0/1/0/all/0/1\">Rabin Adhikari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thapaliya_S/0/1/0/all/0/1\">Safal Thapaliya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basnet_N/0/1/0/all/0/1\">Nirajan Basnet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poudel_S/0/1/0/all/0/1\">Samip Poudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakya_A/0/1/0/all/0/1\">Aman Shakya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanal_B/0/1/0/all/0/1\">Bishesh Khanal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-10-12T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}