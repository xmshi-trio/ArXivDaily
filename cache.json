{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-12-11T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"The Open Review-Based (ORB) dataset: Towards Automatic Assessment of Scientific Papers and Experiment Proposals in High-Energy Physics. (arXiv:2312.04576v1 [cs.DL])","link":"http://arxiv.org/abs/2312.04576","description":"<p>With the Open Science approach becoming important for research, the evolution\ntowards open scientific-paper reviews is making an impact on the scientific\ncommunity. However, there is a lack of publicly available resources for\nconducting research activities related to this subject, as only a limited\nnumber of journals and conferences currently allow access to their review\nprocess for interested parties. In this paper, we introduce the new\ncomprehensive Open Review-Based dataset (ORB); it includes a curated list of\nmore than 36,000 scientific papers with their more than 89,000 reviews and\nfinal decisions. We gather this information from two sources: the\nOpenReview.net and SciPost.org websites. However, given the volatile nature of\nthis domain, the software infrastructure that we introduce to supplement the\nORB dataset is designed to accommodate additional resources in the future. The\nORB deliverables include (1) Python code (interfaces and implementations) to\ntranslate document data and metadata into a structured and high-level\nrepresentation, (2) an ETL process (Extract, Transform, Load) to facilitate the\nautomatic updates from defined sources and (3) data files representing the\nstructured data. The paper presents our data architecture and an overview of\nthe collected data along with relevant statistics. For illustration purposes,\nwe also discuss preliminary Natural-Language-Processing-based experiments that\naim to predict (1) papers' acceptance based on their textual embeddings, and\n(2) grading statistics inferred from embeddings as well. We believe ORB\nprovides a valuable resource for researchers interested in open science and\nreview, with our implementation easing the use of this data for further\nanalysis and experimentation. We plan to update ORB as the field matures as\nwell as introduce new resources even more fitted to dedicated scientific\ndomains such as High-Energy Physics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Szumega_J/0/1/0/all/0/1\">Jaroslaw Szumega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bougueroua_L/0/1/0/all/0/1\">Lamine Bougueroua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkotse_B/0/1/0/all/0/1\">Blerina Gkotse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jouvelot_P/0/1/0/all/0/1\">Pierre Jouvelot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravotti_F/0/1/0/all/0/1\">Federico Ravotti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Psychological Generalist AI: A Survey of Current Applications of Large Language Models and Future Prospects. (arXiv:2312.04578v1 [cs.AI])","link":"http://arxiv.org/abs/2312.04578","description":"<p>The complexity of psychological principles underscore a significant societal\nchallenge, given the vast social implications of psychological problems.\nBridging the gap between understanding these principles and their actual\nclinical and real-world applications demands rigorous exploration and adept\nimplementation. In recent times, the swift advancement of highly adaptive and\nreusable artificial intelligence (AI) models has emerged as a promising way to\nunlock unprecedented capabilities in the realm of psychology. This paper\nemphasizes the importance of performance validation for these large-scale AI\nmodels, emphasizing the need to offer a comprehensive assessment of their\nverification from diverse perspectives. Moreover, we review the cutting-edge\nadvancements and practical implementations of these expansive models in\npsychology, highlighting pivotal work spanning areas such as social media\nanalytics, clinical nursing insights, vigilant community monitoring, and the\nnuanced exploration of psychological theories. Based on our review, we project\nan acceleration in the progress of psychological fields, driven by these\nlarge-scale AI models. These future generalist AI models harbor the potential\nto substantially curtail labor costs and alleviate social stress. However, this\nforward momentum will not be without its set of challenges, especially when\nconsidering the paradigm changes and upgrades required for medical\ninstrumentation and related applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1\">Guanghui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yijing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Changwei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Hongzhi Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1\">Dan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1\">Huijing Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bing Xiang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Sarcasm Detection with OpenAI GPT-based Models. (arXiv:2312.04642v1 [cs.CL])","link":"http://arxiv.org/abs/2312.04642","description":"<p>Sarcasm is a form of irony that requires readers or listeners to interpret\nits intended meaning by considering context and social cues. Machine learning\nclassification models have long had difficulty detecting sarcasm due to its\nsocial complexity and contradictory nature.\n</p>\n<p>This paper explores the applications of the Generative Pretrained Transformer\n(GPT) models, including GPT-3, InstructGPT, GPT-3.5, and GPT-4, in detecting\nsarcasm in natural language. It tests fine-tuned and zero-shot models of\ndifferent sizes and releases.\n</p>\n<p>The GPT models were tested on the political and balanced (pol-bal) portion of\nthe popular Self-Annotated Reddit Corpus (SARC 2.0) sarcasm dataset. In the\nfine-tuning case, the largest fine-tuned GPT-3 model achieves accuracy and\n$F_1$-score of 0.81, outperforming prior models. In the zero-shot case, one of\nGPT-4 models yields an accuracy of 0.70 and $F_1$-score of 0.75. Other models\nscore lower. Additionally, a model's performance may improve or deteriorate\nwith each release, highlighting the need to reassess performance after each\nrelease.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gole_M/0/1/0/all/0/1\">Montgomery Gole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nwadiugwu_W/0/1/0/all/0/1\">Williams-Paul Nwadiugwu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miranskyy_A/0/1/0/all/0/1\">Andriy Miranskyy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PyThaiNLP: Thai Natural Language Processing in Python. (arXiv:2312.04649v1 [cs.CL])","link":"http://arxiv.org/abs/2312.04649","description":"<p>We present PyThaiNLP, a free and open-source natural language processing\n(NLP) library for Thai language implemented in Python. It provides a wide range\nof software, models, and datasets for Thai language. We first provide a brief\nhistorical context of tools for Thai language prior to the development of\nPyThaiNLP. We then outline the functionalities it provided as well as datasets\nand pre-trained language models. We later summarize its development milestones\nand discuss our experience during its development. We conclude by demonstrating\nhow industrial and research communities utilize PyThaiNLP in their work. The\nlibrary is freely available at https://github.com/pythainlp/pythainlp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phatthiyaphaibun_W/0/1/0/all/0/1\">Wannaphong Phatthiyaphaibun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaovavanich_K/0/1/0/all/0/1\">Korakot Chaovavanich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polpanumas_C/0/1/0/all/0/1\">Charin Polpanumas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suriyawongkul_A/0/1/0/all/0/1\">Arthit Suriyawongkul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lowphansirikul_L/0/1/0/all/0/1\">Lalita Lowphansirikul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chormai_P/0/1/0/all/0/1\">Pattarawat Chormai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Limkonchotiwat_P/0/1/0/all/0/1\">Peerat Limkonchotiwat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suntorntip_T/0/1/0/all/0/1\">Thanathip Suntorntip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Udomcharoenchaikit_C/0/1/0/all/0/1\">Can Udomcharoenchaikit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Behavior Cloned Transformers are Path Crawlers for Text Games. (arXiv:2312.04657v1 [cs.CL])","link":"http://arxiv.org/abs/2312.04657","description":"<p>In this work, we introduce a self-supervised behavior cloning transformer for\ntext games, which are challenging benchmarks for multi-step reasoning in\nvirtual environments. Traditionally, Behavior Cloning Transformers excel in\nsuch tasks but rely on supervised training data. Our approach auto-generates\ntraining data by exploring trajectories (defined by common macro-action\nsequences) that lead to reward within the games, while determining the\ngenerality and utility of these trajectories by rapidly training small models\nthen evaluating their performance on unseen development games. Through\nempirical analysis, we show our method consistently uncovers generalizable\ntraining data, achieving about 90\\% performance of supervised systems across\nthree benchmark text games.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruoyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jansen_P/0/1/0/all/0/1\">Peter Jansen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TOD-Flow: Modeling the Structure of Task-Oriented Dialogues. (arXiv:2312.04668v1 [cs.CL])","link":"http://arxiv.org/abs/2312.04668","description":"<p>Task-Oriented Dialogue (TOD) systems have become crucial components in\ninteractive artificial intelligence applications. While recent advances have\ncapitalized on pre-trained language models (PLMs), they exhibit limitations\nregarding transparency and controllability. To address these challenges, we\npropose a novel approach focusing on inferring the TOD-Flow graph from dialogue\ndata annotated with dialog acts, uncovering the underlying task structure in\nthe form of a graph. The inferred TOD-Flow graph can be easily integrated with\nany dialogue model to improve its prediction performance, transparency, and\ncontrollability. Our TOD-Flow graph learns what a model can, should, and should\nnot predict, effectively reducing the search space and providing a rationale\nfor the model's prediction. We show that the proposed TOD-Flow graph better\nresembles human-annotated graphs compared to prior approaches. Furthermore,\nwhen combined with several dialogue policies and end-to-end dialogue models, we\ndemonstrate that our approach significantly improves dialog act classification\nand end-to-end response generation performance in the MultiWOZ and SGD\nbenchmarks. Code available at: https://github.com/srsohn/TOD-Flow\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1\">Sungryull Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Anthony Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Logeswaran_L/0/1/0/all/0/1\">Lajanugen Logeswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dong-Ki Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_D/0/1/0/all/0/1\">Dongsub Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Skill Discovery for Chain-of-Thought Reasoning. (arXiv:2312.04684v1 [cs.CL])","link":"http://arxiv.org/abs/2312.04684","description":"<p>Recent advances in Large Language Models (LLMs) have led to an emergent\nability of chain-of-thought (CoT) prompting, a prompt reasoning strategy that\nadds intermediate rationale steps between questions and answers to construct\nprompts. Conditioned on these prompts, LLMs can effectively learn in context to\ngenerate rationales that lead to more accurate answers than when answering the\nsame question directly. To design LLM prompts, one important setting, called\ndemonstration selection, considers selecting demonstrations from an example\nbank. Existing methods use various heuristics for this selection, but for CoT\nprompting, which involves unique rationales, it is essential to base the\nselection upon the intrinsic skills that CoT rationales need, for instance, the\nskills of addition or subtraction for math word problems.\n</p>\n<p>To address this requirement, we introduce a novel approach named Reasoning\nSkill Discovery (RSD) that use unsupervised learning to create a latent space\nrepresentation of rationales, called a reasoning skill. Simultaneously, RSD\nlearns a reasoning policy to determine the required reasoning skill for a given\nquestion. This can then guide the selection of examples that demonstrate the\nrequired reasoning skills. Our approach offers several desirable properties: it\nis (1) theoretically grounded, (2) sample-efficient, requiring no LLM inference\nor manual prompt design, and (3) LLM-agnostic. Empirically, RSD outperforms\nexisting methods by up to 6% in terms of the answer accuracy across multiple\nreasoning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zifan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haozhu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bespalov_D/0/1/0/all/0/1\">Dmitriy Bespalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1\">Peter Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yanjun Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simul-LLM: A Framework for Exploring High-Quality Simultaneous Translation with Large Language Models. (arXiv:2312.04691v1 [cs.CL])","link":"http://arxiv.org/abs/2312.04691","description":"<p>Large language models (LLMs) with billions of parameters and pretrained on\nmassive amounts of data are now capable of near or better than state-of-the-art\nperformance in a variety of downstream natural language processing tasks.\nNeural machine translation (NMT) is one such task that LLMs have been applied\nto with great success. However, little research has focused on applying LLMs to\nthe more difficult subset of NMT called simultaneous translation (SimulMT),\nwhere translation begins before the entire source context is available to the\nmodel. In this paper, we address key challenges facing LLMs fine-tuned for\nSimulMT, validate classical SimulMT concepts and practices in the context of\nLLMs, explore adapting LLMs that are fine-tuned for NMT to the task of SimulMT,\nand introduce Simul-LLM, the first open-source fine-tuning and evaluation\npipeline development framework for LLMs focused on SimulMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agostinelli_V/0/1/0/all/0/1\">Victor Agostinelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wild_M/0/1/0/all/0/1\">Max Wild</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_M/0/1/0/all/0/1\">Matthew Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuad_K/0/1/0/all/0/1\">Kazi Asif Fuad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lizhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Emotions Across Languages: A Novel Approach for Sentiment Propagation in Multilingual WordNets. (arXiv:2312.04715v1 [cs.CL])","link":"http://arxiv.org/abs/2312.04715","description":"<p>Sentiment analysis involves using WordNets enriched with emotional metadata,\nwhich are valuable resources. However, manual annotation is time-consuming and\nexpensive, resulting in only a few WordNet Lexical Units being annotated. This\npaper introduces two new techniques for automatically propagating sentiment\nannotations from a partially annotated WordNet to its entirety and to a WordNet\nin a different language: Multilingual Structured Synset Embeddings (MSSE) and\nCross-Lingual Deep Neural Sentiment Propagation (CLDNS). We evaluated the\nproposed MSSE+CLDNS method extensively using Princeton WordNet and Polish\nWordNet, which have many inter-lingual relations. Our results show that the\nMSSE+CLDNS method outperforms existing propagation methods, indicating its\neffectiveness in enriching WordNets with emotional metadata across multiple\nlanguages. This work provides a solid foundation for large-scale, multilingual\nsentiment analysis and is valuable for academic research and practical\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kocon_J/0/1/0/all/0/1\">Jan Koco&#x144;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Big to Small Without Losing It All: Text Augmentation with ChatGPT for Efficient Sentiment Analysis. (arXiv:2312.04720v1 [cs.CL])","link":"http://arxiv.org/abs/2312.04720","description":"<p>In the era of artificial intelligence, data is gold but costly to annotate.\nThe paper demonstrates a groundbreaking solution to this dilemma using ChatGPT\nfor text augmentation in sentiment analysis. We leverage ChatGPT's generative\ncapabilities to create synthetic training data that significantly improves the\nperformance of smaller models, making them competitive with, or even\noutperforming, their larger counterparts. This innovation enables models to be\nboth efficient and effective, thereby reducing computational cost, inference\ntime, and memory usage without compromising on quality. Our work marks a key\nadvancement in the cost-effective development and deployment of robust\nsentiment analysis models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wozniak_S/0/1/0/all/0/1\">Stanis&#x142;aw Wo&#x17a;niak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocon_J/0/1/0/all/0/1\">Jan Koco&#x144;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Feedback All You Need? Leveraging Natural Language Feedback in Goal-Conditioned Reinforcement Learning. (arXiv:2312.04736v1 [cs.CL])","link":"http://arxiv.org/abs/2312.04736","description":"<p>Despite numerous successes, the field of reinforcement learning (RL) remains\nfar from matching the impressive generalisation power of human behaviour\nlearning. One possible way to help bridge this gap be to provide RL agents with\nricher, more human-like feedback expressed in natural language. To investigate\nthis idea, we first extend BabyAI to automatically generate language feedback\nfrom the environment dynamics and goal condition success. Then, we modify the\nDecision Transformer architecture to take advantage of this additional signal.\nWe find that training with language feedback either in place of or in addition\nto the return-to-go or goal descriptions improves agents' generalisation\nperformance, and that agents can benefit from feedback even when this is only\navailable during training, but not at inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McCallum_S/0/1/0/all/0/1\">Sabrina McCallum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_Davies_M/0/1/0/all/0/1\">Max Taylor-Davies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1\">Stefano V. Albrecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suglia_A/0/1/0/all/0/1\">Alessandro Suglia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Large Language Models Fine-Tuning On Graphs. (arXiv:2312.04737v1 [cs.LG])","link":"http://arxiv.org/abs/2312.04737","description":"<p>Learning from Text-Attributed Graphs (TAGs) has attracted significant\nattention due to its wide range of real-world applications. The rapid evolution\nof large language models (LLMs) has revolutionized the way we process textual\ndata, which indicates a strong potential to replace shallow text embedding\ngenerally used in Graph Neural Networks (GNNs). However, we find that existing\nLLM approaches that exploit text information in graphs suffer from inferior\ncomputation and data efficiency. In this work, we introduce a novel and\nefficient approach for the end-to-end fine-tuning of Large Language Models\n(LLMs) on TAGs, named LEADING. The proposed approach maintains computation cost\nand memory overhead comparable to the graph-less fine-tuning of LLMs. Moreover,\nit transfers the rick knowledge in LLMs to downstream graph learning tasks\neffectively with limited labeled data in semi-supervised learning. Its superior\ncomputation and data efficiency are demonstrated through comprehensive\nexperiments, offering a promising solution for a wide range of LLMs and graph\nlearning tasks on TAGs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_R/0/1/0/all/0/1\">Rui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xipeng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1\">Ruozhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaorui Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized Narratives from Open-Source Histopathology Videos. (arXiv:2312.04746v1 [cs.CV])","link":"http://arxiv.org/abs/2312.04746","description":"<p>The gigapixel scale of whole slide images (WSIs) poses a challenge for\nhistopathology multi-modal chatbots, requiring a global WSI analysis for\ndiagnosis, compounding evidence from different WSI patches. Current visual\ninstruction datasets, generated through large language models, focus on\ncreating question/answer pairs for individual image patches, which may lack\ndiagnostic capacity on their own in histopathology, further complicated by the\nabsence of spatial grounding in histopathology image captions. To bridge this\ngap, we introduce Quilt-Instruct, a large-scale dataset of 107,131\nhistopathology-specific instruction question/answer pairs, that is collected by\nleveraging educational histopathology videos from YouTube, which provides\nspatial localization of captions by automatically extracting narrators' cursor\nmovements. In addition, we provide contextual reasoning by extracting diagnosis\nand supporting facts from the entire video content to guide the extrapolative\nreasoning of GPT-4. Using Quilt-Instruct, we train Quilt-LLaVA, which can\nreason beyond the given single image patch, enabling diagnostic reasoning and\nthe capability of spatial awareness. To evaluate Quilt-LLaVA, we propose a\ncomprehensive evaluation dataset created from 985 images and 1283\nhuman-generated question-answers. We also thoroughly evaluate Quilt-LLaVA using\npublic histopathology datasets, where Quilt-LLaVA significantly outperforms\nSOTA by over 10% on relative GPT-4 score and 4% and 9% on open and closed set\nVQA. Our code, data, and model are publicly available at quilt-llava.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seyfioglu_M/0/1/0/all/0/1\">Mehmet Saygin Seyfioglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikezogwo_W/0/1/0/all/0/1\">Wisdom O. Ikezogwo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghezloo_F/0/1/0/all/0/1\">Fatemeh Ghezloo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1\">Ranjay Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shapiro_L/0/1/0/all/0/1\">Linda Shapiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forcing Generative Models to Degenerate Ones: The Power of Data Poisoning Attacks. (arXiv:2312.04748v1 [cs.CR])","link":"http://arxiv.org/abs/2312.04748","description":"<p>Growing applications of large language models (LLMs) trained by a third party\nraise serious concerns on the security vulnerability of LLMs.It has been\ndemonstrated that malicious actors can covertly exploit these vulnerabilities\nin LLMs through poisoning attacks aimed at generating undesirable outputs.\nWhile poisoning attacks have received significant attention in the image domain\n(e.g., object detection), and classification tasks, their implications for\ngenerative models, particularly in the realm of natural language generation\n(NLG) tasks, remain poorly understood. To bridge this gap, we perform a\ncomprehensive exploration of various poisoning techniques to assess their\neffectiveness across a range of generative tasks. Furthermore, we introduce a\nrange of metrics designed to quantify the success and stealthiness of poisoning\nattacks specifically tailored to NLG tasks. Through extensive experiments on\nmultiple NLG tasks, LLMs and datasets, we show that it is possible to\nsuccessfully poison an LLM during the fine-tuning stage using as little as 1\\%\nof the total tuning data samples. Our paper presents the first systematic\napproach to comprehend poisoning attacks targeting NLG tasks considering a wide\nrange of triggers and attack settings. We hope our findings will assist the AI\nsecurity community in devising appropriate defenses against such threats.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuli Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadhe_S/0/1/0/all/0/1\">Swanand Ravindra Kadhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1\">Ling Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baracaldo_N/0/1/0/all/0/1\">Nathalie Baracaldo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"First Attempt at Building Parallel Corpora for Machine Translation of Northeast India's Very Low-Resource Languages. (arXiv:2312.04764v1 [cs.CL])","link":"http://arxiv.org/abs/2312.04764","description":"<p>This paper presents the creation of initial bilingual corpora for thirteen\nvery low-resource languages of India, all from Northeast India. It also\npresents the results of initial translation efforts in these languages. It\ncreates the first-ever parallel corpora for these languages and provides\ninitial benchmark neural machine translation results for these languages. We\nintend to extend these corpora to include a large number of low-resource Indian\nlanguages and integrate the effort with our prior work with African and\nAmerican-Indian languages to create corpora covering a large number of\nlanguages from across the world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tonja_A/0/1/0/all/0/1\">Atnafu Lambebo Tonja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mersha_M/0/1/0/all/0/1\">Melkamu Mersha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalita_A/0/1/0/all/0/1\">Ananya Kalita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolesnikova_O/0/1/0/all/0/1\">Olga Kolesnikova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1\">Jugal Kalita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Determine the Most Powerful Pre-trained Language Model without Brute Force Fine-tuning? An Empirical Survey. (arXiv:2312.04775v1 [cs.CL])","link":"http://arxiv.org/abs/2312.04775","description":"<p>Transferability estimation has been attached to great attention in the\ncomputer vision fields. Researchers try to estimate with low computational cost\nthe performance of a model when transferred from a source task to a given\ntarget task. Considering the effectiveness of such estimations, the communities\nof natural language processing also began to study similar problems for the\nselection of pre-trained language models. However, there is a lack of a\ncomprehensive comparison between these estimation methods yet. Also, the\ndifferences between vision and language scenarios make it doubtful whether\nprevious conclusions can be established across fields. In this paper, we first\nconduct a thorough survey of existing transferability estimation methods being\nable to find the most suitable model, then we conduct a detailed empirical\nstudy for the surveyed methods based on the GLUE benchmark. From qualitative\nand quantitative analyses, we demonstrate the strengths and weaknesses of\nexisting methods and show that H-Score generally performs well with\nsuperiorities in effectiveness and efficiency. We also outline the difficulties\nof consideration of training details, applicability to text generation, and\nconsistency to certain metrics which shed light on future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jun Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1\">Hanhua Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_W/0/1/0/all/0/1\">Wenge Rong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hate Cannot Drive out Hate: Forecasting Conversation Incivility following Replies to Hate Speech. (arXiv:2312.04804v1 [cs.CY])","link":"http://arxiv.org/abs/2312.04804","description":"<p>User-generated replies to hate speech are promising means to combat hatred,\nbut questions about whether they can stop incivility in follow-up conversations\nlinger. We argue that effective replies stop incivility from emerging in\nfollow-up conversations - replies that elicit more incivility are\ncounterproductive. This study introduces the task of predicting the incivility\nof conversations following replies to hate speech. We first propose a metric to\nmeasure conversation incivility based on the number of civil and uncivil\ncomments as well as the unique authors involved in the discourse. Our metric\napproximates human judgments more accurately than previous metrics. We then use\nthe metric to evaluate the outcomes of replies to hate speech. A linguistic\nanalysis uncovers the differences in the language of replies that elicit\nfollow-up conversations with high and low incivility. Experimental results show\nthat forecasting incivility is challenging. We close with a qualitative\nanalysis shedding light into the most common errors made by the best model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanco_E/0/1/0/all/0/1\">Eduardo Blanco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1\">Lingzi Hong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting. (arXiv:2312.04807v1 [cs.CL])","link":"http://arxiv.org/abs/2312.04807","description":"<p>Improving neural machine translation (NMT) systems with prompting has\nachieved significant progress in recent years. In this work, we focus on how to\nintegrate multi-knowledge, multiple types of knowledge, into NMT models to\nenhance the performance with prompting. We propose a unified framework, which\ncan integrate effectively multiple types of knowledge including sentences,\nterminologies/phrases and translation templates into NMT models. We utilize\nmultiple types of knowledge as prefix-prompts of input for the encoder and\ndecoder of NMT models to guide the translation process. The approach requires\nno changes to the model architecture and effectively adapts to domain-specific\ntranslation without retraining. The experiments on English-Chinese and\nEnglish-German translation demonstrate that our approach significantly\noutperform strong baselines, achieving high translation quality and terminology\nmatch accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Ke Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HuRef: HUman-REadable Fingerprint for Large Language Models. (arXiv:2312.04828v1 [cs.CL])","link":"http://arxiv.org/abs/2312.04828","description":"<p>Protecting the copyright of large language models (LLMs) has become crucial\ndue to their resource-intensive training and accompanying carefully designed\nlicenses. However, identifying the original base model of an LLM is challenging\ndue to potential parameter alterations through fine-tuning or continued\npretraining. In this study, we introduce HuRef, a human-readable fingerprint\nfor LLMs that uniquely identifies the base model without exposing model\nparameters or interfering with training. We first observe that the vector\ndirection of LLM parameters remains stable after the model has converged during\npretraining, showing negligible perturbations through subsequent training\nsteps, including continued pretraining, supervised fine-tuning (SFT), and RLHF,\nwhich makes it a sufficient condition to identify the base model. The necessity\nis validated by continuing to train an LLM with an extra term to drive away the\nmodel parameters' direction and the model becomes damaged. However, this\ndirection is vulnerable to simple attacks like dimension permutation or matrix\nrotation, which significantly change it without affecting performance. To\naddress this, leveraging the Transformer structure, we systematically analyze\npotential attacks and define three invariant terms that identify an LLM's base\nmodel. We make these invariant terms human-readable by mapping them to a\nGaussian vector using a convolutional encoder and then converting it into a\nnatural image with StyleGAN2. Our method generates a dog image as an identity\nfingerprint for an LLM, where the dog's appearance strongly indicates the LLM's\nbase model. Experimental results across various LLMs demonstrate the\neffectiveness of our method, the generated dog image remains invariant to\ndifferent training steps, including SFT, RLHF, or even continued pretraining\nwith augmented vocabulary in a new language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1\">Boyi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chenghu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinbing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localized Symbolic Knowledge Distillation for Visual Commonsense Models. (arXiv:2312.04837v1 [cs.AI])","link":"http://arxiv.org/abs/2312.04837","description":"<p>Instruction following vision-language (VL) models offer a flexible interface\nthat supports a broad range of multimodal tasks in a zero-shot fashion.\nHowever, interfaces that operate on full images do not directly enable the user\nto \"point to\" and access specific regions within images. This capability is\nimportant not only to support reference-grounded VL benchmarks, but also, for\npractical applications that require precise within-image reasoning. We build\nLocalized Visual Commonsense models, which allow users to specify (multiple)\nregions as input. We train our model by sampling localized commonsense\nknowledge from a large language model (LLM): specifically, we prompt an LLM to\ncollect commonsense knowledge given a global literal image description and a\nlocal literal region description automatically generated by a set of VL models.\nWith a separately trained critic model that selects high-quality examples, we\nfind that training on the localized commonsense corpus can successfully distill\nexisting VL models to support a reference-as-input interface. Empirical results\nand human evaluations in a zero-shot setup demonstrate that our distillation\nmethod results in more precise VL models of reasoning compared to a baseline of\npassing a generated referring expression to an LLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jae Sung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1\">Khyathi Raghavi Chandu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qiuyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FREDSum: A Dialogue Summarization Corpus for French Political Debates. (arXiv:2312.04843v1 [cs.CL])","link":"http://arxiv.org/abs/2312.04843","description":"<p>Recent advances in deep learning, and especially the invention of\nencoder-decoder architectures, has significantly improved the performance of\nabstractive summarization systems. The majority of research has focused on\nwritten documents, however, neglecting the problem of multi-party dialogue\nsummarization. In this paper, we present a dataset of French political debates\nfor the purpose of enhancing resources for multi-lingual dialogue\nsummarization. Our dataset consists of manually transcribed and annotated\npolitical debates, covering a range of topics and perspectives. We highlight\nthe importance of high quality transcription and annotations for training\naccurate and effective dialogue summarization models, and emphasize the need\nfor multilingual resources to support dialogue summarization in non-English\nlanguages. We also provide baseline experiments using state-of-the-art methods,\nand encourage further research in this area to advance the field of dialogue\nsummarization. Our dataset will be made publicly available for use by the\nresearch community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rennard_V/0/1/0/all/0/1\">Virgile Rennard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_G/0/1/0/all/0/1\">Guokan Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grari_D/0/1/0/all/0/1\">Damien Grari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hunter_J/0/1/0/all/0/1\">Julie Hunter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1\">Michalis Vazirgiannis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Apollo's Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates. (arXiv:2312.04854v1 [cs.CL])","link":"http://arxiv.org/abs/2312.04854","description":"<p>Multi-agent debate systems are designed to derive accurate and consistent\nconclusions through adversarial interactions among agents. However, these\nsystems often encounter challenges due to cognitive constraints, manifesting as\n(1) agents' obstinate adherence to incorrect viewpoints and (2) their\npropensity to abandon correct viewpoints. These issues are primarily\nresponsible for the ineffectiveness of such debates. Addressing the challenge\nof cognitive constraints, we introduce a novel framework, the Multi-Agent\nDebate with Retrieval Augmented (MADRA). MADRA incorporates retrieval of prior\nknowledge into the debate process, effectively breaking cognitive constraints\nand enhancing the agents' reasoning capabilities. Furthermore, we have\ndeveloped a self-selection module within this framework, enabling agents to\nautonomously select pertinent evidence, thereby minimizing the impact of\nirrelevant or noisy data. We have comprehensively tested and analyzed MADRA\nacross six diverse datasets. The experimental results demonstrate that our\napproach significantly enhances performance across various tasks, proving the\neffectiveness of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haotian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xiyuan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Weijiang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qianglong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1\">Zheng Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1\">Lian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yi Guan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Explanations to Understand and Repair Embedding-based Entity Alignment. (arXiv:2312.04877v1 [cs.CL])","link":"http://arxiv.org/abs/2312.04877","description":"<p>Entity alignment seeks identical entities in different knowledge graphs,\nwhich is a long-standing task in the database research. Recent work leverages\ndeep learning to embed entities in vector space and align them via nearest\nneighbor search. Although embedding-based entity alignment has gained marked\nsuccess in recent years, it lacks explanations for alignment decisions. In this\npaper, we present the first framework that can generate explanations for\nunderstanding and repairing embedding-based entity alignment results. Given an\nentity alignment pair produced by an embedding model, we first compare its\nneighbor entities and relations to build a matching subgraph as a local\nexplanation. We then construct an alignment dependency graph to understand the\npair from an abstract perspective. Finally, we repair the pair by resolving\nthree types of alignment conflicts based on dependency graphs. Experiments on\nfive datasets demonstrate the effectiveness and generalization of our framework\nin explaining and repairing embedding-based entity alignment results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xiaobin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zequn Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predictive Chemistry Augmented with Text Retrieval. (arXiv:2312.04881v1 [cs.CL])","link":"http://arxiv.org/abs/2312.04881","description":"<p>This paper focuses on using natural language descriptions to enhance\npredictive models in the chemistry field. Conventionally, chemoinformatics\nmodels are trained with extensive structured data manually extracted from the\nliterature. In this paper, we introduce TextReact, a novel method that directly\naugments predictive chemistry with texts retrieved from the literature.\nTextReact retrieves text descriptions relevant for a given chemical reaction,\nand then aligns them with the molecular representation of the reaction. This\nalignment is enhanced via an auxiliary masked LM objective incorporated in the\npredictor training. We empirically validate the framework on two chemistry\ntasks: reaction condition recommendation and one-step retrosynthesis. By\nleveraging text retrieval, TextReact significantly outperforms state-of-the-art\nchemoinformatics models trained solely on molecular data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yujie Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhening Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhengkai Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coley_C/0/1/0/all/0/1\">Connor W. Coley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of Human- and AI-Generated Texts for English, French, German, and Spanish. (arXiv:2312.04882v1 [cs.CL])","link":"http://arxiv.org/abs/2312.04882","description":"<p>In this paper we analyze features to classify human- and AI-generated text\nfor English, French, German and Spanish and compare them across languages. We\ninvestigate two scenarios: (1) The detection of text generated by AI from\nscratch, and (2) the detection of text rephrased by AI. For training and\ntesting the classifiers in this multilingual setting, we created a new text\ncorpus covering 10 topics for each language. For the detection of AI-generated\ntext, the combination of all proposed features performs best, indicating that\nour features are portable to other related languages: The F1-scores are close\nwith 99% for Spanish, 98% for English, 97% for German and 95% for French. For\nthe detection of AI-rephrased text, the systems with all features outperform\nsystems with other features in many cases, but using only document features\nperforms best for German (72%) and Spanish (86%) and only text vector features\nleads to best results for English (78%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schaaff_K/0/1/0/all/0/1\">Kristina Schaaff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlippe_T/0/1/0/all/0/1\">Tim Schlippe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mindner_L/0/1/0/all/0/1\">Lorenz Mindner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KwaiAgents: Generalized Information-seeking Agent System with Large Language Models. (arXiv:2312.04889v1 [cs.AI])","link":"http://arxiv.org/abs/2312.04889","description":"<p>Driven by curiosity, humans have continually sought to explore and understand\nthe world around them, leading to the invention of various tools to satiate\nthis inquisitiveness. Despite not having the capacity to process and memorize\nvast amounts of information in their brains, humans excel in critical thinking,\nplanning, reflection, and harnessing available tools to interact with and\ninterpret the world, enabling them to find answers efficiently. The recent\nadvancements in large language models (LLMs) suggest that machines might also\npossess the aforementioned human-like capabilities, allowing them to exhibit\npowerful abilities even with a constrained parameter count. In this paper, we\nintroduce KwaiAgents, a generalized information-seeking agent system based on\nLLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its\ncognitive core, which is capable of understanding a user's query, behavior\nguidelines, and referencing external documents. The agent can also update and\nretrieve information from its internal memory, plan and execute actions using a\ntime-aware search-browse toolkit, and ultimately provide a comprehensive\nresponse. We further investigate the system's performance when powered by LLMs\nless advanced than GPT-4, and introduce the Meta-Agent Tuning (MAT) framework,\ndesigned to ensure even an open-sourced 7B or 13B model performs well among\nmany agent systems. We exploit both benchmark and human evaluations to\nsystematically validate these capabilities. Extensive experiments show the\nsuperiority of our agent system compared to other autonomous agents and\nhighlight the enhanced generalized agent-abilities of our fine-tuned LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Haojie Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_Z/0/1/0/all/0/1\">Zepeng Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yaojia Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1\">Ruiji Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ophtha-LLaMA2: A Large Language Model for Ophthalmology. (arXiv:2312.04906v1 [cs.CL])","link":"http://arxiv.org/abs/2312.04906","description":"<p>In recent years, pre-trained large language models (LLMs) have achieved\ntremendous success in the field of Natural Language Processing (NLP). Prior\nstudies have primarily focused on general and generic domains, with relatively\nless research on specialized LLMs in the medical field. The specialization and\nhigh accuracy requirements for diagnosis in the medical field, as well as the\nchallenges in collecting large-scale data, have constrained the application and\ndevelopment of LLMs in medical scenarios. In the field of ophthalmology,\nclinical diagnosis mainly relies on doctors' interpretation of reports and\nmaking diagnostic decisions. In order to take advantage of LLMs to provide\ndecision support for doctors, we collected three modalities of ophthalmic\nreport data and fine-tuned the LLaMA2 model, successfully constructing an LLM\ntermed the \"Ophtha-LLaMA2\" specifically tailored for ophthalmic disease\ndiagnosis. Inference test results show that even with a smaller fine-tuning\ndataset, Ophtha-LLaMA2 performs significantly better in ophthalmic diagnosis\ncompared to other LLMs. It demonstrates that the Ophtha-LLaMA2 exhibits\nsatisfying accuracy and efficiency in ophthalmic disease diagnosis, making it a\nvaluable tool for ophthalmologists to provide improved diagnostic support for\npatients. This research provides a useful reference for the application of LLMs\nin the field of ophthalmology, while showcasing the immense potential and\nprospects in this domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Huan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Q/0/1/0/all/0/1\">Qian Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yi Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_T/0/1/0/all/0/1\">Tianyang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jin-Yu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Junjie Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_F/0/1/0/all/0/1\">Fengqian Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhenxiang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yutong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">San-Hua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shi-Nan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Min Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yi Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zoology: Measuring and Improving Recall in Efficient Language Models. (arXiv:2312.04927v1 [cs.CL])","link":"http://arxiv.org/abs/2312.04927","description":"<p>Attention-free language models that combine gating and convolutions are\ngrowing in popularity due to their efficiency and increasingly competitive\nperformance. To better understand these architectures, we pretrain a suite of\n17 attention and \"gated-convolution\" language models, finding that SoTA\ngated-convolution architectures still underperform attention by up to 2.1\nperplexity points on the Pile. In fine-grained analysis, we find 82% of the gap\nis explained by each model's ability to recall information that is previously\nmentioned in-context, e.g. \"Hakuna Matata means no worries Hakuna Matata it\nmeans no\" $\\rightarrow$ \"??\". On this task, termed \"associative recall\", we\nfind that attention outperforms gated-convolutions by a large margin: a 70M\nparameter attention model outperforms a 1.4 billion parameter gated-convolution\nmodel on associative recall. This is surprising because prior work shows gated\nconvolutions can perfectly solve synthetic tests for AR capability. To close\nthe gap between synthetics and real language, we develop a new formalization of\nthe task called multi-query associative recall (MQAR) that better reflects\nactual language. We perform an empirical and theoretical study of MQAR that\nelucidates differences in the parameter-efficiency of attention and\ngated-convolution recall. Informed by our analysis, we evaluate simple\nconvolution-attention hybrids and show that hybrids with input-dependent sparse\nattention patterns can close 97.4% of the gap to attention, while maintaining\nsub-quadratic scaling. Our code is accessible at:\nhttps://github.com/HazyResearch/zoology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Simran Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eyuboglu_S/0/1/0/all/0/1\">Sabri Eyuboglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timalsina_A/0/1/0/all/0/1\">Aman Timalsina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_I/0/1/0/all/0/1\">Isys Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poli_M/0/1/0/all/0/1\">Michael Poli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudra_A/0/1/0/all/0/1\">Atri Rudra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The ICL Consistency Test. (arXiv:2312.04945v1 [cs.CL])","link":"http://arxiv.org/abs/2312.04945","description":"<p>Just like the previous generation of task-tuned models, large language models\n(LLMs) that are adapted to tasks via prompt-based methods like\nin-context-learning (ICL) perform well in some setups but not in others. This\nlack of consistency in prompt-based learning hints at a lack of robust\ngeneralisation. We here introduce the ICL consistency test -- a contribution to\nthe GenBench collaborative benchmark task (CBT) -- which evaluates how\nconsistent a model makes predictions across many different setups while using\nthe same data. The test is based on different established natural language\ninference tasks. We provide preprocessed data constituting 96 different\n'setups' and a metric that estimates model consistency across these setups. The\nmetric is provided on a fine-grained level to understand what properties of a\nsetup render predictions unstable and on an aggregated level to compare overall\nmodel consistency. We conduct an empirical analysis of eight state-of-the-art\nmodels, and our consistency metric reveals how all tested LLMs lack robust\ngeneralisation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weber_L/0/1/0/all/0/1\">Lucas Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruni_E/0/1/0/all/0/1\">Elia Bruni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1\">Dieuwke Hupkes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inversion-Free Image Editing with Natural Language. (arXiv:2312.04965v1 [cs.CV])","link":"http://arxiv.org/abs/2312.04965","description":"<p>Despite recent advances in inversion-based editing, text-guided image\nmanipulation remains challenging for diffusion models. The primary bottlenecks\ninclude 1) the time-consuming nature of the inversion process; 2) the struggle\nto balance consistency with accuracy; 3) the lack of compatibility with\nefficient consistency sampling methods used in consistency models. To address\nthe above issues, we start by asking ourselves if the inversion process can be\neliminated for editing. We show that when the initial sample is known, a\nspecial variance schedule reduces the denoising step to the same form as the\nmulti-step consistency sampling. We name this Denoising Diffusion Consistent\nModel (DDCM), and note that it implies a virtual inversion strategy without\nexplicit inversion in sampling. We further unify the attention control\nmechanisms in a tuning-free framework for text-guided editing. Combining them,\nwe present inversion-free editing (InfEdit), which allows for consistent and\nfaithful editing for both rigid and non-rigid semantic changes, catering to\nintricate modifications without compromising on the image's integrity and\nexplicit inversion. Through extensive experiments, InfEdit shows strong\nperformance in various editing tasks and also maintains a seamless workflow\n(less than 3 seconds on one single A40), demonstrating the potential for\nreal-time applications. Project Page: https://sled-group.github.io/InfEdit/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Sihan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yidong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jiayi Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Ziqiao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Prompt-Based Self-Training With Mapping-Free Automatic Verbalizer for Multi-Class Classification. (arXiv:2312.04982v1 [cs.CL])","link":"http://arxiv.org/abs/2312.04982","description":"<p>Recently, prompt-based fine-tuning has garnered considerable interest as a\ncore technique for few-shot text classification task. This approach\nreformulates the fine-tuning objective to align with the Masked Language\nModeling (MLM) objective. Leveraging unlabeled data, prompt-based self-training\nhas shown greater effectiveness in binary and three-class classification.\nHowever, prompt-based self-training for multi-class classification has not been\nadequately investigated, despite its significant applicability to real-world\nscenarios. Moreover, extending current methods to multi-class classification\nsuffers from the verbalizer that extracts the predicted value of manually\npre-defined single label word for each class from MLM predictions.\nConsequently, we introduce a novel, efficient verbalizer structure, named\nMapping-free Automatic Verbalizer (MAV). Comprising two fully connected layers,\nMAV serves as a trainable verbalizer that automatically extracts the requisite\nword features for classification by capitalizing on all available information\nfrom MLM predictions. Experimental results on five multi-class classification\ndatasets indicate MAV's superior self-training efficacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kho_Y/0/1/0/all/0/1\">Yookyung Kho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jaehee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_P/0/1/0/all/0/1\">Pilsung Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Converting Epics/Stories into Pseudocode using Transformers. (arXiv:2312.05047v1 [cs.CL])","link":"http://arxiv.org/abs/2312.05047","description":"<p>The conversion of user epics or stories into their appropriate representation\nin pseudocode or code is a time-consuming task, which can take up a large\nportion of the time in an industrial project. With this research paper, we aim\nto present a methodology to generate pseudocode from a given agile user story\nof small functionalities so as to reduce the overall time spent on the\nindustrial project. Pseudocode is a programming language agnostic\nrepresentation of the steps involved in a computer program, which can be easily\nconverted into any programming language. Leveraging the potential of Natural\nLanguage Processing, we want to simplify the development process in\norganizations that use the Agile Model of Software Development. We present a\nmethodology to convert a problem described in the English language into\npseudocode. This methodology divides the Text to Pseudocode conversion task\ninto two stages or subtasks, each of which is treated like an individual\nmachine translation task. Stage 1 is Text to Code Conversion and Stage 2 is\nCode to Pseudocode Conversion. We find that the CodeT5 model gives the best\nresults in terms of BLEU score when trained separately on the two subtasks\nmentioned above. BLEU score is a metric that is used to measure the similarity\nbetween a machine-translated text and a set of reference translations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolhatkar_G/0/1/0/all/0/1\">Gaurav Kolhatkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madan_A/0/1/0/all/0/1\">Akshit Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowtal_N/0/1/0/all/0/1\">Nidhi Kowtal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Satyajit Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonawane_S/0/1/0/all/0/1\">Sheetal Sonawane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaCour!: Enabling Research on Argumentation in Hearings of the European Court of Human Rights. (arXiv:2312.05061v1 [cs.CL])","link":"http://arxiv.org/abs/2312.05061","description":"<p>Why does an argument end up in the final court decision? Was it deliberated\nor questioned during the oral hearings? Was there something in the hearings\nthat triggered a particular judge to write a dissenting opinion? Despite the\navailability of the final judgments of the European Court of Human Rights\n(ECHR), none of these legal research questions can currently be answered as the\nECHR's multilingual oral hearings are not transcribed, structured, or\nspeaker-attributed. We address this fundamental gap by presenting LaCour!, the\nfirst corpus of textual oral arguments of the ECHR, consisting of 154 full\nhearings (2.1 million tokens from over 267 hours of video footage) in English,\nFrench, and other court languages, each linked to the corresponding final\njudgment documents. In addition to the transcribed and partially manually\ncorrected text from the video, we provide sentence-level timestamps and\nmanually annotated role and language labels. We also showcase LaCour! in a set\nof preliminary experiments that explore the interplay between questions and\ndissenting opinions. Apart from the use cases in legal NLP, we hope that law\nstudents or other interested parties will also use LaCour! as a learning\nresource, as it is freely available in various formats at\nhttps://huggingface.co/datasets/TrustHLT/LaCour.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Held_L/0/1/0/all/0/1\">Lena Held</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habernal_I/0/1/0/all/0/1\">Ivan Habernal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TMID: A Comprehensive Real-world Dataset for Trademark Infringement Detection in E-Commerce. (arXiv:2312.05103v1 [cs.CL])","link":"http://arxiv.org/abs/2312.05103","description":"<p>Annually, e-commerce platforms incur substantial financial losses due to\ntrademark infringements, making it crucial to identify and mitigate potential\nlegal risks tied to merchant information registered to the platforms. However,\nthe absence of high-quality datasets hampers research in this area. To address\nthis gap, our study introduces TMID, a novel dataset to detect trademark\ninfringement in merchant registrations. This is a real-world dataset sourced\ndirectly from Alipay, one of the world's largest e-commerce and digital payment\nplatforms. As infringement detection is a legal reasoning task requiring an\nunderstanding of the contexts and legal rules, we offer a thorough collection\nof legal rules and merchant and trademark-related contextual information with\nannotations from legal experts. We ensure the data quality by performing an\nextensive statistical analysis. Furthermore, we conduct an empirical study on\nthis dataset to highlight its value and the key challenges. Through this study,\nwe aim to contribute valuable resources to advance research into legal\ncompliance related to trademark infringement within the e-commerce sphere. The\ndataset is available at https://github.com/emnlpTMID/emnlpTMID.github.io .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tongxin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lizhen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Lengthy to Lucid: A Systematic Literature Review on NLP Techniques for Taming Long Sentences. (arXiv:2312.05172v1 [cs.CL])","link":"http://arxiv.org/abs/2312.05172","description":"<p>Long sentences have been a persistent issue in written communication for many\nyears since they make it challenging for readers to grasp the main points or\nfollow the initial intention of the writer. This survey, conducted using the\nPRISMA guidelines, systematically reviews two main strategies for addressing\nthe issue of long sentences: a) sentence compression and b) sentence splitting.\nAn increased trend of interest in this area has been observed since 2005, with\nsignificant growth after 2017. Current research is dominated by supervised\napproaches for both sentence compression and splitting. Yet, there is a\nconsiderable gap in weakly and self-supervised techniques, suggesting an\nopportunity for further research, especially in domains with limited data. In\nthis survey, we categorize and group the most representative methods into a\ncomprehensive taxonomy. We also conduct a comparative evaluation analysis of\nthese methods on common sentence compression and splitting datasets. Finally,\nwe discuss the challenges and limitations of current methods, providing\nvaluable insights for future research directions. This survey is meant to serve\nas a comprehensive resource for addressing the complexities of long sentences.\nWe aim to enable researchers to make further advancements in the field until\nlong sentences are no longer a barrier to effective communication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Passali_T/0/1/0/all/0/1\">Tatiana Passali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzikyriakidis_E/0/1/0/all/0/1\">Efstathios Chatzikyriakidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreadis_S/0/1/0/all/0/1\">Stelios Andreadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stavropoulos_T/0/1/0/all/0/1\">Thanos G. Stavropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matonaki_A/0/1/0/all/0/1\">Anastasia Matonaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fachantidis_A/0/1/0/all/0/1\">Anestis Fachantidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1\">Grigorios Tsoumakas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PathFinder: Guided Search over Multi-Step Reasoning Paths. (arXiv:2312.05180v1 [cs.CL])","link":"http://arxiv.org/abs/2312.05180","description":"<p>With recent advancements in large language models, methods like\nchain-of-thought prompting to elicit reasoning chains have been shown to\nimprove results on reasoning tasks. However, tasks that require multiple steps\nof reasoning still pose significant challenges to state-of-the-art models.\nDrawing inspiration from the beam search algorithm, we propose PathFinder, a\ntree-search-based reasoning path generation approach. It enhances diverse\nbranching and multi-hop reasoning through the integration of dynamic decoding,\nenabled by varying sampling methods and parameters. Using constrained\nreasoning, PathFinder integrates novel quality constraints, pruning, and\nexploration methods to enhance the efficiency and the quality of generation.\nMoreover, it includes scoring and ranking features to improve candidate\nselection. Our approach outperforms competitive baselines on three complex\narithmetic and commonsense reasoning tasks by 6% on average. Our model\ngeneralizes well to longer, unseen reasoning chains, reflecting similar\ncomplexities to beam search with large branching factors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Golovneva_O/0/1/0/all/0/1\">Olga Golovneva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OBrien_S/0/1/0/all/0/1\">Sean O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1\">Ramakanth Pasunuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianlu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazel_Zarandi_M/0/1/0/all/0/1\">Maryam Fazel-Zarandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seamless: Multilingual Expressive and Streaming Speech Translation. (arXiv:2312.05187v1 [cs.CL])","link":"http://arxiv.org/abs/2312.05187","description":"<p>Large-scale automatic speech translation systems today lack key features that\nhelp machine-mediated communication feel seamless when compared to\nhuman-to-human dialogue. In this work, we introduce a family of models that\nenable end-to-end expressive and multilingual translations in a streaming\nfashion. First, we contribute an improved version of the massively multilingual\nand multimodal SeamlessM4T model-SeamlessM4T v2. This newer model,\nincorporating an updated UnitY2 framework, was trained on more low-resource\nlanguage data. SeamlessM4T v2 provides the foundation on which our next two\nmodels are initiated. SeamlessExpressive enables translation that preserves\nvocal styles and prosody. Compared to previous efforts in expressive speech\nresearch, our work addresses certain underexplored aspects of prosody, such as\nspeech rate and pauses, while also preserving the style of one's voice. As for\nSeamlessStreaming, our model leverages the Efficient Monotonic Multihead\nAttention mechanism to generate low-latency target translations without waiting\nfor complete source utterances. As the first of its kind, SeamlessStreaming\nenables simultaneous speech-to-speech/text translation for multiple source and\ntarget languages. To ensure that our models can be used safely and responsibly,\nwe implemented the first known red-teaming effort for multimodal machine\ntranslation, a system for the detection and mitigation of added toxicity, a\nsystematic evaluation of gender bias, and an inaudible localized watermarking\nmechanism designed to dampen the impact of deepfakes. Consequently, we bring\nmajor components from SeamlessExpressive and SeamlessStreaming together to form\nSeamless, the first publicly available system that unlocks expressive\ncross-lingual communication in real-time. The contributions to this work are\npublicly released and accessible at\nhttps://github.com/facebookresearch/seamless_communication\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Communication_S/0/1/0/all/0/1\">Seamless Communication</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrault_L/0/1/0/all/0/1\">Lo&#xef;c Barrault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1\">Yu-An Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meglioli_M/0/1/0/all/0/1\">Mariano Coria Meglioli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dale_D/0/1/0/all/0/1\">David Dale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1\">Ning Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duppenthaler_M/0/1/0/all/0/1\">Mark Duppenthaler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duquenne_P/0/1/0/all/0/1\">Paul-Ambroise Duquenne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellis_B/0/1/0/all/0/1\">Brian Ellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsahar_H/0/1/0/all/0/1\">Hady Elsahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haaheim_J/0/1/0/all/0/1\">Justin Haaheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1\">John Hoffman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_M/0/1/0/all/0/1\">Min-Jae Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klaiber_C/0/1/0/all/0/1\">Christopher Klaiber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulikov_I/0/1/0/all/0/1\">Ilia Kulikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pengwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Licht_D/0/1/0/all/0/1\">Daniel Licht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maillard_J/0/1/0/all/0/1\">Jean Maillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavlyutov_R/0/1/0/all/0/1\">Ruslan Mavlyutov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakotoarison_A/0/1/0/all/0/1\">Alice Rakotoarison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadagopan_K/0/1/0/all/0/1\">Kaushik Ram Sadagopan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_A/0/1/0/all/0/1\">Abinesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Tuan Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenzek_G/0/1/0/all/0/1\">Guillaume Wenzek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yilin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_E/0/1/0/all/0/1\">Ethan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evtimov_I/0/1/0/all/0/1\">Ivan Evtimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_P/0/1/0/all/0/1\">Pierre Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Cynthia Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansanti_P/0/1/0/all/0/1\">Prangthip Hansanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalbassi_E/0/1/0/all/0/1\">Elahe Kalbassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kallet_A/0/1/0/all/0/1\">Amanda Kallet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozhevnikov_A/0/1/0/all/0/1\">Artyom Kozhevnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_G/0/1/0/all/0/1\">Gabriel Mejia Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roman_R/0/1/0/all/0/1\">Robin San Roman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Touret_C/0/1/0/all/0/1\">Christophe Touret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Corinne Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wood_C/0/1/0/all/0/1\">Carleigh Wood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bokai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andrews_P/0/1/0/all/0/1\">Pierre Andrews</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balioglu_C/0/1/0/all/0/1\">Can Balioglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng-Jen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elbayad_M/0/1/0/all/0/1\">Maha Elbayad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1\">Francisco Guzm&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heffernan_K/0/1/0/all/0/1\">Kevin Heffernan</a>, et al. (17 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DelucionQA: Detecting Hallucinations in Domain-specific Question Answering. (arXiv:2312.05200v1 [cs.CL])","link":"http://arxiv.org/abs/2312.05200","description":"<p>Hallucination is a well-known phenomenon in text generated by large language\nmodels (LLMs). The existence of hallucinatory responses is found in almost all\napplication scenarios e.g., summarization, question-answering (QA) etc. For\napplications requiring high reliability (e.g., customer-facing assistants), the\npotential existence of hallucination in LLM-generated text is a critical\nproblem. The amount of hallucination can be reduced by leveraging information\nretrieval to provide relevant background information to the LLM. However, LLMs\ncan still generate hallucinatory content for various reasons (e.g.,\nprioritizing its parametric knowledge over the context, failure to capture the\nrelevant information from the context, etc.). Detecting hallucinations through\nautomated methods is thus paramount. To facilitate research in this direction,\nwe introduce a sophisticated dataset, DelucionQA, that captures hallucinations\nmade by retrieval-augmented LLMs for a domain-specific QA task. Furthermore, we\npropose a set of hallucination detection methods to serve as baselines for\nfuture works from the research community. Analysis and case study are also\nprovided to share valuable insights on hallucination phenomena in the target\nscenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sadat_M/0/1/0/all/0/1\">Mobashir Sadat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhengyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lange_L/0/1/0/all/0/1\">Lukas Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araki_J/0/1/0/all/0/1\">Jun Araki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gundroo_A/0/1/0/all/0/1\">Arsalan Gundroo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bingqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_R/0/1/0/all/0/1\">Rakesh R Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parvez_M/0/1/0/all/0/1\">Md Rizwan Parvez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhe Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HALO: An Ontology for Representing Hallucinations in Generative Models. (arXiv:2312.05209v1 [cs.AI])","link":"http://arxiv.org/abs/2312.05209","description":"<p>Recent progress in generative AI, including large language models (LLMs) like\nChatGPT, has opened up significant opportunities in fields ranging from natural\nlanguage processing to knowledge discovery and data mining. However, there is\nalso a growing awareness that the models can be prone to problems such as\nmaking information up or `hallucinations', and faulty reasoning on seemingly\nsimple problems. Because of the popularity of models like ChatGPT, both\nacademic scholars and citizen scientists have documented hallucinations of\nseveral different types and severity. Despite this body of work, a formal model\nfor describing and representing these hallucinations (with relevant meta-data)\nat a fine-grained level, is still lacking. In this paper, we address this gap\nby presenting the Hallucination Ontology or HALO, a formal, extensible ontology\nwritten in OWL that currently offers support for six different types of\nhallucinations known to arise in LLMs, along with support for provenance and\nexperimental metadata. We also collect and publish a dataset containing\nhallucinations that we inductively gathered across multiple independent Web\nsources, and show that HALO can be successfully used to model this dataset and\nanswer competency questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nananukul_N/0/1/0/all/0/1\">Navapat Nananukul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kejriwal_M/0/1/0/all/0/1\">Mayank Kejriwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning. (arXiv:2312.05230v1 [cs.AI])","link":"http://arxiv.org/abs/2312.05230","description":"<p>Despite their tremendous success in many applications, large language models\noften fall short of consistent reasoning and planning in various (language,\nembodied, and social) scenarios, due to inherent limitations in their\ninference, learning, and modeling capabilities. In this position paper, we\npresent a new perspective of machine reasoning, LAW, that connects the concepts\nof Language models, Agent models, and World models, for more robust and\nversatile reasoning capabilities. In particular, we propose that world and\nagent models are a better abstraction of reasoning, that introduces the crucial\nelements of deliberate human-like reasoning, including beliefs about the world\nand other agents, anticipation of consequences, goals/rewards, and strategic\nplanning. Crucially, language models in LAW serve as a backend to implement the\nsystem or its elements and hence provide the computational power and\nadaptability. We review the recent studies that have made relevant progress and\ndiscuss future research directions towards operationalizing the LAW framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1\">Tianmin Shu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seeing ChatGPT Through Universities' Policies, Resources and Guidelines. (arXiv:2312.05235v1 [cs.CL])","link":"http://arxiv.org/abs/2312.05235","description":"<p>The advancements in Artificial Intelligence (AI) technologies such as ChatGPT\nhave gained popularity in recent days. The integration of ChatGPT in\neducational contexts has already created attractions due to a wide range of\napplications. However, the automatic generation of human-like texts also poses\npotential risks to academic integrity, especially when faced with\nwriting-intensive language courses. Considering the ongoing debates, this study\naims to investigate the academic policies and guidelines established by US\nuniversities regarding the use of ChatGPT in teaching and learning. The data\nsources include academic policies, statements, guidelines as well as relevant\nresources that were provided by the top 50 universities in the United States,\naccording to U.S. News. Thematic analysis and qualitative analysis were\nemployed in the analysis and showed that most top 50 universities were open but\ncautious towards the integration of generative AI in teaching and learning and\nalso expressed their concerns on ethical usage, accuracy, and data privacy.\nMost universities also provided a variety of resources and guidelines,\nincluding syllabus templates/samples, workshops and discussions, shared\narticles, and one-on-one consultations, with focuses on general technical\nintroduction, ethical concerns, pedagogical applications, preventive\nstrategies, data privacy, limitations, and detective tools. The findings will\ninform future policy-making regarding the integration of ChatGPT in\ncollege-level education and influence the provision of supportive resources by\nuniversities for the appropriate application of ChatGPT in education.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_A/0/1/0/all/0/1\">Anh Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mac_S/0/1/0/all/0/1\">Son Mac</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Cognitive Architecture for Machine Consciousness and Artificial Superintelligence: Thought Is Structured by the Iterative Updating of Working Memory. (arXiv:2203.17255v5 [q-bio.NC] UPDATED)","link":"http://arxiv.org/abs/2203.17255","description":"<p>This article provides an analytical framework for how to simulate human-like\nthought processes within a computer. It describes how attention and memory\nshould be structured, updated, and utilized to search for associative additions\nto the stream of thought. The focus is on replicating the mammalian working\nmemory system, which features two forms of persistent activity: sustained\nfiring (preserving information on the order of seconds) and synaptic\npotentiation (preserving information from minutes to hours). The article uses a\nseries of over 40 original figures to systematically demonstrate how the\niterative updating of these working memory stores provides functional structure\nto thought and consciousness. In an AI implementation, these two stores should\nbe updated continuously and in an iterative fashion, meaning each state should\npreserve a proportion of the coactive representations from the state before it.\nThus, the set of concepts in working memory will evolve gradually and\nincrementally over time. This makes each state a revised iteration of the\npreceding state and causes successive states to overlap and blend with respect\nto the information they contain. Transitions between states happen as\npersistent activity spreads activation energy throughout the hierarchical\nnetwork searching long-term memory for the most appropriate representation to\nbe added to the global workspace. The result is a chain of associatively linked\nintermediate states capable of advancing toward a solution or goal. Iterative\nupdating is conceptualized here as an information processing strategy, a model\nof working memory, a theory of consciousness, and an algorithm for designing\nand programming artificial general intelligence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Reser_J/0/1/0/all/0/1\">Jared Edward Reser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Z-BERT-A: a zero-shot Pipeline for Unknown Intent detection. (arXiv:2208.07084v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.07084","description":"<p>Intent discovery is a crucial task in natural language processing, and it is\nincreasingly relevant for various of industrial applications. Identifying\nnovel, unseen intents from user inputs remains one of the biggest challenges in\nthis field. Herein, we propose Zero-Shot-BERT-Adapters, a two-stage method for\nmultilingual intent discovery relying on a Transformer architecture, fine-tuned\nwith Adapters. We train the model for Natural Language Inference (NLI) and\nlater perform unknown intent classification in a zero-shot setting for multiple\nlanguages. In our evaluation, we first analyze the quality of the model after\nadaptive fine-tuning on known classes. Secondly, we evaluate its performance in\ncasting intent classification as an NLI task. Lastly, we test the zero-shot\nperformance of the model on unseen classes, showing how Zero-Shot-BERT-Adapters\ncan effectively perform intent discovery by generating semantically similar\nintents, if not equal, to the ground-truth ones. Our experiments show how\nZero-Shot-BERT-Adapters outperforms various baselines in two zero-shot\nsettings: known intent classification and unseen intent discovery. The proposed\npipeline holds the potential for broad application in customer care. It enables\nautomated dynamic triage using a lightweight model that can be easily deployed\nand scaled in various business scenarios, unlike large language models.\nZero-Shot-BERT-Adapters represents an innovative multi-language approach for\nintent discovery, enabling the online generation of novel intents. A Python\npackage implementing the pipeline and the new datasets we compiled are\navailable at the following link:\nhttps://github.com/GT4SD/zero-shot-bert-adapters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Comi_D/0/1/0/all/0/1\">Daniele Comi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christofidellis_D/0/1/0/all/0/1\">Dimitrios Christofidellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piazza_P/0/1/0/all/0/1\">Pier Francesco Piazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manica_M/0/1/0/all/0/1\">Matteo Manica</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSI++: Updating Transformer Memory with New Documents. (arXiv:2212.09744v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09744","description":"<p>Differentiable Search Indices (DSIs) encode a corpus of documents in model\nparameters and use the same model to answer user queries directly. Despite the\nstrong performance of DSI models, deploying them in situations where the corpus\nchanges over time is computationally expensive because reindexing the corpus\nrequires re-training the model. In this work, we introduce DSI++, a continual\nlearning challenge for DSI to incrementally index new documents while being\nable to answer queries related to both previously and newly indexed documents.\nAcross different model scales and document identifier representations, we show\nthat continual indexing of new documents leads to considerable forgetting of\npreviously indexed documents. We also hypothesize and verify that the model\nexperiences forgetting events during training, leading to unstable learning. To\nmitigate these issues, we investigate two approaches. The first focuses on\nmodifying the training dynamics. Flatter minima implicitly alleviate\nforgetting, so we optimize for flatter loss basins and show that the model\nstably memorizes more documents ($+12\\%$). Next, we introduce a generative\nmemory to sample pseudo-queries for documents and supplement them during\ncontinual indexing to prevent forgetting for the retrieval task. Extensive\nexperiments on novel continual indexing benchmarks based on Natural Questions\n(NQ) and MS MARCO demonstrate that our proposed solution mitigates forgetting\nsignificantly. Concretely, it improves the average Hits@10 by $+21.1\\%$ over\ncompetitive baselines for NQ and requires $6$ times fewer model updates\ncompared to re-training the DSI model for incrementally indexing five corpora\nin a sequence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1\">Sanket Vaibhav Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1\">Jai Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jinfeng Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najork_M/0/1/0/all/0/1\">Marc Najork</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1\">Emma Strubell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SEAM: An Integrated Activation-Coupled Model of Sentence Processing and Eye Movements in Reading. (arXiv:2303.05221v3 [q-bio.NC] UPDATED)","link":"http://arxiv.org/abs/2303.05221","description":"<p>Models of eye-movement control during reading, developed largely within\npsychology, usually focus on visual, attentional, lexical, and motor processes\nbut neglect post-lexical language processing; by contrast, models of sentence\ncomprehension processes, developed largely within psycholinguistics, generally\nfocus only on post-lexical language processes. We present a model that combines\nthese two research threads, by integrating eye-movement control and sentence\nprocessing. Developing such an integrated model is extremely challenging and\ncomputationally demanding, but such an integration is an important step toward\ncomplete mathematical models of natural language comprehension in reading. We\ncombine the SWIFT model of eye-movement control (Seelig et al., 2020,\ndoi:10.1016/j.jmp.<a href=\"/abs/2019.10231\">2019.10231</a>3) with key components of the Lewis and Vasishth\nsentence processing model (Lewis &amp; Vasishth, 2005,\ndoi:10.1207/s15516709cog0000_25). This integration becomes possible, for the\nfirst time, due in part to recent advances in successful parameter\nidentification in dynamical models, which allows us to investigate profile\nlog-likelihoods for individual model parameters. We present a fully implemented\nproof-of-concept model demonstrating how such an integrated model can be\nachieved; our approach includes Bayesian model inference with Markov Chain\nMonte Carlo (MCMC) sampling as a key computational tool. The integrated\nSentence-Processing and Eye-Movement Activation-Coupled Model (SEAM) can\nsuccessfully reproduce eye movement patterns that arise due to similarity-based\ninterference in reading. To our knowledge, this is the first-ever integration\nof a complete process model of eye-movement control with linguistic dependency\ncompletion processes in sentence comprehension. In future work, this proof of\nconcept model will need to be evaluated using a comprehensive set of benchmark\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Rabe_M/0/1/0/all/0/1\">Maximilian M. Rabe</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Paape_D/0/1/0/all/0/1\">Dario Paape</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Mertzen_D/0/1/0/all/0/1\">Daniela Mertzen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Vasishth_S/0/1/0/all/0/1\">Shravan Vasishth</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Engbert_R/0/1/0/all/0/1\">Ralf Engbert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Responsible AI in the Era of Generative AI: A Reference Architecture for Designing Foundation Model based Systems. (arXiv:2304.11090v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.11090","description":"<p>The release of ChatGPT has drawn huge interests on foundations models. There\nis a broad consensus that foundations models will be the fundamental building\nblocks for future AI systems. However, there is a lack of systematic guidance\non the architecture design. Particularly, the the rapidly growing capabilities\nof foundations models can eventually absorb other components of AI systems,\nposing challenges of moving boundary and interface evolution in architecture\ndesign. Furthermore, incorporating foundations models into AI systems raises\nsignificant concerns about responsible AI due to their opaque nature and\nrapidly advancing intelligence. To address these challenges, the paper first\npresents an architecture evolution of AI systems in the era of foundation\nmodels, transitioning from \"foundation-model-as-a-connector\" to\n\"foundation-model-as-a-monolithic architecture\". The paper then identifies key\ndesign decisions and proposes a pattern-oriented reference architecture for\ndesigning responsible foundation-model-based systems. The patterns can enable\nthe potential of foundation models while minimising associated risks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qinghua Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Liming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1\">Zhenchang Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whittle_J/0/1/0/all/0/1\">Jon Whittle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NarrativeXL: A Large-scale Dataset For Long-Term Memory Models. (arXiv:2305.13877v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13877","description":"<p>We propose a new large-scale (nearly a million questions) ultra-long-context\n(more than 50,000 words average document length) reading comprehension dataset.\nUsing GPT 3.5, we summarized each scene in 1,500 hand-curated fiction books\nfrom Project Gutenberg, which resulted in approximately 150 scene-level\nsummaries per book. After that, we created a number of reading comprehension\nquestions based on these summaries, including three types of multiple-choice\nscene recognition questions, as well as free-form narrative reconstruction\nquestions. With 990,595 total questions, our dataset is an order of magnitude\nlarger than the closest alternatives. Crucially, most questions have a known\n``retention demand'', indicating how long-term of a memory is needed to answer\nthem, which should aid long-term memory performance evaluation. We validate our\ndata in four small-scale experiments: one with human labelers, and three with\nexisting language models. We show that our questions 1) adequately represent\nthe source material 2) can be used to diagnose a model's memory capacity 3) are\nnot trivial for modern language models even when the memory demand does not\nexceed those models' context lengths. Lastly, we provide our code which can be\nused to further expand the dataset with minimal human labor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moskvichev_A/0/1/0/all/0/1\">Arseny Moskvichev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_K/0/1/0/all/0/1\">Ky-Vinh Mai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning. (arXiv:2305.14160v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14160","description":"<p>In-context learning (ICL) emerges as a promising capability of large language\nmodels (LLMs) by providing them with demonstration examples to perform diverse\ntasks. However, the underlying mechanism of how LLMs learn from the provided\ncontext remains under-explored. In this paper, we investigate the working\nmechanism of ICL through an information flow lens. Our findings reveal that\nlabel words in the demonstration examples function as anchors: (1) semantic\ninformation aggregates into label word representations during the shallow\ncomputation layers' processing; (2) the consolidated information in label words\nserves as a reference for LLMs' final predictions. Based on these insights, we\nintroduce an anchor re-weighting method to improve ICL performance, a\ndemonstration compression technique to expedite inference, and an analysis\nframework for diagnosing ICL errors in GPT2-XL. The promising applications of\nour findings again validate the uncovered ICL working mechanism and pave the\nway for future studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lean Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Damai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Deli Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Private Transformers for Multi-Domain Dialog Systems. (arXiv:2305.14208v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14208","description":"<p>Large, general purpose language models have demonstrated impressive\nperformance across many different conversational domains. While multi-domain\nlanguage models achieve low overall perplexity, their outputs are not\nguaranteed to stay within the domain of a given input prompt. This paper\nproposes domain privacy as a novel way to quantify how likely a conditional\nlanguage model will leak across domains. We also develop policy functions based\non token-level domain classification, and propose an efficient fine-tuning\nmethod to improve the trained model's domain privacy. Experiments on membership\ninference attacks show that our proposed method has comparable resiliency to\nmethods adapted from recent literature on differentially private language\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kabra_A/0/1/0/all/0/1\">Anmol Kabra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elenberg_E/0/1/0/all/0/1\">Ethan R. Elenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALGO: Synthesizing Algorithmic Programs with LLM-Generated Oracle Verifiers. (arXiv:2305.14591v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14591","description":"<p>Large language models (LLMs) excel at implementing code from functionality\ndescriptions but struggle with algorithmic problems that require not only\nimplementation but also identification of the suitable algorithm. Moreover,\nLLM-generated programs lack guaranteed correctness and require human\nverification. To address these challenges, we propose ALGO, a framework that\nsynthesizes Algorithmic programs with LLM-Generated Oracles to guide the\ngeneration and verify their correctness. ALGO first generates a reference\noracle by prompting an LLM to exhaustively enumerate all the combinations of\nrelevant variables. This oracle is then utilized to guide an arbitrary search\nstrategy in exploring the algorithm space and to verify the synthesized\nalgorithms. Our study shows that the LLM-generated oracles are correct for 88%\nof the cases. With the oracles as verifiers, ALGO can be integrated with any\nexisting code generation model in a model-agnostic manner to enhance its\nperformance. Experiments show that when equipped with ALGO, we achieve an 8x\nbetter one-submission pass rate over the Codex model and a 2.6x better\none-submission pass rate over CodeT, the current state-of-the-art model on\nCodeContests. We can also get 1.3x better pass rate over the ChatGPT Code\nInterpreter on unseen problems. The problem set we used for testing, the\nprompts we used, the verifier and solution programs, and the test cases\ngenerated by ALGO are available at https://github.com/zkx06111/ALGO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kexun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Danqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Jingtao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark. (arXiv:2305.14938v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14938","description":"<p>Large language models (LLMs) have been shown to perform well at a variety of\nsyntactic, discourse, and reasoning tasks. While LLMs are increasingly deployed\nin many forms including conversational agents that interact with humans, we\nlack a grounded benchmark to measure how well LLMs understand \\textit{social}\nlanguage. Here, we introduce a new theory-driven benchmark, SocKET, that\ncontains 58 NLP tasks testing social knowledge which we group into five\ncategories: humor &amp; sarcasm, offensiveness, sentiment &amp; emotion, and\ntrustworthiness. In tests on the benchmark, we demonstrate that current models\nattain only moderate performance but reveal significant potential for task\ntransfer among different types and categories of tasks, which were predicted\nfrom theory. Through zero-shot evaluations, we show that pretrained models\nalready possess some innate but limited capabilities of social language\nunderstanding and training on one category of tasks can improve zero-shot\ntesting on others. Our benchmark provides a systematic way to analyze model\nperformance on an important dimension of language and points to clear room for\nimprovement to build more socially-aware LLMs. The associated resources are\nreleased at https://github.com/minjechoi/SOCKET.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1\">Minje Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jiaxin Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sagar Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_C/0/1/0/all/0/1\">Chang Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Max-Margin Token Selection in Attention Mechanism. (arXiv:2306.13596v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2306.13596","description":"<p>Attention mechanism is a central component of the transformer architecture\nwhich led to the phenomenal success of large language models. However, the\ntheoretical principles underlying the attention mechanism are poorly\nunderstood, especially its nonconvex optimization dynamics. In this work, we\nexplore the seminal softmax-attention model $f(\\boldsymbol{X})=\\langle\n\\boldsymbol{Xv}, \\texttt{softmax}(\\boldsymbol{XWp})\\rangle$, where\n$\\boldsymbol{X}$ is the token sequence and\n$(\\boldsymbol{v},\\boldsymbol{W},\\boldsymbol{p})$ are trainable parameters. We\nprove that running gradient descent on $\\boldsymbol{p}$, or equivalently\n$\\boldsymbol{W}$, converges in direction to a max-margin solution that\nseparates $\\textit{locally-optimal}$ tokens from non-optimal ones. This clearly\nformalizes attention as an optimal token selection mechanism. Remarkably, our\nresults are applicable to general data and precisely characterize\n$\\textit{optimality}$ of tokens in terms of the value embeddings\n$\\boldsymbol{Xv}$ and problem geometry. We also provide a broader\nregularization path analysis that establishes the margin maximizing nature of\nattention even for nonlinear prediction heads. When optimizing $\\boldsymbol{v}$\nand $\\boldsymbol{p}$ simultaneously with logistic loss, we identify conditions\nunder which the regularization paths directionally converge to their respective\nhard-margin SVM solutions where $\\boldsymbol{v}$ separates the input features\nbased on their labels. Interestingly, the SVM formulation of $\\boldsymbol{p}$\nis influenced by the support vector geometry of $\\boldsymbol{v}$. Finally, we\nverify our theoretical findings via numerical experiments and provide insights.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tarzanagh_D/0/1/0/all/0/1\">Davoud Ataee Tarzanagh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingcong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuechen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1\">Samet Oymak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Trustworthiness Landscape of State-of-the-art Generative Models: A Survey and Outlook. (arXiv:2307.16680v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2307.16680","description":"<p>Diffusion models and large language models have emerged as leading-edge\ngenerative models, revolutionizing various aspects of human life. However, the\npractical implementations of these models have also exposed inherent risks,\nbringing to the forefront their evil sides and sparking concerns regarding\ntheir trustworthiness. Despite the wealth of literature on this subject, a\ncomprehensive survey specifically delving into the intersection of large-scale\ngenerative models and their trustworthiness remains largely absent. To bridge\nthis gap, this paper investigates both the long-standing and emerging threats\nassociated with these models across four fundamental dimensions: 1) privacy, 2)\nsecurity, 3) fairness, and 4) responsibility. Based on the investigation\nresults, we develop an extensive map outlining the trustworthiness of large\ngenerative models. After that, we provide practical recommendations and\npotential research directions for future secure applications equipped with\nlarge generative models, ultimately promoting the trustworthiness of the models\nand benefiting the society as a whole.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1\">Mingyuan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FIND: A Function Description Benchmark for Evaluating Interpretability Methods. (arXiv:2309.03886v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.03886","description":"<p>Labeling neural network submodules with human-legible descriptions is useful\nfor many downstream tasks: such descriptions can surface failures, guide\ninterventions, and perhaps even explain important model behaviors. To date,\nmost mechanistic descriptions of trained networks have involved small models,\nnarrowly delimited phenomena, and large amounts of human labor. Labeling all\nhuman-interpretable sub-computations in models of increasing size and\ncomplexity will almost certainly require tools that can generate and validate\ndescriptions automatically. Recently, techniques that use learned models\nin-the-loop for labeling have begun to gain traction, but methods for\nevaluating their efficacy are limited and ad-hoc. How should we validate and\ncompare open-ended labeling tools? This paper introduces FIND (Function\nINterpretation and Description), a benchmark suite for evaluating the building\nblocks of automated interpretability methods. FIND contains functions that\nresemble components of trained neural networks, and accompanying descriptions\nof the kind we seek to generate. The functions span textual and numeric\ndomains, and involve a range of real-world complexities. We evaluate methods\nthat use pretrained language models (LMs) to produce descriptions of function\nbehavior in natural language and code. Additionally, we introduce a new\ninteractive method in which an Automated Interpretability Agent (AIA) generates\nfunction descriptions. We find that an AIA, built from an LM with black-box\naccess to functions, can infer function structure, acting as a scientist by\nforming hypotheses, proposing experiments, and updating descriptions in light\nof new data. However, AIA descriptions tend to capture global function behavior\nand miss local details. These results suggest that FIND will be useful for\nevaluating more sophisticated interpretability methods before they are applied\nto real-world models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwettmann_S/0/1/0/all/0/1\">Sarah Schwettmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaham_T/0/1/0/all/0/1\">Tamar Rott Shaham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Materzynska_J/0/1/0/all/0/1\">Joanna Materzynska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_N/0/1/0/all/0/1\">Neil Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1\">David Bau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Goal-Oriented Prompt Attack and Safety Evaluation for LLMs. (arXiv:2309.11830v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.11830","description":"<p>Large Language Models (LLMs) presents significant priority in text\nunderstanding and generation. However, LLMs suffer from the risk of generating\nharmful contents especially while being employed to applications. There are\nseveral black-box attack methods, such as Prompt Attack, which can change the\nbehaviour of LLMs and induce LLMs to generate unexpected answers with harmful\ncontents. Researchers are interested in Prompt Attack and Defense with LLMs,\nwhile there is no publicly available dataset with high successful attacking\nrate to evaluate the abilities of defending prompt attack. In this paper, we\nintroduce a pipeline to construct high-quality prompt attack samples, along\nwith a Chinese prompt attack dataset called CPAD. Our prompts aim to induce\nLLMs to generate unexpected outputs with several carefully designed prompt\nattack templates and widely concerned attacking contents. Different from\nprevious datasets involving safety estimation, we construct the prompts\nconsidering three dimensions: contents, attacking methods and goals.\nEspecially, the attacking goals indicate the behaviour expected after\nsuccessfully attacking the LLMs, thus the responses can be easily evaluated and\nanalysed. We run several popular Chinese LLMs on our dataset, and the results\nshow that our prompts are significantly harmful to LLMs, with around 70% attack\nsuccess rate to GPT-3.5. CPAD is publicly available at\nhttps://github.com/liuchengyuan123/CPAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chengyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fubang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qing_L/0/1/0/all/0/1\">Lizhi Qing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yangyang Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changlong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM. (arXiv:2309.14348v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.14348","description":"<p>Recently, Large Language Models (LLMs) have made significant advancements and\nare now widely used across various domains. Unfortunately, there has been a\nrising concern that LLMs can be misused to generate harmful or malicious\ncontent. Though a line of research has focused on aligning LLMs with human\nvalues and preventing them from producing inappropriate content, such\nalignments are usually vulnerable and can be bypassed by alignment-breaking\nattacks via adversarially optimized or handcrafted jailbreaking prompts. In\nthis work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against\npotential alignment-breaking attacks. RA-LLM can be directly constructed upon\nan existing aligned LLM with a robust alignment checking function, without\nrequiring any expensive retraining or fine-tuning process of the original LLM.\nFurthermore, we also provide a theoretical analysis for RA-LLM to verify its\neffectiveness in defending against alignment-breaking attacks. Through\nreal-world experiments on open-source large language models, we demonstrate\nthat RA-LLM can successfully defend against both state-of-the-art adversarial\nprompts and popular handcrafted jailbreaking prompts by reducing their attack\nsuccess rates from nearly 100% to around 10% or less.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1\">Bochuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuanpu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinghui Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spider4SPARQL: A Complex Benchmark for Evaluating Knowledge Graph Question Answering Systems. (arXiv:2309.16248v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.16248","description":"<p>With the recent spike in the number and availability of Large Language Models\n(LLMs), it has become increasingly important to provide large and realistic\nbenchmarks for evaluating Knowledge Graph Question Answering (KGQA) systems. So\nfar the majority of benchmarks rely on pattern-based SPARQL query generation\napproaches. The subsequent natural language (NL) question generation is\nconducted through crowdsourcing or other automated methods, such as rule-based\nparaphrasing or NL question templates. Although some of these datasets are of\nconsiderable size, their pitfall lies in their pattern-based generation\napproaches, which do not always generalize well to the vague and linguistically\ndiverse questions asked by humans in real-world contexts. In this paper, we\nintroduce Spider4SPARQL - a new SPARQL benchmark dataset featuring 9,693\npreviously existing manually generated NL questions and 4,721 unique, novel,\nand complex SPARQL queries of varying complexity. In addition to the NL/SPARQL\npairs, we also provide their corresponding 166 knowledge graphs and ontologies,\nwhich cover 138 different domains. Our complex benchmark enables novel ways of\nevaluating the strengths and weaknesses of modern KGQA systems. We evaluate the\nsystem with state-of-the-art KGQA systems as well as LLMs, which achieve only\nup to 45\\% execution accuracy, demonstrating that Spider4SPARQL is a\nchallenging benchmark for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kosten_C/0/1/0/all/0/1\">Catherine Kosten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cudre_Mauroux_P/0/1/0/all/0/1\">Philippe Cudr&#xe9;-Mauroux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stockinger_K/0/1/0/all/0/1\">Kurt Stockinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversational Health Agents: A Personalized LLM-Powered Agent Framework. (arXiv:2310.02374v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.02374","description":"<p>Conversational Health Agents (CHAs) are interactive systems that provide\nhealthcare services, such as assistance, self-awareness, and diagnosis. Current\nCHAs, especially those utilizing Large Language Models (LLMs), primarily focus\non conversation aspects. However, they offer limited agent capabilities\nspecifically lacking multi-step problem-solving, empathetic conversations, and\nmultimodal data analysis. Our aim is to overcome these limitations. In this\npaper, we propose an LLM-powered framework to empower CHAs to generate a\npersonalized response for users' healthcare queries. This framework provides\ncritical thinking, knowledge acquisition, and problem-solving abilities by\nintegrating healthcare data sources, enabling multilingual and multimodal\nconversations, and interacting with various user data analysis tools. We\nillustrate the framework's proficiency in handling complex healthcare tasks via\na case study on stress level estimation, showcasing the agent's cognitive and\noperational capabilities. Powered by our framework, the CHA can provide\nappropriate responses, when the user inquires about their stress level. To\nachieve this, it learns to collect photoplethysmogram signals, converts them\ninto heart rate variability, and interprets them as indicators of stress\nlevels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abbasian_M/0/1/0/all/0/1\">Mahyar Abbasian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azimi_I/0/1/0/all/0/1\">Iman Azimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_A/0/1/0/all/0/1\">Amir M. Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Ramesh Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialogueLLM: Context and Emotion Knowledge-Tuned LLaMA Models for Emotion Recognition in Conversations. (arXiv:2310.11374v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.11374","description":"<p>Large language models (LLMs) and their variants have shown extraordinary\nefficacy across numerous downstream natural language processing (NLP) tasks,\nwhich has presented a new vision for the development of NLP. Despite their\nremarkable performance in natural language generating (NLG), LLMs lack a\ndistinct focus on the emotion understanding domain. As a result, using LLMs for\nemotion recognition may lead to suboptimal and inadequate precision. Another\nlimitation of LLMs is that they are typical trained without leveraging\nmulti-modal information. To overcome these limitations, we propose DialogueLLM,\na context and emotion knowledge tuned LLM that is obtained by fine-tuning LLaMA\nmodels with 13,638 multi-modal (i.e., texts and videos) emotional dialogues.\nThe visual information is considered as the supplementary knowledge to\nconstruct high-quality instructions. We offer a comprehensive evaluation of our\nproposed model on three benchmarking emotion recognition in conversations (ERC)\ndatasets and compare the results against the SOTA baselines and other SOTA\nLLMs. Additionally, DialogueLLM-7B can be easily trained using LoRA on a 40GB\nA100 GPU in 5 hours, facilitating reproducibility for other researchers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yazhou Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_P/0/1/0/all/0/1\">Prayag Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiuchi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Benyou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Pointwise $\\mathcal{V}$-Usable Information In-Context-ly. (arXiv:2310.12300v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.12300","description":"<p>In-context learning (ICL) is a new learning paradigm that has gained\npopularity along with the development of large language models. In this work,\nwe adapt a recently proposed hardness metric, pointwise $\\mathcal{V}$-usable\ninformation (PVI), to an in-context version (in-context PVI). Compared to the\noriginal PVI, in-context PVI is more efficient in that it requires only a few\nexemplars and does not require fine-tuning. We conducted a comprehensive\nempirical analysis to evaluate the reliability of in-context PVI. Our findings\nindicate that in-context PVI estimates exhibit similar characteristics to the\noriginal PVI. Specific to the in-context setting, we show that in-context PVI\nestimates remain consistent across different exemplar selections and numbers of\nshots. The variance of in-context PVI estimates across different exemplar\nselections is insignificant, which suggests that in-context PVI are stable.\nFurthermore, we demonstrate how in-context PVI can be employed to identify\nchallenging instances. Our work highlights the potential of in-context PVI and\nprovides new insights into the capabilities of ICL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Sheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitterman_D/0/1/0/all/0/1\">Danielle Bitterman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savova_G/0/1/0/all/0/1\">Guergana Savova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quality-Diversity through AI Feedback. (arXiv:2310.13032v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.13032","description":"<p>In many text-generation problems, users may prefer not only a single\nresponse, but a diverse range of high-quality outputs from which to choose.\nQuality-diversity (QD) search algorithms aim at such outcomes, by continually\nimproving and diversifying a population of candidates. However, the\napplicability of QD to qualitative domains, like creative writing, has been\nlimited by the difficulty of algorithmically specifying measures of quality and\ndiversity. Interestingly, recent developments in language models (LMs) have\nenabled guiding search through AI feedback, wherein LMs are prompted in natural\nlanguage to evaluate qualitative aspects of text. Leveraging this development,\nwe introduce Quality-Diversity through AI Feedback (QDAIF), wherein an\nevolutionary algorithm applies LMs to both generate variation and evaluate the\nquality and diversity of candidate text. When assessed on creative writing\ndomains, QDAIF covers more of a specified search space with high-quality\nsamples than do non-QD controls. Further, human evaluation of QDAIF-generated\ncreative texts validates reasonable agreement between AI and human evaluation.\nOur results thus highlight the potential of AI feedback to guide open-ended\nsearch for creative and original solutions, providing a recipe that seemingly\ngeneralizes to many domains and modalities. In this way, QDAIF is a step\ntowards AI systems that can independently search, diversify, evaluate, and\nimprove, which are among the core skills underlying human society's capacity\nfor innovation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bradley_H/0/1/0/all/0/1\">Herbie Bradley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teufel_H/0/1/0/all/0/1\">Hannah Teufel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jenny Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oostermeijer_K/0/1/0/all/0/1\">Koen Oostermeijer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellagente_M/0/1/0/all/0/1\">Marco Bellagente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1\">Jeff Clune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanley_K/0/1/0/all/0/1\">Kenneth Stanley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schott_G/0/1/0/all/0/1\">Gr&#xe9;gory Schott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehman_J/0/1/0/all/0/1\">Joel Lehman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4. (arXiv:2311.07361v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.07361","description":"<p>In recent years, groundbreaking advancements in natural language processing\nhave culminated in the emergence of powerful large language models (LLMs),\nwhich have showcased remarkable capabilities across a vast array of domains,\nincluding the understanding, generation, and translation of natural language,\nand even tasks that extend beyond language processing. In this report, we delve\ninto the performance of LLMs within the context of scientific discovery,\nfocusing on GPT-4, the state-of-the-art language model. Our investigation spans\na diverse range of scientific areas encompassing drug discovery, biology,\ncomputational chemistry (density functional theory (DFT) and molecular dynamics\n(MD)), materials design, and partial differential equations (PDE). Evaluating\nGPT-4 on scientific tasks is crucial for uncovering its potential across\nvarious research domains, validating its domain-specific expertise,\naccelerating scientific progress, optimizing resource allocation, guiding\nfuture model development, and fostering interdisciplinary research. Our\nexploration methodology primarily consists of expert-driven case assessments,\nwhich offer qualitative insights into the model's comprehension of intricate\nscientific concepts and relationships, and occasionally benchmark testing,\nwhich quantitatively evaluates the model's capacity to solve well-defined\ndomain-specific problems. Our preliminary exploration indicates that GPT-4\nexhibits promising potential for a variety of scientific applications,\ndemonstrating its aptitude for handling complex problem-solving and knowledge\nintegration tasks. Broadly speaking, we evaluate GPT-4's knowledge base,\nscientific understanding, scientific numerical calculation abilities, and\nvarious scientific prediction capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+AI4Science_M/0/1/0/all/0/1\">Microsoft Research AI4Science</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quantum_M/0/1/0/all/0/1\">Microsoft Azure Quantum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LM-Cocktail: Resilient Tuning of Language Models via Model Merging. (arXiv:2311.13534v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.13534","description":"<p>The pre-trained language models are continually fine-tuned to better support\ndownstream applications. However, this operation may result in significant\nperformance degeneration on general tasks beyond the targeted domain. To\novercome this problem, we propose LM-Cocktail which enables the fine-tuned\nmodel to stay resilient in general perspectives. Our method is conducted in the\nform of model merging, where the fine-tuned language model is merged with the\npre-trained base model or the peer models from other domains through weighted\naverage. Despite simplicity, LM-Cocktail is surprisingly effective: the\nresulted model is able to achieve a strong empirical performance in the whole\nscope of general tasks while preserving a superior capacity in its targeted\ndomain. We conduct comprehensive experiments with LLama and BGE model on\npopular benchmarks, including FLAN, MMLU, MTEB, whose results validate the\nefficacy of our proposed method. The code and checkpoints are available at\nhttps://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shitao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peitian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1\">Xingrun Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\\'UFAL CorPipe at CRAC 2023: Larger Context Improves Multilingual Coreference Resolution. (arXiv:2311.14391v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.14391","description":"<p>We present CorPipe, the winning entry to the CRAC 2023 Shared Task on\nMultilingual Coreference Resolution. Our system is an improved version of our\nearlier multilingual coreference pipeline, and it surpasses other participants\nby a large margin of 4.5 percent points. CorPipe first performs mention\ndetection, followed by coreference linking via an antecedent-maximization\napproach on the retrieved spans. Both tasks are trained jointly on all\navailable corpora using a shared pretrained language model. Our main\nimprovements comprise inputs larger than 512 subwords and changing the mention\ndecoding to support ensembling. The source code is available at\nhttps://github.com/ufal/crac2023-corpipe.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Straka_M/0/1/0/all/0/1\">Milan Straka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges. (arXiv:2311.15766v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.15766","description":"<p>In recent years, large language models (LLMs) have spurred a new research\nparadigm in natural language processing. Despite their excellent capability in\nknowledge-based question answering and reasoning, their potential to retain\nfaulty or even harmful knowledge poses risks of malicious application. The\nchallenge of mitigating this issue and transforming these models into purer\nassistants is crucial for their widespread applicability. Unfortunately,\nRetraining LLMs repeatedly to eliminate undesirable knowledge is impractical\ndue to their immense parameters. Knowledge unlearning, derived from analogous\nstudies on machine unlearning, presents a promising avenue to address this\nconcern and is notably advantageous in the context of LLMs. It allows for the\nremoval of harmful knowledge in an efficient manner, without affecting\nunrelated knowledge in the model. To this end, we provide a survey of knowledge\nunlearning in the era of LLMs. Firstly, we formally define the knowledge\nunlearning problem and distinguish it from related works. Subsequently, we\ncategorize existing knowledge unlearning methods into three classes: those\nbased on parameter optimization, parameter merging, and in-context learning,\nand introduce details of these unlearning methods. We further present\nevaluation datasets used in existing methods, and finally conclude this survey\nby presenting the ongoing challenges and future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_N/0/1/0/all/0/1\">Nianwen Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_D/0/1/0/all/0/1\">Dan Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiqiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SymNoise: Advancing Language Model Fine-tuning with Symmetric Noise. (arXiv:2312.01523v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.01523","description":"<p>In this paper, we introduce a novel fine-tuning technique for language\nmodels, which involves incorporating symmetric noise into the embedding\nprocess. This method aims to enhance the model's function by more stringently\nregulating its local curvature, demonstrating superior performance over the\ncurrent method, NEFTune. When fine-tuning the LLaMA-2-7B model using Alpaca,\nstandard techniques yield a 29.79% score on AlpacaEval. However, our approach,\nSymNoise, increases this score significantly to 69.04%, using symmetric noisy\nembeddings. This is a 6.7% improvement over the state-of-the-art method,\nNEFTune~(64.69%). Furthermore, when tested on various models and stronger\nbaseline instruction datasets, such as Evol-Instruct, ShareGPT, OpenPlatypus,\nSymNoise consistently outperforms NEFTune. The current literature, including\nNEFTune, has underscored the importance of more in-depth research into the\napplication of noise-based strategies in the fine-tuning of language models.\nOur approach, SymNoise, is another significant step towards this direction,\nshowing notable improvement over the existing state-of-the-art method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_A/0/1/0/all/0/1\">Abhay Kumar Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Arjun Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Robustness of Model-Graded Evaluations and Automated Interpretability. (arXiv:2312.03721v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.03721","description":"<p>There has been increasing interest in evaluations of language models for a\nvariety of risks and characteristics. Evaluations relying on natural language\nunderstanding for grading can often be performed at scale by using other\nlanguage models. We test the robustness of these model-graded evaluations to\ninjections on different datasets including a new Deception Eval. These\ninjections resemble direct communication between the testee and the evaluator\nto change their grading. We extrapolate that future, more intelligent models\nmight manipulate or cooperate with their evaluation model. We find significant\nsusceptibility to these injections in state-of-the-art commercial models on all\nexamined evaluations. Furthermore, similar injections can be used on automated\ninterpretability frameworks to produce misleading model-written explanations.\nThe results inspire future work and should caution against unqualified trust in\nevaluations and automated interpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lermen_S/0/1/0/all/0/1\">Simon Lermen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kvapil_O/0/1/0/all/0/1\">Ond&#x159;ej Kvapil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Methods to Estimate Large Language Model Confidence. (arXiv:2312.03733v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.03733","description":"<p>Large Language Models have difficulty communicating uncertainty, which is a\nsignificant obstacle to applying LLMs to complex medical tasks. This study\nevaluates methods to measure LLM confidence when suggesting a diagnosis for\nchallenging clinical vignettes. GPT4 was asked a series of challenging case\nquestions using Chain of Thought and Self Consistency prompting. Multiple\nmethods were investigated to assess model confidence and evaluated on their\nability to predict the models observed accuracy. The methods evaluated were\nIntrinsic Confidence, SC Agreement Frequency and CoT Response Length. SC\nAgreement Frequency correlated with observed accuracy, yielding a higher Area\nunder the Receiver Operating Characteristic Curve compared to Intrinsic\nConfidence and CoT Length analysis. SC agreement is the most useful proxy for\nmodel confidence, especially for medical diagnosis. Model Intrinsic Confidence\nand CoT Response Length exhibit a weaker ability to differentiate between\ncorrect and incorrect answers, preventing them from being reliable and\ninterpretable markers for model confidence. We conclude GPT4 has a limited\nability to assess its own diagnostic accuracy. SC Agreement Frequency is the\nmost useful method to measure GPT4 confidence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kotelanski_M/0/1/0/all/0/1\">Maia Kotelanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallo_R/0/1/0/all/0/1\">Robert Gallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_A/0/1/0/all/0/1\">Ashwin Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savage_T/0/1/0/all/0/1\">Thomas Savage</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Surface: Probing LLaMA Across Scales and Layers. (arXiv:2312.04333v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.04333","description":"<p>This paper presents an in-depth analysis of Large Language Models (LLMs),\nfocusing on LLaMA, a prominent open-source foundational model in natural\nlanguage processing. Instead of assessing LLaMA through its generative output,\nwe design multiple-choice tasks to probe its intrinsic understanding in\nhigh-order tasks such as reasoning and computation. We examine the model\nhorizontally, comparing different sizes, and vertically, assessing different\nlayers. We unveil several key and uncommon findings based on the designed\nprobing tasks: (1) Horizontally, enlarging model sizes almost could not\nautomatically impart additional knowledge or computational prowess. Instead, it\ncan enhance reasoning abilities, especially in math problem solving, and helps\nreduce hallucinations, but only beyond certain size thresholds; (2) In vertical\nanalysis, the lower layers of LLaMA lack substantial arithmetic and factual\nknowledge, showcasing logical thinking, multilingual and recognitive abilities,\nwith top layers housing most computational power and real-world knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1\">Ning Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shining Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Ming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_L/0/1/0/all/0/1\">Linjun Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain of Code: Reasoning with a Language Model-Augmented Code Emulator. (arXiv:2312.04474v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.04474","description":"<p>Code provides a general syntactic structure to build complex programs and\nperform precise computations when paired with a code interpreter - we\nhypothesize that language models (LMs) can leverage code-writing to improve\nChain of Thought reasoning not only for logic and arithmetic tasks, but also\nfor semantic ones (and in particular, those that are a mix of both). For\nexample, consider prompting an LM to write code that counts the number of times\nit detects sarcasm in an essay: the LM may struggle to write an implementation\nfor \"detect_sarcasm(string)\" that can be executed by the interpreter (handling\nthe edge cases would be insurmountable). However, LMs may still produce a valid\nsolution if they not only write code, but also selectively \"emulate\" the\ninterpreter by generating the expected output of \"detect_sarcasm(string)\" and\nother lines of code that cannot be executed. In this work, we propose Chain of\nCode (CoC), a simple yet surprisingly effective extension that improves LM\ncode-driven reasoning. The key idea is to encourage LMs to format semantic\nsub-tasks in a program as flexible pseudocode that the interpreter can\nexplicitly catch undefined behaviors and hand off to simulate with an LM (as an\n\"LMulator\"). Experiments demonstrate that Chain of Code outperforms Chain of\nThought and other baselines across a variety of benchmarks; on BIG-Bench Hard,\nChain of Code achieves 84%, a gain of 12% over Chain of Thought. CoC scales\nwell with large and small models alike, and broadens the scope of reasoning\nquestions that LMs can correctly answer by \"thinking in code\". Project webpage:\nhttps://chain-of-code.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jacky Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Andy Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1\">Karol Hausman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1\">Dorsa Sadigh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1\">Brian Ichter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-12-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/"}}]}]}