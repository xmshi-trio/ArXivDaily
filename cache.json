{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-06-27T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios. (arXiv:2306.13734v1 [eess.AS])","link":"http://arxiv.org/abs/2306.13734","description":"<p>The CHiME challenges have played a significant role in the development and\nevaluation of robust speech recognition (ASR) systems. We introduce the CHiME-7\ndistant ASR (DASR) task, within the 7th CHiME challenge. This task comprises\njoint ASR and diarization in far-field settings with multiple, and possibly\nheterogeneous, recording devices. Different from previous challenges, we\nevaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The goal\nis for participants to devise a single system that can generalize across\ndifferent array geometries and use cases with no a-priori information. Another\ndeparture from earlier CHiME iterations is that participants are allowed to use\nopen-source pre-trained models and datasets. In this paper, we describe the\nchallenge design, motivation, and fundamental research questions in detail. We\nalso present the baseline system, which is fully array-topology agnostic and\nfeatures multi-channel diarization, channel selection, guided source separation\nand a robust ASR model that leverages self-supervised speech representations\n(SSLR).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cornell_S/0/1/0/all/0/1\">Samuele Cornell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wiesner_M/0/1/0/all/0/1\">Matthew Wiesner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raj_D/0/1/0/all/0/1\">Desh Raj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garcia_P/0/1/0/all/0/1\">Paola Garcia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Masuyama_Y/0/1/0/all/0/1\">Yoshiki Masuyama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhong-Qiu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Squartini_S/0/1/0/all/0/1\">Stefano Squartini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khudanpur_S/0/1/0/all/0/1\">Sanjeev Khudanpur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resume Information Extraction via Post-OCR Text Processing. (arXiv:2306.13775v1 [cs.CL])","link":"http://arxiv.org/abs/2306.13775","description":"<p>Information extraction (IE), one of the main tasks of natural language\nprocessing (NLP), has recently increased importance in the use of resumes. In\nstudies on the text to extract information from the CV, sentence classification\nwas generally made using NLP models. In this study, it is aimed to extract\ninformation by classifying all of the text groups after pre-processing such as\nOptical Character Recognition (OCT) and object recognition with the YOLOv8\nmodel of the resumes. The text dataset consists of 286 resumes collected for 5\ndifferent (education, experience, talent, personal and language) job\ndescriptions in the IT industry. The dataset created for object recognition\nconsists of 1198 resumes, which were collected from the open-source internet\nand labeled as sets of text. BERT, BERT-t, DistilBERT, RoBERTa and XLNet were\nused as models. F1 score variances were used to compare the model results. In\naddition, the YOLOv8 model has also been reported comparatively in itself. As a\nresult of the comparison, DistilBERT was showed better results despite having a\nlower number of parameters than other models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Helli_S/0/1/0/all/0/1\">Selahattin Serdar Helli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanberk_S/0/1/0/all/0/1\">Senem Tanberk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavsak_S/0/1/0/all/0/1\">Sena Nur Cavsak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deconstructing Classifiers: Towards A Data Reconstruction Attack Against Text Classification Models. (arXiv:2306.13789v1 [cs.CL])","link":"http://arxiv.org/abs/2306.13789","description":"<p>Natural language processing (NLP) models have become increasingly popular in\nreal-world applications, such as text classification. However, they are\nvulnerable to privacy attacks, including data reconstruction attacks that aim\nto extract the data used to train the model. Most previous studies on data\nreconstruction attacks have focused on LLM, while classification models were\nassumed to be more secure. In this work, we propose a new targeted data\nreconstruction attack called the Mix And Match attack, which takes advantage of\nthe fact that most classification models are based on LLM. The Mix And Match\nattack uses the base model of the target model to generate candidate tokens and\nthen prunes them using the classification head. We extensively demonstrate the\neffectiveness of the attack using both random and organic canaries. This work\nhighlights the importance of considering the privacy risks associated with data\nreconstruction attacks in classification models and offers insights into\npossible leakages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elmahdy_A/0/1/0/all/0/1\">Adel Elmahdy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salem_A/0/1/0/all/0/1\">Ahmed Salem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An analysis of vaccine-related sentiments from development to deployment of COVID-19 vaccines. (arXiv:2306.13797v1 [cs.SI])","link":"http://arxiv.org/abs/2306.13797","description":"<p>Anti-vaccine sentiments have been well-known and reported throughout the\nhistory of viral outbreaks and vaccination programmes. The COVID-19 pandemic\nhad fear and uncertainty about vaccines which has been well expressed on social\nmedia platforms such as Twitter. We analyse Twitter sentiments from the\nbeginning of the COVID-19 pandemic and study the public behaviour during the\nplanning, development and deployment of vaccines expressed in tweets worldwide\nusing a sentiment analysis framework via deep learning models. In this way, we\nprovide visualisation and analysis of anti-vaccine sentiments over the course\nof the COVID-19 pandemic. Our results show a link between the number of tweets,\nthe number of cases, and the change in sentiment polarity scores during major\nwaves of COVID-19 cases. We also found that the first half of the pandemic had\ndrastic changes in the sentiment polarity scores that later stabilised which\nimplies that the vaccine rollout had an impact on the nature of discussions on\nsocial media.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rohitash Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonawane_J/0/1/0/all/0/1\">Jayesh Sonawane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lande_J/0/1/0/all/0/1\">Janhavi Lande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cathy Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Language Speech Emotion Recognition Using Multimodal Dual Attention Transformers. (arXiv:2306.13804v1 [cs.CL])","link":"http://arxiv.org/abs/2306.13804","description":"<p>Despite the recent progress in speech emotion recognition (SER),\nstate-of-the-art systems are unable to achieve improved performance in\ncross-language settings. In this paper, we propose a Multimodal Dual Attention\nTransformer (MDAT) model to improve cross-language SER. Our model utilises\npre-trained models for multimodal feature extraction and is equipped with a\ndual attention mechanism including graph attention and co-attention to capture\ncomplex dependencies across different modalities and achieve improved\ncross-language SER results using minimal target language data. In addition, our\nmodel also exploits a transformer encoder layer for high-level feature\nrepresentation to improve emotion classification accuracy. In this way, MDAT\nperforms refinement of feature representation at various stages and provides\nemotional salient features to the classification layer. This novel approach\nalso ensures the preservation of modality-specific emotional information while\nenhancing cross-modality and cross-language interactions. We assess our model's\nperformance on four publicly available SER datasets and establish its superior\neffectiveness compared to recent approaches and baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaidi_S/0/1/0/all/0/1\">Syed Aun Muhammad Zaidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Latif_S/0/1/0/all/0/1\">Siddique Latif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qadi_J/0/1/0/all/0/1\">Junaid Qadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Double Helix inside the NLP Transformer. (arXiv:2306.13817v1 [cs.AI])","link":"http://arxiv.org/abs/2306.13817","description":"<p>We introduce a framework for analyzing various types of information in an NLP\nTransformer. In this approach, we distinguish four layers of information:\npositional, syntactic, semantic, and contextual. We also argue that the common\npractice of adding positional information to semantic embedding is sub-optimal\nand propose instead a Linear-and-Add approach. Our analysis reveals an\nautogenetic separation of positional information through the deep layers. We\nshow that the distilled positional components of the embedding vectors follow\nthe path of a helix, both on the encoder side and on the decoder side. We\nadditionally show that on the encoder side, the conceptual dimensions generate\nPart-of-Speech (PoS) clusters. On the decoder side, we show that a di-gram\napproach helps to reveal the PoS clusters of the next token. Our approach paves\na way to elucidate the processing of information through the deep layers of an\nNLP Transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jason H.J. Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qingzhen Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data. (arXiv:2306.13840v1 [cs.CL])","link":"http://arxiv.org/abs/2306.13840","description":"<p>Current trends to pre-train capable Large Language Models (LLMs) mostly focus\non scaling of model and dataset size. However, the quality of pre-training data\nis an important factor for training powerful LLMs, yet it is a nebulous concept\nthat has not been fully characterized. Therefore, we use the recently proposed\nTask2Vec diversity coefficient to ground and understand formal aspects of data\nquality, to go beyond scale alone. Specifically, we measure the diversity\ncoefficient of publicly available pre-training datasets to demonstrate that\ntheir formal diversity is high when compared to theoretical lower and upper\nbounds. In addition, to build confidence in the diversity coefficient, we\nconduct interpretability experiments and find that the coefficient aligns with\nintuitive properties of diversity, e.g., it increases as the number of latent\nconcepts increases. We conclude the diversity coefficient is reliable, show\nit's high for publicly available LLM datasets, and conjecture it can be used to\nbuild useful diverse datasets for LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Alycia Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miranda_B/0/1/0/all/0/1\">Brando Miranda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1\">Sanmi Koyejo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Pre-training Truly Better Than Meta-Learning?. (arXiv:2306.13841v1 [cs.LG])","link":"http://arxiv.org/abs/2306.13841","description":"<p>In the context of few-shot learning, it is currently believed that a fixed\npre-trained (PT) model, along with fine-tuning the final layer during\nevaluation, outperforms standard meta-learning algorithms. We re-evaluate these\nclaims under an in-depth empirical examination of an extensive set of formally\ndiverse datasets and compare PT to Model Agnostic Meta-Learning (MAML). Unlike\nprevious work, we emphasize a fair comparison by using: the same architecture,\nthe same optimizer, and all models trained to convergence. Crucially, we use a\nmore rigorous statistical tool -- the effect size (Cohen's d) -- to determine\nthe practical significance of the difference between a model trained with PT\nvs. a MAML. We then use a previously proposed metric -- the diversity\ncoefficient -- to compute the average formal diversity of a dataset. Using this\nanalysis, we demonstrate the following: 1. when the formal diversity of a data\nset is low, PT beats MAML on average and 2. when the formal diversity is high,\nMAML beats PT on average. The caveat is that the magnitude of the average\ndifference between a PT vs. MAML using the effect size is low (according to\nclassical statistical thresholds) -- less than 0.2. Nevertheless, this\nobservation is contrary to the currently held belief that a pre-trained model\nis always better than a meta-learning model. Our extensive experiments consider\n21 few-shot learning benchmarks, including the large-scale few-shot learning\ndataset Meta-Data set. We also show no significant difference between a MAML\nmodel vs. a PT model with GPT-2 on Openwebtext. We, therefore, conclude that a\npre-trained model does not always beat a meta-learned model and that the formal\ndiversity of a dataset is a driving factor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miranda_B/0/1/0/all/0/1\">Brando Miranda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Patrick Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_S/0/1/0/all/0/1\">Saumya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1\">Sanmi Koyejo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IERL: Interpretable Ensemble Representation Learning -- Combining CrowdSourced Knowledge and Distributed Semantic Representations. (arXiv:2306.13865v1 [cs.CL])","link":"http://arxiv.org/abs/2306.13865","description":"<p>Large Language Models (LLMs) encode meanings of words in the form of\ndistributed semantics. Distributed semantics capture common statistical\npatterns among language tokens (words, phrases, and sentences) from large\namounts of data. LLMs perform exceedingly well across General Language\nUnderstanding Evaluation (GLUE) tasks designed to test a model's understanding\nof the meanings of the input tokens. However, recent studies have shown that\nLLMs tend to generate unintended, inconsistent, or wrong texts as outputs when\nprocessing inputs that were seen rarely during training, or inputs that are\nassociated with diverse contexts (e.g., well-known hallucination phenomenon in\nlanguage generation tasks). Crowdsourced and expert-curated knowledge graphs\nsuch as ConceptNet are designed to capture the meaning of words from a compact\nset of well-defined contexts. Thus LLMs may benefit from leveraging such\nknowledge contexts to reduce inconsistencies in outputs. We propose a novel\nensemble learning method, Interpretable Ensemble Representation Learning\n(IERL), that systematically combines LLM and crowdsourced knowledge\nrepresentations of input tokens. IERL has the distinct advantage of being\ninterpretable by design (when was the LLM context used vs. when was the\nknowledge context used?) over state-of-the-art (SOTA) methods, allowing\nscrutiny of the inputs in conjunction with the parameters of the model,\nfacilitating the analysis of models' inconsistent or irrelevant outputs.\nAlthough IERL is agnostic to the choice of LLM and crowdsourced knowledge, we\ndemonstrate our approach using BERT and ConceptNet. We report improved or\ncompetitive results with IERL across GLUE tasks over current SOTA methods and\nsignificantly enhanced model interpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zi_Y/0/1/0/all/0/1\">Yuxin Zi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_V/0/1/0/all/0/1\">Vignesh Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_M/0/1/0/all/0/1\">Manas Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L3Cube-MahaSent-MD: A Multi-domain Marathi Sentiment Analysis Dataset and Transformer Models. (arXiv:2306.13888v1 [cs.CL])","link":"http://arxiv.org/abs/2306.13888","description":"<p>The exploration of sentiment analysis in low-resource languages, such as\nMarathi, has been limited due to the availability of suitable datasets. In this\nwork, we present L3Cube-MahaSent-MD, a multi-domain Marathi sentiment analysis\ndataset, with four different domains - movie reviews, general tweets, TV show\nsubtitles, and political tweets. The dataset consists of around 60,000 manually\ntagged samples covering 3 distinct sentiments - positive, negative, and\nneutral. We create a sub-dataset for each domain comprising 15k samples. The\nMahaSent-MD is the first comprehensive multi-domain sentiment analysis dataset\nwithin the Indic sentiment landscape. We fine-tune different monolingual and\nmultilingual BERT models on these datasets and report the best accuracy with\nthe MahaBERT model. We also present an extensive in-domain and cross-domain\nanalysis thus highlighting the need for low-resource multi-domain datasets. The\ndata and models are available at https://github.com/l3cube-pune/MarathiNLP .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pingle_A/0/1/0/all/0/1\">Aabha Pingle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyawahare_A/0/1/0/all/0/1\">Aditya Vyawahare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_I/0/1/0/all/0/1\">Isha Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tangsali_R/0/1/0/all/0/1\">Rahul Tangsali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating the Causal Effect of Early ArXiving on Paper Acceptance. (arXiv:2306.13891v1 [cs.CL])","link":"http://arxiv.org/abs/2306.13891","description":"<p>What is the effect of releasing a preprint of a paper before it is submitted\nfor peer review? No randomized controlled trial has been conducted, so we turn\nto observational data to answer this question. We use data from the ICLR\nconference (2018--2022) and apply methods from causal inference to estimate the\neffect of arXiving a paper before the reviewing period (early arXiving) on its\nacceptance to the conference. Adjusting for 18 confounders such as topic,\nauthors, and quality, we may estimate the causal effect. However, since quality\nis a challenging construct to estimate, we use the negative outcome control\nmethod, using paper citation count as a control variable to debias the quality\nconfounding effect. Our results suggest that early arXiving may have a small\neffect on a paper's chances of acceptance. However, this effect (when existing)\ndoes not differ significantly across different groups of authors, as grouped by\nauthor citation count and institute rank. This suggests that early arXiving\ndoes not provide an advantage to any particular group.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1\">Yanai Elazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiayao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadden_D/0/1/0/all/0/1\">David Wadden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Math Word Problem Solving by Generating Linguistic Variants of Problem Statements. (arXiv:2306.13899v1 [cs.CL])","link":"http://arxiv.org/abs/2306.13899","description":"<p>The art of mathematical reasoning stands as a fundamental pillar of\nintellectual progress and is a central catalyst in cultivating human ingenuity.\nResearchers have recently published a plethora of works centered around the\ntask of solving Math Word Problems (MWP) $-$ a crucial stride towards general\nAI. These existing models are susceptible to dependency on shallow heuristics\nand spurious correlations to derive the solution expressions. In order to\nameliorate this issue, in this paper, we propose a framework for MWP solvers\nbased on the generation of linguistic variants of the problem text. The\napproach involves solving each of the variant problems and electing the\npredicted expression with the majority of the votes. We use DeBERTa\n(Decoding-enhanced BERT with disentangled attention) as the encoder to leverage\nits rich textual representations and enhanced mask decoder to construct the\nsolution expressions. Furthermore, we introduce a challenging dataset,\n$\\mathrm{P\\small{ARA}\\normalsize{MAWPS}}$, consisting of paraphrased,\nadversarial, and inverse variants of selectively sampled MWPs from the\nbenchmark $\\mathrm{M\\small{AWPS}}$ dataset. We extensively experiment on this\ndataset along with other benchmark datasets using some baseline MWP solver\nmodels. We show that training on linguistic variants of problem statements and\nvoting on candidate predictions improve the mathematical reasoning and\nrobustness of the model. We make our code and data publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raiyan_S/0/1/0/all/0/1\">Syed Rifat Raiyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faiyaz_M/0/1/0/all/0/1\">Md. Nafis Faiyaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_S/0/1/0/all/0/1\">Shah Md. Jawad Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Mohsinul Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmud_H/0/1/0/all/0/1\">Hasan Mahmud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Md Kamrul Hasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-temporal Storytelling? Leveraging Generative Models for Semantic Trajectory Analysis. (arXiv:2306.13905v1 [cs.CL])","link":"http://arxiv.org/abs/2306.13905","description":"<p>In this paper, we lay out a vision for analysing semantic trajectory traces\nand generating synthetic semantic trajectory data (SSTs) using generative\nlanguage model. Leveraging the advancements in deep learning, as evident by\nprogress in the field of natural language processing (NLP), computer vision,\netc. we intend to create intelligent models that can study the semantic\ntrajectories in various contexts, predicting future trends, increasing machine\nunderstanding of the movement of animals, humans, goods, etc. enhancing\nhuman-computer interactions, and contributing to an array of applications\nranging from urban-planning to personalized recommendation engines and business\nstrategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shreya Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Saptarshi Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_P/0/1/0/all/0/1\">Prasenjit Mitra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can GPT-4 Support Analysis of Textual Data in Tasks Requiring Highly Specialized Domain Expertise?. (arXiv:2306.13906v1 [cs.CL])","link":"http://arxiv.org/abs/2306.13906","description":"<p>We evaluated the capability of generative pre-trained transformers~(GPT-4) in\nanalysis of textual data in tasks that require highly specialized domain\nexpertise. Specifically, we focused on the task of analyzing court opinions to\ninterpret legal concepts. We found that GPT-4, prompted with annotation\nguidelines, performs on par with well-trained law student annotators. We\nobserved that, with a relatively minor decrease in performance, GPT-4 can\nperform batch predictions leading to significant cost reductions. However,\nemploying chain-of-thought prompting did not lead to noticeably improved\nperformance on this task. Further, we demonstrated how to analyze GPT-4's\npredictions to identify and mitigate deficiencies in annotation guidelines, and\nsubsequently improve the performance of the model. Finally, we observed that\nthe model is quite brittle, as small formatting related changes in the prompt\nhad a high impact on the predictions. These findings can be leveraged by\nresearchers and practitioners who engage in semantic/pragmatic annotations of\ntexts in the context of the tasks requiring highly specialized domain\nexpertise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Savelka_J/0/1/0/all/0/1\">Jaromir Savelka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashley_K/0/1/0/all/0/1\">Kevin D. Ashley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gray_M/0/1/0/all/0/1\">Morgan A Gray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Westermann_H/0/1/0/all/0/1\">Hannes Westermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huihui Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Mapping of Arguments of Deverbal Nouns to Their Corresponding Verbal Labels. (arXiv:2306.13922v1 [cs.CL])","link":"http://arxiv.org/abs/2306.13922","description":"<p>Deverbal nouns are nominal forms of verbs commonly used in written English\ntexts to describe events or actions, as well as their arguments. However, many\nNLP systems, and in particular pattern-based ones, neglect to handle such\nnominalized constructions. The solutions that do exist for handling arguments\nof nominalized constructions are based on semantic annotation and require\nsemantic ontologies, making their applications restricted to a small set of\nnouns. We propose to adopt instead a more syntactic approach, which maps the\narguments of deverbal nouns to the universal-dependency relations of the\ncorresponding verbal construction. We present an unsupervised mechanism --\nbased on contextualized word representations -- which allows to enrich\nuniversal-dependency trees with dependency arcs denoting arguments of deverbal\nnouns, using the same labels as the corresponding verbal cases. By sharing the\nsame label set as in the verbal case, patterns that were developed for verbs\ncan be applied without modification but with high accuracy also to the nominal\nconstructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weinstein_A/0/1/0/all/0/1\">Aviv Weinstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparison of Pre-trained Language Models for Turkish Address Parsing. (arXiv:2306.13947v1 [cs.CL])","link":"http://arxiv.org/abs/2306.13947","description":"<p>Transformer based pre-trained models such as BERT and its variants, which are\ntrained on large corpora, have demonstrated tremendous success for natural\nlanguage processing (NLP) tasks. Most of academic works are based on the\nEnglish language; however, the number of multilingual and language specific\nstudies increase steadily. Furthermore, several studies claimed that language\nspecific models outperform multilingual models in various tasks. Therefore, the\ncommunity tends to train or fine-tune the models for the language of their case\nstudy, specifically. In this paper, we focus on Turkish maps data and\nthoroughly evaluate both multilingual and Turkish based BERT, DistilBERT,\nELECTRA and RoBERTa. Besides, we also propose a MultiLayer Perceptron (MLP) for\nfine-tuning BERT in addition to the standard approach of one-layer fine-tuning.\nFor the dataset, a mid-sized Address Parsing corpus taken with a relatively\nhigh quality is constructed. Conducted experiments on this dataset indicate\nthat Turkish language specific models with MLP fine-tuning yields slightly\nbetter results when compared to the multilingual fine-tuned models. Moreover,\nvisualization of address tokens' representations further indicates the\neffectiveness of BERT variants for classifying a variety of addresses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Unal_M/0/1/0/all/0/1\">Muhammed Cihat &#xdc;nal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aygun_B/0/1/0/all/0/1\">Bet&#xfc;l Ayg&#xfc;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerek_A/0/1/0/all/0/1\">Ayd&#x131;n Gerek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterizing the Emotion Carriers of COVID-19 Misinformation and Their Impact on Vaccination Outcomes in India and the United States. (arXiv:2306.13954v1 [cs.CL])","link":"http://arxiv.org/abs/2306.13954","description":"<p>The COVID-19 Infodemic had an unprecedented impact on health behaviors and\noutcomes at a global scale. While many studies have focused on a qualitative\nand quantitative understanding of misinformation, including sentiment analysis,\nthere is a gap in understanding the emotion-carriers of misinformation and\ntheir differences across geographies. In this study, we characterized emotion\ncarriers and their impact on vaccination rates in India and the United States.\nA manually labelled dataset was created from 2.3 million tweets and collated\nwith three publicly available datasets (CoAID, AntiVax, CMU) to train deep\nlearning models for misinformation classification. Misinformation labelled\ntweets were further analyzed for behavioral aspects by leveraging Plutchik\nTransformers to determine the emotion for each tweet. Time series analysis was\nconducted to study the impact of misinformation on spatial and temporal\ncharacteristics. Further, categorical classification was performed using\ntransformer models to assign categories for the misinformation tweets.\nWord2Vec+BiLSTM was the best model for misinformation classification, with an\nF1-score of 0.92. The US had the highest proportion of misinformation tweets\n(58.02%), followed by the UK (10.38%) and India (7.33%). Disgust, anticipation,\nand anger were associated with an increased prevalence of misinformation\ntweets. Disgust was the predominant emotion associated with misinformation\ntweets in the US, while anticipation was the predominant emotion in India. For\nIndia, the misinformation rate exhibited a lead relationship with vaccination,\nwhile in the US it lagged behind vaccination. Our study deciphered that\nemotions acted as differential carriers of misinformation across geography and\ntime. These carriers can be monitored to develop strategic interventions for\ncountering misinformation, leading to improved public health.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pal_R/0/1/0/all/0/1\">Ridam Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+S_S/0/1/0/all/0/1\">Sanjana S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahto_D/0/1/0/all/0/1\">Deepak Mahto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_K/0/1/0/all/0/1\">Kriti Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mengi_G/0/1/0/all/0/1\">Gopal Mengi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagpal_S/0/1/0/all/0/1\">Sargun Nagpal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devadiga_A/0/1/0/all/0/1\">Akshaya Devadiga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethi_T/0/1/0/all/0/1\">Tavpritesh Sethi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion Flip Reasoning in Multiparty Conversations. (arXiv:2306.13959v1 [cs.CL])","link":"http://arxiv.org/abs/2306.13959","description":"<p>In a conversational dialogue, speakers may have different emotional states\nand their dynamics play an important role in understanding dialogue's emotional\ndiscourse. However, simply detecting emotions is not sufficient to entirely\ncomprehend the speaker-specific changes in emotion that occur during a\nconversation. To understand the emotional dynamics of speakers in an efficient\nmanner, it is imperative to identify the rationale or instigator behind any\nchanges or flips in emotion expressed by the speaker. In this paper, we explore\nthe task called Instigator based Emotion Flip Reasoning (EFR), which aims to\nidentify the instigator behind a speaker's emotion flip within a conversation.\nFor example, an emotion flip from joy to anger could be caused by an instigator\nlike threat. To facilitate this task, we present MELD-I, a dataset that\nincludes ground-truth EFR instigator labels, which are in line with emotional\npsychology. To evaluate the dataset, we propose a novel neural architecture\ncalled TGIF, which leverages Transformer encoders and stacked GRUs to capture\nthe dialogue context, speaker dynamics, and emotion sequence in a conversation.\nOur evaluation demonstrates state-of-the-art performance (+4-12% increase in\nF1-score) against five baselines used for the task. Further, we establish the\ngeneralizability of TGIF on an unseen dataset in a zero-shot setting.\nAdditionally, we provide a detailed analysis of the competing models,\nhighlighting the advantages and limitations of our neural architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shivani Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dudeja_S/0/1/0/all/0/1\">Shubham Dudeja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fusing Multimodal Signals on Hyper-complex Space for Extreme Abstractive Text Summarization (TL;DR) of Scientific Contents. (arXiv:2306.13968v1 [cs.CL])","link":"http://arxiv.org/abs/2306.13968","description":"<p>The realm of scientific text summarization has experienced remarkable\nprogress due to the availability of annotated brief summaries and ample data.\nHowever, the utilization of multiple input modalities, such as videos and\naudio, has yet to be thoroughly explored. At present, scientific\nmultimodal-input-based text summarization systems tend to employ longer target\nsummaries like abstracts, leading to an underwhelming performance in the task\nof text summarization.\n</p>\n<p>In this paper, we deal with a novel task of extreme abstractive text\nsummarization (aka TL;DR generation) by leveraging multiple input modalities.\nTo this end, we introduce mTLDR, a first-of-its-kind dataset for the\naforementioned task, comprising videos, audio, and text, along with both\nauthor-composed summaries and expert-annotated summaries. The mTLDR dataset\naccompanies a total of 4,182 instances collected from various academic\nconference proceedings, such as ICLR, ACL, and CVPR. Subsequently, we present\nmTLDRgen, an encoder-decoder-based model that employs a novel dual-fused\nhyper-complex Transformer combined with a Wasserstein Riemannian Encoder\nTransformer, to dexterously capture the intricacies between different\nmodalities in a hyper-complex latent geometric space. The hyper-complex\nTransformer captures the intrinsic properties between the modalities, while the\nWasserstein Riemannian Encoder Transformer captures the latent structure of the\nmodalities in the latent space geometry, thereby enabling the model to produce\ndiverse sentences. mTLDRgen outperforms 20 baselines on mTLDR as well as\nanother non-scientific dataset (How2) across three Rouge-based evaluation\nmeasures. Furthermore, based on the qualitative metrics, BERTScore and FEQA,\nand human evaluations, we demonstrate that the summaries generated by mTLDRgen\nare fluent and congruent to the original source material.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Atri_Y/0/1/0/all/0/1\">Yash Kumar Atri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1\">Vikram Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust Aspect-based Sentiment Analysis through Non-counterfactual Augmentations. (arXiv:2306.13971v1 [cs.CL])","link":"http://arxiv.org/abs/2306.13971","description":"<p>While state-of-the-art NLP models have demonstrated excellent performance for\naspect based sentiment analysis (ABSA), substantial evidence has been presented\non their lack of robustness. This is especially manifested as significant\ndegradation in performance when faced with out-of-distribution data. Recent\nsolutions that rely on counterfactually augmented datasets show promising\nresults, but they are inherently limited because of the lack of access to\nexplicit causal structure. In this paper, we present an alternative approach\nthat relies on non-counterfactual data augmentation. Our proposal instead\nrelies on using noisy, cost-efficient data augmentations that preserve\nsemantics associated with the target aspect. Our approach then relies on\nmodelling invariances between different versions of the data to improve\nrobustness. A comprehensive suite of experiments shows that our proposal\nsignificantly improves upon strong pre-trained baselines on both standard and\nrobustness-specific datasets. Our approach further establishes a new\nstate-of-the-art on the ABSA robustness benchmark and transfers well across\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_K/0/1/0/all/0/1\">Kaikai An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chunyang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhyastha_P/0/1/0/all/0/1\">Pranava Madhyastha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models as Sous Chefs: Revising Recipes with GPT-3. (arXiv:2306.13986v1 [cs.CL])","link":"http://arxiv.org/abs/2306.13986","description":"<p>With their remarkably improved text generation and prompting capabilities,\nlarge language models can adapt existing written information into forms that\nare easier to use and understand. In our work, we focus on recipes as an\nexample of complex, diverse, and widely used instructions. We develop a prompt\ngrounded in the original recipe and ingredients list that breaks recipes down\ninto simpler steps. We apply this prompt to recipes from various world\ncuisines, and experiment with several large language models (LLMs), finding\nbest results with GPT-3.5. We also contribute an Amazon Mechanical Turk task\nthat is carefully designed to reduce fatigue while collecting human judgment of\nthe quality of recipe revisions. We find that annotators usually prefer the\nrevision over the original, demonstrating a promising application of LLMs in\nserving as digital sous chefs for recipes and beyond. We release our prompt,\ncode, and MTurk template for public use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_A/0/1/0/all/0/1\">Alyssa Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bryan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zhaoyi Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Multi-Label Classification of Full-Text Scientific Papers. (arXiv:2306.14003v1 [cs.CL])","link":"http://arxiv.org/abs/2306.14003","description":"<p>Instead of relying on human-annotated training samples to build a classifier,\nweakly supervised scientific paper classification aims to classify papers only\nusing category descriptions (e.g., category names, category-indicative\nkeywords). Existing studies on weakly supervised paper classification are less\nconcerned with two challenges: (1) Papers should be classified into not only\ncoarse-grained research topics but also fine-grained themes, and potentially\ninto multiple themes, given a large and fine-grained label space; and (2) full\ntext should be utilized to complement the paper title and abstract for\nclassification. Moreover, instead of viewing the entire paper as a long linear\nsequence, one should exploit the structural information such as citation links\nacross papers and the hierarchy of sections and paragraphs in each paper. To\ntackle these challenges, in this study, we propose FUTEX, a framework that uses\nthe cross-paper network structure and the in-paper hierarchy structure to\nclassify full-text scientific papers under weak supervision. A network-aware\ncontrastive fine-tuning module and a hierarchy-aware aggregation module are\ndesigned to leverage the two types of structural signals, respectively.\nExperiments on two benchmark datasets demonstrate that FUTEX significantly\noutperforms competitive baselines and is on par with fully supervised\nclassifiers that use 1,000 to 60,000 ground-truth training samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1\">Bowen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiusi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yanzhen Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks. (arXiv:2306.14030v1 [cs.CL])","link":"http://arxiv.org/abs/2306.14030","description":"<p>The research on code-mixed data is limited due to the unavailability of\ndedicated code-mixed datasets and pre-trained language models. In this work, we\nfocus on the low-resource Indian language Marathi which lacks any prior work in\ncode-mixing. We present L3Cube-MeCorpus, a large code-mixed Marathi-English\n(Mr-En) corpus with 5 million tweets for pretraining. We also release\nL3Cube-MeBERT and MeRoBERTa, code-mixed BERT-based transformer models\npre-trained on MeCorpus. Furthermore, for benchmarking, we present three\nsupervised datasets MeHate, MeSent, and MeLID for downstream tasks like\ncode-mixed Mr-En hate speech detection, sentiment analysis, and language\nidentification respectively. These evaluation datasets individually consist of\nmanually annotated \\url{~}12,000 Marathi-English code-mixed tweets. Ablations\nshow that the models trained on this novel corpus significantly outperform the\nexisting state-of-the-art BERT models. This is the first work that presents\nartifacts for code-mixed Marathi research. All datasets and models are publicly\nreleased at https://github.com/l3cube-pune/MarathiNLP .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chavan_T/0/1/0/all/0/1\">Tanmay Chavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_O/0/1/0/all/0/1\">Omkar Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kane_A/0/1/0/all/0/1\">Aditya Kane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patankar_S/0/1/0/all/0/1\">Shantanu Patankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weighted Automata Extraction and Explanation of Recurrent Neural Networks for Natural Language Tasks. (arXiv:2306.14040v1 [cs.CL])","link":"http://arxiv.org/abs/2306.14040","description":"<p>Recurrent Neural Networks (RNNs) have achieved tremendous success in\nprocessing sequential data, yet understanding and analyzing their behaviours\nremains a significant challenge. To this end, many efforts have been made to\nextract finite automata from RNNs, which are more amenable for analysis and\nexplanation. However, existing approaches like exact learning and compositional\napproaches for model extraction have limitations in either scalability or\nprecision. In this paper, we propose a novel framework of Weighted Finite\nAutomata (WFA) extraction and explanation to tackle the limitations for natural\nlanguage tasks. First, to address the transition sparsity and context loss\nproblems we identified in WFA extraction for natural language tasks, we propose\nan empirical method to complement missing rules in the transition diagram, and\nadjust transition matrices to enhance the context-awareness of the WFA. We also\npropose two data augmentation tactics to track more dynamic behaviours of RNN,\nwhich further allows us to improve the extraction precision. Based on the\nextracted model, we propose an explanation method for RNNs including a word\nembedding method -- Transition Matrix Embeddings (TME) and TME-based task\noriented explanation for the target RNN. Our evaluation demonstrates the\nadvantage of our method in extraction precision than existing approaches, and\nthe effectiveness of TME-based explanation method in applications to\npretraining and adversarial example generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zeming Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Meng Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Symbolic Chain-of-Thought Distillation: Small Models Can Also \"Think\" Step-by-Step. (arXiv:2306.14050v1 [cs.CL])","link":"http://arxiv.org/abs/2306.14050","description":"<p>Chain-of-thought prompting (e.g., \"Let's think step-by-step\") primes large\nlanguage models to verbalize rationalization for their predictions. While\nchain-of-thought can lead to dramatic performance gains, benefits appear to\nemerge only for sufficiently large models (beyond 50B parameters). We show that\norders-of-magnitude smaller models (125M -- 1.3B parameters) can still benefit\nfrom chain-of-thought prompting. To achieve this, we introduce Symbolic\nChain-of-Thought Distillation (SCoTD), a method to train a smaller student\nmodel on rationalizations sampled from a significantly larger teacher model.\nExperiments across several commonsense benchmarks show that: 1) SCoTD enhances\nthe performance of the student model in both supervised and few-shot settings,\nand especially for challenge sets; 2) sampling many reasoning chains per\ninstance from the teacher is paramount; and 3) after distillation, student\nchain-of-thoughts are judged by humans as comparable to the teacher, despite\norders of magnitude fewer parameters. We test several hypotheses regarding what\nproperties of chain-of-thought samples are important, e.g., diversity vs.\nteacher likelihood vs. open-endedness. We release our corpus of\nchain-of-thought samples and code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DesCo: Learning Object Recognition with Rich Language Descriptions. (arXiv:2306.14060v1 [cs.CV])","link":"http://arxiv.org/abs/2306.14060","description":"<p>Recent development in vision-language approaches has instigated a paradigm\nshift in learning visual recognition models from language supervision. These\napproaches align objects with language queries (e.g. \"a photo of a cat\") and\nimprove the models' adaptability to identify novel objects and domains.\nRecently, several studies have attempted to query these models with complex\nlanguage expressions that include specifications of fine-grained semantic\ndetails, such as attributes, shapes, textures, and relations. However, simply\nincorporating language descriptions as queries does not guarantee accurate\ninterpretation by the models. In fact, our experiments show that GLIP, the\nstate-of-the-art vision-language model for object detection, often disregards\ncontextual information in the language descriptions and instead relies heavily\non detecting objects solely by their names. To tackle the challenges, we\npropose a new description-conditioned (DesCo) paradigm of learning object\nrecognition models with rich language descriptions consisting of two major\ninnovations: 1) we employ a large language model as a commonsense knowledge\nengine to generate rich language descriptions of objects based on object names\nand the raw image-text caption; 2) we design context-sensitive queries to\nimprove the model's ability in deciphering intricate nuances embedded within\ndescriptions and enforce the model to focus on context rather than object names\nalone. On two novel object detection benchmarks, LVIS and OminiLabel, under the\nzero-shot detection setting, our approach achieves 34.8 APr minival (+9.1) and\n29.3 AP (+3.6), respectively, surpassing the prior state-of-the-art models,\nGLIP and FIBER, by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zi-Yi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UAlberta at SemEval-2023 Task 1: Context Augmentation and Translation for Multilingual Visual Word Sense Disambiguation. (arXiv:2306.14067v1 [cs.CL])","link":"http://arxiv.org/abs/2306.14067","description":"<p>We describe the systems of the University of Alberta team for the\nSemEval-2023 Visual Word Sense Disambiguation (V-WSD) Task. We present a novel\nalgorithm that leverages glosses retrieved from BabelNet, in combination with\ntext and image encoders. Furthermore, we compare language-specific encoders\nagainst the application of English encoders to translated texts. As the\ncontexts given in the task datasets are extremely short, we also experiment\nwith augmenting these contexts with descriptions generated by a language model.\nThis yields substantial improvements in accuracy. We describe and evaluate\nadditional V-WSD methods which use image generation and text-conditioned image\nsegmentation. Overall, the results of our official submission rank us 18 out of\n56 teams. Some of our unofficial results are even better than the official\nones. Our code is publicly available at https://github.com/UAlberta-NLP/v-wsd.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ogezi_M/0/1/0/all/0/1\">Michael Ogezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauer_B/0/1/0/all/0/1\">Bradley Hauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omarov_T/0/1/0/all/0/1\">Talgat Omarov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_N/0/1/0/all/0/1\">Ning Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondrak_G/0/1/0/all/0/1\">Grzegorz Kondrak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models. (arXiv:2306.14096v1 [cs.CL])","link":"http://arxiv.org/abs/2306.14096","description":"<p>Entity-level fine-grained sentiment analysis in the financial domain is a\ncrucial subtask of sentiment analysis and currently faces numerous challenges.\nThe primary challenge stems from the lack of high-quality and large-scale\nannotated corpora specifically designed for financial text sentiment analysis,\nwhich in turn limits the availability of data necessary for developing\neffective text processing techniques. Recent advancements in large language\nmodels (LLMs) have yielded remarkable performance in natural language\nprocessing tasks, primarily centered around language pattern matching. In this\npaper, we propose a novel and extensive Chinese fine-grained financial\nsentiment analysis dataset, FinChina SA, for enterprise early warning. We\nthoroughly evaluate and experiment with well-known existing open-source LLMs\nusing our dataset. We firmly believe that our dataset will serve as a valuable\nresource to advance the exploration of real-world financial sentiment analysis\ntasks, which should be the focus of future research. Our dataset and all code\nto replicate the experimental results will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yinyu Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanru Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Weiqiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Youhao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Trustworthy Explanation: On Causal Rationalization. (arXiv:2306.14115v1 [cs.LG])","link":"http://arxiv.org/abs/2306.14115","description":"<p>With recent advances in natural language processing, rationalization becomes\nan essential self-explaining diagram to disentangle the black box by selecting\na subset of input texts to account for the major variation in prediction. Yet,\nexisting association-based approaches on rationalization cannot identify true\nrationales when two or more snippets are highly inter-correlated and thus\nprovide a similar contribution to prediction accuracy, so-called spuriousness.\nTo address this limitation, we novelly leverage two causal desiderata,\nnon-spuriousness and efficiency, into rationalization from the causal inference\nperspective. We formally define a series of probabilities of causation based on\na newly proposed structural causal model of rationalization, with its\ntheoretical identification established as the main component of learning\nnecessary and sufficient rationales. The superior performance of the proposed\ncausal rationalization is demonstrated on real-world review and medical\ndatasets with extensive experiments compared to state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunlong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hengrui Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speaker-change Aware CRF for Dialogue Act Classification. (arXiv:2004.02913v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.02913","description":"<p>Recent work in Dialogue Act (DA) classification approaches the task as a\nsequence labeling problem, using neural network models coupled with a\nConditional Random Field (CRF) as the last layer. CRF models the conditional\nprobability of the target DA label sequence given the input utterance sequence.\nHowever, the task involves another important input sequence, that of speakers,\nwhich is ignored by previous work. To address this limitation, this paper\nproposes a simple modification of the CRF layer that takes speaker-change into\naccount. Experiments on the SwDA corpus show that our modified CRF layer\noutperforms the original one, with very wide margins for some DA labels.\nFurther, visualizations demonstrate that our CRF layer can learn meaningful,\nsophisticated transition patterns between DA label pairs conditioned on\nspeaker-change in an end-to-end way. Code is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shang_G/0/1/0/all/0/1\">Guokan Shang</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Tixier_A/0/1/0/all/0/1\">Antoine Jean-Pierre Tixier</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1\">Michalis Vazirgiannis</a> (1 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Lorre_J/0/1/0/all/0/1\">Jean-Pierre Lorr&#xe9;</a> (2) ((1) &#xc9;cole Polytechnique, (2) Linagora, (3) AUEB)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual Samples Synthesizing and Training for Robust Visual Question Answering. (arXiv:2110.01013v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.01013","description":"<p>Today's VQA models still tend to capture superficial linguistic correlations\nin the training set and fail to generalize to the test set with different QA\ndistributions. To reduce these language biases, recent VQA works introduce an\nauxiliary question-only model to regularize the training of targeted VQA model,\nand achieve dominating performance on diagnostic benchmarks for\nout-of-distribution testing. However, due to complex model design, these\nensemble-based methods are unable to equip themselves with two indispensable\ncharacteristics of an ideal VQA model: 1) Visual-explainable: The model should\nrely on the right visual regions when making decisions. 2) Question-sensitive:\nThe model should be sensitive to the linguistic variations in questions. To\nthis end, we propose a novel model-agnostic Counterfactual Samples Synthesizing\nand Training (CSST) strategy. After training with CSST, VQA models are forced\nto focus on all critical objects and words, which significantly improves both\nvisual-explainable and question-sensitive abilities. Specifically, CSST is\ncomposed of two parts: Counterfactual Samples Synthesizing (CSS) and\nCounterfactual Samples Training (CST). CSS generates counterfactual samples by\ncarefully masking critical objects in images or words in questions and\nassigning pseudo ground-truth answers. CST not only trains the VQA models with\nboth complementary samples to predict respective ground-truth answers, but also\nurges the VQA models to further distinguish the original samples and\nsuperficially similar counterfactual ones. To facilitate the CST training, we\npropose two variants of supervised contrastive loss for VQA, and design an\neffective positive and negative sample selection mechanism based on CSS.\nExtensive experiments have shown the effectiveness of CSST. Particularly, by\nbuilding on top of model LMH+SAR, we achieve record-breaking performance on all\nOOD benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yuhang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yulei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Instance and Knowledge Alignment Pretraining for Aspect-based Sentiment Analysis. (arXiv:2110.13398v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.13398","description":"<p>Aspect-based Sentiment Analysis (ABSA) aims to determine the sentiment\npolarity towards an aspect. Because of the expensive and limited labelled data,\nthe pretraining strategy has become the de-facto standard for ABSA. However,\nthere always exists severe domain shift between the pretraining and downstream\nABSA datasets, hindering the effective knowledge transfer when directly\nfinetuning and making the downstream task performs sub-optimal. To mitigate\nsuch domain shift, we introduce a unified alignment pretraining framework into\nthe vanilla pretrain-finetune pipeline with both instance- and knowledge-level\nalignments. Specifically, we first devise a novel coarse-to-fine retrieval\nsampling approach to select target domain-related instances from the\nlarge-scale pretraining dataset, thus aligning the instances between\npretraining and target domains (First Stage). Then, we introduce a knowledge\nguidance-based strategy to further bridge the domain gap at the knowledge\nlevel. In practice, we formulate the model pretrained on the sampled instances\ninto a knowledge guidance model and a learner model, respectively. On the\ntarget dataset, we design an on-the-fly teacher-student joint fine-tuning\napproach to progressively transfer the knowledge from the knowledge guidance\nmodel to the learner model (Second Stage). Thereby, the learner model can\nmaintain more domain-invariant knowledge when learning new knowledge from the\ntarget dataset. In the Third Stage, the learner model is finetuned to better\nadapt its learned knowledge to the target dataset. Extensive experiments and\nanalyses on several ABSA benchmarks demonstrate the effectiveness and\nuniversality of our proposed pretraining framework. Our source code and models\nare publicly available at https://github.com/WHU-ZQH/UIKA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Juhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1\">Qihuang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models as Knowledge Embeddings. (arXiv:2206.12617v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.12617","description":"<p>Knowledge embeddings (KE) represent a knowledge graph (KG) by embedding\nentities and relations into continuous vector spaces. Existing methods are\nmainly structure-based or description-based. Structure-based methods learn\nrepresentations that preserve the inherent structure of KGs. They cannot well\nrepresent abundant long-tail entities in real-world KGs with limited structural\ninformation. Description-based methods leverage textual information and\nlanguage models. Prior approaches in this direction barely outperform\nstructure-based ones, and suffer from problems like expensive negative sampling\nand restrictive description demand. In this paper, we propose LMKE, which\nadopts Language Models to derive Knowledge Embeddings, aiming at both enriching\nrepresentations of long-tail entities and solving problems of prior\ndescription-based methods. We formulate description-based KE learning with a\ncontrastive learning framework to improve efficiency in training and\nevaluation. Experimental results show that LMKE achieves state-of-the-art\nperformance on KE benchmarks of link prediction and triple classification,\nespecially for long-tail entities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qianyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiaqing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiSCoMaT: Distantly Supervised Composition Extraction from Tables in Materials Science Articles. (arXiv:2207.01079v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.01079","description":"<p>A crucial component in the curation of KB for a scientific domain is\ninformation extraction from tables in the domain's published articles -- tables\ncarry important information (often numeric), which must be adequately extracted\nfor a comprehensive machine understanding of an article. Existing table\nextractors assume prior knowledge of table structure and format, which may not\nbe known in scientific tables. We study a specific and challenging table\nextraction problem: extracting compositions of materials (e.g., glasses,\nalloys). We first observe that materials science researchers organize similar\ncompositions in a wide variety of table styles, necessitating an intelligent\nmodel for table understanding and composition extraction. Consequently, we\ndefine this novel task as a challenge for the ML community and create a\ntraining dataset comprising 4,408 distantly supervised tables, along with 1,475\nmanually annotated dev and test tables. We also present DiSCoMaT, a strong\nbaseline geared towards this specific task, which combines multiple graph\nneural networks with several task-specific regular expressions, features, and\nconstraints. We show that DiSCoMaT outperforms recent table processing\narchitectures by significant margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1\">Tanishq Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaki_M/0/1/0/all/0/1\">Mohd Zaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1\">N. M. Anoop Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-and-Language Pretraining. (arXiv:2207.01772v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.01772","description":"<p>With the burgeoning amount of data of image-text pairs and diversity of\nVision-and-Language (V\\&amp;L) tasks, scholars have introduced an abundance of deep\nlearning models in this research domain. Furthermore, in recent years, transfer\nlearning has also shown tremendous success in Computer Vision for tasks such as\nImage Classification, Object Detection, etc., and in Natural Language\nProcessing for Question Answering, Machine Translation, etc. Inheriting the\nspirit of Transfer Learning, research works in V\\&amp;L have devised multiple\npretraining techniques on large-scale datasets in order to enhance the\nperformance of downstream tasks. The aim of this article is to provide a\ncomprehensive revision of contemporary V\\&amp;L pretraining models. In particular,\nwe categorize and delineate pretraining approaches, along with the summary of\nstate-of-the-art vision-and-language pretrained models. Moreover, a list of\ntraining datasets and downstream tasks is supplied to further polish the\nperspective into V\\&amp;L pretraining. Lastly, we decided to take a further step to\ndiscuss numerous directions for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cong-Duy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaobao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1\">See-Kiong Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1\">Anh Tuan Luu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Cognitive Study on Semantic Similarity Analysis of Large Corpora: A Transformer-based Approach. (arXiv:2207.11716v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.11716","description":"<p>Semantic similarity analysis and modeling is a fundamentally acclaimed task\nin many pioneering applications of natural language processing today. Owing to\nthe sensation of sequential pattern recognition, many neural networks like RNNs\nand LSTMs have achieved satisfactory results in semantic similarity modeling.\nHowever, these solutions are considered inefficient due to their inability to\nprocess information in a non-sequential manner, thus leading to the improper\nextraction of context. Transformers function as the state-of-the-art\narchitecture due to their advantages like non-sequential data processing and\nself-attention. In this paper, we perform semantic similarity analysis and\nmodeling on the U.S Patent Phrase to Phrase Matching Dataset using both\ntraditional and transformer-based techniques. We experiment upon four different\nvariants of the Decoding Enhanced BERT - DeBERTa and enhance its performance by\nperforming K-Fold Cross-Validation. The experimental results demonstrate our\nmethodology's enhanced performance compared to traditional techniques, with an\naverage Pearson correlation score of 0.79.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nemani_P/0/1/0/all/0/1\">Praneeth Nemani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vollala_S/0/1/0/all/0/1\">Satyanarayana Vollala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PROD: Progressive Distillation for Dense Retrieval. (arXiv:2209.13335v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2209.13335","description":"<p>Knowledge distillation is an effective way to transfer knowledge from a\nstrong teacher to an efficient student model. Ideally, we expect the better the\nteacher is, the better the student. However, this expectation does not always\ncome true. It is common that a better teacher model results in a bad student\nvia distillation due to the nonnegligible gap between teacher and student. To\nbridge the gap, we propose PROD, a PROgressive Distillation method, for dense\nretrieval. PROD consists of a teacher progressive distillation and a data\nprogressive distillation to gradually improve the student. We conduct extensive\nexperiments on five widely-used benchmarks, MS MARCO Passage, TREC Passage 19,\nTREC Document 19, MS MARCO Document and Natural Questions, where PROD achieves\nthe state-of-the-art within the distillation methods for dense retrieval. The\ncode and models will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhenghao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_A/0/1/0/all/0/1\">Anlei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jian Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jingwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_R/0/1/0/all/0/1\">Rangan Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KSAT: Knowledge-infused Self Attention Transformer -- Integrating Multiple Domain-Specific Contexts. (arXiv:2210.04307v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04307","description":"<p>Domain-specific language understanding requires integrating multiple pieces\nof relevant contextual information. For example, we see both suicide and\ndepression-related behavior (multiple contexts) in the text ``I have a gun and\nfeel pretty bad about my life, and it wouldn't be the worst thing if I didn't\nwake up tomorrow''. Domain specificity in self-attention architectures is\nhandled by fine-tuning on excerpts from relevant domain specific resources\n(datasets and external knowledge - medical textbook chapters on mental health\ndiagnosis related to suicide and depression). We propose a modified\nself-attention architecture Knowledge-infused Self Attention Transformer (KSAT)\nthat achieves the integration of multiple domain-specific contexts through the\nuse of external knowledge sources. KSAT introduces knowledge-guided biases in\ndedicated self-attention layers for each knowledge source to accomplish this.\nIn addition, KSAT provides mechanics for controlling the trade-off between\nlearning from data and learning from knowledge. Our quantitative and\nqualitative evaluations show that (1) the KSAT architecture provides novel\nhuman-understandable ways to precisely measure and visualize the contributions\nof the infused domain contexts, and (2) KSAT performs competitively with other\nknowledge-infused baselines and significantly outperforms baselines that use\nfine-tuning for domain-specific tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zi_Y/0/1/0/all/0/1\">Yuxin Zi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_V/0/1/0/all/0/1\">Vignesh Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_M/0/1/0/all/0/1\">Manas Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing neural language models for understanding of words of estimative probability. (arXiv:2211.03358v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.03358","description":"<p>Words of estimative probability (WEP) are expressions of a statement's\nplausibility (probably, maybe, likely, doubt, likely, unlikely, impossible...).\nMultiple surveys demonstrate the agreement of human evaluators when assigning\nnumerical probability levels to WEP. For example, highly likely corresponds to\na median chance of 0.90+-0.08 in Fagen-Ulmschneider (2015)'s survey. In this\nwork, we measure the ability of neural language processing models to capture\nthe consensual probability level associated to each WEP. Firstly, we use the\nUNLI dataset (Chen et al., 2020) which associates premises and hypotheses with\ntheir perceived joint probability p, to construct prompts, e.g. \"[PREMISE].\n[WEP], [HYPOTHESIS].\" and assess whether language models can predict whether\nthe WEP consensual probability level is close to p. Secondly, we construct a\ndataset of WEP-based probabilistic reasoning, to test whether language models\ncan reason with WEP compositions. When prompted \"[EVENTA] is likely. [EVENTB]\nis impossible.\", a causal language model should not express that [EVENTA&amp;B] is\nlikely. We show that both tasks are unsolved by off-the-shelf English language\nmodels, but that fine-tuning leads to transferable improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sileo_D/0/1/0/all/0/1\">Damien Sileo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting PaLM for Translation: Assessing Strategies and Performance. (arXiv:2211.09102v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.09102","description":"<p>Large language models (LLMs) that have been trained on multilingual but not\nparallel text exhibit a remarkable ability to translate between languages. We\nprobe this ability in an in-depth study of the pathways language model (PaLM),\nwhich has demonstrated the strongest machine translation (MT) performance among\nsimilarly-trained LLMs to date. We investigate various strategies for choosing\ntranslation examples for few-shot prompting, concluding that example quality is\nthe most important factor. Using optimized prompts, we revisit previous\nassessments of PaLM's MT capabilities with more recent test sets, modern MT\nmetrics, and human evaluation, and find that its performance, while impressive,\nstill lags that of state-of-the-art supervised systems. We conclude by\nproviding an analysis of PaLM's MT output which reveals some interesting\nproperties and prospects for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vilar_D/0/1/0/all/0/1\">David Vilar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1\">Markus Freitag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherry_C/0/1/0/all/0/1\">Colin Cherry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiaming Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratnakar_V/0/1/0/all/0/1\">Viresh Ratnakar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_G/0/1/0/all/0/1\">George Foster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse Demonstrations Improve In-context Compositional Generalization. (arXiv:2212.06800v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.06800","description":"<p>In-context learning has shown great success in i.i.d semantic parsing splits,\nwhere the training and test sets are drawn from the same distribution. In this\nsetup, models are typically prompted with demonstrations that are similar to\nthe input utterance. However, in the setup of compositional generalization,\nwhere models are tested on outputs with structures that are absent from the\ntraining set, selecting similar demonstrations is insufficient, as often no\nexample will be similar enough to the input. In this work, we propose a method\nto select diverse demonstrations that aims to collectively cover all of the\nstructures required in the output program, in order to encourage the model to\ngeneralize to new structures from these demonstrations. We empirically show\nthat combining diverse demonstrations with in-context learning substantially\nimproves performance across three compositional generalization semantic parsing\ndatasets in the pure in-context learning setup and when combined with\nfinetuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levy_I/0/1/0/all/0/1\">Itay Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogin_B/0/1/0/all/0/1\">Ben Bogin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do I have the Knowledge to Answer? Investigating Answerability of Knowledge Base Questions. (arXiv:2212.10189v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10189","description":"<p>When answering natural language questions over knowledge bases, missing\nfacts, incomplete schema and limited scope naturally lead to many questions\nbeing unanswerable. While answerability has been explored in other QA settings,\nit has not been studied for QA over knowledge bases (KBQA). We create\nGrailQAbility, a new benchmark KBQA dataset with unanswerability, by first\nidentifying various forms of KB incompleteness that make questions\nunanswerable, and then systematically adapting GrailQA (a popular KBQA dataset\nwith only answerable questions). Experimenting with three state-of-the-art KBQA\nmodels, we find that all three models suffer a drop in performance even after\nsuitable adaptation for unanswerable questions. In addition, these often detect\nunanswerability for wrong reasons and find specific forms of unanswerability\nparticularly difficult to handle. This underscores the need for further\nresearch in making KBQA systems robust to unanswerability\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patidar_M/0/1/0/all/0/1\">Mayur Patidar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faldu_P/0/1/0/all/0/1\">Prayushi Faldu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Avinash Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vig_L/0/1/0/all/0/1\">Lovekesh Vig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_I/0/1/0/all/0/1\">Indrajit Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceive and predict: self-supervised speech representation based loss functions for speech enhancement. (arXiv:2301.04388v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2301.04388","description":"<p>Recent work in the domain of speech enhancement has explored the use of\nself-supervised speech representations to aid in the training of neural speech\nenhancement models. However, much of this work focuses on using the deepest or\nfinal outputs of self supervised speech representation models, rather than the\nearlier feature encodings. The use of self supervised representations in such a\nway is often not fully motivated. In this work it is shown that the distance\nbetween the feature encodings of clean and noisy speech correlate strongly with\npsychoacoustically motivated measures of speech quality and intelligibility, as\nwell as with human Mean Opinion Score (MOS) ratings. Experiments using this\ndistance as a loss function are performed and improved performance over the use\nof STFT spectrogram distance based loss as well as other common loss functions\nfrom speech enhancement literature is demonstrated using objective measures\nsuch as perceptual evaluation of speech quality (PESQ) and short-time objective\nintelligibility (STOI).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Close_G/0/1/0/all/0/1\">George Close</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravenscroft_W/0/1/0/all/0/1\">William Ravenscroft</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hain_T/0/1/0/all/0/1\">Thomas Hain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goetze_S/0/1/0/all/0/1\">Stefan Goetze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discourse Structure Extraction from Pre-Trained and Fine-Tuned Language Models in Dialogues. (arXiv:2302.05895v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.05895","description":"<p>Discourse processing suffers from data sparsity, especially for dialogues. As\na result, we explore approaches to build discourse structures for dialogues,\nbased on attention matrices from Pre-trained Language Models (PLMs). We\ninvestigate multiple tasks for fine-tuning and show that the dialogue-tailored\nSentence Ordering task performs best. To locate and exploit discourse\ninformation in PLMs, we propose an unsupervised and a semi-supervised method.\nOur proposals achieve encouraging results on the STAC corpus, with F1 scores of\n57.2 and 59.3 for unsupervised and semi-supervised methods, respectively. When\nrestricted to projective trees, our scores improved to 63.3 and 68.1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chuyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huber_P/0/1/0/all/0/1\">Patrick Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wen Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amblard_M/0/1/0/all/0/1\">Maxime Amblard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braud_C/0/1/0/all/0/1\">Chlo&#xe9; Braud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1\">Giuseppe Carenini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequential Query Encoding For Complex Query Answering on Knowledge Graphs. (arXiv:2302.13114v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.13114","description":"<p>Complex Query Answering (CQA) is an important and fundamental task for\nknowledge graph (KG) reasoning. Query encoding (QE) is proposed as a fast and\nrobust solution to CQA. In the encoding process, most existing QE methods first\nparse the logical query into an executable computational direct-acyclic graph\n(DAG), then use neural networks to parameterize the operators, and finally,\nrecursively execute these neuralized operators. However, the\nparameterization-and-execution paradigm may be potentially over-complicated, as\nit can be structurally simplified by a single neural network encoder.\nMeanwhile, sequence encoders, like LSTM and Transformer, proved to be effective\nfor encoding semantic graphs in related tasks. Motivated by this, we propose\nsequential query encoding (SQE) as an alternative to encode queries for CQA.\nInstead of parameterizing and executing the computational graph, SQE first uses\na search-based algorithm to linearize the computational graph to a sequence of\ntokens and then uses a sequence encoder to compute its vector representation.\nThen this vector representation is used as a query embedding to retrieve\nanswers from the embedding space according to similarity scores. Despite its\nsimplicity, SQE demonstrates state-of-the-art neural query encoding performance\non FB15k, FB15k-237, and NELL on an extended benchmark including twenty-nine\ntypes of in-distribution queries. Further experiment shows that SQE also\ndemonstrates comparable knowledge inference capability on out-of-distribution\nqueries, whose query types are not observed during the training process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiaxin Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1\">Tianshi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks. (arXiv:2302.13939v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.13939","description":"<p>As the size of large language models continue to scale, so does the\ncomputational resources required to run it. Spiking Neural Networks (SNNs) have\nemerged as an energy-efficient approach to deep learning that leverage sparse\nand event-driven activations to reduce the computational overhead associated\nwith model inference. While they have become competitive with non-spiking\nmodels on many computer vision tasks, SNNs have also proven to be more\nchallenging to train. As a result, their performance lags behind modern deep\nlearning, and we are yet to see the effectiveness of SNNs in language\ngeneration. In this paper, inspired by the Receptance Weighted Key Value (RWKV)\nlanguage model, we successfully implement `SpikeGPT', a generative language\nmodel with binary, event-driven spiking activation units. We train the proposed\nmodel on two model variants: 45M and 216M parameters. To the best of our\nknowledge, SpikeGPT is the largest backpropagation-trained SNN model to date,\nrendering it suitable for both the generation and comprehension of natural\nlanguage. We achieve this by modifying the transformer block to replace\nmulti-head self attention to reduce quadratic computational complexity O(N^2)\nto linear complexity O(N) with increasing sequence length. Input tokens are\ninstead streamed in sequentially to our attention mechanism (as with typical\nSNNs). Our preliminary experiments show that SpikeGPT remains competitive with\nnon-spiking models on tested benchmarks, while maintaining 20x fewer operations\nwhen processed on neuromorphic hardware that can leverage sparse, event-driven\nactivations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Rui-Jie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qihang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guoqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eshraghian_J/0/1/0/all/0/1\">Jason K. Eshraghian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Transductions and Alignments with RNN Seq2seq Models. (arXiv:2303.06841v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.06841","description":"<p>The paper studies the capabilities of Recurrent-Neural-Network sequence to\nsequence (RNN seq2seq) models in learning four transduction tasks: identity,\nreversal, total reduplication, and quadratic copying. These transductions are\ntraditionally well studied under finite state transducers and attributed with\nincreasing complexity. We find that RNN seq2seq models are only able to\napproximate a mapping that fits the training or in-distribution data, instead\nof learning the underlying functions. Although attention makes learning more\nefficient and robust, it does not overcome the out-of-distribution\ngeneralization limitation. We establish a novel complexity hierarchy for\nlearning the four tasks for attention-less RNN seq2seq models, which may be\nunderstood in terms of the complexity hierarchy of formal languages, instead of\nstring transductions. RNN variants also play a role in the results. In\nparticular, we show that Simple RNN seq2seq models cannot count the input\nlength.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhengxiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2303.12816","description":"<p>Knowledge graph embedding (KGE) that maps entities and relations into vector\nrepresentations is essential for downstream tasks. Conventional KGE methods\nrequire relatively high-dimensional entity representations to preserve the\nstructural information of knowledge graph, but lead to oversized model\nparameters. Recent methods reduce model parameters by adopting low-dimensional\nentity representations, while developing techniques (e.g., knowledge\ndistillation) to compensate for the reduced dimension. However, such operations\nproduce degraded model accuracy and limited reduction of model parameters.\nSpecifically, we view the concatenation of all entity representations as an\nembedding layer, and then conventional KGE methods that adopt high-dimensional\nentity representations equal to enlarging the width of the embedding layer to\ngain expressiveness. To achieve parameter efficiency without sacrificing\naccuracy, we instead increase the depth and propose a deeper embedding network\nfor entity representations, i.e., a narrow embedding layer and a multi-layer\ndimension lifting network (LiftNet). Experiments on three public datasets show\nthat the proposed method (implemented based on TransE and DistMult) with\n4-dimensional entity representations achieves more accurate link prediction\nresults than counterpart parameter-efficient KGE methods and strong KGE\nbaselines, including TransE and DistMult with 512-dimensional entity\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_B/0/1/0/all/0/1\">Borui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yong Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Longxiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">He Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jiong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_T/0/1/0/all/0/1\">Tom Luan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge. (arXiv:2303.14070v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.14070","description":"<p>The primary aim of this research was to address the limitations observed in\nthe medical knowledge of prevalent large language models (LLMs) such as\nChatGPT, by creating a specialized language model with enhanced accuracy in\nmedical advice. We achieved this by adapting and refining the large language\nmodel meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialogues\nsourced from a widely used online medical consultation platform. These\nconversations were cleaned and anonymized to respect privacy concerns. In\naddition to the model refinement, we incorporated a self-directed information\nretrieval mechanism, allowing the model to access and utilize real-time\ninformation from online sources like Wikipedia and data from curated offline\nmedical databases. The fine-tuning of the model with real-world patient-doctor\ninteractions significantly improved the model's ability to understand patient\nneeds and provide informed advice. By equipping the model with self-directed\ninformation retrieval from reliable online and offline sources, we observed\nsubstantial improvements in the accuracy of its responses. Our proposed\nChatDoctor, represents a significant advancement in medical LLMs, demonstrating\na significant improvement in understanding patient inquiries and providing\naccurate advice. Given the high stakes and low error tolerance in the medical\nfield, such enhancements in providing accurate and reliable information are not\nonly beneficial but essential.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zihan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dan_R/0/1/0/all/0/1\">Ruilong Dan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Steve Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">You Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning. (arXiv:2304.01295v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01295","description":"<p>Cross-lingual transfer of language models trained on high-resource languages\nlike English has been widely studied for many NLP tasks, but focus on\nconversational tasks has been rather limited. This is partly due to the high\ncost of obtaining non-English conversational data, which results in limited\ncoverage. In this work, we introduce XSGD, a parallel and large-scale\nmultilingual conversation dataset that we created by translating the\nEnglish-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into\n105 other languages. XSGD contains approximately 330k utterances per language.\nTo facilitate aligned cross-lingual representations, we develop an efficient\nprompt-tuning-based method for learning alignment prompts. We also investigate\ntwo different classifiers: NLI-based and vanilla classifiers, and test\ncross-lingual capability enabled by the aligned prompts. We evaluate our\nmodel's cross-lingual generalization capabilities on two conversation tasks:\nslot-filling and intent classification. Our results demonstrate the strong and\nefficient modeling ability of NLI-based classifiers and the large cross-lingual\ntransfer improvements achieved by our aligned prompts, particularly in few-shot\nsettings. In addition, we highlight the nice results of our approach compared\nto LLMs such as text-davinci-003 and ChatGPT in both zero-shot and few-shot\nsettings. While LLMs exhibit impressive performance in English, their\ncross-lingual capabilities in other languages, particularly low-resource\nlanguages, are limited.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_L/0/1/0/all/0/1\">Lifu Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1\">Jin Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1\">Semih Yavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergence of Symbols in Neural Networks for Semantic Understanding and Communication. (arXiv:2304.06377v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2304.06377","description":"<p>The capacity to generate meaningful symbols and effectively employ them for\nadvanced cognitive processes, such as communication, reasoning, and planning,\nconstitutes a fundamental and distinctive aspect of human intelligence.\nExisting deep neural networks still notably lag human capabilities in terms of\ngenerating symbols for higher cognitive functions. Here, we propose a solution\n(symbol emergence artificial network (SEA-net)) to endow neural networks with\nthe ability to create symbols, understand semantics, and achieve communication.\nSEA-net generates symbols that dynamically configure the network to perform\nspecific tasks. These symbols capture compositional semantic information that\nallows the system to acquire new functions purely by symbolic manipulation or\ncommunication. In addition, these self-generated symbols exhibit an intrinsic\nstructure resembling that of natural language, suggesting a common framework\nunderlying the generation and understanding of symbols in both human brains and\nartificial neural networks. We believe that the proposed framework will be\ninstrumental in producing more capable systems that can synergize the strengths\nof connectionist and symbolic approaches for artificial intelligence (AI).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Liangxuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shan Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Autoregressive NLP Tasks via Modular Linearized Attention. (arXiv:2304.08453v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.08453","description":"<p>Various natural language processing (NLP) tasks necessitate models that are\nefficient and small based on their ultimate application at the edge or in other\nresource-constrained environments. While prior research has reduced the size of\nthese models, increasing computational efficiency without considerable\nperformance impacts remains difficult, especially for autoregressive tasks.\nThis paper proposes modular linearized attention (MLA), which combines multiple\nefficient attention mechanisms, including cosFormer, to maximize inference\nquality while achieving notable speedups. We validate this approach on several\nautoregressive NLP tasks, including speech-to-text neural machine translation\n(S2T NMT), speech-to-text simultaneous translation (SimulST), and\nautoregressive text-to-spectrogram, noting efficiency gains on TTS and\ncompetitive performance for NMT and SimulST during training and inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agostinelli_V/0/1/0/all/0/1\">Victor Agostinelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lizhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks. (arXiv:2304.14732v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.14732","description":"<p>Making the contents generated by Large Language Model (LLM) such as ChatGPT,\naccurate, credible and traceable is crucial, especially in complex\nknowledge-intensive tasks that require multi-step reasoning and each of which\nneeds knowledge to solve. Introducing Information Retrieval (IR) to provide LLM\nwith external knowledge is good potential to solve this problem. However, where\nand how to introduce IR into LLM is a big challenge. Previous work has the\ndisadvantage that the wrong knowledge retrieved by IR misleads the LLM or\nbreaks the reasoning chain of LLM. In this paper, we propose a novel framework\ncalled Search-in-the-Chain (SearChain) for the interaction between LLM and IR\nto solve the challenges. First, LLM generates the global reasoning chain called\nChain-of-Query (CoQ) where each node consists of an IR-oriented query and the\nanswer to the query. Second, IR verifies the answer of each node of CoQ, it\ncorrects the answer that is not consistent with the retrieved information when\nIR gives high confidence, which improves the credibility. Third, LLM can mark\nits missing knowledge in CoQ and IR can provide this knowledge to LLM. These\nthree operations improve the accuracy of LLM for complex knowledge-intensive\ntasks in terms of reasoning ability and knowledge. Finally, SearChain generates\nthe reasoning process and marks references to supporting documents for each\nreasoning step, which improves traceability. SearChain transforms the topology\nof reasoning from chain to tree, which can modify the reasoning direction.\nExperiment shows that SearChain outperforms baselines on complex\nknowledge-intensive tasks including multi-hop question-answering, slot filling,\nfact checking, and long-form question-answering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shicheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment Perception Adversarial Attacks on Neural Machine Translation Systems. (arXiv:2305.01437v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01437","description":"<p>With the advent of deep learning methods, Neural Machine Translation (NMT)\nsystems have become increasingly powerful. However, deep learning based systems\nare susceptible to adversarial attacks, where imperceptible changes to the\ninput can cause undesirable changes at the output of the system. To date there\nhas been little work investigating adversarial attacks on sequence-to-sequence\nsystems, such as NMT models. Previous work in NMT has examined attacks with the\naim of introducing target phrases in the output sequence. In this work,\nadversarial attacks for NMT systems are explored from an output perception\nperspective. Thus the aim of an attack is to change the perception of the\noutput sequence, without altering the perception of the input sequence. For\nexample, an adversary may distort the sentiment of translated reviews to have\nan exaggerated positive sentiment. In practice it is challenging to run\nextensive human perception experiments, so a proxy deep-learning classifier\napplied to the NMT output is used to measure perception changes. Experiments\ndemonstrate that the sentiment perception of NMT systems' output sequences can\nbe changed significantly with small imperceptible changes to input sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1\">Vyas Raina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark Gales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Classification: Financial Reasoning in State-of-the-Art Language Models. (arXiv:2305.01505v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01505","description":"<p>Large Language Models (LLMs), consisting of 100 billion or more parameters,\nhave demonstrated remarkable ability in complex multi-step reasoning tasks.\nHowever, the application of such generic advancements has been limited to a few\nfields, such as clinical or legal, with the field of financial reasoning\nremaining largely unexplored. To the best of our knowledge, the ability of LLMs\nto solve financial reasoning problems has never been dealt with, and whether it\ncan be performed at any scale remains unknown. To address this knowledge gap,\nthis research presents a comprehensive investigation into the potential\napplication of LLMs in the financial domain. The investigation includes a\ndetailed exploration of a range of subjects, including task formulation,\nsynthetic data generation, prompting methods, and evaluation capability.\nFurthermore, the study benchmarks various GPT variants with parameter scales\nranging from 2.8B to 13B, with and without instruction tuning, on diverse\ndataset sizes. By analyzing the results, we reveal that the ability to generate\ncoherent financial reasoning first emerges at 6B parameters, and continues to\nimprove with better instruction-tuning or larger datasets. Additionally, the\nstudy provides a publicly accessible dataset named sFIOG (Synthetic-Financial\nInvestment Opinion Generation), consisting of 11,802 synthetic investment\nthesis samples, to support further research in the field of financial\nreasoning. Overall, this research seeks to contribute to the understanding of\nthe efficacy of language models in the field of finance, with a particular\nemphasis on their ability to engage in sophisticated reasoning and analysis\nwithin the context of investment decision-making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Son_G/0/1/0/all/0/1\">Guijin Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">Hanearl Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahm_M/0/1/0/all/0/1\">Moonjeong Hahm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_K/0/1/0/all/0/1\">Keonju Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Sol Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Taxonomy of Foundation Model based Systems for Responsible-AI-by-Design. (arXiv:2305.05352v4 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2305.05352","description":"<p>The recent release of large language model (LLM) based chatbots, such as\nChatGPT, has attracted significant attention on foundation models. It is widely\nbelieved that foundation models will serve as the fundamental building blocks\nfor future AI systems. As foundation models are in their early stages, the\ndesign of foundation model based systems has not yet been systematically\nexplored. There is little understanding about the impact of introducing\nfoundation models in software architecture. Therefore, in this paper, we\npropose a taxonomy of foundation model based systems, which classifies and\ncompares the characteristics of foundation models and design options of\nfoundation model based systems. Our taxonomy comprises three categories:\nfoundation model pretraining and fine-tuning, architecture design of foundation\nmodel based systems, and responsible-AI-by-design. This taxonomy provides\nconcrete guidance for making major design decisions when designing foundation\nmodel based systems and highlights trade-offs arising from design decisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qinghua Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Liming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1\">Zhenchang Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whittle_J/0/1/0/all/0/1\">Jon Whittle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment. (arXiv:2305.05940v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.05940","description":"<p>In-context learning (ICL) unfolds as large language models become capable of\ninferring test labels conditioned on a few labeled samples without any gradient\nupdate. ICL-enabled large language models provide a promising step forward\ntoward bypassing recurrent annotation costs in a low-resource setting. Yet,\nonly a handful of past studies have explored ICL in a cross-lingual setting, in\nwhich the need for transferring label-knowledge from a high-resource language\nto a low-resource one is immensely crucial. To bridge the gap, we provide the\nfirst in-depth analysis of ICL for cross-lingual text classification. We find\nthat the prevalent mode of selecting random input-label pairs to construct the\nprompt-context is severely limited in the case of cross-lingual ICL, primarily\ndue to the lack of alignment in the input as well as the output spaces. To\nmitigate this, we propose a novel prompt construction strategy -- Cross-lingual\nIn-context Source-Target Alignment (X-InSTA). With an injected coherence in the\nsemantics of the input examples and a task-based alignment across the source\nand target languages, X-InSTA is able to outperform random prompt selection by\na large margin across three different tasks using 44 different cross-lingual\npairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tanwar_E/0/1/0/all/0/1\">Eshaan Tanwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Subhabrata Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borthakur_M/0/1/0/all/0/1\">Manish Borthakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Creative Data Generation: A Review Focusing on Text and Poetry. (arXiv:2305.08493v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08493","description":"<p>The rapid advancement in machine learning has led to a surge in automatic\ndata generation, making it increasingly challenging to differentiate between\nnaturally or human-generated data and machine-generated data. Despite these\nadvancements, the generation of creative data remains a challenge. This paper\naims to investigate and comprehend the essence of creativity, both in general\nand within the context of natural language generation. We review various\napproaches to creative writing devices and tasks, with a specific focus on the\ngeneration of poetry. We aim to shed light on the challenges and opportunities\nin the field of creative data generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elzohbi_M/0/1/0/all/0/1\">Mohamad Elzohbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Richard Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models. (arXiv:2305.10276v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10276","description":"<p>In this paper, we take the initiative to investigate the performance of LLMs\non complex planning tasks that require LLMs to understand a virtual spatial\nenvironment simulated via natural language and act correspondingly in text. We\npropose a benchmark named Natural Language Planning and Action (Natala)\ncomposed of a set of novel tasks: Brick World, NLVR-based Manipulations, and\nNatural Language Navigation. We found that current popular LLMs such as ChatGPT\nstill lack abilities in complex planning. This arises a question -- do the LLMs\nhave a good understanding of the environments described in natural language, or\nmaybe other alternatives such as symbolic representations are neater and hence\nbetter to be understood by LLMs? To this end, we propose a novel method called\nCoS (Chain-of-Symbol Prompting) that represents the complex environments with\ncondensed symbolic spatial representations during the chained intermediate\nthinking steps. CoS is easy to use and does not need additional training on\nLLMs. Extensive experiments indicate that CoS clearly surpasses the performance\nof the Chain-of-Thought (CoT) Prompting in all three planning tasks with even\nfewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT.\nThe performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%)\non Brick World for ChatGPT. CoS also reduces the number of tokens in the prompt\nobviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate\nsteps from demonstrations on Brick World. Code and data available at:\nhttps://github.com/hanxuhu/chain-of-symbol-planning\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hanxu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hongyuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huajian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network. (arXiv:2305.12493v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2305.12493","description":"<p>Contextual information plays a crucial role in speech recognition\ntechnologies and incorporating it into the end-to-end speech recognition models\nhas drawn immense interest recently. However, previous deep bias methods lacked\nexplicit supervision for bias tasks. In this study, we introduce a contextual\nphrase prediction network for an attention-based deep bias method. This network\npredicts context phrases in utterances using contextual embeddings and\ncalculates bias loss to assist in the training of the contextualized model. Our\nmethod achieved a significant word error rate (WER) reduction across various\nend-to-end speech recognition models. Experiments on the LibriSpeech corpus\nshow that our proposed model obtains a 12.1% relative WER improvement over the\nbaseline model, and the WER of the context phrases decreases relatively by\n40.5%. Moreover, by applying a context phrase filtering strategy, we also\neffectively eliminate the WER degradation when using a larger biasing list.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1\">Kaixun Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1\">Zhanheng Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_P/0/1/0/all/0/1\">Pengcheng Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mu_B/0/1/0/all/0/1\">Bingshen Mu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_T/0/1/0/all/0/1\">Tianyi Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoupled Rationalization with Asymmetric Learning Rates: A Flexible Lipschitz Restraint. (arXiv:2305.13599v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.13599","description":"<p>A self-explaining rationalization model is generally constructed by a\ncooperative game where a generator selects the most human-intelligible pieces\nfrom the input text as rationales, followed by a predictor that makes\npredictions based on the selected rationales. However, such a cooperative game\nmay incur the degeneration problem where the predictor overfits to the\nuninformative pieces generated by a not yet well-trained generator and in turn,\nleads the generator to converge to a sub-optimal model that tends to select\nsenseless pieces. In this paper, we theoretically bridge degeneration with the\npredictor's Lipschitz continuity. Then, we empirically propose a simple but\neffective method named DR, which can naturally and flexibly restrain the\nLipschitz constant of the predictor, to address the problem of degeneration.\nThe main idea of DR is to decouple the generator and predictor to allocate them\nwith asymmetric learning rates. A series of experiments conducted on two widely\nused benchmarks have verified the effectiveness of the proposed method. Codes:\n\\href{https://github.com/jugechengzi/Rationalization-DR}{https://github.com/jugechengzi/Rationalization-DR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haozhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">YuanKai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jie Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yixiong Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAIL: Search-Augmented Instruction Learning. (arXiv:2305.15225v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.15225","description":"<p>Large language models (LLMs) have been significantly improved by instruction\nfine-tuning, but still lack transparency and the ability to utilize up-to-date\nknowledge and information. In this work, we propose search-augmented\ninstruction learning (SAIL), which grounds the language generation and\ninstruction following abilities on complex search results generated by in-house\nand external search engines. With an instruction tuning corpus, we collect\nsearch results for each training case from different search APIs and domains,\nand construct a new search-grounded training set containing\n\\textit{(instruction, grounding information, response)} triplets. We then\nfine-tune the LLaMA-7B model on the constructed training set. Since the\ncollected results contain unrelated and disputing languages, the model needs to\nlearn to ground on trustworthy search results, filter out distracting passages,\nand generate the target response. The search result-denoising process entails\nexplicit trustworthy information selection and multi-hop reasoning, since the\nretrieved passages might be informative but not contain the\ninstruction-following answer. Experiments show that the fine-tuned SAIL-7B\nmodel has a strong instruction-following ability, and it performs significantly\nbetter on transparency-sensitive tasks, including open-ended question answering\nand fact checking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hongyin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yung-Sung Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yuan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianhua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xixin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Danny Fox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models are Bounded Pragmatic Speakers. (arXiv:2305.17760v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17760","description":"<p>How do language models \"think\"? This paper formulates a probabilistic\ncognitive model called the bounded pragmatic speaker, which can characterize\nthe operation of different variations of language models. Specifically, we\ndemonstrate that large language models fine-tuned with reinforcement learning\nfrom human feedback (Ouyang et al., 2022) embody a model of thought that\nconceptually resembles a fast-and-slow model (Kahneman, 2011), which\npsychologists have attributed to humans. We discuss the limitations of\nreinforcement learning from human feedback as a fast-and-slow model of thought\nand propose avenues for expanding this framework. In essence, our research\nhighlights the value of adopting a cognitive probabilistic modeling approach to\ngain insights into the comprehension, evaluation, and advancement of language\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khanh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MemeGraphs: Linking Memes to Knowledge Graphs. (arXiv:2305.18391v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.18391","description":"<p>Memes are a popular form of communicating trends and ideas in social media\nand on the internet in general, combining the modalities of images and text.\nThey can express humor and sarcasm but can also have offensive content.\nAnalyzing and classifying memes automatically is challenging since their\ninterpretation relies on the understanding of visual elements, language, and\nbackground knowledge. Thus, it is important to meaningfully represent these\nsources and the interaction between them in order to classify a meme as a\nwhole. In this work, we propose to use scene graphs, that express images in\nterms of objects and their visual relations, and knowledge graphs as structured\nrepresentations for meme classification with a Transformer-based architecture.\nWe compare our approach with ImgBERT, a multimodal model that uses only learned\n(instead of structured) representations of the meme, and observe consistent\nimprovements. We further provide a dataset with human graph annotations that we\ncompare to automatically generated graphs and entity linking. Analysis shows\nthat automatic methods link more entities than human annotators and that\nautomatically generated graphs are better suited for hatefulness classification\nin memes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kougia_V/0/1/0/all/0/1\">Vasiliki Kougia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fetzel_S/0/1/0/all/0/1\">Simon Fetzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchmair_T/0/1/0/all/0/1\">Thomas Kirchmair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cano_E/0/1/0/all/0/1\">Erion &#xc7;ano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baharlou_S/0/1/0/all/0/1\">Sina Moayed Baharlou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharifzadeh_S/0/1/0/all/0/1\">Sahand Sharifzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1\">Benjamin Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning Approach for Cancer Entities Association and Classification. (arXiv:2306.00013v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.00013","description":"<p>According to the World Health Organization (WHO), cancer is the second\nleading cause of death globally. Scientific research on different types of\ncancers grows at an ever-increasing rate, publishing large volumes of research\narticles every year. The insight information and the knowledge of the drug,\ndiagnostics, risk, symptoms, treatments, etc., related to genes are significant\nfactors that help explore and advance the cancer research progression. Manual\nscreening of such a large volume of articles is very laborious and\ntime-consuming to formulate any hypothesis. The study uses the two most\nnon-trivial NLP, Natural Language Processing functions, Entity Recognition, and\ntext classification to discover knowledge from biomedical literature. Named\nEntity Recognition (NER) recognizes and extracts the predefined entities\nrelated to cancer from unstructured text with the support of a user-friendly\ninterface and built-in dictionaries. Text classification helps to explore the\ninsights into the text and simplifies data categorization, querying, and\narticle screening. Machine learning classifiers are also used to build the\nclassification model and Structured Query Languages (SQL) is used to identify\nthe hidden relations that may lead to significant predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeyakodi_G/0/1/0/all/0/1\">G. Jeyakodi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_A/0/1/0/all/0/1\">Arkadeep Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Debapratim Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarukeswari_K/0/1/0/all/0/1\">K. Sarukeswari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amouda_V/0/1/0/all/0/1\">V. Amouda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL. (arXiv:2306.00739v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.00739","description":"<p>One impressive emergent capability of large language models (LLMs) is\ngeneration of code, including Structured Query Language (SQL) for databases.\nFor the task of converting natural language text to SQL queries, Text-to-SQL,\nadaptation of LLMs is of paramount importance, both in in-context learning and\nfine-tuning settings, depending on the amount of adaptation data used. In this\npaper, we propose an LLM-based Text-to-SQL model SQL-PaLM, leveraging on\nPaLM-2, that pushes the state-of-the-art in both settings. Few-shot SQL-PaLM is\nbased on an execution-based self-consistency prompting approach designed for\nText-to-SQL, and achieves 77.3% in test-suite accuracy on Spider, which to our\nbest knowledge is the first to outperform previous state-of-the-art with\nfine-tuning by a significant margin, 4%. Furthermore, we demonstrate that the\nfine-tuned SQL-PALM outperforms it further by another 1%. Towards applying\nSQL-PaLM to real-world scenarios we further evaluate its robustness on other\nchallenging variants of Spider and demonstrate the superior generalization\ncapability of SQL-PaLM. In addition, via extensive case studies, we demonstrate\nthe impressive intelligent capabilities and various success enablers of\nLLM-based Text-to-SQL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Ruoxi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arik_S/0/1/0/all/0/1\">Sercan O. Arik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakhost_H/0/1/0/all/0/1\">Hootan Nakhost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hanjun Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_R/0/1/0/all/0/1\">Rajarishi Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Pengcheng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Evidence-based Instructional Design Expertise through Large Language Models. (arXiv:2306.01006v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.01006","description":"<p>This paper presents a comprehensive exploration of leveraging Large Language\nModels (LLMs), specifically GPT-4, in the field of instructional design. With a\nfocus on scaling evidence-based instructional design expertise, our research\naims to bridge the gap between theoretical educational studies and practical\nimplementation. We discuss the benefits and limitations of AI-driven content\ngeneration, emphasizing the necessity of human oversight in ensuring the\nquality of educational materials. This work is elucidated through two detailed\ncase studies where we applied GPT-4 in creating complex higher-order\nassessments and active learning components for different courses. From our\nexperiences, we provide best practices for effectively using LLMs in\ninstructional design tasks, such as utilizing templates, fine-tuning, handling\nunexpected output, implementing LLM chains, citing references, evaluating\noutput, creating rubrics, grading, and generating distractors. We also share\nour vision of a future recommendation system, where a customized GPT-4 extracts\ninstructional design principles from educational studies and creates\npersonalized, evidence-supported strategies for users' unique educational\ncontexts. Our research contributes to understanding and optimally harnessing\nthe potential of AI-driven language models in enhancing educational outcomes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_G/0/1/0/all/0/1\">Gautam Yadav</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT Informed Graph Neural Network for Stock Movement Prediction. (arXiv:2306.03763v3 [q-fin.ST] UPDATED)","link":"http://arxiv.org/abs/2306.03763","description":"<p>ChatGPT has demonstrated remarkable capabilities across various natural\nlanguage processing (NLP) tasks. However, its potential for inferring dynamic\nnetwork structures from temporal textual data, specifically financial news,\nremains an unexplored frontier. In this research, we introduce a novel\nframework that leverages ChatGPT's graph inference capabilities to enhance\nGraph Neural Networks (GNN). Our framework adeptly extracts evolving network\nstructures from textual data, and incorporates these networks into graph neural\nnetworks for subsequent predictive tasks. The experimental results from stock\nmovement forecasting indicate our model has consistently outperformed the\nstate-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios\nconstructed based on our model's outputs demonstrate higher annualized\ncumulative returns, alongside reduced volatility and maximum drawdown. This\nsuperior performance highlights the potential of ChatGPT for text-based network\ninferences and underscores its promising implications for the financial sector.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Chen_Z/0/1/0/all/0/1\">Zihan Chen</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zheng_L/0/1/0/all/0/1\">Lei Nico Zheng</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Lu_C/0/1/0/all/0/1\">Cheng Lu</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Yuan_J/0/1/0/all/0/1\">Jialu Yuan</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Zhu_D/0/1/0/all/0/1\">Di Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Hybrid Linguistic Features for Turkish Text Readability. (arXiv:2306.03774v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.03774","description":"<p>This paper presents the first comprehensive study on automatic readability\nassessment of Turkish texts. We combine state-of-the-art neural network models\nwith linguistic features at lexical, morphosyntactic, syntactic and discourse\nlevels to develop an advanced readability tool. We evaluate the effectiveness\nof traditional readability formulas compared to modern automated methods and\nidentify key linguistic features that determine the readability of Turkish\ntexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uluslu_A/0/1/0/all/0/1\">Ahmet Yavuz Uluslu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_G/0/1/0/all/0/1\">Gerold Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MOFI: Learning Image Representations from Noisy Entity Annotated Images. (arXiv:2306.07952v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2306.07952","description":"<p>We present MOFI, a new vision foundation model designed to learn image\nrepresentations from noisy entity annotated images. MOFI differs from previous\nwork in two key aspects: ($i$) pre-training data, and ($ii$) training recipe.\nRegarding data, we introduce a new approach to automatically assign entity\nlabels to images from noisy image-text pairs. Our approach involves employing a\nnamed entity recognition model to extract entities from the alt-text, and then\nusing a CLIP model to select the correct entities as labels of the paired\nimage. The approach is simple, does not require costly human annotation, and\ncan be readily scaled up to billions of image-text pairs mined from the web.\nThrough this method, we have created Image-to-Entities (I2E), a new large-scale\ndataset with 1 billion images and 2 million distinct entities, covering rich\nvisual concepts in the wild. Building upon the I2E dataset, we study different\ntraining recipes, including supervised pre-training, contrastive pre-training,\nand multi-task learning. For constrastive pre-training, we treat entity names\nas free-form text, and further enrich them with entity descriptions.\nExperiments show that supervised pre-training with large-scale fine-grained\nentity labels is highly effective for image retrieval tasks, and multi-task\ntraining further improves the performance. The final MOFI model achieves 86.66%\nmAP on the challenging GPR1200 dataset, surpassing the previous\nstate-of-the-art performance of 72.19% from OpenAI's CLIP model. Further\nexperiments on zero-shot and linear probe image classification also show that\nMOFI outperforms a CLIP model trained on the original image-text data,\ndemonstrating the effectiveness of the I2E dataset in learning strong image\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wentao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofeev_A/0/1/0/all/0/1\">Aleksei Timofeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_K/0/1/0/all/0/1\">Kun Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuangning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yantao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1\">Jon Shlens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xianzhi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models. (arXiv:2306.08997v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.08997","description":"<p>We curate a comprehensive dataset of 4,550 questions and solutions from\nproblem sets, midterm exams, and final exams across all MIT Mathematics and\nElectrical Engineering and Computer Science (EECS) courses required for\nobtaining a degree. We evaluate the ability of large language models to fulfill\nthe graduation requirements for any MIT major in Mathematics and EECS. Our\nresults demonstrate that GPT-3.5 successfully solves a third of the entire MIT\ncurriculum, while GPT-4, with prompt engineering, achieves a perfect solve rate\non a test set excluding questions based on images. We fine-tune an open-source\nlarge language model on this dataset. We employ GPT-4 to automatically grade\nmodel responses, providing a detailed performance breakdown by course,\nquestion, and answer type. By embedding questions in a low-dimensional space,\nwe explore the relationships between questions, topics, and classes and\ndiscover which questions and classes are required for solving other questions\nand classes through few-shot learning. Our analysis offers valuable insights\ninto course prerequisites and curriculum design, highlighting language models'\npotential for learning and improving Mathematics and EECS education.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sarah J. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florin_S/0/1/0/all/0/1\">Samuel Florin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Ariel N. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niknafs_E/0/1/0/all/0/1\">Eamon Niknafs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marginean_A/0/1/0/all/0/1\">Andrei Marginean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Annie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyser_K/0/1/0/all/0/1\">Keith Tyser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_Z/0/1/0/all/0/1\">Zad Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hicke_Y/0/1/0/all/0/1\">Yann Hicke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Nikhil Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Udell_M/0/1/0/all/0/1\">Madeleine Udell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buonassisi_T/0/1/0/all/0/1\">Tonio Buonassisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solar_Lezama_A/0/1/0/all/0/1\">Armando Solar-Lezama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1\">Iddo Drori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models. (arXiv:2306.09869v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2306.09869","description":"<p>Despite the remarkable performance of text-to-image diffusion models in image\ngeneration tasks, recent studies have raised the issue that generated images\nsometimes cannot capture the intended semantic contents of the text prompts,\nwhich phenomenon is often called semantic misalignment. To address this, here\nwe present a novel energy-based model (EBM) framework. Specifically, we first\nformulate EBMs of latent image representations and text embeddings in each\ncross-attention layer of the denoising autoencoder. Then, we obtain the\ngradient of the log posterior of context vectors, which can be updated and\ntransferred to the subsequent cross-attention layer, thereby implicitly\nminimizing a nested hierarchy of energy functions. Our latent EBMs further\nallow zero-shot compositional generation as a linear combination of\ncross-attention outputs from different contexts. Using extensive experiments,\nwe demonstrate that the proposed method is highly effective in handling various\nimage generation tasks, including multi-concept generation, text-guided image\ninpainting, and real and synthetic image editing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1\">Geon Yeong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jeongsol Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Beomsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang Wan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation. (arXiv:2306.11222v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2306.11222","description":"<p>Transformer models have achieved remarkable results in various natural\nlanguage tasks, but they are often prohibitively large, requiring massive\nmemories and computational resources. To reduce the size and complexity of\nthese models, we propose LoSparse (Low-Rank and Sparse approximation), a novel\nmodel compression technique that approximates a weight matrix by the sum of a\nlow-rank matrix and a sparse matrix. Our method combines the advantages of both\nlow-rank approximations and pruning, while avoiding their limitations. Low-rank\napproximation compresses the coherent and expressive parts in neurons, while\npruning removes the incoherent and non-expressive parts in neurons. Pruning\nenhances the diversity of low-rank approximations, and low-rank approximation\nprevents pruning from losing too many expressive neurons. We evaluate our\nmethod on natural language understanding, question answering, and natural\nlanguage generation tasks. We show that it significantly outperforms existing\ncompression methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yifan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A novel Counterfactual method for aspect-based sentiment analysis. (arXiv:2306.11260v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.11260","description":"<p>Aspect-based-sentiment-analysis (ABSA) is a fine-grained sentiment evaluation\ntask, which analyze the emotional polarity of the evaluation aspects. However,\nprevious works only focus on the identification of opinion expressions, forget\nthat the diversity of opinion expressions also has great impacts on the ABSA\ntask. To mitigate this problem, we propose a novel counterfactual data\naugmentation method to generate opinion expression with reversed sentiment\npolarity. Specially, the integrated gradients are calculated to identify and\nmask the opinion expression. Then, a prompt with the reverse label is combined\nto the original text, and a pre-trained language model (PLM), T5, is finally\nemployed to retrieve the masks. The experimental results show the proposed\ncounterfactual data augmentation method perform better than current\naugmentation methods on three ABSA datasets, i.e. Laptop, Restaurant and MAMS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dongming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lulu Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhaoshu Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explicit Syntactic Guidance for Neural Text Generation. (arXiv:2306.11485v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.11485","description":"<p>Most existing text generation models follow the sequence-to-sequence\nparadigm. Generative Grammar suggests that humans generate natural language\ntexts by learning language grammar. We propose a syntax-guided generation\nschema, which generates the sequence guided by a constituency parse tree in a\ntop-down direction. The decoding process can be decomposed into two parts: (1)\npredicting the infilling texts for each constituent in the lexicalized syntax\ncontext given the source sentence; (2) mapping and expanding each constituent\nto construct the next-level syntax context. Accordingly, we propose a\nstructural beam search method to find possible syntax structures\nhierarchically. Experiments on paraphrase generation and machine translation\nshow that the proposed method outperforms autoregressive baselines, while also\ndemonstrating effectiveness in terms of interpretability, controllability, and\ndiversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yafu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Leyang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jianhao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yongjing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1\">Wei Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Effective and Compact Contextual Representation for Conformer Transducer Speech Recognition Systems. (arXiv:2306.13307v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2306.13307","description":"<p>Current ASR systems are mainly trained and evaluated at the utterance level.\nLong range cross utterance context can be incorporated. A key task is to derive\na suitable compact representation of the most relevant history contexts. In\ncontrast to previous researches based on either LSTM-RNN encoded histories that\nattenuate the information from longer range contexts, or frame level\nconcatenation of transformer context embeddings, in this paper compact\nlow-dimensional cross utterance contextual features are learned in the\nConformer-Transducer Encoder using specially designed attention pooling layers\nthat are applied over efficiently cached preceding utterances history vectors.\nExperiments on the 1000-hr Gigaspeech corpus demonstrate that the proposed\ncontextualized streaming Conformer-Transducers outperform the baseline using\nutterance internal context only with statistically significant WER reductions\nof 0.7% to 0.5% absolute (4.3% to 3.1% relative) on the dev and test data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cui_M/0/1/0/all/0/1\">Mingyu Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kang_J/0/1/0/all/0/1\">Jiawen Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_J/0/1/0/all/0/1\">Jiajun Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yin_X/0/1/0/all/0/1\">Xi Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_Y/0/1/0/all/0/1\">Yutao Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xie Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xunying Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-06-26T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}}]}]}