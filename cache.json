{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-11-14T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"MatNexus: A Comprehensive Text Mining and Analysis Suite for Materials Discover. (arXiv:2311.06303v1 [cond-mat.mtrl-sci])","link":"http://arxiv.org/abs/2311.06303","description":"<p>MatNexus is a specialized software for the automated collection, processing,\nand analysis of text from scientific articles. Through an integrated suite of\nmodules, the MatNexus facilitates the retrieval of scientific articles,\nprocesses textual data for insights, generates vector representations suitable\nfor machine learning, and offers visualization capabilities for word\nembeddings. With the vast volume of scientific publications, MatNexus stands\nout as an end-to-end tool for researchers aiming to gain insights from\nscientific literature in material science, making the exploration of materials,\nsuch as the electrocatalyst examples we show here, efficient and insightful.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cond-mat/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Stricker_M/0/1/0/all/0/1\">Markus Stricker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion. (arXiv:2311.06318v1 [cs.IR])","link":"http://arxiv.org/abs/2311.06318","description":"<p>Large Language Models (LLMs) excel at tackling various natural language\ntasks. However, due to the significant costs involved in re-training or\nfine-tuning them, they remain largely static and difficult to personalize.\nNevertheless, a variety of applications could benefit from generations that are\ntailored to users' preferences, goals, and knowledge. Among them is web search,\nwhere knowing what a user is trying to accomplish, what they care about, and\nwhat they know can lead to improved search experiences. In this work, we\npropose a novel and general approach that augments an LLM with relevant context\nfrom users' interaction histories with a search engine in order to personalize\nits outputs. Specifically, we construct an entity-centric knowledge store for\neach user based on their search and browsing activities on the web, which is\nthen leveraged to provide contextually relevant LLM prompt augmentations. This\nknowledge store is light-weight, since it only produces user-specific aggregate\nprojections of interests and knowledge onto public knowledge graphs, and\nleverages existing search log infrastructure, thereby mitigating the privacy,\ncompliance, and scalability concerns associated with building deep user\nprofiles for personalization. We then validate our approach on the task of\ncontextual query suggestion, which requires understanding not only the user's\ncurrent search context but also what they historically know and care about.\nThrough a number of experiments based on human evaluation, we show that our\napproach is significantly better than several other LLM-powered baselines,\ngenerating query suggestions that are contextually more relevant, personalized,\nand useful.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1\">Jinheon Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_N/0/1/0/all/0/1\">Nirupama Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucerzan_S/0/1/0/all/0/1\">Silviu Cucerzan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+herring_A/0/1/0/all/0/1\">Allen herring</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jauhar_S/0/1/0/all/0/1\">Sujay Kumar Jauhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of AI Text-to-Image and AI Text-to-Video Generators. (arXiv:2311.06329v1 [cs.CV])","link":"http://arxiv.org/abs/2311.06329","description":"<p>Text-to-Image and Text-to-Video AI generation models are revolutionary\ntechnologies that use deep learning and natural language processing (NLP)\ntechniques to create images and videos from textual descriptions. This paper\ninvestigates cutting-edge approaches in the discipline of Text-to-Image and\nText-to-Video AI generations. The survey provides an overview of the existing\nliterature as well as an analysis of the approaches used in various studies. It\ncovers data preprocessing techniques, neural network types, and evaluation\nmetrics used in the field. In addition, the paper discusses the challenges and\nlimitations of Text-to-Image and Text-to-Video AI generations, as well as\nfuture research directions. Overall, these models have promising potential for\na wide range of applications such as video production, content creation, and\ndigital marketing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aditi Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Schema Graph-Guided Prompt for Multi-Domain Dialogue State Tracking. (arXiv:2311.06345v1 [cs.CL])","link":"http://arxiv.org/abs/2311.06345","description":"<p>Tracking dialogue states is an essential topic in task-oriented dialogue\nsystems, which involve filling in the necessary information in pre-defined\nslots corresponding to a schema. While general pre-trained language models have\nbeen shown effective in slot-filling, their performance is limited when applied\nto specific domains. We propose a graph-based framework that learns\ndomain-specific prompts by incorporating the dialogue schema. Specifically, we\nembed domain-specific schema encoded by a graph neural network into the\npre-trained language model, which allows for relations in the schema to guide\nthe model for better adaptation to the specific domain. Our experiments\ndemonstrate that the proposed graph-based method outperforms other multi-domain\nDST approaches while using similar or fewer trainable parameters. We also\nconduct a comprehensive study of schema graph architectures, parameter usage,\nand module ablation that demonstrate the effectiveness of our model on\nmulti-domain dialogue state tracking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_R/0/1/0/all/0/1\">Ruolin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Ting-Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juang_B/0/1/0/all/0/1\">Biing-Hwang Juang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Definitions from Large Language Models. (arXiv:2311.06362v1 [cs.CL])","link":"http://arxiv.org/abs/2311.06362","description":"<p>Dictionary definitions are historically the arbitrator of what words mean,\nbut this primacy has come under threat by recent progress in NLP, including\nword embeddings and generative models like ChatGPT. We present an exploratory\nstudy of the degree of alignment between word definitions from classical\ndictionaries and these newer computational artifacts. Specifically, we compare\ndefinitions from three published dictionaries to those generated from variants\nof ChatGPT. We show that (i) definitions from different traditional\ndictionaries exhibit more surface form similarity than do model-generated\ndefinitions, (ii) that the ChatGPT definitions are highly accurate, comparable\nto traditional dictionaries, and (iii) ChatGPT-based embedding definitions\nretain their accuracy even on low frequency words, much better than GloVE and\nFastText word embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yunting Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skiena_S/0/1/0/all/0/1\">Steven Skiena</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation Extraction in underexplored biomedical domains: A diversity-optimised sampling and synthetic data generation approach. (arXiv:2311.06364v1 [cs.CL])","link":"http://arxiv.org/abs/2311.06364","description":"<p>The sparsity of labelled data is an obstacle to the development of Relation\nExtraction models and the completion of databases in various biomedical areas.\nWhile being of high interest in drug-discovery, the natural-products\nliterature, reporting the identification of potential bioactive compounds from\norganisms, is a concrete example of such an overlooked topic. To mark the start\nof this new task, we created the first curated evaluation dataset and extracted\nliterature items from the LOTUS database to build training sets. To this end,\nwe developed a new sampler inspired by diversity metrics in ecology, named\nGreedy Maximum Entropy sampler, or GME-sampler\n(https://github.com/idiap/gme-sampler). The strategic optimization of both\nbalance and diversity of the selected items in the evaluation set is important\ngiven the resource-intensive nature of manual curation. After quantifying the\nnoise in the training set, in the form of discrepancies between the input\nabstracts text and the expected output labels, we explored different strategies\naccordingly. Framing the task as an end-to-end Relation Extraction, we\nevaluated the performance of standard fine-tuning as a generative task and\nfew-shot learning with open Large Language Models (LLaMA 7B-65B). In addition\nto their evaluation in few-shot settings, we explore the potential of open\nLarge Language Models (Vicuna-13B) as synthetic data generator and propose a\nnew workflow for this purpose. All evaluated models exhibited substantial\nimprovements when fine-tuned on synthetic abstracts rather than the original\nnoisy data. We provide our best performing (f1-score=59.0) BioGPT-Large model\nfor end-to-end RE of natural-products relationships along with all the\ngenerated synthetic data and the evaluation dataset. See more details at\nhttps://github.com/idiap/abroad-re.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Delmas_M/0/1/0/all/0/1\">Maxime Delmas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wysocka_M/0/1/0/all/0/1\">Magdalena Wysocka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andr&#xe9; Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heaps' Law in GPT-Neo Large Language Model Emulated Corpora. (arXiv:2311.06377v1 [cs.CL])","link":"http://arxiv.org/abs/2311.06377","description":"<p>Heaps' law is an empirical relation in text analysis that predicts vocabulary\ngrowth as a function of corpus size. While this law has been validated in\ndiverse human-authored text corpora, its applicability to large language model\ngenerated text remains unexplored. This study addresses this gap, focusing on\nthe emulation of corpora using the suite of GPT-Neo large language models. To\nconduct our investigation, we emulated corpora of PubMed abstracts using three\ndifferent parameter sizes of the GPT-Neo model. Our emulation strategy involved\nusing the initial five words of each PubMed abstract as a prompt and\ninstructing the model to expand the content up to the original abstract's\nlength. Our findings indicate that the generated corpora adhere to Heaps' law.\nInterestingly, as the GPT-Neo model size grows, its generated vocabulary\nincreasingly adheres to Heaps' law as as observed in human-authored text. To\nfurther improve the richness and authenticity of GPT-Neo outputs, future\niterations could emphasize enhancing model size or refining the model\narchitecture to curtail vocabulary repetition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_U/0/1/0/all/0/1\">Uyen Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Randhawa_G/0/1/0/all/0/1\">Gurjit S. Randhawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheridan_P/0/1/0/all/0/1\">Paul Sheridan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeMuX: Data-efficient Multilingual Learning. (arXiv:2311.06379v1 [cs.CL])","link":"http://arxiv.org/abs/2311.06379","description":"<p>We consider the task of optimally fine-tuning pre-trained multilingual\nmodels, given small amounts of unlabelled target data and an annotation budget.\nIn this paper, we introduce DEMUX, a framework that prescribes the exact\ndata-points to label from vast amounts of unlabelled multilingual data, having\nunknown degrees of overlap with the target set. Unlike most prior works, our\nend-to-end framework is language-agnostic, accounts for model representations,\nand supports multilingual target configurations. Our active learning strategies\nrely upon distance and uncertainty measures to select task-specific neighbors\nthat are most informative to label, given a model. DeMuX outperforms strong\nbaselines in 84% of the test cases, in the zero-shot setting of disjoint source\nand target language sets (including multilingual target pools), across three\nmodels and four tasks. Notably, in low-budget settings (5-100 examples), we\nobserve gains of up to 8-11 F1 points for token-level tasks, and 2-5 F1 for\ncomplex tasks. Our code is released here:\nhttps://github.com/simran-khanuja/demux.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khanuja_S/0/1/0/all/0/1\">Simran Khanuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gowriraj_S/0/1/0/all/0/1\">Srinivas Gowriraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dery_L/0/1/0/all/0/1\">Lucio Dery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Learning for Structured Pruning under Limited Task Data. (arXiv:2311.06382v1 [cs.CL])","link":"http://arxiv.org/abs/2311.06382","description":"<p>Large, pre-trained models are problematic to use in resource constrained\napplications. Fortunately, task-aware structured pruning methods offer a\nsolution. These approaches reduce model size by dropping structural units like\nlayers and attention heads in a manner that takes into account the end-task.\nHowever, these pruning algorithms require more task-specific data than is\ntypically available. We propose a framework which combines structured pruning\nwith transfer learning to reduce the need for task-specific data. Our empirical\nresults answer questions such as: How should the two tasks be coupled? What\nparameters should be transferred? And, when during training should transfer\nlearning be introduced? Leveraging these insights, we demonstrate that our\nframework results in pruned models with improved generalization over strong\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dery_L/0/1/0/all/0/1\">Lucio Dery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grangier_D/0/1/0/all/0/1\">David Grangier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hannun_A/0/1/0/all/0/1\">Awni Hannun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Large Language Models using Skill-Occupation Graph Context for HR-Related Tasks. (arXiv:2311.06383v1 [cs.CL])","link":"http://arxiv.org/abs/2311.06383","description":"<p>Numerous HR applications are centered around resumes and job descriptions.\nWhile they can benefit from advancements in NLP, particularly large language\nmodels, their real-world adoption faces challenges due to absence of\ncomprehensive benchmarks for various HR tasks, and lack of smaller models with\ncompetitive capabilities. In this paper, we aim to bridge this gap by\nintroducing the Resume-Job Description Benchmark (RJDB). We meticulously craft\nthis benchmark to cater to a wide array of HR tasks, including matching and\nexplaining resumes to job descriptions, extracting skills and experiences from\nresumes, and editing resumes. To create this benchmark, we propose to distill\ndomain-specific knowledge from a large language model (LLM). We rely on a\ncurated skill-occupation graph to ensure diversity and provide context for LLMs\ngeneration. Our benchmark includes over 50 thousand triples of job\ndescriptions, matched resumes and unmatched resumes. Using RJDB, we train\nmultiple smaller student models. Our experiments reveal that the student models\nachieve near/better performance than the teacher model (GPT-4), affirming the\neffectiveness of the benchmark. Additionally, we explore the utility of RJDB on\nout-of-distribution data for skill extraction and resume-job description\nmatching, in zero-shot and weak supervision manner. We release our datasets and\ncode to foster further research and industry applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pezeshkpour_P/0/1/0/all/0/1\">Pouya Pezeshkpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iso_H/0/1/0/all/0/1\">Hayate Iso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lake_T/0/1/0/all/0/1\">Thom Lake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhutani_N/0/1/0/all/0/1\">Nikita Bhutani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hruschka_E/0/1/0/all/0/1\">Estevam Hruschka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autoregressive Language Models For Estimating the Entropy of Epic EHR Audit Logs. (arXiv:2311.06401v1 [cs.CL])","link":"http://arxiv.org/abs/2311.06401","description":"<p>EHR audit logs are a highly granular stream of events that capture clinician\nactivities, and is a significant area of interest for research in\ncharacterizing clinician workflow on the electronic health record (EHR).\nExisting techniques to measure the complexity of workflow through EHR audit\nlogs (audit logs) involve time- or frequency-based cross-sectional aggregations\nthat are unable to capture the full complexity of a EHR session. We briefly\nevaluate the usage of transformer-based tabular language model (tabular LM) in\nmeasuring the entropy or disorderedness of action sequences within workflow and\nrelease the evaluated models publicly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Warner_B/0/1/0/all/0/1\">Benjamin C. Warner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannampallil_T/0/1/0/all/0/1\">Thomas Kannampallil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seunghwan Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Modular Approaches for Visual Question Decomposition. (arXiv:2311.06411v1 [cs.CV])","link":"http://arxiv.org/abs/2311.06411","description":"<p>Modular neural networks without additional training have recently been shown\nto surpass end-to-end neural networks on challenging vision-language tasks. The\nlatest such methods simultaneously introduce LLM-based code generation to build\nprograms and a number of skill-specific, task-oriented modules to execute them.\nIn this paper, we focus on ViperGPT and ask where its additional performance\ncomes from and how much is due to the (state-of-art, end-to-end) BLIP-2 model\nit subsumes vs. additional symbolic components. To do so, we conduct a\ncontrolled study (comparing end-to-end, modular, and prompting-based methods\nacross several VQA benchmarks). We find that ViperGPT's reported gains over\nBLIP-2 can be attributed to its selection of task-specific modules, and when we\nrun ViperGPT using a more task-agnostic selection of modules, these gains go\naway. Additionally, ViperGPT retains much of its performance if we make\nprominent alterations to its selection of modules: e.g. removing or retaining\nonly BLIP-2. Finally, we compare ViperGPT against a prompting-based\ndecomposition strategy and find that, on some benchmarks, modular approaches\nsignificantly benefit by representing subtasks with natural language, instead\nof code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1\">Apoorv Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graphs are not Created Equal: Exploring the Properties and Structure of Real KGs. (arXiv:2311.06414v1 [cs.LG])","link":"http://arxiv.org/abs/2311.06414","description":"<p>Despite the recent popularity of knowledge graph (KG) related tasks and\nbenchmarks such as KG embeddings, link prediction, entity alignment and\nevaluation of the reasoning abilities of pretrained language models as KGs, the\nstructure and properties of real KGs are not well studied. In this paper, we\nperform a large scale comparative study of 29 real KG datasets from diverse\ndomains such as the natural sciences, medicine, and NLP to analyze their\nproperties and structural patterns. Based on our findings, we make several\nrecommendations regarding KG-based model development and evaluation. We believe\nthat the rich structural information contained in KGs can benefit the\ndevelopment of better KG models across fields and we hope this study will\ncontribute to breaking the existing data silos between different areas of\nresearch (e.g., ML, NLP, AI for sciences).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teneva_N/0/1/0/all/0/1\">Nedelina Teneva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hruschka_E/0/1/0/all/0/1\">Estevam Hruschka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT Prompting Cannot Estimate Predictive Uncertainty in High-Resource Languages. (arXiv:2311.06427v1 [cs.CL])","link":"http://arxiv.org/abs/2311.06427","description":"<p>ChatGPT took the world by storm for its impressive abilities. Due to its\nrelease without documentation, scientists immediately attempted to identify its\nlimits, mainly through its performance in natural language processing (NLP)\ntasks. This paper aims to join the growing literature regarding ChatGPT's\nabilities by focusing on its performance in high-resource languages and on its\ncapacity to predict its answers' accuracy by giving a confidence level. The\nanalysis of high-resource languages is of interest as studies have shown that\nlow-resource languages perform worse than English in NLP tasks, but no study so\nfar has analysed whether high-resource languages perform as well as English.\nThe analysis of ChatGPT's confidence calibration has not been carried out\nbefore either and is critical to learn about ChatGPT's trustworthiness. In\norder to study these two aspects, five high-resource languages and two NLP\ntasks were chosen. ChatGPT was asked to perform both tasks in the five\nlanguages and to give a numerical confidence value for each answer. The results\nshow that all the selected high-resource languages perform similarly and that\nChatGPT does not have a good confidence calibration, often being overconfident\nand never giving low confidence values.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pelucchi_M/0/1/0/all/0/1\">Martino Pelucchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valdenegro_Toro_M/0/1/0/all/0/1\">Matias Valdenegro-Toro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Separating the Wheat from the Chaff with BREAD: An open-source benchmark and metrics to detect redundancy in text. (arXiv:2311.06440v1 [cs.CL])","link":"http://arxiv.org/abs/2311.06440","description":"<p>Data quality is a problem that perpetually resurfaces throughout the field of\nNLP, regardless of task, domain, or architecture, and remains especially severe\nfor lower-resource languages. A typical and insidious issue, affecting both\ntraining data and model output, is data that is repetitive and dominated by\nlinguistically uninteresting boilerplate, such as price catalogs or\ncomputer-generated log files. Though this problem permeates many web-scraped\ncorpora, there has yet to be a benchmark to test against, or a systematic study\nto find simple metrics that generalize across languages and agree with human\njudgements of data quality. In the present work, we create and release BREAD, a\nhuman-labeled benchmark on repetitive boilerplate vs. plausible linguistic\ncontent, spanning 360 languages. We release several baseline CRED (Character\nREDundancy) scores along with it, and evaluate their effectiveness on BREAD. We\nhope that the community will use this resource to develop better filtering\nmethods, and that our reference implementations of CRED scores can become\nstandard corpus evaluation tools, driving the development of cleaner language\nmodeling corpora, especially in low-resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Caswell_I/0/1/0/all/0/1\">Isaac Caswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lisa Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadimitriou_I/0/1/0/all/0/1\">Isabel Papadimitriou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"THOS: A Benchmark Dataset for Targeted Hate and Offensive Speech. (arXiv:2311.06446v1 [cs.CL])","link":"http://arxiv.org/abs/2311.06446","description":"<p>Detecting harmful content on social media, such as Twitter, is made difficult\nby the fact that the seemingly simple yes/no classification conceals a\nsignificant amount of complexity. Unfortunately, while several datasets have\nbeen collected for training classifiers in hate and offensive speech, there is\na scarcity of datasets labeled with a finer granularity of target classes and\nspecific targets. In this paper, we introduce THOS, a dataset of 8.3k tweets\nmanually labeled with fine-grained annotations about the target of the message.\nWe demonstrate that this dataset makes it feasible to train classifiers, based\non Large Language Models, to perform classification at this level of\ngranularity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Almohaimeed_S/0/1/0/all/0/1\">Saad Almohaimeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almohaimeed_S/0/1/0/all/0/1\">Saleh Almohaimeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafin_A/0/1/0/all/0/1\">Ashfaq Ali Shafin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carbunar_B/0/1/0/all/0/1\">Bogdan Carbunar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boloni_L/0/1/0/all/0/1\">Ladislau B&#xf6;l&#xf6;ni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DocGen: Generating Detailed Parameter Docstrings in Python. (arXiv:2311.06453v1 [cs.SE])","link":"http://arxiv.org/abs/2311.06453","description":"<p>Documentation debt hinders the effective utilization of open-source software.\nAlthough code summarization tools have been helpful for developers, most would\nprefer a detailed account of each parameter in a function rather than a\nhigh-level summary. However, generating such a summary is too intricate for a\nsingle generative model to produce reliably due to the lack of high-quality\ntraining data. Thus, we propose a multi-step approach that combines multiple\ntask-specific models, each adept at producing a specific section of a\ndocstring. The combination of these models ensures the inclusion of each\nsection in the final docstring. We compared the results from our approach with\nexisting generative models using both automatic metrics and a human-centred\nevaluation with 17 participating developers, which proves the superiority of\nour approach over existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venkatkrishna_V/0/1/0/all/0/1\">Vatsal Venkatkrishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagabushanam_D/0/1/0/all/0/1\">Durga Shree Nagabushanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_E/0/1/0/all/0/1\">Emmanuel Iko-Ojo Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fard_F/0/1/0/all/0/1\">Fatemeh H. Fard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidoni_M/0/1/0/all/0/1\">Melina Vidoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Codabux_Z/0/1/0/all/0/1\">Zadia Codabux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Language-based Mental Health Assessment with Item-Response Theory. (arXiv:2311.06467v1 [cs.CL])","link":"http://arxiv.org/abs/2311.06467","description":"<p>Mental health issues widely vary across individuals - the manifestations of\nsigns and symptoms can be fairly heterogeneous. Recently, language-based\ndepression and anxiety assessments have shown promise for capturing this\nheterogeneous nature by evaluating a patient's own language, but such\napproaches require a large sample of words per person to be accurate. In this\nwork, we introduce adaptive language-based assessment - the task of iteratively\nestimating an individual's psychological score based on limited language\nresponses to questions that the model also decides to ask. To this end, we\nexplore two statistical learning-based approaches for measurement/scoring:\nclassical test theory (CTT) and item response theory (IRT). We find that using\nadaptive testing in general can significantly reduce the number of questions\nrequired to achieve high validity (r ~ 0.7) with standardized tests, bringing\ndown from 11 total questions down to 3 for depression and 5 for anxiety. Given\nthe combinatorial nature of the problem, we empirically evaluate multiple\nstrategies for both the ordering and scoring objectives, introducing two new\nmethods: a semi-supervised item response theory based method (ALIRT), and a\nsupervised actor-critic based model. While both of the models achieve\nsignificant improvements over random and fixed orderings, we find ALIRT to be a\nscalable model that achieves the highest accuracy with lower numbers of\nquestions (e.g. achieves Pearson r ~ 0.93 after only 3 questions versus asking\nall 11 questions). Overall, ALIRT allows prompting a reduced number of\nquestions without compromising accuracy or overhead computational costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varadarajan_V/0/1/0/all/0/1\">Vasudha Varadarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sikstrom_S/0/1/0/all/0/1\">Sverker Sikstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kjell_O/0/1/0/all/0/1\">Oscar N.E. Kjell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_H/0/1/0/all/0/1\">H. Andrew Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L3 Ensembles: Lifelong Learning Approach for Ensemble of Foundational Language Models. (arXiv:2311.06493v1 [cs.CL])","link":"http://arxiv.org/abs/2311.06493","description":"<p>Fine-tuning pre-trained foundational language models (FLM) for specific tasks\nis often impractical, especially for resource-constrained devices. This\nnecessitates the development of a Lifelong Learning (L3) framework that\ncontinuously adapts to a stream of Natural Language Processing (NLP) tasks\nefficiently. We propose an approach that focuses on extracting meaningful\nrepresentations from unseen data, constructing a structured knowledge base, and\nimproving task performance incrementally. We conducted experiments on various\nNLP tasks to validate its effectiveness, including benchmarks like GLUE and\nSuperGLUE. We measured good performance across the accuracy, training\nefficiency, and knowledge transfer metrics. Initial experimental results show\nthat the proposed L3 ensemble method increases the model accuracy by 4% ~ 36%\ncompared to the fine-tuned FLM. Furthermore, L3 model outperforms naive\nfine-tuning approaches while maintaining competitive or superior performance\n(up to 15.4% increase in accuracy) compared to the state-of-the-art language\nmodel (T5) for the given task, STS benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shiri_A/0/1/0/all/0/1\">Aidin Shiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_M/0/1/0/all/0/1\">Manas Gaur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledgeable Preference Alignment for LLMs in Domain-specific Question Answering. (arXiv:2311.06503v1 [cs.CL])","link":"http://arxiv.org/abs/2311.06503","description":"<p>Recently, the development of large language models (LLMs) has attracted wide\nattention in academia and industry. Deploying LLMs to real scenarios is one of\nthe key directions in the current Internet industry. In this paper, we present\na novel pipeline to apply LLMs for domain-specific question answering (QA) that\nincorporates domain knowledge graphs (KGs), addressing an important direction\nof LLM application. As a real-world application, the content generated by LLMs\nshould be user-friendly to serve the customers. Additionally, the model needs\nto utilize domain knowledge properly to generate reliable answers. These two\nissues are the two major difficulties in the LLM application as vanilla\nfine-tuning can not adequately address them. We think both requirements can be\nunified as the model preference problem that needs to align with humans to\nachieve practical application. Thus, we introduce Knowledgeable Preference\nAlignmenT (KnowPAT), which constructs two kinds of preference set called style\npreference set and knowledge preference set respectively to tackle the two\nissues. Besides, we design a new alignment objective to align the LLM\npreference with human preference, aiming to train a better LLM for\nreal-scenario domain-specific QA to generate reliable and user-friendly\nanswers. Adequate experiments and comprehensive with 15 baseline methods\ndemonstrate that our KnowPAT is an outperforming pipeline for real-scenario\ndomain-specific QA with LLMs. Our code is open-source at\nhttps://github.com/zjukg/KnowPAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yanxi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fangming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Step by Step to Fairness: Attributing Societal Bias in Task-oriented Dialogue Systems. (arXiv:2311.06513v1 [cs.CL])","link":"http://arxiv.org/abs/2311.06513","description":"<p>Recent works have shown considerable improvements in task-oriented dialogue\n(TOD) systems by utilizing pretrained large language models (LLMs) in an\nend-to-end manner. However, the biased behavior of each component in a TOD\nsystem and the error propagation issue in the end-to-end framework can lead to\nseriously biased TOD responses. Existing works of fairness only focus on the\ntotal bias of a system. In this paper, we propose a diagnosis method to\nattribute bias to each component of a TOD system. With the proposed attribution\nmethod, we can gain a deeper understanding of the sources of bias.\nAdditionally, researchers can mitigate biased model behavior at a more granular\nlevel. We conduct experiments to attribute the TOD system's bias toward three\ndemographic axes: gender, age, and race. Experimental results show that the\nbias of a TOD system usually comes from the response generation model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hsuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rebecca Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1\">Chinnadhurai Sankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shayandeh_S/0/1/0/all/0/1\">Shahin Shayandeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shang-Tse Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bikel_D/0/1/0/all/0/1\">Daniel M. Bikel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimum Description Length Hopfield Networks. (arXiv:2311.06518v1 [cs.LG])","link":"http://arxiv.org/abs/2311.06518","description":"<p>Associative memory architectures are designed for memorization but also\noffer, through their retrieval method, a form of generalization to unseen\ninputs: stored memories can be seen as prototypes from this point of view.\nFocusing on Modern Hopfield Networks (MHN), we show that a large memorization\ncapacity undermines the generalization opportunity. We offer a solution to\nbetter optimize this tradeoff. It relies on Minimum Description Length (MDL) to\ndetermine during training which memories to store, as well as how many of them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abudy_M/0/1/0/all/0/1\">Matan Abudy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_N/0/1/0/all/0/1\">Nur Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chemla_E/0/1/0/all/0/1\">Emmanuel Chemla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katzir_R/0/1/0/all/0/1\">Roni Katzir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How ChatGPT is Solving Vulnerability Management Problem. (arXiv:2311.06530v1 [cs.SE])","link":"http://arxiv.org/abs/2311.06530","description":"<p>Recently, ChatGPT has attracted great attention from the code analysis\ndomain. Prior works show that ChatGPT has the capabilities of processing\nfoundational code analysis tasks, such as abstract syntax tree generation,\nwhich indicates the potential of using ChatGPT to comprehend code syntax and\nstatic behaviors. However, it is unclear whether ChatGPT can complete more\ncomplicated real-world vulnerability management tasks, such as the prediction\nof security relevance and patch correctness, which require an all-encompassing\nunderstanding of various aspects, including code syntax, program semantics, and\nrelated manual comments.\n</p>\n<p>In this paper, we explore ChatGPT's capabilities on 6 tasks involving the\ncomplete vulnerability management process with a large-scale dataset containing\n78,445 samples. For each task, we compare ChatGPT against SOTA approaches,\ninvestigate the impact of different prompts, and explore the difficulties. The\nresults suggest promising potential in leveraging ChatGPT to assist\nvulnerability management. One notable example is ChatGPT's proficiency in tasks\nlike generating titles for software bug reports. Furthermore, our findings\nreveal the difficulties encountered by ChatGPT and shed light on promising\nfuture directions. For instance, directly providing random demonstration\nexamples in the prompt cannot consistently guarantee good performance in\nvulnerability management. By contrast, leveraging ChatGPT in a self-heuristic\nway -- extracting expertise from demonstration examples itself and integrating\nthe extracted expertise in the prompt is a promising research direction.\nBesides, ChatGPT may misunderstand and misuse the information in the prompt.\nConsequently, effectively guiding ChatGPT to focus on helpful information\nrather than the irrelevant content is still an open problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1\">Lirong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kangjie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yifan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenzhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_H/0/1/0/all/0/1\">Haiqin Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Added Toxicity Mitigation at Inference Time for Multimodal and Massively Multilingual Translation. (arXiv:2311.06532v1 [cs.CL])","link":"http://arxiv.org/abs/2311.06532","description":"<p>Added toxicity in the context of translation refers to the fact of producing\na translation output with more toxicity than there exists in the input. In this\npaper, we present MinTox which is a novel pipeline to identify added toxicity\nand mitigate this issue which works at inference time. MinTox uses a toxicity\ndetection classifier which is multimodal (speech and text) and works in\nlanguages at scale. The mitigation method is applied to languages at scale and\ndirectly in text outputs. MinTox is applied to SEAMLESSM4T, which is the latest\nmultimodal and massively multilingual machine translation system. For this\nsystem, MinTox achieves significant added toxicity mitigation across domains,\nmodalities and language directions. MinTox manages to approximately filter out\nfrom 25% to 95% of added toxicity (depending on the modality and domain) while\nkeeping translation quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dale_D/0/1/0/all/0/1\">David Dale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elbayad_M/0/1/0/all/0/1\">Maha Elbayad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bokai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Public Understanding of Court Opinions with Automated Summarizers. (arXiv:2311.06534v1 [cs.CL])","link":"http://arxiv.org/abs/2311.06534","description":"<p>Written judicial opinions are an important tool for building public trust in\ncourt decisions, yet they can be difficult for non-experts to understand. We\npresent a pipeline for using an AI assistant to generate simplified summaries\nof judicial opinions. These are more accessible to the public and more easily\nunderstood by non-experts, We show in a survey experiment that the simplified\nsummaries help respondents understand the key features of a ruling. We discuss\nhow to integrate legal domain knowledge into studies using large language\nmodels. Our results suggest a role both for AI assistants to inform the public,\nand for lawyers to guide the process of generating accessible summaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ash_E/0/1/0/all/0/1\">Elliott Ash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kesari_A/0/1/0/all/0/1\">Aniket Kesari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naidu_S/0/1/0/all/0/1\">Suresh Naidu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Lena Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stammbach_D/0/1/0/all/0/1\">Dominik Stammbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Cross-Lingual Sentiment Classification under Distribution Shift: an Exploratory Study. (arXiv:2311.06549v1 [cs.CL])","link":"http://arxiv.org/abs/2311.06549","description":"<p>The brittleness of finetuned language model performance on\nout-of-distribution (OOD) test samples in unseen domains has been well-studied\nfor English, yet is unexplored for multi-lingual models. Therefore, we study\ngeneralization to OOD test data specifically in zero-shot cross-lingual\ntransfer settings, analyzing performance impacts of both language and domain\nshifts between train and test data. We further assess the effectiveness of\ncounterfactually augmented data (CAD) in improving OOD generalization for the\ncross-lingual setting, since CAD has been shown to benefit in a monolingual\nEnglish setting. Finally, we propose two new approaches for OOD generalization\nthat avoid the costly annotation process associated with CAD, by exploiting the\npower of recent large language models (LLMs). We experiment with 3 multilingual\nmodels, LaBSE, mBERT, and XLM-R trained on English IMDb movie reviews, and\nevaluate on OOD test sets in 13 languages: Amazon product reviews, Tweets, and\nRestaurant reviews. Results echo the OOD performance decline observed in the\nmonolingual English setting. Further, (i) counterfactuals from the original\nhigh-resource language do improve OOD generalization in the low-resource\nlanguage, and (ii) our newly proposed cost-effective approaches reach similar\nor up to +3.1% better accuracy than CAD for Amazon and Restaurant reviews.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raedt_M/0/1/0/all/0/1\">Maarten De Raedt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitew_S/0/1/0/all/0/1\">Semere Kiros Bitew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godin_F/0/1/0/all/0/1\">Fr&#xe9;deric Godin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1\">Thomas Demeester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Develder_C/0/1/0/all/0/1\">Chris Develder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level Event Argument Extraction. (arXiv:2311.06555v1 [cs.CL])","link":"http://arxiv.org/abs/2311.06555","description":"<p>In this study, we investigate in-context learning (ICL) in document-level\nevent argument extraction (EAE). The paper identifies key challenges in this\nproblem, including example selection, context length limitation, abundance of\nevent types, and the limitation of Chain-of-Thought (CoT) prompting in\nnon-reasoning tasks. To address these challenges, we introduce the\nHeuristic-Driven Link-of-Analogy (HD-LoA) prompting method. Specifically, we\nhypothesize and validate that LLMs learn task-specific heuristics from\ndemonstrations via ICL. Building upon this hypothesis, we introduce an explicit\nheuristic-driven demonstration construction approach, which transforms the\nhaphazard example selection process into a methodical method that emphasizes\ntask heuristics. Additionally, inspired by the analogical reasoning of human,\nwe propose the link-of-analogy prompting, which enables LLMs to process new\nsituations by drawing analogies to known situations, enhancing their\nadaptability. Extensive experiments show that our method outperforms the\nexisting prompting methods and few-shot supervised learning methods, exhibiting\nF1 score improvements of 4.53% and 9.38% on the document-level EAE dataset.\nFurthermore, when applied to sentiment analysis and natural language inference\ntasks, the HD-LoA prompting achieves accuracy gains of 2.87% and 2.63%,\nindicating its effectiveness across different tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hanzhang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Junlang Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zijian Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hui Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zixiao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1\">Kezhi Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Classification to Generation: Insights into Crosslingual Retrieval Augmented ICL. (arXiv:2311.06595v1 [cs.CL])","link":"http://arxiv.org/abs/2311.06595","description":"<p>The remarkable ability of Large Language Models (LLMs) to understand and\nfollow instructions has sometimes been limited by their in-context learning\n(ICL) performance in low-resource languages. To address this, we introduce a\nnovel approach that leverages cross-lingual retrieval-augmented in-context\nlearning (CREA-ICL). By extracting semantically similar prompts from\nhigh-resource languages, we aim to improve the zero-shot performance of\nmultilingual pre-trained language models (MPLMs) across diverse tasks. Though\nour approach yields steady improvements in classification tasks, it faces\nchallenges in generation tasks. Our evaluation offers insights into the\nperformance dynamics of retrieval-augmented in-context learning across both\nclassification and generation domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_E/0/1/0/all/0/1\">Ercong Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Sheng Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BizBench: A Quantitative Reasoning Benchmark for Business and Finance. (arXiv:2311.06602v1 [cs.CL])","link":"http://arxiv.org/abs/2311.06602","description":"<p>As large language models (LLMs) impact a growing number of complex domains,\nit is becoming increasingly important to have fair, accurate, and rigorous\nevaluation benchmarks. Evaluating the reasoning skills required for business\nand financial NLP stands out as a particularly difficult challenge. We\nintroduce BizBench, a new benchmark for evaluating models' ability to reason\nabout realistic financial problems. BizBench comprises 8 quantitative reasoning\ntasks. Notably, BizBench targets the complex task of question-answering (QA)\nfor structured and unstructured financial data via program synthesis (i.e.,\ncode generation). We introduce three diverse financially-themed code-generation\ntasks from newly collected and augmented QA data. Additionally, we isolate\ndistinct financial reasoning capabilities required to solve these QA tasks:\nreading comprehension of financial text and tables, which is required to\nextract correct intermediate values; and understanding domain knowledge (e.g.,\nfinancial formulas) needed to calculate complex solutions. Collectively, these\ntasks evaluate a model's financial background knowledge, ability to extract\nnumeric entities from financial documents, and capacity to solve problems with\ncode. We conduct an in-depth evaluation of open-source and commercial LLMs,\nillustrating that BizBench is a challenging benchmark for quantitative\nreasoning in the finance and business domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koncel_Kedziorski_R/0/1/0/all/0/1\">Rik Koncel-Kedziorski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krumdick_M/0/1/0/all/0/1\">Michael Krumdick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_V/0/1/0/all/0/1\">Viet Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_V/0/1/0/all/0/1\">Varshini Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovering_C/0/1/0/all/0/1\">Charles Lovering</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanner_C/0/1/0/all/0/1\">Chris Tanner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models. (arXiv:2311.06607v1 [cs.CV])","link":"http://arxiv.org/abs/2311.06607","description":"<p>Large Multimodal Models have demonstrated impressive capabilities in\nunderstanding general vision-language tasks. However, due to the limitation of\nsupported input resolution (e.g., 448 x 448) as well as the inexhaustive\ndescription of the training image-text pair, these models often encounter\nchallenges when dealing with intricate scene understandings and narratives.\nHere we address the problem by proposing the Monkey. Our contributions are\ntwo-fold: 1) without pretraining from the start, our method can be built upon\nan existing vision encoder (e.g., vit-BigHuge) to effectively improve the input\nresolution capacity up to 896 x 1344 pixels; 2) we propose a multi-level\ndescription generation method, which automatically provides rich information\nthat can guide model to learn contextual association between scenes and\nobjects. Our extensive testing across more than 16 distinct datasets reveals\nthat Monkey achieves consistently competitive performance over the existing\nLMMs on fundamental tasks, such as Image Captioning, General Visual Question\nAnswering (VQA), and Document-oriented VQA. Models, interactive demo, and the\nsource code are provided at the following\nhttps://github.com/Yuliang-Liu/Monkey.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Biao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhiyin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingxu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yabo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PerceptionGPT: Effectively Fusing Visual Perception into LLM. (arXiv:2311.06612v1 [cs.CV])","link":"http://arxiv.org/abs/2311.06612","description":"<p>The integration of visual inputs with large language models (LLMs) has led to\nremarkable advancements in multi-modal capabilities, giving rise to visual\nlarge language models (VLLMs). However, effectively harnessing VLLMs for\nintricate visual perception tasks remains a challenge. In this paper, we\npresent a novel end-to-end framework named PerceptionGPT, which efficiently and\neffectively equips the VLLMs with visual perception abilities by leveraging the\nrepresentation power of LLMs' token embedding. Our proposed method treats the\ntoken embedding of the LLM as the carrier of spatial information, then leverage\nlightweight visual task encoders and decoders to perform visual perception\ntasks (e.g., detection, segmentation). Our approach significantly alleviates\nthe training difficulty suffered by previous approaches that formulate the\nvisual outputs as discrete tokens, and enables achieving superior performance\nwith fewer trainable parameters, less training data and shorted training time.\nMoreover, as only one token embedding is required to decode the visual outputs,\nthe resulting sequence length during inference is significantly reduced.\nConsequently, our approach enables accurate and flexible representations,\nseamless integration of visual perception tasks, and efficient handling of a\nmultiple of visual outputs. We validate the effectiveness and efficiency of our\napproach through extensive experiments. The results demonstrate significant\nimprovements over previous methods with much fewer trainable parameters and GPU\nhours, which facilitates future research in enabling LLMs with visual\nperception abilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pi_R/0/1/0/all/0/1\">Renjie Pi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiahui Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrainerAgent: Customizable and Efficient Model Training through LLM-Powered Multi-Agent System. (arXiv:2311.06622v1 [cs.AI])","link":"http://arxiv.org/abs/2311.06622","description":"<p>Training AI models has always been challenging, especially when there is a\nneed for custom models to provide personalized services. Algorithm engineers\noften face a lengthy process to iteratively develop models tailored to specific\nbusiness requirements, making it even more difficult for non-experts. The quest\nfor high-quality and efficient model development, along with the emergence of\nLarge Language Model (LLM) Agents, has become a key focus in the industry.\nLeveraging the powerful analytical, planning, and decision-making capabilities\nof LLM, we propose a TrainerAgent system comprising a multi-agent framework\nincluding Task, Data, Model and Server agents. These agents analyze\nuser-defined tasks, input data, and requirements (e.g., accuracy, speed),\noptimizing them comprehensively from both data and model perspectives to obtain\nsatisfactory models, and finally deploy these models as online service.\nExperimental evaluations on classical discriminative and generative tasks in\ncomputer vision and natural language processing domains demonstrate that our\nsystem consistently produces models that meet the desired criteria.\nFurthermore, the system exhibits the ability to critically identify and reject\nunattainable tasks, such as fantastical scenarios or unethical requests,\nensuring robustness and safety. This research presents a significant\nadvancement in achieving desired models with increased efficiency and quality\nas compared to traditional model development, facilitated by the integration of\nLLM-powered analysis, decision-making, and execution capabilities, as well as\nthe collaboration among four agents. We anticipate that our work will\ncontribute to the advancement of research on TrainerAgent in both academic and\nindustry communities, potentially establishing it as a new paradigm for model\ndevelopment in the field of AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianke Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhelun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_A/0/1/0/all/0/1\">Aoxiong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1\">Siming Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wanggui He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperMixer: An MLP-based Low Cost Alternative to Transformers. (arXiv:2203.03691v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.03691","description":"<p>Transformer-based architectures are the model of choice for natural language\nunderstanding, but they come at a significant cost, as they have quadratic\ncomplexity in the input length, require a lot of training data, and can be\ndifficult to tune. In the pursuit of lower costs, we investigate simple\nMLP-based architectures. We find that existing architectures such as MLPMixer,\nwhich achieves token mixing through a static MLP applied to each feature\nindependently, are too detached from the inductive biases required for natural\nlanguage understanding. In this paper, we propose a simple variant, HyperMixer,\nwhich forms the token mixing MLP dynamically using hypernetworks. Empirically,\nwe demonstrate that our model performs better than alternative MLP-based\nmodels, and on par with Transformers. In contrast to Transformers, HyperMixer\nachieves these results at substantially lower costs in terms of processing\ntime, training data, and hyperparameter tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mai_F/0/1/0/all/0/1\">Florian Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pannatier_A/0/1/0/all/0/1\">Arnaud Pannatier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fehr_F/0/1/0/all/0/1\">Fabio Fehr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haolin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marelli_F/0/1/0/all/0/1\">Francois Marelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleuret_F/0/1/0/all/0/1\">Francois Fleuret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. (arXiv:2211.13308v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.13308","description":"<p>Learned representations of scientific documents can serve as valuable input\nfeatures for downstream tasks without further fine-tuning. However, existing\nbenchmarks for evaluating these representations fail to capture the diversity\nof relevant tasks. In response, we introduce SciRepEval, the first\ncomprehensive benchmark for training and evaluating scientific document\nrepresentations. It includes 24 challenging and realistic tasks, 8 of which are\nnew, across four formats: classification, regression, ranking and search. We\nthen use this benchmark to study and improve the generalization ability of\nscientific document representation models. We show how state-of-the-art models\nlike SPECTER and SciNCL struggle to generalize across the task formats, and\nthat simple multi-task training fails to improve them. However, a new approach\nthat learns multiple embeddings per document, each tailored to a different\nformat, can improve performance. We experiment with task-format-specific\ncontrol codes and adapters and find they outperform the existing\nsingle-embedding state-of-the-art by over 2 points absolute. We release the\nresulting family of multi-format models, called SPECTER2, for the community to\nuse and build on.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amanpreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DArcy_M/0/1/0/all/0/1\">Mike D&#x27;Arcy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_S/0/1/0/all/0/1\">Sergey Feldman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Language Models Worse than Humans at Following Prompts? It's Complicated. (arXiv:2301.07085v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.07085","description":"<p>Prompts have been the center of progress in advancing language models'\nzero-shot and few-shot performance. However, recent work finds that models can\nperform surprisingly well when given intentionally irrelevant or misleading\nprompts. Such results may be interpreted as evidence that model behavior is not\n\"human like\". In this study, we challenge a central assumption in such work:\nthat humans would perform badly when given pathological instructions. We find\nthat humans are able to reliably ignore irrelevant instructions and thus, like\nmodels, perform well on the underlying task despite an apparent lack of signal\nregarding the task they are being asked to do. However, when given deliberately\nmisleading instructions, humans follow the instructions faithfully, whereas\nmodels do not. Our findings caution that future research should not idealize\nhuman behaviors as a monolith and should not train or evaluate models to mimic\nassumptions about these behaviors without first validating humans' behaviors\nempirically.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1\">Albert Webson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loo_A/0/1/0/all/0/1\">Alyssa Marie Loo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qinan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Theory of Mind Might Have Spontaneously Emerged in Large Language Models. (arXiv:2302.02083v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.02083","description":"<p>We explore the intriguing possibility that theory of mind (ToM), or the\nuniquely human ability to impute unobservable mental states to others, might\nhave spontaneously emerged in large language models (LLMs). We designed 40\nfalse-belief tasks, considered a gold standard in testing ToM in humans, and\nadministered them to several LLMs. Each task included a false-belief scenario,\nthree closely matched true-belief controls, and the reversed versions of all\nfour. Smaller and older models solved no tasks; GPT-3-davinci-003 (from\nNovember 2022) and ChatGPT-3.5-turbo (from March 2023) solved 20% of the tasks;\nChatGPT-4 (from June 2023) solved 75% of the tasks, matching the performance of\nsix-year-old children observed in past studies. These findings suggest the\nintriguing possibility that ToM, previously considered exclusive to humans, may\nhave spontaneously emerged as a byproduct of LLMs' improving language skills.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kosinski_M/0/1/0/all/0/1\">Michal Kosinski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis. (arXiv:2302.08624v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.08624","description":"<p>We introduce InstructABSA, an instruction learning paradigm for Aspect-Based\nSentiment Analysis (ABSA) subtasks. Our method introduces positive, negative,\nand neutral examples to each training sample, and instruction tune the model\n(Tk-Instruct) for ABSA subtasks, yielding significant performance improvements.\nExperimental results on the Sem Eval 2014, 15, and 16 datasets demonstrate that\nInstructABSA outperforms the previous state-of-the-art (SOTA) approaches on\nTerm Extraction (ATE), Sentiment Classification(ATSC) and Sentiment Pair\nExtraction (ASPE) subtasks. In particular, InstructABSA outperforms the\nprevious state-of-the-art (SOTA) on the Rest14 ATE subtask by 5.69% points, the\nRest15 ATSC subtask by 9.59% points, and the Lapt14 AOPE subtask by 3.37%\npoints, surpassing 7x larger models. We also get competitive results on AOOE,\nAOPE, and AOSTE subtasks indicating strong generalization ability to all\nsubtasks. Exploring sample efficiency reveals that just 50% train data is\nrequired to get competitive results with other instruction tuning approaches.\nLastly, we assess the quality of instructions and observe that InstructABSA's\nperformance experiences a decline of ~10% when adding misleading examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scaria_K/0/1/0/all/0/1\">Kevin Scaria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_S/0/1/0/all/0/1\">Siddharth Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawant_S/0/1/0/all/0/1\">Saurabh Arjun Sawant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models. (arXiv:2302.13439v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.13439","description":"<p>The increased deployment of LMs for real-world tasks involving knowledge and\nfacts makes it important to understand model epistemology: what LMs think they\nknow, and how their attitudes toward that knowledge are affected by language\nuse in their inputs. Here, we study an aspect of model epistemology: how\nepistemic markers of certainty, uncertainty, or evidentiality like \"I'm sure\nit's\", \"I think it's\", or \"Wikipedia says it's\" affect models, and whether they\ncontribute to model failures. We develop a typology of epistemic markers and\ninject 50 markers into prompts for question answering. We find that LMs are\nhighly sensitive to epistemic markers in prompts, with accuracies varying more\nthan 80%. Surprisingly, we find that expressions of high certainty result in a\n7% decrease in accuracy as compared to low certainty expressions; similarly,\nfactive verbs hurt performance, while evidentials benefit performance. Our\nanalysis of a popular pretraining dataset shows that these markers of\nuncertainty are associated with answers on question-answering websites, while\nmarkers of certainty are associated with questions. These associations may\nsuggest that the behavior of LMs is based on mimicking observed language use,\nrather than truly reflecting epistemic uncertainty.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaitlyn Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Text Generation. (arXiv:2303.00908v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.00908","description":"<p>Users interact with text, image, code, or other editors on a daily basis.\nHowever, machine learning models are rarely trained in the settings that\nreflect the interactivity between users and their editor. This is\nunderstandable as training AI models with real users is not only slow and\ncostly, but what these models learn may be specific to user interface design\nchoices. Unfortunately, this means most of the research on text, code, and\nimage generation has focused on non-interactive settings, whereby the model is\nexpected to get everything right without accounting for any input from a user\nwho may be willing to help.\n</p>\n<p>We introduce a new Interactive Text Generation task that allows training\ngeneration models interactively without the costs of involving real users, by\nusing user simulators that provide edits that guide the model towards a given\ntarget text. We train our interactive models using Imitation Learning, and our\nexperiments against competitive non-interactive generation models show that\nmodels trained interactively are superior to their non-interactive\ncounterparts, even when all models are given the same budget of user inputs or\nedits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faltings_F/0/1/0/all/0/1\">Felix Faltings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1\">Michel Galley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brantley_K/0/1/0/all/0/1\">Kiant&#xe9; Brantley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weixin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolan_B/0/1/0/all/0/1\">Bill Dolan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Image of the Process Interpretation of Regular Expressions is Not Closed under Bisimulation Collapse. (arXiv:2303.08553v2 [cs.LO] UPDATED)","link":"http://arxiv.org/abs/2303.08553","description":"<p>Axiomatization and expressibility problems for Milner's process semantics\n(1984) of regular expressions modulo bisimilarity have turned out to be\ndifficult for the full class of expressions with deadlock 0 and empty step~1.\nWe report on a phenomenon that arises from the added presence of 1 when 0 is\navailable, and that brings a crucial reason for this difficulty into focus. To\nwit, while interpretations of 1-free regular expressions are closed under\nbisimulation collapse, this is not the case for the interpretations of\narbitrary regular expressions.\n</p>\n<p>Process graph interpretations of 1-free regular expressions satisfy the loop\nexistence and elimination property LEE, which is preserved under bisimulation\ncollapse. These features of LEE were applied for showing that an equational\nproof system for 1-free regular expressions modulo bisimilarity is complete,\nand that it is decidable in polynomial time whether a process graph is\nbisimilar to the interpretation of a 1-free regular expression.\n</p>\n<p>While interpretations of regular expressions do not satisfy the property LEE\nin general, we show that LEE can be recovered by refined interpretations as\ngraphs with 1-transitions refined interpretations with 1-transitions (which are\nsimilar to silent steps for automata). This suggests that LEE can be expedient\nalso for the general axiomatization and expressibility problems. But a new\nphenomenon emerges that needs to be addressed: the property of a process graph\n`to can be refined into a process graph with 1-transitions and with LEE' is not\npreserved under bisimulation collapse. We provide a 10-vertex graph with two\n1-transitions that satisfies LEE, and in which a pair of bisimilar vertices\ncannot be collapsed on to each other while preserving the refinement property.\nThis implies that the image of the process interpretation of regular\nexpressions is not closed under bisimulation collapse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grabmayer_C/0/1/0/all/0/1\">Clemens Grabmayer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2303.12816","description":"<p>Knowledge graph embedding (KGE) that maps entities and relations into vector\nrepresentations is essential for downstream applications. Conventional KGE\nmethods require high-dimensional representations to learn the complex structure\nof knowledge graph, but lead to oversized model parameters. Recent advances\nreduce parameters by low-dimensional entity representations, while developing\ntechniques (e.g., knowledge distillation or reinvented representation forms) to\ncompensate for reduced dimension. However, such operations introduce\ncomplicated computations and model designs that may not benefit large knowledge\ngraphs. To seek a simple strategy to improve the parameter efficiency of\nconventional KGE models, we take inspiration from that deeper neural networks\nrequire exponentially fewer parameters to achieve expressiveness comparable to\nwider networks for compositional structures. We view all entity representations\nas a single-layer embedding network, and conventional KGE methods that adopt\nhigh-dimensional entity representations equal widening the embedding network to\ngain expressiveness. To achieve parameter efficiency, we instead propose a\ndeeper embedding network for entity representations, i.e., a narrow entity\nembedding layer plus a multi-layer dimension lifting network (LiftNet).\nExperiments on three public datasets show that by integrating LiftNet, four\nconventional KGE methods with 16-dimensional representations achieve comparable\nlink prediction accuracy as original models that adopt 512-dimensional\nrepresentations, saving 68.4% to 96.9% parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_B/0/1/0/all/0/1\">Borui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yong Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Longxiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">He Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jiong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_T/0/1/0/all/0/1\">Tom Luan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning. (arXiv:2303.16445v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.16445","description":"<p>Language model probing is often used to test specific capabilities of models.\nHowever, conclusions from such studies may be limited when the probing\nbenchmarks are small and lack statistical power. In this work, we introduce\nnew, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500)\ninspired by psycholinguistic studies. We dramatically extend existing NEG-136\nand ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44\nsentence pairs to 750 each. We also create another version of extended negation\ndataset (NEG-1500-SIMP-TEMP), created using template-based generation. It\nconsists of 770 sentence pairs. We evaluate 22 models on the extended datasets,\nseeing model performance dip 20-57% compared to the original smaller\nbenchmarks. We observe high levels of negation sensitivity in models like BERT\nand ALBERT demonstrating that previous findings might have been skewed due to\nsmaller test sets. Finally, we observe that while GPT3 has generated all the\nexamples in ROLE-1500 is only able to solve 24.6% of them during probing. The\ndatasets and code are available on\n$\\href{https://github.com/text-machine-lab/extending_psycholinguistic_dataset}{Github}$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shivagunde_N/0/1/0/all/0/1\">Namrata Shivagunde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lialin_V/0/1/0/all/0/1\">Vladislav Lialin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1\">Anna Rumshisky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models. (arXiv:2304.03738v3 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2304.03738","description":"<p>As the capabilities of generative language models continue to advance, the\nimplications of biases ingrained within these models have garnered increasing\nattention from researchers, practitioners, and the broader public. This article\ninvestigates the challenges and risks associated with biases in large-scale\nlanguage models like ChatGPT. We discuss the origins of biases, stemming from,\namong others, the nature of training data, model specifications, algorithmic\nconstraints, product design, and policy decisions. We explore the ethical\nconcerns arising from the unintended consequences of biased model outputs. We\nfurther analyze the potential opportunities to mitigate biases, the\ninevitability of some biases, and the implications of deploying these models in\nvarious applications, such as virtual assistants, content generation, and\nchatbots. Finally, we review the current approaches to identify, quantify, and\nmitigate biases in language models, emphasizing the need for a\nmulti-disciplinary, collaborative effort to develop more equitable,\ntransparent, and responsible AI systems. This article aims to stimulate a\nthoughtful dialogue within the artificial intelligence community, encouraging\nresearchers and developers to reflect on the role of biases in generative\nlanguage models and the ongoing pursuit of ethical AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferrara_E/0/1/0/all/0/1\">Emilio Ferrara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MixPro: Simple yet Effective Data Augmentation for Prompt-based Learning. (arXiv:2304.09402v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.09402","description":"<p>Prompt-based learning has shown considerable promise in reformulating various\ndownstream tasks as cloze problems by combining original input with a\npredetermined template. This approach demonstrates its effectiveness,\nespecially in few-shot learning scenarios, where the model is trained on a\nscarce amount of data. Despite its successes, the limited templates and text in\nfew-shot prompt-based learning scenarios leave significant room for performance\nimprovement. Moreover, existing methods sometimes resort to model ensembles,\nwhich, while effective, could potentially hamper model efficiency due to\nincreased computational demands. To address these issues, we introduce MixPro,\nan augmentation method designed to augment both the vanilla input text and the\ntemplates. We implement this through the token-level, the sentence-level, and\nthe template-level Mixup strategies. The experimental results on five few-shot\ndatasets show that MixPro outperforms other augmentation baselines, improving\nmodel performance by an average of 5.08% compared to before augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_L/0/1/0/all/0/1\">Longxu Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yutai Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yunlong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_H/0/1/0/all/0/1\">Honglin Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qingfu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qinghua Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity. (arXiv:2305.13169v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13169","description":"<p>Pretraining is the preliminary and fundamental step in developing capable\nlanguage models (LM). Despite this, pretraining data design is critically\nunder-documented and often guided by empirically unsupported intuitions. To\naddress this, we pretrain 28 1.5B parameter decoder-only models, training on\ndata curated (1) at different times, (2) with varying toxicity and quality\nfilters, and (3) with different domain compositions. First, we quantify the\neffect of pretraining data age. A temporal shift between evaluation data and\npretraining data leads to performance degradation, which is not overcome by\nfinetuning. Second, we explore the effect of quality and toxicity filters,\nshowing a trade-off between performance on standard benchmarks and risk of\ntoxic generations. Our findings indicate there does not exist a\none-size-fits-all solution to filtering training data. We also find that the\neffects of different types of filtering are not predictable from text domain\ncharacteristics. Lastly, we empirically validate that the inclusion of\nheterogeneous data sources, like books and web, is broadly beneficial and\nwarrants greater prioritization. These findings constitute the largest set of\nexperiments to validate, quantify, and expose many undocumented intuitions\nabout text pretraining, which we hope will help support more informed\ndata-centric decisions in LM development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yauney_G/0/1/0/all/0/1\">Gregory Yauney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reif_E/0/1/0/all/0/1\">Emily Reif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Katherine Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1\">Barret Zoph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_K/0/1/0/all/0/1\">Kevin Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mimno_D/0/1/0/all/0/1\">David Mimno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer-Free Data-Efficient Multilingual Slot Labeling. (arXiv:2305.13528v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13528","description":"<p>Slot labeling (SL) is a core component of task-oriented dialogue (ToD)\nsystems, where slots and corresponding values are usually language-, task- and\ndomain-specific. Therefore, extending the system to any new\nlanguage-domain-task configuration requires (re)running an expensive and\nresource-intensive data annotation process. To mitigate the inherent data\nscarcity issue, current research on multilingual ToD assumes that sufficient\nEnglish-language annotated data are always available for particular tasks and\ndomains, and thus operates in a standard cross-lingual transfer setup. In this\nwork, we depart from this often unrealistic assumption. We examine challenging\nscenarios where such transfer-enabling English annotated data cannot be\nguaranteed, and focus on bootstrapping multilingual data-efficient slot\nlabelers in transfer-free scenarios directly in the target languages without\nany English-ready data. We propose a two-stage slot labeling approach (termed\nTWOSL) which transforms standard multilingual sentence encoders into effective\nslot labelers. In Stage 1, relying on SL-adapted contrastive learning with only\na handful of SL-annotated examples, we turn sentence encoders into\ntask-specific span encoders. In Stage 2, we recast SL from a token\nclassification into a simpler, less data-intensive span classification task.\nOur results on two standard multilingual TOD datasets and across diverse\nlanguages confirm the effectiveness and robustness of TWOSL. It is especially\neffective for the most challenging transfer-free few-shot setups, paving the\nway for quick and data-efficient bootstrapping of multilingual slot labelers\nfor ToD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Razumovskaia_E/0/1/0/all/0/1\">Evgeniia Razumovskaia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Attention is Not Enough: Incongruity-Aware Dynamic Hierarchical Fusion for Multimodal Affect Recognition. (arXiv:2305.13583v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13583","description":"<p>Fusing multiple modalities has proven effective for multimodal information\nprocessing. However, the incongruity between modalities poses a challenge for\nmultimodal fusion, especially in affect recognition. In this study, we first\nanalyze how the salient affective information in one modality can be affected\nby the other, and demonstrate that inter-modal incongruity exists latently in\ncrossmodal attention. Based on this finding, we propose the Hierarchical\nCrossmodal Transformer with Dynamic Modality Gating (HCT-DMG), a lightweight\nincongruity-aware model, which dynamically chooses the primary modality in each\ntraining batch and reduces fusion times by leveraging the learned hierarchy in\nthe latent space to alleviate incongruity. The experimental evaluation on five\nbenchmark datasets: CMU-MOSI, CMU-MOSEI, and IEMOCAP (sentiment and emotion),\nwhere incongruity implicitly lies in hard samples, as well as UR-FUNNY (humour)\nand MUStaRD (sarcasm), where incongruity is common, verifies the efficacy of\nour approach, showing that HCT-DMG: 1) outperforms previous multimodal models\nwith a reduced size of approximately 0.8M parameters; 2) recognizes hard\nsamples where incongruity makes affect recognition difficult; 3) mitigates the\nincongruity at the latent level in crossmodal attention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaoting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bell_P/0/1/0/all/0/1\">Peter Bell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Catherine Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Goal-Driven Explainable Clustering via Language Descriptions. (arXiv:2305.13749v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13749","description":"<p>Unsupervised clustering is widely used to explore large corpora, but existing\nformulations neither consider the users' goals nor explain clusters' meanings.\nWe propose a new task formulation, \"Goal-Driven Clustering with Explanations\"\n(GoalEx), which represents both the goal and the explanations as free-form\nlanguage descriptions. For example, to categorize the errors made by a\nsummarization system, the input to GoalEx is a corpus of annotator-written\ncomments for system-generated summaries and a goal description \"cluster the\ncomments based on why the annotators think the summary is imperfect.''; the\noutputs are text clusters each with an explanation (\"this cluster mentions that\nthe summary misses important context information.\"), which relates to the goal\nand precisely explain which comments should (not) belong to a cluster. To\ntackle GoalEx, we prompt a language model with \"[corpus subset] + [goal] +\nBrainstorm a list of explanations each representing a cluster.\"; then we\nclassify whether each sample belongs to a cluster based on its explanation;\nfinally, we use integer linear programming to select a subset of candidate\nclusters to cover most samples while minimizing overlaps. Under both automatic\nand human evaluation on corpora with or without labels, our method produces\nmore accurate and goal-related explanations than prior methods. We release our\ndata and implementation at https://github.com/ZihanWangKi/GoalEx.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">Ruiqi Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of African American Language Bias in Natural Language Generation. (arXiv:2305.14291v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14291","description":"<p>We evaluate how well LLMs understand African American Language (AAL) in\ncomparison to their performance on White Mainstream English (WME), the\nencouraged \"standard\" form of English taught in American classrooms. We measure\nLLM performance using automatic metrics and human judgments for two tasks: a\ncounterpart generation task, where a model generates AAL (or WME) given WME (or\nAAL), and a masked span prediction (MSP) task, where models predict a phrase\nthat was removed from their input. Our contributions include: (1) evaluation of\nsix pre-trained, large language models on the two language generation tasks;\n(2) a novel dataset of AAL text from multiple contexts (social media, hip-hop\nlyrics, focus groups, and linguistic interviews) with human-annotated\ncounterparts in WME; and (3) documentation of model performance gaps that\nsuggest bias and identification of trends in lack of understanding of AAL\nfeatures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deas_N/0/1/0/all/0/1\">Nicholas Deas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grieser_J/0/1/0/all/0/1\">Jessi Grieser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleiner_S/0/1/0/all/0/1\">Shana Kleiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patton_D/0/1/0/all/0/1\">Desmond Patton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turcan_E/0/1/0/all/0/1\">Elsbeth Turcan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ghostbuster: Detecting Text Ghostwritten by Large Language Models. (arXiv:2305.15047v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.15047","description":"<p>We introduce Ghostbuster, a state-of-the-art system for detecting\nAI-generated text. Our method works by passing documents through a series of\nweaker language models, running a structured search over possible combinations\nof their features, and then training a classifier on the selected features to\npredict whether documents are AI-generated. Crucially, Ghostbuster does not\nrequire access to token probabilities from the target model, making it useful\nfor detecting text generated by black-box models or unknown model versions. In\nconjunction with our model, we release three new datasets of human- and\nAI-generated text as detection benchmarks in the domains of student essays,\ncreative writing, and news articles. We compare Ghostbuster to a variety of\nexisting detectors, including DetectGPT and GPTZero, as well as a new RoBERTa\nbaseline. Ghostbuster achieves 99.0 F1 when evaluated across domains, which is\n5.9 F1 higher than the best preexisting model. It also outperforms all previous\napproaches in generalization across writing domains (+7.5 F1), prompting\nstrategies (+2.1 F1), and language models (+4.4 F1). We also analyze the\nrobustness of our system to a variety of perturbations and paraphrasing attacks\nand evaluate its performance on documents written by non-native English\nspeakers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1\">Vivek Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleisig_E/0/1/0/all/0/1\">Eve Fleisig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomlin_N/0/1/0/all/0/1\">Nicholas Tomlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Source Code Data Augmentation for Deep Learning: A Survey. (arXiv:2305.19915v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.19915","description":"<p>The increasingly popular adoption of deep learning models in many critical\nsource code tasks motivates the development of data augmentation (DA)\ntechniques to enhance training data and improve various capabilities (e.g.,\nrobustness and generalizability) of these models. Although a series of DA\nmethods have been proposed and tailored for source code models, there lacks a\ncomprehensive survey and examination to understand their effectiveness and\nimplications. This paper fills this gap by conducting a comprehensive and\nintegrative survey of data augmentation for source code, wherein we\nsystematically compile and encapsulate existing literature to provide a\ncomprehensive overview of the field. We start with an introduction of data\naugmentation in source code and then provide a discussion on major\nrepresentative approaches. Next, we highlight the general strategies and\ntechniques to optimize the DA quality. Subsequently, we underscore techniques\nuseful in real-world source code scenarios and downstream tasks. Finally, we\noutline the prevailing challenges and potential opportunities for future\nresearch. In essence, we aim to demystify the corpus of existing literature on\nsource code DA for deep learning, and foster further exploration in this\nsphere. Complementing this, we present a continually updated GitHub repository\nthat hosts a list of update-to-date papers on DA for source code modeling,\naccessible at \\url{https://github.com/terryyz/DataAug4Code}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1\">Terry Yue Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhou Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhensu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xiaoning Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1\">Zhenchang Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_D/0/1/0/all/0/1\">David Lo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Generate Better Than Your LLM. (arXiv:2306.11816v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2306.11816","description":"<p>Reinforcement learning (RL) has emerged as a powerful paradigm for\nfine-tuning Large Language Models (LLMs) for text generation. In particular,\nrecent LLMs such as ChatGPT and GPT-4 can engage in fluent conversations with\nusers after finetuning with RL. Capitalizing on key properties of text\ngeneration, we seek to investigate RL algorithms beyond general purpose\nalgorithms like Proximal Policy Optimization (PPO). In particular, we extend RL\nalgorithms to allow them to interact with a dynamic black-box guide LLM and\npropose RL with guided feedback (RLGF), a suite of RL algorithms for LLM\nfine-tuning. We provide two ways for the guide LLM to interact with the LLM to\nbe optimized for maximizing rewards. The guide LLM can generate text which\nserves as additional starting states for the RL optimization procedure. The\nguide LLM can also be used to complete the partial sentences generated by the\nLLM that is being optimized, treating the guide LLM as an expert to imitate and\nsurpass eventually. We experiment on the IMDB positive sentiment, CommonGen,\nand TL;DR summarization tasks. We show that our RL algorithms achieve higher\nperformance than supervised learning (SL) and the RL baseline PPO,\ndemonstrating the benefit of interaction with the guide LLM. On both CommonGen\nand TL;DR, we not only outperform our SL baselines but also improve upon PPO\nacross a variety of metrics beyond the one we optimized for. Our code can be\nfound at https://github.com/Cornell-RL/tril.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jonathan D. Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brantley_K/0/1/0/all/0/1\">Kiante Brantley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamurthy_R/0/1/0/all/0/1\">Rajkumar Ramamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1\">Dipendra Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wen Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Style Over Substance: Evaluation Biases for Large Language Models. (arXiv:2307.03025v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.03025","description":"<p>As large language models (LLMs) continue to advance, accurately and\ncomprehensively evaluating their performance becomes increasingly challenging.\nRanking the relative performance of LLMs based on Elo ratings, according to\nhuman judgment, is gaining more popularity. However, the extent to which humans\nand LLMs are capable evaluators remains uncertain. This study investigates the\nbehavior of crowd-sourced and expert annotators, as well as LLMs, when\ncomparing outputs from different models. To achieve this, we curate a dataset\nof intentionally flawed machine-generated answers. Our findings reveal a\nconcerning bias in the evaluation process, as answers with factual errors are\nrated more favorably than answers that are too short or contained grammatical\nerrors. To address this issue, we propose independently evaluating\nmachine-generated text across multiple dimensions, rather than merging all the\nevaluation aspects into a single score. We instantiate this idea with the Elo\nrating system, resulting in the Multi-Elo Rating System (MERS). Empirical\nresults from our study reveal that this proposed approach significantly\nenhances the quality of LLM-based evaluations, particularly in terms of factual\naccuracy. However, there is no significant improvement in crowd-sourced-based\nevaluations, indicating the need for further investigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RLTF: Reinforcement Learning from Unit Test Feedback. (arXiv:2307.04349v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2307.04349","description":"<p>The goal of program synthesis, or code generation, is to generate executable\ncode based on given descriptions. Recently, there has been an increasing number\nof studies employing reinforcement learning (RL) to improve the performance of\nlarge language models (LLMs) for code. However, current representative works\neither rely solely on offline frameworks, limiting the exploration of new\nsample spaces, or fall short in the utilization of unit test signals, not\naccounting for specific error locations within the code. To address these\nissues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback,\na novel online RL framework with unit test feedback of multi-granularity for\nrefining code LLMs. Our approach generates data in real-time during training\nand simultaneously utilizes fine-grained feedback signals to guide the model\ntowards producing higher-quality code. Extensive experiments show that RLTF\nachieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our\ncode is available at: https://github.com/Zyq-scut/RLTF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiate Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yiqin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1\">Kaiwen Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1\">Deheng Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2307.06440","description":"<p>The computation necessary for training Transformer-based language models has\nskyrocketed in recent years. This trend has motivated research on efficient\ntraining algorithms designed to improve training, validation, and downstream\nperformance faster than standard training. In this work, we revisit three\ncategories of such algorithms: dynamic architectures (layer stacking, layer\ndropping), batch selection (selective backprop, RHO loss), and efficient\noptimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed\ncomputation budget using such methods, we find that their training, validation,\nand downstream gains vanish compared to a baseline with a fully-decayed\nlearning rate. We define an evaluation protocol that enables computation to be\ndone on arbitrary machines by mapping all computation time to a reference\nmachine which we call reference system time. We discuss the limitations of our\nproposed protocol and release our code to encourage rigorous research in\nefficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1\">Jean Kaddour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Key_O/0/1/0/all/0/1\">Oscar Key</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nawrot_P/0/1/0/all/0/1\">Piotr Nawrot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1\">Pasquale Minervini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1\">Matt J. Kusner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An In-Depth Evaluation of Federated Learning on Biomedical Natural Language Processing. (arXiv:2307.11254v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.11254","description":"<p>Language models (LMs) such as BERT and GPT have revolutionized natural\nlanguage processing (NLP). However, the medical field faces challenges in\ntraining LMs due to limited data access and privacy constraints imposed by\nregulations like the Health Insurance Portability and Accountability Act\n(HIPPA) and the General Data Protection Regulation (GDPR). Federated learning\n(FL) offers a decentralized solution that enables collaborative learning while\nensuring data privacy. In this study, we evaluated FL on 2 biomedical NLP tasks\nencompassing 8 corpora using 6 LMs. Our results show that: 1) FL models\nconsistently outperformed models trained on individual clients' data and\nsometimes performed comparably with models trained with polled data; 2) with\nthe fixed number of total data, FL models training with more clients produced\ninferior performance but pre-trained transformer-based models exhibited great\nresilience. 3) FL models significantly outperformed large language models using\nzero-/one-shot learning and offered lightning inference speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Le Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Gaoxiang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+zhou_s/0/1/0/all/0/1\">sicheng zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+chen_j/0/1/0/all/0/1\">jiandong chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Ziyue Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Ju Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Understand and Can be Enhanced by Emotional Stimuli. (arXiv:2307.11760v7 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.11760","description":"<p>Emotional intelligence significantly impacts our daily behaviors and\ninteractions. Although Large Language Models (LLMs) are increasingly viewed as\na stride toward artificial general intelligence, exhibiting impressive\nperformance in numerous tasks, it is still uncertain if LLMs can genuinely\ngrasp psychological emotional stimuli. Understanding and responding to\nemotional cues gives humans a distinct advantage in problem-solving. In this\npaper, we take the first step towards exploring the ability of LLMs to\nunderstand emotional stimuli. To this end, we first conduct automatic\nexperiments on 45 tasks using various LLMs, including Flan-T5-Large, Vicuna,\nLlama 2, BLOOM, ChatGPT, and GPT-4. Our tasks span deterministic and generative\napplications that represent comprehensive evaluation scenarios. Our automatic\nexperiments show that LLMs have a grasp of emotional intelligence, and their\nperformance can be improved with emotional prompts (which we call\n\"EmotionPrompt\" that combines the original prompt with emotional stimuli),\ne.g., 8.00% relative performance improvement in Instruction Induction and 115%\nin BIG-Bench. In addition to those deterministic tasks that can be\nautomatically evaluated using existing metrics, we conducted a human study with\n106 participants to assess the quality of generative tasks using both vanilla\nand emotional prompts. Our human study results demonstrate that EmotionPrompt\nsignificantly boosts the performance of generative tasks (10.9% average\nimprovement in terms of performance, truthfulness, and responsibility metrics).\nWe provide an in-depth discussion regarding why EmotionPrompt works for LLMs\nand the factors that may influence its performance. We posit that EmotionPrompt\nheralds a novel avenue for exploring interdisciplinary knowledge for human-LLMs\ninteraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yixuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kaijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1\">Wenxin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1\">Jianxun Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1\">Fang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models. (arXiv:2307.11772v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2307.11772","description":"<p>The task of entity alignment between knowledge graphs (KGs) aims to identify\nevery pair of entities from two different KGs that represent the same entity.\nMany machine learning-based methods have been proposed for this task. However,\nto our best knowledge, existing methods all require manually crafted seed\nalignments, which are expensive to obtain. In this paper, we propose the first\nfully automatic alignment method named AutoAlign, which does not require any\nmanually crafted seed alignments. Specifically, for predicate embeddings,\nAutoAlign constructs a predicate-proximity-graph with the help of large\nlanguage models to automatically capture the similarity between predicates\nacross two KGs. For entity embeddings, AutoAlign first computes the entity\nembeddings of each KG independently using TransE, and then shifts the two KGs'\nentity embeddings into the same vector space by computing the similarity\nbetween entities based on their attributes. Thus, both predicate alignment and\nentity alignment can be done without manually crafted seed alignments.\nAutoAlign is not only fully automatic, but also highly effective. Experiments\nusing real-world KGs show that AutoAlign improves the performance of entity\nalignment significantly compared to state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trisedya_B/0/1/0/all/0/1\">Bayu Distiawan Trisedya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaoyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jianzhong Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Imitation Game: Detecting Human and AI-Generated Texts in the Era of ChatGPT and BARD. (arXiv:2307.12166v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.12166","description":"<p>The potential of artificial intelligence (AI)-based large language models\n(LLMs) holds considerable promise in revolutionizing education, research, and\npractice. However, distinguishing between human-written and AI-generated text\nhas become a significant task. This paper presents a comparative study,\nintroducing a novel dataset of human-written and LLM-generated texts in\ndifferent genres: essays, stories, poetry, and Python code. We employ several\nmachine learning models to classify the texts. Results demonstrate the efficacy\nof these models in discerning between human and AI-generated text, despite the\ndataset's limited sample size. However, the task becomes more challenging when\nclassifying GPT-generated text, particularly in story writing. The results\nindicate that the models exhibit superior performance in binary classification\ntasks, such as distinguishing human-generated text from a specific LLM,\ncompared to the more complex multiclass tasks that involve discerning among\nhuman-generated and multiple LLMs. Our findings provide insightful implications\nfor AI text detection while our dataset paves the way for future research in\nthis evolving area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hayawi_K/0/1/0/all/0/1\">Kadhim Hayawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1\">Sakib Shahriar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_S/0/1/0/all/0/1\">Sujith Samuel Mathew</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation for Neural Machine Translation using Generative Language Model. (arXiv:2307.16833v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.16833","description":"<p>Despite the rapid growth in model architecture, the scarcity of large\nparallel corpora remains the main bottleneck in Neural Machine Translation.\nData augmentation is a technique that enhances the performance of data-hungry\nmodels by generating synthetic data instead of collecting new ones. We explore\nprompt-based data augmentation approaches that leverage large-scale language\nmodels such as ChatGPT. To create a synthetic parallel corpus, we compare 3\nmethods using different prompts. We employ two assessment metrics to measure\nthe diversity of the generated synthetic data. This approach requires no\nfurther model training cost, which is mandatory in other augmentation methods\nlike back-translation. The proposed method improves the unaugmented baseline by\n0.68 BLEU score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Seokjin Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Su Ah Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_W/0/1/0/all/0/1\">Woohwan Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Token-Scaled Logit Distillation for Ternary Weight Generative Language Models. (arXiv:2308.06744v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.06744","description":"<p>Generative Language Models (GLMs) have shown impressive performance in tasks\nsuch as text generation, understanding, and reasoning. However, the large model\nsize poses challenges for practical deployment. To solve this problem,\nQuantization-Aware Training (QAT) has become increasingly popular. However,\ncurrent QAT methods for generative models have resulted in a noticeable loss of\naccuracy. To counteract this issue, we propose a novel knowledge distillation\nmethod specifically designed for GLMs. Our method, called token-scaled logit\ndistillation, prevents overfitting and provides superior learning from the\nteacher model and ground truth. This research marks the first evaluation of\nternary weight quantization-aware training of large-scale GLMs with less than\n1.0 degradation in perplexity and achieves enhanced accuracy in tasks like\ncommon-sense QA and arithmetic reasoning as well as natural language\nunderstanding. Our code is available at https://github.com/aiha-lab/TSLD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sihwa Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Janghwan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Sukjin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Du-Seong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_W/0/1/0/all/0/1\">Wonyong Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jungwook Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic. (arXiv:2308.07336v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2308.07336","description":"<p>We study a synthetic corpus based approach for language models (LMs) to\nacquire logical deductive reasoning ability. The previous studies generated\ndeduction examples using specific sets of deduction rules. However, these rules\nwere limited or otherwise arbitrary, limiting the generalizability of acquired\nreasoning ability. We rethink this and adopt a well-grounded set of deduction\nrules based on formal logic theory, which can derive any other deduction rules\nwhen combined in a multistep way. Then, using the proposed corpora, which we\nname FLD (Formal Logic Deduction), we first evaluate and analyze the logical\nreasoning ability of the latest LLMs. Even GPT-4 can solve only half of the\nproblems, suggesting that pure logical reasoning isolated from knowledge is\nstill challenging for the LLMs, and additional training specialized in logical\nreasoning is indeed essential. We next empirically verify that LMs trained on\nFLD corpora acquire more generalizable reasoning ability. Furthermore, we\nidentify the aspects of reasoning ability on which deduction corpora can\nenhance LMs and those on which they cannot, and discuss future directions on\neach aspect. The released corpora serve both as learning resources and as\nchallenging benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Morishita_T/0/1/0/all/0/1\">Terufumi Morishita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morio_G/0/1/0/all/0/1\">Gaku Morio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamaguchi_A/0/1/0/all/0/1\">Atsuki Yamaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogawa_Y/0/1/0/all/0/1\">Yasuhiro Sogawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Activation Addition: Steering Language Models Without Optimization. (arXiv:2308.10248v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.10248","description":"<p>Reliably controlling the behavior of large language models is a pressing open\nproblem. Existing methods include supervised finetuning, reinforcement learning\nfrom human feedback, prompt engineering and guided decoding. We instead\ninvestigate activation engineering: modifying activations at inference-time to\npredictably alter model behavior. We bias the forward pass with a 'steering\nvector' implicitly specified through natural language. Past work learned these\nsteering vectors; our Activation Addition (ActAdd) method instead computes them\nby taking the activation differences which result from pairs of prompts.\n</p>\n<p>We demonstrate ActAdd on GPT-2 on OpenWebText and ConceptNet, and replicate\nthe effect on Llama-13B and GPT-J-6B. Our approach yields inference-time\ncontrol over high-level properties of output &amp; preserves performance on\noff-target topics. The method requires far less compute and implementation\neffort than finetuning and RLHF, allows for natural language specification by\nusers, and its overhead scales naturally with model size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Turner_A/0/1/0/all/0/1\">Alexander Matt Turner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiergart_L/0/1/0/all/0/1\">Lisa Thiergart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Udell_D/0/1/0/all/0/1\">David Udell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leech_G/0/1/0/all/0/1\">Gavin Leech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mini_U/0/1/0/all/0/1\">Ulisse Mini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacDiarmid_M/0/1/0/all/0/1\">Monte MacDiarmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Insights Into the Nutritional Prevention of Macular Degeneration based on a Comparative Topic Modeling Approach. (arXiv:2309.00312v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.00312","description":"<p>Topic modeling and text mining are subsets of Natural Language Processing\n(NLP) with relevance for conducting meta-analysis (MA) and systematic review\n(SR). For evidence synthesis, the above NLP methods are conventionally used for\ntopic-specific literature searches or extracting values from reports to\nautomate essential phases of SR and MA. Instead, this work proposes a\ncomparative topic modeling approach to analyze reports of contradictory results\non the same general research question. Specifically, the objective is to\nidentify topics exhibiting distinct associations with significant results for\nan outcome of interest by ranking them according to their proportional\noccurrence in (and consistency of distribution across) reports of significant\neffects. The proposed method was tested on broad-scope studies addressing\nwhether supplemental nutritional compounds significantly benefit macular\ndegeneration (MD). Six compounds were identified as having a particular\nassociation with reports of significant results for benefiting MD. Four of\nthese were further supported in terms of effectiveness upon conducting a\nfollow-up literature search for validation (omega-3 fatty acids, copper,\nzeaxanthin, and nitrates). The two not supported by the follow-up literature\nsearch (niacin and molybdenum) also had scores in the lowest range under the\nproposed scoring system, suggesting that the proposed methods score for a given\ntopic may be a viable proxy for its degree of association with the outcome of\ninterest and can be helpful in the search for potentially causal relationships.\nThese results underpin the proposed methods potential to add specificity in\nunderstanding effects from broad-scope reports, elucidate topics of interest\nfor future research, and guide evidence synthesis in a systematic and scalable\nway. All of this is accomplished while yielding valuable insights into the\nprevention of MD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jacaruso_L/0/1/0/all/0/1\">Lucas Cassiel Jacaruso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs Between Groundedness and Human Preference. (arXiv:2310.03184v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.03184","description":"<p>For middle-school math students, interactive question-answering (QA) with\ntutors is an effective way to learn. The flexibility and emergent capabilities\nof generative large language models (LLMs) has led to a surge of interest in\nautomating portions of the tutoring process - including interactive QA to\nsupport conceptual discussion of mathematical concepts. However, LLM responses\nto math questions can be incorrect or mismatched to the educational context -\nsuch as being misaligned with a school's curriculum. One potential solution is\nretrieval-augmented generation (RAG), which involves incorporating a vetted\nexternal knowledge source in the LLM prompt to increase response quality. In\nthis paper, we designed prompts that retrieve and use content from a\nhigh-quality open-source math textbook to generate responses to real student\nquestions. We evaluate the efficacy of this RAG system for middle-school\nalgebra and geometry QA by administering a multi-condition survey, finding that\nhumans prefer responses generated using RAG, but not when responses are too\ngrounded in the textbook content. We argue that while RAG is able to improve\nresponse quality, designers of math QA systems must consider trade-offs between\ngenerating responses preferred by students and responses closely matched to\nspecific educational resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levonian_Z/0/1/0/all/0/1\">Zachary Levonian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenglu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wangda Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gade_A/0/1/0/all/0/1\">Anoushka Gade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henkel_O/0/1/0/all/0/1\">Owen Henkel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Postle_M/0/1/0/all/0/1\">Millie-Ellen Postle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_W/0/1/0/all/0/1\">Wanli Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KoMultiText: Large-Scale Korean Text Dataset for Classifying Biased Speech in Real-World Online Services. (arXiv:2310.04313v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.04313","description":"<p>With the growth of online services, the need for advanced text classification\nalgorithms, such as sentiment analysis and biased text detection, has become\nincreasingly evident. The anonymous nature of online services often leads to\nthe presence of biased and harmful language, posing challenges to maintaining\nthe health of online communities. This phenomenon is especially relevant in\nSouth Korea, where large-scale hate speech detection algorithms have not yet\nbeen broadly explored. In this paper, we introduce \"KoMultiText\", a new\ncomprehensive, large-scale dataset collected from a well-known South Korean SNS\nplatform. Our proposed dataset provides annotations including (1) Preferences,\n(2) Profanities, and (3) Nine types of Bias for the text samples, enabling\nmulti-task learning for simultaneous classification of user-generated texts.\nLeveraging state-of-the-art BERT-based language models, our approach surpasses\nhuman-level accuracy across diverse classification tasks, as measured by\nvarious metrics. Beyond academic contributions, our work can provide practical\nsolutions for real-world hate speech and bias mitigation, contributing directly\nto the improvement of online community health. Our work provides a robust\nfoundation for future research aiming to improve the quality of online\ndiscourse and foster societal well-being. All source codes and datasets are\npublicly accessible at https://github.com/Dasol-Choi/KoMultiText.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dasol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jooyoung Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1\">Eunsun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Jinwoo Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Heejune Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_D/0/1/0/all/0/1\">Dongbin Na</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets. (arXiv:2310.04793v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.04793","description":"<p>In the swiftly expanding domain of Natural Language Processing (NLP), the\npotential of GPT-based models for the financial sector is increasingly evident.\nHowever, the integration of these models with financial datasets presents\nchallenges, notably in determining their adeptness and relevance. This paper\nintroduces a distinctive approach anchored in the Instruction Tuning paradigm\nfor open-source large language models, specifically adapted for financial\ncontexts. Through this methodology, we capitalize on the interoperability of\nopen-source models, ensuring a seamless and transparent integration. We begin\nby explaining the Instruction Tuning paradigm, highlighting its effectiveness\nfor immediate integration. The paper presents a benchmarking scheme designed\nfor end-to-end training and testing, employing a cost-effective progression.\nFirstly, we assess basic competencies and fundamental tasks, such as Named\nEntity Recognition (NER) and sentiment analysis to enhance specialization.\nNext, we delve into a comprehensive model, executing multi-task operations by\namalgamating all instructional tunings to examine versatility. Finally, we\nexplore the zero-shot capabilities by earmarking unseen tasks and incorporating\nnovel datasets to understand adaptability in uncharted terrains. Such a\nparadigm fortifies the principles of openness and reproducibility, laying a\nrobust foundation for future investigations in open-source financial large\nlanguage models (FinLLMs).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Neng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongyang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Christina Dan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Investigation of LLMs' Inefficacy in Understanding Converse Relations. (arXiv:2310.05163v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.05163","description":"<p>Large Language Models (LLMs) have achieved remarkable success in many formal\nlanguage oriented tasks, such as structural data-to-text and semantic parsing.\nHowever current benchmarks mostly follow the data distribution of the\npre-training data of LLMs. Therefore, a natural question rises that do LLMs\nreally understand the structured semantics of formal languages. In this paper,\nwe investigate this problem on a special case, converse binary relation. We\nintroduce a new benchmark ConvRe focusing on converse relations, which contains\n17 relations and 1240 triples extracted from popular knowledge graph completion\ndatasets. Our ConvRE features two tasks, Re2Text and Text2Re, which are\nformulated as multi-choice question answering to evaluate LLMs' ability to\ndetermine the matching between relations and associated text. For the\nevaluation protocol, apart from different prompting methods, we further\nintroduce variants to the test text and few-shot example text. We conduct\nexperiments on three popular LLM families and have observed various scaling\ntrends. The results suggest that LLMs often resort to shortcut learning and\nstill face challenges on our proposed benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1\">Chengwen Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1\">Binyuan Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bailin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jinwang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laili_Y/0/1/0/all/0/1\">Yuanjun Laili</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Adaptive Tokenization: Enhancing Long-Form Text Generation Efficacy in Mental Health and Beyond. (arXiv:2310.05317v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.05317","description":"<p>We propose task-adaptive tokenization as a way to adapt the generation\npipeline to the specifics of a downstream task and enhance long-form generation\nin mental health. Inspired by insights from cognitive science, our\ntask-adaptive tokenizer samples variable segmentations from multiple outcomes,\nwith sampling probabilities optimized based on task-specific data. We introduce\na strategy for building a specialized vocabulary and introduce a vocabulary\nmerging protocol that allows for the integration of task-specific tokens into\nthe pre-trained model's tokenization step. Through extensive experiments on\npsychological question-answering tasks in both Chinese and English, we find\nthat our task-adaptive tokenization approach brings a significant improvement\nin generation performance while using up to 60% fewer tokens. Preliminary\nexperiments point to promising results when using our tokenization approach\nwith very large language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Siyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_N/0/1/0/all/0/1\">Naihao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabour_S/0/1/0/all/0/1\">Sahand Sabour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yilin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated clinical coding using off-the-shelf large language models. (arXiv:2310.06552v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2310.06552","description":"<p>The task of assigning diagnostic ICD codes to patient hospital admissions is\ntypically performed by expert human coders. Efforts towards automated ICD\ncoding are dominated by supervised deep learning models. However, difficulties\nin learning to predict the large number of rare codes remain a barrier to\nadoption in clinical practice. In this work, we leverage off-the-shelf\npre-trained generative large language models (LLMs) to develop a practical\nsolution that is suitable for zero-shot and few-shot code assignment, with no\nneed for further task-specific training. Unsupervised pre-training alone does\nnot guarantee precise knowledge of the ICD ontology and specialist clinical\ncoding task, therefore we frame the task as information extraction, providing a\ndescription of each coded concept and asking the model to retrieve related\nmentions. For efficiency, rather than iterating over all codes, we leverage the\nhierarchical nature of the ICD ontology to sparsely search for relevant codes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boyle_J/0/1/0/all/0/1\">Joseph S. Boyle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kascenas_A/0/1/0/all/0/1\">Antanas Kascenas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lok_P/0/1/0/all/0/1\">Pat Lok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liakata_M/0/1/0/all/0/1\">Maria Liakata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ONeil_A/0/1/0/all/0/1\">Alison Q. O&#x27;Neil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dont Add, dont Miss: Effective Content Preserving Generation from Pre-Selected Text Spans. (arXiv:2310.09017v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.09017","description":"<p>The recently introduced Controlled Text Reduction (CTR) task isolates the\ntext generation step within typical summarization-style tasks. It does so by\nchallenging models to generate coherent text conforming to pre-selected content\nwithin the input text (``highlights''). This framing enables increased\nmodularity in summarization-like tasks, allowing to couple a single CTR model\nwith various content-selection setups and modules. However, there are currently\nno reliable CTR models, while the performance of the existing baseline for the\ntask is mediocre, falling short of practical utility. Here, we address this gap\nby introducing a high-quality, open-source CTR model that tackles two prior key\nlimitations: inadequate enforcement of the content-preservation constraint, and\nsuboptimal silver training data. Addressing these, we amplify the\ncontent-preservation constraint in both training, via RL, and inference, via a\ncontrolled decoding strategy. Further, we substantially improve the silver\ntraining data quality via GPT-4 distillation. Overall, pairing the distilled\ndataset with the highlight-adherence strategies yields marked gains over the\ncurrent baseline, of up to 30 ROUGE-L points, providing a reliable CTR model\nfor downstream use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Slobodkin_A/0/1/0/all/0/1\">Aviv Slobodkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1\">Avi Caciularu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirsch_E/0/1/0/all/0/1\">Eran Hirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1\">Ido Dagan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models. (arXiv:2310.09624v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.09624","description":"<p>As large language models are integrated into society, robustness toward a\nsuite of prompts is increasingly important to maintain reliability in a\nhigh-variance environment.Robustness evaluations must comprehensively\nencapsulate the various settings in which a user may invoke an intelligent\nsystem. This paper proposes ASSERT, Automated Safety Scenario Red Teaming,\nconsisting of three methods -- semantically aligned augmentation, target\nbootstrapping, and adversarial knowledge injection. For robust safety\nevaluation, we apply these methods in the critical domain of AI safety to\nalgorithmically generate a test suite of prompts covering diverse robustness\nsettings -- semantic equivalence, related scenarios, and adversarial. We\npartition our prompts into four safety domains for a fine-grained analysis of\nhow the domain affects model performance. Despite dedicated safeguards in\nexisting state-of-the-art models, we find statistically significant performance\ndifferences of up to 11% in absolute classification accuracy among semantically\nrelated scenarios and error rates of up to 19% absolute error in zero-shot\nadversarial settings, raising concerns for users' physical safety.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mei_A/0/1/0/all/0/1\">Alex Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1\">Sharon Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuning ChatGPT for Automatic Scoring. (arXiv:2310.10072v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.10072","description":"<p>This study highlights the potential of fine-tuned ChatGPT (GPT-3.5) for\nautomatically scoring student written constructed responses using example\nassessment tasks in science education. Recent studies on OpenAI's generative\nmodel GPT-3.5 proved its superiority in predicting the natural language with\nhigh accuracy and human-like responses. GPT-3.5 has been trained over enormous\nonline language materials such as journals and Wikipedia; therefore, more than\ndirect usage of pre-trained GPT-3.5 is required for automatic scoring as\nstudents utilize a different language than trained material. These imply that a\ndomain-specific model, fine-tuned over data for specific tasks, can enhance\nmodel performance. In this study, we fine-tuned GPT-3.5 on six assessment tasks\nwith a diverse dataset of middle-school and high-school student responses and\nexpert scoring. The six tasks comprise two multi-label and four multi-class\nassessment tasks. We compare the performance of fine-tuned GPT-3.5 with the\nfine-tuned state-of-the-art Google's generated language model, BERT. The\nresults show that in-domain training corpora constructed from science questions\nand responses for BERT achieved average accuracy = 0.838, SD = 0.069. GPT-3.5\nshows a remarkable average increase (9.1%) in automatic scoring accuracy (mean\n= 9.15, SD = 0.042) for the six tasks, p =0.001 &lt; 0.05. Specifically, for\nmulti-label tasks (item 1 with 5 labels; item 2 with 10 labels), GPT-3.5\nachieved significantly higher scoring accuracy than BERT across all the labels,\nwith the second item achieving a 7.1% increase. The average scoring increase\nfor the four multi-class items for GPT-3.5 was 10.6% compared to BERT. Our\nstudy confirmed the effectiveness of fine-tuned GPT-3.5 for automatic scoring\nof student responses on domain-specific data in education with high accuracy.\nWe have released fine-tuned models for public use and community engagement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Latif_E/0/1/0/all/0/1\">Ehsan Latif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaoming Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning. (arXiv:2310.11670v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.11670","description":"<p>Parameter-efficient fine-tuning (PEFT) has shown its effectiveness in\nadapting the pre-trained language models to downstream tasks while only\nupdating a small number of parameters. Despite the success, most existing\nmethods independently adapt to each task without considering knowledge transfer\nbetween tasks and are limited to low-data regimes. To overcome this issue, we\npropose Prototype-based HyperAdapter (PHA), a novel framework built on the\nadapter-tuning and hypernetwork. It introduces an instance-dense retriever and\na prototypical hypernetwork to generate the conditional modules in a\nsample-efficient manner. This leads to comparable performance improvements\nagainst existing PEFT methods on multi-task learning and few-shot transfer\nlearning. More importantly, when the available data size gets smaller, our\nmethod outperforms other strong baselines by a large margin. Based on our\nextensive empirical experiments across various datasets, we demonstrate that\nPHA strikes a better trade-off between trainable parameters, accuracy on stream\ntasks, and sample efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhaofeng He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs. (arXiv:2310.11689v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.11689","description":"<p>Large language models (LLMs) have recently shown great advances in a variety\nof tasks, including natural language understanding and generation. However,\ntheir use in high-stakes decision-making scenarios is still limited due to the\npotential for errors. Selective prediction is a technique that can be used to\nimprove the reliability of the LLMs by allowing them to abstain from making\npredictions when they are unsure of the answer. In this work, we propose a\nnovel framework for adaptation with self-evaluation to improve the selective\nprediction performance of LLMs. Our framework is based on the idea of using\nparameter-efficient tuning to adapt the LLM to the specific task at hand while\nimproving its ability to perform self-evaluation. We evaluate our method on a\nvariety of question-answering (QA) datasets and show that it outperforms\nstate-of-the-art selective prediction methods. For example, on the CoQA\nbenchmark, our method improves the AUACC from 91.23% to 92.63% and improves the\nAUROC from 74.61% to 80.25%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jinsung Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1\">Sayna Ebrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arik_S/0/1/0/all/0/1\">Sercan O Arik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Low-resource Fine-grained Named Entity Recognition by Leveraging Coarse-grained Datasets. (arXiv:2310.11715v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.11715","description":"<p>Named Entity Recognition (NER) frequently suffers from the problem of\ninsufficient labeled data, particularly in fine-grained NER scenarios. Although\n$K$-shot learning techniques can be applied, their performance tends to\nsaturate when the number of annotations exceeds several tens of labels. To\novercome this problem, we utilize existing coarse-grained datasets that offer a\nlarge number of annotations. A straightforward approach to address this problem\nis pre-finetuning, which employs coarse-grained data for representation\nlearning. However, it cannot directly utilize the relationships between\nfine-grained and coarse-grained entities, although a fine-grained entity type\nis likely to be a subcategory of a coarse-grained entity type. We propose a\nfine-grained NER model with a Fine-to-Coarse(F2C) mapping matrix to leverage\nthe hierarchical structure explicitly. In addition, we present an inconsistency\nfiltering method to eliminate coarse-grained entities that are inconsistent\nwith fine-grained entity types to avoid performance degradation. Our\nexperimental results show that our method outperforms both $K$-shot learning\nand supervised learning methods when dealing with a small number of\nfine-grained annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Su Ah Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Seokjin Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_W/0/1/0/all/0/1\">Woohwan Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models. (arXiv:2310.11877v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.11877","description":"<p>Large language models (LLMs) have been shown to possess impressive\ncapabilities, while also raising crucial concerns about the faithfulness of\ntheir responses. A primary issue arising in this context is the management of\n(un)answerable queries by LLMs, which often results in hallucinatory behavior\ndue to overconfidence. In this paper, we explore the behavior of LLMs when\npresented with (un)answerable queries. We ask: do models represent the fact\nthat the question is (un)answerable when generating a hallucinatory answer? Our\nresults show strong indications that such models encode the answerability of an\ninput query, with the representation of the first decoded token often being a\nstrong indicator. These findings shed new light on the spatial organization\nwithin the latent representations of LLMs, unveiling previously unexplored\nfacets of these models. Moreover, they pave the way for the development of\nimproved decoding techniques with better adherence to factual generation,\nparticularly in scenarios where query (un)answerability is a concern.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Slobodkin_A/0/1/0/all/0/1\">Aviv Slobodkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldman_O/0/1/0/all/0/1\">Omer Goldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1\">Avi Caciularu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1\">Ido Dagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning for Inference in Dialogue. (arXiv:2310.12467v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.12467","description":"<p>Inference, especially those derived from inductive processes, is a crucial\ncomponent in our conversation to complement the information implicitly or\nexplicitly conveyed by a speaker. While recent large language models show\nremarkable advances in inference tasks, their performance in inductive\nreasoning, where not all information is present in the context, is far behind\ndeductive reasoning. In this paper, we analyze the behavior of the models based\non the task difficulty defined by the semantic information gap -- which\ndistinguishes inductive and deductive reasoning (Johnson-Laird, 1988, 1993).\nOur analysis reveals that the disparity in information between dialogue\ncontexts and desired inferences poses a significant challenge to the inductive\ninference process. To mitigate this information gap, we investigate a\ncontrastive learning approach by feeding negative samples. Our experiments\nsuggest negative samples help models understand what is wrong and improve their\ninference generations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1\">Etsuko Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1\">Bryan Wilie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Ziwei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_W/0/1/0/all/0/1\">Willy Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases. (arXiv:2310.14303v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.14303","description":"<p>Red-teaming has been a widely adopted way to evaluate the harmfulness of\nLarge Language Models (LLMs). It aims to jailbreak a model's safety behavior to\nmake it act as a helpful agent disregarding the harmfulness of the query.\nExisting methods are primarily based on input text-based red-teaming such as\nadversarial prompts, low-resource prompts, or contextualized prompts to\ncondition the model in a way to bypass its safe behavior. Bypassing the\nguardrails uncovers hidden harmful information and biases in the model that are\nleft untreated or newly introduced by its safety training. However,\nprompt-based attacks fail to provide such a diagnosis owing to their low attack\nsuccess rate, and applicability to specific models. In this paper, we present a\nnew perspective on LLM safety research i.e., parametric red-teaming through\nUnalignment. It simply (instruction) tunes the model parameters to break model\nguardrails that are not deeply rooted in the model's behavior. Unalignment\nusing as few as 100 examples can significantly bypass commonly referred to as\nCHATGPT, to the point where it responds with an 88% success rate to harmful\nqueries on two safety benchmark datasets. On open-source models such as\nVICUNA-7B and LLAMA-2-CHAT 7B AND 13B, it shows an attack success rate of more\nthan 91%. On bias evaluations, Unalignment exposes inherent biases in\nsafety-aligned models such as CHATGPT and LLAMA- 2-CHAT where the model's\nresponses are strongly biased and opinionated 64% of the time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhardwaj_R/0/1/0/all/0/1\">Rishabh Bhardwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Chaos to Clarity: Claim Normalization to Empower Fact-Checking. (arXiv:2310.14338v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.14338","description":"<p>With the rise of social media, users are exposed to many misleading claims.\nHowever, the pervasive noise inherent in these posts presents a challenge in\nidentifying precise and prominent claims that require verification. Extracting\nthe important claims from such posts is arduous and time-consuming, yet it is\nan underexplored problem. Here, we aim to bridge this gap. We introduce a novel\ntask, Claim Normalization (aka ClaimNorm), which aims to decompose complex and\nnoisy social media posts into more straightforward and understandable forms,\ntermed normalized claims. We propose CACN, a pioneering approach that leverages\nchain-of-thought and claim check-worthiness estimation, mimicking human\nreasoning processes, to comprehend intricate claims. Moreover, we capitalize on\nthe in-context learning capabilities of large language models to provide\nguidance and to improve claim normalization. To evaluate the effectiveness of\nour proposed model, we meticulously compile a comprehensive real-world dataset,\nCLAN, comprising more than 6k instances of social media posts alongside their\nrespective normalized claims. Our experiments demonstrate that CACN outperforms\nseveral baselines across various evaluation measures. Finally, our rigorous\nerror analysis validates CACN's capabilities and pitfalls.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sundriyal_M/0/1/0/all/0/1\">Megha Sundriyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TATA: Stance Detection via Topic-Agnostic and Topic-Aware Embeddings. (arXiv:2310.14450v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.14450","description":"<p>Stance detection is important for understanding different attitudes and\nbeliefs on the Internet. However, given that a passage's stance toward a given\ntopic is often highly dependent on that topic, building a stance detection\nmodel that generalizes to unseen topics is difficult. In this work, we propose\nusing contrastive learning as well as an unlabeled dataset of news articles\nthat cover a variety of different topics to train topic-agnostic/TAG and\ntopic-aware/TAW embeddings for use in downstream stance detection. Combining\nthese embeddings in our full TATA model, we achieve state-of-the-art\nperformance across several public stance detection datasets (0.771 $F_1$-score\non the Zero-shot VAST dataset). We release our code and data at\nhttps://github.com/hanshanley/tata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hanley_H/0/1/0/all/0/1\">Hans W. A. Hanley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durumeric_Z/0/1/0/all/0/1\">Zakir Durumeric</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Interplay between Fairness and Explainability. (arXiv:2310.16607v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.16607","description":"<p>In order to build reliable and trustworthy NLP applications, models need to\nbe both fair across different demographics and explainable. Usually these two\nobjectives, fairness and explainability, are optimized and/or examined\nindependently of each other. Instead, we argue that forthcoming, trustworthy\nNLP systems should consider both. In this work, we perform a first study to\nunderstand how they influence each other: do fair(er) models rely on more\nplausible rationales? and vice versa. To this end, we conduct experiments on\ntwo English multi-class text classification datasets, BIOS and ECtHR, that\nprovide information on gender and nationality, respectively, as well as\nhuman-annotated rationales. We fine-tune pre-trained language models with\nseveral methods for (i) bias mitigation, which aims to improve fairness; (ii)\nrationale extraction, which aims to produce plausible explanations. We find\nthat bias mitigation algorithms do not always lead to fairer models. Moreover,\nwe discover that empirical fairness and explainability are orthogonal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brandl_S/0/1/0/all/0/1\">Stephanie Brandl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1\">Emanuele Bugliarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Managing AI Risks in an Era of Rapid Progress. (arXiv:2310.17688v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2310.17688","description":"<p>In this short consensus paper, we outline risks from upcoming, advanced AI\nsystems. We examine large-scale social harms and malicious uses, as well as an\nirreversible loss of human control over autonomous AI systems. In light of\nrapid and continuing AI progress, we propose urgent priorities for AI R&amp;D and\ngovernance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinton_G/0/1/0/all/0/1\">Geoffrey Hinton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Andrew Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harari_Y/0/1/0/all/0/1\">Yuval Noah Harari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya-Qin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1\">Lan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalev_Shwartz_S/0/1/0/all/0/1\">Shai Shalev-Shwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadfield_G/0/1/0/all/0/1\">Gillian Hadfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1\">Jeff Clune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maharaj_T/0/1/0/all/0/1\">Tegan Maharaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1\">Frank Hutter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baydin_A/0/1/0/all/0/1\">At&#x131;l&#x131;m G&#xfc;ne&#x15f; Baydin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McIlraith_S/0/1/0/all/0/1\">Sheila McIlraith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1\">Ashwin Acharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krueger_D/0/1/0/all/0/1\">David Krueger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1\">Anca Dragan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1\">Stuart Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahneman_D/0/1/0/all/0/1\">Daniel Kahneman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1\">Jan Brauner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mindermann_S/0/1/0/all/0/1\">S&#xf6;ren Mindermann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleBART: Decorate Pretrained Model with Style Adapters for Unsupervised Stylistic Headline Generation. (arXiv:2310.17743v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.17743","description":"<p>Stylistic headline generation is the task to generate a headline that not\nonly summarizes the content of an article, but also reflects a desired style\nthat attracts users. As style-specific article-headline pairs are scarce,\nprevious researches focus on unsupervised approaches with a standard headline\ngeneration dataset and mono-style corpora. In this work, we follow this line\nand propose StyleBART, an unsupervised approach for stylistic headline\ngeneration. Our method decorates the pretrained BART model with adapters that\nare responsible for different styles and allows the generation of headlines\nwith diverse styles by simply switching the adapters. Different from previous\nworks, StyleBART separates the task of style learning and headline generation,\nmaking it possible to freely combine the base model and the style adapters\nduring inference. We further propose an inverse paraphrasing task to enhance\nthe style adapters. Extensive automatic and human evaluations show that\nStyleBART achieves new state-of-the-art performance in the unsupervised\nstylistic headline generation task, producing high-quality headlines with the\ndesired style.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yajing Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_B/0/1/0/all/0/1\">Boya Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanhua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpinSummEval: Revisiting Automated Evaluation for Opinion Summarization. (arXiv:2310.18122v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.18122","description":"<p>Opinion summarization sets itself apart from other types of summarization\ntasks due to its distinctive focus on aspects and sentiments. Although certain\nautomated evaluation methods like ROUGE have gained popularity, we have found\nthem to be unreliable measures for assessing the quality of opinion summaries.\nIn this paper, we present OpinSummEval, a dataset comprising human judgments\nand outputs from 14 opinion summarization models. We further explore the\ncorrelation between 24 automatic metrics and human ratings across four\ndimensions. Our findings indicate that metrics based on neural networks\ngenerally outperform non-neural ones. However, even metrics built on powerful\nbackbones, such as BART and GPT-3/3.5, do not consistently correlate well\nacross all dimensions, highlighting the need for advancements in automated\nevaluation methods for opinion summarization. The code and data are publicly\navailable at https://github.com/A-Chicharito-S/OpinSummEval/tree/main.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuchen Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Not Harm Protected Groups in Debiasing Language Representation Models. (arXiv:2310.18458v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.18458","description":"<p>Language Representation Models (LRMs) trained with real-world data may\ncapture and exacerbate undesired bias and cause unfair treatment of people in\nvarious demographic groups. Several techniques have been investigated for\napplying interventions to LRMs to remove bias in benchmark evaluations on, for\nexample, word embeddings. However, the negative side effects of debiasing\ninterventions are usually not revealed in the downstream tasks. We propose\nxGAP-DEBIAS, a set of evaluations on assessing the fairness of debiasing. In\nthis work, We examine four debiasing techniques on a real-world text\nclassification task and show that reducing biasing is at the cost of degrading\nperformance for all demographic groups, including those the debiasing\ntechniques aim to protect. We advocate that a debiasing technique should have\ngood downstream performance with the constraint of ensuring no harm to the\nprotected group.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chloe Qinyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stureborg_R/0/1/0/all/0/1\">Rickard Stureborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fain_B/0/1/0/all/0/1\">Brandon Fain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion. (arXiv:2310.19056v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2310.19056","description":"<p>Query expansion is a commonly-used technique in many search systems to better\nrepresent users' information needs with additional query terms. Existing\nstudies for this task usually propose to expand a query with retrieved or\ngenerated contextual documents. However, both types of methods have clear\nlimitations. For retrieval-based methods, the documents retrieved with the\noriginal query might not be accurate enough to reveal the search intent,\nespecially when the query is brief or ambiguous. For generation-based methods,\nexisting models can hardly be trained or aligned on a particular corpus, due to\nthe lack of corpus-specific labeled data. In this paper, we propose a novel\nLarge Language Model (LLM) based mutual verification framework for query\nexpansion, which alleviates the aforementioned limitations. Specifically, we\nfirst design a query-query-document generation pipeline, which can effectively\nleverage the contextual knowledge encoded in LLMs to generate sub-queries and\ncorresponding documents from multiple perspectives. Next, we employ a mutual\nverification method for both generated and retrieved contextual documents,\nwhere 1) retrieved documents are filtered with the external contextual\nknowledge in generated documents, and 2) generated documents are filtered with\nthe corpus-specific knowledge in retrieved documents. Overall, the proposed\nmethod allows retrieved and generated documents to complement each other to\nfinalize a better query expansion. We conduct extensive experiments on three\ninformation retrieval datasets, i.e., TREC-DL-2020, TREC-COVID, and MSMARCO.\nThe results demonstrate that our method outperforms other baselines\nsignificantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_P/0/1/0/all/0/1\">Pengyue Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiding Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiangyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaopeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1\">Changying Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuaiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dawei Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Pre-trained Language Model into Neural Machine Translation. (arXiv:2310.19680v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.19680","description":"<p>Neural Machine Translation (NMT) has become a significant technology in\nnatural language processing through extensive research and development.\nHowever, the deficiency of high-quality bilingual language pair data still\nposes a major challenge to improving NMT performance. Recent studies are\nexploring the use of contextual information from pre-trained language model\n(PLM) to address this problem. Yet, the issue of incompatibility between PLM\nand NMT model remains unresolved. This study proposes a PLM-integrated NMT\n(PiNMT) model to overcome the identified problems. The PiNMT model consists of\nthree critical components, PLM Multi Layer Converter, Embedding Fusion, and\nCosine Alignment, each playing a vital role in providing effective PLM\ninformation to NMT. Furthermore, two training strategies, Separate Learning\nRates and Dual Step Training, are also introduced in this paper. By\nimplementing the proposed PiNMT model and training strategy, we achieved\nstate-of-the-art performance on the IWSLT'14 En$\\leftrightarrow$De dataset.\nThis study's outcomes are noteworthy as they demonstrate a novel approach for\nefficiently integrating PLM with NMT to overcome incompatibility and enhance\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Soon-Jae Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_C/0/1/0/all/0/1\">Chang-Sung Jeong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COPAL-ID: Indonesian Language Reasoning with Local Culture and Nuances. (arXiv:2311.01012v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.01012","description":"<p>We present publicly available COPAL-ID, a novel Indonesian language common\nsense reasoning dataset. Unlike the previous Indonesian COPA dataset\n(XCOPA-ID), COPAL-ID incorporates Indonesian local and cultural nuances, and\ntherefore, provides a more natural portrayal of day-to-day causal reasoning\nwithin the Indonesian cultural sphere. Professionally written by natives from\nscratch, COPAL-ID is more fluent and free from awkward phrases, unlike the\ntranslated XCOPA-ID. In addition, we present COPAL-ID in both standard\nIndonesian and in Jakartan Indonesian--a dialect commonly used in daily\nconversation. COPAL-ID poses a greater challenge for existing open-sourced and\nclosed state-of-the-art multilingual language models, yet is trivially easy for\nhumans. Our findings suggest that even the current best open-source,\nmultilingual model struggles to perform well, achieving 65.47% accuracy on\nCOPAL-ID, significantly lower than on the culturally-devoid XCOPA-ID (79.40%).\nDespite GPT-4's impressive score, it suffers the same performance degradation\ncompared to its XCOPA-ID score, and it still falls short of human performance.\nThis shows that these language models are still way behind in comprehending the\nlocal nuances of Indonesian.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wibowo_H/0/1/0/all/0/1\">Haryo Akbarianto Wibowo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuadi_E/0/1/0/all/0/1\">Erland Hilman Fuadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nityasya_M/0/1/0/all/0/1\">Made Nindyatama Nityasya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasojo_R/0/1/0/all/0/1\">Radityo Eko Prasojo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models. (arXiv:2311.01305v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2311.01305","description":"<p>Large language models(LLMs) exhibit excellent performance across a variety of\ntasks, but they come with significant computational and storage costs.\nQuantizing these models is an effective way to alleviate this issue. However,\nexisting methods struggle to strike a balance between model accuracy and\nhardware efficiency. This is where we introduce AWEQ, a post-training method\nthat requires no additional training overhead. AWEQ excels in both\nultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization.\nThere is an observation that weight quantization is less challenging than\nactivation quantization. AWEQ transfers the difficulty of activation\nquantization to weights using channel equalization, achieving a balance between\nthe quantization difficulties of both, and thereby maximizing performance. We\nhave further refined the equalization method to mitigate quantization bias\nerror, ensuring the robustness of the model. Extensive experiments on popular\nmodels such as LLaMA and OPT demonstrate that AWEQ outperforms all existing\npost-training quantization methods for large models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baisong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingwang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haixiao Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization. (arXiv:2311.01544v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.01544","description":"<p>Large Language Models (LLMs) have reshaped natural language processing with\ntheir impressive capabilities. Their ever-increasing size, however, raised\nconcerns about their effective deployment and the need for LLM compressions.\nThis study introduces the Divergent Token metrics (DTMs), a novel approach for\nassessing compressed LLMs, addressing the limitations of traditional perplexity\nor accuracy measures that fail to accurately reflect text generation quality.\nDTMs focus on token divergence, that allow deeper insights into the subtleties\nof model compression, i.p. when evaluating component's impacts individually.\nUtilizing the First Divergent Token metric (FDTM) in model sparsification\nreveals that a quarter of all attention components can be pruned beyond 90% on\nthe Llama-2 model family, still keeping SOTA performance. For quantization FDTM\nsuggests that over 80% of parameters can naively be transformed to int8 without\nspecial outlier management. These evaluations indicate the necessity of\nchoosing appropriate compressions for parameters individually-and that FDTM can\nidentify those-while standard metrics result in deteriorated outcomes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deiseroth_B/0/1/0/all/0/1\">Bj&#xf6;rn Deiseroth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuer_M/0/1/0/all/0/1\">Max Meuer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritsch_N/0/1/0/all/0/1\">Nikolas Gritsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eichenberg_C/0/1/0/all/0/1\">Constantin Eichenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1\">Patrick Schramowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assenmacher_M/0/1/0/all/0/1\">Matthias A&#xdf;enmacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models. (arXiv:2311.01732v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.01732","description":"<p>Large Language Models (LLMs) have significantly advanced the field of Natural\nLanguage Processing (NLP), but their lack of interpretability has been a major\nconcern. Current methods for interpreting LLMs are post hoc, applied after\ninference time, and have limitations such as their focus on low-level features\nand lack of explainability at higher level text units. In this work, we\nintroduce proto-lm, a prototypical network-based white-box framework that\nallows LLMs to learn immediately interpretable embeddings during the\nfine-tuning stage while maintaining competitive performance. Our method's\napplicability and interpretability are demonstrated through experiments on a\nwide range of NLP tasks, and our results indicate a new possibility of creating\ninterpretable models without sacrificing performance. This novel approach to\ninterpretability in LLMs can pave the way for more interpretable models without\nthe need to sacrifice performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sean Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassanpour_S/0/1/0/all/0/1\">Saeed Hassanpour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Citance-Contextualized Summarization of Scientific Papers. (arXiv:2311.02408v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.02408","description":"<p>Current approaches to automatic summarization of scientific papers generate\ninformative summaries in the form of abstracts. However, abstracts are not\nintended to show the relationship between a paper and the references cited in\nit. We propose a new contextualized summarization approach that can generate an\ninformative summary conditioned on a given sentence containing the citation of\na reference (a so-called \"citance\"). This summary outlines the content of the\ncited paper relevant to the citation location. Thus, our approach extracts and\nmodels the citances of a paper, retrieves relevant passages from cited papers,\nand generates abstractive summaries tailored to each citance. We evaluate our\napproach using $\\textbf{Webis-Context-SciSumm-2023}$, a new dataset containing\n540K~computer science papers and 4.6M~citances therein.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Syed_S/0/1/0/all/0/1\">Shahbaz Syed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakimi_A/0/1/0/all/0/1\">Ahmad Dawar Hakimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Khatib_K/0/1/0/all/0/1\">Khalid Al-Khatib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChaTA: Towards an Intelligent Question-Answer Teaching Assistant using Open-Source LLMs. (arXiv:2311.02775v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2311.02775","description":"<p>Responding to the thousands of student questions on online QA platforms each\nsemester has a considerable human cost, particularly in computing courses with\nrapidly growing enrollments. To address the challenges of scalable and\nintelligent question-answering (QA), we introduce an innovative solution that\nleverages open-source Large Language Models (LLMs) from the LLaMA-2 family to\nensure data privacy. Our approach combines augmentation techniques such as\nretrieval augmented generation (RAG), supervised fine-tuning (SFT), and\nlearning from human preferences data using Direct Preference Optimization\n(DPO). Through extensive experimentation on a Piazza dataset from an\nintroductory CS course, comprising 10,000 QA pairs and 1,500 pairs of\npreference data, we demonstrate a significant 30% improvement in the quality of\nanswers, with RAG being a particularly impactful addition. Our contributions\ninclude the development of a novel architecture for educational QA, extensive\nevaluations of LLM performance utilizing both human assessments and LLM-based\nmetrics, and insights into the challenges and future directions of educational\ndata processing. This work paves the way for the development of CHATA, an\nintelligent QA assistant customizable for courses with an online QA platform\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hicke_Y/0/1/0/all/0/1\">Yann Hicke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Anmol Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qianou Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denny_P/0/1/0/all/0/1\">Paul Denny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NExT-Chat: An LMM for Chat, Detection and Segmentation. (arXiv:2311.04498v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2311.04498","description":"<p>The development of large language models (LLMs) has greatly advanced the\nfield of multimodal understanding, leading to the emergence of large multimodal\nmodels (LMMs). In order to enhance the level of visual comprehension, recent\nstudies have equipped LMMs with region-level understanding capabilities by\nrepresenting object bounding box coordinates as a series of text sequences\n(pixel2seq). In this paper, we introduce a novel paradigm for object location\nmodeling called pixel2emb method, where we ask the LMM to output the location\nembeddings and then decoded by different decoders. This paradigm allows for\ndifferent location formats (such as bounding boxes and masks) to be used in\nmultimodal conversations Furthermore, this kind of embedding based location\nmodeling enables the utilization of existing practices in localization tasks,\nsuch as detection and segmentation. In scenarios with limited resources, our\npixel2emb demonstrates superior performance compared to existing\nstate-of-the-art (SOTA) approaches in both the location input and output tasks\nunder fair comparison. Leveraging the proposed pixel2emb method, we train an\nLMM named NExT-Chat and demonstrate its capability of handling multiple tasks\nlike visual grounding, region caption, and grounded reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Benchmark and Contamination for Language Models with Rephrased Samples. (arXiv:2311.04850v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.04850","description":"<p>Large language models are increasingly trained on all the data ever produced\nby humans. Many have raised concerns about the trustworthiness of public\nbenchmarks due to potential contamination in pre-training or fine-tuning\ndatasets. While most data decontamination efforts apply string matching (e.g.,\nn-gram overlap) to remove benchmark data, we show that these methods are\ninsufficient, and simple variations of test data (e.g., paraphrasing,\ntranslation) can easily bypass these decontamination measures. Furthermore, we\ndemonstrate that if such variation of test data is not eliminated, a 13B model\ncan easily overfit a test benchmark and achieve drastically high performance,\non par with GPT-4. We validate such observations in widely used benchmarks such\nas MMLU, GSK8k, and HumanEval. To address this growing risk, we propose a\nstronger LLM-based decontamination method and apply it to widely used\npre-training and fine-tuning datasets, revealing significant previously unknown\ntest overlap. For example, in pre-training sets such as RedPajama-Data-1T and\nStarCoder-Data, we identified that 8-18\\% of the HumanEval benchmark overlaps.\nInterestingly, we also find such contamination in synthetic dataset generated\nby GPT-3.5/4, suggesting a potential risk of unintentional contamination. We\nurge the community to adopt stronger decontamination approaches when using\npublic benchmarks. Moreover, we call for the community to actively develop\nfresh one-time exams to evaluate models accurately. Our decontamination tool is\npublicly available at https://github.com/lm-sys/llm-decontaminator.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_W/0/1/0/all/0/1\">Wei-Lin Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lianmin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1\">Ion Stoica</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Improved Transformer-based Model for Detecting Phishing, Spam, and Ham: A Large Language Model Approach. (arXiv:2311.04913v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.04913","description":"<p>Phishing and spam detection is long standing challenge that has been the\nsubject of much academic research. Large Language Models (LLM) have vast\npotential to transform society and provide new and innovative approaches to\nsolve well-established challenges. Phishing and spam have caused financial\nhardships and lost time and resources to email users all over the world and\nfrequently serve as an entry point for ransomware threat actors. While\ndetection approaches exist, especially heuristic-based approaches, LLMs offer\nthe potential to venture into a new unexplored area for understanding and\nsolving this challenge. LLMs have rapidly altered the landscape from business,\nconsumers, and throughout academia and demonstrate transformational potential\nfor the potential of society. Based on this, applying these new and innovative\napproaches to email detection is a rational next step in academic research. In\nthis work, we present IPSDM, our model based on fine-tuning the BERT family of\nmodels to specifically detect phishing and spam email. We demonstrate our\nfine-tuned version, IPSDM, is able to better classify emails in both unbalanced\nand balanced datasets. This work serves as an important first step towards\nemploying LLMs to improve the security of our information systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jamal_S/0/1/0/all/0/1\">Suhaima Jamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wimmer_H/0/1/0/all/0/1\">Hayden Wimmer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Removing RLHF Protections in GPT-4 via Fine-Tuning. (arXiv:2311.05553v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.05553","description":"<p>As large language models (LLMs) have increased in their capabilities, so does\ntheir potential for dual use. To reduce harmful outputs, produces and vendors\nof LLMs have used reinforcement learning with human feedback (RLHF). In tandem,\nLLM vendors have been increasingly enabling fine-tuning of their most powerful\nmodels. However, concurrent work has shown that fine-tuning can remove RLHF\nprotections. We may expect that the most powerful models currently available\n(GPT-4) are less susceptible to fine-tuning attacks.\n</p>\n<p>In this work, we show the contrary: fine-tuning allows attackers to remove\nRLHF protections with as few as 340 examples and a 95% success rate. These\ntraining examples can be automatically generated with weaker models. We further\nshow that removing RLHF protections does not decrease usefulness on\nnon-censored outputs, providing evidence that our fine-tuning strategy does not\ndecrease usefulness despite using weaker models to generate training data. Our\nresults show the need for further research on protections on LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Q/0/1/0/all/0/1\">Qiusi Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1\">Richard Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bindu_R/0/1/0/all/0/1\">Rohan Bindu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Daniel Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild. (arXiv:2311.06237v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.06237","description":"<p>Engaging in the deliberate generation of abnormal outputs from large language\nmodels (LLMs) by attacking them is a novel human activity. This paper presents\na thorough exposition of how and why people perform such attacks. Using a\nformal qualitative methodology, we interviewed dozens of practitioners from a\nbroad range of backgrounds, all contributors to this novel work of attempting\nto cause LLMs to fail. We relate and connect this activity between its\npractitioners' motivations and goals; the strategies and techniques they\ndeploy; and the crucial role the community plays. As a result, this paper\npresents a grounded theory of how and why people attack large language models:\nLLM red teaming in the wild.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inie_N/0/1/0/all/0/1\">Nanna Inie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stray_J/0/1/0/all/0/1\">Jonathan Stray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derczynski_L/0/1/0/all/0/1\">Leon Derczynski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time Travel in LLMs: Tracing Data Contamination in Large Language Models. (arXiv:2308.08493v2 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2308.08493","description":"<p>Data contamination, i.e., the presence of test data from downstream tasks in\nthe training data of large language models (LLMs), is a potential major issue\nin measuring LLMs' real effectiveness on other tasks. We propose a\nstraightforward yet effective method for identifying data contamination within\nLLMs. At its core, our approach starts by identifying potential contamination\nat the instance level; using this information, our approach then assesses wider\ncontamination at the partition level. To estimate contamination of individual\ninstances, we employ \"guided instruction:\" a prompt consisting of the dataset\nname, partition type, and the random-length initial segment of a reference\ninstance, asking the LLM to complete it. An instance is flagged as contaminated\nif the LLM's output either exactly or nearly matches the latter segment of the\nreference. To understand if an entire partition is contaminated, we propose two\nideas. The first idea marks a dataset partition as contaminated if the average\noverlap score with the reference instances (as measured by ROUGE-L or BLEURT)\nis statistically significantly better with the completions from guided\ninstruction compared to a \"general instruction\" that does not include the\ndataset and partition name. The second idea marks a dataset partition as\ncontaminated if a classifier based on GPT-4 with few-shot in-context learning\nprompt marks multiple generated completions as exact/near-exact matches of the\ncorresponding reference instances. Our best method achieves an accuracy between\n92% and 100% in detecting if an LLM is contaminated with seven datasets,\ncontaining train and test/validation partitions, when contrasted with manual\nevaluation by human experts. Further, our findings indicate that GPT-4 is\ncontaminated with AG News, WNLI, and XSum datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Golchin_S/0/1/0/all/0/1\">Shahriar Golchin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1\">Mihai Surdeanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-11-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}