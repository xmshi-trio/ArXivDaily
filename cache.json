{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-03-01T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Phone and speaker spatial organization in self-supervised speech representations. (arXiv:2302.14055v1 [cs.SD])","link":"http://arxiv.org/abs/2302.14055","description":"<p>Self-supervised representations of speech are currently being widely used for\na large number of applications. Recently, some efforts have been made in trying\nto analyze the type of information present in each of these representations.\nMost such work uses downstream models to test whether the representations can\nbe successfully used for a specific task. The downstream models, though,\ntypically perform nonlinear operations on the representation extracting\ninformation that may not have been readily available in the original\nrepresentation. In this work, we analyze the spatial organization of phone and\nspeaker information in several state-of-the-art speech representations using\nmethods that do not require a downstream model. We measure how different layers\nencode basic acoustic parameters such as formants and pitch using\nrepresentation similarity analysis. Further, we study the extent to which each\nrepresentation clusters the speech samples by phone or speaker classes using\nnon-parametric statistical testing. Our results indicate that models represent\nthese speech attributes differently depending on the target task used during\npretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Riera_P/0/1/0/all/0/1\">Pablo Riera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cerdeiro_M/0/1/0/all/0/1\">Manuela Cerdeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pepino_L/0/1/0/all/0/1\">Leonardo Pepino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_L/0/1/0/all/0/1\">Luciana Ferrer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-modal Contrastive Learning for Multimodal Fake News Detection. (arXiv:2302.14057v1 [cs.LG])","link":"http://arxiv.org/abs/2302.14057","description":"<p>Automatic detection of multimodal fake news has gained a widespread attention\nrecently. Many existing approaches seek to fuse unimodal features to produce\nmultimodal news representations. However, the potential of powerful cross-modal\ncontrastive learning methods for fake news detection has not been well\nexploited. Besides, how to aggregate features from different modalities to\nboost the performance of the decision-making process is still an open question.\nTo address that, we propose COOLANT, a cross-modal contrastive learning\nframework for multimodal fake news detection, aiming to achieve more accurate\nimage-text alignment. To further improve the alignment precision, we leverage\nan auxiliary task to soften the loss term of negative samples during the\ncontrast process. A cross-modal fusion module is developed to learn the\ncross-modality correlations. An attention mechanism with an attention guidance\nmodule is implemented to help effectively and interpretably aggregate the\naligned unimodal representations and the cross-modality correlations. Finally,\nwe evaluate the COOLANT and conduct a comparative study on two widely used\ndatasets, Twitter and Weibo. The experimental results demonstrate that our\nCOOLANT outperforms previous approaches by a large margin and achieves new\nstate-of-the-art results on the two datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longzheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chuang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongbo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaohan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siqi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explanations for Automatic Speech Recognition. (arXiv:2302.14062v1 [cs.SD])","link":"http://arxiv.org/abs/2302.14062","description":"<p>We address quality assessment for neural network based ASR by providing\nexplanations that help increase our understanding of the system and ultimately\nhelp build trust in the system. Compared to simple classification labels,\nexplaining transcriptions is more challenging as judging their correctness is\nnot straightforward and transcriptions as a variable-length sequence is not\nhandled by existing interpretable machine learning models. We provide an\nexplanation for an ASR transcription as a subset of audio frames that is both a\nminimal and sufficient cause of the transcription. To do this, we adapt\nexisting explainable AI (XAI) techniques from image classification-Statistical\nFault Localisation(SFL) and Causal. Additionally, we use an adapted version of\nLocal Interpretable Model-Agnostic Explanations (LIME) for ASR as a baseline in\nour experiments. We evaluate the quality of the explanations generated by the\nproposed techniques over three different ASR ,Google API, the baseline model of\nSphinx, Deepspeech and 100 audio samples from the Commonvoice dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaoliang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bell_P/0/1/0/all/0/1\">Peter Bell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_A/0/1/0/all/0/1\">Ajitha Rajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning. (arXiv:2302.14115v1 [cs.CV])","link":"http://arxiv.org/abs/2302.14115","description":"<p>In this work, we introduce Vid2Seq, a multi-modal single-stage dense event\ncaptioning model pretrained on narrated videos which are readily-available at\nscale. The Vid2Seq architecture augments a language model with special time\ntokens, allowing it to seamlessly predict event boundaries and textual\ndescriptions in the same output sequence. Such a unified model requires\nlarge-scale training data, which is not available in current annotated\ndatasets. We show that it is possible to leverage unlabeled narrated videos for\ndense video captioning, by reformulating sentence boundaries of transcribed\nspeech as pseudo event boundaries, and using the transcribed speech sentences\nas pseudo event captions. The resulting Vid2Seq model pretrained on the\nYT-Temporal-1B dataset improves the state of the art on a variety of dense\nvideo captioning benchmarks including YouCook2, ViTT and ActivityNet Captions.\nVid2Seq also generalizes well to the video paragraph captioning task and the\nstandard task of video clip captioning. Our code and models will be publicly\nreleased at https://antoyang.github.io/vid2seq.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Antoine Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagrani_A/0/1/0/all/0/1\">Arsha Nagrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_P/0/1/0/all/0/1\">Paul Hongsuck Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miech_A/0/1/0/all/0/1\">Antoine Miech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pont_Tuset_J/0/1/0/all/0/1\">Jordi Pont-Tuset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">Josef Sivic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Pruning of Self-Supervised Pre-trained Models for Speech Recognition and Understanding. (arXiv:2302.14132v1 [cs.CL])","link":"http://arxiv.org/abs/2302.14132","description":"<p>Self-supervised speech representation learning (SSL) has shown to be\neffective in various downstream tasks, but SSL models are usually large and\nslow. Model compression techniques such as pruning aim to reduce the model size\nand computation without degradation in accuracy. Prior studies focus on the\npruning of Transformers; however, speech models not only utilize a stack of\nTransformer blocks, but also combine a frontend network based on multiple\nconvolutional layers for low-level feature representation learning. This\nfrontend has a small size but a heavy computational cost. In this work, we\npropose three task-specific structured pruning methods to deal with such\nheterogeneous networks. Experiments on LibriSpeech and SLURP show that the\nproposed method is more accurate than the original wav2vec2-base with 10% to\n30% less computation, and is able to reduce the computation by 40% to 50%\nwithout any degradation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kwangyoun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Felix Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_P/0/1/0/all/0/1\">Prashant Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TabGenie: A Toolkit for Table-to-Text Generation. (arXiv:2302.14169v1 [cs.CL])","link":"http://arxiv.org/abs/2302.14169","description":"<p>Heterogenity of data-to-text generation datasets limits the research on\ndata-to-text generation systems. We present TabGenie - a toolkit which enables\nresearchers to explore, preprocess, and analyze a variety of data-to-text\ngeneration datasets through the unified framework of table-to-text generation.\nIn TabGenie, all the inputs are represented as tables with associated metadata.\nThe tables can be explored through the web interface, which also provides an\ninteractive mode for debugging table-to-text generation, facilitates\nside-by-side comparison of generated system outputs, and allows easy exports\nfor manual analysis. Furthermore, TabGenie is equipped with command line\nprocessing tools and Python bindings for unified dataset loading and\nprocessing. We release TabGenie as a PyPI package and provide its open-source\ncode and a live demo at https://github.com/kasnerz/tabgenie.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kasner_Z/0/1/0/all/0/1\">Zden&#x11b;k Kasner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garanina_E/0/1/0/all/0/1\">Ekaterina Garanina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Platek_O/0/1/0/all/0/1\">Ond&#x159;ej Pl&#xe1;tek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1\">Ond&#x159;ej Du&#x161;ek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Character-level Translations Worth the Wait? An Extensive Comparison of Character- and Subword-level Models for Machine Translation. (arXiv:2302.14220v1 [cs.CL])","link":"http://arxiv.org/abs/2302.14220","description":"<p>Pretrained large character-level language models have been recently\nrevitalized and shown to be competitive with subword models across a range of\nNLP tasks. However, there has not been any research showing their effectiveness\nin neural machine translation (NMT). This work performs an extensive comparison\nacross multiple languages and experimental conditions of state-of-the-art\ncharacter- and subword-level pre-trained models (ByT5 and mT5, respectively) on\nNMT, and shows that the former not only are effective in translation, but\nfrequently outperform subword models, particularly in cases where training data\nis limited. The only drawback of character models appears to be their\ninefficiency (at least 4 times slower to train and for inference). Further\nanalysis indicates that character models are capable of implicitly translating\non the word or subword level, thereby nullifying a major potential weakness of\noperating on the character level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Edman_L/0/1/0/all/0/1\">Lukas Edman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toral_A/0/1/0/all/0/1\">Antonio Toral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noord_G/0/1/0/all/0/1\">Gertjan van Noord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weighted Sampling for Masked Language Modeling. (arXiv:2302.14225v1 [cs.CL])","link":"http://arxiv.org/abs/2302.14225","description":"<p>Masked Language Modeling (MLM) is widely used to pretrain language models.\nThe standard random masking strategy in MLM causes the pre-trained language\nmodels (PLMs) to be biased toward high-frequency tokens. Representation\nlearning of rare tokens is poor and PLMs have limited performance on downstream\ntasks. To alleviate this frequency bias issue, we propose two simple and\neffective Weighted Sampling strategies for masking tokens based on the token\nfrequency and training loss. We apply these two strategies to BERT and obtain\nWeighted-Sampled BERT (WSBERT). Experiments on the Semantic Textual Similarity\nbenchmark (STS) show that WSBERT significantly improves sentence embeddings\nover BERT. Combining WSBERT with calibration methods and prompt learning\nfurther improves sentence embeddings. We also investigate fine-tuning WSBERT on\nthe GLUE benchmark and show that Weighted Sampling also improves the transfer\nlearning capability of the backbone PLM. We further analyze and provide\ninsights into how WSBERT improves token embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_K/0/1/0/all/0/1\">Kongzhang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual Summarization via ChatGPT. (arXiv:2302.14229v1 [cs.CL])","link":"http://arxiv.org/abs/2302.14229","description":"<p>Given a document in a source language, cross-lingual summarization (CLS) aims\nto generate a summary in a different target language. Recently, the emergence\nof ChatGPT has attracted wide attention from the computational linguistics\ncommunity. However, it is not yet known the performance of ChatGPT on CLS. In\nthis report, we empirically use various prompts to guide ChatGPT to perform\nzero-shot CLS from different paradigms (i.e., end-to-end and pipeline), and\nprovide a preliminary evaluation on its generated summaries.We find that\nChatGPT originally prefers to produce lengthy summaries with more detailed\ninformation. But with the help of an interactive prompt, ChatGPT can balance\nbetween informativeness and conciseness, and significantly improve its CLS\nperformance. Experimental results on three widely-used CLS datasets show that\nChatGPT outperforms the advanced GPT 3.5 model (i.e., text-davinci-003). In\naddition, we provide qualitative case studies to show the superiority of\nChatGPT on CLS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1\">Jianfeng Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Goal Driven Discovery of Distributional Differences via Language Descriptions. (arXiv:2302.14233v1 [cs.CL])","link":"http://arxiv.org/abs/2302.14233","description":"<p>Mining large corpora can generate useful discoveries but is time-consuming\nfor humans. We formulate a new task, D5, that automatically discovers\ndifferences between two large corpora in a goal-driven way. The task input is a\nproblem comprising a research goal \"$\\textit{comparing the side effects of drug\nA and drug B}$\" and a corpus pair (two large collections of patients'\nself-reported reactions after taking each drug). The output is a language\ndescription (discovery) of how these corpora differ (patients taking drug A\n\"$\\textit{mention feelings of paranoia}$\" more often). We build a D5 system,\nand to quantitatively measure its performance, we 1) contribute a meta-dataset,\nOpenD5, aggregating 675 open-ended problems ranging across business, social\nsciences, humanities, machine learning, and health, and 2) propose a set of\nunified evaluation metrics: validity, relevance, novelty, and significance.\nWith the dataset and the unified metrics, we confirm that language models can\nuse the goals to propose more relevant, novel, and significant candidate\ndiscoveries. Finally, our system produces discoveries previously unknown to the\nauthors on a wide range of applications in OpenD5, including temporal and\ndemographic differences in discussion topics, political stances and stereotypes\nin speech, insights in commercial reviews, and error patterns in NLP models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">Ruiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peter Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Steve Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_J/0/1/0/all/0/1\">Jinwoo Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmented Transformers with Adaptive n-grams Embedding for Multilingual Scene Text Recognition. (arXiv:2302.14261v1 [cs.CL])","link":"http://arxiv.org/abs/2302.14261","description":"<p>While vision transformers have been highly successful in improving the\nperformance in image-based tasks, not much work has been reported on applying\ntransformers to multilingual scene text recognition due to the complexities in\nthe visual appearance of multilingual texts. To fill the gap, this paper\nproposes an augmented transformer architecture with n-grams embedding and\ncross-language rectification (TANGER). TANGER consists of a primary transformer\nwith single patch embeddings of visual images, and a supplementary transformer\nwith adaptive n-grams embeddings that aims to flexibly explore the potential\ncorrelations between neighbouring visual patches, which is essential for\nfeature extraction from multilingual scene texts. Cross-language rectification\nis achieved with a loss function that takes into account both language\nidentification and contextual coherence scoring. Extensive comparative studies\nare conducted on four widely used benchmark datasets as well as a new\nmultilingual scene text dataset containing Indonesian, English, and Chinese\ncollected from tourism scenes in Indonesia. Our experimental results\ndemonstrate that TANGER is considerably better compared to the\nstate-of-the-art, especially in handling complex multilingual scene texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xueming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhihang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yaochu Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HugNLP: A Unified and Comprehensive Library for Natural Language Processing. (arXiv:2302.14286v1 [cs.CL])","link":"http://arxiv.org/abs/2302.14286","description":"<p>In this paper, we introduce HugNLP, a unified and comprehensive library for\nnatural language processing (NLP) with the prevalent backend of HuggingFace\nTransformers, which is designed for NLP researchers to easily utilize\noff-the-shelf algorithms and develop novel methods with user-defined models and\ntasks in real-world scenarios. HugNLP consists of a hierarchical structure\nincluding models, processors and applications that unifies the learning process\nof pre-trained language models (PLMs) on different NLP tasks. Additionally, we\npresent some featured NLP applications to show the effectiveness of HugNLP,\nsuch as knowledge-enhanced PLMs, universal information extraction, low-resource\nmining, and code understanding and generation, etc. The source code will be\nreleased on GitHub (https://github.com/wjn1996/HugNLP).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qiushi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenkang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Ming Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniFLG: Unified Facial Landmark Generator from Text or Speech. (arXiv:2302.14337v1 [cs.CV])","link":"http://arxiv.org/abs/2302.14337","description":"<p>Talking face generation has been extensively investigated owing to its wide\napplicability. The two primary frameworks used for talking face generation\ncomprise a text-driven framework, which generates synchronized speech and\ntalking faces from text, and a speech-driven framework, which generates talking\nfaces from speech. To integrate these frameworks, this paper proposes a unified\nfacial landmark generator (UniFLG). The proposed system exploits end-to-end\ntext-to-speech not only for synthesizing speech but also for extracting a\nseries of latent representations that are common to text and speech, and feeds\nit to a landmark decoder to generate facial landmarks. We demonstrate that our\nsystem achieves higher naturalness in both speech synthesis and facial landmark\ngeneration compared to the state-of-the-art text-driven method. We further\ndemonstrate that our system can generate facial landmarks from speech of\nspeakers without facial video data or even speech data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitsui_K/0/1/0/all/0/1\">Kentaro Mitsui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hono_Y/0/1/0/all/0/1\">Yukiya Hono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawada_K/0/1/0/all/0/1\">Kei Sawada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear Spaces of Meanings: the Compositional Language of VLMs. (arXiv:2302.14383v1 [cs.LG])","link":"http://arxiv.org/abs/2302.14383","description":"<p>We investigate compositional structures in vector data embeddings from\npre-trained vision-language models (VLMs). Traditionally, compositionality has\nbeen associated with algebraic operations on embeddings of words from a\npre-existing vocabulary. In contrast, we seek to approximate label\nrepresentations from a text encoder as combinations of a smaller set of vectors\nin the embedding space. These vectors can be seen as \"ideal words\" which can be\nused to generate new concepts in an efficient way. We present a theoretical\nframework for understanding linear compositionality, drawing connections with\nmathematical representation theory and previous definitions of disentanglement.\nWe provide theoretical and empirical evidence that ideal words provide good\ncompositional approximations of composite concepts and can be more effective\nthan token-based decompositions of the same concepts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trager_M/0/1/0/all/0/1\">Matthew Trager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perera_P/0/1/0/all/0/1\">Pramuditha Perera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zancato_L/0/1/0/all/0/1\">Luca Zancato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1\">Alessandro Achille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_P/0/1/0/all/0/1\">Parminder Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information-Restricted Neural Language Models Reveal Different Brain Regions' Sensitivity to Semantics, Syntax and Context. (arXiv:2302.14389v1 [cs.CL])","link":"http://arxiv.org/abs/2302.14389","description":"<p>A fundamental question in neurolinguistics concerns the brain regions\ninvolved in syntactic and semantic processing during speech comprehension, both\nat the lexical (word processing) and supra-lexical levels (sentence and\ndiscourse processing). To what extent are these regions separated or\nintertwined? To address this question, we trained a lexical language model,\nGlove, and a supra-lexical language model, GPT-2, on a text corpus from which\nwe selectively removed either syntactic or semantic information. We then\nassessed to what extent these information-restricted models were able to\npredict the time-courses of fMRI signal of humans listening to naturalistic\ntext. We also manipulated the size of contextual information provided to GPT-2\nin order to determine the windows of integration of brain regions involved in\nsupra-lexical processing. Our analyses show that, while most brain regions\ninvolved in language are sensitive to both syntactic and semantic variables,\nthe relative magnitudes of these effects vary a lot across these regions.\nFurthermore, we found an asymmetry between the left and right hemispheres, with\nsemantic and syntactic processing being more dissociated in the left hemisphere\nthan in the right, and the left and right hemispheres showing respectively\ngreater sensitivity to short and long contexts. The use of\ninformation-restricted NLP models thus shed new light on the spatial\norganization of syntactic processing, semantic processing and compositionality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pasquiou_A/0/1/0/all/0/1\">Alexandre Pasquiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakretz_Y/0/1/0/all/0/1\">Yair Lakretz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thirion_B/0/1/0/all/0/1\">Bertrand Thirion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pallier_C/0/1/0/all/0/1\">Christophe Pallier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLM-Dialog: Noise-tolerant Pre-training for Knowledge-grounded Dialogue Generation. (arXiv:2302.14401v1 [cs.CL])","link":"http://arxiv.org/abs/2302.14401","description":"<p>We present GLM-Dialog, a large-scale language model (LLM) with 10B parameters\ncapable of knowledge-grounded conversation in Chinese using a search engine to\naccess the Internet knowledge. GLM-Dialog offers a series of applicable\ntechniques for exploiting various external knowledge including both helpful and\nnoisy knowledge, enabling the creation of robust knowledge-grounded dialogue\nLLMs with limited proper datasets. To evaluate the GLM-Dialog more fairly, we\nalso propose a novel evaluation method to allow humans to converse with\nmultiple deployed bots simultaneously and compare their performance implicitly\ninstead of explicitly rating using multidimensional metrics.Comprehensive\nevaluations from automatic to human perspective demonstrate the advantages of\nGLM-Dialog comparing with existing open source Chinese dialogue models. We\nrelease both the model checkpoint and source code, and also deploy it as a\nWeChat application to interact with users. We offer our evaluation platform\nonline in an effort to prompt the development of open source models and\nreliable dialogue evaluation systems. The additional easy-to-use toolkit that\nconsists of short text entity linking, query generation, and helpful knowledge\nclassification is also released to enable diverse applications. All the source\ncode is available on Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaokang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Li_D/0/1/0/all/0/1\">Daniel Zhang-Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jifan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zijun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zeyao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiqi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haohua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nianyi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Sunrui Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instruction Clarification Requests in Multimodal Collaborative Dialogue Games: Tasks, and an Analysis of the CoDraw Dataset. (arXiv:2302.14406v1 [cs.CL])","link":"http://arxiv.org/abs/2302.14406","description":"<p>In visual instruction-following dialogue games, players can engage in repair\nmechanisms in face of an ambiguous or underspecified instruction that cannot be\nfully mapped to actions in the world. In this work, we annotate Instruction\nClarification Requests (iCRs) in CoDraw, an existing dataset of interactions in\na multimodal collaborative dialogue game. We show that it contains lexically\nand semantically diverse iCRs being produced self-motivatedly by players\ndeciding to clarify in order to solve the task successfully. With 8.8k iCRs\nfound in 9.9k dialogues, CoDraw-iCR (v1) is a large spontaneous iCR corpus,\nmaking it a valuable resource for data-driven research on clarification in\ndialogue. We then formalise and provide baseline models for two tasks:\nDetermining when to make an iCR and how to recognise them, in order to\ninvestigate to what extent these tasks are learnable from data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madureira_B/0/1/0/all/0/1\">Brielen Madureira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlangen_D/0/1/0/all/0/1\">David Schlangen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMoA: Sparse Mixture of Adapters to Mitigate Multiple Dataset Biases. (arXiv:2302.14413v1 [cs.CL])","link":"http://arxiv.org/abs/2302.14413","description":"<p>Recent studies reveal that various biases exist in different NLP tasks, and\nover-reliance on biases results in models' poor generalization ability and low\nadversarial robustness. To mitigate datasets biases, previous works propose\nlots of debiasing techniques to tackle specific biases, which perform well on\nrespective adversarial sets but fail to mitigate other biases. In this paper,\nwe propose a new debiasing method Sparse Mixture-of-Adapters (SMoA), which can\nmitigate multiple dataset biases effectively and efficiently. Experiments on\nNatural Language Inference and Paraphrase Identification tasks demonstrate that\nSMoA outperforms full-finetuning, adapter tuning baselines, and prior strong\ndebiasing methods. Further analysis indicates the interpretability of SMoA that\nsub-adapter can capture specific pattern from the training data and specialize\nto handle specific bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jing Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text classification dataset and analysis for Uzbek language. (arXiv:2302.14494v1 [cs.CL])","link":"http://arxiv.org/abs/2302.14494","description":"<p>Text classification is an important task in Natural Language Processing\n(NLP), where the goal is to categorize text data into predefined classes. In\nthis study, we analyse the dataset creation steps and evaluation techniques of\nmulti-label news categorisation task as part of text classification. We first\npresent a newly obtained dataset for Uzbek text classification, which was\ncollected from 10 different news and press websites and covers 15 categories of\nnews, press and law texts. We also present a comprehensive evaluation of\ndifferent models, ranging from traditional bag-of-words models to deep learning\narchitectures, on this newly created dataset. Our experiments show that the\nRecurrent Neural Network (RNN) and Convolutional Neural Network (CNN) based\nmodels outperform the rule-based models. The best performance is achieved by\nthe BERTbek model, which is a transformer-based BERT model trained on the Uzbek\ncorpus. Our findings provide a good baseline for further research in Uzbek text\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuriyozov_E/0/1/0/all/0/1\">Elmurod Kuriyozov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salaev_U/0/1/0/all/0/1\">Ulugbek Salaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matlatipov_S/0/1/0/all/0/1\">Sanatbek Matlatipov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matlatipov_G/0/1/0/all/0/1\">Gayrat Matlatipov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Long Text Modeling with Transformers. (arXiv:2302.14502v1 [cs.CL])","link":"http://arxiv.org/abs/2302.14502","description":"<p>Modeling long texts has been an essential technique in the field of natural\nlanguage processing (NLP). With the ever-growing number of long documents, it\nis important to develop effective modeling methods that can process and analyze\nsuch texts. However, long texts pose important research challenges for existing\ntext models, with more complex semantics and special characteristics. In this\npaper, we provide an overview of the recent advances on long texts modeling\nbased on Transformer models. Firstly, we introduce the formal definition of\nlong text modeling. Then, as the core content, we discuss how to process long\ninput to satisfy the length limitation and design improved Transformer\narchitectures to effectively extend the maximum context length. Following this,\nwe discuss how to adapt Transformer models to capture the special\ncharacteristics of long texts. Finally, we describe four typical applications\ninvolving long text modeling and conclude this paper with a discussion of\nfuture directions. Our survey intends to provide researchers with a synthesis\nand pointer to related work on long text modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zican Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lunyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Are State-of-the-Art Evaluators of Translation Quality. (arXiv:2302.14520v1 [cs.CL])","link":"http://arxiv.org/abs/2302.14520","description":"<p>We describe GEMBA, a GPT-based metric for assessment of translation quality,\nwhich works both with a reference translation and without. In our evaluation,\nwe focus on zero-shot prompting, comparing four prompt variants in two modes,\nbased on the availability of the reference. We investigate seven versions of\nGPT models, including ChatGPT. We show that our method for translation quality\nassessment only works with GPT 3.5 and larger models. Comparing to results from\nWMT22's Metrics shared task, our method achieves state-of-the-art accuracy in\nboth modes when compared to MQM-based human labels. Our results are valid on\nthe system level for all three WMT22 Metrics shared task language pairs, namely\nEnglish into German, English into Russian, and Chinese into English. This\nprovides a first glimpse into the usefulness of pre-trained, generative large\nlanguage models for quality assessment of translations. We publicly release all\nour code and prompt templates used for the experiments described in this work,\nas well as all corresponding scoring results, to allow for external validation\nand reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kocmi_T/0/1/0/all/0/1\">Tom Kocmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Federmann_C/0/1/0/all/0/1\">Christian Federmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Heteronym Resolution Pipeline Using RAD-TTS Aligners. (arXiv:2302.14523v1 [cs.CL])","link":"http://arxiv.org/abs/2302.14523","description":"<p>Grapheme-to-phoneme (G2P) transduction is part of the standard text-to-speech\n(TTS) pipeline. However, G2P conversion is difficult for languages that contain\nheteronyms -- words that have one spelling but can be pronounced in multiple\nways. G2P datasets with annotated heteronyms are limited in size and expensive\nto create, as human labeling remains the primary method for heteronym\ndisambiguation. We propose a RAD-TTS Aligner-based pipeline to automatically\ndisambiguate heteronyms in datasets that contain both audio with text\ntranscripts. The best pronunciation can be chosen by generating all possible\ncandidates for each heteronym and scoring them with an Aligner model. The\nresulting labels can be used to create training datasets for use in both\nmulti-stage and end-to-end G2P systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jocelyn Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakhturina_E/0/1/0/all/0/1\">Evelina Bakhturina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tatanov_O/0/1/0/all/0/1\">Oktai Tatanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spacerini: Plug-and-play Search Engines with Pyserini and Hugging Face. (arXiv:2302.14534v1 [cs.IR])","link":"http://arxiv.org/abs/2302.14534","description":"<p>We present Spacerini, a modular framework for seamless building and\ndeployment of interactive search applications, designed to facilitate the\nqualitative analysis of large scale research datasets. Spacerini integrates\nfeatures from both the Pyserini toolkit and the Hugging Face ecosystem to ease\nthe indexing text collections and deploy them as search engines for ad-hoc\nexploration and to make the retrieval of relevant data points quick and\nefficient. The user-friendly interface enables searching through massive\ndatasets in a no-code fashion, making Spacerini broadly accessible to anyone\nlooking to qualitatively audit their text collections. This is useful both to\nIR~researchers aiming to demonstrate the capabilities of their indexes in a\nsimple and interactive way, and to NLP~researchers looking to better understand\nand audit the failure modes of large language models. The framework is open\nsource and available on GitHub: https://github.com/castorini/hf-spacerini, and\nincludes utilities to load, pre-process, index, and deploy local and web search\napplications. A portfolio of applications created with Spacerini for a\nmultitude of use cases can be found by visiting https://hf.co/spacerini.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akiki_C/0/1/0/all/0/1\">Christopher Akiki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogundepo_O/0/1/0/all/0/1\">Odunayo Ogundepo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piktus_A/0/1/0/all/0/1\">Aleksandra Piktus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oladipo_A/0/1/0/all/0/1\">Akintunde Oladipo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The 2022 NIST Language Recognition Evaluation. (arXiv:2302.14624v1 [cs.CL])","link":"http://arxiv.org/abs/2302.14624","description":"<p>In 2022, the U.S. National Institute of Standards and Technology (NIST)\nconducted the latest Language Recognition Evaluation (LRE) in an ongoing series\nadministered by NIST since 1996 to foster research in language recognition and\nto measure state-of-the-art technology. Similar to previous LREs, LRE22 focused\non conversational telephone speech (CTS) and broadcast narrowband speech (BNBS)\ndata. LRE22 also introduced new evaluation features, such as an emphasis on\nAfrican languages, including low resource languages, and a test set consisting\nof segments containing between 3s and 35s of speech randomly sampled and\nextracted from longer recordings. A total of 21 research organizations, forming\n16 teams, participated in this 3-month long evaluation and made a total of 65\nvalid system submissions to be evaluated. This paper presents an overview of\nLRE22 and an analysis of system performance over different evaluation\nconditions. The evaluation results suggest that Oromo and Tigrinya are easier\nto detect while Xhosa and Zulu are more challenging. A greater confusability is\nseen for some language pairs. When speech duration increased, system\nperformance significantly increased up to a certain duration, and then a\ndiminishing return on system performance is observed afterward.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yooyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenberg_C/0/1/0/all/0/1\">Craig Greenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godard_E/0/1/0/all/0/1\">Eliot Godard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butt_A/0/1/0/all/0/1\">Asad A. Butt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singer_E/0/1/0/all/0/1\">Elliot Singer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Trang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mason_L/0/1/0/all/0/1\">Lisa Mason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reynolds_D/0/1/0/all/0/1\">Douglas Reynolds</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"H-AES: Towards Automated Essay Scoring for Hindi. (arXiv:2302.14635v1 [cs.CL])","link":"http://arxiv.org/abs/2302.14635","description":"<p>The use of Natural Language Processing (NLP) for Automated Essay Scoring\n(AES) has been well explored in the English language, with benchmark models\nexhibiting performance comparable to human scorers. However, AES in Hindi and\nother low-resource languages remains unexplored. In this study, we reproduce\nand compare state-of-the-art methods for AES in the Hindi domain. We employ\nclassical feature-based Machine Learning (ML) and advanced end-to-end models,\nincluding LSTM Networks and Fine-Tuned Transformer Architecture, in our\napproach and derive results comparable to those in the English language domain.\nHindi being a low-resource language, lacks a dedicated essay-scoring corpus. We\ntrain and evaluate our models using translated English essays and empirically\nmeasure their performance on our own small-scale, real-world Hindi corpus. We\nfollow this up with an in-depth analysis discussing prompt-specific behavior of\ndifferent language models implemented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Shubhankar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pupneja_A/0/1/0/all/0/1\">Anirudh Pupneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mital_S/0/1/0/all/0/1\">Shivaansh Mital</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_C/0/1/0/all/0/1\">Cheril Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bawkar_M/0/1/0/all/0/1\">Manish Bawkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_L/0/1/0/all/0/1\">Lakshman Prasad Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ajit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_Y/0/1/0/all/0/1\">Yaman Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rushali Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpeechFormer++: A Hierarchical Efficient Framework for Paralinguistic Speech Processing. (arXiv:2302.14638v1 [eess.AS])","link":"http://arxiv.org/abs/2302.14638","description":"<p>Paralinguistic speech processing is important in addressing many issues, such\nas sentiment and neurocognitive disorder analyses. Recently, Transformer has\nachieved remarkable success in the natural language processing field and has\ndemonstrated its adaptation to speech. However, previous works on Transformer\nin the speech field have not incorporated the properties of speech, leaving the\nfull potential of Transformer unexplored. In this paper, we consider the\ncharacteristics of speech and propose a general structure-based framework,\ncalled SpeechFormer++, for paralinguistic speech processing. More concretely,\nfollowing the component relationship in the speech signal, we design a unit\nencoder to model the intra- and inter-unit information (i.e., frames, phones,\nand words) efficiently. According to the hierarchical relationship, we utilize\nmerging blocks to generate features at different granularities, which is\nconsistent with the structural pattern in the speech signal. Moreover, a word\nencoder is introduced to integrate word-grained features into each unit\nencoder, which effectively balances fine-grained and coarse-grained\ninformation. SpeechFormer++ is evaluated on the speech emotion recognition\n(IEMOCAP &amp; MELD), depression classification (DAIC-WOZ) and Alzheimer's disease\ndetection (Pitt) tasks. The results show that SpeechFormer++ outperforms the\nstandard Transformer while greatly reducing the computational cost.\nFurthermore, it delivers superior results compared to the state-of-the-art\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_W/0/1/0/all/0/1\">Weidong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xing_X/0/1/0/all/0/1\">Xiaofen Xing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xiangmin Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pang_J/0/1/0/all/0/1\">Jianxin Pang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Du_L/0/1/0/all/0/1\">Lan Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthesizing Mixed-type Electronic Health Records using Diffusion Models. (arXiv:2302.14679v1 [cs.LG])","link":"http://arxiv.org/abs/2302.14679","description":"<p>Electronic Health Records (EHRs) contain sensitive patient information, which\npresents privacy concerns when sharing such data. Synthetic data generation is\na promising solution to mitigate these risks, often relying on deep generative\nmodels such as Generative Adversarial Networks (GANs). However, recent studies\nhave shown that diffusion models offer several advantages over GANs, such as\ngeneration of more realistic synthetic data and stable training in generating\ndata modalities, including image, text, and sound. In this work, we investigate\nthe potential of diffusion models for generating realistic mixed-type tabular\nEHRs, comparing TabDDPM model with existing methods on four datasets in terms\nof data quality, utility, privacy, and augmentation. Our experiments\ndemonstrate that TabDDPM outperforms the state-of-the-art models across all\nevaluation metrics, except for privacy, which confirms the trade-off between\nprivacy and utility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ceritli_T/0/1/0/all/0/1\">Taha Ceritli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosheh_G/0/1/0/all/0/1\">Ghadeer O. Ghosheh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_V/0/1/0/all/0/1\">Vinod Kumar Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tingting Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creagh_A/0/1/0/all/0/1\">Andrew P. Creagh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1\">David A. Clifton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Which One Are You Referring To? Multimodal Object Identification in Situated Dialogue. (arXiv:2302.14680v1 [cs.CL])","link":"http://arxiv.org/abs/2302.14680","description":"<p>The demand for multimodal dialogue systems has been rising in various\ndomains, emphasizing the importance of interpreting multimodal inputs from\nconversational and situational contexts. We explore three methods to tackle\nthis problem and evaluate them on the largest situated dialogue dataset, SIMMC\n2.1. Our best method, scene-dialogue alignment, improves the performance by\n~20% F1-score compared to the SIMMC 2.1 baselines. We provide analysis and\ndiscussion regarding the limitation of our methods and the potential directions\nfor future works. Our code is publicly available at\nhttps://github.com/holylovenia/multimodal-object-identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-Context Instruction Learning. (arXiv:2302.14691v1 [cs.CL])","link":"http://arxiv.org/abs/2302.14691","description":"<p>Instruction learning of Large Language Models (LLMs) has enabled zero-shot\ntask generalization. However, instruction learning has been predominantly\napproached as a fine-tuning problem, including instruction tuning and\nreinforcement learning from human feedback, where LLMs are multi-task\nfine-tuned on various tasks with instructions. In this paper, we present a\nsurprising finding that applying in-context learning to instruction learning,\nreferred to as In-Context Instruction Learning (ICIL), significantly improves\nthe zero-shot task generalization performance for both pretrained and\ninstruction-fine-tuned models. One of the core advantages of ICIL is that it\nuses a single fixed prompt to evaluate all tasks, which is a concatenation of\ncross-task demonstrations. In particular, we demonstrate that the most powerful\ninstruction-fine-tuned baseline (text-davinci-003) also benefits from ICIL by\n9.3%, indicating that the effect of ICIL is complementary to instruction-based\nfine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Seonghyeon Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_H/0/1/0/all/0/1\">Hyeonbin Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sohee Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_H/0/1/0/all/0/1\">Hyeongu Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yireun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Japanese CCGBank empirically correct? A case study of passive and causative constructions. (arXiv:2302.14708v1 [cs.CL])","link":"http://arxiv.org/abs/2302.14708","description":"<p>The Japanese CCGBank serves as training and evaluation data for developing\nJapanese CCG parsers. However, since it is automatically generated from the\nKyoto Corpus, a dependency treebank, its linguistic validity still needs to be\nsufficiently verified. In this paper, we focus on the analysis of\npassive/causative constructions in the Japanese CCGBank and show that, together\nwith the compositional semantics of ccg2lambda, a semantic parsing system, it\nyields empirically wrong predictions for the nested construction of passives\nand causatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bekki_D/0/1/0/all/0/1\">Daisuke Bekki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yanaka_H/0/1/0/all/0/1\">Hitomi Yanaka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-training through Classifier Disagreement for Cross-Domain Opinion Target Extraction. (arXiv:2302.14719v1 [cs.CL])","link":"http://arxiv.org/abs/2302.14719","description":"<p>Opinion target extraction (OTE) or aspect extraction (AE) is a fundamental\ntask in opinion mining that aims to extract the targets (or aspects) on which\nopinions have been expressed. Recent work focus on cross-domain OTE, which is\ntypically encountered in real-world scenarios, where the testing and training\ndistributions differ. Most methods use domain adversarial neural networks that\naim to reduce the domain gap between the labelled source and unlabelled target\ndomains to improve target domain performance. However, this approach only\naligns feature distributions and does not account for class-wise feature\nalignment, leading to suboptimal results. Semi-supervised learning (SSL) has\nbeen explored as a solution, but is limited by the quality of pseudo-labels\ngenerated by the model. Inspired by the theoretical foundations in domain\nadaptation [2], we propose a new SSL approach that opts for selecting target\nsamples whose model output from a domain-specific teacher and student network\ndisagree on the unlabelled target data, in an effort to boost the target domain\nperformance. Extensive experiments on benchmark cross-domain OTE datasets show\nthat this approach is effective and performs consistently well in settings with\nlarge domain shifts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Kai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Richong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensah_S/0/1/0/all/0/1\">Samuel Mensah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yongyi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xudong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatically Classifying Emotions based on Text: A Comparative Exploration of Different Datasets. (arXiv:2302.14727v1 [cs.CL])","link":"http://arxiv.org/abs/2302.14727","description":"<p>Emotion Classification based on text is a task with many applications which\nhas received growing interest in recent years. This paper presents a\npreliminary study with the goal to help researchers and practitioners gain\ninsight into relatively new datasets as well as emotion classification in\ngeneral. We focus on three datasets that were recently presented in the related\nliterature, and we explore the performance of traditional as well as\nstate-of-the-art deep learning models in the presence of different\ncharacteristics in the data. We also explore the use of data augmentation in\norder to improve performance. Our experimental work shows that state-of-the-art\nmodels such as RoBERTa perform the best for all cases. We also provide\nobservations and discussion that highlight the complexity of emotion\nclassification in these datasets and test out the applicability of the models\nto actual social media posts we collected and labeled.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koufakou_A/0/1/0/all/0/1\">Anna Koufakou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garciga_J/0/1/0/all/0/1\">Jairo Garciga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1\">Adam Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morelli_J/0/1/0/all/0/1\">Joseph Morelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_C/0/1/0/all/0/1\">Christopher Frank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Representations of Text and Knowledge Graphs for Retrieval and Evaluation. (arXiv:2302.14785v1 [cs.CL])","link":"http://arxiv.org/abs/2302.14785","description":"<p>A key feature of neural models is that they can produce semantic vector\nrepresentations of objects (texts, images, speech, etc.) ensuring that similar\nobjects are close to each other in the vector space. While much work has\nfocused on learning representations for other modalities, there are no aligned\ncross-modal representations for text and knowledge base (KB) elements. One\nchallenge for learning such representations is the lack of parallel data, which\nwe use contrastive training on heuristics-based datasets and data augmentation\nto overcome, training embedding models on (KB graph, text) pairs. On WebNLG, a\ncleaner manually crafted dataset, we show that they learn aligned\nrepresentations suitable for retrieval. We then fine-tune on annotated data to\ncreate EREDAT (Ensembled Representations for Evaluation of DAta-to-Text), a\nsimilarity metric between English text and KB graphs. EREDAT outperforms or\nmatches state-of-the-art metrics in terms of correlation with human judgments\non WebNLG even though, unlike them, it does not require a reference text to\ncompare against.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scao_T/0/1/0/all/0/1\">Teven Le Scao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardent_C/0/1/0/all/0/1\">Claire Gardent</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discourse Relation Embeddings: Representing the Relations between Discourse Segments in Social Media. (arXiv:2105.01306v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.01306","description":"<p>Discourse relations are typically modeled as a discrete class that\ncharacterizes the relation between segments of text (e.g. causal explanations,\nexpansions). However, such predefined discrete classes limits the universe of\npotential relationships and their nuanced differences. Analogous to contextual\nword embeddings, we propose representing discourse relations as points in high\ndimensional continuous space. However, unlike words, discourse relations often\nhave no surface form (relations are between two segments, often with no word or\nphrase in that gap) which presents a challenge for existing embedding\ntechniques. We present a novel method for automatically creating discourse\nrelation embeddings (DiscRE), addressing the embedding challenge through a\nweakly supervised, multitask approach to learn diverse and nuanced relations\nbetween discourse segments in social media. Results show DiscRE can: (1) obtain\nthe best performance on Twitter discourse relation classification task (macro\nF1=0.76) (2) improve the state of the art in social media causality prediction\n(from F1=.79 to .81), (3) perform beyond modern sentence and contextual word\nembeddings at traditional discourse relation classification, and (4) capture\nnovel nuanced relations (e.g. relations semantically at the intersection of\ncausal explanations and counterfactuals).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Son_Y/0/1/0/all/0/1\">Youngseo Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varadarajan_V/0/1/0/all/0/1\">Vasudha Varadarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_H/0/1/0/all/0/1\">H Andrew Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDERank: A Masked Document Embedding Rank Approach for Unsupervised Keyphrase Extraction. (arXiv:2110.06651v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06651","description":"<p>Keyphrase extraction (KPE) automatically extracts phrases in a document that\nprovide a concise summary of the core content, which benefits downstream\ninformation retrieval and NLP tasks. Previous state-of-the-art (SOTA) methods\nselect candidate keyphrases based on the similarity between learned\nrepresentations of the candidates and the document. They suffer performance\ndegradation on long documents due to discrepancy between sequence lengths which\ncauses mismatch between representations of keyphrase candidates and the\ndocument. In this work, we propose a novel unsupervised embedding-based KPE\napproach, Masked Document Embedding Rank (MDERank), to address this problem by\nleveraging a mask strategy and ranking candidates by the similarity between\nembeddings of the source document and the masked document. We further develop a\nKPE-oriented BERT (KPEBERT) model by proposing a novel self-supervised\ncontrastive learning method, which is more compatible to MDERank than vanilla\nBERT. Comprehensive evaluations on six KPE benchmarks demonstrate that the\nproposed MDERank outperforms state-of-the-art unsupervised KPE approach by\naverage 1.80 $F1@15$ improvement. MDERank further benefits from KPEBERT and\noverall achieves average 3.53 $F1@15$ improvement over the SOTA SIFRank. Our\ncode is available at \\url{https://github.com/LinhanZ/mderank}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xin Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Police Text Analysis: Topic Modeling and Spatial Relative Density Estimation. (arXiv:2202.04176v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.04176","description":"<p>We analyze a large corpus of police incident narrative documents in\nunderstanding the spatial distribution of the topics. The motivation for doing\nthis is that police narratives in each incident report contains very\nfine-grained information that is richer than the category that is manually\nassigned by the police. Our approach is to split the corpus into topics using\ntwo different unsupervised machine learning algorithms - Latent Dirichlet\nAllocation and Non-negative Matrix Factorization. We validate the performance\nof each learned topic model using model coherence. Then, using a k-nearest\nneighbors density ratio estimation (kNN-DRE) approach that we propose, we\nestimate the spatial density ratio per topic and use this for data discovery\nand analysis of each topic, allowing for insights into the described incidents\nat scale. We provide a qualitative assessment of each topic and highlight some\nkey benefits for using our kNN-DRE model for estimating spatial trends.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huestis_Mitchell_S/0/1/0/all/0/1\">Sarah Huestis-Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiuyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yao Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding The Robustness of Self-supervised Learning Through Topic Modeling. (arXiv:2203.03539v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.03539","description":"<p>Self-supervised learning has significantly improved the performance of many\nNLP tasks. However, how can self-supervised learning discover useful\nrepresentations, and why is it better than traditional approaches such as\nprobabilistic models are still largely unknown. In this paper, we focus on the\ncontext of topic modeling and highlight a key advantage of self-supervised\nlearning - when applied to data generated by topic models, self-supervised\nlearning can be oblivious to the specific model, and hence is less susceptible\nto model misspecification. In particular, we prove that commonly used\nself-supervised objectives based on reconstruction or contrastive samples can\nboth recover useful posterior information for general topic models.\nEmpirically, we show that the same objectives can perform on par with posterior\ninference using the correct model, while outperforming posterior inference\nusing misspecified models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zeping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shiyou Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_C/0/1/0/all/0/1\">Cindy Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1\">Rong Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gold Doesn't Always Glitter: Spectral Removal of Linear and Nonlinear Guarded Attribute Information. (arXiv:2203.07893v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07893","description":"<p>We describe a simple and effective method (Spectral Attribute removaL; SAL)\nto remove private or guarded information from neural representations. Our\nmethod uses matrix decomposition to project the input representations into\ndirections with reduced covariance with the guarded information rather than\nmaximal covariance as factorization methods normally use. We begin with linear\ninformation removal and proceed to generalize our algorithm to the case of\nnonlinear information removal using kernels. Our experiments demonstrate that\nour algorithm retains better main task performance after removing the guarded\ninformation compared to previous work. In addition, our experiments demonstrate\nthat we need a relatively small amount of guarded attribute data to remove\ninformation about these attributes, which lowers the exposure to sensitive data\nand is more suitable for low-resource scenarios. Code is available at\nhttps://github.com/jasonshaoshun/SAL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1\">Shun Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziser_Y/0/1/0/all/0/1\">Yftah Ziser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Shay B. Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. (arXiv:2203.13474v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.13474","description":"<p>Program synthesis strives to generate a computer program as a solution to a\ngiven problem specification, expressed with input-output examples or natural\nlanguage descriptions. The prevalence of large language models advances the\nstate-of-the-art for program synthesis, though limited training resources and\ndata impede open access to such models. To democratize this, we train and\nrelease a family of large language models up to 16.1B parameters, called\nCODEGEN, on natural language and programming language data, and open source the\ntraining library JAXFORMER. We show the utility of the trained model by\ndemonstrating that it is competitive with the previous state-of-the-art on\nzero-shot Python code generation on HumanEval. We further investigate the\nmulti-step paradigm for program synthesis, where a single program is factorized\ninto multiple prompts specifying subproblems. To this end, we construct an open\nbenchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse\nproblem sets that are factorized into multi-turn prompts. Our analysis on MTPB\nshows that the same intent provided to CODEGEN in multi-turn fashion\nsignificantly improves program synthesis over that provided as a single turn.\nWe make the training library JAXFORMER and model checkpoints available as open\nsource contribution: https://github.com/salesforce/CodeGen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nijkamp_E/0/1/0/all/0/1\">Erik Nijkamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_H/0/1/0/all/0/1\">Hiroaki Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_L/0/1/0/all/0/1\">Lifu Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UL2: Unifying Language Learning Paradigms. (arXiv:2205.05131v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.05131","description":"<p>Existing pre-trained models are generally geared towards a particular class\nof problems. To date, there seems to be still no consensus on what the right\narchitecture and pre-training setup should be. This paper presents a unified\nframework for pre-training models that are universally effective across\ndatasets and setups. We begin by disentangling architectural archetypes with\npre-training objectives -- two concepts that are commonly conflated. Next, we\npresent a generalized &amp; unified perspective for self-supervision in NLP and\nshow how different pre-training objectives can be cast as one another and how\ninterpolating between different objectives can be effective. We then propose\nMixture-of-Denoisers (MoD), a pre-training objective that combines diverse\npre-training paradigms together. We furthermore introduce a notion of mode\nswitching, wherein downstream fine-tuning is associated with specific\npre-training schemes. We conduct extensive ablative experiments to compare\nmultiple pre-training objectives and find that our method pushes the\nPareto-frontier by outperforming T5 &amp; GPT-like models across multiple diverse\nsetups. By scaling our model up to 20B parameters, we achieve SOTA performance\non 50 well-established supervised finetuning based NLP tasks. Our model also\nachieve strong results at in-context learning, outperforming 175B GPT-3 on\nzero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot\nsummarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B\nalso works well with chain-of-thought prompting and reasoning, making it an\nappealing choice for research into reasoning at a small to medium scale of 20B\nparameters. Finally, we apply FLAN instruction tuning to the UL2 20B model,\nachieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release\nFlax-based T5X checkpoints for the UL2 20B &amp; Flan-UL2 20B.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakeri_S/0/1/0/all/0/1\">Siamak Shakeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huaixiu Steven Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ER-Test: Evaluating Explanation Regularization Methods for Language Models. (arXiv:2205.12542v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12542","description":"<p>By explaining how humans would solve a given task, human rationales can\nprovide strong learning signal for neural language models (LMs). Explanation\nregularization (ER) aims to improve LM generalization by pushing the LM's\nmachine rationales (Which input tokens did the LM focus on?) to align with\nhuman rationales (Which input tokens would humans focus on?). Though prior\nworks primarily study ER via in-distribution (ID) evaluation,\nout-of-distribution (OOD) generalization is often more critical in real-world\nscenarios, yet ER's effect on OOD generalization has been underexplored. In\nthis paper, we introduce ER-Test, a framework for evaluating ER models' OOD\ngeneralization along three dimensions: unseen dataset tests, contrast set\ntests, and functional tests. Using ER-Test, we extensively analyze how ER\nmodels' OOD generalization varies with different ER design choices. Across two\ntasks and six datasets, ER-Test shows that ER has little impact on ID\nperformance but can yield large OOD performance gains. Also, we find that ER\ncan improve OOD performance even with limited rationale supervision. ER-Test's\nresults help demonstrate ER's utility and establish best practices for using ER\neffectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_B/0/1/0/all/0/1\">Brihi Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Aaron Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_S/0/1/0/all/0/1\">Shaoliang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanjabi_M/0/1/0/all/0/1\">Maziar Sanjabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1\">Hamed Firooz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Networks and the Chomsky Hierarchy. (arXiv:2207.02098v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2207.02098","description":"<p>Reliable generalization lies at the heart of safe ML and AI. However,\nunderstanding when and how neural networks generalize remains one of the most\nimportant unsolved problems in the field. In this work, we conduct an extensive\nempirical study (20'910 models, 15 tasks) to investigate whether insights from\nthe theory of computation can predict the limits of neural network\ngeneralization in practice. We demonstrate that grouping tasks according to the\nChomsky hierarchy allows us to forecast whether certain architectures will be\nable to generalize to out-of-distribution inputs. This includes negative\nresults where even extensive amounts of data and training time never lead to\nany non-trivial generalization, despite models having sufficient capacity to\nfit the training data perfectly. Our results show that, for our subset of\ntasks, RNNs and Transformers fail to generalize on non-regular tasks, LSTMs can\nsolve regular and counter-language tasks, and only networks augmented with\nstructured memory (such as a stack or memory tape) can successfully generalize\non context-free and context-sensitive tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deletang_G/0/1/0/all/0/1\">Gr&#xe9;goire Del&#xe9;tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruoss_A/0/1/0/all/0/1\">Anian Ruoss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grau_Moya_J/0/1/0/all/0/1\">Jordi Grau-Moya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genewein_T/0/1/0/all/0/1\">Tim Genewein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenliang_L/0/1/0/all/0/1\">Li Kevin Wenliang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catt_E/0/1/0/all/0/1\">Elliot Catt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cundy_C/0/1/0/all/0/1\">Chris Cundy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_M/0/1/0/all/0/1\">Marcus Hutter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Legg_S/0/1/0/all/0/1\">Shane Legg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veness_J/0/1/0/all/0/1\">Joel Veness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_P/0/1/0/all/0/1\">Pedro A. Ortega</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the effect of domain selection on automatic speech recognition performance: a case study on Bangladeshi Bangla. (arXiv:2210.12921v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12921","description":"<p>The performance of data-driven natural language processing systems is\ncontingent upon the quality of corpora. However, principal corpus design\ncriteria are often not identified and examined adequately, particularly in the\nspeech processing discipline. Speech corpora development requires additional\nattention with regard to clean/noisy, read/spontaneous, multi-talker speech,\naccents/dialects, etc. Domain selection is also a crucial decision point in\nspeech corpus development. In this study, we demonstrate the significance of\ndomain selection by assessing a state-of-the-art Bangla automatic speech\nrecognition (ASR) model on a novel multi-domain Bangladeshi Bangla ASR\nevaluation benchmark - BanSpeech, which contains 7.2 hours of speech and 9802\nutterances from 19 distinct domains. The ASR model has been trained with deep\nconvolutional neural network (CNN), layer normalization technique, and\nConnectionist Temporal Classification (CTC) loss criterion on SUBAK.KO, a\nmostly read speech corpus for the low-resource and morphologically rich\nlanguage Bangla. Experimental evaluation reveals the ASR model on SUBAK.KO\nfaces difficulty recognizing speech from domains with mostly spontaneous speech\nand has a high number of out-of-vocabulary (OOV) words. The same ASR model, on\nthe other hand, performs better in read speech domains and contains fewer OOV\nwords. In addition, we report the outcomes of our experiments with layer\nnormalization, input feature extraction, number of convolutional layers, etc.,\nand set a baseline on SUBAK.KO. The BanSpeech will be publicly available to\nmeet the need for a challenging evaluation benchmark for Bangla ASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Samin_A/0/1/0/all/0/1\">Ahnaf Mozib Samin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobir_M/0/1/0/all/0/1\">M. Humayan Kobir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafee_M/0/1/0/all/0/1\">Md. Mushtaq Shahriyar Rafee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1\">M. Firoz Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Mehedi Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_P/0/1/0/all/0/1\">Partha Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kibria_S/0/1/0/all/0/1\">Shafkat Kibria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">M. Shahidur Rahman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ThoughtSource: A central hub for large language model reasoning data. (arXiv:2301.11596v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.11596","description":"<p>Large language models (LLMs) such as GPT-3 and ChatGPT have recently\ndemonstrated impressive results across a wide range of tasks. LLMs are still\nlimited, however, in that they frequently fail at complex reasoning, their\nreasoning processes are opaque, they are prone to 'hallucinate' facts, and\nthere are concerns about their underlying biases. Letting models verbalize\nreasoning steps as natural language, a technique known as chain-of-thought\nprompting, has recently been proposed as a way to address some of these issues.\nHere we present the first release of ThoughtSource, a meta-dataset and software\nlibrary for chain-of-thought (CoT) reasoning. The goal of ThoughtSource is to\nimprove future artificial intelligence systems by facilitating qualitative\nunderstanding of CoTs, enabling empirical evaluations, and providing training\ndata. This first release of ThoughtSource integrates six scientific/medical,\nthree general-domain and five math word question answering datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ott_S/0/1/0/all/0/1\">Simon Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hebenstreit_K/0/1/0/all/0/1\">Konstantin Hebenstreit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lievin_V/0/1/0/all/0/1\">Valentin Li&#xe9;vin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hother_C/0/1/0/all/0/1\">Christoffer Egeberg Hother</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Milad Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayrhauser_M/0/1/0/all/0/1\">Maximilian Mayrhauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Praas_R/0/1/0/all/0/1\">Robert Praas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1\">Ole Winther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1\">Matthias Samwald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity. (arXiv:2302.04023v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.04023","description":"<p>This paper proposes a framework for quantitatively evaluating interactive\nLLMs such as ChatGPT using publicly available data sets. We carry out an\nextensive technical evaluation of ChatGPT using 23 data sets covering 8\ndifferent common NLP application tasks. We evaluate the multitask, multilingual\nand multi-modal aspects of ChatGPT based on these data sets and a newly\ndesigned multimodal dataset. We find that ChatGPT outperforms LLMs with\nzero-shot learning on most tasks and even outperforms fine-tuned models on some\ntasks. We find that it is better at understanding non-Latin script languages\nthan generating them. It is able to generate multimodal content from textual\nprompts, via an intermediate code generation step. Moreover, we find that\nChatGPT is 63.41% accurate on average in 10 different reasoning categories\nunder logical reasoning, non-textual reasoning, and commonsense reasoning,\nhence making it an unreliable reasoner. It is, for example, better at deductive\nthan inductive reasoning. ChatGPT suffers from hallucination problems like\nother LLMs and it generates more extrinsic hallucinations from its parametric\nmemory as it does not have access to an external knowledge base. Finally, the\ninteractive feature of ChatGPT enables human collaboration with the underlying\nLLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++\non machine translation, in a multi-turn \"prompt engineering\" fashion. We also\nrelease codebase for evaluation set extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bang_Y/0/1/0/all/0/1\">Yejin Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1\">Bryan Wilie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Ziwei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_W/0/1/0/all/0/1\">Willy Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_Q/0/1/0/all/0/1\">Quyet V. Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plan-then-Seam: Towards Efficient Table-to-Text Generation. (arXiv:2302.05138v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.05138","description":"<p>Table-to-text generation aims at automatically generating text to help people\nconveniently obtain salient information in tables. Recent works explicitly\ndecompose the generation process into content planning and surface generation\nstages, employing two autoregressive networks for them respectively. However,\nthey are computationally expensive due to the non-parallelizable nature of\nautoregressive decoding and the redundant parameters of two networks. In this\npaper, we propose the first totally non-autoregressive table-to-text model\n(Plan-then-Seam, PTS) that produces its outputs in parallel with one single\nnetwork. PTS firstly writes and calibrates one plan of the content to be\ngenerated with a novel rethinking pointer predictor, and then takes the plan as\nthe context for seaming to decode the description. These two steps share\nparameters and perform iteratively to capture token inter-dependency while\nkeeping parallel decoding. Experiments on two public benchmarks show that PTS\nachieves 3.0~5.6 times speedup for inference time, reducing 50% parameters,\nwhile maintaining as least comparable performance against strong two-stage\ntable-to-text competitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_R/0/1/0/all/0/1\">Ruiying Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Chengyang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Can Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Binhua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Divergence-Based Domain Transferability for Zero-Shot Classification. (arXiv:2302.05735v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.05735","description":"<p>Transferring learned patterns from pretrained neural language models has been\nshown to significantly improve effectiveness across a variety of language-based\ntasks, meanwhile further tuning on intermediate tasks has been demonstrated to\nprovide additional performance benefits, provided the intermediate task is\nsufficiently related to the target task. However, how to identify related tasks\nis an open problem, and brute-force searching effective task combinations is\nprohibitively expensive. Hence, the question arises, are we able to improve the\neffectiveness and efficiency of tasks with no training examples through\nselective fine-tuning? In this paper, we explore statistical measures that\napproximate the divergence between domain representations as a means to\nestimate whether tuning using one task pair will exhibit performance benefits\nover tuning another. This estimation can then be used to reduce the number of\ntask pairs that need to be tested by eliminating pairs that are unlikely to\nprovide benefits. Through experimentation over 58 tasks and over 6,600 task\npair combinations, we demonstrate that statistical measures can distinguish\neffective task pairs, and the resulting estimates can reduce end-to-end runtime\nby up to 40%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pugantsov_A/0/1/0/all/0/1\">Alexander Pugantsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCreadie_R/0/1/0/all/0/1\">Richard McCreadie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLSP2022-EVJVQA Challenge: Multilingual Visual Question Answering. (arXiv:2302.11752v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.11752","description":"<p>Visual Question Answering (VQA) is a challenging task of natural language\nprocessing (NLP) and computer vision (CV), attracting significant attention\nfrom researchers. English is a resource-rich language that has witnessed\nvarious developments in datasets and models for visual question answering.\nVisual question answering in other languages also would be developed for\nresources and models. In addition, there is no multilingual dataset targeting\nthe visual content of a particular country with its own objects and cultural\ncharacteristics. To address the weakness, we provide the research community\nwith a benchmark dataset named EVJVQA, including 33,000+ pairs of\nquestion-answer over three languages: Vietnamese, English, and Japanese, on\napproximately 5,000 images taken from Vietnam for evaluating multilingual VQA\nsystems or models. EVJVQA is used as a benchmark dataset for the challenge of\nmultilingual visual question answering at the 9th Workshop on Vietnamese\nLanguage and Speech Processing (VLSP 2022). This task attracted 62 participant\nteams from various universities and organizations. In this article, we present\ndetails of the organization of the challenge, an overview of the methods\nemployed by shared-task participants, and the results. The highest performances\nare 0.4392 in F1-score and 0.4009 in BLUE on the private test set. The\nmultilingual QA systems proposed by the top 2 teams use ViT for the pre-trained\nvision model and mT5 for the pre-trained language model, a powerful pre-trained\nlanguage model based on the transformer architecture. EVJVQA is a challenging\ndataset that motivates NLP and CV researchers to further explore the\nmultilingual models or systems for visual question answering systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Nghia Hieu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_D/0/1/0/all/0/1\">Duong T.D Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Khanh Quoc Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fairness in Language Models Beyond English: Gaps and Challenges. (arXiv:2302.12578v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.12578","description":"<p>With language models becoming increasingly ubiquitous, it has become\nessential to address their inequitable treatment of diverse demographic groups\nand factors. Most research on evaluating and mitigating fairness harms has been\nconcentrated on English, while multilingual models and non-English languages\nhave received comparatively little attention. This paper presents a survey of\nfairness in multilingual and non-English contexts, highlighting the\nshortcomings of current research and the difficulties faced by methods designed\nfor English. We contend that the multitude of diverse cultures and languages\nacross the world makes it infeasible to achieve comprehensive coverage in terms\nof constructing fairness datasets. Thus, the measurement and mitigation of\nbiases must evolve beyond the current dataset-driven practices that are\nnarrowly focused on specific dimensions and types of biases and, therefore,\nimpossible to scale across languages and cultures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_K/0/1/0/all/0/1\">Krithika Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitaram_S/0/1/0/all/0/1\">Sunayana Sitaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spanish Built Factual Freectianary (Spanish-BFF): the first AI-generated free dictionary. (arXiv:2302.12746v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.12746","description":"<p>Dictionaries are one of the oldest and most used linguistic resources.\nBuilding them is a complex task that, to the best of our knowledge, has yet to\nbe explored with generative Large Language Models (LLMs). We introduce the\n\"Spanish Built Factual Freectianary\" (Spanish-BFF) as the first Spanish\nAI-generated dictionary. This first-of-its-kind free dictionary uses GPT-3. We\nalso define future steps we aim to follow to improve this initial commitment to\nthe field, such as more additional languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ortega_Martin_M/0/1/0/all/0/1\">Miguel Ortega-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Sierra_O/0/1/0/all/0/1\">&#xd3;scar Garc&#xed;a-Sierra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardoiz_A/0/1/0/all/0/1\">Alfonso Ardoiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armenteros_J/0/1/0/all/0/1\">Juan Carlos Armenteros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jorge &#xc1;lvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_A/0/1/0/all/0/1\">Adri&#xe1;n Alonso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-Finetuning for Few-Shot Emotional Speech Recognition. (arXiv:2302.12921v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.12921","description":"<p>Speech models have long been known to overfit individual speakers for many\nclassification tasks. This leads to poor generalization in settings where the\nspeakers are out-of-domain or out-of-distribution, as is common in production\nenvironments. We view speaker adaptation as a few-shot learning problem and\npropose investigating transfer learning approaches inspired by recent success\nwith pre-trained models in natural language tasks. We propose pre-finetuning\nspeech models on difficult tasks to distill knowledge into few-shot downstream\nclassification objectives. We pre-finetune Wav2Vec2.0 on every permutation of\nfour multiclass emotional speech recognition corpora and evaluate our\npre-finetuned models through 33,600 few-shot fine-tuning trials on the\nEmotional Speech Dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Maximillian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatAug: Leveraging ChatGPT for Text Data Augmentation. (arXiv:2302.13007v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.13007","description":"<p>Text data augmentation is an effective strategy for overcoming the challenge\nof limited sample sizes in many natural language processing (NLP) tasks. This\nchallenge is especially prominent in the few-shot learning scenario, where the\ndata in the target domain is generally much scarcer and of lowered quality. A\nnatural and widely-used strategy to mitigate such challenges is to perform data\naugmentation on the training data to better capture the data invariance and\nincrease the sample size. However, current text data augmentation methods\neither can not ensure the correct labeling of the generated data (lacking\nfaithfulness) or can not ensure sufficient diversity in the generated data\n(lacking completeness), or both. Inspired by the recent success of large\nlanguage models, especially the development of ChatGPT, which demonstrated\nimproved language comprehension abilities, in this work, we propose a text data\naugmentation approach based on ChatGPT (named ChatAug). ChatGPT is trained on\ndata with unparalleled linguistic richness and employs a reinforcement training\nprocess with large-scale human feedback, which endows the model with affinity\nto the naturalness of human language. Our text data augmentation approach\nChatAug rephrases each sentence in the training samples into multiple\nconceptually similar but semantically different samples. The augmented samples\ncan then be used in downstream model training. Experiment results on few-shot\nlearning text classification tasks show the superior performance of the\nproposed ChatAug approach over state-of-the-art text data augmentation methods\nin terms of testing accuracy and distribution of the augmented samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Haixing Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1\">Wenxiong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaoke Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ninghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dajiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hongmin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Quanzheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequential Query Encoding For Complex Query Answering on Knowledge Graphs. (arXiv:2302.13114v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.13114","description":"<p>Complex Query Answering (CQA) is an important and fundamental task for\nknowledge graph (KG) reasoning. Query encoding (QE) is proposed as a fast and\nrobust solution to CQA. In the encoding process, most existing QE methods first\nparse the logical query into an executable computational direct-acyclic graph\n(DAG), then use neural networks to parameterize the operators, and finally,\nrecursively execute these neuralized operators. However, the\nparameterization-and-execution paradigm may be potentially over-complicated, as\nit can be structurally simplified by a single neural network encoder.\nMeanwhile, sequence encoders, like LSTM and Transformer, proved to be effective\nfor encoding semantic graphs in related tasks. Motivated by this, we propose\nsequential query encoding (SQE) as an alternative to encode queries for CQA.\nInstead of parameterizing and executing the computational graph, SQE first uses\na search-based algorithm to linearize the computational graph to a sequence of\ntokens and then uses a sequence encoder to compute its vector representation.\nThen this vector representation is used as a query embedding to retrieve\nanswers from the embedding space according to similarity scores. Despite its\nsimplicity, SQE demonstrates state-of-the-art neural query encoding performance\non FB15k, FB15k-237, and NELL on an extended benchmark including twenty-nine\ntypes of in-distribution queries. Further experiment shows that SQE also\ndemonstrates comparable knowledge inference capability on out-of-distribution\nqueries, whose query types are not observed during the training process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiaxin Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1\">Tianshi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding Supporting Examples for In-Context Learning. (arXiv:2302.13539v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.13539","description":"<p>In-context learning is a new learning paradigm where a language model\nobserves a few examples and then straightly outputs the test input's\nprediction. Previous works have shown that in-context learning is sensitive to\nthe provided examples and randomly sampled examples show significantly unstable\nperformance. In this paper, we propose to find ``supporting examples'' for\nin-context learning: Given the training dataset, we need to select one\npermutation of a few examples, which are informative for the task's in-context\nlearning and lead to superior performance. Although in traditional\ngradient-based learning, e.g., fine-tuning, there are numerous methods to find\na ``coreset'' from the entire dataset, they are sub-optimal and not suitable\nfor this problem since in-context learning occurs in the language model's\ninference without gradients or parameter updates. Additionally, the strong\ndependence among in-context examples makes this problem an NP-hard\ncombinatorial optimization problem and enumerating all possible permutations is\ninfeasible. Hence we propose a two-stage method to tackle this challenge. First\nwe propose a novel metric to select informative examples based on the language\nmodel's feedback, with a progressive filtering strategy. And then we propose a\ndiversity-guided beam search method to refine and evaluate the selected\nexamples, iteratively. The experimental results show our method significantly\noutperforms a wide range of baselines, and further analyses show the\neffectiveness of our method and shed light on the properties of supporting\nexamples and in-context learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaonan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Independent Evaluation of ChatGPT on Mathematical Word Problems (MWP). (arXiv:2302.13814v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.13814","description":"<p>We study the performance of a commercially available large language model\n(LLM) known as ChatGPT on math word problems (MWPs) from the dataset DRAW-1K.\nTo our knowledge, this is the first independent evaluation of ChatGPT. We found\nthat ChatGPT's performance changes dramatically based on the requirement to\nshow its work, failing 20% of the time when it provides work compared with 84%\nwhen it does not. Further several factors about MWPs relating to the number of\nunknowns and number of operations that lead to a higher probability of failure\nwhen compared with the prior, specifically noting (across all experiments) that\nthe probability of failure increases linearly with the number of addition and\nsubtraction operations. We also have released the dataset of ChatGPT's\nresponses to the MWPs to support further work on the characterization of LLM\nperformance and present baseline machine learning models to predict if ChatGPT\ncan correctly answer an MWP. We have released a dataset comprised of ChatGPT's\nresponses to support further research in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shakarian_P/0/1/0/all/0/1\">Paulo Shakarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyyalamudi_A/0/1/0/all/0/1\">Abhinav Koyyalamudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngu_N/0/1/0/all/0/1\">Noel Ngu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mareedu_L/0/1/0/all/0/1\">Lakshmivihari Mareedu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks. (arXiv:2302.13939v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.13939","description":"<p>As the size of large language models continue to scale, so does the\ncomputational resources required to run it. Spiking neural networks (SNNs) have\nemerged as an energy-efficient approach to deep learning that leverage sparse\nand event-driven activations to reduce the computational overhead associated\nwith model inference. While they have become competitive with non-spiking\nmodels on many computer vision tasks, SNNs have also proven to be more\nchallenging to train. As a result, their performance lags behind modern deep\nlearning, and we are yet to see the effectiveness of SNNs in language\ngeneration. In this paper, inspired by the RWKV language model, we successfully\nimplement `SpikeGPT', a generative language model with pure binary,\nevent-driven spiking activation units. We train the proposed model on three\nmodel variants: 45M, 125M and 260M parameters. To the best of our knowledge,\nthis is 4x larger than any functional backprop-trained SNN to date. We achieve\nthis by modifying the transformer block to replace multi-head self attention to\nreduce quadratic computational complexity to linear with increasing sequence\nlength. Input tokens are instead streamed in sequentially to our attention\nmechanism (as with typical SNNs). Our preliminary experiments show that\nSpikeGPT remains competitive with non-spiking models on tested benchmarks,\nwhile maintaining 5x less energy consumption when processed on neuromorphic\nhardware that can leverage sparse, event-driven activations. Our code\nimplementation is available at https://github.com/ridgerchu/SpikeGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Rui-Jie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qihang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eshraghian_J/0/1/0/all/0/1\">Jason K. Eshraghian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-02-28T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}