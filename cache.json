{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-11-21T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"CoLI-Machine Learning Approaches for Code-mixed Language Identification at the Word Level in Kannada-English Texts. (arXiv:2211.09847v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09847","description":"<p>The task of automatically identifying a language used in a given text is\ncalled Language Identification (LI). India is a multilingual country and many\nIndians especially youths are comfortable with Hindi and English, in addition\nto their local languages. Hence, they often use more than one language to post\ntheir comments on social media. Texts containing more than one language are\ncalled \"code-mixed texts\" and are a good source of input for LI. Languages in\nthese texts may be mixed at sentence level, word level or even at sub-word\nlevel. LI at word level is a sequence labeling problem where each and every\nword in a sentence is tagged with one of the languages in the predefined set of\nlanguages. In order to address word level LI in code-mixed Kannada-English\n(Kn-En) texts, this work presents i) the construction of code-mixed Kn-En\ndataset called CoLI-Kenglish dataset, ii) code-mixed Kn-En embedding and iii)\nlearning models using Machine Learning (ML), Deep Learning (DL) and Transfer\nLearning (TL) approaches. Code-mixed Kn-En texts are extracted from Kannada\nYouTube video comments to construct CoLI-Kenglish dataset and code-mixed Kn-En\nembedding. The words in CoLI-Kenglish dataset are grouped into six major\ncategories, namely, \"Kannada\", \"English\", \"Mixed-language\", \"Name\", \"Location\"\nand \"Other\". The learning models, namely, CoLI-vectors and CoLI-ngrams based on\nML, CoLI-BiLSTM based on DL and CoLI-ULMFiT based on TL approaches are built\nand evaluated using CoLI-Kenglish dataset. The performances of the learning\nmodels illustrated, the superiority of CoLI-ngrams model, compared to other\nmodels with a macro average F1-score of 0.64. However, the results of all the\nlearning models were quite competitive with each other.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shashirekha_H/0/1/0/all/0/1\">H.L. Shashirekha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balouchzahi_F/0/1/0/all/0/1\">F. Balouchzahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anusha_M/0/1/0/all/0/1\">M.D. Anusha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidorov_G/0/1/0/all/0/1\">G. Sidorov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProtSi: Prototypical Siamese Network with Data Augmentation for Few-Shot Subjective Answer Evaluation. (arXiv:2211.09855v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09855","description":"<p>Subjective answer evaluation is a time-consuming and tedious task, and the\nquality of the evaluation is heavily influenced by a variety of subjective\npersonal characteristics. Instead, machine evaluation can effectively assist\neducators in saving time while also ensuring that evaluations are fair and\nrealistic. However, most existing methods using regular machine learning and\nnatural language processing techniques are generally hampered by a lack of\nannotated answers and poor model interpretability, making them unsuitable for\nreal-world use. To solve these challenges, we propose ProtSi Network, a unique\nsemi-supervised architecture that for the first time uses few-shot learning to\nsubjective answer evaluation. To evaluate students' answers by similarity\nprototypes, ProtSi Network simulates the natural process of evaluator scoring\nanswers by combining Siamese Network which consists of BERT and encoder layers\nwith Prototypical Network. We employed an unsupervised diverse paraphrasing\nmodel ProtAugment, in order to prevent overfitting for effective few-shot text\nclassification. By integrating contrastive learning, the discriminative text\nissue can be mitigated. Experiments on the Kaggle Short Scoring Dataset\ndemonstrate that the ProtSi Network outperforms the most recent baseline models\nin terms of accuracy and quadratic weighted kappa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yining Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jingxi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1\">Gaurav Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing Hallucinations in Neural Machine Translation with Feature Attribution. (arXiv:2211.09878v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09878","description":"<p>Neural conditional language generation models achieve the state-of-the-art in\nNeural Machine Translation (NMT) but are highly dependent on the quality of\nparallel training dataset. When trained on low-quality datasets, these models\nare prone to various error types, including hallucinations, i.e. outputs that\nare fluent, but unrelated to the source sentences. These errors are\nparticularly dangerous, because on the surface the translation can be perceived\nas a correct output, especially if the reader does not understand the source\nlanguage. We present a case study focusing on model understanding and\nregularisation to reduce hallucinations in NMT. We first use feature\nattribution methods to study the behaviour of an NMT model that produces\nhallucinations. We then leverage these methods to propose a novel loss function\nthat substantially helps reduce hallucinations and does not require retraining\nthe model from scratch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jo&#xeb;l Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fomicheva_M/0/1/0/all/0/1\">Marina Fomicheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summarizing Community-based Question-Answer Pairs. (arXiv:2211.09892v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09892","description":"<p>Community-based Question Answering (CQA), which allows users to acquire their\ndesired information, has increasingly become an essential component of online\nservices in various domains such as E-commerce, travel, and dining. However, an\noverwhelming number of CQA pairs makes it difficult for users without\nparticular intent to find useful information spread over CQA pairs. To help\nusers quickly digest the key information, we propose the novel CQA\nsummarization task that aims to create a concise summary from CQA pairs. To\nthis end, we first design a multi-stage data annotation process and create a\nbenchmark dataset, CoQASUM, based on the Amazon QA corpus. We then compare a\ncollection of extractive and abstractive summarization methods and establish a\nstrong baseline approach DedupLED for the CQA summarization task. Our\nexperiment further confirms two key challenges, sentence-type transfer and\ndeduplication removal, towards the CQA summarization task. Our data and code\nare publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_T/0/1/0/all/0/1\">Ting-Yao Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhara_Y/0/1/0/all/0/1\">Yoshi Suhara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Planning with Large Language Models via Corrective Re-prompting. (arXiv:2211.09935v1 [cs.AI])","link":"http://arxiv.org/abs/2211.09935","description":"<p>Extracting the common sense knowledge present in Large Language Models (LLMs)\noffers a path to designing intelligent, embodied agents. Related works have\nqueried LLMs with a wide-range of contextual information, such as goals, sensor\nobservations and scene descriptions, to generate high-level action plans for\nspecific tasks; however these approaches often involve human intervention or\nadditional machinery to enable sensor-motor interactions. In this work, we\npropose a prompting-based strategy for extracting executable plans from an LLM,\nwhich leverages a novel and readily-accessible source of information:\nprecondition errors. Our approach assumes that actions are only afforded\nexecution in certain contexts, i.e., implicit preconditions must be met for an\naction to execute (e.g., a door must be unlocked to open it), and that the\nembodied agent has the ability to determine if the action is/is not executable\nin the current context (e.g., detect if a precondition error is present). When\nan agent is unable to execute an action, our approach re-prompts the LLM with\nprecondition error information to extract an executable corrective action to\nachieve the intended goal in the current context. We evaluate our approach in\nthe VirtualHome simulation environment on 88 different tasks and 7 scenes. We\nevaluate different prompt templates and compare to methods that naively\nre-sample actions from the LLM. Our approach, using precondition errors,\nimproves executability and semantic correctness of plans, while also reducing\nthe number of re-prompts required when querying actions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raman_S/0/1/0/all/0/1\">Shreyas Sundara Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_V/0/1/0/all/0/1\">Vanya Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosen_E/0/1/0/all/0/1\">Eric Rosen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Idrees_I/0/1/0/all/0/1\">Ifrah Idrees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulius_D/0/1/0/all/0/1\">David Paulius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tellex_S/0/1/0/all/0/1\">Stefanie Tellex</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainability Via Causal Self-Talk. (arXiv:2211.09937v1 [cs.AI])","link":"http://arxiv.org/abs/2211.09937","description":"<p>Explaining the behavior of AI systems is an important problem that, in\npractice, is generally avoided. While the XAI community has been developing an\nabundance of techniques, most incur a set of costs that the wider deep learning\ncommunity has been unwilling to pay in most situations. We take a pragmatic\nview of the issue, and define a set of desiderata that capture both the\nambitions of XAI and the practical constraints of deep learning. We describe an\neffective way to satisfy all the desiderata: train the AI system to build a\ncausal model of itself. We develop an instance of this solution for Deep RL\nagents: Causal Self-Talk. CST operates by training the agent to communicate\nwith itself across time. We implement this method in a simulated 3D\nenvironment, and show how it enables agents to generate faithful and\nsemantically-meaningful explanations of their own behavior. Beyond\nexplanations, we also demonstrate that these learned models provide new ways of\nbuilding semantic control interfaces to AI systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_N/0/1/0/all/0/1\">Nicholas A. Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junkyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabinowitz_N/0/1/0/all/0/1\">Neil Rabinowitz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Professional Presentation and Projected Power: A Case Study of Implicit Gender Information in English CVs. (arXiv:2211.09942v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09942","description":"<p>Gender discrimination in hiring is a pertinent and persistent bias in\nsociety, and a common motivating example for exploring bias in NLP. However,\nthe manifestation of gendered language in application materials has received\nlimited attention. This paper investigates the framing of skills and background\nin CVs of self-identified men and women. We introduce a data set of 1.8K\nauthentic, English-language, CVs from the US, covering 16 occupations, allowing\nus to partially control for the confound occupation-specific gender base rates.\nWe find that (1) women use more verbs evoking impressions of low power; and (2)\nclassifiers capture gender signal even after data balancing and removal of\npronouns and named entities, and this holds for both transformer-based and\nlinear classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinrui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Njoto_S/0/1/0/all/0/1\">Sheilla Njoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheong_M/0/1/0/all/0/1\">Marc Cheong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruppanner_L/0/1/0/all/0/1\">Leah Ruppanner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frermann_L/0/1/0/all/0/1\">Lea Frermann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MelHuBERT: A simplified HuBERT on Mel spectrogram. (arXiv:2211.09944v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09944","description":"<p>Self-supervised models have had great success in learning speech\nrepresentations that can generalize to various downstream tasks. HuBERT, in\nparticular, achieves strong performance while being relatively simple in\ntraining compared to others. The original experimental setting is\ncomputationally extensive, hindering the reproducibility of the models. It is\nalso unclear why certain design decisions are made, such as the ad-hoc loss\nfunction, and whether these decisions have an impact on the learned\nrepresentations. We propose MelHuBERT, a simplified version of HuBERT that\ntakes Mel spectrograms as input, significantly reducing computation and memory\nconsumption. We study several aspects of training, including the loss function,\nmulti-stage training, and streaming options. Our result is a efficient yet\nperformant model that can be trained on a single GPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tzu-Quan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compressing Transformer-based self-supervised models for speech processing. (arXiv:2211.09949v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09949","description":"<p>Despite the success of Transformers in self-supervised learning with\napplications to various downstream tasks, the computational cost of training\nand inference remains a major challenge for applying these models to a wide\nspectrum of devices. Several isolated attempts have been made to compress\nTransformers, prior to applying them to downstream tasks. In this work, we aim\nto provide context for the isolated results, studying several commonly used\ncompression techniques, including weight pruning, head pruning, low-rank\napproximation, and knowledge distillation. We report wall-clock time, the\nnumber of parameters, and the number of multiply-accumulate operations for\nthese techniques, charting the landscape of compressing Transformer-based\nself-supervised models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tzu-Quan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tsung-Huan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chun-Yao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kuang-Ming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1\">Tzu-hsun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Explaining Subjective Ground of Individuals on Social Media. (arXiv:2211.09953v1 [cs.CL])","link":"http://arxiv.org/abs/2211.09953","description":"<p>Large-scale language models have been reducing the gap between machines and\nhumans in understanding the real world, yet understanding an individual's\ntheory of mind and behavior from text is far from being resolved.\n</p>\n<p>This research proposes a neural model -- Subjective Ground Attention -- that\nlearns subjective grounds of individuals and accounts for their judgments on\nsituations of others posted on social media. Using simple attention modules as\nwell as taking one's previous activities into consideration, we empirically\nshow that our model provides human-readable explanations of an individual's\nsubjective preference in judging social situations. We further qualitatively\nevaluate the explanations generated by the model and claim that our model\nlearns an individual's subjective orientation towards abstract moral concepts\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Younghun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Who Says Elephants Can't Run: Bringing Large Scale MoE Models into Cloud Scale Production. (arXiv:2211.10017v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10017","description":"<p>Mixture of Experts (MoE) models with conditional execution of sparsely\nactivated layers have enabled training models with a much larger number of\nparameters. As a result, these models have achieved significantly better\nquality on various natural language processing tasks including machine\ntranslation. However, it remains challenging to deploy such models in real-life\nscenarios due to the large memory requirements and inefficient inference. In\nthis work, we introduce a highly efficient inference framework with several\noptimization approaches to accelerate the computation of sparse models and cut\ndown the memory consumption significantly. While we achieve up to 26x speed-up\nin terms of throughput, we also reduce the model size almost to one eighth of\nthe original 32-bit float model by quantizing expert weights into 4-bit\nintegers. As a result, we are able to deploy 136x larger models with 27% less\ncost and significantly better quality compared to the existing solutions. This\nenables a paradigm shift in deploying large scale multilingual MoE transformers\nmodels replacing the traditional practice of distilling teacher models into\ndozens of smaller models per language or task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henry_R/0/1/0/all/0/1\">Rawn Henry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fahim_R/0/1/0/all/0/1\">Raffy Fahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadalla_H/0/1/0/all/0/1\">Hany Hassan Awadalla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dataset for Hyper-Relational Extraction and a Cube-Filling Approach. (arXiv:2211.10018v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10018","description":"<p>Relation extraction has the potential for large-scale knowledge graph\nconstruction, but current methods do not consider the qualifier attributes for\neach relation triplet, such as time, quantity or location. The qualifiers form\nhyper-relational facts which better capture the rich and complex knowledge\ngraph structure. For example, the relation triplet (Leonard Parker, Educated\nAt, Harvard University) can be factually enriched by including the qualifier\n(End Time, 1967). Hence, we propose the task of hyper-relational extraction to\nextract more specific and complete facts from text. To support the task, we\nconstruct HyperRED, a large-scale and general-purpose dataset. Existing models\ncannot perform hyper-relational extraction as it requires a model to consider\nthe interaction between three entities. Hence, we propose CubeRE, a\ncube-filling model inspired by table-filling approaches and explicitly\nconsiders the interaction between relation triplets and qualifiers. To improve\nmodel scalability and reduce negative class imbalance, we further propose a\ncube-pruning method. Our experiments show that CubeRE outperforms strong\nbaselines and reveal possible directions for future research. Our code and data\nare available at github.com/declare-lab/HyperRED.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chia_Y/0/1/0/all/0/1\">Yew Ken Chia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aljunied_S/0/1/0/all/0/1\">Sharifah Mahani Aljunied</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overview of the WANLP 2022 Shared Task on Propaganda Detection in Arabic. (arXiv:2211.10057v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10057","description":"<p>Propaganda is the expression of an opinion or an action by an individual or a\ngroup deliberately designed to influence the opinions or the actions of other\nindividuals or groups with reference to predetermined ends, which is achieved\nby means of well-defined rhetorical and psychological devices. Propaganda\ntechniques are commonly used in social media to manipulate or to mislead users.\nThus, there has been a lot of recent research on automatic detection of\npropaganda techniques in text as well as in memes. However, so far the focus\nhas been primarily on English. With the aim to bridge this language gap, we ran\na shared task on detecting propaganda techniques in Arabic tweets as part of\nthe WANLP 2022 workshop, which included two subtasks. Subtask~1 asks to\nidentify the set of propaganda techniques used in a tweet, which is a\nmultilabel classification problem, while Subtask~2 asks to detect the\npropaganda techniques used in a tweet together with the exact span(s) of text\nin which each propaganda technique appears. The task attracted 63 team\nregistrations, and eventually 14 and 3 teams made submissions for subtask 1 and\n2, respectively. Finally, 11 teams submitted system description papers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaghouani_W/0/1/0/all/0/1\">Wajdi Zaghouani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metadata Might Make Language Models Better. (arXiv:2211.10086v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10086","description":"<p>This paper discusses the benefits of including metadata when training\nlanguage models on historical collections. Using 19th-century newspapers as a\ncase study, we extend the time-masking approach proposed by Rosin et al., 2022\nand compare different strategies for inserting temporal, political and\ngeographical information into a Masked Language Model. After fine-tuning\nseveral DistilBERT on enhanced input data, we provide a systematic evaluation\nof these models on a set of evaluation tasks: pseudo-perplexity, metadata\nmask-filling and supervised classification. We find that showing relevant\nmetadata to a language model has a beneficial impact and may even produce more\nrobust and fairer models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beelen_K/0/1/0/all/0/1\">Kaspar Beelen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strien_D/0/1/0/all/0/1\">Daniel van Strien</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Native Language Identification with Transformer Adapters. (arXiv:2211.10117v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10117","description":"<p>Native language identification (NLI) is the task of automatically identifying\nthe native language (L1) of an individual based on their language production in\na learned language. It is useful for a variety of purposes including marketing,\nsecurity and educational applications. NLI is usually framed as a multi-label\nclassification task, where numerous designed features are combined to achieve\nstate-of-the-art results. Recently deep generative approach based on\ntransformer decoders (GPT-2) outperformed its counterparts and achieved the\nbest results on the NLI benchmark datasets. We investigate this approach to\ndetermine the practical implications compared to traditional state-of-the-art\nNLI systems. We introduce transformer adapters to address memory limitations\nand improve training/inference speed to scale NLI applications for production.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uluslu_A/0/1/0/all/0/1\">Ahmet Yavuz Uluslu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_G/0/1/0/all/0/1\">Gerold Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FiE: Building a Global Probability Space by Leveraging Early Fusion in Encoder for Open-Domain Question Answering. (arXiv:2211.10147v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10147","description":"<p>Generative models have recently started to outperform extractive models in\nOpen Domain Question Answering, largely by leveraging their decoder to attend\nover multiple encoded passages and combining their information. However,\ngenerative models tend to be larger than extractive models due to the need for\na decoder, run slower during inference due to auto-regressive decoder beam\nsearch, and their generated output often suffers from hallucinations. We\npropose to extend transformer encoders with the ability to fuse information\nfrom multiple passages, using global representation to provide cross-sample\nattention over all tokens across samples. Furthermore, we propose an\nalternative answer span probability calculation to better aggregate answer\nscores in the global space of all samples. Using our proposed method, we\noutperform the current state-of-the-art method by $2.5$ Exact Match score on\nthe Natural Question dataset while using only $25\\%$ of parameters and $35\\%$\nof the latency during inference, and $4.4$ Exact Match on WebQuestions dataset.\nWhen coupled with synthetic data augmentation, we outperform larger models on\nthe TriviaQA dataset as well. The latency and parameter savings of our method\nmake it particularly attractive for open-domain question answering, as these\nmodels are often compute-intensive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kedia_A/0/1/0/all/0/1\">Akhil Kedia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaidi_M/0/1/0/all/0/1\">Mohd Abbas Zaidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Haejun Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overview of the HASOC Subtrack at FIRE 2022: Offensive Language Identification in Marathi. (arXiv:2211.10163v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10163","description":"<p>The widespread of offensive content online has become a reason for great\nconcern in recent years, motivating researchers to develop robust systems\ncapable of identifying such content automatically. With the goal of carrying\nout a fair evaluation of these systems, several international competitions have\nbeen organized, providing the community with important benchmark data and\nevaluation methods for various languages. Organized since 2019, the HASOC (Hate\nSpeech and Offensive Content Identification) shared task is one of these\ninitiatives. In its fourth iteration, HASOC 2022 included three subtracks for\nEnglish, Hindi, and Marathi. In this paper, we report the results of the HASOC\n2022 Marathi subtrack which provided participants with a dataset containing\ndata from Twitter manually annotated using the popular OLID taxonomy. The\nMarathi track featured three additional subtracks, each corresponding to one\nlevel of the taxonomy: Task A - offensive content identification (offensive vs.\nnon-offensive); Task B - categorization of offensive types (targeted vs.\nuntargeted), and Task C - offensive target identification (individual vs. group\nvs. others). Overall, 59 runs were submitted by 10 teams. The best systems\nobtained an F1 of 0.9745 for Subtrack 3A, an F1 of 0.9207 for Subtrack 3B, and\nF1 of 0.9607 for Subtrack 3C. The best performing algorithms were a mixture of\ntraditional and deep learning approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+North_K/0/1/0/all/0/1\">Kai North</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Premasiri_D/0/1/0/all/0/1\">Damith Premasiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GoSum: Extractive Summarization of Long Documents by Reinforcement Learning and Graph Organized discourse state. (arXiv:2211.10247v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10247","description":"<p>Handling long texts with structural information and excluding redundancy\nbetween summary sentences are essential in extractive document summarization.\nIn this work, we propose GoSum, a novel reinforcement-learning-based extractive\nmodel for long-paper summarization. GoSum encodes states by building a\nheterogeneous graph from different discourse levels for each input document. We\nevaluate the model on two datasets of scientific articles summarization: PubMed\nand arXiv where it outperforms all extractive summarization models and most of\nthe strong abstractive baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Junyi Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaodi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shanfeng Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context Variance Evaluation of Pretrained Language Models for Prompt-based Biomedical Knowledge Probing. (arXiv:2211.10265v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10265","description":"<p>Pretrained language models (PLMs) have motivated research on what kinds of\nknowledge these models learn. Fill-in-the-blanks problem (e.g., cloze tests) is\na natural approach for gauging such knowledge. BioLAMA generates prompts for\nbiomedical factual knowledge triples and uses the Top-k accuracy metric to\nevaluate different PLMs' knowledge. However, existing research has shown that\nsuch prompt-based knowledge probing methods can only probe a lower bound of\nknowledge. Many factors like prompt-based probing biases make the LAMA\nbenchmark unreliable and unstable. This problem is more prominent in BioLAMA.\nThe severe long-tailed distribution in vocabulary and large-N-M relation make\nthe performance gap between LAMA and BioLAMA remain notable. To address these,\nwe introduce context variance into the prompt generation and propose a new\nrank-change-based evaluation metric. Different from the previous known-unknown\nevaluation criteria, we propose the concept of \"Misunderstand\" in LAMA for the\nfirst time. Through experiments on 12 PLMs, our context variance prompts and\nUnderstand-Confuse-Misunderstand (UCM) metric makes BioLAMA more friendly to\nlarge-N-M relations and rare relations. We also conducted a set of control\nexperiments to disentangle \"understand\" from just \"read and copy\".\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zonghai Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Copy Mechanism for Handling Knowledge Base Elements in SPARQL Neural Machine Translation. (arXiv:2211.10271v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10271","description":"<p>Neural Machine Translation (NMT) models from English to SPARQL are a\npromising development for SPARQL query generation. However, current\narchitectures are unable to integrate the knowledge base (KB) schema and handle\nquestions on knowledge resources, classes, and properties unseen during\ntraining, rendering them unusable outside the scope of topics covered in the\ntraining set. Inspired by the performance gains in natural language processing\ntasks, we propose to integrate a copy mechanism for neural SPARQL query\ngeneration as a way to tackle this issue. We illustrate our proposal by adding\na copy layer and a dynamic knowledge base vocabulary to two Seq2Seq\narchitectures (CNNs and Transformers). This layer makes the models copy KB\nelements directly from the questions, instead of generating them. We evaluate\nour approach on state-of-the-art datasets, including datasets referencing\nunknown KB elements and measure the accuracy of the copy-augmented\narchitectures. Our results show a considerable increase in performance on all\ndatasets compared to non-copy architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hirigoyen_R/0/1/0/all/0/1\">Rose Hirigoyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zouaq_A/0/1/0/all/0/1\">Amal Zouaq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reyd_S/0/1/0/all/0/1\">Samuel Reyd</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GENIUS: Sketch-based Language Model Pre-training via Extreme and Selective Masking for Text Generation and Augmentation. (arXiv:2211.10330v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10330","description":"<p>We introduce GENIUS: a conditional text generation model using sketches as\ninput, which can fill in the missing contexts for a given sketch (key\ninformation consisting of textual spans, phrases, or words, concatenated by\nmask tokens). GENIUS is pre-trained on a large-scale textual corpus with a\nnovel reconstruction from sketch objective using an extreme and selective\nmasking strategy, enabling it to generate diverse and high-quality texts given\nsketches. Comparison with other competitive conditional language models (CLMs)\nreveals the superiority of GENIUS's text generation quality. We further show\nthat GENIUS can be used as a strong and ready-to-use data augmentation tool for\nvarious natural language processing (NLP) tasks. Most existing textual data\naugmentation methods are either too conservative, by making small changes to\nthe original text, or too aggressive, by creating entirely new samples. With\nGENIUS, we propose GeniusAug, which first extracts the target-aware sketches\nfrom the original training set and then generates new samples based on the\nsketches. Empirical experiments on 6 text classification datasets show that\nGeniusAug significantly improves the models' performance in both\nin-distribution (ID) and out-of-distribution (OOD) settings. We also\ndemonstrate the effectiveness of GeniusAug on named entity recognition (NER)\nand machine reading comprehension (MRC) tasks. (Code and models are publicly\navailable at https://github.com/microsoft/SCGLab and\nhttps://github.com/beyondguo/genius)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Biyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Songqiao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hailiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CITADEL: Conditional Token Interaction via Dynamic Lexical Routing for Efficient and Effective Multi-Vector Retrieval. (arXiv:2211.10411v1 [cs.IR])","link":"http://arxiv.org/abs/2211.10411","description":"<p>Multi-vector retrieval methods combine the merits of sparse (e.g. BM25) and\ndense (e.g. DPR) retrievers and have achieved state-of-the-art performance on\nvarious retrieval tasks. These methods, however, are orders of magnitude slower\nand need much more space to store their indices compared to their single-vector\ncounterparts. In this paper, we unify different multi-vector retrieval models\nfrom a token routing viewpoint and propose conditional token interaction via\ndynamic lexical routing, namely CITADEL, for efficient and effective\nmulti-vector retrieval. CITADEL learns to route different token vectors to the\npredicted lexical ``keys'' such that a query token vector only interacts with\ndocument token vectors routed to the same key. This design significantly\nreduces the computation cost while maintaining high accuracy. Notably, CITADEL\nachieves the same or slightly better performance than the previous state of the\nart, ColBERT-v2, on both in-domain (MS MARCO) and out-of-domain (BEIR)\nevaluations, while being nearly 40 times faster. Code and data are available at\nhttps://github.com/facebookresearch/dpr-scale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minghan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Sheng-Chieh Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas Oguz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghoshal_A/0/1/0/all/0/1\">Asish Ghoshal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PAL: Program-aided Language Models. (arXiv:2211.10435v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10435","description":"<p>Large language models (LLMs) have recently demonstrated an impressive ability\nto perform arithmetic and symbolic reasoning tasks when provided with a few\nexamples at test time (few-shot prompting). Much of this success can be\nattributed to prompting methods for reasoning, such as chain-of-thought, that\nemploy LLMs for both understanding the problem description by decomposing it\ninto steps, as well as solving each step of the problem. While LLMs seem to be\nadept at this sort of step-by-step decomposition, LLMs often make logical and\narithmetic mistakes in the solution part, even when the problem is correctly\ndecomposed. We present Program-Aided Language models (PaL): a new method that\nuses the LLM to understand natural language problems and generate programs as\nthe intermediate reasoning steps, but offloads the solution step to a\nprogrammatic runtime such as a Python interpreter. With PaL, decomposing the\nnatural language problem into runnable steps remains the only learning task for\nthe LLM, while solving is delegated to the interpreter. We experiment with 12\nreasoning tasks from BIG-Bench Hard and other benchmarks, including\nmathematical reasoning, symbolic reasoning, and algorithmic problems. In all\nthese natural language reasoning tasks, generating code using an LLM and\nreasoning using a Python interpreter leads to more accurate results than much\nlarger models, and we set new state-of-the-art results in all 12 benchmarks.\nFor example, PaL using Codex achieves state-of-the-art few-shot accuracy on the\nGSM benchmark of math word problems when the model is allowed only a single\ndecoding, surpassing PaLM-540B with chain-of-thought prompting by an absolute\n8% .In three reasoning tasks from the BIG-Bench Hard benchmark, PaL outperforms\nCoT by 11%. On GSM-hard, a more challenging version of GSM that we create, PaL\noutperforms chain-of-thought by an absolute 40%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Luyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1\">Uri Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callan_J/0/1/0/all/0/1\">Jamie Callan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. (arXiv:2211.10438v1 [cs.CL])","link":"http://arxiv.org/abs/2211.10438","description":"<p>Large language models (LLMs) show excellent performance but are compute- and\nmemory-intensive. Quantization can reduce memory and accelerate inference.\nHowever, for LLMs beyond 100 billion parameters, existing methods cannot\nmaintain accuracy or do not run efficiently on hardware. We propose\nSmoothQuant, a training-free, accuracy-preserving, and general-purpose\npost-training quantization (PTQ) solution to enable 8-bit weight, 8-bit\nactivation (W8A8) quantization for LLMs that can be implemented efficiently. We\nobserve that systematic outliers appear at fixed activation channels. Based on\nthe fact that weights are easy to quantize while activations are not,\nSmoothQuant smooths the activation outliers by migrating the quantization\ndifficulty from activations to weights with a mathematically equivalent\ntransformation. SmoothQuant enables an INT8 quantization of both weights and\nactivations for all the GEMMs in LLMs, including OPT-175B, BLOOM-176B and\nGLM-130B. SmoothQuant has better hardware efficiency than existing techniques\nusing mixed-precision activation quantization or weight-only quantization. We\ndemonstrate up to 1.56x speedup and 2x memory reduction for LLMs with\nnegligible loss in accuracy. Thanks to the hardware-friendly design, we\nintegrate SmoothQuant into FasterTransformer, a state-of-the-art LLM serving\nframework, and achieve faster inference speed with half the number of GPUs\ncompared to FP16. Our work offers a turn-key solution that reduces hardware\ncosts and democratizes LLMs. Code will be released at:\nhttps://github.com/mit-han-lab/smoothquant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1\">Guangxuan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Ji Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seznec_M/0/1/0/all/0/1\">Mickael Seznec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demouth_J/0/1/0/all/0/1\">Julien Demouth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling chronic pain experiences from online reports using the Reddit Reports of Chronic Pain dataset. (arXiv:2108.10218v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.10218","description":"<p>Objective: Reveal and quantify qualities of reported experiences of chronic\npain on social media, from multiple pathological backgrounds, by means of the\nnovel Reddit Reports of Chronic Pain (RRCP) dataset, using Natural Language\nProcessing techniques. Materials and Methods: Define and validate the RRCP\ndataset for a set of subreddits related to chronic pain. Identify the main\nconcerns discussed in each subreddit. Model each subreddit according to their\nmain concerns. Compare subreddit models. Results: The RRCP dataset comprises\n86,537 Reddit submissions from 12 subreddits related to chronic pain (each\nrelated to one pathological background). Each RRCP subreddit has various main\nconcerns. Some of these concerns are shared between multiple subreddits (e.g.,\nthe subreddit Sciatica semantically entails the subreddit backpain in their\nvarious concerns, but not the other way around), whilst some concerns are\nexclusive to specific subreddits (e.g., Interstitialcystitis and\nCrohnsDisease). Discussion: These results suggest that the reported experience\nof chronic pain, from multiple pathologies (i.e., subreddits), has concerns\nrelevant to all, and concerns exclusive to certain pathologies. Our analysis\ndetails each of these concerns and their similarity relations. Conclusion:\nAlthough limited by intrinsic qualities of the Reddit platform, to the best of\nour knowledge, this is the first research work attempting to model the\nlinguistic expression of various chronic pain-inducing pathologies and\ncomparing these models to identify and quantify the similarities and\ndifferences between the corresponding emergent chronic pain experiences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nunes_D/0/1/0/all/0/1\">Diogo A.P. Nunes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_Gomes_J/0/1/0/all/0/1\">Joana Ferreira-Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neto_F/0/1/0/all/0/1\">Fani Neto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matos_D/0/1/0/all/0/1\">David Martins de Matos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cheating Automatic Short Answer Grading: On the Adversarial Usage of Adjectives and Adverbs. (arXiv:2201.08318v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.08318","description":"<p>Automatic grading models are valued for the time and effort saved during the\ninstruction of large student bodies. Especially with the increasing\ndigitization of education and interest in large-scale standardized testing, the\npopularity of automatic grading has risen to the point where commercial\nsolutions are widely available and used. However, for short answer formats,\nautomatic grading is challenging due to natural language ambiguity and\nversatility. While automatic short answer grading models are beginning to\ncompare to human performance on some datasets, their robustness, especially to\nadversarially manipulated data, is questionable. Exploitable vulnerabilities in\ngrading models can have far-reaching consequences ranging from cheating\nstudents receiving undeserved credit to undermining automatic grading\naltogether - even when most predictions are valid. In this paper, we devise a\nblack-box adversarial attack tailored to the educational short answer grading\nscenario to investigate the grading models' robustness. In our attack, we\ninsert adjectives and adverbs into natural places of incorrect student answers,\nfooling the model into predicting them as correct. We observed a loss of\nprediction accuracy between 10 and 22 percentage points using the\nstate-of-the-art models BERT and T5. While our attack made answers appear less\nnatural to humans in our experiments, it did not significantly increase the\ngraders' suspicions of cheating. Based on our experiments, we provide\nrecommendations for utilizing automatic grading systems more safely in\npractice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Filighera_A/0/1/0/all/0/1\">Anna Filighera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochs_S/0/1/0/all/0/1\">Sebastian Ochs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steuer_T/0/1/0/all/0/1\">Tim Steuer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tregel_T/0/1/0/all/0/1\">Thomas Tregel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaPrompt: Adaptive Model Training for Prompt-based NLP. (arXiv:2202.04824v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.04824","description":"<p>Prompt-based learning, with its capability to tackle zero-shot and few-shot\nNLP tasks, has gained much attention in community. The main idea is to bridge\nthe gap between NLP downstream tasks and language modeling (LM), by mapping\nthese tasks into natural language prompts, which are then filled by pre-trained\nlanguage models (PLMs). However, for prompt learning, there are still two\nsalient gaps between NLP tasks and pretraining. First, prompt information is\nnot necessarily sufficiently present during LM pretraining. Second,\ntask-specific data are not necessarily well represented during pretraining. We\naddress these two issues by proposing AdaPrompt, adaptively retrieving external\ndata for continual pretraining of PLMs by making use of both task and prompt\ncharacteristics. In addition, we make use of knowledge in Natural Language\nInference models for deriving adaptive verbalizers. Experimental results on\nfive NLP benchmarks show that AdaPrompt can improve over standard PLMs in\nfew-shot settings. In addition, in zero-shot settings, our method outperforms\nstandard prompt-based methods by up to 26.35\\% relative error reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone. (arXiv:2206.07643v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.07643","description":"<p>Vision-language (VL) pre-training has recently received considerable\nattention. However, most existing end-to-end pre-training approaches either\nonly aim to tackle VL tasks such as image-text retrieval, visual question\nanswering (VQA) and image captioning that test high-level understanding of\nimages, or only target region-level understanding for tasks such as phrase\ngrounding and object detection. We present FIBER (Fusion-In-the-Backbone-based\ntransformER), a new VL model architecture that can seamlessly handle both these\ntypes of tasks. Instead of having dedicated transformer layers for fusion after\nthe uni-modal backbones, FIBER pushes multimodal fusion deep into the model by\ninserting cross-attention into the image and text backbones, bringing gains in\nterms of memory and performance. In addition, unlike previous work that is\neither only pre-trained on image-text data or on fine-grained data with\nbox-level annotations, we present a two-stage pre-training strategy that uses\nboth these kinds of data efficiently: (i) coarse-grained pre-training based on\nimage-text data; followed by (ii) fine-grained pre-training based on\nimage-text-box data. We conduct comprehensive experiments on a wide range of VL\ntasks, ranging from VQA, image captioning, and retrieval, to phrase grounding,\nreferring expression comprehension, and object detection. Using deep multimodal\nfusion coupled with the two-stage pre-training, FIBER provides consistent\nperformance improvements over strong baselines across all tasks, often\noutperforming methods using magnitudes more data. Code is available at\nhttps://github.com/microsoft/FIBER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zi-Yi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_A/0/1/0/all/0/1\">Aishwarya Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Algorithms for Weighted Pushdown Automata. (arXiv:2210.06884v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06884","description":"<p>Weighted pushdown automata (WPDAs) are at the core of many natural language\nprocessing tasks, like syntax-based statistical machine translation and\ntransition-based dependency parsing. As most existing dynamic programming\nalgorithms are designed for context-free grammars (CFGs), algorithms for PDAs\noften resort to a PDA-to-CFG conversion. In this paper, we develop novel\nalgorithms that operate directly on WPDAs. Our algorithms are inspired by\nLang's algorithm, but use a more general definition of pushdown automaton and\neither reduce the space requirements by a factor of $|\\Gamma|$ (the size of the\nstack alphabet) or reduce the runtime by a factor of more than $|Q|$ (the\nnumber of states). When run on the same class of PDAs as Lang's algorithm, our\nalgorithm is both more space-efficient by a factor of $|\\Gamma|$ and more\ntime-efficient by a factor of $|Q| \\cdot |\\Gamma|$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Butoi_A/0/1/0/all/0/1\">Alexandra Butoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DuSell_B/0/1/0/all/0/1\">Brian DuSell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vieira_T/0/1/0/all/0/1\">Tim Vieira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1\">David Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simulating realistic speech overlaps improves multi-talker ASR. (arXiv:2210.15715v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2210.15715","description":"<p>Multi-talker automatic speech recognition (ASR) has been studied to generate\ntranscriptions of natural conversation including overlapping speech of multiple\nspeakers. Due to the difficulty in acquiring real conversation data with\nhigh-quality human transcriptions, a na\\\"ive simulation of multi-talker speech\nby randomly mixing multiple utterances was conventionally used for model\ntraining. In this work, we propose an improved technique to simulate\nmulti-talker overlapping speech with realistic speech overlaps, where an\narbitrary pattern of speech overlaps is represented by a sequence of discrete\ntokens. With this representation, speech overlapping patterns can be learned\nfrom real conversations based on a statistical language model, such as N-gram,\nwhich can be then used to generate multi-talker speech for training. In our\nexperiments, multi-talker ASR models trained with the proposed method show\nconsistent improvement on the word error rates across multiple datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_M/0/1/0/all/0/1\">Muqiao Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sivasankaran_S/0/1/0/all/0/1\">Sunit Sivasankaran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning an Artificial Language for Knowledge-Sharing in Multilingual Translation. (arXiv:2211.01292v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.01292","description":"<p>The cornerstone of multilingual neural translation is shared representations\nacross languages. Given the theoretically infinite representation power of\nneural networks, semantically identical sentences are likely represented\ndifferently. While representing sentences in the continuous latent space\nensures expressiveness, it introduces the risk of capturing of irrelevant\nfeatures which hinders the learning of a common representation. In this work,\nwe discretize the encoder output latent space of multilingual models by\nassigning encoder states to entries in a codebook, which in effect represents\nsource sentences in a new artificial language. This discretization process not\nonly offers a new way to interpret the otherwise black-box model\nrepresentations, but, more importantly, gives potential for increasing\nrobustness in unseen testing conditions. We validate our approach on\nlarge-scale experiments with realistic data volumes and domains. When tested in\nzero-shot conditions, our approach is competitive with two strong alternatives\nfrom the literature. We also use the learned artificial language to analyze\nmodel behavior, and discover that using a similar bridge language increases\nknowledge-sharing among the remaining languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Danni Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niehues_J/0/1/0/all/0/1\">Jan Niehues</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SLATE: A Sequence Labeling Approach for Task Extraction from Free-form Inked Content. (arXiv:2211.04454v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.04454","description":"<p>We present SLATE, a sequence labeling approach for extracting tasks from\nfree-form content such as digitally handwritten (or \"inked\") notes on a virtual\nwhiteboard. Our approach allows us to create a single, low-latency model to\nsimultaneously perform sentence segmentation and classification of these\nsentences into task/non-task sentences. SLATE greatly outperforms a baseline\ntwo-model (sentence segmentation followed by classification model) approach,\nachieving a task F1 score of 84.4%, a sentence segmentation (boundary\nsimilarity) score of 88.4% and three times lower latency compared to the\nbaseline. Furthermore, we provide insights into tackling challenges of\nperforming NLP on the inking domain. We release both our code and dataset for\nthis novel task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_A/0/1/0/all/0/1\">Apurva Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serrao_R/0/1/0/all/0/1\">Ryan Serrao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1\">Biyi Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antonius_G/0/1/0/all/0/1\">Gilbert Antonius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jenna Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tra My Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Sheng Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nosakhare_E/0/1/0/all/0/1\">Ehi Nosakhare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaffer_I/0/1/0/all/0/1\">Irene Shaffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Soundararajan Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vivek Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEE: A Novel Multilingual Event Extraction Dataset. (arXiv:2211.05955v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05955","description":"<p>Event Extraction (EE) is one of the fundamental tasks in Information\nExtraction (IE) that aims to recognize event mentions and their arguments\n(i.e., participants) from text. Due to its importance, extensive methods and\nresources have been developed for Event Extraction. However, one limitation of\ncurrent research for EE involves the under-exploration for non-English\nlanguages in which the lack of high-quality multilingual EE datasets for model\ntraining and evaluation has been the main hindrance. To address this\nlimitation, we propose a novel Multilingual Event Extraction dataset (MEE) that\nprovides annotation for more than 50K event mentions in 8 typologically\ndifferent languages. MEE comprehensively annotates data for entity mentions,\nevent triggers and event arguments. We conduct extensive experiments on the\nproposed dataset to reveal challenges and opportunities for multilingual EE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Veyseh_A/0/1/0/all/0/1\">Amir Pouran Ben Veyseh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_J/0/1/0/all/0/1\">Javid Ebrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thien Huu Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MINION: a Large-Scale and Diverse Dataset for Multilingual Event Detection. (arXiv:2211.05958v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05958","description":"<p>Event Detection (ED) is the task of identifying and classifying trigger words\nof event mentions in text. Despite considerable research efforts in recent\nyears for English text, the task of ED in other languages has been\nsignificantly less explored. Switching to non-English languages, important\nresearch questions for ED include how well existing ED models perform on\ndifferent languages, how challenging ED is in other languages, and how well ED\nknowledge and annotation can be transferred across languages. To answer those\nquestions, it is crucial to obtain multilingual ED datasets that provide\nconsistent event annotation for multiple languages. There exist some\nmultilingual ED datasets; however, they tend to cover a handful of languages\nand mainly focus on popular ones. Many languages are not covered in existing\nmultilingual ED datasets. In addition, the current datasets are often small and\nnot accessible to the public. To overcome those shortcomings, we introduce a\nnew large-scale multilingual dataset for ED (called MINION) that consistently\nannotates events for 8 different languages; 5 of them have not been supported\nby existing multilingual datasets. We also perform extensive experiments and\nanalysis to demonstrate the challenges and transferability of ED across\nlanguages in MINION that in all call for more research effort in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Veyseh_A/0/1/0/all/0/1\">Amir Pouran Ben Veyseh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thien Huu Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Knowledge-Enhanced Pre-trained Language Models. (arXiv:2211.05994v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05994","description":"<p>Pre-trained Language Models (PLMs) which are trained on large text corpus via\nself-supervised learning method, have yielded promising performance on various\ntasks in Natural Language Processing (NLP). However, though PLMs with huge\nparameters can effectively possess rich knowledge learned from massive training\ntext and benefit downstream tasks at the fine-tuning stage, they still have\nsome limitations such as poor reasoning ability due to the lack of external\nknowledge. Research has been dedicated to incorporating knowledge into PLMs to\ntackle these issues. In this paper, we present a comprehensive review of\nKnowledge-Enhanced Pre-trained Language Models (KE-PLMs) to provide a clear\ninsight into this thriving field. We introduce appropriate taxonomies\nrespectively for Natural Language Understanding (NLU) and Natural Language\nGeneration (NLG) to highlight these two main tasks of NLP. For NLU, we divide\nthe types of knowledge into four categories: linguistic knowledge, text\nknowledge, knowledge graph (KG), and rule knowledge. The KE-PLMs for NLG are\ncategorized into KG-based and retrieval-based methods. Finally, we point out\nsome promising future directions of KE-PLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Linmei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zeyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziwang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-VQG: Generating Engaging Questions for Multiple Images. (arXiv:2211.07441v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07441","description":"<p>Generating engaging content has drawn much recent attention in the NLP\ncommunity. Asking questions is a natural way to respond to photos and promote\nawareness. However, most answers to questions in traditional question-answering\n(QA) datasets are factoids, which reduce individuals' willingness to answer.\nFurthermore, traditional visual question generation (VQG) confines the source\ndata for question generation to single images, resulting in a limited ability\nto comprehend time-series information of the underlying event. In this paper,\nwe propose generating engaging questions from multiple images. We present MVQG,\na new dataset, and establish a series of baselines, including both end-to-end\nand dual-stage architectures. Results show that building stories behind the\nimage sequence enables models to generate engaging questions, which confirms\nour assumption that people typically construct a picture of the event in their\nminds before asking questions. These results open up an exciting challenge for\nvisual-and-language models to implicitly construct a story behind a series of\nphotos to allow for creativity and experience sharing and hence draw attention\nto downstream applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_M/0/1/0/all/0/1\">Min-Hsuan Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_V/0/1/0/all/0/1\">Vicent Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haung_T/0/1/0/all/0/1\">Ting-Hao &#x27;Kenneth&#x27; Haung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ku_L/0/1/0/all/0/1\">Lun-Wei Ku</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Much Hate with #china? A Preliminary Analysis on China-related Hateful Tweets Two Years After the Covid Pandemic Began. (arXiv:2211.06116v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2211.06116","description":"<p>Following the outbreak of a global pandemic, online content is filled with\nhate speech. Donald Trump's ''Chinese Virus'' tweet shifted the blame for the\nspread of the Covid-19 virus to China and the Chinese people, which triggered a\nnew round of anti-China hate both online and offline. This research intends to\nexamine China-related hate speech on Twitter during the two years following the\nburst of the pandemic (2020 and 2021). Through Twitter's API, in total\n2,172,333 tweets hashtagged #china posted during the time were collected. By\nemploying multiple state-of-the-art pretrained language models for hate speech\ndetection, we identify a wide range of hate of various types, resulting in an\nautomatically labeled anti-China hate speech dataset. We identify a hateful\nrate in #china tweets of 2.5% in 2020 and 1.9% in 2021. This is well above the\naverage rate of online hate speech on Twitter at 0.6% identified in Gao et al.,\n2017. We further analyzed the longitudinal development of #china tweets and\nthose identified as hateful in 2020 and 2021 through visualizing the daily\nnumber and hate rate over the two years. Our keyword analysis of hate speech in\n#china tweets reveals the most frequently mentioned terms in the hateful #china\ntweets, which can be used for further social science studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinghua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_Z/0/1/0/all/0/1\">Zarah Weiss</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I Can't Believe There's No Images! Learning Visual Tasks Using only Language Data. (arXiv:2211.09778v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2211.09778","description":"<p>Many high-level skills that are required for computer vision tasks, such as\nparsing questions, comparing and contrasting semantics, and writing\ndescriptions, are also required in other domains such as natural language\nprocessing. In this paper, we ask whether this makes it possible to learn those\nskills from text data and then use them to complete vision tasks without ever\ntraining on visual training data. Key to our approach is exploiting the joint\nembedding space of contrastively trained vision and language encoders. In\npractice, there can be systematic differences between embedding spaces for\ndifferent modalities in contrastive models, and we analyze how these\ndifferences affect our approach and study a variety of strategies to mitigate\nthis concern. We produce models using only text training data on three tasks:\nimage captioning, visual entailment and visual question answering, and evaluate\nthem on standard benchmarks using images. We find that this kind of transfer is\npossible and results in only a small drop in performance relative to models\ntrained on images. We also showcase a variety of stylistic image captioning\nmodels that were trained using no image data and no human-curated language\ndata, but instead text data from books, the web, or language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Sophia Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_C/0/1/0/all/0/1\">Christopher Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1\">Aniruddha Kembhavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-11-20T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}