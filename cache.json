{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-05-04T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Stars Are All You Need: A Distantly Supervised Pyramid Network for Document-Level End-to-End Sentiment Analysis. (arXiv:2305.01710v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01710","description":"<p>In this paper, we propose document-level end-to-end sentiment analysis to\nefficiently understand aspect and review sentiment expressed in online reviews\nin a unified manner. In particular, we assume that star rating labels are a\n\"coarse-grained synthesis\" of aspect ratings across in the review. We propose a\nDistantly Supervised Pyramid Network (DSPN) to efficiently perform\nAspect-Category Detection, Aspect-Category Sentiment Analysis, and Rating\nPrediction using only document star rating labels for training. By performing\nthese three related sentiment subtasks in an end-to-end manner, DSPN can\nextract aspects mentioned in the review, identify the corresponding sentiments,\nand predict the star rating labels. We evaluate DSPN on multi-aspect review\ndatasets in English and Chinese and find that with only star rating labels for\nsupervision, DSPN can perform comparably well to a variety of benchmark models.\nWe also demonstrate the interpretability of DSPN's outputs on reviews to show\nthe pyramid structure inherent in document level end-to-end sentiment analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenchang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalor_J/0/1/0/all/0/1\">John P. Lalor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner. (arXiv:2305.01711v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01711","description":"<p>Language models (LMs) trained on vast quantities of unlabelled data have\ngreatly advanced the field of natural language processing (NLP). In this study,\nwe re-visit the widely accepted notion in NLP that continued pre-training LMs\non task-related texts improves the performance of fine-tuning (FT) in\ndownstream tasks. Through experiments on eight single-sentence tasks and eight\nsentence-pair tasks in both semi-supervised and fully-supervised settings, we\nfind that conventional continued pre-training does not consistently provide\nbenefits and can even be detrimental for sentence-pair tasks or when\nprompt-based FT is used. To tackle these issues, we propose Prompt-based\nContinued Pre-training (PCP), which combines the idea of instruction tuning\nwith conventional continued pre-training. Our approach aims to improve the\nperformance of prompt-based FT by presenting both task-related texts and prompt\ntemplates to LMs through unsupervised pre-training objectives before\nfine-tuning for the target task. Our empirical evaluations on 21 benchmarks\ndemonstrate that the PCP consistently improves the performance of\nstate-of-the-art prompt-based FT approaches (up to 20.1% absolute) in both\nsemi-supervised and fully-supervised settings, even with only hundreds of\nunlabelled examples. Additionally, prompt-based FT with the PCP outperforms\nstate-of-the-art semi-supervised approaches with greater simplicity,\neliminating the need for an iterative process and extra data augmentation. Our\nfurther analysis explores the performance lower bound of the PCP and reveals\nthat the advantages of PCP persist across different sizes of models and\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhengxiang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1\">Aldo Lipani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Disentangled Semantic Spaces of Explanations via Invertible Neural Networks. (arXiv:2305.01713v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01713","description":"<p>Disentangling sentence representations over continuous spaces can be a\ncritical process in improving interpretability and semantic control by\nlocalising explicit generative factors. Such process confers to neural-based\nlanguage models some of the advantages that are characteristic of symbolic\nmodels, while keeping their flexibility. This work presents a methodology for\ndisentangling the hidden space of a BERT-GPT2 autoencoder by transforming it\ninto a more separable semantic space with the support of a flow-based\ninvertible neural network (INN). Experimental results indicate that the INN can\ntransform the distributed hidden space into a better semantically disentangled\nlatent space, resulting in better interpretability and controllability, when\ncompared to recent state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_D/0/1/0/all/0/1\">Danilo S. Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratt_Hartmann_I/0/1/0/all/0/1\">Ian Pratt-Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andr&#xe9; Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stance Detection With Supervised, Zero-Shot, and Few-Shot Applications. (arXiv:2305.01723v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01723","description":"<p>Stance detection is the identification of an author's beliefs about a subject\nfrom a document. Researchers widely rely on sentiment analysis to accomplish\nthis. However, recent research has show that sentiment analysis is only loosely\ncorrelated with stance, if at all. This paper advances methods in text analysis\nby precisely defining the task of stance detection, providing a generalized\nframework for the task, and then presenting three distinct approaches for\nperforming stance detection: supervised classification, zero-shot\nclassification with NLI classifiers, and in-context learning. In doing so, I\ndemonstrate how zero-shot and few-shot language classifiers can replace human\nlabelers for a variety of tasks and discuss how their application and\nlimitations differ from supervised classifiers. Finally, I demonstrate an\napplication of zero-shot stance detection by replicating Block Jr et al.\n(2022).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burnham_M/0/1/0/all/0/1\">Michael Burnham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiffuSum: Generation Enhanced Extractive Summarization with Diffusion. (arXiv:2305.01735v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01735","description":"<p>Extractive summarization aims to form a summary by directly extracting\nsentences from the source document. Existing works mostly formulate it as a\nsequence labeling problem by making individual sentence label predictions. This\npaper proposes DiffuSum, a novel paradigm for extractive summarization, by\ndirectly generating the desired summary sentence representations with diffusion\nmodels and extracting sentences based on sentence representation matching. In\naddition, DiffuSum jointly optimizes a contrastive sentence encoder with a\nmatching loss for sentence representation alignment and a multi-class\ncontrastive loss for representation diversity. Experimental results show that\nDiffuSum achieves the new state-of-the-art extractive results on CNN/DailyMail\nwith ROUGE scores of $44.83/22.56/40.56$. Experiments on the other two datasets\nwith different summary lengths also demonstrate the effectiveness of DiffuSum.\nThe strong performance of our framework shows the great potential of adapting\ngenerative models for extractive summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot In-context Learning for Knowledge Base Question Answering. (arXiv:2305.01750v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01750","description":"<p>Question answering over knowledge bases is considered a difficult problem due\nto the challenge of generalizing to a wide variety of possible natural language\nquestions. Additionally, the heterogeneity of knowledge base schema items\nbetween different knowledge bases often necessitates specialized training for\ndifferent knowledge base question-answering (KBQA) datasets. To handle\nquestions over diverse KBQA datasets with a unified training-free framework, we\npropose KB-BINDER, which for the first time enables few-shot in-context\nlearning over KBQA tasks. Firstly, KB-BINDER leverages large language models\nlike Codex to generate logical forms as the draft for a specific question by\nimitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledge\nbase to bind the generated draft to an executable one with BM25 score matching.\nThe experimental results on four public heterogeneous KBQA datasets show that\nKB-BINDER can achieve a strong performance with only a few in-context\ndemonstrations. Especially on GraphQA and 3-hop MetaQA, KB-BINDER can even\noutperform the state-of-the-art trained models. On GrailQA and WebQSP, our\nmodel is also on par with other fully-trained models. We believe KB-BINDER can\nserve as an important baseline for future research. We plan to release all the\ncode and data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+LI_T/0/1/0/all/0/1\">Tianle LI</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xueguang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_A/0/1/0/all/0/1\">Alex Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of Speaker Anonymization on Emotional Speech. (arXiv:2305.01759v1 [eess.AS])","link":"http://arxiv.org/abs/2305.01759","description":"<p>Speech data carries a range of personal information, such as the speaker's\nidentity and emotional state. These attributes can be used for malicious\npurposes. With the development of virtual assistants, a new generation of\nprivacy threats has emerged. Current studies have addressed the topic of\npreserving speech privacy. One of them, the VoicePrivacy initiative aims to\npromote the development of privacy preservation tools for speech technology.\nThe task selected for the VoicePrivacy 2020 Challenge (VPC) is about speaker\nanonymization. The goal is to hide the source speaker's identity while\npreserving the linguistic information. The baseline of the VPC makes use of a\nvoice conversion. This paper studies the impact of the speaker anonymization\nbaseline system of the VPC on emotional information present in speech\nutterances. Evaluation is performed following the VPC rules regarding the\nattackers' knowledge about the anonymization system. Our results show that the\nVPC baseline system does not suppress speakers' emotions against informed\nattackers. When comparing anonymized speech to original speech, the emotion\nrecognition performance is degraded by 15\\% relative to IEMOCAP data, similar\nto the degradation observed for automatic speech recognition used to evaluate\nthe preservation of the linguistic information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nourtel_H/0/1/0/all/0/1\">Hubert Nourtel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Champion_P/0/1/0/all/0/1\">Pierre Champion</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jouvet_D/0/1/0/all/0/1\">Denis Jouvet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Larcher_A/0/1/0/all/0/1\">Anthony Larcher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tahon_M/0/1/0/all/0/1\">Marie Tahon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Psychologically-Inspired Causal Prompts. (arXiv:2305.01764v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01764","description":"<p>NLP datasets are richer than just input-output pairs; rather, they carry\ncausal relations between the input and output variables. In this work, we take\nsentiment classification as an example and look into the causal relations\nbetween the review (X) and sentiment (Y). As psychology studies show that\nlanguage can affect emotion, different psychological processes are evoked when\na person first makes a rating and then self-rationalizes their feeling in a\nreview (where the sentiment causes the review, i.e., Y -&gt; X), versus first\ndescribes their experience, and weighs the pros and cons to give a final rating\n(where the review causes the sentiment, i.e., X -&gt; Y ). Furthermore, it is also\na completely different psychological process if an annotator infers the\noriginal rating of the user by theory of mind (ToM) (where the review causes\nthe rating, i.e., X -ToM-&gt; Y ). In this paper, we verbalize these three causal\nmechanisms of human psychological processes of sentiment classification into\nthree different causal prompts, and study (1) how differently they perform, and\n(2) what nature of sentiment classification data leads to agreement or\ndiversity in the model responses elicited by the prompts. We suggest future\nwork raise awareness of different causal structures in NLP tasks. Our code and\ndata are at https://github.com/cogito233/psych-causal-prompt\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1\">Zhiheng Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattern_J/0/1/0/all/0/1\">Justus Mattern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoelkopf_B/0/1/0/all/0/1\">Bernhard Schoelkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SLTUNET: A Simple Unified Model for Sign Language Translation. (arXiv:2305.01778v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01778","description":"<p>Despite recent successes with neural models for sign language translation\n(SLT), translation quality still lags behind spoken languages because of the\ndata scarcity and modality gap between sign video and text. To address both\nproblems, we investigate strategies for cross-modality representation sharing\nfor SLT. We propose SLTUNET, a simple unified neural model designed to support\nmultiple SLTrelated tasks jointly, such as sign-to-gloss, gloss-to-text and\nsign-to-text translation. Jointly modeling different tasks endows SLTUNET with\nthe capability to explore the cross-task relatedness that could help narrow the\nmodality gap. In addition, this allows us to leverage the knowledge from\nexternal resources, such as abundant parallel data used for spoken-language\nmachine translation (MT). We show in experiments that SLTUNET achieves\ncompetitive and even state-of-the-art performance on PHOENIX-2014T and\nCSL-Daily when augmented with MT data and equipped with a set of optimization\ntechniques. We further use the DGS Corpus for end-to-end SLT for the first\ntime. It covers broader domains with a significantly larger vocabulary, which\nis more challenging and which we consider to allow for a more realistic\nassessment of the current state of SLT than the former two. Still, SLTUNET\nobtains improved results on the DGS Corpus. Code is available at\nhttps://github.com/bzhangGo/sltunet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Biao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1\">Mathias M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information. (arXiv:2305.01788v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01788","description":"<p>Visual Word Sense Disambiguation (VWSD) is a task to find the image that most\naccurately depicts the correct sense of the target word for the given context.\nPreviously, image-text matching models often suffered from recognizing\npolysemous words. This paper introduces an unsupervised VWSD approach that uses\ngloss information of an external lexical knowledge-base, especially the sense\ndefinitions. Specifically, we suggest employing Bayesian inference to\nincorporate the sense definitions when sense information of the answer is not\nprovided. In addition, to ameliorate the out-of-dictionary (OOD) issue, we\npropose a context-aware definition generation with GPT-3. Experimental results\nshow that the VWSD performance significantly increased with our Bayesian\ninference-based approach. In addition, our context-aware definition generation\nachieved prominent performance improvement in OOD examples exhibiting better\nperformance than the existing definition generation method. We will publish\nsource codes as soon as possible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1\">Sunjae Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garodia_R/0/1/0/all/0/1\">Rishabh Garodia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minhwa Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Procedural Planning via Dual Text-Image Prompting. (arXiv:2305.01795v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01795","description":"<p>Embodied agents have achieved prominent performance in following human\ninstructions to complete tasks. However, the potential of providing\ninstructions informed by texts and images to assist humans in completing tasks\nremains underexplored. To uncover this capability, we present the multimodal\nprocedural planning (MPP) task, in which models are given a high-level goal and\ngenerate plans of paired text-image steps, providing more complementary and\ninformative guidance than unimodal plans. The key challenges of MPP are to\nensure the informativeness, temporal coherence,and accuracy of plans across\nmodalities. To tackle this, we propose Text-Image Prompting (TIP), a\ndual-modality prompting method that jointly leverages zero-shot reasoning\nability in large language models (LLMs) and compelling text-to-image generation\nability from diffusion-based models. TIP improves the interaction in the dual\nmodalities using Text-to-Image Bridge and Image-to-Text Bridge, allowing LLMs\nto guide the textual-grounded image plan generation and leveraging the\ndescriptions of image plans to ground the textual plan reversely. To address\nthe lack of relevant datasets, we collect WIKIPLAN and RECIPEPLAN as a testbed\nfor MPP. Our results show compelling human preferences and automatic scores\nagainst unimodal and multimodal baselines on WIKIPLAN and RECIPEPLAN in terms\nof informativeness, temporal coherence, and plan accuracy. Our code and data:\nhttps://github.com/YujieLu10/MPP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanrong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KEPLET: Knowledge-Enhanced Pretrained Language Model with Topic Entity Awareness. (arXiv:2305.01810v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01810","description":"<p>In recent years, Pre-trained Language Models (PLMs) have shown their\nsuperiority by pre-training on unstructured text corpus and then fine-tuning on\ndownstream tasks. On entity-rich textual resources like Wikipedia,\nKnowledge-Enhanced PLMs (KEPLMs) incorporate the interactions between tokens\nand mentioned entities in pre-training, and are thus more effective on\nentity-centric tasks such as entity linking and relation classification.\nAlthough exploiting Wikipedia's rich structures to some extent, conventional\nKEPLMs still neglect a unique layout of the corpus where each Wikipedia page is\naround a topic entity (identified by the page URL and shown in the page title).\nIn this paper, we demonstrate that KEPLMs without incorporating the topic\nentities will lead to insufficient entity interaction and biased (relation)\nword semantics. We thus propose KEPLET, a novel Knowledge-Enhanced Pre-trained\nLanguagE model with Topic entity awareness. In an end-to-end manner, KEPLET\nidentifies where to add the topic entity's information in a Wikipedia sentence,\nfuses such information into token and mentioned entities representations, and\nsupervises the network learning, through which it takes topic entities back\ninto consideration. Experiments demonstrated the generality and superiority of\nKEPLET which was applied to two representative KEPLMs, achieving significant\nimprovements on four entity-centric tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yichuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jialong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyumin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chengyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Benjamin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Derek Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Post-Abstention: Towards Reliably Re-Attempting the Abstained Instances in QA. (arXiv:2305.01812v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01812","description":"<p>Despite remarkable progress made in natural language processing, even the\nstate-of-the-art models often make incorrect predictions. Such predictions\nhamper the reliability of systems and limit their widespread adoption in\nreal-world applications. 'Selective prediction' partly addresses the above\nconcern by enabling models to abstain from answering when their predictions are\nlikely to be incorrect. While selective prediction is advantageous, it leaves\nus with a pertinent question 'what to do after abstention'. To this end, we\npresent an explorative study on 'Post-Abstention', a task that allows\nre-attempting the abstained instances with the aim of increasing 'coverage' of\nthe system without significantly sacrificing its 'accuracy'. We first provide\nmathematical formulation of this task and then explore several methods to solve\nit. Comprehensive experiments on 11 QA datasets show that these methods lead to\nconsiderable risk improvements -- performance metric of the Post-Abstention\ntask -- both in the in-domain and the out-of-domain settings. We also conduct a\nthorough analysis of these results which further leads to several interesting\nfindings. Finally, we believe that our work will encourage and facilitate\nfurther research in this important area of addressing the reliability of NLP\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPTutor: a ChatGPT-powered programming tool for code explanation. (arXiv:2305.01863v1 [cs.HC])","link":"http://arxiv.org/abs/2305.01863","description":"<p>Learning new programming skills requires tailored guidance. With the\nemergence of advanced Natural Language Generation models like the ChatGPT API,\nthere is now a possibility of creating a convenient and personalized tutoring\nsystem with AI for computer science education. This paper presents GPTutor, a\nChatGPT-powered programming tool, which is a Visual Studio Code extension using\nthe ChatGPT API to provide programming code explanations. By integrating Visual\nStudio Code API, GPTutor can comprehensively analyze the provided code by\nreferencing the relevant source codes. As a result, GPTutor can use designed\nprompts to explain the selected code with a pop-up message. GPTutor is now\npublished at the Visual Studio Code Extension Marketplace, and its source code\nis openly accessible on GitHub. Preliminary evaluation indicates that GPTutor\ndelivers the most concise and accurate explanations compared to vanilla ChatGPT\nand GitHub Copilot. Moreover, the feedback from students and teachers indicated\nthat GPTutor is user-friendly and can explain given codes satisfactorily.\nFinally, we discuss possible future research directions for GPTutor. This\nincludes enhancing its performance and personalization via further prompt\nprogramming, as well as evaluating the effectiveness of GPTutor with real\nusers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Eason Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Ray Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Han-Shin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_Y/0/1/0/all/0/1\">Yuen-Hsien Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang-Yi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causality-aware Concept Extraction based on Knowledge-guided Prompting. (arXiv:2305.01876v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01876","description":"<p>Concepts benefit natural language understanding but are far from complete in\nexisting knowledge graphs (KGs). Recently, pre-trained language models (PLMs)\nhave been widely used in text-based concept extraction (CE). However, PLMs tend\nto mine the co-occurrence associations from massive corpus as pre-trained\nknowledge rather than the real causal effect between tokens.As a result, the\npre-trained knowledge confounds PLMs to extract biased concepts based on\nspurious co-occurrence correlations, inevitably resulting in low precision. In\nthis paper, through the lens of a Structural Causal Model (SCM), we propose\nequipping the PLM-based extractor with a knowledge-guided prompt as an\nintervention to alleviate concept bias. The prompt adopts the topic of the\ngiven entity from the existing knowledge in KGs to mitigate the spurious\nco-occurrence correlations between entities and biased concepts. Our extensive\nexperiments on representative multilingual KG datasets justify that our\nproposed prompt can effectively alleviate concept bias and improve the\nperformance of PLM-based CE models.The code has been released on\nhttps://github.com/siyuyuan/KPCE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Siyu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Deqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinxi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Shuyu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiaqing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Rui Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCOTT: Self-Consistent Chain-of-Thought Distillation. (arXiv:2305.01879v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01879","description":"<p>Large language models (LMs) beyond a certain scale, demonstrate the emergent\ncapability of generating free-text rationales for their predictions via\nchain-of-thought (CoT) prompting. While CoT can yield dramatically improved\nperformance, such gains are only observed for sufficiently large LMs. Even more\nconcerning, there is little guarantee that the generated rationales are\nconsistent with LM's predictions or faithfully justify the decisions. In this\nwork, we propose a faithful knowledge distillation method to learn a small,\nself-consistent CoT model from a teacher model that is orders of magnitude\nlarger. To form better supervision, we elicit rationales supporting the gold\nanswers from a large LM (teacher) by contrastive decoding, which encourages the\nteacher to generate tokens that become more plausible only when the answer is\nconsidered. To ensure faithful distillation, we use the teacher-generated\nrationales to learn a student LM with a counterfactual reasoning objective,\nwhich prevents the student from ignoring the rationales to make inconsistent\npredictions. Experiments show that, while yielding comparable end-task\nperformance, our method can generate CoT rationales that are more faithful than\nbaselines do. Further analysis suggests that such a model respects the\nrationales more when making decisions; thus, we can improve its performance\nmore by refining its rationales.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhengyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yifan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Event Detection: An Empirical Study and a Unified View. (arXiv:2305.01901v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01901","description":"<p>Few-shot event detection (ED) has been widely studied, while this brings\nnoticeable discrepancies, e.g., various motivations, tasks, and experimental\nsettings, that hinder the understanding of models for future progress. This\npaper presents a thorough empirical study, a unified view of ED models, and a\nbetter unified baseline. For fair evaluation, we choose two practical settings:\nlow-resource setting to assess generalization ability and class-transfer\nsetting for transferability. We compare ten representative methods on three\ndatasets, which are roughly grouped into prompt-based and prototype-based\nmodels for detailed analysis. To investigate the superior performance of\nprototype-based methods, we break down the design and build a unified\nframework. Based on that, we not only propose a simple yet effective method\n(e.g., 2.7% F1 gains under low-resource setting) but also offer many valuable\nresearch insights for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yubo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zehao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Natural Language Watermarking through Invariant Features. (arXiv:2305.01904v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01904","description":"<p>Recent years have witnessed a proliferation of valuable original natural\nlanguage contents found in subscription-based media outlets, web novel\nplatforms, and outputs of large language models. Without proper security\nmeasures, however, these contents are susceptible to illegal piracy and\npotential misuse. This calls for a secure watermarking system to guarantee\ncopyright protection through leakage tracing or ownership identification. To\neffectively combat piracy and protect copyrights, a watermarking framework\nshould be able not only to embed adequate bits of information but also extract\nthe watermarks in a robust manner despite possible corruption. In this work, we\nexplore ways to advance both payload and robustness by following a well-known\nproposition from image watermarking and identify features in natural language\nthat are invariant to minor corruption. Through a systematic analysis of the\npossible sources of errors, we further propose a corruption-resistant infill\nmodel. Our full method improves upon the previous work on robustness by +16.8%\npoint on average on four datasets, three corruption types, and two corruption\nratios. Code available at https://github.com/bangawayoo/nlp-watermarking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">KiYoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_W/0/1/0/all/0/1\">Wonhyuk Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Jiho Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Interventions-based Few-Shot Named Entity Recognition. (arXiv:2305.01914v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01914","description":"<p>Few-shot named entity recognition (NER) systems aims at recognizing new\nclasses of entities based on a few labeled samples. A significant challenge in\nthe few-shot regime is prone to overfitting than the tasks with abundant\nsamples. The heavy overfitting in few-shot learning is mainly led by spurious\ncorrelation caused by the few samples selection bias. To alleviate the problem\nof the spurious correlation in the few-shot NER, in this paper, we propose a\ncausal intervention-based few-shot NER method. Based on the prototypical\nnetwork, the method intervenes in the context and prototype via backdoor\nadjustment during training. In particular, intervening in the context of the\none-shot scenario is very difficult, so we intervene in the prototype via\nincremental learning, which can also avoid catastrophic forgetting. Our\nexperiments on different benchmarks show that our approach achieves new\nstate-of-the-art results (achieving up to 29% absolute improvement and 12% on\naverage for all tasks).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongbin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_C/0/1/0/all/0/1\">Chunping Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Contrastive Learning of Sentence Embeddings from AI Feedback. (arXiv:2305.01918v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01918","description":"<p>Contrastive learning has become a popular approach in natural language\nprocessing, particularly for the learning of sentence embeddings. However, the\ndiscrete nature of natural language makes it difficult to ensure the quality of\npositive and negative sample pairs generated through data augmentation methods.\nAlthough supervised contrastive learning can produce more accurate sample pairs\nwith human feedback labels, it still lacks fine-grained training signals. In\nthis paper, we propose to improve \\textbf{C}ontrastive \\textbf{L}earning of\nsentence embeddings from \\textbf{AI} \\textbf{F}eedback \\textbf{(CLAIF)}. Our\nmethod utilizes AI feedback from large pre-trained language models (LLMs) to\nconstruct sample pairs with fine-grained sample similarity scores to improve\ncontrastive learning. Besides, we combine human feedback and AI feedback to\nprovide better supervision signals for supervised contrastive learning of\nsentence embeddings. Experimental results show that our method achieves\nstate-of-the-art performance on several semantic textual similarity (STS) and\ntransfer learning tasks compared to other unsupervised and supervised\ncontrastive learning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1\">Qinyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaogui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Meta-Learning for Zero-Shot Relation Triplet Extraction. (arXiv:2305.01920v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01920","description":"<p>The zero-shot relation triplet extraction (ZeroRTE) task aims to extract\nrelation triplets from a piece of text with unseen relation types. The seminal\nwork adopts the pre-trained generative model to generate synthetic samples for\nnew relations. However, current generative models lack the optimization process\nof model generalization on different tasks during training, and thus have\nlimited generalization capability. For this reason, we propose a novel\ngenerative meta-learning framework which exploits the `learning-to-learn'\nability of meta-learning to boost the generalization capability of generative\nmodels. Specifically, we first design a task-aware generative model which can\nlearn the general knowledge by forcing the optimization process to be conducted\nacross multiple tasks. Based on it, we then present three generative\nmeta-learning approaches designated for three typical meta-learning categories.\nExtensive experimental results demonstrate that our framework achieves a new\nstate-of-the-art performance for the ZeroRTE task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wanli Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_T/0/1/0/all/0/1\">Tieyun Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Large Language Models Be an Alternative to Human Evaluations?. (arXiv:2305.01937v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01937","description":"<p>Human evaluation is indispensable and inevitable for assessing the quality of\ntexts generated by machine learning models or written by humans. However, human\nevaluation is very difficult to reproduce and its quality is notoriously\nunstable, hindering fair comparisons among different natural language\nprocessing (NLP) models and algorithms. Recently, large language models (LLMs)\nhave demonstrated exceptional performance on unseen tasks when only the task\ninstructions are provided. In this paper, we explore if such an ability of the\nLLMs can be used as an alternative to human evaluation. We present the LLMs\nwith the exact same instructions, samples to be evaluated, and questions used\nto conduct human evaluation, and then ask the LLMs to generate responses to\nthose questions; we dub this LLM evaluation. We use human evaluation and LLM\nevaluation to evaluate the texts in two NLP tasks: open-ended story generation\nand adversarial attacks. We show that the result of LLM evaluation is\nconsistent with the results obtained by expert human evaluation: the texts\nrated higher by human experts are also rated higher by the LLMs. We also find\nthat the results of LLM evaluation are stable over different formatting of the\ntask instructions and the sampling algorithm used to generate the answer. We\nare the first to show the potential of using LLMs to assess the quality of\ntexts and discuss the limitations and ethical considerations of LLM evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_C/0/1/0/all/0/1\">Cheng-Han Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Doc2SoarGraph: Discrete Reasoning over Visually-Rich Table-Text Documents with Semantic-Oriented Hierarchical Graphs. (arXiv:2305.01938v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01938","description":"<p>Discrete reasoning over table-text documents (e.g., financial reports) gains\nincreasing attention in recent two years. Existing works mostly simplify this\nchallenge by manually selecting and transforming document pages to structured\ntables and paragraphs, hindering their practical application. In this work, we\nexplore a more realistic problem setting in the form of TAT-DQA, i.e. to answer\nthe question over a visually-rich table-text document. Specifically, we propose\na novel Doc2SoarGraph framework with enhanced discrete reasoning capability by\nharnessing the differences and correlations among different elements (e.g.,\nquantities, dates) of the given question and document with Semantic-oriented\nhierarchical Graph structures. We conduct extensive experiments on TAT-DQA\ndataset, and the results show that our proposed framework outperforms the best\nbaseline model by 17.73% and 16.91% in terms of Exact Match (EM) and F1 score\nrespectively on the test set, achieving the new state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengbin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fuli Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zifeng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Moxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TempoSum: Evaluating the Temporal Generalization of Abstractive Summarization. (arXiv:2305.01951v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01951","description":"<p>Recent pre-trained language models (PLMs) achieve promising results in\nexisting abstractive summarization datasets. However, existing summarization\nbenchmarks overlap in time with the standard pre-training corpora and\nfinetuning datasets. Hence, the strong performance of PLMs may rely on the\nparametric knowledge that is memorized during pre-training and fine-tuning.\nMoreover, the knowledge memorized by PLMs may quickly become outdated, which\naffects the generalization performance of PLMs on future data. In this work, we\npropose TempoSum, a novel benchmark that contains data samples from 2010 to\n2022, to understand the temporal generalization ability of abstractive\nsummarization models. Through extensive human evaluation, we show that\nparametric knowledge stored in summarization models significantly affects the\nfaithfulness of the generated summaries on future data. Moreover, existing\nfaithfulness enhancement methods cannot reliably improve the faithfulness of\nsummarization models on future data. Finally, we discuss several\nrecommendations to the research community on how to evaluate and improve the\ntemporal generalization capability of text summarization models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheang_C/0/1/0/all/0/1\">Chi Seng Cheang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1\">Hou Pong Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek F. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaocong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yanming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shudong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1\">Lidia S. Chao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeqAug: Sequential Feature Resampling as a modality agnostic augmentation method. (arXiv:2305.01954v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01954","description":"<p>Data augmentation is a prevalent technique for improving performance in\nvarious machine learning applications. We propose SeqAug, a modality-agnostic\naugmentation method that is tailored towards sequences of extracted features.\nThe core idea of SeqAug is to augment the sequence by resampling from the\nunderlying feature distribution. Resampling is performed by randomly selecting\nfeature dimensions and permuting them along the temporal axis. Experiments on\nCMU-MOSEI verify that SeqAug is modality agnostic; it can be successfully\napplied to a single modality or multiple modalities. We further verify its\ncompatibility with both recurrent and transformer architectures, and also\ndemonstrate comparable to state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Georgiou_E/0/1/0/all/0/1\">Efthymios Georgiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potamianos_A/0/1/0/all/0/1\">Alexandros Potamianos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NorQuAD: Norwegian Question Answering Dataset. (arXiv:2305.01957v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01957","description":"<p>In this paper we present NorQuAD: the first Norwegian question answering\ndataset for machine reading comprehension. The dataset consists of 4,752\nmanually created question-answer pairs. We here detail the data collection\nprocedure and present statistics of the dataset. We also benchmark several\nmultilingual and Norwegian monolingual language models on the dataset and\ncompare them against human performance. The dataset will be made freely\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivanova_S/0/1/0/all/0/1\">Sardana Ivanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreassen_F/0/1/0/all/0/1\">Fredrik Aas Andreassen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jentoft_M/0/1/0/all/0/1\">Matias Jentoft</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wold_S/0/1/0/all/0/1\">Sondre Wold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovrelid_L/0/1/0/all/0/1\">Lilja &#xd8;vrelid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysing the Impact of Audio Quality on the Use of Naturalistic Long-Form Recordings for Infant-Directed Speech Research. (arXiv:2305.01965v1 [cs.CL])","link":"http://arxiv.org/abs/2305.01965","description":"<p>Modelling of early language acquisition aims to understand how infants\nbootstrap their language skills. The modelling encompasses properties of the\ninput data used for training the models, the cognitive hypotheses and their\nalgorithmic implementations being tested, and the evaluation methodologies to\ncompare models to human data. Recent developments have enabled the use of more\nnaturalistic training data for computational models. This also motivates\ndevelopment of more naturalistic tests of model behaviour. A crucial step\ntowards such an aim is to develop representative speech datasets consisting of\nspeech heard by infants in their natural environments. However, a major\ndrawback of such recordings is that they are typically noisy, and it is\ncurrently unclear how the sound quality could affect analyses and modelling\nexperiments conducted on such data. In this paper, we explore this aspect for\nthe case of infant-directed speech (IDS) and adult-directed speech (ADS)\nanalysis. First, we manually and automatically annotated audio quality of\nutterances extracted from two corpora of child-centred long-form recordings (in\nEnglish and French). We then compared acoustic features of IDS and ADS in an\nin-lab dataset and across different audio quality subsets of naturalistic data.\nFinally, we assessed how the audio quality and recording environment may change\nthe conclusions of a modelling analysis using a recent self-supervised learning\nmodel. Our results show that the use of modest and high audio quality\nnaturalistic speech data result in largely similar conclusions on IDS and ADS\nin terms of acoustic analyses and modelling experiments. We also found that an\nautomatic sound quality assessment tool can be used to screen out useful parts\nof long-form recordings for a closer analysis with comparable results to that\nof manual quality annotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blandon_M/0/1/0/all/0/1\">Mar&#xed;a Andrea Cruz Bland&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristia_A/0/1/0/all/0/1\">Alejandrina Cristia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1\">Okko R&#xe4;s&#xe4;nen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural language processing on customer note data. (arXiv:2305.02029v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02029","description":"<p>Automatic analysis of customer data for businesses is an area that is of\ninterest to companies. Business to business data is studied rarely in academia\ndue to the sensitive nature of such information. Applying natural language\nprocessing can speed up the analysis of prohibitively large sets of data. This\npaper addresses this subject and applies sentiment analysis, topic modelling\nand keyword extraction to a B2B data set. We show that accurate sentiment can\nbe extracted from the notes automatically and the notes can be sorted by\nrelevance into different topics. We see that without clear separation topics\ncan lack relevance to a business context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hilditch_A/0/1/0/all/0/1\">Andrew Hilditch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webb_D/0/1/0/all/0/1\">David Webb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baca_J/0/1/0/all/0/1\">Jozef Baca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armitage_T/0/1/0/all/0/1\">Tom Armitage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shardlow_M/0/1/0/all/0/1\">Matthew Shardlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Appleby_P/0/1/0/all/0/1\">Peter Appleby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training. (arXiv:2305.02031v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02031","description":"<p>Modern Natural Language Generation (NLG) models come with massive\ncomputational and storage requirements. In this work, we study the potential of\ncompressing them, which is crucial for real-world applications serving millions\nof users. We focus on Knowledge Distillation (KD) techniques, in which a small\nstudent model learns to imitate a large teacher model, allowing to transfer\nknowledge from the teacher to the student. In contrast to much of the previous\nwork, our goal is to optimize the model for a specific NLG task and a specific\ndataset. Typically, in real-world applications, in addition to labeled data\nthere is abundant unlabeled task-specific data, which is crucial for attaining\nhigh compression rates via KD. In this work, we conduct a systematic study of\ntask-specific KD techniques for various NLG tasks under realistic assumptions.\nWe discuss the special characteristics of NLG distillation and particularly the\nexposure bias problem. Following, we derive a family of Pseudo-Target (PT)\naugmentation methods, substantially extending prior work on sequence-level KD.\nWe propose the Joint-Teaching method for NLG distillation, which applies\nword-level KD to multiple PTs generated by both the teacher and the student.\nOur study provides practical model design observations and demonstrates the\neffectiveness of PT training for task-specific KD in NLG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Calderon_N/0/1/0/all/0/1\">Nitay Calderon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kantor_A/0/1/0/all/0/1\">Amir Kantor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Response-conditioned Turn-taking Prediction. (arXiv:2305.02036v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02036","description":"<p>Previous approaches to turn-taking and response generation in conversational\nsystems have treated it as a two-stage process: First, the end of a turn is\ndetected (based on conversation history), then the system generates an\nappropriate response. Humans, however, do not take the turn just because it is\nlikely, but also consider whether what they want to say fits the position. In\nthis paper, we present a model (an extension of TurnGPT) that conditions the\nend-of-turn prediction on both conversation history and what the next speaker\nwants to say. We found that our model consistently outperforms the baseline\nmodel in a variety of metrics. The improvement is most prominent in two\nscenarios where turn predictions can be ambiguous solely from the conversation\nhistory: 1) when the current utterance contains a statement followed by a\nquestion; 2) when the end of the current utterance semantically matches the\nresponse. Treating the turn-prediction and response-ranking as a one-stage\nprocess, our findings suggest that our model can be used as an incremental\nresponse ranker, which can be applied in various settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bing&#x27;er Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekstedt_E/0/1/0/all/0/1\">Erik Ekstedt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skantze_G/0/1/0/all/0/1\">Gabriel Skantze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Considerations for Ethical Speech Recognition Datasets. (arXiv:2305.02081v1 [cs.CY])","link":"http://arxiv.org/abs/2305.02081","description":"<p>Speech AI Technologies are largely trained on publicly available datasets or\nby the massive web-crawling of speech. In both cases, data acquisition focuses\non minimizing collection effort, without necessarily taking the data subjects'\nprotection or user needs into consideration. This results to models that are\nnot robust when used on users who deviate from the dominant demographics in the\ntraining set, discriminating individuals having different dialects, accents,\nspeaking styles, and disfluencies. In this talk, we use automatic speech\nrecognition as a case study and examine the properties that ethical speech\ndatasets should possess towards responsible AI applications. We showcase\ndiversity issues, inclusion practices, and necessary considerations that can\nimprove trained models, while facilitating model explainability and protecting\nusers and data subjects. We argue for the legal &amp; privacy protection of data\nsubjects, targeted data sampling corresponding to user demographics &amp; needs,\nappropriate meta data that ensure explainability &amp; accountability in cases of\nmodel failure, and the sociotechnical \\&amp; situated model design. We hope this\ntalk can inspire researchers \\&amp; practitioners to design and use more\nhuman-centric datasets in speech technologies and other domains, in ways that\nempower and respect users, while improving machine learning models' robustness\nand utility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papakyriakopoulos_O/0/1/0/all/0/1\">Orestis Papakyriakopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_A/0/1/0/all/0/1\">Alice Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What makes a good pause? Investigating the turn-holding effects of fillers. (arXiv:2305.02101v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02101","description":"<p>Filled pauses (or fillers), such as \"uh\" and \"um\", are frequent in\nspontaneous speech and can serve as a turn-holding cue for the listener,\nindicating that the current speaker is not done yet. In this paper, we use the\nrecently proposed Voice Activity Projection (VAP) model, which is a deep\nlearning model trained to predict the dynamics of conversation, to analyse the\neffects of filled pauses on the expected turn-hold probability. The results\nshow that, while filled pauses do indeed have a turn-holding effect, it is\nperhaps not as strong as could be expected, probably due to the redundancy of\nother cues. We also find that the prosodic properties and position of the\nfiller has a significant effect on the turn-hold probability. However, contrary\nto what has been suggested in previous work, there is no difference between\n\"uh\" and \"um\" in this regard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bing&#x27;er Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekstedt_E/0/1/0/all/0/1\">Erik Ekstedt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skantze_G/0/1/0/all/0/1\">Gabriel Skantze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Background Knowledge Grounding for Readable, Relevant, and Factual Biomedical Lay Summaries. (arXiv:2305.02104v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02104","description":"<p>Communication of scientific findings to the public is important for keeping\nnon-experts informed of developments such as life-saving medical treatments.\nHowever, generating readable lay summaries from scientific documents is\nchallenging, and currently, these summaries suffer from critical factual\nerrors. One popular intervention for improving factuality is using additional\nexternal knowledge to provide factual grounding. However, it is unclear how\nthese grounding sources should be retrieved, selected, or integrated, and how\nsupplementary grounding documents might affect the readability or relevance of\nthe generated summaries. We develop a simple method for selecting grounding\nsources and integrating them with source documents. We then use the BioLaySum\nsummarization dataset to evaluate the effects of different grounding sources on\nsummary quality. We found that grounding source documents improves the\nrelevance and readability of lay summaries but does not improve factuality of\nlay summaries. This continues to be true in zero-shot summarization settings\nwhere we hypothesized that grounding might be even more important for factual\nlay summaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosati_D/0/1/0/all/0/1\">Domenic Rosati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-RE: In-context Learning for Relation Extraction using Large Language Models. (arXiv:2305.02105v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02105","description":"<p>In spite of the potential for ground-breaking achievements offered by large\nlanguage models (LLMs) (e.g., GPT-3), they still lag significantly behind\nfully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE).\nThis is due to the two major shortcomings of LLMs in RE: (1) low relevance\nregarding entity and relation in retrieved demonstrations for in-context\nlearning; and (2) the strong inclination to wrongly classify NULL examples into\nother pre-defined labels.\n</p>\n<p>In this paper, we propose GPT-RE to bridge the gap between LLMs and\nfully-supervised baselines. GPT-RE successfully addresses the aforementioned\nissues by (1) incorporating task-specific entity representations in\ndemonstration retrieval; and (2) enriching the demonstrations with gold\nlabel-induced reasoning logic. We evaluate GPT-RE on four widely-used RE\ndatasets, and observe that GPT-RE achieves improvements over not only existing\nGPT-3 baselines, but also fully-supervised baselines. Specifically, GPT-RE\nachieves SOTA performances on the Semeval and SciERC datasets, and competitive\nperformances on the TACRED and ACE05 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zhen Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1\">Fei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhuoyuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qianying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Haiyue Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurohashi_S/0/1/0/all/0/1\">Sadao Kurohashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pay More Attention to Relation Exploration for Knowledge Base Question Answering. (arXiv:2305.02118v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02118","description":"<p>Knowledge base question answering (KBQA) is a challenging task that aims to\nretrieve correct answers from large-scale knowledge bases. Existing attempts\nprimarily focus on entity representation and final answer reasoning, which\nresults in limited supervision for this task. Moreover, the relations, which\nempirically determine the reasoning path selection, are not fully considered in\nrecent advancements. In this study, we propose a novel framework, RE-KBQA, that\nutilizes relations in the knowledge base to enhance entity representation and\nintroduce additional supervision. We explore guidance from relations in three\naspects, including (1) distinguishing similar entities by employing a\nvariational graph auto-encoder to learn relation importance; (2) exploring\nextra supervision by predicting relation distributions as soft labels with a\nmulti-task scheme; (3) designing a relation-guided re-ranking algorithm for\npost-processing. Experimental results on two benchmark datasets demonstrate the\neffectiveness and superiority of our framework, improving the F1 score by 5.7%\nfrom 40.5 to 46.3 on CWQ and 5.8% from 62.8 to 68.5 on WebQSP, better or on par\nwith state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huiwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wen Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Min Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1\">Daniel Hershcovich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Curriculum View of Robust Loss Functions. (arXiv:2305.02139v1 [cs.LG])","link":"http://arxiv.org/abs/2305.02139","description":"<p>Robust loss functions are designed to combat the adverse impacts of label\nnoise, whose robustness is typically supported by theoretical bounds agnostic\nto the training dynamics. However, these bounds may fail to characterize the\nempirical performance as it remains unclear why robust loss functions can\nunderfit. We show that most loss functions can be rewritten into a form with\nthe same class-score margin and different sample-weighting functions. The\nresulting curriculum view provides a straightforward analysis of the training\ndynamics, which helps attribute underfitting to diminished average sample\nweights and noise robustness to larger weights for clean samples. We show that\nsimple fixes to the curriculums can make underfitting robust loss functions\ncompetitive with the state-of-the-art, and training schedules can substantially\naffect the noise robustness even with robust loss functions. Code is available\nat \\url{github}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zebin Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying the Correlation Between Language Distance and Cross-Lingual Transfer in a Multilingual Representation Space. (arXiv:2305.02151v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02151","description":"<p>Prior research has investigated the impact of various linguistic features on\ncross-lingual transfer performance. In this study, we investigate the manner in\nwhich this effect can be mapped onto the representation space. While past\nstudies have focused on the impact on cross-lingual alignment in multilingual\nlanguage models during fine-tuning, this study examines the absolute evolution\nof the respective language representation spaces produced by MLLMs. We place a\nspecific emphasis on the role of linguistic characteristics and investigate\ntheir inter-correlation with the impact on representation spaces and\ncross-lingual transfer performance. Additionally, this paper provides\npreliminary evidence of how these findings can be leveraged to enhance transfer\nto linguistically distant languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Philippy_F/0/1/0/all/0/1\">Fred Philippy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Siwen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddadan_S/0/1/0/all/0/1\">Shohreh Haddadan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Listwise Document Reranking with a Large Language Model. (arXiv:2305.02156v1 [cs.IR])","link":"http://arxiv.org/abs/2305.02156","description":"<p>Supervised ranking methods based on bi-encoder or cross-encoder architectures\nhave shown success in multi-stage text ranking tasks, but they require large\namounts of relevance judgments as training data. In this work, we propose\nListwise Reranker with a Large Language Model (LRL), which achieves strong\nreranking effectiveness without using any task-specific training data.\nDifferent from the existing pointwise ranking methods, where documents are\nscored independently and ranked according to the scores, LRL directly generates\na reordered list of document identifiers given the candidate documents.\nExperiments on three TREC web search datasets demonstrate that LRL not only\noutperforms zero-shot pointwise methods when reranking first-stage retrieval\nresults, but can also act as a final-stage reranker to improve the top-ranked\nresults of a pointwise method for improved efficiency. Additionally, we apply\nour approach to subsets of MIRACL, a recent multilingual retrieval dataset,\nwith results showing its potential to generalize across different languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xueguang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pradeep_R/0/1/0/all/0/1\">Ronak Pradeep</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Language Models' Predictions with High-Impact Concepts. (arXiv:2305.02160v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02160","description":"<p>The emergence of large-scale pretrained language models has posed\nunprecedented challenges in deriving explanations of why the model has made\nsome predictions. Stemmed from the compositional nature of languages, spurious\ncorrelations have further undermined the trustworthiness of NLP systems,\nleading to unreliable model explanations that are merely correlated with the\noutput predictions. To encourage fairness and transparency, there exists an\nurgent demand for reliable explanations that allow users to consistently\nunderstand the model's behavior. In this work, we propose a complete framework\nfor extending concept-based interpretability methods to NLP. Specifically, we\npropose a post-hoc interpretability method for extracting predictive high-level\nfeatures (concepts) from the pretrained model's hidden layer activations. We\noptimize for features whose existence causes the output predictions to change\nsubstantially, \\ie generates a high impact. Moreover, we devise several\nevaluation metrics that can be universally applied. Extensive experiments on\nreal and synthetic tasks demonstrate that our method achieves superior results\non {predictive impact}, usability, and faithfulness compared to the baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruochen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Statistical Exploration of Text Partition Into Constituents: The Case of the Priestly Source in the Books of Genesis and Exodus. (arXiv:2305.02170v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02170","description":"<p>We present a pipeline for a statistical textual exploration, offering a\nstylometry-based explanation and statistical validation of a hypothesized\npartition of a text. Given a parameterization of the text, our pipeline: (1)\ndetects literary features yielding the optimal overlap between the hypothesized\nand unsupervised partitions, (2) performs a hypothesis-testing analysis to\nquantify the statistical significance of the optimal overlap, while conserving\nimplicit correlations between units of text that are more likely to be grouped,\nand (3) extracts and quantifies the importance of features most responsible for\nthe classification, estimates their statistical stability and cluster-wise\nabundance.\n</p>\n<p>We apply our pipeline to the first two books in the Bible, where one\nstylistic component stands out in the eyes of biblical scholars, namely, the\nPriestly component. We identify and explore statistically significant stylistic\ndifferences between the Priestly and non-Priestly components.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoffe_G/0/1/0/all/0/1\">Gideon Yoffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buhler_A/0/1/0/all/0/1\">Axel B&#xfc;hler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dershowitz_N/0/1/0/all/0/1\">Nachum Dershowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finkelstein_I/0/1/0/all/0/1\">Israel Finkelstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piasetzky_E/0/1/0/all/0/1\">Eli Piasetzky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romer_T/0/1/0/all/0/1\">Thomas R&#xf6;mer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sober_B/0/1/0/all/0/1\">Barak Sober</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Being Parameter-Efficient: A Stratified Sparsely Activated Transformer with Dynamic Capacity. (arXiv:2305.02176v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02176","description":"<p>Mixture-of-experts (MoE) models that employ sparse activation have\ndemonstrated effectiveness in significantly increasing the number of parameters\nwhile maintaining low computational requirements per token. However, recent\nstudies have established that MoE models are inherently parameter-inefficient\nas the improvement in performance diminishes with an increasing number of\nexperts. We hypothesize this parameter inefficiency is a result of all experts\nhaving equal capacity, which may not adequately meet the varying complexity\nrequirements of different tokens or tasks, e.g., in a multilingual setting,\nlanguages based on their resource levels might require different capacities. In\nlight of this, we propose Stratified Mixture of Experts(SMoE) models, which\nfeature a stratified structure and can assign dynamic capacity to different\ntokens. We demonstrate the effectiveness of SMoE on two multilingual machine\ntranslation benchmarks, where it outperforms multiple state-of-the-art MoE\nmodels. On a diverse 15-language dataset, SMoE improves the translation quality\nover vanilla MoE by +0.93 BLEU points on average. Additionally, SMoE is\nparameter-efficient, matching vanilla MoE performance with around 50\\% fewer\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elbayad_M/0/1/0/all/0/1\">Maha Elbayad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_K/0/1/0/all/0/1\">Kenton Murray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maillard_J/0/1/0/all/0/1\">Jean Maillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goswami_V/0/1/0/all/0/1\">Vedanuj Goswami</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Linguistic Properties of Monolingual BERTs with Typological Classification among Languages. (arXiv:2305.02215v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02215","description":"<p>The overwhelming success of transformers is a real conundrum stimulating a\ncompelling question: are these machines replicating some traditional linguistic\nmodels or discovering radically new theories? In this paper, we propose a novel\nstandpoint to investigate this important question. Using typological\nsimilarities among languages, we aim to layer-wise compare transformers for\ndifferent languages to observe whether these similarities emerge for particular\nlayers. For this investigation, we propose to use Centered kernel alignment to\nmeasure similarity among weight matrices. We discovered that syntactic\ntypological similarity is consistent with the similarity among weights in the\nmiddle layers. This finding confirms results obtained by syntactically probing\nBERT and, thus, gives an important confirmation that BERT is replicating\ntraditional linguistic models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ranaldi_F/0/1/0/all/0/1\">Federico Ranaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruzzetti_E/0/1/0/all/0/1\">Elena Sofia Ruzzetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Logozzo_F/0/1/0/all/0/1\">Felicia Logozzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mastromattei_M/0/1/0/all/0/1\">Michele Mastromattei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranaldi_L/0/1/0/all/0/1\">Leonardo Ranaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanzotto_F/0/1/0/all/0/1\">Fabio Massimo Zanzotto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clinical Note Generation from Doctor-Patient Conversations using Large Language Models: Insights from MEDIQA-Chat. (arXiv:2305.02220v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02220","description":"<p>This paper describes our submission to the MEDIQA-Chat 2023 shared task for\nautomatic clinical note generation from doctor-patient conversations. We report\nresults for two approaches: the first fine-tunes a pre-trained language model\n(PLM) on the shared task data, and the second uses few-shot in-context learning\n(ICL) with a large language model (LLM). Both achieve high performance as\nmeasured by automatic metrics (e.g. ROUGE, BERTScore) and ranked second and\nfirst, respectively, of all submissions to the shared task. Expert human\nscrutiny indicates that notes generated via the ICL-based approach with GPT-4\nare preferred about as often as human-written notes, making it a promising path\ntoward automated note generation from doctor-patient conversations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giorgi_J/0/1/0/all/0/1\">John Giorgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toma_A/0/1/0/all/0/1\">Augustin Toma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Ronald Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sondra Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_K/0/1/0/all/0/1\">Kevin R. An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Grace X. Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AttenWalker: Unsupervised Long-Document Question Answering via Attention-based Graph Walking. (arXiv:2305.02235v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02235","description":"<p>Annotating long-document question answering (long-document QA) pairs is\ntime-consuming and expensive. To alleviate the problem, it might be possible to\ngenerate long-document QA pairs via unsupervised question answering (UQA)\nmethods. However, existing UQA tasks are based on short documents, and can\nhardly incorporate long-range information. To tackle the problem, we propose a\nnew task, named unsupervised long-document question answering (ULQA), aiming to\ngenerate high-quality long-document QA instances in an unsupervised manner.\nBesides, we propose AttenWalker, a novel unsupervised method to aggregate and\ngenerate answers with long-range dependency so as to construct long-document QA\npairs. Specifically, AttenWalker is composed of three modules, i.e., span\ncollector, span linker and answer aggregator. Firstly, the span collector takes\nadvantage of constituent parsing and reconstruction loss to select informative\ncandidate spans for constructing answers. Secondly, by going through the\nattention graph of a pre-trained long-document model, potentially interrelated\ntext spans (that might be far apart) could be linked together via an\nattention-walking algorithm. Thirdly, in the answer aggregator, linked spans\nare aggregated into the final answer via the mask-filling ability of a\npre-trained model. Extensive experiments show that AttenWalker outperforms\nprevious methods on Qasper and NarrativeQA. In addition, AttenWalker also shows\nstrong performance in the few-shot learning setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yuxiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Benefits of Label-Description Training for Zero-Shot Text Classification. (arXiv:2305.02239v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02239","description":"<p>Large language models have improved zero-shot text classification by allowing\nthe transfer of semantic knowledge from the training data in order to classify\namong specific label sets in downstream tasks. We propose a simple way to\nfurther improve zero-shot accuracies with minimal effort. We curate small\nfinetuning datasets intended to describe the labels for a task. Unlike typical\nfinetuning data, which has texts annotated with labels, our data simply\ndescribes the labels in language, e.g., using a few related terms,\ndictionary/encyclopedia entries, and short templates. Across a range of topic\nand sentiment datasets, our method is more accurate than zero-shot by 15-17%\nabsolute. It is also more robust to choices required for zero-shot\nclassification, such as patterns for prompting the model to classify and\nmappings from labels to tokens in the model's vocabulary. Furthermore, since\nour data merely describes the labels but does not use input texts, finetuning\non it yields a model that performs strongly on multiple text domains for a\ngiven label set, even improving over few-shot out-of-domain classification in\nmultiple settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lingyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1\">Debanjan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimpel_K/0/1/0/all/0/1\">Kevin Gimpel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Training and Decoding for Pivot-based Cascaded Translation Model. (arXiv:2305.02261v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02261","description":"<p>Utilizing pivot language effectively can significantly improve low-resource\nmachine translation. Usually, the two translation models, source-pivot and\npivot-target, are trained individually and do not utilize the limited (source,\ntarget) parallel data. This work proposes an end-to-end training method for the\ncascaded translation model and configures an improved decoding algorithm. The\ninput of the pivot-target model is modified to weighted pivot embedding based\non the probability distribution output by the source-pivot model. This allows\nthe model to be trained end-to-end. In addition, we mitigate the inconsistency\nbetween tokens and probability distributions while using beam search in pivot\ndecoding. Experiments demonstrate that our method enhances the quality of\ntranslation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangyou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihua Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text. (arXiv:2305.02265v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02265","description":"<p>Pretrained Vision-Language Models (VLMs) have achieved remarkable performance\nin image retrieval from text. However, their performance drops drastically when\nconfronted with linguistically complex texts that they struggle to comprehend.\nInspired by the Divide-and-Conquer algorithm and dual-process theory, in this\npaper, we regard linguistically complex texts as compound proposition texts\ncomposed of multiple simple proposition sentences and propose an end-to-end\nNeural Divide-and-Conquer Reasoning framework, dubbed NDCR. It contains three\nmain components: 1)Divide: a proposition generator divides the compound\nproposition text into simple proposition sentences and produces their\ncorresponding representations, 2)Conquer: a pretrained VLMs-based\nvisual-linguistic interactor achieves the interaction between decomposed\nproposition sentences and images, 3)Combine: a neural-symbolic reasoner\ncombines the above reasoning states to obtain the final solution via a neural\nlogic reasoning approach. According to the dual-process theory, the\nvisual-linguistic interactor and neural-symbolic reasoner could be regarded as\nanalogical reasoning System 1 and logical reasoning System 2. We conduct\nextensive experiments on a challenging image retrieval from contextual\ndescriptions data set. Experimental results and analyses indicate NDCR\nsignificantly improves performance in the complex image-text reasoning problem.\nCode link: https://github.com/YunxinLi/NDCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Baotian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yunxin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M2-CTTS: End-to-End Multi-scale Multi-modal Conversational Text-to-Speech Synthesis. (arXiv:2305.02269v1 [cs.SD])","link":"http://arxiv.org/abs/2305.02269","description":"<p>Conversational text-to-speech (TTS) aims to synthesize speech with proper\nprosody of reply based on the historical conversation. However, it is still a\nchallenge to comprehensively model the conversation, and a majority of\nconversational TTS systems only focus on extracting global information and omit\nlocal prosody features, which contain important fine-grained information like\nkeywords and emphasis. Moreover, it is insufficient to only consider the\ntextual features, and acoustic features also contain various prosody\ninformation. Hence, we propose M2-CTTS, an end-to-end multi-scale multi-modal\nconversational text-to-speech system, aiming to comprehensively utilize\nhistorical conversation and enhance prosodic expression. More specifically, we\ndesign a textual context module and an acoustic context module with both\ncoarse-grained and fine-grained modeling. Experimental results demonstrate that\nour model mixed with fine-grained context information and additionally\nconsidering acoustic features achieves better prosody performance and\nnaturalness in CMOS tests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jinlong Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yayue Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fengping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Ya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yingming Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1\">Jianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jianqing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiaen Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating BERT-based Scientific Relation Classifiers for Scholarly Knowledge Graph Construction on Digital Library Collections. (arXiv:2305.02291v1 [cs.DL])","link":"http://arxiv.org/abs/2305.02291","description":"<p>The rapid growth of research publications has placed great demands on digital\nlibraries (DL) for advanced information management technologies. To cater to\nthese demands, techniques relying on knowledge-graph structures are being\nadvocated. In such graph-based pipelines, inferring semantic relations between\nrelated scientific concepts is a crucial step. Recently, BERT-based pre-trained\nmodels have been popularly explored for automatic relation classification.\nDespite significant progress, most of them were evaluated in different\nscenarios, which limits their comparability. Furthermore, existing methods are\nprimarily evaluated on clean texts, which ignores the digitization context of\nearly scholarly publications in terms of machine scanning and optical character\nrecognition (OCR). In such cases, the texts may contain OCR noise, in turn\ncreating uncertainty about existing classifiers' performances. To address these\nlimitations, we started by creating OCR-noisy texts based on three clean\ncorpora. Given these parallel corpora, we conducted a thorough empirical\nevaluation of eight Bert-based classification models by focusing on three\nfactors: (1) Bert variants; (2) classification strategies; and, (3) OCR noise\nimpacts. Experiments on clean data show that the domain-specific pre-trained\nBert is the best variant to identify scientific relations. The strategy of\npredicting a single relation each time outperforms the one simultaneously\nidentifying multiple relations in general. The optimal classifier's performance\ncan decline by around 10% to 20% in F-score on the noisy corpora. Insights\ndiscussed in this study can help DL stakeholders select techniques for building\noptimal knowledge-graph-based systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Ming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DSouza_J/0/1/0/all/0/1\">Jennifer D&#x27;Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auer_S/0/1/0/all/0/1\">S&#xf6;ren Auer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downie_J/0/1/0/all/0/1\">J. Stephen Downie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Efficacy of Length-Controllable Machine Translation. (arXiv:2305.02300v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02300","description":"<p>Length-controllable machine translation is a type of constrained translation.\nIt aims to contain the original meaning as much as possible while controlling\nthe length of the translation. We can use automatic summarization or machine\ntranslation evaluation metrics for length-controllable machine translation, but\nthis is not necessarily suitable and accurate. This work is the first attempt\nto evaluate the automatic metrics for length-controllable machine translation\ntasks systematically. We conduct a rigorous human evaluation on two translation\ndirections and evaluate 18 summarization or translation evaluation metrics. We\nfind that BLEURT and COMET have the highest correlation with human evaluation\nand are most suitable as evaluation metrics for length-controllable machine\ntranslation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weixuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangyou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihua Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes. (arXiv:2305.02301v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02301","description":"<p>Deploying large language models (LLMs) is challenging because they are memory\ninefficient and compute-intensive for practical applications. In reaction,\nresearchers train smaller task-specific models by either finetuning with human\nlabels or distilling using LLM-generated labels. However, finetuning and\ndistillation require large amounts of training data to achieve comparable\nperformance to LLMs. We introduce Distilling step-by-step, a new mechanism that\n(a) trains smaller models that outperform LLMs, and (b) achieves so by\nleveraging less training data needed by finetuning or distillation. Our method\nextracts LLM rationales as additional supervision for small models within a\nmulti-task training framework. We present three findings across 4 NLP\nbenchmarks: First, compared to both finetuning and distillation, our mechanism\nachieves better performance with much fewer labeled/unlabeled training\nexamples. Second, compared to LLMs, we achieve better performance using\nsubstantially smaller model sizes. Third, we reduce both the model size and the\namount of data required to outperform LLMs; our 770M T5 model outperforms the\n540B PaLM model using only 80% of available data on a benchmark task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cheng-Yu Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1\">Chih-Kuan Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakhost_H/0/1/0/all/0/1\">Hootan Nakhost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1\">Yasuhisa Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratner_A/0/1/0/all/0/1\">Alexander Ratner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1\">Ranjay Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings. (arXiv:2305.02317v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02317","description":"<p>Recent advances in large language models elicit reasoning in a chain of\nthought that allows models to decompose problems in a human-like fashion.\nThough this paradigm improves multi-step reasoning ability in language models,\nit is limited by being unimodal and applied mainly to question-answering tasks.\nWe claim that incorporating visual augmentation into reasoning is essential,\nespecially for complex, imaginative tasks. Consequently, we introduce VCoT, a\nnovel method that leverages chain of thought prompting with vision-language\ngrounding to recursively bridge the logical gaps within sequential data. Our\nmethod uses visual guidance to generate synthetic multimodal infillings that\nadd consistent and novel information to reduce the logical gaps for downstream\ntasks that can benefit from temporal reasoning, as well as provide\ninterpretability into models' multi-step reasoning. We apply VCoT to the Visual\nStorytelling and WikiHow summarization datasets and demonstrate through human\nevaluation that VCoT offers novel and consistent synthetic data augmentation\nbeating chain of thought baselines, which can be used to enhance downstream\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rose_D/0/1/0/all/0/1\">Daniel Rose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Himakunthala_V/0/1/0/all/0/1\">Vaishnavi Himakunthala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_A/0/1/0/all/0/1\">Andy Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ryan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_A/0/1/0/all/0/1\">Alex Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonar_C/0/1/0/all/0/1\">Chinmay Sonar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirza_D/0/1/0/all/0/1\">Diba Mirza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterizing Political Bias in Automatic Summaries: A Case Study of Trump and Biden. (arXiv:2305.02321v1 [cs.CL])","link":"http://arxiv.org/abs/2305.02321","description":"<p>Growing literature has shown that powerful NLP systems may encode social\nbiases; however, the political bias of summarization models remains relatively\nunknown. In this work, we use an entity replacement method to investigate the\nportrayal of politicians in automatically generated summaries of news articles.\nWe develop a computational framework based on political entities and lexical\nresources, and use it to assess biases about Donald Trump and Joe Biden in both\nextractive and abstractive summarization models. We find consistent\ndifferences, such as stronger associations of a collective US government (i.e.,\nadministration) with Biden than with Trump. These summary dissimilarities are\nmost prominent when the entity is heavily featured in the source article. Our\nsystematic characterization provides a framework for future studies of bias in\nsummarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Karen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uni-Encoder: A Fast and Accurate Response Selection Paradigm for Generation-Based Dialogue Systems. (arXiv:2106.01263v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.01263","description":"<p>Sample-and-rank is a key decoding strategy for modern generation-based\ndialogue systems. It helps achieve diverse and high-quality responses by\nselecting an answer from a small pool of generated candidates. The current\nstate-of-the-art ranking methods mainly use an encoding paradigm called\nCross-Encoder, which separately encodes each context-candidate pair and ranks\nthe candidates according to their fitness scores. However, Cross-Encoder\nrepeatedly encodes the same lengthy context for each candidate, resulting in\nhigh computational costs. Poly-Encoder addresses the above problems by reducing\nthe interaction between context and candidates, but with a price of performance\ndrop. In this work, we develop a new paradigm called Uni-Encoder, that keeps\nthe full attention over each pair as in Cross-Encoder while only encoding the\ncontext once, as in Poly-Encoder. Uni-Encoder encodes all the candidates with\nthe context in one forward pass. We use the same positional embedding for all\ncandidates to ensure they are treated equally and design a new attention\nmechanism to avoid confusion. Our Uni-Encoder can simulate other ranking\nparadigms using different attention and response concatenation methods.\nExtensive experiments show that our proposed paradigm achieves new\nstate-of-the-art results on four benchmark datasets with high computational\nefficiency. For instance, it improves R10@1 by 2.9% with an approximately 4X\nfaster inference speed on the Ubuntu V2 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chiyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haofei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1\">Pengfei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Leyang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1\">Zhenzhong Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What do End-to-End Speech Models Learn about Speaker, Language and Channel Information? A Layer-wise and Neuron-level Analysis. (arXiv:2107.00439v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.00439","description":"<p>Deep neural networks are inherently opaque and challenging to interpret.\nUnlike hand-crafted feature-based models, we struggle to comprehend the\nconcepts learned and how they interact within these models. This understanding\nis crucial not only for debugging purposes but also for ensuring fairness in\nethical decision-making. In our study, we conduct a post-hoc functional\ninterpretability analysis of pretrained speech models using the probing\nframework [1]. Specifically, we analyze utterance-level representations of\nspeech models trained for various tasks such as speaker recognition and dialect\nidentification. We conduct layer and neuron-wise analyses, probing for speaker,\nlanguage, and channel properties. Our study aims to answer the following\nquestions: i) what information is captured within the representations? ii) how\nis it represented and distributed? and iii) can we identify a minimal subset of\nthe network that possesses this information?\n</p>\n<p>Our results reveal several novel findings, including: i) channel and gender\ninformation are distributed across the network, ii) the information is\nredundantly available in neurons with respect to a task, iii) complex\nproperties such as dialectal information are encoded only in the task-oriented\npretrained network, iv) and is localised in the upper layers, v) we can extract\na minimal subset of neurons encoding the pre-defined property, vi) salient\nneurons are sometimes shared between properties, vii) our analysis highlights\nthe presence of biases (for example gender) in the network. Our\ncross-architectural comparison indicates that: i) the pretrained models capture\nspeaker-invariant information, and ii) CNN models are competitive with\nTransformer models in encoding various understudied properties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrani_N/0/1/0/all/0/1\">Nadir Durrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt-based Zero-shot Relation Extraction with Semantic Knowledge Augmentation. (arXiv:2112.04539v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.04539","description":"<p>In relation triplet extraction (RTE), recognizing unseen (new) relations for\nwhich there are no training instances is a challenging task. Efforts have been\nmade to recognize unseen relations based on question-answering models or\nrelation descriptions. However, these approaches miss the semantic information\nabout connections between seen and unseen relations. In this paper, We propose\na prompt-based model with semantic knowledge augmentation (ZS-SKA) to recognize\nunseen relations under the zero-shot setting. We present a new word-level\nanalogy-based sentence translation rule and generate augmented instances with\nunseen relations from instances with seen relations using that new rule. We\ndesign prompts with weighted virtual label construction based on an external\nknowledge graph to integrate semantic knowledge information learned from seen\nrelations. Instead of using the actual label sets in the prompt template, we\nconstruct weighted virtual label words. We learn the representations of both\nseen and unseen relations with augmented instances and prompts. We then\ncalculate the distance between the generated representations using prototypical\nnetworks to predict unseen relations. Extensive experiments conducted on three\npublic datasets FewRel, Wiki-ZSL, and NYT, show that ZS-SKA outperforms\nstate-of-the-art methods under the zero-shot scenarios. Our experimental\nresults also demonstrate the effectiveness and robustness of ZS-SKA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_J/0/1/0/all/0/1\">Jiaying Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eldardiry_H/0/1/0/all/0/1\">Hoda Eldardiry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning of Sociopragmatic Meaning in Social Media. (arXiv:2203.07648v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07648","description":"<p>Recent progress in representation and contrastive learning in NLP has not\nwidely considered the class of \\textit{sociopragmatic meaning} (i.e., meaning\nin interaction within different language communities). To bridge this gap, we\npropose a novel framework for learning task-agnostic representations\ntransferable to a wide range of sociopragmatic tasks (e.g., emotion, hate\nspeech, humor, sarcasm). Our framework outperforms other contrastive learning\nframeworks for both in-domain and out-of-domain data, across both the general\nand few-shot settings. For example, compared to two popular pre-trained\nlanguage models, our method obtains an improvement of $11.66$ average $F_1$ on\n$16$ datasets when fine-tuned on only $20$ training samples per dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_G/0/1/0/all/0/1\">Ganesh Jawahar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Adversarial Purification as Defense against Adversarial Attacks. (arXiv:2203.14207v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.14207","description":"<p>Adversarial purification is a successful defense mechanism against\nadversarial attacks without requiring knowledge of the form of the incoming\nattack. Generally, adversarial purification aims to remove the adversarial\nperturbations therefore can make correct predictions based on the recovered\nclean samples. Despite the success of adversarial purification in the computer\nvision field that incorporates generative models such as energy-based models\nand diffusion models, using purification as a defense strategy against textual\nadversarial attacks is rarely explored. In this work, we introduce a novel\nadversarial purification method that focuses on defending against textual\nadversarial attacks. With the help of language models, we can inject noise by\nmasking input texts and reconstructing the masked texts based on the masked\nlanguage models. In this way, we construct an adversarial purification process\nfor textual models against the most widely used word-substitution adversarial\nattacks. We test our proposed adversarial purification method on several strong\nadversarial attack methods including Textfooler and BERT-Attack and\nexperimental results indicate that the purification algorithm can successfully\ndefend against strong word-substitution attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Demin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ontology-Driven and Weakly Supervised Rare Disease Identification from Clinical Notes. (arXiv:2205.05656v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.05656","description":"<p>Computational text phenotyping is the practice of identifying patients with\ncertain disorders and traits from clinical notes. Rare diseases are challenging\nto be identified due to few cases available for machine learning and the need\nfor data annotation from domain experts. We propose a method using ontologies\nand weak supervision, with recent pre-trained contextual representations from\nBi-directional Transformers (e.g. BERT). The ontology-based framework includes\ntwo steps: (i) Text-to-UMLS, extracting phenotypes by contextually linking\nmentions to concepts in Unified Medical Language System (UMLS), with a Named\nEntity Recognition and Linking (NER+L) tool, SemEHR, and weak supervision with\ncustomised rules and contextual mention representation; (ii) UMLS-to-ORDO,\nmatching UMLS concepts to rare diseases in Orphanet Rare Disease Ontology\n(ORDO). The weakly supervised approach is proposed to learn a phenotype\nconfirmation model to improve Text-to-UMLS linking, without annotated data from\ndomain experts. We evaluated the approach on three clinical datasets, MIMIC-III\ndischarge summaries, MIMIC-III radiology reports, and NHS Tayside brain imaging\nreports from two institutions in the US and the UK, with annotations. The\nimprovements in the precision were pronounced (by over 30% to 50% absolute\nscore for Text-to-UMLS linking), with almost no loss of recall compared to the\nexisting NER+L tool, SemEHR. Results on radiology reports from MIMIC-III and\nNHS Tayside were consistent with the discharge summaries. The overall pipeline\nprocessing clinical notes can extract rare disease cases, mostly uncaptured in\nstructured data (manually assigned ICD codes). We discuss the usefulness of the\nweak supervision approach and propose directions for future studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suarez_Paniagua_V/0/1/0/all/0/1\">V&#xed;ctor Su&#xe1;rez-Paniagua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huayu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Minhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casey_A/0/1/0/all/0/1\">Arlene Casey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davidson_E/0/1/0/all/0/1\">Emma Davidson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alex_B/0/1/0/all/0/1\">Beatrice Alex</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whiteley_W/0/1/0/all/0/1\">William Whiteley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Honghan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Diminishing Returns of Masked Language Models to Science. (arXiv:2205.11342v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11342","description":"<p>Transformer-based masked language models such as BERT, trained on general\ncorpora, have shown impressive performance on downstream tasks. It has also\nbeen demonstrated that the downstream task performance of such models can be\nimproved by pretraining larger models for longer on more data. In this work, we\nempirically evaluate the extent to which these results extend to tasks in\nscience. We use 14 domain-specific transformer-based models (including\nScholarBERT, a new 770M-parameter science-focused masked language model\npretrained on up to 225B tokens) to evaluate the impact of training data, model\nsize, pretraining and finetuning time on 12 downstream scientific tasks.\nInterestingly, we find that increasing model sizes, training data, or compute\ntime does not always lead to significant improvements (i.e., &gt;1% F1), if at\nall, in scientific information extraction tasks and offered possible\nexplanations for the surprising performance differences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1\">Zhi Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajith_A/0/1/0/all/0/1\">Aswathy Ajith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauloski_G/0/1/0/all/0/1\">Gregory Pauloski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duede_E/0/1/0/all/0/1\">Eamon Duede</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chard_K/0/1/0/all/0/1\">Kyle Chard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_I/0/1/0/all/0/1\">Ian Foster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks. (arXiv:2205.15171v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.15171","description":"<p>Societal biases are reflected in large pre-trained language models and their\nfine-tuned versions on downstream tasks. Common in-processing bias mitigation\napproaches, such as adversarial training and mutual information removal,\nintroduce additional optimization criteria, and update the model to reach a new\ndebiased state. However, in practice, end-users and practitioners might prefer\nto switch back to the original model, or apply debiasing only on a specific\nsubset of protected attributes. To enable this, we propose a novel modular bias\nmitigation approach, consisting of stand-alone highly sparse debiasing\nsubnetworks, where each debiasing module can be integrated into the core model\non-demand at inference time. Our approach draws from the concept of \\emph{diff}\npruning, and proposes a novel training regime adaptable to various\nrepresentation disentanglement optimizations. We conduct experiments on three\nclassification tasks with gender, race, and age as protected attributes. The\nresults show that our modular approach, while maintaining task performance,\nimproves (or at least remains on-par with) the effectiveness of bias mitigation\nin comparison with baseline finetuning. Particularly on a two-attribute\ndataset, our approach with separately learned debiasing subnetworks shows\neffective utilization of either or both the subnetworks for selective bias\nmitigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hauzenberger_L/0/1/0/all/0/1\">Lukas Hauzenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masoudian_S/0/1/0/all/0/1\">Shahed Masoudian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1\">Deepak Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schedl_M/0/1/0/all/0/1\">Markus Schedl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekabsaz_N/0/1/0/all/0/1\">Navid Rekabsaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ContraCLM: Contrastive Learning For Causal Language Model. (arXiv:2210.01185v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.01185","description":"<p>Despite exciting progress in causal language models, the expressiveness of\nthe representations is largely limited due to poor discrimination ability. To\nremedy this issue, we present ContraCLM, a novel contrastive learning framework\nat both token-level and sequence-level. We assess ContraCLM on a variety of\ndownstream tasks. We show that ContraCLM enhances discrimination of the\nrepresentations and bridges the gap with the encoder-only models, which makes\ncausal language models better suited for tasks beyond language generation.\nSpecifically, we attain $44\\%$ relative improvement on the Semantic Textual\nSimilarity tasks and $34\\%$ on Code-to-Code Search tasks. Furthermore, by\nimproving the expressiveness of the representations, ContraCLM also boosts the\nsource code generation capability with $9\\%$ relative improvement on execution\naccuracy on the HumanEval benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1\">Nihal Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dejiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_F/0/1/0/all/0/1\">Feng Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaopeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Ming Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nallapati_R/0/1/0/all/0/1\">Ramesh Nallapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_B/0/1/0/all/0/1\">Baishakhi Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_P/0/1/0/all/0/1\">Parminder Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaofei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Far Are We from Real Synonym Substitution Attacks?. (arXiv:2210.02844v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.02844","description":"<p>In this paper, we explore the following question: how far are we from real\nsynonym substitution attacks (SSAs). We approach this question by examining how\nSSAs replace words in the original sentence and show that there are still\nunresolved obstacles that make current SSAs generate invalid adversarial\nsamples. We reveal that four widely used word substitution methods generate a\nlarge fraction of invalid substitution words that are ungrammatical or do not\npreserve the original sentence's semantics. Next, we show that the semantic and\ngrammatical constraints used in SSAs for detecting invalid word replacements\nare highly insufficient in detecting invalid adversarial samples. Our work is\nan important stepping stone to constructing better SSAs in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_C/0/1/0/all/0/1\">Cheng-Han Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Kernel-Based View of Language Model Fine-Tuning. (arXiv:2210.05643v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.05643","description":"<p>It has become standard to solve NLP tasks by fine-tuning pre-trained language\nmodels (LMs), especially in low-data settings. There is minimal theoretical\nunderstanding of empirical success, e.g., why fine-tuning a model with $10^8$\nor more parameters on a couple dozen training points does not result in\noverfitting. We investigate whether the Neural Tangent Kernel (NTK) - which\noriginated as a model to study the gradient descent dynamics of infinitely wide\nnetworks with suitable random initialization - describes fine-tuning of\npre-trained LMs. This study was inspired by the decent performance of NTK for\ncomputer vision tasks (Wei et al., 2022). We extend the NTK formalism to Adam\nand use Tensor Programs (Yang, 2020) to characterize conditions under which the\nNTK lens may describe fine-tuning updates to pre-trained language models.\nExtensive experiments on 14 NLP tasks validate our theory and show that\nformulating the downstream task as a masked word prediction problem through\nprompting often induces kernel-based dynamics during fine-tuning. Finally, we\nuse this kernel view to propose an explanation for the success of\nparameter-efficient subspace-based fine-tuning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malladi_S/0/1/0/all/0/1\">Sadhika Malladi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wettig_A/0/1/0/all/0/1\">Alexander Wettig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dingli Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Sanjeev Arora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe. (arXiv:2210.14348v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.14348","description":"<p>Privacy concerns have attracted increasing attention in data-driven products\ndue to the tendency of machine learning models to memorize sensitive training\ndata. Generating synthetic versions of such data with a formal privacy\nguarantee, such as differential privacy (DP), provides a promising path to\nmitigating these privacy concerns, but previous approaches in this direction\nhave typically failed to produce synthetic data of high quality. In this work,\nwe show that a simple and practical recipe in the text domain is effective:\nsimply fine-tuning a pretrained generative language model with DP enables the\nmodel to generate useful synthetic text with strong privacy protection. Through\nextensive empirical analyses on both benchmark and private customer data, we\ndemonstrate that our method produces synthetic text that is competitive in\nterms of utility with its non-private counterpart, meanwhile providing strong\nprotection against potential privacy leakages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inan_H/0/1/0/all/0/1\">Huseyin A. Inan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuechen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1\">Girish Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAnallen_J/0/1/0/all/0/1\">Julia McAnallen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shajari_H/0/1/0/all/0/1\">Hoda Shajari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levitan_D/0/1/0/all/0/1\">David Levitan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_R/0/1/0/all/0/1\">Robert Sim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Egocentric Audio-Visual Noise Suppression. (arXiv:2211.03643v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2211.03643","description":"<p>This paper studies audio-visual noise suppression for egocentric videos --\nwhere the speaker is not captured in the video. Instead, potential noise\nsources are visible on screen with the camera emulating the off-screen\nspeaker's view of the outside world. This setting is different from prior work\nin audio-visual speech enhancement that relies on lip and facial visuals. In\nthis paper, we first demonstrate that egocentric visual information is helpful\nfor noise suppression. We compare object recognition and action\nclassification-based visual feature extractors and investigate methods to align\naudio and visual representations. Then, we examine different fusion strategies\nfor the aligned features, and locations within the noise suppression model to\nincorporate visual information. Experiments demonstrate that visual features\nare most helpful when used to generate additive correction masks. Finally, in\norder to ensure that the visual features are discriminative with respect to\ndifferent noise types, we introduce a multi-task learning framework that\njointly optimizes audio-visual noise suppression and video-based acoustic event\ndetection. This proposed multi-task framework outperforms the audio-only\nbaseline on all metrics, including a 0.16 PESQ improvement. Extensive ablations\nreveal the improved performance of the proposed model with multiple active\ndistractors, overall noise types, and across different SNRs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Roshan Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Weipeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Ju Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakomkin_E/0/1/0/all/0/1\">Egor Lakomkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalgaonkar_K/0/1/0/all/0/1\">Kaustubh Kalgaonkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning. (arXiv:2212.03760v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2212.03760","description":"<p>Recent studies have proposed unified user modeling frameworks that leverage\nuser behavior data from various applications. Many of them benefit from\nutilizing users' behavior sequences as plain texts, representing rich\ninformation in any domain or system without losing generality. Hence, a\nquestion arises: Can language modeling for user history corpus help improve\nrecommender systems? While its versatile usability has been widely investigated\nin many domains, its applications to recommender systems still remain\nunderexplored. We show that language modeling applied directly to task-specific\nuser histories achieves excellent results on diverse recommendation tasks.\nAlso, leveraging additional task-agnostic user histories delivers significant\nperformance benefits. We further demonstrate that our approach can provide\npromising transfer learning capabilities for a broad spectrum of real-world\nrecommender systems, even on unseen domains and services.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1\">Kyuyong Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_H/0/1/0/all/0/1\">Hanock Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1\">Wonjae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jisu Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Seungjae Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyung-Min Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness of Learning from Task Instructions. (arXiv:2212.03813v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.03813","description":"<p>Traditional supervised learning mostly works on individual tasks and requires\ntraining on a large set of task-specific examples. This paradigm seriously\nhinders the development of task generalization since preparing a task-specific\nexample set is costly. To build a system that can quickly and easily generalize\nto new tasks, task instructions have been adopted as an emerging trend of\nsupervision recently. These instructions give the model the definition of the\ntask and allow the model to output the appropriate answer based on the\ninstructions and inputs. However, task instructions are often expressed in\ndifferent forms, which can be interpreted from two threads: first, some\ninstructions are short sentences and are pretrained language model (PLM)\noriented, such as prompts, while other instructions are paragraphs and are\nhuman-oriented, such as those in Amazon MTurk; second, different end-users very\nlikely explain the same task with instructions of different textual\nexpressions. A robust system for task generalization should be able to handle\nany new tasks regardless of the variability of instructions.\n</p>\n<p>However, the system robustness in dealing with instruction-driven task\ngeneralization is still unexplored. This work investigates the system\nrobustness when the instructions of new tasks are (i) manipulated, (ii)\nparaphrased, or (iii) from different levels of conciseness. To our knowledge,\nthis is the first work that systematically studies how robust a PLM is when it\nis supervised by instructions with different factors of variability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiasheng Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hongyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hanzi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liangyu Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1\">Hongyuan Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenpeng Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages. (arXiv:2212.09651v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09651","description":"<p>Multilingual Pretrained Language Models (MPLMs) have shown their strong\nmultilinguality in recent empirical cross-lingual transfer studies. In this\npaper, we propose the Prompts Augmented by Retrieval Crosslingually (PARC)\npipeline to improve the zero-shot performance on low-resource languages (LRLs)\nby augmenting the context with semantically similar sentences retrieved from a\nhigh-resource language (HRL) as prompts. PARC improves the zero-shot\nperformance on three downstream tasks (binary sentiment classification, topic\ncategorization and natural language inference) with multilingual parallel test\nsets across 10 LRLs covering 6 language families in both unlabeled settings\n(+5.1%) and labeled settings (+16.3%). PARC-labeled also outperforms the\nfinetuning baseline by 3.7%. We find a significant positive correlation between\ncross-lingual transfer performance on one side, and the similarity between the\nhigh- and low-resource languages as well as the amount of low-resource\npretraining data on the other side. A robustness analysis suggests that PARC\nhas the potential to achieve even stronger performance with more powerful\nMPLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_E/0/1/0/all/0/1\">Ercong Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Sheng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_H/0/1/0/all/0/1\">Helmut Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments. (arXiv:2212.09683v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09683","description":"<p>We present a human-in-the-loop evaluation framework for fact-checking novel\nmisinformation claims and identifying social media messages that support them.\nOur approach extracts check-worthy claims, which are aggregated and ranked for\nreview. Stance classifiers are then used to identify tweets supporting novel\nmisinformation claims, which are further reviewed to determine whether they\nviolate relevant policies. To demonstrate the feasibility of our approach, we\ndevelop a baseline system based on modern NLP methods for human-in-the-loop\nfact-checking in the domain of COVID-19 treatments. Using our baseline system,\nwe show that human fact-checkers can identify 124 tweets per hour that violate\nTwitter's policies on COVID-19 misinformation. We will make our code, data,\nbaseline models, and detailed annotation guidelines available to support the\nevaluation of human-in-the-loop systems that identify novel misinformation\ndirectly from raw user-generated content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mendes_E/0/1/0/all/0/1\">Ethan Mendes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments. (arXiv:2212.09736v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09736","description":"<p>A key missing capacity of current language models (LMs) is grounding to\nreal-world environments. Most existing work for grounded language understanding\nuses LMs to directly generate plans that can be executed in the environment to\nachieve the desired effects. It thereby casts the burden of ensuring\ngrammaticality, faithfulness, and controllability all on the LMs. We propose\nPangu, a generic framework for grounded language understanding that capitalizes\non the discriminative ability of LMs instead of their generative ability. Pangu\nconsists of a symbolic agent and a neural LM working in a concerted fashion:\nThe agent explores the environment to incrementally construct valid plans, and\nthe LM evaluates the plausibility of the candidate plans to guide the search\nprocess. A case study on the challenging problem of knowledge base question\nanswering (KBQA), which features a massive environment, demonstrates the\nremarkable effectiveness and flexibility of Pangu: A BERT-base LM is sufficient\nfor setting a new record on standard KBQA datasets, and larger LMs further\nbring substantial gains. Pangu also enables, for the first time, effective\nfew-shot in-context learning for KBQA with large LMs such as Codex.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeqDiffuSeq: Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence Generation. (arXiv:2212.10325v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10325","description":"<p>Diffusion model, a new generative modelling paradigm, has achieved great\nsuccess in image, audio, and video generation. However, considering the\ndiscrete categorical nature of text, it is not trivial to extend continuous\ndiffusion models to natural language, and text diffusion models are less\nstudied. Sequence-to-sequence text generation is one of the essential natural\nlanguage processing topics. In this work, we apply diffusion models to approach\nsequence-to-sequence text generation, and explore whether the superiority\ngeneration performance of diffusion model can transfer to natural language\ndomain. We propose SeqDiffuSeq, a text diffusion model for sequence-to-sequence\ngeneration. SeqDiffuSeq uses an encoder-decoder Transformers architecture to\nmodel denoising function. In order to improve generation quality, SeqDiffuSeq\ncombines the self-conditioning technique and a newly proposed adaptive noise\nschedule technique. The adaptive noise schedule has the difficulty of denoising\nevenly distributed across time steps, and considers exclusive noise schedules\nfor tokens at different positional order. Experiment results illustrate the\ngood performance on sequence-to-sequence generation in terms of text quality\nand inference time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongyi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering. (arXiv:2212.10375v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10375","description":"<p>Despite the surprising few-shot performance of in-context learning (ICL), it\nis still a common practice to randomly sample examples to serve as context.\nThis paper advocates a new principle for ICL: self-adaptive in-context\nlearning. The self-adaption mechanism is introduced to help each sample find an\nin-context example permutation (i.e., selection and ordering) that can derive\nthe correct prediction, thus maximizing performance. To validate the\neffectiveness of self-adaptive ICL, we propose a general select-then-rank\nframework and instantiate it with new selection and ranking algorithms. Upon\nextensive evaluation on eight different NLP datasets, our self-adaptive ICL\nmethod achieves a 40% relative improvement over the common practice setting.\nFurther analysis reveals the enormous potential of self-adaptive ICL that it\nmight be able to close the gap between ICL and finetuning given more advanced\nalgorithms. Our code is released to facilitate future research in this area:\nhttps://github.com/Shark-NLP/self-adaptive-ICL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaoxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiacheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Character-Aware Models Improve Visual Text Rendering. (arXiv:2212.10562v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10562","description":"<p>Current image generation models struggle to reliably produce well-formed\nvisual text. In this paper, we investigate a key contributing factor: popular\ntext-to-image models lack character-level input features, making it much harder\nto predict a word's visual makeup as a series of glyphs. To quantify this\neffect, we conduct a series of experiments comparing character-aware vs.\ncharacter-blind text encoders. In the text-only domain, we find that\ncharacter-aware models provide large gains on a novel spelling task\n(WikiSpell). Applying our learnings to the visual domain, we train a suite of\nimage generation models, and show that character-aware variants outperform\ntheir character-blind counterparts across a range of novel text rendering tasks\n(our DrawText benchmark). Our models set a much higher state-of-the-art on\nvisual spelling, with 30+ point accuracy gains over competitors on rare words,\ndespite training on far fewer examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rosanne Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrette_D/0/1/0/all/0/1\">Dan Garrette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saharia_C/0/1/0/all/0/1\">Chitwan Saharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blok_I/0/1/0/all/0/1\">Irina Blok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mical_R/0/1/0/all/0/1\">RJ Mical</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1\">Mohammad Norouzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"APAM: Adaptive Pre-training and Adaptive Meta Learning in Language Model for Noisy Labels and Long-tailed Learning. (arXiv:2302.03488v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.03488","description":"<p>Practical natural language processing (NLP) tasks are commonly long-tailed\nwith noisy labels. Those problems challenge the generalization and robustness\nof complex models such as Deep Neural Networks (DNNs). Some commonly used\nresampling techniques, such as oversampling or undersampling, could easily lead\nto overfitting. It is growing popular to learn the data weights leveraging a\nsmall amount of metadata. Besides, recent studies have shown the advantages of\nself-supervised pre-training, particularly to the under-represented data. In\nthis work, we propose a general framework to handle the problem of both\nlong-tail and noisy labels. The model is adapted to the domain of problems in a\ncontrastive learning manner. The re-weighting module is a feed-forward network\nthat learns explicit weighting functions and adapts weights according to\nmetadata. The framework further adapts weights of terms in the loss function\nthrough a combination of the polynomial expansion of cross-entropy loss and\nfocal loss. Our extensive experiments show that the proposed framework\nconsistently outperforms baseline methods. Lastly, our sensitive analysis\nemphasizes the capability of the proposed framework to handle the long-tailed\nproblem and mitigate the negative impact of noisy labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chi_S/0/1/0/all/0/1\">Sunyi Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhenyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zheng Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DocILE Benchmark for Document Information Localization and Extraction. (arXiv:2302.05658v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.05658","description":"<p>This paper introduces the DocILE benchmark with the largest dataset of\nbusiness documents for the tasks of Key Information Localization and Extraction\nand Line Item Recognition. It contains 6.7k annotated business documents, 100k\nsynthetically generated documents, and nearly~1M unlabeled documents for\nunsupervised pre-training. The dataset has been built with knowledge of domain-\nand task-specific aspects, resulting in the following key features: (i)\nannotations in 55 classes, which surpasses the granularity of previously\npublished key information extraction datasets by a large margin; (ii) Line Item\nRecognition represents a highly practical information extraction task, where\nkey information has to be assigned to items in a table; (iii) documents come\nfrom numerous layouts and the test set includes zero- and few-shot cases as\nwell as layouts commonly seen in the training set. The benchmark comes with\nseveral baselines, including RoBERTa, LayoutLMv3 and DETR-based Table\nTransformer; applied to both tasks of the DocILE benchmark, with results shared\nin this paper, offering a quick starting point for future work. The dataset,\nbaselines and supplementary material are available at\nhttps://github.com/rossumai/docile.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Simsa_S/0/1/0/all/0/1\">&#x160;t&#x11b;p&#xe1;n &#x160;imsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sulc_M/0/1/0/all/0/1\">Milan &#x160;ulc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uricar_M/0/1/0/all/0/1\">Michal U&#x159;i&#x10d;&#xe1;&#x159;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_Y/0/1/0/all/0/1\">Yash Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamdi_A/0/1/0/all/0/1\">Ahmed Hamdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocian_M/0/1/0/all/0/1\">Mat&#x11b;j Koci&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skalicky_M/0/1/0/all/0/1\">Maty&#xe1;&#x161; Skalick&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Ji&#x159;&#xed; Matas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1\">Antoine Doucet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coustaty_M/0/1/0/all/0/1\">Micka&#xeb;l Coustaty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1\">Dimosthenis Karatzas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Product Question Answering in E-Commerce: A Survey. (arXiv:2302.08092v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.08092","description":"<p>Product question answering (PQA), aiming to automatically provide instant\nresponses to customer's questions in E-Commerce platforms, has drawn increasing\nattention in recent years. Compared with typical QA problems, PQA exhibits\nunique challenges such as the subjectivity and reliability of user-generated\ncontents in E-commerce platforms. Therefore, various problem settings and novel\nmethods have been proposed to capture these special characteristics. In this\npaper, we aim to systematically review existing research efforts on PQA.\nSpecifically, we categorize PQA studies into four problem settings in terms of\nthe form of provided answers. We analyze the pros and cons, as well as present\nexisting datasets and evaluation protocols for each setting. We further\nsummarize the most significant challenges that characterize PQA from general QA\napplications and discuss their corresponding solutions. Finally, we conclude\nthis paper by providing the prospect on several future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Task Graph Generation from Instructional Video Transcripts. (arXiv:2302.09173v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2302.09173","description":"<p>This work explores the problem of generating task graphs of real-world\nactivities. Different from prior formulations, we consider a setting where text\ntranscripts of instructional videos performing a real-world activity (e.g.,\nmaking coffee) are provided and the goal is to identify the key steps relevant\nto the task as well as the dependency relationship between these key steps. We\npropose a novel task graph generation approach that combines the reasoning\ncapabilities of instruction-tuned language models along with clustering and\nranking components to generate accurate task graphs in a completely\nunsupervised manner. We show that the proposed approach generates more accurate\ntask graphs compared to a supervised learning approach on tasks from the ProceL\nand CrossTask datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Logeswaran_L/0/1/0/all/0/1\">Lajanugen Logeswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1\">Sungryull Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Yunseok Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Moontae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Social Media for Early Detection of Depression in COVID-19 Patients. (arXiv:2302.12044v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.12044","description":"<p>The COVID-19 pandemic has caused substantial damage to global health. Even\nthough three years have passed, the world continues to struggle with the virus.\nConcerns are growing about the impact of COVID-19 on the mental health of\ninfected individuals, who are more likely to experience depression, which can\nhave long-lasting consequences for both the affected individuals and the world.\nDetection and intervention at an early stage can reduce the risk of depression\nin COVID-19 patients. In this paper, we investigated the relationship between\nCOVID-19 infection and depression through social media analysis. Firstly, we\nmanaged a dataset of COVID-19 patients that contains information about their\nsocial media activity both before and after infection. Secondly,We conducted an\nextensive analysis of this dataset to investigate the characteristic of\nCOVID-19 patients with a higher risk of depression. Thirdly, we proposed a deep\nneural network for early prediction of depression risk. This model considers\ndaily mood swings as a psychiatric signal and incorporates textual and\nemotional characteristics via knowledge distillation. Experimental results\ndemonstrate that our proposed framework outperforms baselines in detecting\ndepression risk, with an AUROC of 0.9317 and an AUPRC of 0.8116. Our model has\nthe potential to enable public health organizations to initiate prompt\nintervention with high-risk patients\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiageng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yining Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shixu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Architext: Language-Driven Generative Architecture Design. (arXiv:2303.07519v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.07519","description":"<p>Architectural design is a highly complex practice that involves a wide\ndiversity of disciplines, technologies, proprietary design software, expertise,\nand an almost infinite number of constraints, across a vast array of design\ntasks. Enabling intuitive, accessible, and scalable design processes is an\nimportant step towards performance-driven and sustainable design for all. To\nthat end, we introduce Architext, a novel semantic generation assistive tool.\nArchitext enables design generation with only natural language prompts, given\nto large-scale Language Models, as input. We conduct a thorough quantitative\nevaluation of Architext's downstream task performance, focusing on semantic\naccuracy and diversity for a number of pre-trained language models ranging from\n120 million to 6 billion parameters. Architext models are able to learn the\nspecific design task, generating valid residential layouts at a near 100% rate.\nAccuracy shows great improvement when scaling the models, with the largest\nmodel (GPT-J) yielding impressive accuracy ranging between 25% to over 80% for\ndifferent prompt categories. We open source the finetuned Architext models and\nour synthetic dataset, hoping to inspire experimentation in this exciting area\nof design research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galanos_T/0/1/0/all/0/1\">Theodoros Galanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liapis_A/0/1/0/all/0/1\">Antonios Liapis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yannakakis_G/0/1/0/all/0/1\">Georgios N. Yannakakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning. (arXiv:2303.10475v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.10475","description":"<p>Task semantics can be expressed by a set of input-to-output examples or a\npiece of textual instruction. Conventional machine learning approaches for\nnatural language processing (NLP) mainly rely on the availability of\nlarge-scale sets of task-specific examples. Two issues arise: first, collecting\ntask-specific labeled examples does not apply to scenarios where tasks may be\ntoo complicated or costly to annotate, or the system is required to handle a\nnew task immediately; second, this is not user-friendly since end-users are\nprobably more willing to provide task description rather than a set of examples\nbefore using the system. Therefore, the community is paying increasing interest\nin a new supervision-seeking paradigm for NLP: learning from task instructions.\nDespite its impressive progress, there are some common issues that the\ncommunity struggles with. This survey paper tries to summarize and provide\ninsights into the current research on instruction learning, particularly by\nanswering the following questions: (i) What is task instruction, and what\ninstruction types exist? (ii) How to model instructions? (iii) What factors\ninfluence and explain the instructions' performance? (iv) What challenges\nremain in instruction learning? To our knowledge, this is the first\ncomprehensive survey about textual instructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_R/0/1/0/all/0/1\">Renze Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenpeng Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Image-Text Matching Improves Retrieval-based Chest X-Ray Report Generation. (arXiv:2303.17579v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.17579","description":"<p>Automated generation of clinically accurate radiology reports can improve\npatient care. Previous report generation methods that rely on image captioning\nmodels often generate incoherent and incorrect text due to their lack of\nrelevant domain knowledge, while retrieval-based attempts frequently retrieve\nreports that are irrelevant to the input image. In this work, we propose\nContrastive X-Ray REport Match (X-REM), a novel retrieval-based radiology\nreport generation module that uses an image-text matching score to measure the\nsimilarity of a chest X-ray image and radiology report for report retrieval. We\nobserve that computing the image-text matching score with a language-image\nmodel can effectively capture the fine-grained interaction between image and\ntext that is often lost when using cosine similarity. X-REM outperforms\nmultiple prior radiology report generation modules in terms of both natural\nlanguage and clinical metrics. Human evaluation of the generated reports\nsuggests that X-REM increased the number of zero-error reports and decreased\nthe average error severity compared to the baseline retrieval approach. Our\ncode is available at: https://github.com/rajpurkarlab/X-REM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jaehwan Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_K/0/1/0/all/0/1\">Katherine Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Andrew Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartung_S/0/1/0/all/0/1\">Sina Hartung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behzadi_F/0/1/0/all/0/1\">Fardad Behzadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calle_J/0/1/0/all/0/1\">Juan Calle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osayande_D/0/1/0/all/0/1\">David Osayande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohlen_M/0/1/0/all/0/1\">Michael Pohlen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adithan_S/0/1/0/all/0/1\">Subathra Adithan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ImpressionGPT: An Iterative Optimizing Framework for Radiology Report Summarization with ChatGPT. (arXiv:2304.08448v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.08448","description":"<p>The 'Impression' section of a radiology report is a critical basis for\ncommunication between radiologists and other physicians, and it is typically\nwritten by radiologists based on the 'Findings' section. However, writing\nnumerous impressions can be laborious and error-prone for radiologists.\nAlthough recent studies have achieved promising results in automatic impression\ngeneration using large-scale medical text data for pre-training and fine-tuning\npre-trained language models, such models often require substantial amounts of\nmedical text data and have poor generalization performance. While large\nlanguage models (LLMs) like ChatGPT have shown strong generalization\ncapabilities and performance, their performance in specific domains, such as\nradiology, remains under-investigated and potentially limited. To address this\nlimitation, we propose ImpressionGPT, which leverages the in-context learning\ncapability of LLMs by constructing dynamic contexts using domain-specific,\nindividualized data. This dynamic prompt approach enables the model to learn\ncontextual knowledge from semantically similar examples from existing data.\nAdditionally, we design an iterative optimization algorithm that performs\nautomatic evaluation on the generated impression results and composes the\ncorresponding instruction prompts to further optimize the model. The proposed\nImpressionGPT model achieves state-of-the-art performance on both MIMIC-CXR and\nOpenI datasets without requiring additional training data or fine-tuning the\nLLMs. This work presents a paradigm for localizing LLMs that can be applied in\na wide range of similar application scenarios, bridging the gap between\ngeneral-purpose LLMs and the specific language processing needs of various\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shaochen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yaonai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Lei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xiaoyan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dajiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CitePrompt: Using Prompts to Identify Citation Intent in Scientific Papers. (arXiv:2304.12730v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.12730","description":"<p>Citations in scientific papers not only help us trace the intellectual\nlineage but also are a useful indicator of the scientific significance of the\nwork. Citation intents prove beneficial as they specify the role of the\ncitation in a given context. In this paper, we present CitePrompt, a framework\nwhich uses the hitherto unexplored approach of prompt-based learning for\ncitation intent classification. We argue that with the proper choice of the\npretrained language model, the prompt template, and the prompt verbalizer, we\ncan not only get results that are better than or comparable to those obtained\nwith the state-of-the-art methods but also do it with much less exterior\ninformation about the scientific document. We report state-of-the-art results\non the ACL-ARC dataset, and also show significant improvement on the SciCite\ndataset over all baseline models except one. As suitably large labelled\ndatasets for citation intent classification can be quite hard to find, in a\nfirst, we propose the conversion of this task to the few-shot and zero-shot\nsettings. For the ACL-ARC dataset, we report a 53.86% F1 score for the\nzero-shot setting, which improves to 63.61% and 66.99% for the 5-shot and\n10-shot settings, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lahiri_A/0/1/0/all/0/1\">Avishek Lahiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_D/0/1/0/all/0/1\">Debarshi Kumar Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_I/0/1/0/all/0/1\">Imon Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks. (arXiv:2304.14732v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.14732","description":"<p>With the wide application of Large Language Models (LLMs) such as ChatGPT,\nhow to make the contents generated by LLM accurate and credible becomes very\nimportant, especially in complex knowledge-intensive tasks. In this paper, we\npropose a novel framework called Search-in-the-Chain (SearChain) to improve the\naccuracy, credibility and traceability of LLM-generated content for multi-hop\nquestion answering, which is a typical complex knowledge-intensive task.\nSearChain is a framework that deeply integrates LLM and information retrieval\n(IR). In SearChain, LLM constructs a chain-of-query, which is the decomposition\nof the multi-hop question. Each node of the chain is a query-answer pair\nconsisting of an IR-oriented query and the answer generated by LLM for this\nquery. IR verifies, completes, and traces the information of each node of the\nchain, so as to guide LLM to construct the correct chain-of-query, and finally\nanswer the multi-hop question. SearChain makes LLM change from trying to give a\nanswer to trying to construct the chain-of-query when faced with the multi-hop\nquestion, which can stimulate the knowledge-reasoning ability and provides the\ninterface for IR to be deeply involved in reasoning process of LLM. IR\ninteracts with each node of chain-of-query of LLM. It verifies the information\nof the node and provides the unknown knowledge to LLM, which ensures the\naccuracy of the whole chain in the process of LLM generating the answer.\nBesides, the contents returned by LLM to the user include not only the final\nanswer but also the reasoning process for the question, that is, the\nchain-of-query and the supporting documents retrieved by IR for each node of\nthe chain, which improves the credibility and traceability of the contents\ngenerated by LLM. Experimental results show SearChain outperforms related\nbaselines on four multi-hop question-answering datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shicheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds. (arXiv:2305.00969v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2305.00969","description":"<p>This paper describes the Ubenwa CryCeleb dataset - a labeled collection of\ninfant cries, and the accompanying CryCeleb 2023 task - a public speaker\nverification challenge based on infant cry sounds. We release for academic\nusage more than 6 hours of manually segmented cry sounds from 786 newborns to\nencourage research in infant cry analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Budaghyan_D/0/1/0/all/0/1\">David Budaghyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorin_A/0/1/0/all/0/1\">Arsenii Gorin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subakan_C/0/1/0/all/0/1\">Cem Subakan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onu_C/0/1/0/all/0/1\">Charles C. Onu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. (arXiv:2305.01219v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01219","description":"<p>The prompt-based learning paradigm, which bridges the gap between\npre-training and fine-tuning, achieves state-of-the-art performance on several\nNLP tasks, particularly in few-shot settings. Despite being widely applied,\nprompt-based learning is vulnerable to backdoor attacks. Textual backdoor\nattacks are designed to introduce targeted vulnerabilities into models by\npoisoning a subset of training samples through trigger injection and label\nmodification. However, they suffer from flaws such as abnormal natural language\nexpressions resulting from the trigger and incorrect labeling of poisoned\nsamples. In this study, we propose ProAttack, a novel and efficient method for\nperforming clean-label backdoor attacks based on the prompt, which uses the\nprompt itself as a trigger. Our method does not require external triggers and\nensures correct labeling of poisoned samples, improving the stealthy nature of\nthe backdoor attack. With extensive experiments on rich-resource and few-shot\ntext classification tasks, we empirically validate ProAttack's competitive\nperformance in textual backdoor attacks. Notably, in the rich-resource setting,\nProAttack achieves state-of-the-art attack success rates in the clean-label\nbackdoor attack benchmark without external triggers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jinming Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuan_L/0/1/0/all/0/1\">Luu Anh Tuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01555","description":"<p>Scaling language models have revolutionized widespread NLP tasks, yet little\ncomprehensively explored few-shot relation extraction with large language\nmodels. In this paper, we investigate principal methodologies, in-context\nlearning and data generation, for few-shot relation extraction via GPT-3.5\nthrough exhaustive experiments. To enhance few-shot performance, we further\npropose task-related instructions and schema-constrained data generation. We\nobserve that in-context learning can achieve performance on par with previous\nprompt learning approaches, and data generation with the large language model\ncan boost previous solutions to obtain new state-of-the-art few-shot results on\nfour widely-studied relation extraction datasets. We hope our work can inspire\nfuture research for the capabilities of large language models in few-shot\nrelation extraction. Code is available in\nhttps://github.com/zjunlp/DeepKE/tree/main/example/llm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuqi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models. (arXiv:2305.01645v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01645","description":"<p>Fine-tuning large models is highly effective, however, inference using these\nmodels can be expensive and produces carbon emissions. Knowledge distillation\nhas been shown to be a practical solution to reduce inference costs, but the\ndistillation process itself requires significant computational resources.\nRather than buying or renting GPUs to fine-tune, then distill a large model, an\nNLP practitioner who needs a compact model might also choose to simply allocate\nan available budget to hire annotators and manually label additional\nfine-tuning data. In this paper, we investigate how to most efficiently use a\nfixed budget to build a compact model. Through our extensive experiments on six\ndiverse NLP tasks, we find that distilling from T5-XXL (11B) to T5-Small (60M)\nleads to almost always a cost-efficient option compared to annotating more data\nto directly train a compact model (T5-Small (60M)). We further demonstrate that\nthe optimal amount of distillation that maximizes utility varies across\ndifferent budgetary scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Junmo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-05-03T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}