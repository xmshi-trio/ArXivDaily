{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-03-31T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"BEVERS: A General, Simple, and Performant Framework for Automatic Fact Verification. (arXiv:2303.16974v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16974","description":"<p>Automatic fact verification has become an increasingly popular topic in\nrecent years and among datasets the Fact Extraction and VERification (FEVER)\ndataset is one of the most popular. In this work we present BEVERS, a tuned\nbaseline system for the FEVER dataset. Our pipeline uses standard approaches\nfor document retrieval, sentence selection, and final claim classification,\nhowever, we spend considerable effort ensuring optimal performance for each\ncomponent. The results are that BEVERS achieves the highest FEVER score and\nlabel accuracy among all systems, published or unpublished. We also apply this\npipeline to another fact verification dataset, Scifact, and achieve the highest\nlabel accuracy among all systems on that dataset as well. We also make our full\ncode available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DeHaven_M/0/1/0/all/0/1\">Mitchell DeHaven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scott_S/0/1/0/all/0/1\">Stephen Scott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting to the Low-Resource Double-Bind: Investigating Low-Compute Methods on Low-Resource African Languages. (arXiv:2303.16985v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16985","description":"<p>Many natural language processing (NLP) tasks make use of massively\npre-trained language models, which are computationally expensive. However,\naccess to high computational resources added to the issue of data scarcity of\nAfrican languages constitutes a real barrier to research experiments on these\nlanguages. In this work, we explore the applicability of low-compute approaches\nsuch as language adapters in the context of this low-resource double-bind. We\nintend to answer the following question: do language adapters allow those who\nare doubly bound by data and compute to practically build useful models?\nThrough fine-tuning experiments on African languages, we evaluate their\neffectiveness as cost-effective approaches to low-resource African NLP. Using\nsolely free compute resources, our results show that language adapters achieve\ncomparable performances to massive pre-trained language models which are heavy\non computational resources. This opens the door to further experimentation and\nexploration on full-extent of language adapters capacities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leong_C/0/1/0/all/0/1\">Colin Leong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shandilya_H/0/1/0/all/0/1\">Herumb Shandilya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dossou_B/0/1/0/all/0/1\">Bonaventure F. P. Dossou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonja_A/0/1/0/all/0/1\">Atnafu Lambebo Tonja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1\">Joel Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omotayo_A/0/1/0/all/0/1\">Abdul-Hakeem Omotayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yousuf_O/0/1/0/all/0/1\">Oreen Yousuf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akinjobi_Z/0/1/0/all/0/1\">Zainab Akinjobi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1\">Chris Chinenye Emezue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsudeen Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolawole_S/0/1/0/all/0/1\">Steven Kolawole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Younwoo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adewumi_T/0/1/0/all/0/1\">Tosin Adewumi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ContraSim -- A Similarity Measure Based on Contrastive Learning. (arXiv:2303.16992v1 [cs.CL])","link":"http://arxiv.org/abs/2303.16992","description":"<p>Recent work has compared neural network representations via similarity-based\nanalyses, shedding light on how different aspects (architecture, training data,\netc.) affect models' internal representations. The quality of a similarity\nmeasure is typically evaluated by its success in assigning a high score to\nrepresentations that are expected to be matched. However, existing similarity\nmeasures perform mediocrely on standard benchmarks. In this work, we develop a\nnew similarity measure, dubbed ContraSim, based on contrastive learning. In\ncontrast to common closed-form similarity measures, ContraSim learns a\nparameterized measure by using both similar and dissimilar examples. We perform\nan extensive experimental evaluation of our method, with both language and\nvision models, on the standard layer prediction benchmark and two new\nbenchmarks that we introduce: the multilingual benchmark and the image-caption\nbenchmark. In all cases, ContraSim achieves much higher accuracy than previous\nsimilarity measures, even when presented with challenging examples, and reveals\nnew insights not captured by previous measures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahamim_A/0/1/0/all/0/1\">Adir Rahamim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating GPT-3.5 and GPT-4 Models on Brazilian University Admission Exams. (arXiv:2303.17003v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17003","description":"<p>The present study aims to explore the capabilities of Language Models (LMs)\nin tackling high-stakes multiple-choice tests, represented here by the Exame\nNacional do Ensino M\\'edio (ENEM), a multidisciplinary entrance examination\nwidely adopted by Brazilian universities. This exam poses challenging tasks for\nLMs, since its questions may span into multiple fields of knowledge, requiring\nunderstanding of information from diverse domains. For instance, a question may\nrequire comprehension of both statistics and biology to be solved. This work\nanalyzed responses generated by GPT-3.5 and GPT-4 models for questions\npresented in the 2009-2017 exams, as well as for questions of the 2022 exam,\nwhich were made public after the training of the models was completed.\nFurthermore, different prompt strategies were tested, including the use of\nChain-of-Thought (CoT) prompts to generate explanations for answers. On the\n2022 edition, the best-performing model, GPT-4 with CoT, achieved an accuracy\nof 87%, largely surpassing GPT-3.5 by 11 points. The code and data used on\nexperiments are available at https://github.com/piresramon/gpt-4-enem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nunes_D/0/1/0/all/0/1\">Desnes Nunes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Primi_R/0/1/0/all/0/1\">Ricardo Primi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pires_R/0/1/0/all/0/1\">Ramon Pires</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotufo_R/0/1/0/all/0/1\">Roberto Lotufo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How do decoding algorithms distribute information in dialogue responses?. (arXiv:2303.17006v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17006","description":"<p>Humans tend to follow the Uniform Information Density (UID) principle by\ndistributing information evenly in utterances. We study if decoding algorithms\nimplicitly follow this UID principle, and under what conditions adherence to\nUID might be desirable for dialogue generation. We generate responses using\ndifferent decoding algorithms with GPT-2 on the Persona-Chat dataset and\ncollect human judgments on their quality using Amazon Mechanical Turk. We find\nthat (i) surprisingly, model-generated responses follow the UID principle to a\ngreater extent than human responses, and (ii) decoding algorithms that promote\nUID do not generate higher-quality responses. Instead, when we control for\nsurprisal, non-uniformity of information density correlates with the quality of\nresponses with very low/high surprisal. Our findings indicate that encouraging\nnon-uniform responses is a potential solution to the ``likelihood trap''\nproblem (quality degradation in very high-likelihood text). Our dataset\ncontaining multiple candidate responses per dialog history along with\nhuman-annotated quality ratings is available at\nhttps://huggingface.co/datasets/saranya132/dialog_uid_gpt2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venkatraman_S/0/1/0/all/0/1\">Saranya Venkatraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reitter_D/0/1/0/all/0/1\">David Reitter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents. (arXiv:2303.17071v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17071","description":"<p>Large language models (LLMs) have emerged as valuable tools for many natural\nlanguage understanding tasks. In safety-critical applications such as\nhealthcare, the utility of these models is governed by their ability to\ngenerate outputs that are factually accurate and complete. In this work, we\npresent dialog-enabled resolving agents (DERA). DERA is a paradigm made\npossible by the increased conversational abilities of LLMs, namely GPT-4. It\nprovides a simple, interpretable forum for models to communicate feedback and\niteratively improve output. We frame our dialog as a discussion between two\nagent types - a Researcher, who processes information and identifies crucial\nproblem components, and a Decider, who has the autonomy to integrate the\nResearcher's information and makes judgments on the final output.\n</p>\n<p>We test DERA against three clinically-focused tasks. For medical conversation\nsummarization and care plan generation, DERA shows significant improvement over\nthe base GPT-4 performance in both human expert preference evaluations and\nquantitative metrics. In a new finding, we also show that GPT-4's performance\n(70%) on an open-ended version of the MedQA question-answering (QA) dataset\n(Jin et al. 2021, USMLE) is well above the passing level (60%), with DERA\nshowing similar performance. We release the open-ended MEDQA dataset at\nhttps://github.com/curai/curai-research/tree/main/DERA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nair_V/0/1/0/all/0/1\">Varun Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schumacher_E/0/1/0/all/0/1\">Elliot Schumacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tso_G/0/1/0/all/0/1\">Geoffrey Tso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannan_A/0/1/0/all/0/1\">Anitha Kannan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TLAG: An Informative Trigger and Label-Aware Knowledge Guided Model for Dialogue-based Relation Extraction. (arXiv:2303.17119v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17119","description":"<p>Dialogue-based Relation Extraction (DRE) aims to predict the relation type of\nargument pairs that are mentioned in dialogue. The latest trigger-enhanced\nmethods propose trigger prediction tasks to promote DRE. However, these methods\nare not able to fully leverage the trigger information and even bring noise to\nrelation extraction. To solve these problems, we propose TLAG, which fully\nleverages the trigger and label-aware knowledge to guide the relation\nextraction. First, we design an adaptive trigger fusion module to fully\nleverage the trigger information. Then, we introduce label-aware knowledge to\nfurther promote our model's performance. Experimental results on the DialogRE\ndataset show that our TLAG outperforms the baseline models, and detailed\nanalyses demonstrate the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_H/0/1/0/all/0/1\">Hao An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongsheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhihong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TreePiece: Faster Semantic Parsing via Tree Tokenization. (arXiv:2303.17161v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17161","description":"<p>Autoregressive (AR) encoder-decoder neural networks have proved successful in\nmany NLP problems, including Semantic Parsing -- a task that translates natural\nlanguage to machine-readable parse trees. However, the sequential prediction\nprocess of AR models can be slow. To accelerate AR for semantic parsing, we\nintroduce a new technique called TreePiece that tokenizes a parse tree into\nsubtrees and generates one subtree per decoding step. On TopV2 benchmark,\nTreePiece shows 4.6 times faster decoding speed than standard AR, and\ncomparable speed but significantly higher accuracy compared to\nNon-Autoregressive (NAR).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sid Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Akshat Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livshits_S/0/1/0/all/0/1\">Sasha Livshits</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Nordic Pile: A 1.2TB Nordic Dataset for Language Modeling. (arXiv:2303.17183v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17183","description":"<p>Pre-training Large Language Models (LLMs) require massive amounts of text\ndata, and the performance of the LLMs typically correlates with the scale and\nquality of the datasets. This means that it may be challenging to build LLMs\nfor smaller languages such as Nordic ones, where the availability of text\ncorpora is limited. In order to facilitate the development of the LLMS in the\nNordic languages, we curate a high-quality dataset consisting of 1.2TB of text,\nin all of the major North Germanic languages (Danish, Icelandic, Norwegian, and\nSwedish), as well as some high-quality English data. This paper details our\nconsiderations and processes for collecting, cleaning, and filtering the\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ohman_J/0/1/0/all/0/1\">Joey &#xd6;hman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verlinden_S/0/1/0/all/0/1\">Severine Verlinden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekgren_A/0/1/0/all/0/1\">Ariel Ekgren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gyllensten_A/0/1/0/all/0/1\">Amaru Cuba Gyllensten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isbister_T/0/1/0/all/0/1\">Tim Isbister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gogoulou_E/0/1/0/all/0/1\">Evangelia Gogoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlsson_F/0/1/0/all/0/1\">Fredrik Carlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahlgren_M/0/1/0/all/0/1\">Magnus Sahlgren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Humans in Humans Out: On GPT Converging Toward Common Sense in both Success and Failure. (arXiv:2303.17276v1 [cs.AI])","link":"http://arxiv.org/abs/2303.17276","description":"<p>Increase in computational scale and fine-tuning has seen a dramatic\nimprovement in the quality of outputs of large language models (LLMs) like GPT.\nGiven that both GPT-3 and GPT-4 were trained on large quantities of\nhuman-generated text, we might ask to what extent their outputs reflect\npatterns of human thinking, both for correct and incorrect cases. The Erotetic\nTheory of Reason (ETR) provides a symbolic generative model of both human\nsuccess and failure in thinking, across propositional, quantified, and\nprobabilistic reasoning, as well as decision-making. We presented GPT-3,\nGPT-3.5, and GPT-4 with 61 central inference and judgment problems from a\nrecent book-length presentation of ETR, consisting of experimentally verified\ndata-points on human judgment and extrapolated data-points predicted by ETR,\nwith correct inference patterns as well as fallacies and framing effects (the\nETR61 benchmark). ETR61 includes classics like Wason's card task, illusory\ninferences, the decoy effect, and opportunity-cost neglect, among others. GPT-3\nshowed evidence of ETR-predicted outputs for 59% of these examples, rising to\n77% in GPT-3.5 and 75% in GPT-4. Remarkably, the production of human-like\nfallacious judgments increased from 18% in GPT-3 to 33% in GPT-3.5 and 34% in\nGPT-4. This suggests that larger and more advanced LLMs may develop a tendency\ntoward more human-like mistakes, as relevant thought patterns are inherent in\nhuman-produced training data. According to ETR, the same fundamental patterns\nare involved both in successful and unsuccessful ordinary reasoning, so that\nthe \"bad\" cases could paradoxically be learned from the \"good\" cases. We\nfurther present preliminary evidence that ETR-inspired prompt engineering could\nreduce instances of these mistakes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koralus_P/0/1/0/all/0/1\">Philipp Koralus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Mascianica_V/0/1/0/all/0/1\">Vincent Wang-Ma&#x15b;cianica</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Yes but.. Can ChatGPT Identify Entities in Historical Documents?. (arXiv:2303.17322v1 [cs.DL])","link":"http://arxiv.org/abs/2303.17322","description":"<p>Large language models (LLMs) have been leveraged for several years now,\nobtaining state-of-the-art performance in recognizing entities from modern\ndocuments. For the last few months, the conversational agent ChatGPT has\n\"prompted\" a lot of interest in the scientific community and public due to its\ncapacity of generating plausible-sounding answers. In this paper, we explore\nthis ability by probing it in the named entity recognition and classification\n(NERC) task in primary sources (e.g., historical newspapers and classical\ncommentaries) in a zero-shot manner and by comparing it with state-of-the-art\nLM-based systems. Our findings indicate several shortcomings in identifying\nentities in historical text that range from the consistency of entity\nannotation guidelines, entity complexity, and code-switching, to the\nspecificity of prompting. Moreover, as expected, the inaccessibility of\nhistorical archives to the public (and thus on the Internet) also impacts its\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Gallardo_C/0/1/0/all/0/1\">Carlos-Emiliano Gonz&#xe1;lez-Gallardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boros_E/0/1/0/all/0/1\">Emanuela Boros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girdhar_N/0/1/0/all/0/1\">Nancy Girdhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamdi_A/0/1/0/all/0/1\">Ahmed Hamdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_J/0/1/0/all/0/1\">Jose G. Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1\">Antoine Doucet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topics in the Haystack: Extracting and Evaluating Topics beyond Coherence. (arXiv:2303.17324v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17324","description":"<p>Extracting and identifying latent topics in large text corpora has gained\nincreasing importance in Natural Language Processing (NLP). Most models,\nwhether probabilistic models similar to Latent Dirichlet Allocation (LDA) or\nneural topic models, follow the same underlying approach of topic\ninterpretability and topic extraction. We propose a method that incorporates a\ndeeper understanding of both sentence and document themes, and goes beyond\nsimply analyzing word frequencies in the data. This allows our model to detect\nlatent topics that may include uncommon words or neologisms, as well as words\nnot present in the documents themselves. Additionally, we propose several new\nevaluation metrics based on intruder words and similarity measures in the\nsemantic space. We present correlation coefficients with human identification\nof intruder words and achieve near-human level results at the word-intrusion\ntask. We demonstrate the competitive performance of our method with a large\nbenchmark study, and achieve superior results compared to state-of-the-art\ntopic modeling and document clustering models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thielmann_A/0/1/0/all/0/1\">Anton Thielmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seifert_Q/0/1/0/all/0/1\">Quentin Seifert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reuter_A/0/1/0/all/0/1\">Arik Reuter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergherr_E/0/1/0/all/0/1\">Elisabeth Bergherr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safken_B/0/1/0/all/0/1\">Benjamin S&#xe4;fken</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A BERT-based Unsupervised Grammatical Error Correction Framework. (arXiv:2303.17367v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17367","description":"<p>Grammatical error correction (GEC) is a challenging task of natural language\nprocessing techniques. While more attempts are being made in this approach for\nuniversal languages like English or Chinese, relatively little work has been\ndone for low-resource languages for the lack of large annotated corpora. In\nlow-resource languages, the current unsupervised GEC based on language model\nscoring performs well. However, the pre-trained language model is still to be\nexplored in this context. This study proposes a BERT-based unsupervised GEC\nframework, where GEC is viewed as multi-class classification task. The\nframework contains three modules: data flow construction module, sentence\nperplexity scoring module, and error detecting and correcting module. We\npropose a novel scoring method for pseudo-perplexity to evaluate a sentence's\nprobable correctness and construct a Tagalog corpus for Tagalog GEC research.\nIt obtains competitive performance on the Tagalog corpus we construct and\nopen-source Indonesian corpus and it demonstrates that our framework is\ncomplementary to baseline method for low-resource GEC task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1\">Menglan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Aimin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research. (arXiv:2303.17395v1 [eess.AS])","link":"http://arxiv.org/abs/2303.17395","description":"<p>The advancement of audio-language (AL) multimodal learning tasks has been\nsignificant in recent years. However, researchers face challenges due to the\ncostly and time-consuming collection process of existing audio-language\ndatasets, which are limited in size. To address this data scarcity issue, we\nintroduce WavCaps, the first large-scale weakly-labelled audio captioning\ndataset, comprising approximately 400k audio clips with paired captions. We\nsourced audio clips and their raw descriptions from web sources and a sound\nevent detection dataset. However, the online-harvested raw descriptions are\nhighly noisy and unsuitable for direct use in tasks such as automated audio\ncaptioning. To overcome this issue, we propose a three-stage processing\npipeline for filtering noisy data and generating high-quality captions, where\nChatGPT, a large language model, is leveraged to filter and transform raw\ndescriptions automatically. We conduct a comprehensive analysis of the\ncharacteristics of WavCaps dataset and evaluate it on multiple downstream\naudio-language multimodal learning tasks. The systems trained on WavCaps\noutperform previous state-of-the-art (SOTA) models by a significant margin. Our\naspiration is for the WavCaps dataset we have proposed to facilitate research\nin audio-language multimodal learning and demonstrate the potential of\nutilizing ChatGPT to enhance academic research. Our dataset and codes are\navailable at https://github.com/XinhaoMei/WavCaps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mei_X/0/1/0/all/0/1\">Xinhao Mei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_C/0/1/0/all/0/1\">Chutong Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Haohe Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kong_Q/0/1/0/all/0/1\">Qiuqiang Kong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ko_T/0/1/0/all/0/1\">Tom Ko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_C/0/1/0/all/0/1\">Chengqi Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Plumbley_M/0/1/0/all/0/1\">Mark D. Plumbley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wenwu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medical Intervention Duration Estimation Using Language-enhanced Transformer Encoder with Medical Prompts. (arXiv:2303.17408v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17408","description":"<p>In recent years, estimating the duration of medical intervention based on\nelectronic health records (EHRs) has gained significant attention in the filed\nof clinical decision support. However, current models largely focus on\nstructured data, leaving out information from the unstructured clinical\nfree-text data. To address this, we present a novel language-enhanced\ntransformer-based framework, which projects all relevant clinical data\nmodalities (continuous, categorical, binary, and free-text features) into a\nharmonized language latent space using a pre-trained sentence encoder with the\nhelp of medical prompts. The proposed method enables the integration of\ninformation from different modalities within the cell transformer encoder and\nleads to more accurate duration estimation for medical intervention. Our\nexperimental results on both US-based (length of stay in ICU estimation) and\nAsian (surgical duration prediction) medical datasets demonstrate the\neffectiveness of our proposed framework, which outperforms tailored baseline\napproaches and exhibits robustness to data corruption in EHRs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Y/0/1/0/all/0/1\">Yucheng Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1\">Xiang Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_D/0/1/0/all/0/1\">Daniel J. Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdullah_H/0/1/0/all/0/1\">Hairil Rizal Abdullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1\">Mengling Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Cross-Cultural Alignment between ChatGPT and Human Societies: An Empirical Study. (arXiv:2303.17466v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17466","description":"<p>The recent release of ChatGPT has garnered widespread recognition for its\nexceptional ability to generate human-like responses in dialogue. Given its\nusage by users from various nations and its training on a vast multilingual\ncorpus that incorporates diverse cultural and societal norms, it is crucial to\nevaluate its effectiveness in cultural adaptation. In this paper, we\ninvestigate the underlying cultural background of ChatGPT by analyzing its\nresponses to questions designed to quantify human cultural differences. Our\nfindings suggest that, when prompted with American context, ChatGPT exhibits a\nstrong alignment with American culture, but it adapts less effectively to other\ncultural contexts. Furthermore, by using different prompts to probe the model,\nwe show that English prompts reduce the variance in model responses, flattening\nout cultural differences and biasing them towards American culture. This study\nprovides valuable insights into the cultural implications of ChatGPT and\nhighlights the necessity of greater diversity and cultural awareness in\nlanguage technologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Li Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seolhwa Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabello_L/0/1/0/all/0/1\">Laura Cabello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Min Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1\">Daniel Hershcovich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient distributed representations beyond negative sampling. (arXiv:2303.17475v1 [cs.LG])","link":"http://arxiv.org/abs/2303.17475","description":"<p>This article describes an efficient method to learn distributed\nrepresentations, also known as embeddings. This is accomplished minimizing an\nobjective function similar to the one introduced in the Word2Vec algorithm and\nlater adopted in several works. The optimization computational bottleneck is\nthe calculation of the softmax normalization constants for which a number of\noperations scaling quadratically with the sample size is required. This\ncomplexity is unsuited for large datasets and negative sampling is a popular\nworkaround, allowing one to obtain distributed representations in linear time\nwith respect to the sample size. Negative sampling consists, however, in a\nchange of the loss function and hence solves a different optimization problem\nfrom the one originally proposed. Our contribution is to show that the sotfmax\nnormalization constants can be estimated in linear time, allowing us to design\nan efficient optimization strategy to learn distributed representations. We\ntest our approximation on two popular applications related to word and node\nembeddings. The results evidence competing performance in terms of accuracy\nwith respect to negative sampling with a remarkably lower computational time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DallAmico_L/0/1/0/all/0/1\">Lorenzo Dall&#x27;Amico</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belliardo_E/0/1/0/all/0/1\">Enrico Maria Belliardo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models can Solve Computer Tasks. (arXiv:2303.17491v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17491","description":"<p>Agents capable of carrying out general tasks on a computer can improve\nefficiency and productivity by automating repetitive tasks and assisting in\ncomplex problem-solving. Ideally, such agents should be able to solve new\ncomputer tasks presented to them through natural language commands. However,\nprevious approaches to this problem require large amounts of expert\ndemonstrations and task-specific reward functions, both of which are\nimpractical for new tasks. In this work, we show that a pre-trained large\nlanguage model (LLM) agent can execute computer tasks guided by natural\nlanguage using a simple prompting scheme where the agent recursively criticizes\nand improves its output (RCI). The RCI approach significantly outperforms\nexisting LLM methods for automating computer tasks and surpasses supervised\nlearning (SL) and reinforcement learning (RL) approaches on the MiniWoB++\nbenchmark. RCI is competitive with the state-of-the-art SL+RL method, using\nonly a handful of demonstrations per task rather than tens of thousands, and\nwithout a task-specific reward function. Furthermore, we demonstrate RCI\nprompting's effectiveness in enhancing LLMs' reasoning abilities on a suite of\nnatural language reasoning tasks, outperforming chain of thought (CoT)\nprompting. We find that RCI combined with CoT performs better than either\nseparately.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Geunwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldi_P/0/1/0/all/0/1\">Pierre Baldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAleer_S/0/1/0/all/0/1\">Stephen McAleer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On pitfalls (and advantages) of sophisticated large language models. (arXiv:2303.17511v1 [cs.CY])","link":"http://arxiv.org/abs/2303.17511","description":"<p>Natural language processing based on large language models (LLMs) is a\nbooming field of AI research. After neural networks have proven to outperform\nhumans in games and practical domains based on pattern recognition, we might\nstand now at a road junction where artificial entities might eventually enter\nthe realm of human communication. However, this comes with serious risks. Due\nto the inherent limitations regarding the reliability of neural networks,\noverreliance on LLMs can have disruptive consequences. Since it will be\nincreasingly difficult to distinguish between human-written and\nmachine-generated text, one is confronted with new ethical challenges. This\nbegins with the no longer undoubtedly verifiable human authorship and continues\nwith various types of fraud, such as a new form of plagiarism. This also\nconcerns the violation of privacy rights, the possibility of circulating\ncounterfeits of humans, and, last but not least, it makes a massive spread of\nmisinformation possible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Strasser_A/0/1/0/all/0/1\">Anna Strasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Diproche CNL through autoformalization via GPT-3. (arXiv:2303.17513v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17513","description":"<p>The Diproche system is an automated proof checker for texts written in a\ncontrolled fragment of German, designed for didactical applications in classes\nintroducing students to proofs for the first time. The first version of the\nsystem used a controlled natural language for which a Prolog formalization\nroutine was written. In this paper, we explore the possibility of prompting\nlarge language models for autoformalization in the context of Diproche, with\nencouraging first results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carl_M/0/1/0/all/0/1\">Merlin Carl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hindi as a Second Language: Improving Visually Grounded Speech with Semantically Similar Samples. (arXiv:2303.17517v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17517","description":"<p>The objective of this work is to explore the learning of visually grounded\nspeech models (VGS) from multilingual perspective. Bilingual VGS models are\ngenerally trained with an equal number of spoken captions from both languages.\nHowever, in reality, there can be an imbalance among the languages for the\navailable spoken captions. Our key contribution in this work is to leverage the\npower of a high-resource language in a bilingual visually grounded speech model\nto improve the performance of a low-resource language. We introduce two methods\nto distill the knowledge of high-resource language into low-resource languages:\n(1) incorporating a strong pre-trained high-resource language encoder and (2)\nusing semantically similar spoken captions. Our experiments show that combining\nthese two approaches effectively enables the low-resource language to surpass\nthe performances of monolingual and bilingual counterparts for cross-modal\nretrieval tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ryu_H/0/1/0/all/0/1\">Hyeonggon Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senocak_A/0/1/0/all/0/1\">Arda Senocak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Joon Son Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Whose Opinions Do Language Models Reflect?. (arXiv:2303.17548v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17548","description":"<p>Language models (LMs) are increasingly being used in open-ended contexts,\nwhere the opinions reflected by LMs in response to subjective queries can have\na profound impact, both on user satisfaction, as well as shaping the views of\nsociety at large. In this work, we put forth a quantitative framework to\ninvestigate the opinions reflected by LMs -- by leveraging high-quality public\nopinion polls and their associated human responses. Using this framework, we\ncreate OpinionsQA, a new dataset for evaluating the alignment of LM opinions\nwith those of 60 US demographic groups over topics ranging from abortion to\nautomation. Across topics, we find substantial misalignment between the views\nreflected by current LMs and those of US demographic groups: on par with the\nDemocrat-Republican divide on climate change. Notably, this misalignment\npersists even after explicitly steering the LMs towards particular demographic\ngroups. Our analysis not only confirms prior observations about the\nleft-leaning tendencies of some human feedback-tuned LMs, but also surfaces\ngroups whose opinions are poorly reflected by current LMs (e.g., 65+ and\nwidowed individuals). Our code and data are available at\nhttps://github.com/tatsu-lab/opinions_qa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santurkar_S/0/1/0/all/0/1\">Shibani Santurkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1\">Esin Durmus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ladhak_F/0/1/0/all/0/1\">Faisal Ladhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Cinoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recognition, recall, and retention of few-shot memories in large language models. (arXiv:2303.17557v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17557","description":"<p>The training of modern large language models (LLMs) takes place in a regime\nwhere most training examples are seen only a few times by the model during the\ncourse of training. What does a model remember about such examples seen only a\nfew times during training and how long does that memory persist in the face of\ncontinuous training with new examples? Here, we investigate these questions\nthrough simple recognition, recall, and retention experiments with LLMs. In\nrecognition experiments, we ask if the model can distinguish the seen example\nfrom a novel example; in recall experiments, we ask if the model can correctly\nrecall the seen example when cued by a part of it; and in retention\nexperiments, we periodically probe the model's memory for the original examples\nas the model is trained continuously with new examples. We find that a single\nexposure is generally sufficient for a model to achieve near perfect accuracy\neven in very challenging recognition experiments. We estimate that the\nrecognition performance of even small language models easily exceeds human\nrecognition performance reported in similar experiments with humans (Shepard,\n1967). Achieving near perfect recall takes more exposures, but most models can\ndo it in just 3 exposures. The flip side of this remarkable capacity for fast\nlearning is that precise memories are quickly overwritten: recall performance\nfor the original examples drops steeply over the first 10 training updates with\nnew examples, followed by a more gradual decline. Even after 100K updates,\nhowever, some of the original examples are still recalled near perfectly. A\nqualitatively similar retention pattern has been observed in human long-term\nmemory retention studies before (Bahrick, 1984). Finally, recognition is much\nmore robust to interference than recall and memory for natural language\nsentences is generally superior to memory for stimuli without structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orhan_A/0/1/0/all/0/1\">A. Emin Orhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BloombergGPT: A Large Language Model for Finance. (arXiv:2303.17564v1 [cs.LG])","link":"http://arxiv.org/abs/2303.17564","description":"<p>The use of NLP in the realm of financial technology is broad and complex,\nwith applications ranging from sentiment analysis and named entity recognition\nto question answering. Large Language Models (LLMs) have been shown to be\neffective on a variety of tasks; however, no LLM specialized for the financial\ndomain has been reported in literature. In this work, we present BloombergGPT,\na 50 billion parameter language model that is trained on a wide range of\nfinancial data. We construct a 363 billion token dataset based on Bloomberg's\nextensive data sources, perhaps the largest domain-specific dataset yet,\naugmented with 345 billion tokens from general purpose datasets. We validate\nBloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite\nof internal benchmarks that most accurately reflect our intended usage. Our\nmixed dataset training leads to a model that outperforms existing models on\nfinancial tasks by significant margins without sacrificing performance on\ngeneral LLM benchmarks. Additionally, we explain our modeling choices, training\nprocess, and evaluation methodology. As a next step, we plan to release\ntraining logs (Chronicles) detailing our experience in training BloombergGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shijie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irsoy_O/0/1/0/all/0/1\">Ozan Irsoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Steven Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabravolski_V/0/1/0/all/0/1\">Vadim Dabravolski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dredze_M/0/1/0/all/0/1\">Mark Dredze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kambadur_P/0/1/0/all/0/1\">Prabhanjan Kambadur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_D/0/1/0/all/0/1\">David Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mann_G/0/1/0/all/0/1\">Gideon Mann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Elastic Weight Removal for Faithful and Abstractive Dialogue Generation. (arXiv:2303.17574v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17574","description":"<p>Ideally, dialogue systems should generate responses that are faithful to the\nknowledge contained in relevant documents. However, many models generate\nhallucinated responses instead that contradict it or contain unverifiable\ninformation. To mitigate such undesirable behaviour, it has been proposed to\nfine-tune a `negative expert' on negative examples and subtract its parameters\nfrom those of a pre-trained model. However, intuitively, this does not take\ninto account that some parameters are more responsible than others in causing\nhallucinations. Thus, we propose to weigh their individual importance via (an\napproximation of) the Fisher Information matrix, which measures the uncertainty\nof their estimate. We call this method Elastic Weight Removal (EWR). We\nevaluate our method -- using different variants of Flan-T5 as a backbone\nlanguage model -- on multiple datasets for information-seeking dialogue\ngeneration and compare our method with state-of-the-art techniques for\nfaithfulness, such as CTRL, Quark, DExperts, and Noisy Channel reranking.\nExtensive automatic and human evaluation shows that EWR systematically\nincreases faithfulness at minor costs in terms of other metrics. However, we\nnotice that only discouraging hallucinations may increase extractiveness, i.e.\nshallow copy-pasting of document spans, which can be undesirable. Hence, as a\nsecond main contribution, we show that our method can be extended to\nsimultaneously discourage hallucinations and extractive responses. We publicly\nrelease the code for reproducing EWR and all baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Daheim_N/0/1/0/all/0/1\">Nico Daheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziri_N/0/1/0/all/0/1\">Nouha Dziri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo M. Ponti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Image-Text Matching Improves Retrieval-based Chest X-Ray Report Generation. (arXiv:2303.17579v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17579","description":"<p>Automated generation of clinically accurate radiology reports can improve\npatient care. Previous report generation methods that rely on image captioning\nmodels often generate incoherent and incorrect text due to their lack of\nrelevant domain knowledge, while retrieval-based attempts frequently retrieve\nreports that are irrelevant to the input image. In this work, we propose\nContrastive X-Ray REport Match (X-REM), a novel retrieval-based radiology\nreport generation module that uses an image-text matching score to measure the\nsimilarity of a chest X-ray image and radiology report for report retrieval. We\nobserve that computing the image-text matching score with a language-image\nmodel can effectively capture the fine-grained interaction between image and\ntext that is often lost when using cosine similarity. X-REM outperforms\nmultiple prior radiology report generation modules in terms of both natural\nlanguage and clinical metrics. Human evaluation of the generated reports\nsuggests that X-REM increased the number of zero-error reports and decreased\nthe average error severity compared to the baseline retrieval approach. Our\ncode is available at: https://github.com/rajpurkarlab/X-REM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jaehwan Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_K/0/1/0/all/0/1\">Katherine Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Andrew Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartung_S/0/1/0/all/0/1\">Sina Hartung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behzadi_F/0/1/0/all/0/1\">Fardad Behzadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calle_J/0/1/0/all/0/1\">Juan Calle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osayande_D/0/1/0/all/0/1\">David Osayande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohlen_M/0/1/0/all/0/1\">Michael Pohlen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adithan_S/0/1/0/all/0/1\">Subathra Adithan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. (arXiv:2303.17580v1 [cs.CL])","link":"http://arxiv.org/abs/2303.17580","description":"<p>Solving complicated AI tasks with different domains and modalities is a key\nstep toward artificial general intelligence (AGI). While there are abundant AI\nmodels available for different domains and modalities, they cannot handle\ncomplicated AI tasks. Considering large language models (LLMs) have exhibited\nexceptional ability in language understanding, generation, interaction, and\nreasoning, we advocate that LLMs could act as a controller to manage existing\nAI models to solve complicated AI tasks and language could be a generic\ninterface to empower this. Based on this philosophy, we present HuggingGPT, a\nsystem that leverages LLMs (e.g., ChatGPT) to connect various AI models in\nmachine learning communities (e.g., HuggingFace) to solve AI tasks.\nSpecifically, we use ChatGPT to conduct task planning when receiving a user\nrequest, select models according to their function descriptions available in\nHuggingFace, execute each subtask with the selected AI model, and summarize the\nresponse according to the execution results. By leveraging the strong language\ncapability of ChatGPT and abundant AI models in HuggingFace, HuggingGPT is able\nto cover numerous sophisticated AI tasks in different modalities and domains\nand achieve impressive results in language, vision, speech, and other\nchallenging tasks, which paves a new way towards AGI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yongliang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weiming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Going Beyond Nouns With Vision & Language Models Using Synthetic Data. (arXiv:2303.17590v1 [cs.CV])","link":"http://arxiv.org/abs/2303.17590","description":"<p>Large-scale pre-trained Vision &amp; Language (VL) models have shown remarkable\nperformance in many applications, enabling replacing a fixed set of supported\nclasses with zero-shot open vocabulary reasoning over (almost arbitrary)\nnatural language prompts. However, recent works have uncovered a fundamental\nweakness of these models. For example, their difficulty to understand Visual\nLanguage Concepts (VLC) that go 'beyond nouns' such as the meaning of\nnon-object words (e.g., attributes, actions, relations, states, etc.), or\ndifficulty in performing compositional reasoning such as understanding the\nsignificance of the order of the words in a sentence. In this work, we\ninvestigate to which extent purely synthetic data could be leveraged to teach\nthese models to overcome such shortcomings without compromising their zero-shot\ncapabilities. We contribute Synthetic Visual Concepts (SyViC) - a million-scale\nsynthetic dataset and data generation codebase allowing to generate additional\nsuitable data to improve VLC understanding and compositional reasoning of VL\nmodels. Additionally, we propose a general VL finetuning strategy for\neffectively leveraging SyViC towards achieving these improvements. Our\nextensive experiments and ablations on VL-Checklist, Winoground, and ARO\nbenchmarks demonstrate that it is possible to adapt strong pre-trained VL\nmodels with synthetic data significantly enhancing their VLC understanding\n(e.g. by 9.9% on ARO and 4.3% on VL-Checklist) with under 1% drop in their\nzero-shot accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cascante_Bonilla_P/0/1/0/all/0/1\">Paola Cascante-Bonilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shehada_K/0/1/0/all/0/1\">Khaled Shehada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">James Seale Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doveh_S/0/1/0/all/0/1\">Sivan Doveh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_G/0/1/0/all/0/1\">G&#xfc;l Varol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliva_A/0/1/0/all/0/1\">Aude Oliva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1\">Vicente Ordonez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1\">Leonid Karlinsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Continual Learning with Global Prototypes: Counteracting Negative Representation Drift. (arXiv:2205.12186v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12186","description":"<p>Continual learning (CL) aims to learn a sequence of tasks over time, with\ndata distributions shifting from one task to another. When training on new task\ndata, data representations from old tasks may drift. Some negative\nrepresentation drift can result in catastrophic forgetting, by causing the\nlocally learned class prototypes and data representations to correlate poorly\nacross tasks. To mitigate such representation drift, we propose a method that\nfinds global prototypes to guide the learning, and learns data representations\nwith the regularization of the self-supervised information. Specifically, for\nNLP tasks, we formulate each task in a masked language modeling style, and\nlearn the task via a neighbor attention mechanism over a pre-trained language\nmodel. Experimental results show that our proposed method can learn fairly\nconsistent representations with less representation drift, and significantly\nreduce catastrophic forgetting in CL without resampling data from past tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xueying Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jinghuan Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yifan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained Image Captioning with CLIP Reward. (arXiv:2205.13115v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.13115","description":"<p>Modern image captioning models are usually trained with text similarity\nobjectives. However, since reference captions in public datasets often describe\nthe most salient common objects, models trained with text similarity objectives\ntend to ignore specific and detailed aspects of an image that distinguish it\nfrom others. Toward more descriptive and distinctive caption generation, we\npropose using CLIP, a multimodal encoder trained on huge image-text pairs from\nweb, to calculate multimodal similarity and use it as a reward function. We\nalso propose a simple finetuning strategy of the CLIP text encoder to improve\ngrammar that does not require extra text annotation. This completely eliminates\nthe need for reference captions during the reward computation. To\ncomprehensively evaluate descriptive captions, we introduce FineCapEval, a new\ndataset for caption evaluation with fine-grained criteria: overall, background,\nobject, relations. In our experiments on text-to-image retrieval and\nFineCapEval, the proposed CLIP-guided model generates more distinctive captions\nthan the CIDEr-optimized model. We also show that our unsupervised grammar\nfinetuning of the CLIP text encoder alleviates the degeneration problem of the\nnaive CLIP reward. Lastly, we show human analysis where the annotators strongly\nprefer the CLIP reward to the CIDEr and MLE objectives according to various\ncriteria. Code and Data: https://github.com/j-min/CLIP-Caption-Reward\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_A/0/1/0/all/0/1\">Ajinkya Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition. (arXiv:2206.08317v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2206.08317","description":"<p>Transformers have recently dominated the ASR field. Although able to yield\ngood performance, they involve an autoregressive (AR) decoder to generate\ntokens one by one, which is computationally inefficient. To speed up inference,\nnon-autoregressive (NAR) methods, e.g. single-step NAR, were designed, to\nenable parallel generation. However, due to an independence assumption within\nthe output tokens, performance of single-step NAR is inferior to that of AR\nmodels, especially with a large-scale corpus. There are two challenges to\nimproving single-step NAR: Firstly to accurately predict the number of output\ntokens and extract hidden variables; secondly, to enhance modeling of\ninterdependence between output tokens. To tackle both challenges, we propose a\nfast and accurate parallel transformer, termed Paraformer. This utilizes a\ncontinuous integrate-and-fire based predictor to predict the number of tokens\nand generate hidden variables. A glancing language model (GLM) sampler then\ngenerates semantic embeddings to enhance the NAR decoder's ability to model\ncontext interdependence. Finally, we design a strategy to generate negative\nsamples for minimum word error rate training to further improve performance.\nExperiments using the public AISHELL-1, AISHELL-2 benchmark, and an\nindustrial-level 20,000 hour task demonstrate that the proposed Paraformer can\nattain comparable performance to the state-of-the-art AR transformer, with more\nthan 10x speedup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhifu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McLoughlin_I/0/1/0/all/0/1\">Ian McLoughlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhijie Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence. (arXiv:2209.02970v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.02970","description":"<p>Nowadays, foundation models become one of fundamental infrastructures in\nartificial intelligence, paving ways to the general intelligence. However, the\nreality presents two urgent challenges: existing foundation models are\ndominated by the English-language community; users are often given limited\nresources and thus cannot always use foundation models. To support the\ndevelopment of the Chinese-language community, we introduce an open-source\nproject, called Fengshenbang, which leads by the research center for Cognitive\nComputing and Natural Language (CCNL). Our project has comprehensive\ncapabilities, including large pre-trained models, user-friendly APIs,\nbenchmarks, datasets, and others. We wrap all these in three sub-projects: the\nFengshenbang Model, the Fengshen Framework, and the Fengshen Benchmark. An\nopen-source roadmap, Fengshenbang, aims to re-evaluate the open-source\ncommunity of Chinese pre-trained large-scale models, prompting the development\nof the entire Chinese large-scale model community. We also want to build a\nuser-centered open-source ecosystem to allow individuals to access the desired\nmodels to match their computing resources. Furthermore, we invite companies,\ncolleges, and research institutions to collaborate with us to build the\nlarge-scale open-source model-based ecosystem. We hope that this project will\nbe the foundation of Chinese cognitive intelligence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_R/0/1/0/all/0/1\">Ruyi Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Ping Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoqun Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junqing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1\">Jianheng Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiayu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanghan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Junyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Ting Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_K/0/1/0/all/0/1\">Kunhao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaojun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhongshen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chongpei Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-Family Adapters for Low-Resource Multilingual Neural Machine Translation. (arXiv:2209.15236v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.15236","description":"<p>Large multilingual models trained with self-supervision achieve\nstate-of-the-art results in a wide range of natural language processing tasks.\nSelf-supervised pretrained models are often fine-tuned on parallel data from\none or multiple language pairs for machine translation. Multilingual\nfine-tuning improves performance on low-resource languages but requires\nmodifying the entire model and can be prohibitively expensive. Training a new\nadapter on each language pair or training a single adapter on all language\npairs without updating the pretrained model has been proposed as a\nparameter-efficient alternative. However, the former does not permit any\nsharing between languages, while the latter shares parameters for all languages\nand is susceptible to negative interference. In this paper, we propose training\nlanguage-family adapters on top of mBART-50 to facilitate cross-lingual\ntransfer. Our approach outperforms related baselines, yielding higher\ntranslation scores on average when translating from English to 17 different\nlow-resource languages. We also show that language-family adapters provide an\neffective method to translate to languages unseen during pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chronopoulou_A/0/1/0/all/0/1\">Alexandra Chronopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stojanovski_D/0/1/0/all/0/1\">Dario Stojanovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Temporal Modelling of Clinical Depression through Social Media Text. (arXiv:2211.07717v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07717","description":"<p>We describe the development of a model to detect user-level clinical\ndepression based on a user's temporal social media posts. Our model uses a\nDepression Symptoms Detection (DSD) classifier, which is trained on the largest\nexisting samples of clinician annotated tweets for clinical depression\nsymptoms. We subsequently use our DSD model to extract clinically relevant\nfeatures, e.g., depression scores and their consequent temporal patterns, as\nwell as user posting activity patterns, e.g., quantifying their ``no activity''\nor ``silence.'' Furthermore, to evaluate the efficacy of these extracted\nfeatures, we create three kinds of datasets including a test dataset, from two\nexisting well-known benchmark datasets for user-level depression detection. We\nthen provide accuracy measures based on single features, baseline features and\nfeature ablation tests, at several different levels of temporal granularity.\nThe relevant data distributions and clinical depression detection related\nsettings can be exploited to draw a complete picture of the impact of different\nfeatures across our created datasets. Finally, we show that, in general, only\nsemantic oriented representation models perform well. However, clinical\nfeatures may enhance overall performance provided that the training and testing\ndistribution is similar, and there is more data in a user's timeline. The\nconsequence is that the predictive capability of depression scores increase\nsignificantly while used in a more sensitive clinical depression detection\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1\">Nawshad Farruque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1\">Randy Goebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivapalan_S/0/1/0/all/0/1\">Sudhakar Sivapalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar R. Za&#xef;ane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models. (arXiv:2212.04088v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2212.04088","description":"<p>This study focuses on using large language models (LLMs) as a planner for\nembodied agents that can follow natural language instructions to complete\ncomplex tasks in a visually-perceived environment. The high data cost and poor\nsample efficiency of existing methods hinders the development of versatile\nagents that are capable of many tasks and can learn new tasks quickly. In this\nwork, we propose a novel method, LLM-Planner, that harnesses the power of large\nlanguage models to do few-shot planning for embodied agents. We further propose\na simple but effective way to enhance LLMs with physical grounding to generate\nand update plans that are grounded in the current environment. Experiments on\nthe ALFRED dataset show that our method can achieve very competitive few-shot\nperformance: Despite using less than 0.5% of paired training data, LLM-Planner\nachieves competitive performance with recent baselines that are trained using\nthe full training data. Existing methods can barely complete any task\nsuccessfully under the same few-shot setting. Our work opens the door for\ndeveloping versatile and sample-efficient embodied agents that can quickly\nlearn many tasks. Website: https://dki-lab.github.io/LLM-Planner\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chan Hee Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiaman Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Washington_C/0/1/0/all/0/1\">Clayton Washington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadler_B/0/1/0/all/0/1\">Brian M. Sadler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TempCLR: Temporal Alignment Representation with Contrastive Learning. (arXiv:2212.13738v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2212.13738","description":"<p>Video representation learning has been successful in video-text pre-training\nfor zero-shot transfer, where each sentence is trained to be close to the\npaired video clips in a common feature space. For long videos, given a\nparagraph of description where the sentences describe different segments of the\nvideo, by matching all sentence-clip pairs, the paragraph and the full video\nare aligned implicitly. However, such unit-level comparison may ignore global\ntemporal context, which inevitably limits the generalization ability. In this\npaper, we propose a contrastive learning framework TempCLR to compare the full\nvideo and the paragraph explicitly. As the video/paragraph is formulated as a\nsequence of clips/sentences, under the constraint of their temporal order, we\nuse dynamic time warping to compute the minimum cumulative cost over\nsentence-clip pairs as the sequence-level distance. To explore the temporal\ndynamics, we break the consistency of temporal succession by shuffling video\nclips w.r.t. temporal granularity. Then, we obtain the representations for\nclips/sentences, which perceive the temporal information and thus facilitate\nthe sequence alignment. In addition to pre-training on the video and paragraph,\nour approach can also generalize on the matching between video instances. We\nevaluate our approach on video retrieval, action step localization, and\nfew-shot action recognition, and achieve consistent performance gain over all\nthree tasks. Detailed ablation studies are provided to justify the approach\ndesign.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuncong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiawei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shiyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1\">Guangxing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Correct answers\" from the psychology of artificial intelligence. (arXiv:2302.07267v4 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2302.07267","description":"<p>We re-replicate 14 psychology studies from the Many Labs 2 replication\nproject (Klein et al., 2018) with OpenAI's text-davinci-003 model, colloquially\nknown as GPT3.5. Among the eight studies we could analyse, our GPT sample\nreplicated 37.5% of the original results and 37.5% of the Many Labs 2 results.\nWe could not analyse the remaining six studies, due to an unexpected phenomenon\nwe call the \"correct answer\" effect. Different runs of GPT3.5 answered nuanced\nquestions probing political orientation, economic preference, judgement, and\nmoral philosophy with zero or near-zero variation in responses: with the\nsupposedly \"correct answer.\" Most but not all of these \"correct answers\" were\nrobust to changing the order of answer choices. One exception occurred in the\nMoral Foundations Theory survey (Graham et al., 2009), for which GPT3.5 almost\nalways identified as a conservative in the original condition (N=1,030, 99.6%)\nand as a liberal in the reverse-order condition (N=1,030, 99.3%). GPT3.5's\nresponses to subsequent questions revealed post-hoc rationalisation; there was\na relative bias in the direction of its previously reported political\norientation. But both self-reported GPT conservatives and self-reported GPT\nliberals revealed right-leaning Moral Foundations, although the right-leaning\nbias of self-reported GPT liberals was weaker. We hypothesise that this pattern\nwas learned from a conservative bias in the model's largely Internet-based\ntraining data. Since AI models of the future may be trained on much of the same\nInternet data as GPT3.5, our results raise concerns that a hypothetical AI-led\nfuture may be subject to a diminished diversity of thought.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_P/0/1/0/all/0/1\">Peter S. Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoenegger_P/0/1/0/all/0/1\">Philipp Schoenegger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chongyang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT. (arXiv:2302.09419v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2302.09419","description":"<p>Pretrained Foundation Models (PFMs) are regarded as the foundation for\nvarious downstream tasks with different data modalities. A PFM (e.g., BERT,\nChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable\nparameter initialization for a wide range of downstream applications. BERT\nlearns bidirectional encoder representations from Transformers, which are\ntrained on large datasets as contextual language models. Similarly, the\ngenerative pretrained transformer (GPT) method employs Transformers as the\nfeature extractor and is trained using an autoregressive paradigm on large\ndatasets. Recently, ChatGPT shows promising success on large language models,\nwhich applies an autoregressive language model with zero shot or few shot\nprompting. The remarkable achievements of PFM have brought significant\nbreakthroughs to various fields of AI. Numerous studies have proposed different\nmethods, raising the demand for an updated survey. This study provides a\ncomprehensive review of recent research advancements, challenges, and\nopportunities for PFMs in text, image, graph, as well as other data modalities.\nThe review covers the basic components and existing pretraining methods used in\nnatural language processing, computer vision, and graph learning. Additionally,\nit explores advanced PFMs used for different data modalities and unified PFMs\nthat consider data quality and quantity. The review also discusses research\nrelated to the fundamentals of PFMs, such as model efficiency and compression,\nsecurity, and privacy. Finally, the study provides key implications, future\nresearch directions, challenges, and open problems in the field of PFMs.\nOverall, this survey aims to shed light on the research of the PFMs on\nscalability, security, logical reasoning ability, cross-domain learning\nability, and the user-friendly interactive ability for artificial general\nintelligence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Ce Zhou</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangjing Wang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Ji_C/0/1/0/all/0/1\">Cheng Ji</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qiben Yan</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jia Wu</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a> (5), <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengtao Xie</a> (6), <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a> (7), <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a> (8), <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a> (9), <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a> (3) ((1) Michigan State University, (2) Beihang University, (3) Lehigh University, (4) Macquarie University, (5) Nanyang Technological University, (6) University of California San Diego, (7) Salesforce AI Research, (8) Duke University, (9) University of Illinois at Chicago)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Text Structuralization with Instruction-tuned Language Models. (arXiv:2303.14956v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.14956","description":"<p>Text structuralization is one of the important fields of natural language\nprocessing (NLP) consists of information extraction (IE) and structure\nformalization. However, current studies of text structuralization suffer from a\nshortage of manually annotated high-quality datasets from different domains and\nlanguages, which require specialized professional knowledge. In addition, most\nIE methods are designed for a specific type of structured data, e.g., entities,\nrelations, and events, making them hard to generalize to others. In this work,\nwe propose a simple and efficient approach to instruct large language model\n(LLM) to extract a variety of structures from texts. More concretely, we add a\nprefix and a suffix instruction to indicate the desired IE task and structure\ntype, respectively, before feeding the text into a LLM. Experiments on two LLMs\nshow that this approach can enable language models to perform comparable with\nother state-of-the-art methods on datasets of a variety of languages and\nknowledge, and can generalize to other IE sub-tasks via changing the content of\ninstruction. Another benefit of our approach is that it can help researchers to\nbuild datasets in low-source and domain-specific scenarios, e.g., fields in\nfinance and law, with low cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_X/0/1/0/all/0/1\">Xuanfan Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huayang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks. (arXiv:2303.16839v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2303.16839","description":"<p>The development of language models have moved from encoder-decoder to\ndecoder-only designs. In addition, the common knowledge has it that the two\nmost popular multimodal tasks, the generative and contrastive tasks, tend to\nconflict with one another, are hard to accommodate in one architecture, and\nfurther need complex adaptations for downstream tasks. We propose a novel\nparadigm of training with a decoder-only model for multimodal tasks, which is\nsurprisingly effective in jointly learning of these disparate vision-language\ntasks. This is done with a simple model, called MaMMUT. It consists of a single\nvision encoder and a text decoder, and is able to accommodate contrastive and\ngenerative learning by a novel two-pass approach on the text decoder. We\ndemonstrate that joint learning of these diverse objectives is simple,\neffective, and maximizes the weight-sharing of the model across these tasks.\nFurthermore, the same architecture enables straightforward extensions to\nopen-vocabulary object detection and video-language tasks. The model tackles a\ndiverse range of tasks, while being modest in capacity. Our model achieves the\nstate of the art on image-text and text-image retrieval, video question\nanswering and open-vocabulary detection tasks, outperforming much larger and\nmore extensively trained foundational models. It shows very competitive results\non VQA and Video Captioning, especially considering its capacity. Ablations\nconfirm the flexibility and advantages of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuo_W/0/1/0/all/0/1\">Weicheng Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piergiovanni_A/0/1/0/all/0/1\">AJ Piergiovanni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dahun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caine_B/0/1/0/all/0/1\">Ben Caine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogale_A/0/1/0/all/0/1\">Abhijit Ogale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Claire Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelova_A/0/1/0/all/0/1\">Anelia Angelova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-03-30T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}