{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-03-16T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"MEDBERT.de: A Comprehensive German BERT Model for the Medical Domain. (arXiv:2303.08179v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08179","description":"<p>This paper presents medBERT.de, a pre-trained German BERT model specifically\ndesigned for the German medical domain. The model has been trained on a large\ncorpus of 4.7 Million German medical documents and has been shown to achieve\nnew state-of-the-art performance on eight different medical benchmarks covering\na wide range of disciplines and medical document types. In addition to\nevaluating the overall performance of the model, this paper also conducts a\nmore in-depth analysis of its capabilities. We investigate the impact of data\ndeduplication on the model's performance, as well as the potential benefits of\nusing more efficient tokenization methods. Our results indicate that\ndomain-specific models such as medBERT.de are particularly useful for longer\ntexts, and that deduplication of training data does not necessarily lead to\nimproved performance. Furthermore, we found that efficient tokenization plays\nonly a minor role in improving model performance, and attribute most of the\nimproved performance to the large amount of training data. To encourage further\nresearch, the pre-trained model weights and new benchmarks based on\nradiological data are made publicly available for use by the scientific\ncommunity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bressem_K/0/1/0/all/0/1\">Keno K. Bressem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papaioannou_J/0/1/0/all/0/1\">Jens-Michalis Papaioannou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grundmann_P/0/1/0/all/0/1\">Paul Grundmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borchert_F/0/1/0/all/0/1\">Florian Borchert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_L/0/1/0/all/0/1\">Lisa C. Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Leonhard Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_F/0/1/0/all/0/1\">Felix Busch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lina Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loyen_J/0/1/0/all/0/1\">Jan P. Loyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niehues_S/0/1/0/all/0/1\">Stefan M. Niehues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augustin_M/0/1/0/all/0/1\">Moritz Augustin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosser_L/0/1/0/all/0/1\">Lennart Grosser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1\">Marcus R. Makowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aerts_H/0/1/0/all/0/1\">Hugo JWL. Aerts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loser_A/0/1/0/all/0/1\">Alexander L&#xf6;ser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NL4Opt Competition: Formulating Optimization Problems Based on Their Natural Language Descriptions. (arXiv:2303.08233v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08233","description":"<p>The Natural Language for Optimization (NL4Opt) Competition was created to\ninvestigate methods of extracting the meaning and formulation of an\noptimization problem based on its text description. Specifically, the goal of\nthe competition is to increase the accessibility and usability of optimization\nsolvers by allowing non-experts to interface with them using natural language.\nWe separate this challenging goal into two sub-tasks: (1) recognize and label\nthe semantic entities that correspond to the components of the optimization\nproblem; (2) generate a meaning representation (i.e., a logical form) of the\nproblem from its detected problem entities. The first task aims to reduce\nambiguity by detecting and tagging the entities of the optimization problems.\nThe second task creates an intermediate representation of the linear\nprogramming (LP) problem that is converted into a format that can be used by\ncommercial solvers. In this report, we present the LP word problem dataset and\nshared tasks for the NeurIPS 2022 competition. Furthermore, we present the\nwinning solutions. Through this competition, we hope to bring interest towards\nthe development of novel machine learning applications and datasets for\noptimization modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramamonjison_R/0/1/0/all/0/1\">Rindranirina Ramamonjison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Timothy T. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Raymond Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haley Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1\">Giuseppe Carenini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaddar_B/0/1/0/all/0/1\">Bissan Ghaddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shiqi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mostajabdaveh_M/0/1/0/all/0/1\">Mahdi Mostajabdaveh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1\">Amin Banitalebi-Dehkordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zirui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextualized Medication Information Extraction Using Transformer-based Deep Learning Architectures. (arXiv:2303.08259v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08259","description":"<p>Objective: To develop a natural language processing (NLP) system to extract\nmedications and contextual information that help understand drug changes. This\nproject is part of the 2022 n2c2 challenge.\n</p>\n<p>Materials and methods: We developed NLP systems for medication mention\nextraction, event classification (indicating medication changes discussed or\nnot), and context classification to classify medication changes context into 5\northogonal dimensions related to drug changes. We explored 6 state-of-the-art\npretrained transformer models for the three subtasks, including GatorTron, a\nlarge language model pretrained using &gt;90 billion words of text (including &gt;80\nbillion words from &gt;290 million clinical notes identified at the University of\nFlorida Health). We evaluated our NLP systems using annotated data and\nevaluation scripts provided by the 2022 n2c2 organizers.\n</p>\n<p>Results:Our GatorTron models achieved the best F1-scores of 0.9828 for\nmedication extraction (ranked 3rd), 0.9379 for event classification (ranked\n2nd), and the best micro-average accuracy of 0.9126 for context classification.\nGatorTron outperformed existing transformer models pretrained using smaller\ngeneral English text and clinical text corpora, indicating the advantage of\nlarge language models.\n</p>\n<p>Conclusion: This study demonstrated the advantage of using large transformer\nmodels for contextual medication information extraction from clinical\nnarratives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Aokun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zehao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clinical Concept and Relation Extraction Using Prompt-based Machine Reading Comprehension. (arXiv:2303.08262v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08262","description":"<p>Objective: To develop a natural language processing system that solves both\nclinical concept extraction and relation extraction in a unified prompt-based\nmachine reading comprehension (MRC) architecture with good generalizability for\ncross-institution applications.\n</p>\n<p>Methods: We formulate both clinical concept extraction and relation\nextraction using a unified prompt-based MRC architecture and explore\nstate-of-the-art transformer models. We compare our MRC models with existing\ndeep learning models for concept extraction and end-to-end relation extraction\nusing two benchmark datasets developed by the 2018 National NLP Clinical\nChallenges (n2c2) challenge (medications and adverse drug events) and the 2022\nn2c2 challenge (relations of social determinants of health [SDoH]). We also\nevaluate the transfer learning ability of the proposed MRC models in a\ncross-institution setting. We perform error analyses and examine how different\nprompting strategies affect the performance of MRC models.\n</p>\n<p>Results and Conclusion: The proposed MRC models achieve state-of-the-art\nperformance for clinical concept and relation extraction on the two benchmark\ndatasets, outperforming previous non-MRC transformer models. GatorTron-MRC\nachieves the best strict and lenient F1-scores for concept extraction,\noutperforming previous deep learning models on the two datasets by 1%~3% and\n0.7%~1.3%, respectively. For end-to-end relation extraction, GatorTron-MRC and\nBERT-MIMIC-MRC achieve the best F1-scores, outperforming previous deep learning\nmodels by 0.9%~2.4% and 10%-11%, respectively. For cross-institution\nevaluation, GatorTron-MRC outperforms traditional GatorTron by 6.4% and 16% for\nthe two datasets, respectively. The proposed method is better at handling\nnested/overlapped concepts, extracting relations, and has good portability for\ncross-institute applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1\">Cheng Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zehao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hogan_W/0/1/0/all/0/1\">William R. Hogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chat with the Environment: Interactive Multimodal Perception using Large Language Models. (arXiv:2303.08268v1 [cs.RO])","link":"http://arxiv.org/abs/2303.08268","description":"<p>Programming robot behaviour in a complex world faces challenges on multiple\nlevels, from dextrous low-level skills to high-level planning and reasoning.\nRecent pre-trained Large Language Models (LLMs) have shown remarkable reasoning\nability in zero-shot robotic planning. However, it remains challenging to\nground LLMs in multimodal sensory input and continuous action output, while\nenabling a robot to interact with its environment and acquire novel information\nas its policies unfold. We develop a robot interaction scenario with a\npartially observable state, which necessitates a robot to decide on a range of\nepistemic actions in order to sample sensory information among multiple\nmodalities, before being able to execute the task correctly. An interactive\nperception framework is therefore proposed with an LLM as its backbone, whose\nability is exploited to instruct epistemic actions and to reason over the\nresulting multimodal sensations (vision, sound, haptics, proprioception), as\nwell as to plan an entire task execution based on the interactively acquired\ninformation. Our study demonstrates that LLMs can provide high-level planning\nand reasoning skills and control interactive robot behaviour in a multimodal\nenvironment, while multimodal modules with the context of the environmental\nstate help ground the LLMs and extend their processing ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xufeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengdi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1\">Cornelius Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hafez_M/0/1/0/all/0/1\">Muhammad Burhan Hafez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-likelihood relationship in transformers. (arXiv:2303.08288v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08288","description":"<p>We analyze how large language models (LLMs) represent out-of-context words,\ninvestigating their reliance on the given context to capture their semantics.\nOur likelihood-guided text perturbations reveal a correlation between token\nlikelihood and attention values in transformer-based language models. Extensive\nexperiments reveal that unexpected tokens cause the model to attend less to the\ninformation coming from themselves to compute their representations,\nparticularly at higher layers. These findings have valuable implications for\nassessing the robustness of LLMs in real-world scenarios. Fully reproducible\ncodebase at https://github.com/Flegyas/AttentionLikelihood.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruscio_V/0/1/0/all/0/1\">Valeria Ruscio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maiorca_V/0/1/0/all/0/1\">Valentino Maiorca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1\">Fabrizio Silvestri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rediscovery of CNN's Versatility for Text-based Encoding of Raw Electronic Health Records. (arXiv:2303.08290v1 [cs.LG])","link":"http://arxiv.org/abs/2303.08290","description":"<p>Making the most use of abundant information in electronic health records\n(EHR) is rapidly becoming an important topic in the medical domain. Recent work\npresented a promising framework that embeds entire features in raw EHR data\nregardless of its form and medical code standards. The framework, however, only\nfocuses on encoding EHR with minimal preprocessing and fails to consider how to\nlearn efficient EHR representation in terms of computation and memory usage. In\nthis paper, we search for a versatile encoder not only reducing the large data\ninto a manageable size but also well preserving the core information of\npatients to perform diverse clinical tasks. We found that hierarchically\nstructured Convolutional Neural Network (CNN) often outperforms the\nstate-of-the-art model on diverse tasks such as reconstruction, prediction, and\ngeneration, even with fewer parameters and less training time. Moreover, it\nturns out that making use of the inherent hierarchy of EHR data can boost the\nperformance of any kind of backbone models and clinical tasks performed.\nThrough extensive experiments, we present concrete evidence to generalize our\nresearch findings into real-world practice. We give a clear guideline on\nbuilding the encoder based on the research findings captured while exploring\nnumerous settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_E/0/1/0/all/0/1\">Eunbyeol Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Min Jae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hur_K/0/1/0/all/0/1\">Kyunghoon Hur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiyoun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jinsung Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Study on Post-Training Quantization for Large Language Models. (arXiv:2303.08302v1 [cs.LG])","link":"http://arxiv.org/abs/2303.08302","description":"<p>Post-training quantization (\\ptq) had been recently shown as a compromising\nmethod to reduce the memory consumption and/or compute cost for large language\nmodels. However, a comprehensive study about the effect of different\nquantization schemes, different model families, different \\ptq methods,\ndifferent quantization bit precision, etc, is still missing. In this work, we\nprovide an extensive study on those components over tens of thousands of\nzero-shot experiments. Our results show that (1) Fine-grained quantization and\n\\ptq methods (instead of naive round-to-nearest quantization) are necessary to\nachieve good accuracy and (2) Higher bits (e.g., 5 bits) with coarse-grained\nquantization is more powerful than lower bits (e.g., 4 bits) with very\nfine-grained quantization (whose effective bits is similar to 5-bits). We also\npresent recommendations about how to utilize quantization for \\llms with\ndifferent sizes, and leave suggestions of future opportunities and system work\nthat are not resolved in this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaoxia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Youn_S/0/1/0/all/0/1\">Stephen Youn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxiong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-speaker Emotion Transfer by Manipulating Speech Style Latents. (arXiv:2303.08329v1 [cs.SD])","link":"http://arxiv.org/abs/2303.08329","description":"<p>In recent years, emotional text-to-speech has shown considerable progress.\nHowever, it requires a large amount of labeled data, which is not easily\naccessible. Even if it is possible to acquire an emotional speech dataset,\nthere is still a limitation in controlling emotion intensity. In this work, we\npropose a novel method for cross-speaker emotion transfer and manipulation\nusing vector arithmetic in latent style space. By leveraging only a few labeled\nsamples, we generate emotional speech from reading-style speech without losing\nthe speaker identity. Furthermore, emotion strength is readily controllable\nusing a scalar value, providing an intuitive way for users to manipulate\nspeech. Experimental results show the proposed method affords superior\nperformance in terms of expressiveness, naturalness, and controllability,\npreserving speaker identity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jo_S/0/1/0/all/0/1\">Suhee Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Younggun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1\">Yookyung Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_Y/0/1/0/all/0/1\">Yeongtae Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taesu Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FactReranker: Fact-guided Reranker for Faithful Radiology Report Summarization. (arXiv:2303.08335v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08335","description":"<p>Automatic radiology report summarization is a crucial clinical task, whose\nkey challenge is to maintain factual accuracy between produced summaries and\nground truth radiology findings. Existing research adopts reinforcement\nlearning to directly optimize factual consistency metrics such as CheXBert or\nRadGraph score. However, their decoding method using greedy search or beam\nsearch considers no factual consistency when picking the optimal candidate,\nleading to limited factual consistency improvement. To address it, we propose a\nnovel second-stage summarizing approach FactReranker, the first attempt that\nlearns to choose the best summary from all candidates based on their estimated\nfactual consistency score. We propose to extract medical facts of the input\nmedical report, its gold summary, and candidate summaries based on the RadGraph\nschema and design the fact-guided reranker to efficiently incorporate the\nextracted medical facts for selecting the optimal summary. We decompose the\nfact-guided reranker into the factual knowledge graph generation and the\nfactual scorer, which allows the reranker to model the mapping between the\nmedical facts of the input text and its gold summary, thus can select the\noptimal summary even the gold summary can't be observed during inference. We\nalso present a fact-based ranking metric (RadMRR) for measuring the ability of\nthe reranker on selecting factual consistent candidates. Experimental results\non two benchmark datasets demonstrate the superiority of our method in\ngenerating summaries with higher factual consistency scores when compared with\nexisting methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qianqian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinpeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiayu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PR-MCS: Perturbation Robust Metric for MultiLingual Image Captioning. (arXiv:2303.08389v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08389","description":"<p>Vulnerability to lexical perturbation is a critical weakness of automatic\nevaluation metrics for image captioning. This paper proposes Perturbation\nRobust Multi-Lingual CLIPScore(PR-MCS), which exhibits robustness to such\nperturbations, as a novel reference-free image captioning metric applicable to\nmultiple languages. To achieve perturbation robustness, we fine-tune the text\nencoder of CLIP with our language-agnostic method to distinguish the perturbed\ntext from the original text. To verify the robustness of PR-MCS, we introduce a\nnew fine-grained evaluation dataset consisting of detailed captions, critical\nobjects, and the relationships between the objects for 3, 000 images in five\nlanguages. In our experiments, PR-MCS significantly outperforms baseline\nmetrics in capturing lexical noise of all various perturbation types in all\nfive languages, proving that PR-MCS is highly robust to lexical perturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yongil Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_Y/0/1/0/all/0/1\">Yerin Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_H/0/1/0/all/0/1\">Hyeongu Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1\">Kyomin Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Cross-institutional Evaluation on Breast Cancer Phenotyping NLP Algorithms on Electronic Health Records. (arXiv:2303.08448v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08448","description":"<p>Objective: The generalizability of clinical large language models is usually\nignored during the model development process. This study evaluated the\ngeneralizability of BERT-based clinical NLP models across different clinical\nsettings through a breast cancer phenotype extraction task.\n</p>\n<p>Materials and Methods: Two clinical corpora of breast cancer patients were\ncollected from the electronic health records from the University of Minnesota\nand the Mayo Clinic, and annotated following the same guideline. We developed\nthree types of NLP models (i.e., conditional random field, bi-directional long\nshort-term memory and CancerBERT) to extract cancer phenotypes from clinical\ntexts. The models were evaluated for their generalizability on different test\nsets with different learning strategies (model transfer vs. locally trained).\nThe entity coverage score was assessed with their association with the model\nperformances.\n</p>\n<p>Results: We manually annotated 200 and 161 clinical documents at UMN and MC,\nrespectively. The corpora of the two institutes were found to have higher\nsimilarity between the target entities than the overall corpora. The CancerBERT\nmodels obtained the best performances among the independent test sets from two\nclinical institutes and the permutation test set. The CancerBERT model\ndeveloped in one institute and further fine-tuned in another institute achieved\nreasonable performance compared to the model developed on local data (micro-F1:\n0.925 vs 0.932).\n</p>\n<p>Conclusions: The results indicate the CancerBERT model has the best learning\nability and generalizability among the three types of clinical NLP models. The\ngeneralizability of the models was found to be correlated with the similarity\nof the target entities between the corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sicheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Ju Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blaes_A/0/1/0/all/0/1\">Anne Blaes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation. (arXiv:2303.08518v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08518","description":"<p>Large Language Models (LLMs) are popular for their impressive abilities, but\nthe need for model-specific fine-tuning or task-specific prompt engineering can\nhinder their generalization. We propose UPRISE (Universal Prompt Retrieval for\nImproving zero-Shot Evaluation), which tunes a lightweight and versatile\nretriever that automatically retrieves prompts for a given zero-shot task\ninput. Specifically, we demonstrate universality in a cross-task and\ncross-model scenario: the retriever is tuned on a diverse set of tasks, but\ntested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for\ntuning the retriever, but test the retriever on different LLMs of much larger\nscales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that\nUPRISE mitigates the hallucination problem in our experiments with ChatGPT,\nsuggesting its potential to improve even the strongest LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1\">Daixuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1\">Junyu Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yuefeng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianfeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_D/0/1/0/all/0/1\">Denvy Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Image of the Process Interpretation of Regular Expressions is Not Closed under Bisimulation Collapse. (arXiv:2303.08553v1 [cs.LO])","link":"http://arxiv.org/abs/2303.08553","description":"<p>Axiomatization and expressibility problems for Milner's process semantics\n(1984) of regular expressions modulo bisimilarity have turned out to be\ndifficult for the full class of expressions with deadlock 0 and empty step~1.\nWe report on a phenomenon that arises from the added presence of 1 when 0 is\navailable, and that brings a crucial reason for this difficulty into focus. To\nwit, while interpretations of 1-free regular expressions are closed under\nbisimulation collapse, this is not the case for the interpretations of\narbitrary regular expressions.\n</p>\n<p>Process graph interpretations of 1-free regular expressions satisfy the loop\nexistence and elimination property LEE, which is preserved under bisimulation\ncollapse. These features of LEE were applied for showing that an equational\nproof system for 1-free regular expressions modulo bisimilarity is complete,\nand that it is decidable in polynomial time whether a process graph is\nbisimilar to the interpretation of a 1-free regular expression.\n</p>\n<p>While interpretations of regular expressions do not satisfy the property LEE\nin general, we show that LEE can be recovered by refined interpretations as\ngraphs with 1-transitions refined interpretations with 1-transitions (which are\nsimilar to silent steps for automata). This suggests that LEE can be expedient\nalso for the general axiomatization and expressibility problems. But a new\nphenomenon emerges that needs to be addressed: the property of a process graph\n`to can be refined into a process graph with 1-transitions and with LEE' is not\npreserved under bisimulation collapse. We provide a 10-vertex graph with two\n1-transitions that satisfies LEE, and in which a pair of bisimilar vertices\ncannot be collapsed on to each other while preserving the refinement property.\nThis implies that the image of the process interpretation of regular\nexpressions is not closed under bisimulation collapse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grabmayer_C/0/1/0/all/0/1\">Clemens Grabmayer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!. (arXiv:2303.08559v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08559","description":"<p>Large Language Models (LLMs) have made remarkable strides in various tasks.\nHowever, whether they are competitive few-shot solvers for information\nextraction (IE) tasks and surpass fine-tuned small Pre-trained Language Models\n(SLMs) remains an open problem. This paper aims to provide a thorough answer to\nthis problem, and moreover, to explore an approach towards effective and\neconomical IE systems that combine the strengths of LLMs and SLMs. Through\nextensive experiments on eight datasets across three IE tasks, we show that\nLLMs are not effective few-shot information extractors in general, given their\nunsatisfactory performance in most settings and the high latency and budget\nrequirements. However, we demonstrate that LLMs can well complement SLMs and\neffectively solve hard samples that SLMs struggle with. Building on these\nfindings, we propose an adaptive filter-then-rerank paradigm, in which SLMs act\nas filters and LLMs act as rerankers. By utilizing LLMs to rerank a small\nportion of difficult samples identified by SLMs, our preliminary system\nconsistently achieves promising improvements (2.1% F1-gain on average) on\nvarious IE tasks, with acceptable cost of time and money.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yubo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">YongChing Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distinguishing Cause from Effect on Categorical Data: The Uniform Channel Model. (arXiv:2303.08572v1 [cs.LG])","link":"http://arxiv.org/abs/2303.08572","description":"<p>Distinguishing cause from effect using observations of a pair of random\nvariables is a core problem in causal discovery. Most approaches proposed for\nthis task, namely additive noise models (ANM), are only adequate for\nquantitative data. We propose a criterion to address the cause-effect problem\nwith categorical variables (living in sets with no meaningful order), inspired\nby seeing a conditional probability mass function (pmf) as a discrete\nmemoryless channel. We select as the most likely causal direction the one in\nwhich the conditional pmf is closer to a uniform channel (UC). The rationale is\nthat, in a UC, as in an ANM, the conditional entropy (of the effect given the\ncause) is independent of the cause distribution, in agreement with the\nprinciple of independence of cause and mechanism. Our approach, which we call\nthe uniform channel model (UCM), thus extends the ANM rationale to categorical\nvariables. To assess how close a conditional pmf (estimated from data) is to a\nUC, we use statistical testing, supported by a closed-form estimate of a UC\nchannel. On the theoretical front, we prove identifiability of the UCM and show\nits equivalence with a structural causal model with a low-cardinality exogenous\nvariable. Finally, the proposed method compares favorably with recent\nstate-of-the-art alternatives in experiments on synthetic, benchmark, and real\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Figueiredo_M/0/1/0/all/0/1\">M&#xe1;rio A. T. Figueiredo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_C/0/1/0/all/0/1\">Catarina A. Oliveira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Uncertainty Estimation with Gaussian Process for Reliable Dialog Response Retrieval. (arXiv:2303.08599v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08599","description":"<p>Deep neural networks have achieved remarkable performance in retrieval-based\ndialogue systems, but they are shown to be ill calibrated. Though basic\ncalibration methods like Monte Carlo Dropout and Ensemble can calibrate well,\nthese methods are time-consuming in the training or inference stages. To tackle\nthese challenges, we propose an efficient uncertainty calibration framework\nGPF-BERT for BERT-based conversational search, which employs a Gaussian Process\nlayer and the focal loss on top of the BERT architecture to achieve a\nhigh-quality neural ranker. Extensive experiments are conducted to verify the\neffectiveness of our method. In comparison with basic calibration methods,\nGPF-BERT achieves the lowest empirical calibration error (ECE) in three\nin-domain datasets and the distributional shift tasks, while yielding the\nhighest $R_{10}@1$ and MAP performance on most cases. In terms of time\nconsumption, our GPF-BERT has an 8$\\times$ speedup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1\">Tong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhitao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Ning Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GCRE-GPT: A Generative Model for Comparative Relation Extraction. (arXiv:2303.08601v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08601","description":"<p>Given comparative text, comparative relation extraction aims to extract two\ntargets (\\eg two cameras) in comparison and the aspect they are compared for\n(\\eg image quality). The extracted comparative relations form the basis of\nfurther opinion analysis.Existing solutions formulate this task as a sequence\nlabeling task, to extract targets and aspects. However, they cannot directly\nextract comparative relation(s) from text. In this paper, we show that\ncomparative relations can be directly extracted with high accuracy, by\ngenerative model. Based on GPT-2, we propose a Generation-based Comparative\nRelation Extractor (GCRE-GPT). Experiment results show that \\modelname achieves\nstate-of-the-art accuracy on two datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yequan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hengran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xuying Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Calibration and Uncertainty with P\\'{o}lya-Gamma Augmentation for Dialog Retrieval Models. (arXiv:2303.08606v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08606","description":"<p>Deep neural retrieval models have amply demonstrated their power but\nestimating the reliability of their predictions remains challenging. Most\ndialog response retrieval models output a single score for a response on how\nrelevant it is to a given question. However, the bad calibration of deep neural\nnetwork results in various uncertainty for the single score such that the\nunreliable predictions always misinform user decisions. To investigate these\nissues, we present an efficient calibration and uncertainty estimation\nframework PG-DRR for dialog response retrieval models which adds a Gaussian\nProcess layer to a deterministic deep neural network and recovers conjugacy for\ntractable posterior inference by P\\'{o}lya-Gamma augmentation. Finally, PG-DRR\nachieves the lowest empirical calibration error (ECE) in the in-domain datasets\nand the distributional shift task while keeping $R_{10}@1$ and MAP performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1\">Tong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1\">Shijing Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Ning Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhitao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Query Generation for Evidence Collection from Web Search Engines. (arXiv:2303.08652v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08652","description":"<p>It is widely accepted that so-called facts can be checked by searching for\ninformation on the Internet. This process requires a fact-checker to formulate\na search query based on the fact and to present it to a search engine. Then,\nrelevant and believable passages need to be identified in the search results\nbefore a decision is made. This process is carried out by sub-editors at many\nnews and media organisations on a daily basis. Here, we ask the question as to\nwhether it is possible to automate the first step, that of query generation.\nCan we automatically formulate search queries based on factual statements which\nare similar to those formulated by human experts? Here, we consider similarity\nboth in terms of textual similarity and with respect to relevant documents\nbeing returned by a search engine. First, we introduce a moderate-sized\nevidence collection dataset which includes 390 factual statements together with\nassociated human-generated search queries and search results. Then, we\ninvestigate generating queries using a number of rule-based and automatic text\ngeneration methods based on pre-trained large language models (LLMs). We show\nthat these methods have different merits and propose a hybrid approach which\nhas superior performance in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prieto_Chavana_N/0/1/0/all/0/1\">Nestor Prieto-Chavana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weeds_J/0/1/0/all/0/1\">Julie Weeds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weir_D/0/1/0/all/0/1\">David Weir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mirror: A Natural Language Interface for Data Querying, Summarization, and Visualization. (arXiv:2303.08697v1 [cs.DB])","link":"http://arxiv.org/abs/2303.08697","description":"<p>We present Mirror, an open-source platform for data exploration and analysis\npowered by large language models. Mirror offers an intuitive natural language\ninterface for querying databases, and automatically generates executable SQL\ncommands to retrieve relevant data and summarize it in natural language. In\naddition, users can preview and manually edit the generated SQL commands to\nensure the accuracy of their queries. Mirror also generates visualizations to\nfacilitate understanding of the data. Designed with flexibility and human input\nin mind, Mirror is suitable for both experienced data analysts and\nnon-technical professionals looking to gain insights from their data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Penghan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Artificial Influence: An Analysis Of AI-Driven Persuasion. (arXiv:2303.08721v1 [cs.CY])","link":"http://arxiv.org/abs/2303.08721","description":"<p>Persuasion is a key aspect of what it means to be human, and is central to\nbusiness, politics, and other endeavors. Advancements in artificial\nintelligence (AI) have produced AI systems that are capable of persuading\nhumans to buy products, watch videos, click on search results, and more. Even\nsystems that are not explicitly designed to persuade may do so in practice. In\nthe future, increasingly anthropomorphic AI systems may form ongoing\nrelationships with users, increasing their persuasive power. This paper\ninvestigates the uncertain future of persuasive AI systems. We examine ways\nthat AI could qualitatively alter our relationship to and views regarding\npersuasion by shifting the balance of persuasive power, allowing personalized\npersuasion to be deployed at scale, powering misinformation campaigns, and\nchanging the way humans can shape their own discourse. We consider ways\nAI-driven persuasion could differ from human-driven persuasion. We warn that\nubiquitous highlypersuasive AI systems could alter our information environment\nso significantly so as to contribute to a loss of human control of our own\nfuture. In response, we examine several potential responses to AI-driven\npersuasion: prohibition, identification of AI agents, truthful AI, and legal\nremedies. We conclude that none of these solutions will be airtight, and that\nindividuals and governments will need to take active steps to guard against the\nmost pernicious effects of persuasive AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burtell_M/0/1/0/all/0/1\">Matthew Burtell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodside_T/0/1/0/all/0/1\">Thomas Woodside</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-4 Technical Report. (arXiv:2303.08774v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08774","description":"<p>We report the development of GPT-4, a large-scale, multimodal model which can\naccept image and text inputs and produce text outputs. While less capable than\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance on\nvarious professional and academic benchmarks, including passing a simulated bar\nexam with a score around the top 10% of test takers. GPT-4 is a\nTransformer-based model pre-trained to predict the next token in a document.\nThe post-training alignment process results in improved performance on measures\nof factuality and adherence to desired behavior. A core component of this\nproject was developing infrastructure and optimization methods that behave\npredictably across a wide range of scales. This allowed us to accurately\npredict some aspects of GPT-4's performance based on models trained with no\nmore than 1/1,000th the compute of GPT-4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+OpenAI/0/1/0/all/0/1\">OpenAI</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cascading and Direct Approaches to Unsupervised Constituency Parsing on Spoken Sentences. (arXiv:2303.08809v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08809","description":"<p>Past work on unsupervised parsing is constrained to written form. In this\npaper, we present the first study on unsupervised spoken constituency parsing\ngiven unlabeled spoken sentences and unpaired textual data. The goal is to\ndetermine the spoken sentences' hierarchical syntactic structure in the form of\nconstituency parse trees, such that each node is a span of audio that\ncorresponds to a constituent. We compare two approaches: (1) cascading an\nunsupervised automatic speech recognition (ASR) model and an unsupervised\nparser to obtain parse trees on ASR transcripts, and (2) direct training an\nunsupervised parser on continuous word-level speech representations. This is\ndone by first splitting utterances into sequences of word-level segments, and\naggregating self-supervised speech representations within segments to obtain\nsegment embeddings. We find that separately training a parser on the unpaired\ntext and directly applying it on ASR transcripts for inference produces better\nresults for unsupervised parsing. Additionally, our results suggest that\naccurate segmentation alone may be sufficient to parse spoken sentences\naccurately. Finally, we show the direct approach may learn head-directionality\ncorrectly for both head-initial and head-final languages without any explicit\ninductive bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tseng_Y/0/1/0/all/0/1\">Yuan Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Cheng-I Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label prompt for multi-label text classification. (arXiv:2106.10076v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.10076","description":"<p>One of the key problems in multi-label text classification is how to take\nadvantage of the correlation among labels. However, it is very challenging to\ndirectly model the correlations among labels in a complex and unknown label\nspace. In this paper, we propose a Label Mask multi-label text classification\nmodel (LM-MTC), which is inspired by the idea of cloze questions of language\nmodel. LM-MTC is able to capture implicit relationships among labels through\nthe powerful ability of pre-train language models. On the basis, we assign a\ndifferent token to each potential label, and randomly mask the token with a\ncertain probability to build a label based Masked Language Model (MLM). We\ntrain the MTC and MLM together, further improving the generalization ability of\nthe model. A large number of experiments on multiple datasets demonstrate the\neffectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Rui Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingbing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zelong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_H/0/1/0/all/0/1\">Haining An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Large and Diverse Arabic Corpus for Language Modeling. (arXiv:2201.09227v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.09227","description":"<p>Language models (LMs) have introduced a major paradigm shift in Natural\nLanguage Processing (NLP) modeling where large pre-trained LMs became integral\nto most of the NLP tasks. The LMs are intelligent enough to find useful and\nrelevant representations of the language without any supervision. Perhaps,\nthese models are used to fine-tune typical NLP tasks with significantly high\naccuracy as compared to the traditional approaches. Conversely, the training of\nthese models requires a massively large corpus that is a good representation of\nthe language. English LMs generally perform better than their other language\ncounterparts, due to the availability of massive English corpora. This work\nelaborates on the design and development of a large Arabic corpus. It consists\nof over 500 GB of Arabic cleaned text targeted at improving cross-domain\nknowledge and downstream generalization capability of large-scale language\nmodels. Moreover, the corpus is utilized in the training of a large Arabic LM.\nIn order to evaluate the effectiveness of the LM, a number of typical NLP tasks\nare fine-tuned. The tasks demonstrate a significant boost from 4.5 to 8.5% when\ncompared to tasks fine-tuned on multi-lingual BERT (mBERT). To the best of my\nknowledge, this is currently the largest clean and diverse Arabic corpus ever\ncollected.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Abbas Raza Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_M/0/1/0/all/0/1\">Muhammad Ajmal Siddiqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Algunaibet_R/0/1/0/all/0/1\">Rema Algunaibet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_H/0/1/0/all/0/1\">Hasan Raza Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Vision and Language Modeling for Multi-modal Representation Learning. (arXiv:2208.02131v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2208.02131","description":"<p>In this paper, we study how to use masked signal modeling in vision and\nlanguage (V+L) representation learning. Instead of developing masked language\nmodeling (MLM) and masked image modeling (MIM) independently, we propose to\nbuild joint masked vision and language modeling, where the masked signal of one\nmodality is reconstructed with the help from another modality. This is\nmotivated by the nature of image-text paired data that both of the image and\nthe text convey almost the same information but in different formats. The\nmasked signal reconstruction of one modality conditioned on another modality\ncan also implicitly learn cross-modal alignment between language tokens and\nimage patches. Our experiments on various V+L tasks show that the proposed\nmethod, along with common V+L alignment losses, achieves state-of-the-art\nperformance in the regime of millions of pre-training data. Also, we\noutperforms the other competitors by a significant margin in limited data\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_G/0/1/0/all/0/1\">Gukyeong Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhaowei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1\">Avinash Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bas_E/0/1/0/all/0/1\">Erhan Bas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhotika_R/0/1/0/all/0/1\">Rahul Bhotika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Opinion Summarization Using Approximate Geodesics. (arXiv:2209.07496v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.07496","description":"<p>Opinion summarization is the task of creating summaries capturing popular\nopinions from user reviews. In this paper, we introduce Geodesic Summarizer\n(GeoSumm), a novel system to perform unsupervised extractive opinion\nsummarization. GeoSumm involves an encoder-decoder based representation\nlearning model, that generates representations of text as a distribution over\nlatent semantic units. GeoSumm generates these representations by performing\ndictionary learning over pre-trained text representations at multiple decoder\nlayers. We then use these representations to quantify the relevance of review\nsentences using a novel approximate geodesic distance based scoring mechanism.\nWe use the relevance scores to identify popular opinions in order to compose\ngeneral and aspect-specific summaries. Our proposed model, GeoSumm, achieves\nstate-of-the-art performance on three opinion summarization datasets. We\nperform additional experiments to analyze the functioning of our model and\nshowcase the generalization ability of {\\X} across different domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Somnath Basu Roy Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monath_N/0/1/0/all/0/1\">Nicholas Monath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1\">Avinava Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1\">Amr Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Snigdha Chaturvedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How GPT-3 responds to different publics on climate change and Black Lives Matter: A critical appraisal of equity in conversational AI. (arXiv:2209.13627v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2209.13627","description":"<p>Autoregressive language models, which use deep learning to produce human-like\ntexts, have become increasingly widespread. Such models are powering popular\nvirtual assistants in areas like smart health, finance, and autonomous driving.\nWhile the parameters of these large language models are improving, concerns\npersist that these models might not work equally for all subgroups in society.\nDespite growing discussions of AI fairness across disciplines, there lacks\nsystemic metrics to assess what equity means in dialogue systems and how to\nengage different populations in the assessment loop. Grounded in theories of\ndeliberative democracy and science and technology studies, this paper proposes\nan analytical framework for unpacking the meaning of equity in human-AI\ndialogues. Using this framework, we conducted an auditing study to examine how\nGPT-3 responded to different sub-populations on crucial science and social\ntopics: climate change and the Black Lives Matter (BLM) movement. Our corpus\nconsists of over 20,000 rounds of dialogues between GPT-3 and 3290 individuals\nwho vary in gender, race and ethnicity, education level, English as a first\nlanguage, and opinions toward the issues. We found a substantively worse user\nexperience with GPT-3 among the opinion and the education minority\nsubpopulations; however, these two groups achieved the largest knowledge gain,\nchanging attitudes toward supporting BLM and climate change efforts after the\nchat. We traced these user experience divides to conversational differences and\nfound that GPT-3 used more negative expressions when it responded to the\neducation and opinion minority groups, compared to its responses to the\nmajority groups. We discuss the implications of our findings for a deliberative\nconversational AI system that centralizes diversity, equity, and inclusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kaiping Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_A/0/1/0/all/0/1\">Anqi Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burapacheep_J/0/1/0/all/0/1\">Jirayu Burapacheep</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DABERT: Dual Attention Enhanced BERT for Semantic Matching. (arXiv:2210.03454v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03454","description":"<p>Transformer-based pre-trained language models such as BERT have achieved\nremarkable results in Semantic Sentence Matching. However, existing models\nstill suffer from insufficient ability to capture subtle differences. Minor\nnoise like word addition, deletion, and modification of sentences may cause\nflipped predictions. To alleviate this problem, we propose a novel Dual\nAttention Enhanced BERT (DABERT) to enhance the ability of BERT to capture\nfine-grained differences in sentence pairs. DABERT comprises (1) Dual Attention\nmodule, which measures soft word matches by introducing a new dual channel\nalignment mechanism to model affinity and difference attention. (2) Adaptive\nFusion module, this module uses attention to learn the aggregation of\ndifference and affinity features, and generates a vector describing the\nmatching details of sentence pairs. We conduct extensive experiments on\nwell-studied semantic matching and robustness test datasets, and the\nexperimental results show the effectiveness of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Di Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jian Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perplexity from PLM Is Unreliable for Evaluating Text Quality. (arXiv:2210.05892v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05892","description":"<p>Recently, amounts of works utilize perplexity~(PPL) to evaluate the quality\nof the generated text. They suppose that if the value of PPL is smaller, the\nquality(i.e. fluency) of the text to be evaluated is better. However, we find\nthat the PPL referee is unqualified and it cannot evaluate the generated text\nfairly for the following reasons: (i) The PPL of short text is larger than long\ntext, which goes against common sense, (ii) The repeated text span could damage\nthe performance of PPL, and (iii) The punctuation marks could affect the\nperformance of PPL heavily. Experiments show that the PPL is unreliable for\nevaluating the quality of given text. Last, we discuss the key problems with\nevaluating text quality using language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yequan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiawen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xuying Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting. (arXiv:2210.07179v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.07179","description":"<p>Large pre-trained models have proved to be remarkable zero- and\n(prompt-based) few-shot learners in unimodal vision and language tasks. We\npropose MAPL, a simple and parameter-efficient method that reuses frozen\npre-trained unimodal models and leverages their strong generalization\ncapabilities in multimodal vision-language (VL) settings. MAPL learns a\nlightweight mapping between the representation spaces of unimodal models using\naligned image-text data, and can generalize to unseen VL tasks from just a few\nin-context examples. The small number of trainable parameters makes MAPL\neffective at low-data and in-domain learning. Moreover, MAPL's modularity\nenables easy extension to other pre-trained models. Extensive experiments on\nseveral visual question answering and image captioning benchmarks show that\nMAPL achieves superior or competitive performance compared to similar methods\nwhile training orders of magnitude fewer parameters. MAPL can be trained in\njust a few hours using modest computational resources and public datasets. We\nrelease our code and pre-trained model weights at\nhttps://github.com/mair-lab/mapl.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manas_O/0/1/0/all/0/1\">Oscar Ma&#xf1;as</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1\">Pau Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmadi_S/0/1/0/all/0/1\">Saba Ahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1\">Aida Nematzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_Y/0/1/0/all/0/1\">Yash Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Aishwarya Agrawal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion. (arXiv:2210.08471v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08471","description":"<p>Transformer-based pre-trained models like BERT have achieved great progress\non Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also\nshown general benefits in multiple NLP tasks. However, how to efficiently\nintegrate dependency prior structure into pre-trained models to better model\ncomplex semantic matching relations is still unsettled. In this paper, we\npropose the \\textbf{D}ependency-Enhanced \\textbf{A}daptive \\textbf{F}usion\n\\textbf{A}ttention (\\textbf{DAFA}), which explicitly introduces dependency\nstructure into pre-trained models and adaptively fuses it with semantic\ninformation. Specifically, \\textbf{\\emph{(i)}} DAFA first proposes a\nstructure-sensitive paradigm to construct a dependency matrix for calibrating\nattention weights. It adopts an adaptive fusion module to integrate the\nobtained dependency information and the original semantic signals. Moreover,\nDAFA reconstructs the attention calculation flow and provides better\ninterpretability. By applying it on BERT, our method achieves state-of-the-art\nor competitive performance on 10 public datasets, demonstrating the benefits of\nadaptively fusing dependency structure in semantic matching task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jian Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Di Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rumei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1\">Minlong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yongxin Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Virtuoso: Massive Multilingual Speech-Text Joint Semi-Supervised Learning for Text-To-Speech. (arXiv:2210.15447v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2210.15447","description":"<p>This paper proposes Virtuoso, a massively multilingual speech-text joint\nsemi-supervised learning framework for text-to-speech synthesis (TTS) models.\nExisting multilingual TTS typically supports tens of languages, which are a\nsmall fraction of the thousands of languages in the world. One difficulty to\nscale multilingual TTS to hundreds of languages is collecting high-quality\nspeech-text paired data in low-resource languages. This study extends Maestro,\na speech-text joint pretraining framework for automatic speech recognition\n(ASR), to speech generation tasks. To train a TTS model from various types of\nspeech and text data, different training schemes are designed to handle\nsupervised (paired TTS and ASR data) and unsupervised (untranscribed speech and\nunspoken text) datasets. Experimental evaluation shows that 1) multilingual TTS\nmodels trained on Virtuoso can achieve significantly better naturalness and\nintelligibility than baseline ones in seen languages, and 2) they can\nsynthesize reasonably intelligible and naturally sounding speech for unseen\nlanguages where no high-quality paired TTS data is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saeki_T/0/1/0/all/0/1\">Takaaki Saeki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zen_H/0/1/0/all/0/1\">Heiga Zen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhehuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morioka_N/0/1/0/all/0/1\">Nobuyuki Morioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gary Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_A/0/1/0/all/0/1\">Andrew Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramabhadran_B/0/1/0/all/0/1\">Bhuvana Ramabhadran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Terminology-aware Medical Dialogue Generation. (arXiv:2210.15551v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.15551","description":"<p>Medical dialogue generation aims to generate responses according to a history\nof dialogue turns between doctors and patients. Unlike open-domain dialogue\ngeneration, this requires background knowledge specific to the medical domain.\nExisting generative frameworks for medical dialogue generation fall short of\nincorporating domain-specific knowledge, especially with regard to medical\nterminology. In this paper, we propose a novel framework to improve medical\ndialogue generation by considering features centered on domain-specific\nterminology. We leverage an attention mechanism to incorporate terminologically\ncentred features, and fill in the semantic gap between medical background\nknowledge and common utterances by enforcing language models to learn\nterminology representations with an auxiliary terminology recognition task.\nExperimental results demonstrate the effectiveness of our approach, in which\nour proposed framework outperforms SOTA language models. Additionally, we\nprovide a new dataset with medical terminology annotations to support the\nresearch on medical dialogue generation. Our dataset and code are available at\nhttps://github.com/tangg555/meddialog.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loakman_T/0/1/0/all/0/1\">Tyler Loakman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1\">Frank Guerin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Acoustic Word Embeddings from Pre-trained Self-supervised Speech Models. (arXiv:2210.16043v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.16043","description":"<p>Given the strong results of self-supervised models on various tasks, there\nhave been surprisingly few studies exploring self-supervised representations\nfor acoustic word embeddings (AWE), fixed-dimensional vectors representing\nvariable-length spoken word segments. In this work, we study several\npre-trained models and pooling methods for constructing AWEs with\nself-supervised representations. Owing to the contextualized nature of\nself-supervised representations, we hypothesize that simple pooling methods,\nsuch as averaging, might already be useful for constructing AWEs. When\nevaluating on a standard word discrimination task, we find that HuBERT\nrepresentations with mean-pooling rival the state of the art on English AWEs.\nMore surprisingly, despite being trained only on English, HuBERT\nrepresentations evaluated on Xitsonga, Mandarin, and French consistently\noutperform the multilingual model XLSR-53 (as well as Wav2Vec 2.0 trained on\nEnglish).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanabria_R/0/1/0/all/0/1\">Ramon Sanabria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwater_S/0/1/0/all/0/1\">Sharon Goldwater</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Spoken Language Understanding with Tree-constrained Pointer Generator. (arXiv:2210.16554v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.16554","description":"<p>End-to-end spoken language understanding (SLU) suffers from the long-tail\nword problem. This paper exploits contextual biasing, a technique to improve\nthe speech recognition of rare words, in end-to-end SLU systems. Specifically,\na tree-constrained pointer generator (TCPGen), a powerful and efficient biasing\nmodel component, is studied, which leverages a slot shortlist with\ncorresponding entities to extract biasing lists. Meanwhile, to bias the SLU\nmodel output slot distribution, a slot probability biasing (SPB) mechanism is\nproposed to calculate a slot distribution from TCPGen. Experiments on the SLURP\ndataset showed consistent SLU-F1 improvements using TCPGen and SPB, especially\non unseen entities. On a new split by holding out 5 slot types for the test,\nTCPGen with SPB achieved zero-shot learning with an SLU-F1 score over 50%\ncompared to baselines which can not deal with it. In addition to slot filling,\nthe intent classification accuracy was also improved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangzhi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Once-for-All Sequence Compression for Self-Supervised Speech Models. (arXiv:2211.02332v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.02332","description":"<p>The sequence length along the time axis is often the dominant factor of the\ncomputation in speech processing. Works have been proposed to reduce the\nsequence length for lowering the computational cost in self-supervised speech\nmodels. However, different downstream tasks have different tolerance of\nsequence compressing, so a model that produces a fixed compressing rate may not\nfit all tasks. In this work, we introduce a once-for-all (OFA) sequence\ncompression framework for self-supervised speech models that supports a\ncontinuous range of operating compressing rates. The framework is evaluated on\nvarious tasks, showing marginal degradation compared to the fixed compressing\nrate variants with a smooth performance-efficiency trade-off. We further\nexplore adaptive compressing rate learning, demonstrating the ability to select\ntask-specific preferred frame periods without needing a grid search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hsuan-Jui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yen Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding BLOOM: An empirical study on diverse NLP tasks. (arXiv:2211.14865v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.14865","description":"<p>We view the landscape of large language models (LLMs) through the lens of the\nrecently released BLOOM model to understand the performance of BLOOM and other\ndecoder-only LLMs compared to BERT-style encoder-only models. We achieve this\nby evaluating the smaller BLOOM model variants (\\textit{350m/560m} and\n\\textit{1b3/1b7}) on several NLP benchmark datasets and popular leaderboards.\nWe make the following observations: (1) BLOOM performance does not scale with\nparameter size, unlike other LLMs like GPT and BERT. Experiments fine-tuning\nBLOOM models show that the 560m variant performs similarly to or better than\nthe 1b7 variant, (2) Zero-shot cross-lingual and multi-lingual fine-tuning\nexperiments show that BLOOM is at par or worse than monolingual GPT-2 models,\nand (3) Toxicity analysis of prompt-based text generation using the\nRealToxicityPrompts dataset shows that the text generated by BLOOM is at least\n17\\% less toxic than GPT-2 and GPT-3 models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dakle_P/0/1/0/all/0/1\">Parag Pravin Dakle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rallabandi_S/0/1/0/all/0/1\">SaiKrishna Rallabandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_P/0/1/0/all/0/1\">Preethi Raghavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Category Discovery with Decoupled Prototypical Network. (arXiv:2211.15115v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.15115","description":"<p>Generalized Category Discovery (GCD) aims to recognize both known and novel\ncategories from a set of unlabeled data, based on another dataset labeled with\nonly known categories. Without considering differences between known and novel\ncategories, current methods learn about them in a coupled manner, which can\nhurt model's generalization and discriminative ability. Furthermore, the\ncoupled training approach prevents these models transferring category-specific\nknowledge explicitly from labeled data to unlabeled data, which can lose\nhigh-level semantic information and impair model performance. To mitigate above\nlimitations, we present a novel model called Decoupled Prototypical Network\n(DPN). By formulating a bipartite matching problem for category prototypes, DPN\ncan not only decouple known and novel categories to achieve different training\ntargets effectively, but also align known categories in labeled and unlabeled\ndata to transfer category-specific knowledge explicitly and capture high-level\nsemantics. Furthermore, DPN can learn more discriminative features for both\nknown and novel categories through our proposed Semantic-aware Prototypical\nLearning (SPL). Besides capturing meaningful semantic information, SPL can also\nalleviate the noise of hard pseudo labels through semantic-weighted soft\nassignment. Extensive experiments show that DPN outperforms state-of-the-art\nmodels by a large margin on all evaluation metrics across multiple benchmark\ndatasets. Code and data are available at https://github.com/Lackel/DPN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_W/0/1/0/all/0/1\">Wenbin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_F/0/1/0/all/0/1\">Feng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qinghua Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">QianYing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Ping Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Author as Character and Narrator: Deconstructing Personal Narratives from the r/AmITheAsshole Reddit Community. (arXiv:2301.08104v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.08104","description":"<p>In the r/AmITheAsshole subreddit, people anonymously share first person\nnarratives that contain some moral dilemma or conflict and ask the community to\njudge who is at fault (i.e., who is \"the asshole\"). In general, first person\nnarratives are a unique storytelling domain where the author is the narrator\n(the person telling the story) but can also be a character (the person living\nthe story) and, thus, the author has two distinct voices presented in the\nstory. In this study, we identify linguistic and narrative features associated\nwith the author as the character or as a narrator. We use these features to\nanswer the following questions: (1) what makes an asshole character and (2)\nwhat makes an asshole narrator? We extract both Author-as-Character features\n(e.g., demographics, narrative event chain, and emotional arc) and\nAuthor-as-Narrator features (i.e., the style and emotion of the story as a\nwhole) in order to identify which aspects of the narrative are correlated with\nthe final moral judgment. Our work shows that \"assholes\" as Characters frame\nthemselves as lacking agency with a more positive personal arc, while\n\"assholes\" as Narrators will tell emotional and opinionated stories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giorgi_S/0/1/0/all/0/1\">Salvatore Giorgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Ke Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_A/0/1/0/all/0/1\">Alexander H. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1\">Lara J. Martin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring The Impact Of Programming Language Distribution. (arXiv:2302.01973v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.01973","description":"<p>Current benchmarks for evaluating neural code models focus on only a small\nsubset of programming languages, excluding many popular languages such as Go or\nRust. To ameliorate this issue, we present the BabelCode framework for\nexecution-based evaluation of any benchmark in any language. BabelCode enables\nnew investigations into the qualitative performance of models' memory, runtime,\nand individual test case results. Additionally, we present a new code\ntranslation dataset called Translating Python Programming Puzzles (TP3) from\nthe Python Programming Puzzles (Schuster et al. 2021) benchmark that involves\ntranslating expert-level python functions to any language. With both BabelCode\nand the TP3 benchmark, we investigate if balancing the distributions of 14\nlanguages in a training dataset improves a large language model's performance\non low-resource languages. Training a model on a balanced corpus results in, on\naverage, 12.34% higher $pass@k$ across all tasks and languages compared to the\nbaseline. We find that this strategy achieves 66.48% better $pass@k$ on\nlow-resource languages at the cost of only a 12.94% decrease to high-resource\nlanguages. In our three translation tasks, this strategy yields, on average,\n30.77% better low-resource $pass@k$ while having 19.58% worse high-resource\n$pass@k$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orlanski_G/0/1/0/all/0/1\">Gabriel Orlanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1\">Kefan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_J/0/1/0/all/0/1\">Jeffrey Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howland_J/0/1/0/all/0/1\">Joshua Howland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malmaud_J/0/1/0/all/0/1\">Jonathan Malmaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1\">Jacob Austin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rishah Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catasta_M/0/1/0/all/0/1\">Michele Catasta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Theory of Mind May Have Spontaneously Emerged in Large Language Models. (arXiv:2302.02083v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.02083","description":"<p>Theory of mind (ToM), or the ability to impute unobservable mental states to\nothers, is central to human social interactions, communication, empathy,\nself-consciousness, and morality. We tested several language models using 40\nclassic false-belief tasks widely used to test ToM in humans. The models\npublished before 2020 showed virtually no ability to solve ToM tasks. Yet, the\nfirst version of GPT-3 (\"davinci-001\"), published in May 2020, solved about 40%\nof false-belief tasks-performance comparable with 3.5-year-old children. Its\nsecond version (\"davinci-002\"; January 2022) solved 70% of false-belief tasks,\nperformance comparable with six-year-olds. Its most recent version, GPT-3.5\n(\"davinci-003\"; November 2022), solved 90% of false-belief tasks, at the level\nof seven-year-olds. GPT-4 published in March 2023 solved nearly all the tasks\n(95%). These findings suggest that ToM-like ability (thus far considered to be\nuniquely human) may have spontaneously emerged as a byproduct of language\nmodels' improving language skills.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kosinski_M/0/1/0/all/0/1\">Michal Kosinski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TRESTLE: Toolkit for Reproducible Execution of Speech, Text and Language Experiments. (arXiv:2302.07322v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.07322","description":"<p>The evidence is growing that machine and deep learning methods can learn the\nsubtle differences between the language produced by people with various forms\nof cognitive impairment such as dementia and cognitively healthy individuals.\nValuable public data repositories such as TalkBank have made it possible for\nresearchers in the computational community to join forces and learn from each\nother to make significant advances in this area. However, due to variability in\napproaches and data selection strategies used by various researchers, results\nobtained by different groups have been difficult to compare directly. In this\npaper, we present TRESTLE (\\textbf{T}oolkit for \\textbf{R}eproducible\n\\textbf{E}xecution of \\textbf{S}peech \\textbf{T}ext and \\textbf{L}anguage\n\\textbf{E}xperiments), an open source platform that focuses on two datasets\nfrom the TalkBank repository with dementia detection as an illustrative domain.\nSuccessfully deployed in the hackallenge (Hackathon/Challenge) of the\nInternational Workshop on Health Intelligence at AAAI 2022, TRESTLE provides a\nprecise digital blueprint of the data pre-processing and selection strategies\nthat can be reused via TRESTLE by other researchers seeking comparable results\nwith their peers and current state-of-the-art (SOTA) approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changye Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weizhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1\">Trevor Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalowski_M/0/1/0/all/0/1\">Martin Michalowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pakhomov_S/0/1/0/all/0/1\">Serguei Pakhomov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FiTs: Fine-grained Two-stage Training for Knowledge-aware Question Answering. (arXiv:2302.11799v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.11799","description":"<p>Knowledge-aware question answering (KAQA) requires the model to answer\nquestions over a knowledge base, which is essential for both open-domain QA and\ndomain-specific QA, especially when language models alone cannot provide all\nthe knowledge needed. Despite the promising result of recent KAQA systems which\ntend to integrate linguistic knowledge from pre-trained language models (PLM)\nand factual knowledge from knowledge graphs (KG) to answer complex questions, a\nbottleneck exists in effectively fusing the representations from PLMs and KGs\nbecause of (i) the semantic and distributional gaps between them, and (ii) the\ndifficulties in joint reasoning over the provided knowledge from both\nmodalities. To address the above two problems, we propose a Fine-grained\nTwo-stage training framework (FiTs) to boost the KAQA system performance: The\nfirst stage aims at aligning representations from the PLM and the KG, thus\nbridging the modality gaps between them, named knowledge adaptive\npost-training. The second stage, called knowledge-aware fine-tuning, aims to\nimprove the model's joint reasoning ability based on the aligned\nrepresentations. In detail, we fine-tune the post-trained model via two\nauxiliary self-supervised tasks in addition to the QA supervision. Extensive\nexperiments demonstrate that our approach achieves state-of-the-art performance\non three benchmarks in the commonsense reasoning (i.e., CommonsenseQA,\nOpenbookQA) and medical question answering (i.e., MedQA-USMILE) domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qichen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1\">Bowen Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Which One Are You Referring To? Multimodal Object Identification in Situated Dialogue. (arXiv:2302.14680v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.14680","description":"<p>The demand for multimodal dialogue systems has been rising in various\ndomains, emphasizing the importance of interpreting multimodal inputs from\nconversational and situational contexts. We explore three methods to tackle\nthis problem and evaluate them on the largest situated dialogue dataset, SIMMC\n2.1. Our best method, scene-dialogue alignment, improves the performance by\n~20% F1-score compared to the SIMMC 2.1 baselines. We provide analysis and\ndiscussion regarding the limitation of our methods and the potential directions\nfor future works. Our code is publicly available at\nhttps://github.com/holylovenia/multimodal-object-identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images. (arXiv:2303.07274v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2303.07274","description":"<p>Weird, unusual, and uncanny images pique the curiosity of observers because\nthey challenge commonsense. For example, an image released during the 2022\nworld cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo\nplaying chess, which playfully violates our expectation that their competition\nshould occur on the football field. Humans can easily recognize and interpret\nthese unconventional images, but can AI models do the same? We introduce\nWHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is\ncomprised of purposefully commonsense-defying images created by designers using\npublicly-available image generation tools like Midjourney. We consider several\ntasks posed over the dataset. In addition to image captioning, cross-modal\nmatching, and visual question answering, we introduce a difficult explanation\ngeneration task, where models must identify and explain why a given image is\nunusual. Our results show that state-of-the-art models such as GPT3 and BLIP2\nstill lag behind human performance on WHOOPS!. We hope our dataset will inspire\nthe development of AI models with stronger visual commonsense reasoning\nabilities. Data, models and code are available at the project website:\nwhoops-benchmark.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bitton_Guetta_N/0/1/0/all/0/1\">Nitzan Bitton-Guetta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1\">Yonatan Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1\">Yuval Elovici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Architext: Language-Driven Generative Architecture Design. (arXiv:2303.07519v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.07519","description":"<p>Architectural design is a highly complex practice that involves a wide\ndiversity of disciplines, technologies, proprietary design software, expertise,\nand an almost infinite number of constraints, across a vast array of design\ntasks. Enabling intuitive, accessible, and scalable design processes is an\nimportant step towards performance-driven and sustainable design for all. To\nthat end, we introduce Architext, a novel semantic generation assistive tool.\nArchitext enables design generation with only natural language prompts, given\nto large-scale Language Models, as input. We conduct a thorough quantitative\nevaluation of Architext's downstream task performance, focusing on semantic\naccuracy and diversity for a number of pre-trained language models ranging from\n120 million to 6 billion parameters. Architext models are able to learn the\nspecific design task, generating valid residential layouts at a near 100% rate.\nAccuracy shows great improvement when scaling the models, with the largest\nmodel (GPT-J) yielding impressive accuracy ranging between 25% to over 80% for\ndifferent prompt categories. We open source the finetuned Architext models and\nour synthetic dataset, hoping to inspire experimentation in this exciting area\nof design research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galanos_T/0/1/0/all/0/1\">Theodoros Galanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liapis_A/0/1/0/all/0/1\">Antonios Liapis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yannakakis_G/0/1/0/all/0/1\">Georgios N. Yannakakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-03-15T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}