{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-03-15T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"MetaTroll: Few-shot Detection of State-Sponsored Trolls with Transformer Adapters. (arXiv:2303.07354v1 [cs.CL])","link":"http://arxiv.org/abs/2303.07354","description":"<p>State-sponsored trolls are the main actors of influence campaigns on social\nmedia and automatic troll detection is important to combat misinformation at\nscale. Existing troll detection models are developed based on training data for\nknown campaigns (e.g.\\ the influence campaign by Russia's Internet Research\nAgency on the 2016 US Election), and they fall short when dealing with {\\em\nnovel} campaigns with new targets. We propose MetaTroll, a text-based troll\ndetection model based on the meta-learning framework that enables high\nportability and parameter-efficient adaptation to new campaigns using only a\nhandful of labelled samples for few-shot transfer. We introduce\n\\textit{campaign-specific} transformer adapters to MetaTroll to ``memorise''\ncampaign-specific knowledge so as to tackle catastrophic forgetting, where a\nmodel ``forgets'' how to detect trolls from older campaigns due to continual\nadaptation. Our experiments demonstrate that MetaTroll substantially\noutperforms baselines and state-of-the-art few-shot text classification models.\nLastly, we explore simple approaches to extend MetaTroll to multilingual and\nmultimodal detection. Source code for MetaTroll is available at:\nhttps://github.com/ltian678/metatroll-code.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Lin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiuzhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AMOM: Adaptive Masking over Masking for Conditional Masked Language Model. (arXiv:2303.07457v1 [cs.CL])","link":"http://arxiv.org/abs/2303.07457","description":"<p>Transformer-based autoregressive (AR) methods have achieved appealing\nperformance for varied sequence-to-sequence generation tasks, e.g., neural\nmachine translation, summarization, and code generation, but suffer from low\ninference efficiency. To speed up the inference stage, many non-autoregressive\n(NAR) strategies have been proposed in the past few years. Among them, the\nconditional masked language model (CMLM) is one of the most versatile\nframeworks, as it can support many different sequence generation scenarios and\nachieve very competitive performance on these tasks. In this paper, we further\nintroduce a simple yet effective adaptive masking over masking strategy to\nenhance the refinement capability of the decoder and make the encoder\noptimization easier. Experiments on \\textbf{3} different tasks (neural machine\ntranslation, summarization, and code generation) with \\textbf{15} datasets in\ntotal confirm that our proposed simple method achieves significant performance\nimprovement over the strong CMLM model. Surprisingly, our proposed model yields\nstate-of-the-art performance on neural machine translation (\\textbf{34.62} BLEU\non WMT16 EN$\\to$RO, \\textbf{34.82} BLEU on WMT16 RO$\\to$EN, and \\textbf{34.84}\nBLEU on IWSLT De$\\to$En) and even better performance than the \\textbf{AR}\nTransformer on \\textbf{7} benchmark datasets with at least \\textbf{2.2$\\times$}\nspeedup. Our code is available at GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yisheng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lijun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan-Tie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Architext: Language-Driven Generative Architecture Design. (arXiv:2303.07519v1 [cs.CL])","link":"http://arxiv.org/abs/2303.07519","description":"<p>Architectural design is a highly complex practice that involves a wide\ndiversity of disciplines, technologies, proprietary design software, expertise,\nand an almost infinite number of constraints, across a vast array of design\ntasks. Enabling intuitive, accessible, and scalable design processes is an\nimportant step towards performance-driven and sustainable design for all. To\nthat end, we introduce Architext, a novel semantic generation assistive tool.\nArchitext enables design generation with only natural language prompts, given\nto large-scale Language Models, as input. We conduct a thorough quantitative\nevaluation of Architext's downstream task performance, focusing on semantic\naccuracy and diversity for a number of pre-trained language models ranging from\n120 million to 6 billion parameters. Architext models are able to learn the\nspecific design task, generating valid residential layouts at a near 100\\%\nrate. Accuracy shows great improvement when scaling the models, with the\nlargest model (GPT-J) yielding impressive accuracy ranging between 25% to over\n80% for different prompt categories. We open source the finetuned Architext\nmodels and our synthetic dataset, hoping to inspire experimentation in this\nexciting area of design research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galanos_T/0/1/0/all/0/1\">Theodoros Galanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liapis_A/0/1/0/all/0/1\">Antonios Liapis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yannakakis_G/0/1/0/all/0/1\">Georgios N. Yannakakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio Visual Language Maps for Robot Navigation. (arXiv:2303.07522v1 [cs.RO])","link":"http://arxiv.org/abs/2303.07522","description":"<p>While interacting in the world is a multi-sensory experience, many robots\ncontinue to predominantly rely on visual perception to map and navigate in\ntheir environments. In this work, we propose Audio-Visual-Language Maps\n(AVLMaps), a unified 3D spatial map representation for storing cross-modal\ninformation from audio, visual, and language cues. AVLMaps integrate the\nopen-vocabulary capabilities of multimodal foundation models pre-trained on\nInternet-scale data by fusing their features into a centralized 3D voxel grid.\nIn the context of navigation, we show that AVLMaps enable robot systems to\nindex goals in the map based on multimodal queries, e.g., textual descriptions,\nimages, or audio snippets of landmarks. In particular, the addition of audio\ninformation enables robots to more reliably disambiguate goal locations.\nExtensive experiments in simulation show that AVLMaps enable zero-shot\nmultimodal goal navigation from multimodal prompts and provide 50% better\nrecall in ambiguous scenarios. These capabilities extend to mobile robots in\nthe real world - navigating to landmarks referring to visual, audio, and\nspatial concepts. Videos and code are available at: https://avlmaps.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chenguang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mees_O/0/1/0/all/0/1\">Oier Mees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Andy Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1\">Wolfram Burgard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffusion Models in NLP: A Survey. (arXiv:2303.07576v1 [cs.CL])","link":"http://arxiv.org/abs/2303.07576","description":"<p>Diffusion models have become a powerful family of deep generative models,\nwith record-breaking performance in many applications. This paper first gives\nan overview and derivation of the basic theory of diffusion models, then\nreviews the research results of diffusion models in the field of natural\nlanguage processing, from text generation, text-driven image generation and\nother four aspects, and analyzes and summarizes the relevant literature\nmaterials sorted out, and finally records the experience and feelings of this\ntopic literature review research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuansong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Input-length-shortening and text generation via attention values. (arXiv:2303.07585v1 [cs.CL])","link":"http://arxiv.org/abs/2303.07585","description":"<p>Identifying words that impact a task's performance more than others is a\nchallenge in natural language processing. Transformers models have recently\naddressed this issue by incorporating an attention mechanism that assigns\ngreater attention (i.e., relevance) scores to some words than others. Because\nof the attention mechanism's high computational cost, transformer models\nusually have an input-length limitation caused by hardware constraints. This\nlimitation applies to many transformers, including the well-known bidirectional\nencoder representations of the transformer (BERT) model. In this paper, we\nexamined BERT's attention assignment mechanism, focusing on two questions: (1)\nHow can attention be employed to reduce input length? (2) How can attention be\nused as a control mechanism for conditional text generation? We investigated\nthese questions in the context of a text classification task. We discovered\nthat BERT's early layers assign more critical attention scores for text\nclassification tasks compared to later layers. We demonstrated that the first\nlayer's attention sums could be used to filter tokens in a given sequence,\nconsiderably decreasing the input length while maintaining good test accuracy.\nWe also applied filtering, which uses a compute-efficient semantic similarities\nalgorithm, and discovered that retaining approximately 6\\% of the original\nsequence is sufficient to obtain 86.5\\% accuracy. Finally, we showed that we\ncould generate data in a stable manner and indistinguishable from the original\none by only using a small percentage (10\\%) of the tokens with high attention\nscores according to BERT's first layer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_N/0/1/0/all/0/1\">Ne&#x15f;et &#xd6;zkan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_A/0/1/0/all/0/1\">Alex Yuxuan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bensemann_J/0/1/0/all/0/1\">Joshua Bensemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Q/0/1/0/all/0/1\">Qiming Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartill_T/0/1/0/all/0/1\">Tim Hartill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gahegan_M/0/1/0/all/0/1\">Mark Gahegan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witbrock_M/0/1/0/all/0/1\">Michael Witbrock</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences. (arXiv:2303.07610v1 [cs.CL])","link":"http://arxiv.org/abs/2303.07610","description":"<p>As a natural language assistant, ChatGPT is capable of performing various\ntasks, including but not limited to article generation, code completion, and\ndata analysis. Furthermore, ChatGPT has consistently demonstrated a remarkable\nlevel of accuracy and reliability in terms of content evaluation, exhibiting\nthe capability of mimicking human preferences. To further explore ChatGPT's\npotential in this regard, a study is conducted to assess its ability to rank\ncontent. In order to do so, a test set consisting of prompts is created,\ncovering a wide range of use cases, and five models are utilized to generate\ncorresponding responses. ChatGPT is then instructed to rank the responses\ngenerated by these models. The results on the test set show that ChatGPT's\nranking preferences are consistent with human to a certain extent. This\npreliminary experimental finding implies that ChatGPT's zero-shot ranking\ncapability could be used to reduce annotation pressure in a number of ranking\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yunjie Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yiping Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_C/0/1/0/all/0/1\">Chao Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peiyan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_D/0/1/0/all/0/1\">Dongyu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1\">Baochang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Life Cycle of Knowledge in Big Language Models: A Survey. (arXiv:2303.07616v1 [cs.CL])","link":"http://arxiv.org/abs/2303.07616","description":"<p>Knowledge plays a critical role in artificial intelligence. Recently, the\nextensive success of pre-trained language models (PLMs) has raised significant\nattention about how knowledge can be acquired, maintained, updated and used by\nlanguage models. Despite the enormous amount of related studies, there still\nlacks a unified view of how knowledge circulates within language models\nthroughout the learning, tuning, and application processes, which may prevent\nus from further understanding the connections between current progress or\nrealizing existing limitations. In this survey, we revisit PLMs as\nknowledge-based systems by dividing the life circle of knowledge in PLMs into\nfive critical periods, and investigating how knowledge circulates when it is\nbuilt, maintained and used. To this end, we systematically review existing\nstudies of each period of the knowledge life cycle, summarize the main\nchallenges and current limitations, and discuss future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1\">Boxi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I3D: Transformer architectures with input-dependent dynamic depth for speech recognition. (arXiv:2303.07624v1 [cs.CL])","link":"http://arxiv.org/abs/2303.07624","description":"<p>Transformer-based end-to-end speech recognition has achieved great success.\nHowever, the large footprint and computational overhead make it difficult to\ndeploy these models in some real-world applications. Model compression\ntechniques can reduce the model size and speed up inference, but the compressed\nmodel has a fixed architecture which might be suboptimal. We propose a novel\nTransformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong\nperformance-efficiency trade-offs. With a similar number of layers at inference\ntime, I3D-based models outperform the vanilla Transformer and the static pruned\nmodel via iterative layer pruning. We also present interesting analysis on the\ngate probabilities and the input-dependency, which helps us better understand\ndeep encoders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaesong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Alzheimer's Disease detection based on paralinguistic and pre-trained features. (arXiv:2303.07650v1 [cs.CL])","link":"http://arxiv.org/abs/2303.07650","description":"<p>We present our submission to the ICASSP-SPGC-2023 ADReSS-M Challenge Task,\nwhich aims to investigate which acoustic features can be generalized and\ntransferred across languages for Alzheimer's Disease (AD) prediction. The\nchallenge consists of two tasks: one is to classify the speech of AD patients\nand healthy individuals, and the other is to infer Mini Mental State\nExamination (MMSE) score based on speech only. The difficulty is mainly\nembodied in the mismatch of the dataset, in which the training set is in\nEnglish while the test set is in Greek. We extract paralinguistic features\nusing openSmile toolkit and acoustic features using XLSR-53. In addition, we\nextract linguistic features after transcribing the speech into text. These\nfeatures are used as indicators for AD detection in our method. Our method\nachieves an accuracy of 69.6% on the classification task and a root mean\nsquared error (RMSE) of 4.788 on the regression task. The results show that our\nproposed method is expected to achieve automatic multilingual Alzheimer's\nDisease detection through spontaneous speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuchu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1\">Yu Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei-Qiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RenewNAT: Renewing Potential Translation for Non-Autoregressive Transformer. (arXiv:2303.07665v1 [cs.CL])","link":"http://arxiv.org/abs/2303.07665","description":"<p>Non-autoregressive neural machine translation (NAT) models are proposed to\naccelerate the inference process while maintaining relatively high performance.\nHowever, existing NAT models are difficult to achieve the desired\nefficiency-quality trade-off. For one thing, fully NAT models with efficient\ninference perform inferior to their autoregressive counterparts. For another,\niterative NAT models can, though, achieve comparable performance while\ndiminishing the advantage of speed. In this paper, we propose RenewNAT, a\nflexible framework with high efficiency and effectiveness, to incorporate the\nmerits of fully and iterative NAT models. RenewNAT first generates the\npotential translation results and then renews them in a single pass. It can\nachieve significant performance improvements at the same expense as traditional\nNAT models (without introducing additional model parameters and decoding\nlatency). Experimental results on various translation benchmarks (e.g.,\n\\textbf{4} WMT) show that our framework consistently improves the performance\nof strong fully NAT methods (e.g., GLAT and DSLP) without additional speed\noverhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Pei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yisheng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query2doc: Query Expansion with Large Language Models. (arXiv:2303.07678v1 [cs.IR])","link":"http://arxiv.org/abs/2303.07678","description":"<p>This paper introduces a simple yet effective query expansion approach,\ndenoted as query2doc, to improve both sparse and dense retrieval systems. The\nproposed method first generates pseudo-documents by few-shot prompting large\nlanguage models (LLMs), and then expands the query with generated\npseudo-documents. LLMs are trained on web-scale text corpora and are adept at\nknowledge memorization. The pseudo-documents from LLMs often contain highly\nrelevant information that can aid in query disambiguation and guide the\nretrievers. Experimental results demonstrate that query2doc boosts the\nperformance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and\nTREC DL, without any model fine-tuning. Furthermore, our method also benefits\nstate-of-the-art dense retrievers in terms of both in-domain and out-of-domain\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1\">Nan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QI-TTS: Questioning Intonation Control for Emotional Speech Synthesis. (arXiv:2303.07682v1 [cs.SD])","link":"http://arxiv.org/abs/2303.07682","description":"<p>Recent expressive text to speech (TTS) models focus on synthesizing emotional\nspeech, but some fine-grained styles such as intonation are neglected. In this\npaper, we propose QI-TTS which aims to better transfer and control intonation\nto further deliver the speaker's questioning intention while transferring\nemotion from reference speech. We propose a multi-style extractor to extract\nstyle embedding from two different levels. While the sentence level represents\nemotion, the final syllable level represents intonation. For fine-grained\nintonation control, we use relative attributes to represent intonation\nintensity at the syllable level.Experiments have validated the effectiveness of\nQI-TTS for improving intonation expressiveness in emotional speech synthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haobin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xulong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Ning Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Alignment Mask CTC: Improved Mask-CTC with Aligned Cross Entropy. (arXiv:2303.07687v1 [cs.SD])","link":"http://arxiv.org/abs/2303.07687","description":"<p>Because of predicting all the target tokens in parallel, the\nnon-autoregressive models greatly improve the decoding efficiency of speech\nrecognition compared with traditional autoregressive models. In this work, we\npresent dynamic alignment Mask CTC, introducing two methods: (1) Aligned Cross\nEntropy (AXE), finding the monotonic alignment that minimizes the cross-entropy\nloss through dynamic programming, (2) Dynamic Rectification, creating new\ntraining samples by replacing some masks with model predicted tokens. The AXE\nignores the absolute position alignment between prediction and ground truth\nsentence and focuses on tokens matching in relative order. The dynamic\nrectification method makes the model capable of simulating the non-mask but\npossible wrong tokens, even if they have high confidence. Our experiments on\nWSJ dataset demonstrated that not only AXE loss but also the rectification\nmethod could improve the WER performance of Mask CTC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xulong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haobin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Ning Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Attention Model for Aspect-Level Sentiment Classification. (arXiv:2303.07689v1 [cs.CL])","link":"http://arxiv.org/abs/2303.07689","description":"<p>I propose a novel dual-attention model(DAM) for aspect-level sentiment\nclassification. Many methods have been proposed, such as support vector\nmachines for artificial design features, long short-term memory networks based\non attention mechanisms, and graph neural networks based on dependency parsing.\nWhile these methods all have decent performance, I think they all miss one\nimportant piece of syntactic information: dependency labels. Based on this\nidea, this paper proposes a model using dependency labels for the attention\nmechanism to do this task. We evaluate the proposed approach on three datasets:\nlaptop and restaurant are from SemEval 2014, and the last one is a twitter\ndataset. Experimental results show that the dual attention model has good\nperformance on all three datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Mengfei Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Prosody for Cross-Speaker Style Transfer by Semi-Supervised Style Extractor and Hierarchical Modeling in Speech Synthesis. (arXiv:2303.07711v1 [cs.SD])","link":"http://arxiv.org/abs/2303.07711","description":"<p>Cross-speaker style transfer in speech synthesis aims at transferring a style\nfrom source speaker to synthesized speech of a target speaker's timbre. In most\nprevious methods, the synthesized fine-grained prosody features often represent\nthe source speaker's average style, similar to the one-to-many problem(i.e.,\nmultiple prosody variations correspond to the same text). In response to this\nproblem, a strength-controlled semi-supervised style extractor is proposed to\ndisentangle the style from content and timbre, improving the representation and\ninterpretability of the global style embedding, which can alleviate the\none-to-many mapping and data imbalance problems in prosody prediction. A\nhierarchical prosody predictor is proposed to improve prosody modeling. We find\nthat better style transfer can be achieved by using the source speaker's\nprosody features that are easily predicted. Additionally, a\nspeaker-transfer-wise cycle consistency loss is proposed to assist the model in\nlearning unseen style-timbre combinations during the training phase.\nExperimental results show that the method outperforms the baseline. We provide\na website with audio samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiang_C/0/1/0/all/0/1\">Chunyu Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Peng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_H/0/1/0/all/0/1\">Hao Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaorui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Good Neighbors Are All You Need for Chinese Grapheme-to-Phoneme Conversion. (arXiv:2303.07726v1 [cs.CL])","link":"http://arxiv.org/abs/2303.07726","description":"<p>Most Chinese Grapheme-to-Phoneme (G2P) systems employ a three-stage framework\nthat first transforms input sequences into character embeddings, obtains\nlinguistic information using language models, and then predicts the phonemes\nbased on global context about the entire input sequence. However, linguistic\nknowledge alone is often inadequate. Language models frequently encode overly\ngeneral structures of a sentence and fail to cover specific cases needed to use\nphonetic knowledge. Also, a handcrafted post-processing system is needed to\naddress the problems relevant to the tone of the characters. However, the\nsystem exhibits inconsistency in the segmentation of word boundaries which\nconsequently degrades the performance of the G2P system. To address these\nissues, we propose the Reinforcer that provides strong inductive bias for\nlanguage models by emphasizing the phonological information between neighboring\ncharacters to help disambiguate pronunciations. Experimental results show that\nthe Reinforcer boosts the cutting-edge architectures by a large margin. We also\ncombine the Reinforcer with a large-scale pre-trained model and demonstrate the\nvalidity of using neighboring context in knowledge transfer scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jungjun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Changjin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_G/0/1/0/all/0/1\">Gyuhyeon Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chae_G/0/1/0/all/0/1\">Gyeongsu Chae</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Image-Text Retrieval via Keyword-Guided Pre-Screening. (arXiv:2303.07740v1 [cs.CV])","link":"http://arxiv.org/abs/2303.07740","description":"<p>Under the flourishing development in performance, current image-text\nretrieval methods suffer from $N$-related time complexity, which hinders their\napplication in practice. Targeting at efficiency improvement, this paper\npresents a simple and effective keyword-guided pre-screening framework for the\nimage-text retrieval. Specifically, we convert the image and text data into the\nkeywords and perform the keyword matching across modalities to exclude a large\nnumber of irrelevant gallery samples prior to the retrieval network. For the\nkeyword prediction, we transfer it into a multi-label classification problem\nand propose a multi-task learning scheme by appending the multi-label\nclassifiers to the image-text retrieval network to achieve a lightweight and\nhigh-performance keyword prediction. For the keyword matching, we introduce the\ninverted index in the search engine and create a win-win situation on both time\nand space complexities for the pre-screening. Extensive experiments on two\nwidely-used datasets, i.e., Flickr30K and MS-COCO, verify the effectiveness of\nthe proposed framework. The proposed framework equipped with only two embedding\nlayers achieves $O(1)$ querying time complexity, while improving the retrieval\nefficiency and keeping its performance, when applied prior to the common\nimage-text retrieval methods. Our code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Min Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Ziqiang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"X-ReCoSa: Multi-Scale Context Aggregation For Multi-Turn Dialogue Generation. (arXiv:2303.07833v1 [cs.CL])","link":"http://arxiv.org/abs/2303.07833","description":"<p>In multi-turn dialogue generation, responses are not only related to the\ntopic and background of the context but also related to words and phrases in\nthe sentences of the context. However, currently widely used hierarchical\ndialog models solely rely on context representations from the utterance-level\nencoder, ignoring the sentence representations output by the word-level\nencoder. This inevitably results in a loss of information while decoding and\ngenerating. In this paper, we propose a new dialog model X-ReCoSa to tackle\nthis problem which aggregates multi-scale context information for hierarchical\ndialog models. Specifically, we divide the generation decoder into upper and\nlower parts, namely the intention part and the generation part. Firstly, the\nintention part takes context representations as input to generate the intention\nof the response. Then the generation part generates words depending on sentence\nrepresentations. Therefore, the hierarchical information has been fused into\nresponse generation. we conduct experiments on the English dataset DailyDialog.\nExperimental results exhibit that our method outperforms baseline models on\nboth automatic metric-based and human-based evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Danqin Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geolocation Predicting of Tweets Using BERT-Based Models. (arXiv:2303.07865v1 [cs.CL])","link":"http://arxiv.org/abs/2303.07865","description":"<p>This research is aimed to solve the tweet/user geolocation prediction task\nand provide a flexible methodology for the geotagging of textual big data. The\nsuggested approach implements neural networks for natural language processing\n(NLP) to estimate the location as coordinate pairs (longitude, latitude) and\ntwo-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models\nhas been finetuned on a Twitter dataset using pretrained Bidirectional Encoder\nRepresentations from Transformers (BERT) as base models. Performance metrics\nshow a median error of fewer than 30 km on a worldwide-level, and fewer than 15\nkm on the US-level datasets for the models trained and evaluated on text\nfeatures of tweets' content and metadata context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lutsai_K/0/1/0/all/0/1\">Kateryna Lutsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampert_C/0/1/0/all/0/1\">Christoph H. Lampert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Learnability of In-Context Learning. (arXiv:2303.07895v1 [cs.CL])","link":"http://arxiv.org/abs/2303.07895","description":"<p>In-context learning is a surprising and important phenomenon that emerged\nwhen modern language models were scaled to billions of learned parameters.\nWithout modifying a large language model's weights, it can be tuned to perform\nvarious downstream natural language tasks simply by including concatenated\ntraining examples of these tasks in its input. Though disruptive for many\npractical applications of large language models, this emergent learning\nparadigm is not well understood from a theoretical perspective. In this paper,\nwe propose a first-of-its-kind PAC based framework for in-context learnability,\nand use it to provide the first finite sample complexity results for the\nin-context learning setup. Our framework includes an initial pretraining phase,\nwhich fits a function to the pretraining distribution, and then a second\nin-context learning phase, which keeps this function constant and concatenates\ntraining examples of the downstream task in its input. We use our framework in\norder to prove that, under mild assumptions, when the pretraining distribution\nis a mixture of latent tasks (a model often considered for natural language\npretraining), these tasks can be efficiently learned via in-context learning,\neven though the model's weights are unchanged and the input significantly\ndiverges from the pretraining distribution. Our theoretical analysis reveals\nthat in this setting, in-context learning is more about identifying the task\nthan about learning it, a result which is in line with a series of recent\nempirical findings. We hope that the in-context learnability framework\npresented in this paper will facilitate future progress towards a deeper\nunderstanding of this important new learning paradigm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wies_N/0/1/0/all/0/1\">Noam Wies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1\">Yoav Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1\">Amnon Shashua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting Offline Speech Translation Models for Streaming with Future-Aware Distillation and Inference. (arXiv:2303.07914v1 [cs.CL])","link":"http://arxiv.org/abs/2303.07914","description":"<p>A popular approach to streaming speech translation is to employ a single\noffline model with a \\textit{wait-$k$} policy to support different latency\nrequirements, which is simpler than training multiple online models with\ndifferent latency constraints. However, there is a mismatch problem in using a\nmodel trained with complete utterances for streaming inference with partial\ninput. We demonstrate that speech representations extracted at the end of a\nstreaming input are significantly different from those extracted from a\ncomplete utterance. To address this issue, we propose a new approach called\nFuture-Aware Streaming Translation (FAST) that adapts an offline ST model for\nstreaming input. FAST includes a Future-Aware Inference (FAI) strategy that\nincorporates future context through a trainable masked embedding, and a\nFuture-Aware Distillation (FAD) framework that transfers future context from an\napproximation of full speech to streaming input. Our experiments on the MuST-C\nEnDe, EnEs, and EnFr benchmarks show that FAST achieves better trade-offs\nbetween translation quality and latency than strong baselines. Extensive\nanalyses suggest that our methods effectively alleviate the aforementioned\nmismatch problem between offline training and online inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1\">Biao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_K/0/1/0/all/0/1\">Kai Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1\">Minpeng Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yidong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaodong Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Accented Speech Recognition with Multi-Domain Training. (arXiv:2303.07924v1 [cs.LG])","link":"http://arxiv.org/abs/2303.07924","description":"<p>Thanks to the rise of self-supervised learning, automatic speech recognition\n(ASR) systems now achieve near-human performance on a wide variety of datasets.\nHowever, they still lack generalization capability and are not robust to domain\nshifts like accent variations. In this work, we use speech audio representing\nfour different French accents to create fine-tuning datasets that improve the\nrobustness of pre-trained ASR models. By incorporating various accents in the\ntraining set, we obtain both in-domain and out-of-domain improvements. Our\nnumerical experiments show that we can reduce error rates by up to 25%\n(relative) on African and Belgian accents compared to single-domain training\nwhile keeping a good performance on standard French.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maison_L/0/1/0/all/0/1\">Lucas Maison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteve_Y/0/1/0/all/0/1\">Yannick Est&#xe8;ve</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Theory of Emergent In-Context Learning as Implicit Structure Induction. (arXiv:2303.07971v1 [cs.CL])","link":"http://arxiv.org/abs/2303.07971","description":"<p>Scaling large language models (LLMs) leads to an emergent capacity to learn\nin-context from example demonstrations. Despite progress, theoretical\nunderstanding of this phenomenon remains limited. We argue that in-context\nlearning relies on recombination of compositional operations found in natural\nlanguage data. We derive an information-theoretic bound showing how in-context\nlearning abilities arise from generic next-token prediction when the\npretraining distribution has sufficient amounts of compositional structure,\nunder linguistically motivated assumptions. A second bound provides a\ntheoretical justification for the empirical success of prompting LLMs to output\nintermediate steps towards an answer. To validate theoretical predictions, we\nintroduce a controlled setup for inducing in-context learning; unlike previous\napproaches, it accounts for the compositional nature of language. Trained\ntransformers can perform in-context learning for a range of tasks, in a manner\nconsistent with the theoretical results. Mirroring real-world LLMs in a\nminiature setup, in-context learning emerges when scaling parameters and data,\nand models perform better when prompted to output intermediate steps. Probing\nshows that in-context learning is supported by a representation of the input's\ncompositional structure. Taken together, these results provide a step towards\ntheoretical understanding of emergent behavior in large language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hahn_M/0/1/0/all/0/1\">Michael Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Navin Goyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding the Needle in a Haystack: Unsupervised Rationale Extraction from Long Text Classifiers. (arXiv:2303.07991v1 [cs.CL])","link":"http://arxiv.org/abs/2303.07991","description":"<p>Long-sequence transformers are designed to improve the representation of\nlonger texts by language models and their performance on downstream\ndocument-level tasks. However, not much is understood about the quality of\ntoken-level predictions in long-form models. We investigate the performance of\nsuch architectures in the context of document classification with unsupervised\nrationale extraction. We find standard soft attention methods to perform\nsignificantly worse when combined with the Longformer language model. We\npropose a compositional soft attention architecture that applies RoBERTa\nsentence-wise to extract plausible rationales at the token-level. We find this\nmethod to significantly outperform Longformer-driven baselines on sentiment\nclassification datasets, while also exhibiting significantly lower runtimes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bujel_K/0/1/0/all/0/1\">Kamil Bujel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caines_A/0/1/0/all/0/1\">Andrew Caines</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yannakoudakis_H/0/1/0/all/0/1\">Helen Yannakoudakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_M/0/1/0/all/0/1\">Marek Rei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of ChatGPT as a Question Answering System for Answering Complex Questions. (arXiv:2303.07992v1 [cs.CL])","link":"http://arxiv.org/abs/2303.07992","description":"<p>ChatGPT is a powerful large language model (LLM) that has made remarkable\nprogress in natural language understanding. Nevertheless, the performance and\nlimitations of the model still need to be extensively evaluated. As ChatGPT\ncovers resources such as Wikipedia and supports natural language question\nanswering, it has garnered attention as a potential replacement for traditional\nknowledge based question answering (KBQA) models. Complex question answering is\na challenge task of KBQA, which comprehensively tests the ability of models in\nsemantic parsing and reasoning. To assess the performance of ChatGPT as a\nquestion answering system (QAS) using its own knowledge, we present a framework\nthat evaluates its ability to answer complex questions. Our approach involves\ncategorizing the potential features of complex questions and describing each\ntest question with multiple labels to identify combinatorial reasoning.\nFollowing the black-box testing specifications of CheckList proposed by Ribeiro\net.al, we develop an evaluation method to measure the functionality and\nreliability of ChatGPT in reasoning for answering complex questions. We use the\nproposed framework to evaluate the performance of ChatGPT in question answering\non 8 real-world KB-based CQA datasets, including 6 English and 2 multilingual\ndatasets, with a total of approximately 190,000 test cases. We compare the\nevaluation results of ChatGPT, GPT-3.5, GPT-3, and FLAN-T5 to identify common\nlong-term problems in LLMs. The dataset and code are available at\nhttps://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-ChatGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yiming Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_D/0/1/0/all/0/1\">Dehai Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_N/0/1/0/all/0/1\">Nan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yongrui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guilin Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Efficient Learning of Natural Language to Linear Temporal Logic Translators for Robot Task Specification. (arXiv:2303.08006v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08006","description":"<p>To make robots accessible to a broad audience, it is critical to endow them\nwith the ability to take universal modes of communication, like commands given\nin natural language, and extract a concrete desired task specification, defined\nusing a formal language like linear temporal logic (LTL). In this paper, we\npresent a learning-based approach for translating from natural language\ncommands to LTL specifications with very limited human-labeled training data.\nThis is in stark contrast to existing natural-language to LTL translators,\nwhich require large human-labeled datasets, often in the form of labeled pairs\nof LTL formulas and natural language commands, to train the translator. To\nreduce reliance on human data, our approach generates a large synthetic\ntraining dataset through algorithmic generation of LTL formulas, conversion to\nstructured English, and then exploiting the paraphrasing capabilities of modern\nlarge language models (LLMs) to synthesize a diverse corpus of natural language\ncommands corresponding to the LTL formulas. We use this generated data to\nfinetune an LLM and apply a constrained decoding procedure at inference time to\nensure the returned LTL formula is syntactically correct. We evaluate our\napproach on three existing LTL/natural language datasets and show that we can\ntranslate natural language commands at 75\\% accuracy with far less human data\n($\\le$12 annotations). Moreover, when training on large human-annotated\ndatasets, our method achieves higher test accuracy (95\\% on average) than prior\nwork. Finally, we show the translated formulas can be used to plan\nlong-horizon, multi-stage tasks on a 12D quadrotor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jiayi Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_G/0/1/0/all/0/1\">Glen Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berenson_D/0/1/0/all/0/1\">Dmitry Berenson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does ChatGPT resemble humans in language use?. (arXiv:2303.08014v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08014","description":"<p>Large language models (LLMs) and LLM-driven chatbots such as ChatGPT have\nshown remarkable capacities in comprehending and producing language. However,\ntheir internal workings remain a black box in cognitive terms, and it is\nunclear whether LLMs and chatbots can develop humanlike characteristics in\nlanguage use. Cognitive scientists have devised many experiments that probe,\nand have made great progress in explaining, how people process language. We\nsubjected ChatGPT to 12 of these experiments, pre-registered and with 1,000\nruns per experiment. In 10 of them, ChatGPT replicated the human pattern of\nlanguage use. It associated unfamiliar words with different meanings depending\non their forms, continued to access recently encountered meanings of ambiguous\nwords, reused recent sentence structures, reinterpreted implausible sentences\nthat were likely to have been corrupted by noise, glossed over errors, drew\nreasonable inferences, associated causality with different discourse entities\naccording to verb semantics, and accessed different meanings and retrieved\ndifferent words depending on the identity of its interlocutor. However, unlike\nhumans, it did not prefer using shorter words to convey less informative\ncontent and it did not use context to disambiguate syntactic ambiguities. We\ndiscuss how these convergences and divergences may occur in the transformer\narchitecture. Overall, these experiments demonstrate that LLM-driven chatbots\nlike ChatGPT are capable of mimicking human language processing to a great\nextent, and that they have the potential to provide insights into how people\nlearn and use language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhenguang G. Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haslett_D/0/1/0/all/0/1\">David A. Haslett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_X/0/1/0/all/0/1\">Xufeng Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pickering_M/0/1/0/all/0/1\">Martin J. Pickering</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of Abuse in Financial Transaction Descriptions Using Machine Learning. (arXiv:2303.08016v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08016","description":"<p>Since introducing changes to the New Payments Platform (NPP) to include\nlonger messages as payment descriptions, it has been identified that people are\nnow using it for communication, and in some cases, the system was being used as\na targeted form of domestic and family violence. This type of tech-assisted\nabuse poses new challenges in terms of identification, actions and approaches\nto rectify this behaviour. Commonwealth Bank of Australia's Artificial\nIntelligence Labs team (CBA AI Labs) has developed a new system using advances\nin deep learning models for natural language processing (NLP) to create a\npowerful abuse detector that periodically scores all the transactions, and\nidentifies cases of high-risk abuse in millions of records. In this paper, we\ndescribe the problem of tech-assisted abuse in the context of banking services,\noutline the developed model and its performance, and the operating framework\nmore broadly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leontjeva_A/0/1/0/all/0/1\">Anna Leontjeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richards_G/0/1/0/all/0/1\">Genevieve Richards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sriskandaraja_K/0/1/0/all/0/1\">Kaavya Sriskandaraja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perchman_J/0/1/0/all/0/1\">Jessica Perchman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pizzato_L/0/1/0/all/0/1\">Luiz Pizzato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing Deep Learning Model Parameters with the Bees Algorithm for Improved Medical Text Classification. (arXiv:2303.08021v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08021","description":"<p>This paper introduces a novel mechanism to obtain the optimal parameters of a\ndeep learning model using the Bees Algorithm, which is a recent promising swarm\nintelligence algorithm. The optimization problem is to maximize the accuracy of\nclassifying ailments based on medical text given the initial hyper-parameters\nto be adjusted throughout a definite number of iterations. Experiments included\ntwo different datasets: English and Arabic. The highest accuracy achieved is\n99.63% on the English dataset using Long Short-Term Memory (LSTM) along with\nthe Bees Algorithm, and 88% on the Arabic dataset using AraBERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaaban_M/0/1/0/all/0/1\">Mai A. Shaaban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashkash_M/0/1/0/all/0/1\">Mariam Kashkash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alghfeli_M/0/1/0/all/0/1\">Maryam Alghfeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_A/0/1/0/all/0/1\">Adham Ibrahim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BODEGA: Benchmark for Adversarial Example Generation in Credibility Assessment. (arXiv:2303.08032v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08032","description":"<p>Text classification methods have been widely investigated as a way to detect\ncontent of low credibility: fake news, social media bots, propaganda, etc.\nQuite accurate models (likely based on deep neural networks) help in moderating\npublic electronic platforms and often cause content creators to face rejection\nof their submissions or removal of already published texts. Having the\nincentive to evade further detection, content creators try to come up with a\nslightly modified version of the text (known as an attack with an adversarial\nexample) that exploit the weaknesses of classifiers and result in a different\noutput. Here we introduce BODEGA: a benchmark for testing both victim models\nand attack methods on four misinformation detection tasks in an evaluation\nframework designed to simulate real use-cases of content moderation. We also\nsystematically test the robustness of popular text classifiers against\navailable attacking techniques and discover that, indeed, in some cases barely\nsignificant changes in input text can mislead the models. We openly share the\nBODEGA code and data in hope of enhancing the comparability and replicability\nof further research in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Przybyla_P/0/1/0/all/0/1\">Piotr Przyby&#x142;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shvets_A/0/1/0/all/0/1\">Alexander Shvets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saggion_H/0/1/0/all/0/1\">Horacio Saggion</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models (GPT) Struggle to Answer Multiple-Choice Questions about Code. (arXiv:2303.08033v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08033","description":"<p>We analyzed effectiveness of three generative pre-trained transformer (GPT)\nmodels in answering multiple-choice question (MCQ) assessments, often involving\nshort snippets of code, from introductory and intermediate programming courses\nat the postsecondary level. This emerging technology stirs countless\ndiscussions of its potential uses (e.g., exercise generation, code explanation)\nas well as misuses in programming education (e.g., cheating). However, the\ncapabilities of GPT models and their limitations to reason about and/or analyze\ncode in educational settings have been under-explored. We evaluated several\nOpenAI's GPT models on formative and summative MCQ assessments from three\nPython courses (530 questions). We found that MCQs containing code snippets are\nnot answered as successfully as those that only contain natural language. While\nquestions requiring to fill-in a blank in the code or completing a natural\nlanguage statement about the snippet are handled rather successfully, MCQs that\nrequire analysis and/or reasoning about the code (e.g., what is true/false\nabout the snippet, or what is its output) appear to be the most challenging.\nThese findings can be leveraged by educators to adapt their instructional\npractices and assessments in programming courses, so that GPT becomes a\nvaluable assistant for a learner as opposed to a source of confusion and/or\npotential hindrance in the learning process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Savelka_J/0/1/0/all/0/1\">Jaromir Savelka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Arav Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogart_C/0/1/0/all/0/1\">Christopher Bogart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakr_M/0/1/0/all/0/1\">Majd Sakr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progress Note Understanding -- Assessment and Plan Reasoning: Overview of the 2022 N2C2 Track 3 Shared Task. (arXiv:2303.08038v1 [cs.AI])","link":"http://arxiv.org/abs/2303.08038","description":"<p>Daily progress notes are common types in the electronic health record (EHR)\nwhere healthcare providers document the patient's daily progress and treatment\nplans. The EHR is designed to document all the care provided to patients, but\nit also enables note bloat with extraneous information that distracts from the\ndiagnoses and treatment plans. Applications of natural language processing\n(NLP) in the EHR is a growing field with the majority of methods in information\nextraction. Few tasks use NLP methods for downstream diagnostic decision\nsupport. We introduced the 2022 National NLP Clinical Challenge (N2C2) Track 3:\nProgress Note Understanding - Assessment and Plan Reasoning as one step towards\na new suite of tasks. The Assessment and Plan Reasoning task focuses on the\nmost critical components of progress notes, Assessment and Plan subsections\nwhere health problems and diagnoses are contained. The goal of the task was to\ndevelop and evaluate NLP systems that automatically predict causal relations\nbetween the overall status of the patient contained in the Assessment section\nand its relation to each component of the Plan section which contains the\ndiagnoses and treatment plans. The goal of the task was to identify and\nprioritize diagnoses as the first steps in diagnostic decision support to find\nthe most relevant information in long documents like daily progress notes. We\npresent the results of 2022 n2c2 Track 3 and provide a description of the data,\nevaluation, participation and system performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yanjun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dligach_D/0/1/0/all/0/1\">Dmitriy Dligach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_T/0/1/0/all/0/1\">Timothy Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Churpek_M/0/1/0/all/0/1\">Matthew M Churpek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uzuner_O/0/1/0/all/0/1\">Ozlem Uzuner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afshar_M/0/1/0/all/0/1\">Majid Afshar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TQ-Net: Mixed Contrastive Representation Learning For Heterogeneous Test Questions. (arXiv:2303.08039v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08039","description":"<p>Recently, more and more people study online for the convenience of access to\nmassive learning materials (e.g. test questions/notes), thus accurately\nunderstanding learning materials became a crucial issue, which is essential for\nmany educational applications. Previous studies focus on using language models\nto represent the question data. However, test questions (TQ) are usually\nheterogeneous and multi-modal, e.g., some of them may only contain text, while\nothers half contain images with information beyond their literal description.\nIn this context, both supervised and unsupervised methods are difficult to\nlearn a fused representation of questions. Meanwhile, this problem cannot be\nsolved by conventional methods such as image caption, as the images may contain\ninformation complementary rather than duplicate to the text. In this paper, we\nfirst improve previous text-only representation with a two-stage unsupervised\ninstance level contrastive based pre-training method (MCL: Mixture Unsupervised\nContrastive Learning). Then, TQ-Net was proposed to fuse the content of images\nto the representation of heterogeneous data. Finally, supervised contrastive\nlearning was conducted on relevance prediction-related downstream tasks, which\nhelped the model to learn the representation of questions effectively. We\nconducted extensive experiments on question-based tasks on large-scale,\nreal-world datasets, which demonstrated the effectiveness of TQ-Net and improve\nthe precision of downstream applications (e.g. similar questions +2.02% and\nknowledge point prediction +7.20%). Our code will be available, and we will\nopen-source a subset of our data to promote the development of relative\nstudies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">He Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xihua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xuemin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shan Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Happy-GLL: modular, reusable and complete top-down parsers for parameterized nonterminals. (arXiv:2303.08044v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08044","description":"<p>Parser generators and parser combinator libraries are the most popular tools\nfor producing parsers. Parser combinators use the host language to provide\nreusable components in the form of higher-order functions with parsers as\nparameters. Very few parser generators support this kind of reuse through\nabstraction and even fewer generate parsers that are as modular and reusable as\nthe parts of the grammar for which they are produced. This paper presents a\nstrategy for generating modular, reusable and complete top-down parsers from\nsyntax descriptions with parameterized nonterminals, based on the FUN-GLL\nvariant of the GLL algorithm.\n</p>\n<p>The strategy is discussed and demonstrated as a novel back-end for the Happy\nparser generator. Happy grammars can contain `parameterized nonterminals' in\nwhich parameters abstract over grammar symbols, granting an abstraction\nmechanism to define reusable grammar operators. However, the existing Happy\nback-ends do not deliver on the full potential of parameterized nonterminals as\nparameterized nonterminals cannot be reused across grammars. Moreover, the\nparser generation process may fail to terminate or may result in exponentially\nlarge parsers generated in an exponential amount of time.\n</p>\n<p>The GLL back-end presented in this paper implements parameterized\nnonterminals successfully by generating higher-order functions that resemble\nparser combinators, inheriting all the advantages of top-down parsing. The\nback-end is capable of generating parsers for the full class of context-free\ngrammars, generates parsers in linear time and generates parsers that find all\nderivations of the input string. To our knowledge, the presented GLL back-end\nmakes Happy the first parser generator that combines all these features.\n</p>\n<p>This paper describes the translation procedure of the GLL back-end and\ncompares it to the LALR and GLR back-ends of Happy in several experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Binsbergen_L/0/1/0/all/0/1\">L. Thomas van Binsbergen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frolich_D/0/1/0/all/0/1\">Damian Frolich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Verbal behavior without syntactic structures: beyond Skinner and Chomsky. (arXiv:2303.08080v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08080","description":"<p>What does it mean to know language? Since the Chomskian revolution, one\npopular answer to this question has been: to possess a generative grammar that\nexclusively licenses certain syntactic structures. Decades later, not even an\napproximation to such a grammar, for any language, has been formulated; the\nidea that grammar is universal and innately specified has proved barren; and\nattempts to show how it could be learned from experience invariably come up\nshort. To move on from this impasse, we must rediscover the extent to which\nlanguage is like any other human behavior: dynamic, social, multimodal,\npatterned, and purposive, its purpose being to promote desirable actions (or\nthoughts) in others and self. Recent psychological, computational,\nneurobiological, and evolutionary insights into the shaping and structure of\nbehavior may then point us toward a new, viable account of language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Edelman_S/0/1/0/all/0/1\">Shimon Edelman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simfluence: Modeling the Influence of Individual Training Examples by Simulating Training Runs. (arXiv:2303.08114v1 [cs.LG])","link":"http://arxiv.org/abs/2303.08114","description":"<p>Training data attribution (TDA) methods offer to trace a model's prediction\non any given example back to specific influential training examples. Existing\napproaches do so by assigning a scalar influence score to each training\nexample, under a simplifying assumption that influence is additive. But in\nreality, we observe that training examples interact in highly non-additive ways\ndue to factors such as inter-example redundancy, training order, and curriculum\nlearning effects.\n</p>\n<p>To study such interactions, we propose Simfluence, a new paradigm for TDA\nwhere the goal is not to produce a single influence score per example, but\ninstead a training run simulator: the user asks, ``If my model had trained on\nexample $z_1$, then $z_2$, ..., then $z_n$, how would it behave on\n$z_{test}$?''; the simulator should then output a simulated training run, which\nis a time series predicting the loss on $z_{test}$ at every step of the\nsimulated run. This enables users to answer counterfactual questions about what\ntheir model would have learned under different training curricula, and to\ndirectly see where in training that learning would occur.\n</p>\n<p>We present a simulator, Simfluence-Linear, that captures non-additive\ninteractions and is often able to predict the spiky trajectory of individual\nexample losses with surprising fidelity. Furthermore, we show that existing TDA\nmethods such as TracIn and influence functions can be viewed as special cases\nof Simfluence-Linear. This enables us to directly compare methods in terms of\ntheir simulation accuracy, subsuming several prior TDA approaches to\nevaluation. In experiments on large language model (LLM) fine-tuning, we show\nthat our method predicts loss trajectories with much higher accuracy than\nexisting TDA methods (doubling Spearman's correlation and reducing mean-squared\nerror by 75%) across several tasks, models, and training methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guu_K/0/1/0/all/0/1\">Kelvin Guu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1\">Albert Webson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixon_L/0/1/0/all/0/1\">Lucas Dixon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenney_I/0/1/0/all/0/1\">Ian Tenney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolukbasi_T/0/1/0/all/0/1\">Tolga Bolukbasi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Transformers Parse while Predicting the Masked Word?. (arXiv:2303.08117v1 [cs.CL])","link":"http://arxiv.org/abs/2303.08117","description":"<p>Pre-trained language models have been shown to encode linguistic structures,\ne.g. dependency and constituency parse trees, in their embeddings while being\ntrained on unsupervised loss functions like masked language modeling. Some\ndoubts have been raised whether the models actually are doing parsing or only\nsome computation weakly correlated with it. We study questions: (a) Is it\npossible to explicitly describe transformers with realistic embedding\ndimension, number of heads, etc. that are capable of doing parsing -- or even\napproximate parsing? (b) Why do pre-trained models capture parsing structure?\nThis paper takes a step toward answering these questions in the context of\ngenerative modeling with PCFGs. We show that masked language models like BERT\nor RoBERTa of moderate sizes can approximately execute the Inside-Outside\nalgorithm for the English PCFG [Marcus et al, 1993]. We also show that the\nInside-Outside algorithm is optimal for masked language modeling loss on the\nPCFG-generated data. We also give a construction of transformers with $50$\nlayers, $15$ attention heads, and $1275$ dimensional embeddings in average such\nthat using its embeddings it is possible to do constituency parsing with\n$&gt;70\\%$ F1 score on PTB dataset. We conduct probing experiments on models\npre-trained on PCFG-generated data to show that this not only allows recovery\nof approximate parse tree, but also recovers marginal span probabilities\ncomputed by the Inside-Outside algorithm, which suggests an implicit bias of\nmasked language modeling towards this algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haoyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panigrahi_A/0/1/0/all/0/1\">Abhishek Panigrahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1\">Rong Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Sanjeev Arora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CB2: Collaborative Natural Language Interaction Research Platform. (arXiv:2303.08127v1 [cs.LG])","link":"http://arxiv.org/abs/2303.08127","description":"<p>CB2 is a multi-agent platform to study collaborative natural language\ninteraction in a grounded task-oriented scenario. It includes a 3D game\nenvironment, a backend server designed to serve trained models to human agents,\nand various tools and processes to enable scalable studies. We deploy CB2 at\nhttps://cb2.ai as a system demonstration with a learned instruction following\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharf_J/0/1/0/all/0/1\">Jacob Sharf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gul_M/0/1/0/all/0/1\">Mustafa Omer Gul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph. (arXiv:2101.06397v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.06397","description":"<p>In this paper, we propose an explanation of representation for self-attention\nnetwork (SAN) based neural sequence encoders, which regards the information\ncaptured by the model and the encoding of the model as graph structure and the\ngeneration of these graph structures respectively. The proposed explanation\napplies to existing works on SAN-based models and can explain the relationship\namong the ability to capture the structural or linguistic information, depth of\nmodel, and length of sentence, and can also be extended to other models such as\nrecurrent neural network based models. We also propose a revisited multigraph\ncalled Multi-order-Graph (MoG) based on our explanation to model the graph\nstructures in the SAN-based model as subgraphs in MoG and convert the encoding\nof SAN-based model to the generation of MoG. Based on our explanation, we\nfurther introduce a Graph-Transformer by enhancing the ability to capture\nmultiple subgraphs of different orders and focusing on subgraphs of high\norders. Experimental results on multiple neural machine translation tasks show\nthat the Graph-Transformer can yield effective performance improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1\">Sufeng Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Thought Flow Nets: From Single Predictions to Trains of Model Thought. (arXiv:2107.12220v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.12220","description":"<p>When humans solve complex problems, they typically create a sequence of ideas\n(involving an intuitive decision, reflection, error correction, etc.) in order\nto reach a conclusive decision. Contrary to this, today's models are mostly\ntrained to map an input to one single and fixed output. In this paper, we\ninvestigate how we can give models the opportunity of a second, third and\n$k$-th thought. Taking inspiration from Hegel's dialectics, we propose the\nconcept of a thought flow which creates a sequence of predictions. We present a\nself-correction mechanism that is trained to estimate the model's correctness\nand performs iterative prediction updates based on the correctness prediction's\ngradient. We introduce our method at the example of question answering and\nconduct extensive experiments that demonstrate (i) our method's ability to\ncorrect its own predictions and (ii) its potential to notably improve model\nperformances. In addition, we conduct a qualitative analysis of thought flow\ncorrection patterns and explore how thought flow predictions affect human users\nwithin a crowdsourcing study. We find that (iii) thought flows enable improved\nuser performance and are perceived as more natural, correct, and intelligent as\nsingle and/or top-3 predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schuff_H/0/1/0/all/0/1\">Hendrik Schuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pretrained Language Models are Symbolic Mathematics Solvers too!. (arXiv:2110.03501v3 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2110.03501","description":"<p>Solving symbolic mathematics has always been of in the arena of human\ningenuity that needs compositional reasoning and recurrence. However, recent\nstudies have shown that large-scale language models such as transformers are\nuniversal and surprisingly can be trained as a sequence-to-sequence task to\nsolve complex mathematical equations. These large transformer models need\nhumongous amounts of training data to generalize to unseen symbolic mathematics\nproblems. In this paper, we present a sample efficient way of solving the\nsymbolic tasks by first pretraining the transformer model with language\ntranslation and then fine-tuning the pretrained transformer model to solve the\ndownstream task of symbolic mathematics. We achieve comparable accuracy on the\nintegration task with our pretrained model while using around $1.5$ orders of\nmagnitude less number of training samples with respect to the state-of-the-art\ndeep learning for symbolic mathematics. The test accuracy on differential\nequation tasks is considerably lower comparing with integration as they need\nhigher order recursions that are not present in language translations. We\npropose the generalizability of our pretrained language model from Anna\nKarenina Principle (AKP). We pretrain our model with different pairs of\nlanguage translations. Our results show language bias in solving symbolic\nmathematics tasks. Finally, we study the robustness of the fine-tuned model on\nsymbolic math tasks against distribution shift, and our approach generalizes\nbetter in distribution shift scenarios for the function integration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Noorbakhsh_K/0/1/0/all/0/1\">Kimia Noorbakhsh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sulaiman_M/0/1/0/all/0/1\">Modar Sulaiman</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sharifi_M/0/1/0/all/0/1\">Mahdi Sharifi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Roy_K/0/1/0/all/0/1\">Kallol Roy</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jamshidi_P/0/1/0/all/0/1\">Pooyan Jamshidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DECAR: Deep Clustering for learning general-purpose Audio Representations. (arXiv:2110.08895v4 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2110.08895","description":"<p>We introduce DECAR, a self-supervised pre-training approach for learning\ngeneral-purpose audio representations. Our system is based on clustering: it\nutilizes an offline clustering step to provide target labels that act as\npseudo-labels for solving a prediction task. We develop on top of recent\nadvances in self-supervised learning for computer vision and design a\nlightweight, easy-to-use self-supervised pre-training scheme. We pre-train\nDECAR embeddings on a balanced subset of the large-scale Audioset dataset and\ntransfer those representations to 9 downstream classification tasks, including\nspeech, music, animal sounds, and acoustic scenes. Furthermore, we conduct\nablation studies identifying key design choices and also make all our code and\npre-trained models publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katta_S/0/1/0/all/0/1\">Sandesh V Katta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seth_A/0/1/0/all/0/1\">Ashish Seth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph Augmented Network Towards Multiview Representation Learning for Aspect-based Sentiment Analysis. (arXiv:2201.04831v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.04831","description":"<p>Aspect-based sentiment analysis (ABSA) is a fine-grained task of sentiment\nanalysis. To better comprehend long complicated sentences and obtain accurate\naspect-specific information, linguistic and commonsense knowledge are generally\nrequired in this task. However, most current methods employ complicated and\ninefficient approaches to incorporate external knowledge, e.g., directly\nsearching the graph nodes. Additionally, the complementarity between external\nknowledge and linguistic information has not been thoroughly studied. To this\nend, we propose a knowledge graph augmented network KGAN, which aims to\neffectively incorporate external knowledge with explicitly syntactic and\ncontextual information. In particular, KGAN captures the sentiment feature\nrepresentations from multiple different perspectives, i.e., context-, syntax-\nand knowledge-based. First, KGAN learns the contextual and syntactic\nrepresentations in parallel to fully extract the semantic features. Then, KGAN\nintegrates the knowledge graphs into the embedding space, based on which the\naspect-specific knowledge representations are further obtained via an attention\nmechanism. Last, we propose a hierarchical fusion module to complement these\nmulti-view representations in a local-to-global manner. Extensive experiments\non five popular ABSA benchmarks demonstrate the effectiveness and robustness of\nour KGAN. Notably, with the help of the pretrained model of RoBERTa, KGAN\nachieves a new record of state-of-the-art performance among all datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1\">Qihuang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Juhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer. (arXiv:2202.02113v7 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.02113","description":"<p>Knowledge graph completion aims to address the problem of extending a KG with\nmissing triples. In this paper, we provide an approach GenKGC, which converts\nknowledge graph completion to sequence-to-sequence generation task with the\npre-trained language model. We further introduce relation-guided demonstration\nand entity-aware hierarchical decoding for better representation learning and\nfast inference. Experimental results on three datasets show that our approach\ncan obtain better or comparable performance than baselines and achieve faster\ninference speed compared with previous methods with pre-trained language\nmodels. We also release a new large-scale Chinese knowledge graph dataset\nAliopenKG500 for research purpose. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKG/tree/main/GenKGC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoubo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relphormer: Relational Graph Transformer for Knowledge Graph Representations. (arXiv:2205.10852v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10852","description":"<p>Transformers have achieved remarkable performance in widespread fields,\nincluding natural language processing, computer vision and graph mining.\nHowever, vanilla Transformer architectures have not yielded promising\nimprovements in the Knowledge Graph (KG) representations, where the\ntranslational distance paradigm dominates this area. Note that vanilla\nTransformer architectures struggle to capture the intrinsically heterogeneous\nstructural and semantic information of knowledge graphs. To this end, we\npropose a new variant of Transformer for knowledge graph representations dubbed\nRelphormer. Specifically, we introduce Triple2Seq which can dynamically sample\ncontextualized sub-graph sequences as the input to alleviate the heterogeneity\nissue. We propose a novel structure-enhanced self-attention mechanism to encode\nthe relational information and keep the semantic information within entities\nand relations. Moreover, we utilize masked knowledge modeling for general\nknowledge graph representation learning, which can be applied to various\nKG-based tasks including knowledge graph completion, question answering, and\nrecommendation. Experimental results on six datasets show that Relphormer can\nobtain better performance compared with baselines. Code is available in\nhttps://github.com/zjunlp/Relphormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving CTC-based ASR Models with Gated Interlayer Collaboration. (arXiv:2205.12462v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12462","description":"<p>The CTC-based automatic speech recognition (ASR) models without the external\nlanguage model usually lack the capacity to model conditional dependencies and\ntextual interactions. In this paper, we present a Gated Interlayer\nCollaboration (GIC) mechanism to improve the performance of CTC-based models,\nwhich introduces textual information into the model and thus relaxes the\nconditional independence assumption of CTC-based models. Specifically, we\nconsider the weighted sum of token embeddings as the textual representation for\neach position, where the position-specific weights are the softmax probability\ndistribution constructed via inter-layer auxiliary CTC losses. The textual\nrepresentations are then fused with acoustic features by developing a gate\nunit. Experiments on AISHELL-1, TEDLIUM2, and AIDATATANG corpora show that the\nproposed method outperforms several strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuting Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Binbin Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human heuristics for AI-generated language are flawed. (arXiv:2206.07271v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.07271","description":"<p>Human communication is increasingly intermixed with language generated by AI.\nAcross chat, email, and social media, AI systems suggest words, complete\nsentences, or produce entire conversations. AI-generated language is often not\nidentified as such but presented as language written by humans, raising\nconcerns about novel forms of deception and manipulation. Here, we study how\nhumans discern whether verbal self-presentations, one of the most personal and\nconsequential forms of language, were generated by AI. In six experiments,\nparticipants (N = 4,600) were unable to detect self-presentations generated by\nstate-of-the-art AI language models in professional, hospitality, and dating\ncontexts. A computational analysis of language features shows that human\njudgments of AI-generated language are hindered by intuitive but flawed\nheuristics such as associating first-person pronouns, use of contractions, or\nfamily topics with human-written language. We experimentally demonstrate that\nthese heuristics make human judgment of AI-generated language predictable and\nmanipulable, allowing AI systems to produce text perceived as \"more human than\nhuman.\" We discuss solutions, such as AI accents, to reduce the deceptive\npotential of language generated by AI, limiting the subversion of human\nintuition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jakesch_M/0/1/0/all/0/1\">Maurice Jakesch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hancock_J/0/1/0/all/0/1\">Jeffrey Hancock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naaman_M/0/1/0/all/0/1\">Mor Naaman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ordinal analysis of lexical patterns. (arXiv:2208.11175v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.11175","description":"<p>Words are fundamental linguistic units that connect thoughts and things\nthrough meaning. However, words do not appear independently in a text sequence.\nThe existence of syntactic rules induces correlations among neighboring words.\nUsing an ordinal pattern approach, we present an analysis of lexical\nstatistical connections for 11 major languages. We find that the diverse\nmanners that languages utilize to express word relations give rise to unique\npattern structural distributions. Furthermore, fluctuations of these pattern\ndistributions for a given language can allow us to determine both the\nhistorical period when the text was written and its author. Taken together, our\nresults emphasize the relevance of ordinal time series analysis in linguistic\ntypology, historical linguistics and stylometry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_D/0/1/0/all/0/1\">David Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zunino_L/0/1/0/all/0/1\">Luciano Zunino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gregorio_J/0/1/0/all/0/1\">Juan De Gregorio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toral_R/0/1/0/all/0/1\">Raul Toral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirasso_C/0/1/0/all/0/1\">Claudio Mirasso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural inhibition during speech planning contributes to contrastive hyperarticulation. (arXiv:2209.12278v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.12278","description":"<p>Previous work has demonstrated that words are hyperarticulated on dimensions\nof speech that differentiate them from a minimal pair competitor. This\nphenomenon has been termed contrastive hyperarticulation (CH). We present a\ndynamic neural field (DNF) model of voice onset time (VOT) planning that\nderives CH from an inhibitory influence of the minimal pair competitor during\nplanning. We test some predictions of the model with a novel experiment\ninvestigating CH of voiceless stop consonant VOT in pseudowords. The results\ndemonstrate a CH effect in pseudowords, consistent with a basis for the effect\nin the real-time planning and production of speech. The scope and magnitude of\nCH in pseudowords was reduced compared to CH in real words, consistent with a\nrole for interactive activation between lexical and phonological levels of\nplanning. We discuss the potential of our model to unify an apparently\ndisparate set of phenomena, from CH to phonological neighborhood effects to\nphonetic trace effects in speech errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stern_M/0/1/0/all/0/1\">Michael C. Stern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_J/0/1/0/all/0/1\">Jason A. Shaw</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Speech Translation with Dynamic Latent Perceivers. (arXiv:2210.16264v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.16264","description":"<p>Transformers have been the dominant architecture for Speech Translation in\nrecent years, achieving significant improvements in translation quality. Since\nspeech signals are longer than their textual counterparts, and due to the\nquadratic complexity of the Transformer, a down-sampling step is essential for\nits adoption in Speech Translation. Instead, in this research, we propose to\nease the complexity by using a Perceiver encoder to map the speech inputs to a\nfixed-length latent representation. Furthermore, we introduce a novel way of\ntraining Perceivers, with Dynamic Latent Access (DLA), unlocking larger latent\nspaces without any additional computational overhead. Speech-to-Text Perceivers\nwith DLA can match the performance of Transformer baselines across three\nlanguage pairs in MuST-C. Finally, a DLA-trained model is easily adaptable to\nDLA at inference, and can be flexibly deployed with various computational\nbudgets, without significant drops in translation quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsiamas_I/0/1/0/all/0/1\">Ioannis Tsiamas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Gerard I. G&#xe1;llego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonollosa_J/0/1/0/all/0/1\">Jos&#xe9; A. R. Fonollosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Suffix Retrieval-Augmented Language Modeling. (arXiv:2211.03053v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.03053","description":"<p>Causal language modeling (LM) uses word history to predict the next word.\nBERT, on the other hand, makes use of bi-directional word information in a\nsentence to predict words at masked positions. While BERT is effective in\nsequence encoding, it is non-causal by nature and is not designed for sequence\ngeneration. In this paper, we propose a novel language model, SUffix\nREtrieval-Augmented LM (SUREALM), that simulates a bi-directional contextual\neffect in an autoregressive manner. SUREALM employs an embedding retriever to\nsearch for training sentences in a data store that share similar word history\nduring sequence generation. In particular, the suffix portions of the retrieved\nsentences mimick the \"future\" context. We evaluated our proposed model on the\nDSTC9 spoken dialogue corpus and showed promising word perplexity reduction on\nthe validation and test set compared to competitive baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zecheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tam_Y/0/1/0/all/0/1\">Yik-Cheung Tam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Relation Discovery: Towards General and Label-aware Open Relation Extraction. (arXiv:2211.04215v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.04215","description":"<p>Open Relation Extraction (OpenRE) aims to discover novel relations from open\ndomains. Previous OpenRE methods mainly suffer from two problems: (1)\nInsufficient capacity to discriminate between known and novel relations. When\nextending conventional test settings to a more general setting where test data\nmight also come from seen classes, existing approaches have a significant\nperformance decline. (2) Secondary labeling must be performed before practical\napplication. Existing methods cannot label human-readable and meaningful types\nfor novel relations, which is urgently required by the downstream tasks. To\naddress these issues, we propose the Active Relation Discovery (ARD) framework,\nwhich utilizes relational outlier detection for discriminating known and novel\nrelations and involves active learning for labeling novel relations. Extensive\nexperiments on three real-world datasets show that ARD significantly\noutperforms previous state-of-the-art methods on both conventional and our\nproposed general OpenRE settings. The source code and datasets will be\navailable for reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hong-Gee Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Navigation as Attackers Wish? Towards Building Byzantine-Robust Embodied Agents under Federated Learning. (arXiv:2211.14769v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2211.14769","description":"<p>Federated embodied agent learning protects the data privacy of individual\nvisual environments by keeping data locally at each client (the individual\nenvironment) during training. However, since the local data is inaccessible to\nthe server under federated learning, attackers may easily poison the training\ndata of the local client to build a backdoor in the agent without notice.\nDeploying such an agent raises the risk of potential harm to humans, as the\nattackers may easily navigate and control the agent as they wish via the\nbackdoor. Towards Byzantine-robust federated embodied agent learning, in this\npaper, we study the attack and defense for the task of vision-and-language\nnavigation (VLN), where the agent is required to follow natural language\ninstructions to navigate indoor environments. First, we introduce a simple but\neffective attack strategy, Navigation as Wish (NAW), in which the malicious\nclient manipulates local trajectory data to implant a backdoor into the global\nmodel. Results on two VLN datasets (R2R and RxR) show that NAW can easily\nnavigate the deployed VLN agent regardless of the language instruction, without\naffecting its performance on normal test sets. Then, we propose a new\nPrompt-Based Aggregation (PBA) to defend against the NAW attack in federated\nVLN, which provides the server with a ''prompt'' of the vision-and-language\nalignment variance between the benign and malicious clients so that they can be\ndistinguished during training. We validate the effectiveness of the PBA method\non protecting the global model from the NAW attack, which outperforms other\nstate-of-the-art defense methods by a large margin in the defense metrics on\nR2R and RxR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Di_Z/0/1/0/all/0/1\">Zonglin Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiwen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cihang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SuS-X: Training-Free Name-Only Transfer of Vision-Language Models. (arXiv:2211.16198v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.16198","description":"<p>Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet\neffective way to train large-scale vision-language models. CLIP demonstrates\nimpressive zero-shot classification and retrieval on diverse downstream tasks.\nHowever, to leverage its full potential, fine-tuning still appears to be\nnecessary. Fine-tuning the entire CLIP model can be resource-intensive and\nunstable. Moreover, recent methods that aim to circumvent this need for\nfine-tuning still require access to images from the target distribution. In\nthis paper, we pursue a different approach and explore the regime of\ntraining-free \"name-only transfer\" in which the only knowledge we possess about\nthe downstream task comprises the names of downstream target categories. We\npropose a novel method, SuS-X, consisting of two key building blocks -- SuS and\nTIP-X, that requires neither intensive fine-tuning nor costly labelled data.\nSuS-X achieves state-of-the-art zero-shot classification results on 19\nbenchmark datasets. We further show the utility of TIP-X in the training-free\nfew-shot setting, where we again achieve state-of-the-art results over strong\ntraining-free baselines. Code is available at\nhttps://github.com/vishaal27/SuS-X.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Udandarao_V/0/1/0/all/0/1\">Vishaal Udandarao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ankush Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1\">Samuel Albanie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Concept Knowledge Graph for User Next Intent Prediction at Alipay. (arXiv:2301.00503v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.00503","description":"<p>This paper illustrates the technologies of user next intent prediction with a\nconcept knowledge graph. The system has been deployed on the Web at Alipay,\nserving more than 100 million daily active users. To explicitly characterize\nuser intent, we propose AlipayKG, which is an offline concept knowledge graph\nin the Life-Service domain modeling the historical behaviors of users, the rich\ncontent interacted by users and the relations between them. We further\nintroduce a Transformer-based model which integrates expert rules from the\nknowledge graph to infer the online user's next intent. Experimental results\ndemonstrate that the proposed system can effectively enhance the performance of\nthe downstream tasks while retaining explainability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yacheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Q/0/1/0/all/0/1\">Qianghuai Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruopeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1\">Yixin Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TriNet: stabilizing self-supervised learning from complete or slow collapse on ASR. (arXiv:2301.00656v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2301.00656","description":"<p>Self-supervised learning (SSL) models confront challenges of abrupt\ninformational collapse or slow dimensional collapse. We propose TriNet, which\nintroduces a novel triple-branch architecture for preventing collapse and\nstabilizing the pre-training. TriNet learns the SSL latent embedding space and\nincorporates it to a higher level space for predicting pseudo target vectors\ngenerated by a frozen teacher. Our experimental results show that the proposed\nmethod notably stabilizes and accelerates pre-training and achieves a relative\nword error rate reduction (WERR) of 6.06% compared to the state-of-the-art\n(SOTA) Data2vec for a downstream benchmark ASR task. We will release our code\nat https://github.com/tencent-ailab/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cao_L/0/1/0/all/0/1\">Lixin Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_B/0/1/0/all/0/1\">Ben Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLADIS: A General and Large Acronym Disambiguation Benchmark. (arXiv:2302.01860v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.01860","description":"<p>Acronym Disambiguation (AD) is crucial for natural language understanding on\nvarious sources, including biomedical reports, scientific papers, and search\nengine queries. However, existing acronym disambiguation benchmarks and tools\nare limited to specific domains, and the size of prior benchmarks is rather\nsmall. To accelerate the research on acronym disambiguation, we construct a new\nbenchmark named GLADIS with three components: (1) a much larger acronym\ndictionary with 1.5M acronyms and 6.4M long forms; (2) a pre-training corpus\nwith 160 million sentences; (3) three datasets that cover the general,\nscientific, and biomedical domains. We then pre-train a language model,\n\\emph{AcroBERT}, on our constructed corpus for general acronym disambiguation,\nand show the challenges and values of our new benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lihu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varoquaux_G/0/1/0/all/0/1\">Ga&#xeb;l Varoquaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suchanek_F/0/1/0/all/0/1\">Fabian M. Suchanek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks. (arXiv:2302.08399v5 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2302.08399","description":"<p>Intuitive psychology is a pillar of common-sense reasoning. The replication\nof this reasoning in machine intelligence is an important stepping-stone on the\nway to human-like artificial intelligence. Several recent tasks and benchmarks\nfor examining this reasoning in Large-Large Models have focused in particular\non belief attribution in Theory-of-Mind tasks. These tasks have shown both\nsuccesses and failures. We consider in particular a recent purported success\ncase, and show that small variations that maintain the principles of ToM turn\nthe results on their head. We argue that in general, the zero-hypothesis for\nmodel evaluation in intuitive psychology should be skeptical, and that outlying\nfailure cases should outweigh average success rates. We also consider what\npossible future successes on Theory-of-Mind tasks by more powerful LLMs would\nmean for ToM tasks with people.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ullman_T/0/1/0/all/0/1\">Tomer Ullman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time-aware Multiway Adaptive Fusion Network for Temporal Knowledge Graph Question Answering. (arXiv:2302.12529v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.12529","description":"<p>Knowledge graphs (KGs) have received increasing attention due to its wide\napplications on natural language processing. However, its use case on temporal\nquestion answering (QA) has not been well-explored. Most of existing methods\nare developed based on pre-trained language models, which might not be capable\nto learn \\emph{temporal-specific} presentations of entities in terms of\ntemporal KGQA task. To alleviate this problem, we propose a novel\n\\textbf{T}ime-aware \\textbf{M}ultiway \\textbf{A}daptive (\\textbf{TMA}) fusion\nnetwork. Inspired by the step-by-step reasoning behavior of humans. For each\ngiven question, TMA first extracts the relevant concepts from the KG, and then\nfeeds them into a multiway adaptive module to produce a\n\\emph{temporal-specific} representation of the question. This representation\ncan be incorporated with the pre-trained KG embedding to generate the final\nprediction. Empirical results verify that the proposed model achieves better\nperformance than the state-of-the-art models in the benchmark dataset. Notably,\nthe Hits@1 and Hits@10 results of TMA on the CronQuestions dataset's complex\nquestions are absolutely improved by 24\\% and 10\\% compared to the\nbest-performing baseline. Furthermore, we also show that TMA employing an\nadaptive fusion mechanism can provide interpretability by analyzing the\nproportion of information in question representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yonghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Di Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1\">Fang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Rui Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Path Modeling for Semantic Matching by Perceiving Subtle Conflicts. (arXiv:2302.12530v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.12530","description":"<p>Transformer-based pre-trained models have achieved great improvements in\nsemantic matching. However, existing models still suffer from insufficient\nability to capture subtle differences. The modification, addition and deletion\nof words in sentence pairs may make it difficult for the model to predict their\nrelationship. To alleviate this problem, we propose a novel Dual Path Modeling\nFramework to enhance the model's ability to perceive subtle differences in\nsentence pairs by separately modeling affinity and difference semantics. Based\non dual-path modeling framework we design the Dual Path Modeling Network\n(DPM-Net) to recognize semantic relations. And we conduct extensive experiments\non 10 well-studied semantic matching and robustness test datasets, and the\nexperimental results show that our proposed method achieves consistent\nimprovements over baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1\">Chao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Di Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification. (arXiv:2303.07142v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.07142","description":"<p>This case study investigates the task of job classification in a real-world\nsetting, where the goal is to determine whether an English-language job posting\nis appropriate for a graduate or entry-level position. We explore multiple\napproaches to text classification, including supervised approaches such as\ntraditional models like Support Vector Machines (SVMs) and state-of-the-art\ndeep learning methods such as DeBERTa. We compare them with Large Language\nModels (LLMs) used in both few-shot and zero-shot classification settings. To\naccomplish this task, we employ prompt engineering, a technique that involves\ndesigning prompts to guide the LLMs towards the desired output. Specifically,\nwe evaluate the performance of two commercially available state-of-the-art\nGPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also\nconduct a detailed analysis of the impact of different aspects of prompt\nengineering on the model's performance. Our results show that, with a\nwell-designed prompt, a zero-shot gpt-3.5-turbo classifier outperforms all\nother models, achieving a 6% increase in Precision@95% Recall compared to the\nbest supervised approach. Furthermore, we observe that the wording of the\nprompt is a critical factor in eliciting the appropriate \"reasoning\" in the\nmodel, and that seemingly minor aspects of the prompt significantly affect the\nmodel's performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clavie_B/0/1/0/all/0/1\">Benjamin Clavi&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciceu_A/0/1/0/all/0/1\">Alexandru Ciceu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naylor_F/0/1/0/all/0/1\">Frederick Naylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soulie_G/0/1/0/all/0/1\">Guillaume Souli&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brightwell_T/0/1/0/all/0/1\">Thomas Brightwell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Models Trained on Indian Legal Data Fair?. (arXiv:2303.07247v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.07247","description":"<p>Recent advances and applications of language technology and artificial\nintelligence have enabled much success across multiple domains like law,\nmedical and mental health. AI-based Language Models, like Judgement Prediction,\nhave recently been proposed for the legal sector. However, these models are\nstrife with encoded social biases picked up from the training data. While bias\nand fairness have been studied across NLP, most studies primarily locate\nthemselves within a Western context. In this work, we present an initial\ninvestigation of fairness from the Indian perspective in the legal domain. We\nhighlight the propagation of learnt algorithmic biases in the bail prediction\ntask for models trained on Hindi legal documents. We evaluate the fairness gap\nusing demographic parity and show that a decision tree model trained for the\nbail prediction task has an overall fairness disparity of 0.237 between input\nfeatures associated with Hindus and Muslims. Additionally, we highlight the\nneed for further research and studies in the avenues of fairness/bias in\napplying AI in the legal sector with a specific focus on the Indian context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Girhepuje_S/0/1/0/all/0/1\">Sahil Girhepuje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_A/0/1/0/all/0/1\">Anmol Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_G/0/1/0/all/0/1\">Gokul S Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_S/0/1/0/all/0/1\">Shreya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_S/0/1/0/all/0/1\">Satyendra Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1\">Ponnurangam Kumaraguru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravindran_B/0/1/0/all/0/1\">Balaraman Ravindran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-03-14T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}