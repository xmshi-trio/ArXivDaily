{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-04-21T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"The eBible Corpus: Data and Model Benchmarks for Bible Translation for Low-Resource Languages. (arXiv:2304.09919v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09919","description":"<p>Efficiently and accurately translating a corpus into a low-resource language\nremains a challenge, regardless of the strategies employed, whether manual,\nautomated, or a combination of the two. Many Christian organizations are\ndedicated to the task of translating the Holy Bible into languages that lack a\nmodern translation. Bible translation (BT) work is currently underway for over\n3000 extremely low resource languages. We introduce the eBible corpus: a\ndataset containing 1009 translations of portions of the Bible with data in 833\ndifferent languages across 75 language families. In addition to a BT\nbenchmarking dataset, we introduce model performance benchmarks built on the No\nLanguage Left Behind (NLLB) neural machine translation (NMT) models. Finally,\nwe describe several problems specific to the domain of BT and consider how the\nestablished data and model benchmarks might be used for future translation\nefforts. For a BT task trained with NLLB, Austronesian and Trans-New Guinea\nlanguage families achieve 35.1 and 31.6 BLEU scores respectively, which spurs\nfuture innovations for NMT for low-resource languages in Papua New Guinea.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akerman_V/0/1/0/all/0/1\">Vesa Akerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baines_D/0/1/0/all/0/1\">David Baines</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daspit_D/0/1/0/all/0/1\">Damien Daspit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermjakob_U/0/1/0/all/0/1\">Ulf Hermjakob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_T/0/1/0/all/0/1\">Taeho Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leong_C/0/1/0/all/0/1\">Colin Leong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_M/0/1/0/all/0/1\">Michael Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1\">Joel Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robie_J/0/1/0/all/0/1\">Jonathan Robie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwarting_M/0/1/0/all/0/1\">Marcus Schwarting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Catch Me If You Can: Identifying Fraudulent Physician Reviews with Large Language Models Using Generative Pre-Trained Transformers. (arXiv:2304.09948v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09948","description":"<p>The proliferation of fake reviews of doctors has potentially detrimental\nconsequences for patient well-being and has prompted concern among consumer\nprotection groups and regulatory bodies. Yet despite significant advancements\nin the fields of machine learning and natural language processing, there\nremains limited comprehension of the characteristics differentiating fraudulent\nfrom authentic reviews. This study utilizes a novel pre-labeled dataset of\n38048 physician reviews to establish the effectiveness of large language models\nin classifying reviews. Specifically, we compare the performance of traditional\nML models, such as logistic regression and support vector machines, to\ngenerative pre-trained transformer models. Furthermore, we use GPT4, the newest\nmodel in the GPT family, to uncover the key dimensions along which fake and\ngenuine physician reviews differ. Our findings reveal significantly superior\nperformance of GPT-3 over traditional ML models in this context. Additionally,\nour analysis suggests that GPT3 requires a smaller training sample than\ntraditional models, suggesting its appropriateness for tasks with scarce\ntraining data. Moreover, the superiority of GPT3 performance increases in the\ncold start context i.e., when there are no prior reviews of a doctor. Finally,\nwe employ GPT4 to reveal the crucial dimensions that distinguish fake physician\nreviews. In sharp contrast to previous findings in the literature that were\nobtained using simulated data, our findings from a real-world dataset show that\nfake reviews are generally more clinically detailed, more reserved in\nsentiment, and have better structure and grammar than authentic ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shukla_A/0/1/0/all/0/1\">Aishwarya Deep Shukla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_L/0/1/0/all/0/1\">Laksh Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mein_J/0/1/0/all/0/1\">Jie Mein</a> (JM)Goh, <a href=\"http://arxiv.org/find/cs/1/au:+Guodong/0/1/0/all/0/1\">Guodong</a> (Gordon)Gao, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1\">Ritu Agarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-resource Bilingual Dialect Lexicon Induction with Large Language Models. (arXiv:2304.09957v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09957","description":"<p>Bilingual word lexicons are crucial tools for multilingual natural language\nunderstanding and machine translation tasks, as they facilitate the mapping of\nwords in one language to their synonyms in another language. To achieve this,\nnumerous papers have explored bilingual lexicon induction (BLI) in\nhigh-resource scenarios, using a typical pipeline consisting of two\nunsupervised steps: bitext mining and word alignment, both of which rely on\npre-trained large language models~(LLMs).\n</p>\n<p>In this paper, we present an analysis of the BLI pipeline for German and two\nof its dialects, Bavarian and Alemannic. This setup poses several unique\nchallenges, including the scarcity of resources, the relatedness of the\nlanguages, and the lack of standardization in the orthography of dialects. To\nevaluate the BLI outputs, we analyze them with respect to word frequency and\npairwise edit distance. Additionally, we release two evaluation datasets\ncomprising 1,500 bilingual sentence pairs and 1,000 bilingual word pairs. They\nwere manually judged for their semantic similarity for each Bavarian-German and\nAlemannic-German language pair.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Latent Space Theory for Emergent Abilities in Large Language Models. (arXiv:2304.09960v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09960","description":"<p>Languages are not created randomly but rather to communicate information.\nThere is a strong association between languages and their underlying meanings,\nresulting in a sparse joint distribution that is heavily peaked according to\ntheir correlations. Moreover, these peak values happen to match with the\nmarginal distribution of languages due to the sparsity. With the advent of LLMs\ntrained on big data and large models, we can now precisely assess the marginal\ndistribution of languages, providing a convenient means of exploring the sparse\nstructures in the joint distribution for effective inferences. In this paper,\nwe categorize languages as either unambiguous or {\\epsilon}-ambiguous and\npresent quantitative results to demonstrate that the emergent abilities of\nLLMs, such as language understanding, in-context learning, chain-of-thought\nprompting, and effective instruction fine-tuning, can all be attributed to\nBayesian inference on the sparse joint distribution of languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hui Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MasakhaNEWS: News Topic Classification for African languages. (arXiv:2304.09972v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09972","description":"<p>African languages are severely under-represented in NLP research due to lack\nof datasets covering several NLP tasks. While there are individual language\nspecific datasets that are being expanded to different tasks, only a handful of\nNLP tasks (e.g. named entity recognition and machine translation) have\nstandardized benchmark datasets covering several geographical and\ntypologically-diverse African languages. In this paper, we develop MasakhaNEWS\n-- a new benchmark dataset for news topic classification covering 16 languages\nwidely spoken in Africa. We provide an evaluation of baseline models by\ntraining classical machine learning models and fine-tuning several language\nmodels. Furthermore, we explore several alternatives to full fine-tuning of\nlanguage models that are better suited for zero-shot and few-shot learning such\nas cross-lingual parameter-efficient fine-tuning (like MAD-X), pattern\nexploiting training (PET), prompting language models (like ChatGPT), and\nprompt-free sentence transformer fine-tuning (SetFit and Cohere Embedding API).\nOur evaluation in zero-shot setting shows the potential of prompting ChatGPT\nfor news topic classification in low-resource African languages, achieving an\naverage performance of 70 F1 points without leveraging additional supervision\nlike MAD-X. In few-shot setting, we show that with as little as 10 examples per\nlabel, we achieved more than 90\\% (i.e. 86.0 F1 points) of the performance of\nfull supervised training (92.6 F1 points) leveraging the PET approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masiak_M/0/1/0/all/0/1\">Marek Masiak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azime_I/0/1/0/all/0/1\">Israel Abebe Azime</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba Oluwadara Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonja_A/0/1/0/all/0/1\">Atnafu Lambebo Tonja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mwase_C/0/1/0/all/0/1\">Christine Mwase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogundepo_O/0/1/0/all/0/1\">Odunayo Ogundepo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dossou_B/0/1/0/all/0/1\">Bonaventure F. P. Dossou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oladipo_A/0/1/0/all/0/1\">Akintunde Oladipo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nixdorf_D/0/1/0/all/0/1\">Doreen Nixdorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1\">Chris Chinenye Emezue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+al_azzawi_S/0/1/0/all/0/1\">Sana Sabah al-azzawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sibanda_B/0/1/0/all/0/1\">Blessing K. Sibanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David_D/0/1/0/all/0/1\">Davis David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ndolela_L/0/1/0/all/0/1\">Lolwethu Ndolela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukiibi_J/0/1/0/all/0/1\">Jonathan Mukiibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajayi_T/0/1/0/all/0/1\">Tunde Oluwaseyi Ajayi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngoli_T/0/1/0/all/0/1\">Tatiana Moteu Ngoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Odhiambo_B/0/1/0/all/0/1\">Brian Odhiambo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owodunni_A/0/1/0/all/0/1\">Abraham Toluwase Owodunni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obiefuna_N/0/1/0/all/0/1\">Nnaemeka C. Obiefuna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen Hassan Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdullahi_S/0/1/0/all/0/1\">Saheed Salahudeen Abdullahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yigezu_M/0/1/0/all/0/1\">Mesay Gemeda Yigezu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwadabe_T/0/1/0/all/0/1\">Tajuddeen Gwadabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdulmumin_I/0/1/0/all/0/1\">Idris Abdulmumin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bame_M/0/1/0/all/0/1\">Mahlet Taye Bame</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awoyomi_O/0/1/0/all/0/1\">Oluwabusayo Olufunke Awoyomi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shode_I/0/1/0/all/0/1\">Iyanuoluwa Shode</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_T/0/1/0/all/0/1\">Tolulope Anu Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailani_H/0/1/0/all/0/1\">Habiba Abdulganiy Kailani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omotayo_A/0/1/0/all/0/1\">Abdul-Hakeem Omotayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeeko_A/0/1/0/all/0/1\">Adetola Adeeko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abeeb_A/0/1/0/all/0/1\">Afolabi Abeeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aremu_A/0/1/0/all/0/1\">Anuoluwapo Aremu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samuel_O/0/1/0/all/0/1\">Olanrewaju Samuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siro_C/0/1/0/all/0/1\">Clemencia Siro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimotho_W/0/1/0/all/0/1\">Wangari Kimotho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogbu_O/0/1/0/all/0/1\">Onyekachi Raphael Ogbu</a>, et al. (23 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Radar de Parit\\'e: An NLP system to measure gender representation in French news stories. (arXiv:2304.09982v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09982","description":"<p>We present the Radar de Parit\\'e, an automated Natural Language Processing\n(NLP) system that measures the proportion of women and men quoted daily in six\nCanadian French-language media outlets. We outline the system's architecture\nand detail the challenges we overcame to address French-specific issues, in\nparticular regarding coreference resolution, a new contribution to the NLP\nliterature on French. We also showcase statistics covering over one year's\nworth of data (282,512 news articles). Our results highlight the\nunderrepresentation of women in news stories, while also illustrating the\napplication of modern NLP methods to measure gender representation and address\nsocietal issues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soumah_V/0/1/0/all/0/1\">Valentin-Gabriel Soumah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_P/0/1/0/all/0/1\">Prashanth Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eibl_P/0/1/0/all/0/1\">Philipp Eibl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taboada_M/0/1/0/all/0/1\">Maite Taboada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supporting Human-AI Collaboration in Auditing LLMs with LLMs. (arXiv:2304.09991v1 [cs.HC])","link":"http://arxiv.org/abs/2304.09991","description":"<p>Large language models are becoming increasingly pervasive and ubiquitous in\nsociety via deployment in sociotechnical systems. Yet these language models, be\nit for classification or generation, have been shown to be biased and behave\nirresponsibly, causing harm to people at scale. It is crucial to audit these\nlanguage models rigorously. Existing auditing tools leverage either or both\nhumans and AI to find failures. In this work, we draw upon literature in\nhuman-AI collaboration and sensemaking, and conduct interviews with research\nexperts in safe and fair AI, to build upon the auditing tool: AdaTest (Ribeiro\nand Lundberg, 2022), which is powered by a generative large language model\n(LLM). Through the design process we highlight the importance of sensemaking\nand human-AI communication to leverage complementary strengths of humans and\ngenerative models in collaborative auditing. To evaluate the effectiveness of\nthe augmented tool, AdaTest++, we conduct user studies with participants\nauditing two commercial language models: OpenAI's GPT-3 and Azure's sentiment\nanalysis model. Qualitative analysis shows that AdaTest++ effectively leverages\nhuman strengths such as schematization, hypothesis formation and testing.\nFurther, with our tool, participants identified a variety of failures modes,\ncovering 26 different topics over 2 tasks, that have been shown before in\nformal audits and also those previously under-reported.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_C/0/1/0/all/0/1\">Charvi Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1\">Marco Tulio Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_N/0/1/0/all/0/1\">Nicholas King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amershi_S/0/1/0/all/0/1\">Saleema Amershi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can ChatGPT Reproduce Human-Generated Labels? A Study of Social Computing Tasks. (arXiv:2304.10145v1 [cs.AI])","link":"http://arxiv.org/abs/2304.10145","description":"<p>The release of ChatGPT has uncovered a range of possibilities whereby large\nlanguage models (LLMs) can substitute human intelligence. In this paper, we\nseek to understand whether ChatGPT has the potential to reproduce\nhuman-generated label annotations in social computing tasks. Such an\nachievement could significantly reduce the cost and complexity of social\ncomputing research. As such, we use ChatGPT to re-label five seminal datasets\ncovering stance detection (2x), sentiment analysis, hate speech, and bot\ndetection. Our results highlight that ChatGPT does have the potential to handle\nthese data annotation tasks, although a number of challenges remain. ChatGPT\nobtains an average precision 0.609. Performance is highest for the sentiment\nanalysis dataset, with ChatGPT correctly annotating 64.9% of tweets. Yet, we\nshow that performance varies substantially across individual labels. We believe\nthis work can open up new lines of analysis and act as a basis for future\nresearch into the exploitation of ChatGPT for human annotation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yiming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peixian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haq_E/0/1/0/all/0/1\">Ehsan-Ul Haq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_P/0/1/0/all/0/1\">Pan Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyson_G/0/1/0/all/0/1\">Gareth Tyson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Independence of Association Bias and Empirical Fairness in Language Models. (arXiv:2304.10153v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10153","description":"<p>The societal impact of pre-trained language models has prompted researchers\nto probe them for strong associations between protected attributes and\nvalue-loaded terms, from slur to prestigious job titles. Such work is said to\nprobe models for bias or fairness-or such probes 'into representational biases'\nare said to be 'motivated by fairness'-suggesting an intimate connection\nbetween bias and fairness. We provide conceptual clarity by distinguishing\nbetween association biases (Caliskan et al., 2022) and empirical fairness (Shen\net al., 2022) and show the two can be independent. Our main contribution,\nhowever, is showing why this should not come as a surprise. To this end, we\nfirst provide a thought experiment, showing how association bias and empirical\nfairness can be completely orthogonal. Next, we provide empirical evidence that\nthere is no correlation between bias metrics and fairness metrics across the\nmost widely used language models. Finally, we survey the sociological and\npsychological literature and show how this literature provides ample support\nfor expecting these metrics to be uncorrelated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cabello_L/0/1/0/all/0/1\">Laura Cabello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jorgensen_A/0/1/0/all/0/1\">Anna Katrine J&#xf8;rgensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Manipulating Tokenization Aid Cross-Lingual Transfer? A Study on POS Tagging for Non-Standardized Languages. (arXiv:2304.10158v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10158","description":"<p>One of the challenges with finetuning pretrained language models (PLMs) is\nthat their tokenizer is optimized for the language(s) it was pretrained on, but\nbrittle when it comes to previously unseen variations in the data. This can for\ninstance be observed when finetuning PLMs on one language and evaluating them\non data in a closely related language variety with no standardized orthography.\nDespite the high linguistic similarity, tokenization no longer corresponds to\nmeaningful representations of the target data, leading to low performance in,\ne.g., part-of-speech tagging.\n</p>\n<p>In this work, we finetune PLMs on seven languages from three different\nfamilies and analyze their zero-shot performance on closely related,\nnon-standardized varieties. We consider different measures for the divergence\nin the tokenization of the source and target data, and the way they can be\nadjusted by manipulating the tokenization during the finetuning step. Overall,\nwe find that the similarity between the percentage of words that get split into\nsubwords in the source and target data (the split word ratio difference) is the\nstrongest predictor for model performance on target data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blaschke_V/0/1/0/all/0/1\">Verena Blaschke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing FOMC Minutes: Accuracy and Constraints of Language Models. (arXiv:2304.10164v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10164","description":"<p>This research article analyzes the language used in the official statements\nreleased by the Federal Open Market Committee (FOMC) after its scheduled\nmeetings to gain insights into the impact of FOMC official statements on\nfinancial markets and economic forecasting. The study reveals that the FOMC is\ncareful to avoid expressing emotion in their sentences and follows a set of\ntemplates to cover economic situations. The analysis employs advanced language\nmodeling techniques such as VADER and FinBERT, and a trial test with GPT-4. The\nresults show that FinBERT outperforms other techniques in predicting negative\nsentiment accurately. However, the study also highlights the challenges and\nlimitations of using current NLP techniques to analyze FOMC texts and suggests\nthe potential for enhancing language models and exploring alternative\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1\">Wonseong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sporer_J/0/1/0/all/0/1\">Jan Frederic Sp&#xf6;rer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Handschuh_S/0/1/0/all/0/1\">Siegfried Handschuh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoT-MoTE: Exploring ConTextual Masked Auto-Encoder Pre-training with Mixture-of-Textual-Experts for Passage Retrieval. (arXiv:2304.10195v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10195","description":"<p>Passage retrieval aims to retrieve relevant passages from large collections\nof the open-domain corpus. Contextual Masked Auto-Encoding has been proven\neffective in representation bottleneck pre-training of a monolithic\ndual-encoder for passage retrieval. Siamese or fully separated dual-encoders\nare often adopted as basic retrieval architecture in the pre-training and\nfine-tuning stages for encoding queries and passages into their latent\nembedding spaces. However, simply sharing or separating the parameters of the\ndual-encoder results in an imbalanced discrimination of the embedding spaces.\nIn this work, we propose to pre-train Contextual Masked Auto-Encoder with\nMixture-of-Textual-Experts (CoT-MoTE). Specifically, we incorporate\ntextual-specific experts for individually encoding the distinct properties of\nqueries and passages. Meanwhile, a shared self-attention layer is still kept\nfor unified attention modeling. Results on large-scale passage retrieval\nbenchmarks show steady improvement in retrieval performances. The quantitive\nanalysis also shows a more balanced discrimination of the latent embedding\nspaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1\">Guangyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songlin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Paracrawl for Document-level Neural Machine Translation. (arXiv:2304.10216v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10216","description":"<p>Document-level neural machine translation (NMT) has outperformed\nsentence-level NMT on a number of datasets. However, document-level NMT is\nstill not widely adopted in real-world translation systems mainly due to the\nlack of large-scale general-domain training data for document-level NMT. We\nexamine the effectiveness of using Paracrawl for learning document-level\ntranslation. Paracrawl is a large-scale parallel corpus crawled from the\nInternet and contains data from various domains. The official Paracrawl corpus\nwas released as parallel sentences (extracted from parallel webpages) and\ntherefore previous works only used Paracrawl for learning sentence-level\ntranslation. In this work, we extract parallel paragraphs from Paracrawl\nparallel webpages using automatic sentence alignments and we use the extracted\nparallel paragraphs as parallel documents for training document-level\ntranslation models. We show that document-level NMT models trained with only\nparallel paragraphs from Paracrawl can be used to translate real documents from\nTED, News and Europarl, outperforming sentence-level NMT models. We also\nperform a targeted pronoun evaluation and show that document-level models\ntrained with Paracrawl data can help context-aware pronoun translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghussin_Y/0/1/0/all/0/1\">Yusser Al Ghussin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genabith_J/0/1/0/all/0/1\">Josef van Genabith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Open Intent Classification with K-center Contrastive Learning and Adjustable Decision Boundary. (arXiv:2304.10220v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10220","description":"<p>Open intent classification, which aims to correctly classify the known\nintents into their corresponding classes while identifying the new unknown\n(open) intents, is an essential but challenging task in dialogue systems. In\nthis paper, we introduce novel K-center contrastive learning and adjustable\ndecision boundary learning (CLAB) to improve the effectiveness of open intent\nclassification. First, we pre-train a feature encoder on the labeled training\ninstances, which transfers knowledge from known intents to unknown intents.\nSpecifically, we devise a K-center contrastive learning algorithm to learn\ndiscriminative and balanced intent features, improving the generalization of\nthe model for recognizing open intents. Second, we devise an adjustable\ndecision boundary learning method with expanding and shrinking (ADBES) to\ndetermine the suitable decision conditions. Concretely, we learn a decision\nboundary for each known intent class, which consists of a decision center and\nthe radius of the decision boundary. We then expand the radius of the decision\nboundary to accommodate more in-class instances if the out-of-class instances\nare far from the decision boundary; otherwise, we shrink the radius of the\ndecision boundary. Extensive experiments on three benchmark datasets clearly\ndemonstrate the effectiveness of our method for open intent classification. For\nreproducibility, we submit the code at: https://github.com/lxk00/CLAP\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaokang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianquan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1\">Jingjing Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Benyou Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Indian Sign Language Recognition Using Mediapipe Holistic. (arXiv:2304.10256v1 [cs.CV])","link":"http://arxiv.org/abs/2304.10256","description":"<p>Deaf individuals confront significant communication obstacles on a daily\nbasis. Their inability to hear makes it difficult for them to communicate with\nthose who do not understand sign language. Moreover, it presents difficulties\nin educational, occupational, and social contexts. By providing alternative\ncommunication channels, technology can play a crucial role in overcoming these\nobstacles. One such technology that can facilitate communication between deaf\nand hearing individuals is sign language recognition. We will create a robust\nsystem for sign language recognition in order to convert Indian Sign Language\nto text or speech. We will evaluate the proposed system and compare CNN and\nLSTM models. Since there are both static and gesture sign languages, a robust\nmodel is required to distinguish between them. In this study, we discovered\nthat a CNN model captures letters and characters for recognition of static sign\nlanguage better than an LSTM model, but it outperforms CNN by monitoring hands,\nfaces, and pose in gesture sign language phrases and sentences. The creation of\na text-to-sign language paradigm is essential since it will enhance the sign\nlanguage-dependent deaf and hard-of-hearing population's communication skills.\nEven though the sign-to-text translation is just one side of communication, not\nall deaf or hard-of-hearing people are proficient in reading or writing text.\nSome may have difficulty comprehending written language due to educational or\nliteracy issues. Therefore, a text-to-sign language paradigm would allow them\nto comprehend text-based information and participate in a variety of social,\neducational, and professional settings.\n</p>\n<p>Keywords: deaf and hard-of-hearing, DHH, Indian sign language, CNN, LSTM,\nstatic and gesture sign languages, text-to-sign language model, MediaPipe\nHolistic, sign language recognition, SLR, SLT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+G_D/0/1/0/all/0/1\">Dr. Velmathi G</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_K/0/1/0/all/0/1\">Kaushal Goyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is augmentation effective to improve prediction in imbalanced text datasets?. (arXiv:2304.10283v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10283","description":"<p>Imbalanced datasets present a significant challenge for machine learning\nmodels, often leading to biased predictions. To address this issue, data\naugmentation techniques are widely used in natural language processing (NLP) to\ngenerate new samples for the minority class. However, in this paper, we\nchallenge the common assumption that data augmentation is always necessary to\nimprove predictions on imbalanced datasets. Instead, we argue that adjusting\nthe classifier cutoffs without data augmentation can produce similar results to\noversampling techniques. Our study provides theoretical and empirical evidence\nto support this claim. Our findings contribute to a better understanding of the\nstrengths and limitations of different approaches to dealing with imbalanced\ndata, and help researchers and practitioners make informed decisions about\nwhich methods to use for a given task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Assuncao_G/0/1/0/all/0/1\">Gabriel O. Assun&#xe7;&#xe3;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izbicki_R/0/1/0/all/0/1\">Rafael Izbicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prates_M/0/1/0/all/0/1\">Marcos O. Prates</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decouple Non-parametric Knowledge Distillation For End-to-end Speech Translation. (arXiv:2304.10295v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10295","description":"<p>Existing techniques often attempt to make knowledge transfer from a powerful\nmachine translation (MT) to speech translation (ST) model with some elaborate\ntechniques, which often requires transcription as extra input during training.\nHowever, transcriptions are not always available, and how to improve the ST\nmodel performance without transcription, i.e., data efficiency, has rarely been\nstudied in the literature. In this paper, we propose Decoupled Non-parametric\nKnowledge Distillation (DNKD) from data perspective to improve the data\nefficiency. Our method follows the knowledge distillation paradigm. However,\ninstead of obtaining the teacher distribution from a sophisticated MT model, we\nconstruct it from a non-parametric datastore via k-Nearest-Neighbor (kNN)\nretrieval, which removes the dependence on transcription and MT model. Then we\ndecouple the classic knowledge distillation loss into target and non-target\ndistillation to enhance the effect of the knowledge among non-target logits,\nwhich is the prominent \"dark knowledge\". Experiments on MuST-C corpus show\nthat, the proposed method can achieve consistent improvement over the strong\nbaseline without requiring any transcription.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_N/0/1/0/all/0/1\">Nianwen Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yaqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xukui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_D/0/1/0/all/0/1\">Dan Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Speech Translation by Cross-Modal Multi-Grained Contrastive Learning. (arXiv:2304.10309v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10309","description":"<p>The end-to-end speech translation (E2E-ST) model has gradually become a\nmainstream paradigm due to its low latency and less error propagation. However,\nit is non-trivial to train such a model well due to the task complexity and\ndata scarcity. The speech-and-text modality differences result in the E2E-ST\nmodel performance usually inferior to the corresponding machine translation\n(MT) model. Based on the above observation, existing methods often use\nsharingmechanisms to carry out implicit knowledge transfer by imposing various\nconstraints. However, the final model often performs worse on the MT task than\nthe MT model trained alone, which means that the knowledge transfer ability of\nthis method is also limited. To deal with these problems, we propose the FCCL\n(Fine- and Coarse- Granularity Contrastive Learning) approach for E2E-ST, which\nmakes explicit knowledge transfer through cross-modal multi-grained contrastive\nlearning. A key ingredient of our approach is applying contrastive learning at\nboth sentence- and frame-level to give the comprehensive guide for extracting\nspeech representations containing rich semantic information.In addition, we\nadopt a simple whitening method to alleviate the representation degeneration in\nthe MT model, which adversely affects contrast learning. Experiments on the\nMuST-C benchmark show that our proposed approach significantly outperforms the\nstate-of-the-art E2E-ST baselines on all eight language pairs. Further analysis\nindicates that FCCL can free up its capacity from learning grammatical\nstructure information and force more layers to learn semantic information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_N/0/1/0/all/0/1\">Nianwen Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yaqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xukui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_D/0/1/0/all/0/1\">Dan Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei-Qiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DropDim: A Regularization Method for Transformer Networks. (arXiv:2304.10321v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10321","description":"<p>We introduceDropDim, a structured dropout method designed for regularizing\nthe self-attention mechanism, which is a key component of the transformer. In\ncontrast to the general dropout method, which randomly drops neurons, DropDim\ndrops part of the embedding dimensions. In this way, the semantic information\ncan be completely discarded. Thus, the excessive coadapting between different\nembedding dimensions can be broken, and the self-attention is forced to encode\nmeaningful featureswith a certain number of embedding dimensions erased.\nExperiments on a wide range of tasks executed on the MUST-C English-Germany\ndataset show that DropDim can effectively improve model performance, reduce\nover-fitting, and show complementary effects with other regularization methods.\nWhen combined with label smoothing, the WER can be reduced from 19.1% to 15.1%\non the ASR task, and the BLEU value can be increased from26.90 to 28.38 on the\nMT task. On the ST task, the model can reach a BLEU score of 22.99, an increase\nby 1.86 BLEU points compared to the strong baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_D/0/1/0/all/0/1\">Dan Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_K/0/1/0/all/0/1\">Keji Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xukui Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Benchmark for Scientific Understanding in Humans and Machines. (arXiv:2304.10327v1 [cs.AI])","link":"http://arxiv.org/abs/2304.10327","description":"<p>Scientific understanding is a fundamental goal of science, allowing us to\nexplain the world. There is currently no good way to measure the scientific\nunderstanding of agents, whether these be humans or Artificial Intelligence\nsystems. Without a clear benchmark, it is challenging to evaluate and compare\ndifferent levels of and approaches to scientific understanding. In this\nRoadmap, we propose a framework to create a benchmark for scientific\nunderstanding, utilizing tools from philosophy of science. We adopt a\nbehavioral notion according to which genuine understanding should be recognized\nas an ability to perform certain tasks. We extend this notion by considering a\nset of questions that can gauge different levels of scientific understanding,\ncovering information retrieval, the capability to arrange information to\nproduce an explanation, and the ability to infer how things would be different\nunder different circumstances. The Scientific Understanding Benchmark (SUB),\nwhich is formed by a set of these tests, allows for the evaluation and\ncomparison of different approaches. Benchmarking plays a crucial role in\nestablishing trust, ensuring quality control, and providing a basis for\nperformance evaluation. By aligning machine and human scientific understanding\nwe can improve their utility, ultimately advancing scientific understanding and\nhelping to discover new insights within machines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barman_K/0/1/0/all/0/1\">Kristian Gonzalez Barman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caron_S/0/1/0/all/0/1\">Sascha Caron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Claassen_T/0/1/0/all/0/1\">Tom Claassen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Regt_H/0/1/0/all/0/1\">Henk de Regt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interventional Probing in High Dimensions: An NLI Case Study. (arXiv:2304.10346v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10346","description":"<p>Probing strategies have been shown to detect the presence of various\nlinguistic features in large language models; in particular, semantic features\nintermediate to the \"natural logic\" fragment of the Natural Language Inference\ntask (NLI). In the case of natural logic, the relation between the intermediate\nfeatures and the entailment label is explicitly known: as such, this provides a\nripe setting for interventional studies on the NLI models' representations,\nallowing for stronger causal conjectures and a deeper critical analysis of\ninterventional probing methods. In this work, we carry out new and existing\nrepresentation-level interventions to investigate the effect of these semantic\nfeatures on NLI classification: we perform amnesic probing (which removes\nfeatures as directed by learned linear probes) and introduce the mnestic\nprobing variation (which forgets all dimensions except the probe-selected\nones). Furthermore, we delve into the limitations of these methods and outline\nsome pitfalls have been obscuring the effectivity of interventional probing\nstudies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rozanova_J/0/1/0/all/0/1\">Julia Rozanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1\">Marco Valentino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordeiro_L/0/1/0/all/0/1\">Lucas Cordeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andre Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt-Learning for Cross-Lingual Relation Extraction. (arXiv:2304.10354v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10354","description":"<p>Relation Extraction (RE) is a crucial task in Information Extraction, which\nentails predicting relationships between entities within a given sentence.\nHowever, extending pre-trained RE models to other languages is challenging,\nparticularly in real-world scenarios where Cross-Lingual Relation Extraction\n(XRE) is required. Despite recent advancements in Prompt-Learning, which\ninvolves transferring knowledge from Multilingual Pre-trained Language Models\n(PLMs) to diverse downstream tasks, there is limited research on the effective\nuse of multilingual PLMs with prompts to improve XRE. In this paper, we present\na novel XRE algorithm based on Prompt-Tuning, referred to as Prompt-XRE. To\nevaluate its effectiveness, we design and implement several prompt templates,\nincluding hard, soft, and hybrid prompts, and empirically test their\nperformance on competitive multilingual PLMs, specifically mBART. Our extensive\nexperiments, conducted on the low-resource ACE05 benchmark across multiple\nlanguages, demonstrate that our Prompt-XRE algorithm significantly outperforms\nboth vanilla multilingual PLMs and other existing models, achieving\nstate-of-the-art performance in XRE. To further show the generalization of our\nPrompt-XRE on larger data scales, we construct and release a new XRE dataset-\nWMT17-EnZh XRE, containing 0.9M English-Chinese pairs extracted from WMT 2017\nparallel corpus. Experiments on WMT17-EnZh XRE also show the effectiveness of\nour Prompt-XRE against other competitive baselines. The code and newly\nconstructed dataset are freely available at\n\\url{https://github.com/HSU-CHIA-MING/Prompt-XRE}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Chiaming Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zan_C/0/1/0/all/0/1\">Changtong Zan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenbin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CKBP v2: An Expert-Annotated Evaluation Set for Commonsense Knowledge Base Population. (arXiv:2304.10392v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10392","description":"<p>Populating Commonsense Knowledge Bases (CSKB) is an important yet hard task\nin NLP, as it tackles knowledge from external sources with unseen events and\nentities. Fang et al. (2021a) proposed a CSKB Population benchmark with an\nevaluation set CKBP v1. However, CKBP v1 adopts crowdsourced annotations that\nsuffer from a substantial fraction of incorrect answers, and the evaluation set\nis not well-aligned with the external knowledge source as a result of random\nsampling. In this paper, we introduce CKBP v2, a new high-quality CSKB\nPopulation benchmark, which addresses the two mentioned problems by using\nexperts instead of crowd-sourced annotation and by adding diversified\nadversarial samples to make the evaluation set more representative. We conduct\nextensive experiments comparing state-of-the-art methods for CSKB Population on\nthe new evaluation set for future research comparisons. Empirical results show\nthat the population task is still challenging, even for large language models\n(LLM) such as ChatGPT. Codes and data are available at\nhttps://github.com/HKUST-KnowComp/CSKB-Population.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1\">Tianqing Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_Q/0/1/0/all/0/1\">Quyet V. Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Sehyun Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-NER: Named Entity Recognition via Large Language Models. (arXiv:2304.10428v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10428","description":"<p>Despite the fact that large-scale Language Models (LLM) have achieved SOTA\nperformances on a variety of NLP tasks, its performance on NER is still\nsignificantly below supervised baselines. This is due to the gap between the\ntwo tasks the NER and LLMs: the former is a sequence labeling task in nature\nwhile the latter is a text-generation model.\n</p>\n<p>In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the\ngap by transforming the sequence labeling task to a generation task that can be\neasily adapted by LLMs e.g., the task of finding location entities in the input\ntext \"Columbus is a city\" is transformed to generate the text sequence\n\"@@Columbus## is a city\", where special tokens @@## marks the entity to\nextract. To efficiently address the \"hallucination\" issue of LLMs, where LLMs\nhave a strong inclination to over-confidently label NULL inputs as entities, we\npropose a self-verification strategy by prompting LLMs to ask itself whether\nthe extracted entities belong to a labeled entity tag.\n</p>\n<p>We conduct experiments on five widely adopted NER datasets, and GPT-NER\nachieves comparable performances to fully supervised baselines, which is the\nfirst time as far as we are concerned. More importantly, we find that GPT-NER\nexhibits a greater ability in the low-resource and few-shot setups, when the\namount of training data is extremely scarce, GPT-NER performs significantly\nbetter than supervised models. This demonstrates the capabilities of GPT-NER in\nreal-world NER applications where the number of labeled examples is limited.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_R/0/1/0/all/0/1\">Rongbin Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Safety Assessment of Chinese Large Language Models. (arXiv:2304.10436v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10436","description":"<p>With the rapid popularity of large language models such as ChatGPT and GPT-4,\na growing amount of attention is paid to their safety concerns. These models\nmay generate insulting and discriminatory content, reflect incorrect social\nvalues, and may be used for malicious purposes such as fraud and dissemination\nof misleading information. Evaluating and enhancing their safety is\nparticularly essential for the wide application of large language models\n(LLMs). To further promote the safe deployment of LLMs, we develop a Chinese\nLLM safety assessment benchmark. Our benchmark explores the comprehensive\nsafety performance of LLMs from two perspectives: 8 kinds of typical safety\nscenarios and 6 types of more challenging instruction attacks. Our benchmark is\nbased on a straightforward process in which it provides the test prompts and\nevaluates the safety of the generated responses from the evaluated model. In\nevaluation, we utilize the LLM's strong evaluation ability and develop it as a\nsafety evaluator by prompting. On top of this benchmark, we conduct safety\nassessments and analyze 15 LLMs including the OpenAI GPT series and other\nwell-known Chinese LLMs, where we observe some interesting findings. For\nexample, we find that instruction attacks are more likely to expose safety\nissues of all LLMs. Moreover, to promote the development and deployment of\nsafe, responsible, and ethical AI, we publicly release SafetyPrompts including\n100k augmented prompts and responses by LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhexin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiawen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jiale Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-specific Continued Pretraining of Language Models for Capturing Long Context in Mental Health. (arXiv:2304.10447v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10447","description":"<p>Pretrained language models have been used in various natural language\nprocessing applications. In the mental health domain, domain-specific language\nmodels are pretrained and released, which facilitates the early detection of\nmental health conditions. Social posts, e.g., on Reddit, are usually long\ndocuments. However, there are no domain-specific pretrained models for\nlong-sequence modeling in the mental health domain. This paper conducts\ndomain-specific continued pretraining to capture the long context for mental\nhealth. Specifically, we train and release MentalXLNet and MentalLongformer\nbased on XLNet and Longformer. We evaluate the mental health classification\nperformance and the long-range ability of these two domain-specific pretrained\nmodels. Our models are released in HuggingFace.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1\">Sophia Ananiadou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiedemann_J/0/1/0/all/0/1\">J&#xf6;rg Tiedemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phoenix: Democratizing ChatGPT across Languages. (arXiv:2304.10453v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10453","description":"<p>This paper presents our efforts to democratize ChatGPT across language. We\nrelease a large language model \"Phoenix\", achieving competitive performance\namong open-source English and Chinese models while excelling in languages with\nlimited resources (covering both Latin and non-Latin languages). We believe\nthis work will be beneficial to make ChatGPT more accessible, especially in\ncountries where people cannot use ChatGPT due to restrictions from OpenAI or\nlocal goverments. Our data, code, and models are available at\nhttps://github.com/FreedomIntelligence/LLMZoo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhihong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1\">Feng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tiannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Juhao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianquan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Benyou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Program with Natural Language. (arXiv:2304.10464v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10464","description":"<p>Large Language Models (LLMs) have shown remarkable performance in various\nbasic natural language tasks, which raises hopes for achieving Artificial\nGeneral Intelligence. To better complete complex tasks, we need LLMs to program\nfor the task and then follow the program to generate a specific solution for\nthe test sample. We propose using natural language as a new programming\nlanguage to describe task procedures, making them easily understandable to both\nhumans and LLMs. LLMs are capable of directly generating natural language\nprograms, but these programs may still contain factual errors or incomplete\nsteps. Therefore, we further propose the Learning to Program (LP) method to ask\nLLMs themselves to learn natural language programs from the training dataset of\ncomplex tasks and then use the learned program to guide inference. Our\nexperiments on the AMPS (high school math) and Math (competition mathematics\nproblems) datasets demonstrate the effectiveness of our approach. When testing\nChatGPT on 10 tasks from the AMPS dataset, our LP method's average performance\noutperformed the direct zero-shot test performance by 18.3$\\%$. We release our\ncode at \\url{https://github.com/microsoft/NaturalLanguageProgram}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yiduo Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yaobo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenshan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A primer on getting neologisms from foreign languages to under-resourced languages. (arXiv:2304.10495v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10495","description":"<p>Mainly due to lack of support, most under-resourced languages have a reduced\nlexicon in most realms and domains of increasing importance, then their\nspeakers need to significantly augment it. Although neologisms should arise\nfrom the languages themselves, external sources are widely accepted. However,\nwe dispute the \"common sense\" of using the imposed official languages, which\nare highly probably a legacy of colonialism, as the only source, and we propose\nto introduce neologisms from any language as long as these neologisms \"sound\nlike\" native words of the target languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Camacho_L/0/1/0/all/0/1\">Luis Camacho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Can We Detect Substance Use Disorder?\": Knowledge and Time Aware Classification on Social Media from Darkweb. (arXiv:2304.10512v1 [cs.LG])","link":"http://arxiv.org/abs/2304.10512","description":"<p>Opioid and substance misuse is rampant in the United States today, with the\nphenomenon known as the \"opioid crisis\". The relationship between substance use\nand mental health has been extensively studied, with one possible relationship\nbeing: substance misuse causes poor mental health. However, the lack of\nevidence on the relationship has resulted in opioids being largely inaccessible\nthrough legal means. This study analyzes the substance use posts on social\nmedia with opioids being sold through crypto market listings. We use the Drug\nAbuse Ontology, state-of-the-art deep learning, and knowledge-aware BERT-based\nmodels to generate sentiment and emotion for the social media posts to\nunderstand users' perceptions on social media by investigating questions such\nas: which synthetic opioids people are optimistic, neutral, or negative about?\nor what kind of drugs induced fear and sorrow? or what kind of drugs people\nlove or are thankful about? or which drugs people think negatively about? or\nwhich opioids cause little to no sentimental reaction. We discuss how we\ncrawled crypto market data and its use in extracting posts for fentanyl,\nfentanyl analogs, and other novel synthetic opioids. We also perform topic\nanalysis associated with the generated sentiments and emotions to understand\nwhich topics correlate with people's responses to various drugs. Additionally,\nwe analyze time-aware neural models built on these features while considering\nhistorical sentiment and emotional activity of posts related to a drug. The\nmost effective model performs well (statistically significant) with\n(macroF1=82.12, recall =83.58) to identify substance use disorder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lokala_U/0/1/0/all/0/1\">Usha Lokala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phukan_O/0/1/0/all/0/1\">Orchid Chetia Phukan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dastidar_T/0/1/0/all/0/1\">Triyasha Ghosh Dastidar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamy_F/0/1/0/all/0/1\">Francois Lamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniulaityte_R/0/1/0/all/0/1\">Raminta Daniulaityte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why Does ChatGPT Fall Short in Answering Questions Faithfully?. (arXiv:2304.10513v1 [cs.CL])","link":"http://arxiv.org/abs/2304.10513","description":"<p>Recent advancements in Large Language Models, such as ChatGPT, have\ndemonstrated significant potential to impact various aspects of human life.\nHowever, ChatGPT still faces challenges in aspects like faithfulness. Taking\nquestion answering as a representative application, we seek to understand why\nChatGPT falls short in answering questions faithfully. To address this\nquestion, we attempt to analyze the failures of ChatGPT in complex open-domain\nquestion answering and identifies the abilities under the failures.\nSpecifically, we categorize ChatGPT's failures into four types: comprehension,\nfactualness, specificity, and inference. We further pinpoint three critical\nabilities associated with QA failures: knowledge memorization, knowledge\nassociation, and knowledge reasoning. Additionally, we conduct experiments\ncentered on these abilities and propose potential approaches to enhance\nfaithfulness. The results indicate that furnishing the model with fine-grained\nexternal knowledge, hints for knowledge association, and guidance for reasoning\ncan empower the model to answer questions more faithfully.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Media Slant is Contagious. (arXiv:2202.07269v3 [econ.GN] UPDATED)","link":"http://arxiv.org/abs/2202.07269","description":"<p>This paper examines the diffusion of media slant, specifically how partisan\ncontent from national cable news affects local newspapers in the U.S.,\n2005-2008. We use a text-based measure of cable news slant trained on content\nfrom Fox News Channel (FNC), CNN, and MSNBC to analyze how local newspapers\nadopt FNC's slant over CNN/MSNBC's. Our findings show that local news becomes\nmore similar to FNC content in response to an exogenous increase in local FNC\nviewership. This shift is not limited to borrowing from cable news, but rather,\nlocal newspapers' own content changes. Further, cable TV slant polarizes local\nnews content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/econ/1/au:+Widmer_P/0/1/0/all/0/1\">Philine Widmer</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Galletta_S/0/1/0/all/0/1\">Sergio Galletta</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Ash_E/0/1/0/all/0/1\">Elliott Ash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gold Doesn't Always Glitter: Spectral Removal of Linear and Nonlinear Guarded Attribute Information. (arXiv:2203.07893v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07893","description":"<p>We describe a simple and effective method (Spectral Attribute removaL; SAL)\nto remove private or guarded information from neural representations. Our\nmethod uses matrix decomposition to project the input representations into\ndirections with reduced covariance with the guarded information rather than\nmaximal covariance as factorization methods normally use. We begin with linear\ninformation removal and proceed to generalize our algorithm to the case of\nnonlinear information removal using kernels. Our experiments demonstrate that\nour algorithm retains better main task performance after removing the guarded\ninformation compared to previous work. In addition, our experiments demonstrate\nthat we need a relatively small amount of guarded attribute data to remove\ninformation about these attributes, which lowers the exposure to sensitive data\nand is more suitable for low-resource scenarios. Code is available at\nhttps://github.com/jasonshaoshun/SAL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1\">Shun Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziser_Y/0/1/0/all/0/1\">Yftah Ziser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Shay B. Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI Autonomy : Self-Initiated Open-World Continual Learning and Adaptation. (arXiv:2203.08994v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2203.08994","description":"<p>As more and more AI agents are used in practice, it is time to think about\nhow to make these agents fully autonomous so that they can (1) learn by\nthemselves continually in a self-motivated and self-initiated manner rather\nthan being retrained offline periodically on the initiation of human engineers\nand (2) accommodate or adapt to unexpected or novel circumstances. As the\nreal-world is an open environment that is full of unknowns or novelties, the\ncapabilities of detecting novelties, characterizing them,\naccommodating/adapting to them, gathering ground-truth training data and\nincrementally learning the unknowns/novelties become critical in making the AI\nagent more and more knowledgeable, powerful and self-sustainable over time. The\nkey challenge here is how to automate the process so that it is carried out\ncontinually on the agent's own initiative and through its own interactions with\nhumans, other agents and the environment just like human on-the-job learning.\nThis paper proposes a framework (called SOLA) for this learning paradigm to\npromote the research of building autonomous and continual learning enabled AI\nagents. To show feasibility, an implemented agent is also described.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazumder_S/0/1/0/all/0/1\">Sahisnu Mazumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robertson_E/0/1/0/all/0/1\">Eric Robertson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grigsby_S/0/1/0/all/0/1\">Scott Grigsby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT Meets CTC: New Formulation of End-to-End Speech Recognition with Pre-trained Masked Language Model. (arXiv:2210.16663v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2210.16663","description":"<p>This paper presents BERT-CTC, a novel formulation of end-to-end speech\nrecognition that adapts BERT for connectionist temporal classification (CTC).\nOur formulation relaxes the conditional independence assumptions used in\nconventional CTC and incorporates linguistic knowledge through the explicit\noutput dependency obtained by BERT contextual embedding. BERT-CTC attends to\nthe full contexts of the input and hypothesized output sequences via the\nself-attention mechanism. This mechanism encourages a model to learn\ninner/inter-dependencies between the audio and token representations while\nmaintaining CTC's training efficiency. During inference, BERT-CTC combines a\nmask-predict algorithm with CTC decoding, which iteratively refines an output\nsequence. The experimental results reveal that BERT-CTC improves over\nconventional approaches across variations in speaking styles and languages.\nFinally, we show that the semantic representations in BERT-CTC are beneficial\ntowards downstream spoken language understanding tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Higuchi_Y/0/1/0/all/0/1\">Yosuke Higuchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arora_S/0/1/0/all/0/1\">Siddhant Arora</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ogawa_T/0/1/0/all/0/1\">Tetsuji Ogawa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kobayashi_T/0/1/0/all/0/1\">Tetsunori Kobayashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEAformer: Multi-modal Entity Alignment Transformer for Meta Modality Hybrid. (arXiv:2212.14454v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2212.14454","description":"<p>As an important variant of entity alignment (EA), multi-modal entity\nalignment (MMEA) aims to discover identical entities across different knowledge\ngraphs (KGs) with relevant images attached. We noticed that current MMEA\nalgorithms all globally adopt the KG-level modality fusion strategies for\nmulti-modal entity representation but ignore the variation in modality\npreferences for individual entities, hurting the robustness to potential noise\ninvolved in modalities (e.g., blurry images and relations). In this paper, we\npresent MEAformer, a multi-modal entity alignment transformer approach for meta\nmodality hybrid, which dynamically predicts the mutual correlation coefficients\namong modalities for entity-level feature aggregation. A modal-aware hard\nentity replay strategy is further proposed for addressing vague entity details.\nExperimental results show that our model not only achieves SOTA performance on\nmultiple training scenarios including supervised, unsupervised, iterative, and\nlow resource, but also has a comparable number of parameters, optimistic speed,\nand good interpretability. Our code and data are available at\nhttps://github.com/zjukg/MEAformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Lingbing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1\">Yuxia Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jeff Z. Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Wenting Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Correct answers\" from the psychology of artificial intelligence. (arXiv:2302.07267v5 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2302.07267","description":"<p>We re-replicate 14 psychology studies from the Many Labs 2 replication\nproject (Klein et al., 2018) with OpenAI's text-davinci-003 model, colloquially\nknown as GPT3.5. Among the eight studies we could analyse, our GPT sample\nreplicated 37.5% of the original results and 37.5% of the Many Labs 2 results.\nWe could not analyse the remaining six studies, due to an unexpected phenomenon\nwe call the \"correct answer\" effect. Different runs of GPT3.5 answered nuanced\nquestions probing political orientation, economic preference, judgement, and\nmoral philosophy with zero or near-zero variation in responses: with the\nsupposedly \"correct answer.\" Most but not all of these \"correct answers\" were\nrobust to changing the order of answer choices. One exception occurred in the\nMoral Foundations Theory survey (Graham et al., 2009), for which GPT3.5 almost\nalways identified as a conservative in the original condition (N=1,030, 99.6%)\nand as a liberal in the reverse-order condition (N=1,030, 99.3%). GPT3.5's\nresponses to subsequent questions revealed post-hoc rationalisation; there was\na relative bias in the direction of its previously reported political\norientation. But both self-reported GPT conservatives and self-reported GPT\nliberals revealed right-leaning Moral Foundations, although the right-leaning\nbias of self-reported GPT liberals was weaker. We hypothesise that this pattern\nwas learned from a conservative bias in the model's largely Internet-based\ntraining data. Since AI models of the future may be trained on much of the same\nInternet data as GPT3.5, our results raise concerns that a hypothetical AI-led\nfuture may be subject to a diminished diversity of thought.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_P/0/1/0/all/0/1\">Peter S. Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoenegger_P/0/1/0/all/0/1\">Philipp Schoenegger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chongyang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis. (arXiv:2302.08624v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.08624","description":"<p>In this paper, we present InstructABSA, Aspect Based Sentiment Analysis\n(ABSA) using the instruction learning paradigm for the ABSA subtasks: Aspect\nTerm Extraction (ATE), Aspect Term Sentiment Classification (ATSC), and Joint\nTask modeling. Our method introduces positive, negative, and neutral examples\nto each training sample, and instruction tunes the model (Tk-Instruct) the ABSA\nsubtasks, yielding significant performance improvements. Experimental results\non the Sem Eval 2014, 15, and 16 datasets demonstrate that InstructABSA\noutperforms the previous state-of-the-art (SOTA) approaches on the three ABSA\nsubtasks (ATE, ATSC, and Joint Task) by a significant margin, outperforming 7x\nlarger models. In particular, InstructABSA surpasses the SOTA on the Rest14 ATE\nsubtask by 5.69% points, Rest15 ATSC subtask by 9.59% points, and on the Lapt14\nJoint Task by 3.37% points. Our results also suggest a strong generalization\nability to new domains across all three subtasks\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scaria_K/0/1/0/all/0/1\">Kevin Scaria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_S/0/1/0/all/0/1\">Siddharth Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawant_S/0/1/0/all/0/1\">Saurabh Arjun Sawant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyena Hierarchy: Towards Larger Convolutional Language Models. (arXiv:2302.10866v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.10866","description":"<p>Recent advances in deep learning have relied heavily on the use of large\nTransformers due to their ability to learn at scale. However, the core building\nblock of Transformers, the attention operator, exhibits quadratic cost in\nsequence length, limiting the amount of context accessible. Existing\nsubquadratic methods based on low-rank and sparse approximations need to be\ncombined with dense attention layers to match Transformers, indicating a gap in\ncapability. In this work, we propose Hyena, a subquadratic drop-in replacement\nfor attention constructed by interleaving implicitly parametrized long\nconvolutions and data-controlled gating. In recall and reasoning tasks on\nsequences of thousands to hundreds of thousands of tokens, Hyena improves\naccuracy by more than 50 points over operators relying on state-spaces and\nother implicit and explicit methods, matching attention-based models. We set a\nnew state-of-the-art for dense-attention-free architectures on language\nmodeling in standard datasets (WikiText103 and The Pile), reaching Transformer\nquality with a 20% reduction in training compute required at sequence length\n2K. Hyena operators are twice as fast as highly optimized attention at sequence\nlength 8K, and 100x faster at sequence length 64K.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poli_M/0/1/0/all/0/1\">Michael Poli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massaroli_S/0/1/0/all/0/1\">Stefano Massaroli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_E/0/1/0/all/0/1\">Eric Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1\">Daniel Y. Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dao_T/0/1/0/all/0/1\">Tri Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baccus_S/0/1/0/all/0/1\">Stephen Baccus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ParroT: Translating During Chat Using Large Language Models. (arXiv:2304.02426v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.02426","description":"<p>Large language models (LLMs) like ChatGPT and GPT-4 have exhibited remarkable\nabilities on a wide range of natural language processing (NLP) tasks, including\nvarious machine translation abilities accomplished during chat. However, these\nmodels are only accessible through restricted APIs, which creates barriers to\nnew research and advancements in the field. Therefore, we propose the\n$\\mathbf{ParroT}$ framework to enhance and regulate the translation abilities\nduring chat based on open-sourced LLMs (i.e., LLaMA-7b, BLOOMZ-7b-mt) and human\nwritten translation and evaluation data. Specifically, ParroT reformulates\ntranslation data into the instruction-following style, and introduces a\n\"$\\mathbf{Hint}$\" field for incorporating extra requirements to regulate the\ntranslation process. Accordingly, we propose three instruction types for\nfinetuning ParroT models, including translation instruction, contrastive\ninstruction, and error-guided instruction. We can finetune either the full\nmodels or partial parameters via low rank adaptation (LoRA). Experiments on\nFlores subsets and WMT22 test sets suggest that translation instruction\nimproves the translation performance of vanilla LLMs significantly while\nerror-guided instruction can lead to a further improvement, which demonstrates\nthe importance of learning from low-quality translations annotated by human.\nMeanwhile, the ParroT models can also preserve the ability on general tasks\nwith the Alpaca multi-task dataset involved in finetuning. Please refer to our\nGithub project for more implementation details:\nhttps://github.com/wxjiao/ParroT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenxiang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jen-tse Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4. (arXiv:2304.03439v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.03439","description":"<p>Harnessing logical reasoning ability is a comprehensive natural language\nunderstanding endeavor. With the release of Generative Pretrained Transformer 4\n(GPT-4), highlighted as \"advanced\" at reasoning tasks, we are eager to learn\nthe GPT-4 performance on various logical reasoning tasks. This report analyses\nmultiple logical reasoning datasets, with popular benchmarks like LogiQA and\nReClor, and newly-released datasets like AR-LSAT. We test the multi-choice\nreading comprehension and natural language inference tasks with benchmarks\nrequiring logical reasoning. We further construct a logical reasoning\nout-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4.\nWe also make a performance comparison between ChatGPT and GPT-4. Experiment\nresults show that ChatGPT performs significantly better than the RoBERTa\nfine-tuning method on most logical reasoning benchmarks. With early access to\nthe GPT-4 API we are able to conduct intense experiments on the GPT-4 model.\nThe results show GPT-4 yields even higher performance on most logical reasoning\ndatasets. Among benchmarks, ChatGPT and GPT-4 do relatively well on well-known\ndatasets like LogiQA and ReClor. However, the performance drops significantly\nwhen handling newly released and out-of-distribution datasets. Logical\nreasoning remains challenging for ChatGPT and GPT-4, especially on\nout-of-distribution and natural language inference datasets. We release the\nprompt-style logical reasoning datasets as a benchmark suite and name it\nLogiEval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hanmeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_R/0/1/0/all/0/1\">Ruoxi Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1\">Zhiyang Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qiji Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Similarity-Aware Multimodal Prompt Learning for Fake News Detection. (arXiv:2304.04187v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.04187","description":"<p>The standard paradigm for fake news detection mainly utilizes text\ninformation to model the truthfulness of news. However, the discourse of online\nfake news is typically subtle and it requires expert knowledge to use textual\ninformation to debunk fake news. Recently, studies focusing on multimodal fake\nnews detection have outperformed text-only methods. Recent approaches utilizing\nthe pre-trained model to extract unimodal features, or fine-tuning the\npre-trained model directly, have become a new paradigm for detecting fake news.\nAgain, this paradigm either requires a large number of training instances, or\nupdates the entire set of pre-trained model parameters, making real-world fake\nnews detection impractical. Furthermore, traditional multimodal methods fuse\nthe cross-modal features directly without considering that the uncorrelated\nsemantic representation might inject noise into the multimodal features. This\npaper proposes a Similarity-Aware Multimodal Prompt Learning (SAMPLE)\nframework. First, we incorporate prompt learning into multimodal fake news\ndetection. Prompt learning, which only tunes prompts with a frozen language\nmodel, can reduce memory usage significantly and achieve comparable\nperformances, compared with fine-tuning. We analyse three prompt templates with\na soft verbalizer to detect fake news. In addition, we introduce the\nsimilarity-aware fusing method to adaptively fuse the intensity of multimodal\nrepresentation and mitigate the noise injection via uncorrelated cross-modal\nfeatures. For evaluation, SAMPLE surpasses the F1 and the accuracies of\nprevious works on two benchmark multimodal datasets, demonstrating the\neffectiveness of the proposed method in detecting fake news. In addition,\nSAMPLE also is superior to other approaches regardless of few-shot and\ndata-rich settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Ye Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaomin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yimin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaoman Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingyi Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maynard_D/0/1/0/all/0/1\">Diana Maynard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergence of Symbols in Neural Networks for Semantic Understanding and Communication. (arXiv:2304.06377v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2304.06377","description":"<p>Being able to create meaningful symbols and proficiently use them for higher\ncognitive functions such as communication, reasoning, planning, etc., is\nessential and unique for human intelligence. Current deep neural networks are\nstill far behind human's ability to create symbols for such higher cognitive\nfunctions. Here we propose a solution, named SEA-net, to endow neural networks\nwith ability of symbol creation, semantic understanding and communication.\nSEA-net generates symbols that dynamically configure the network to perform\nspecific tasks. These symbols capture compositional semantic information that\nenables the system to acquire new functions purely by symbolic manipulation or\ncommunication. In addition, we found that these self-generated symbols exhibit\nan intrinsic structure resembling that of natural language, suggesting a common\nframework underlying the generation and understanding of symbols in both human\nbrains and artificial neural networks. We hope that it will be instrumental in\nproducing more capable systems in the future that can synergize the strengths\nof connectionist and symbolic approaches for AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Liangxuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shan Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-code LLM: Visual Programming over LLMs. (arXiv:2304.08103v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.08103","description":"<p>Effectively utilizing LLMs for complex tasks is challenging, often involving\na time-consuming and uncontrollable prompt engineering process. This paper\nintroduces a novel human-LLM interaction framework, Low-code LLM. It\nincorporates six types of simple low-code visual programming interactions, all\nsupported by clicking, dragging, or text editing, to achieve more controllable\nand stable responses. Through visual interaction with a graphical user\ninterface, users can incorporate their ideas into the workflow without writing\ntrivial prompts. The proposed Low-code LLM framework consists of a Planning LLM\nthat designs a structured planning workflow for complex tasks, which can be\ncorrespondingly edited and confirmed by users through low-code visual\nprogramming operations, and an Executing LLM that generates responses following\nthe user-confirmed workflow. We highlight three advantages of the low-code LLM:\ncontrollable generation results, user-friendly human-LLM interaction, and\nbroadly applicable scenarios. We demonstrate its benefits using four typical\napplications. By introducing this approach, we aim to bridge the gap between\nhumans and LLMs, enabling more effective and efficient utilization of LLMs for\ncomplex tasks. Our system will be soon publicly available at LowCodeLLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuzhe Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shaoguang Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenshan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zehua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yaobo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_W/0/1/0/all/0/1\">Wang You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1\">Ting Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tien_J/0/1/0/all/0/1\">Jonathan Tien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes. (arXiv:2304.09433v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.09433","description":"<p>A long standing goal of the data management community is to develop general,\nautomated systems that ingest semi-structured documents and output queryable\ntables without human effort or domain specific customization. Given the sheer\nvariety of potential documents, state-of-the art systems make simplifying\nassumptions and use domain specific training. In this work, we ask whether we\ncan maintain generality by using large language models (LLMs). LLMs, which are\npretrained on broad data, can perform diverse downstream tasks simply\nconditioned on natural language task descriptions.\n</p>\n<p>We propose and evaluate EVAPORATE, a simple, prototype system powered by\nLLMs. We identify two fundamentally different strategies for implementing this\nsystem: prompt the LLM to directly extract values from documents or prompt the\nLLM to synthesize code that performs the extraction. Our evaluations show a\ncost-quality tradeoff between these two approaches. Code synthesis is cheap,\nbut far less accurate than directly processing each document with the LLM. To\nimprove quality while maintaining low cost, we propose an extended code\nsynthesis implementation, EVAPORATE-CODE+, which achieves better quality than\ndirect extraction. Our key insight is to generate many candidate functions and\nensemble their extractions using weak supervision. EVAPORATE-CODE+ not only\noutperforms the state-of-the art systems, but does so using a sublinear pass\nover the documents with the LLM. This equates to a 110x reduction in the number\nof tokens the LLM needs to process, averaged across 16 real-world evaluation\nsettings of 10k documents each.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Simran Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Brandon Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eyuboglu_S/0/1/0/all/0/1\">Sabri Eyuboglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_A/0/1/0/all/0/1\">Avanika Narayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hojel_A/0/1/0/all/0/1\">Andrew Hojel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trummer_I/0/1/0/all/0/1\">Immanuel Trummer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging Natural Language Processing and Psycholinguistics: computationally grounded semantic similarity datasets for Basque and Spanish. (arXiv:2304.09616v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.09616","description":"<p>We present a computationally-grounded word similarity dataset based on two\nwell-known Natural Language Processing resources; text corpora and knowledge\nbases. This dataset aims to fulfil a gap in psycholinguistic research by\nproviding a variety of quantifications of semantic similarity in an extensive\nset of noun pairs controlled by variables that play a significant role in\nlexical processing. The dataset creation has consisted in three steps, 1)\ncomputing four key psycholinguistic features for each noun; concreteness,\nfrequency, semantic and phonological neighbourhood density; 2) pairing nouns\nacross these four variables; 3) for each noun pair, assigning three types of\nword similarity measurements, computed out of text, Wordnet and hybrid\nembeddings. The present dataset includes noun pairs' information in Basque and\nEuropean Spanish, but further work intends to extend it to more languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goikoetxea_J/0/1/0/all/0/1\">J. Goikoetxea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arantzeta_M/0/1/0/all/0/1\">M. Arantzeta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_I/0/1/0/all/0/1\">I. San Martin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-04-20T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}