{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-10-11T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Vulnerability Clustering and other Machine Learning Applications of Semantic Vulnerability Embeddings. (arXiv:2310.05935v1 [cs.CR])","link":"http://arxiv.org/abs/2310.05935","description":"<p>Cyber-security vulnerabilities are usually published in form of short natural\nlanguage descriptions (e.g., in form of MITRE's CVE list) that over time are\nfurther manually enriched with labels such as those defined by the Common\nVulnerability Scoring System (CVSS). In the Vulnerability AI (Analytics and\nIntelligence) project, we investigated different types of semantic\nvulnerability embeddings based on natural language processing (NLP) techniques\nto obtain a concise representation of the vulnerability space. We also\nevaluated their use as a foundation for machine learning applications that can\nsupport cyber-security researchers and analysts in risk assessment and other\nrelated activities. The particular applications we explored and briefly\nsummarize in this report are clustering, classification, and visualization, as\nwell as a new logic-based approach to evaluate theories about the vulnerability\nspace.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stehr_M/0/1/0/all/0/1\">Mark-Oliver Stehr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minyoung Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fingerprint Attack: Client De-Anonymization in Federated Learning. (arXiv:2310.05960v1 [cs.CR])","link":"http://arxiv.org/abs/2310.05960","description":"<p>Federated Learning allows collaborative training without data sharing in\nsettings where participants do not trust the central server and one another.\nPrivacy can be further improved by ensuring that communication between the\nparticipants and the server is anonymized through a shuffle; decoupling the\nparticipant identity from their data. This paper seeks to examine whether such\na defense is adequate to guarantee anonymity, by proposing a novel\nfingerprinting attack over gradients sent by the participants to the server. We\nshow that clustering of gradients can easily break the anonymization in an\nempirical study of learning federated language models on two language corpora.\nWe then show that training with differential privacy can provide a practical\ndefense against our fingerprint attack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiongkai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohrimenko_O/0/1/0/all/0/1\">Olga Ohrimenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Embeddings for Measuring Text Relatedness: Unveiling Sentiments and Relationships in Online Comments. (arXiv:2310.05964v1 [cs.CL])","link":"http://arxiv.org/abs/2310.05964","description":"<p>After a pandemic that caused internet usage to grow by 70%, there has been an\nincreased number of people all across the world using social media.\nApplications like Twitter, Meta Threads, YouTube, and Reddit have become\nincreasingly pervasive, leaving almost no digital space where public opinion is\nnot expressed. This paper investigates sentiment and semantic relationships\namong comments across various social media platforms, as well as discusses the\nimportance of shared opinions across these different media platforms, using\nword embeddings to analyze components in sentences and documents. It allows\nresearchers, politicians, and business representatives to trace a path of\nshared sentiment among users across the world. This research paper presents\nmultiple approaches that measure the relatedness of text extracted from user\ncomments on these popular online platforms. By leveraging embeddings, which\ncapture semantic relationships between words and help analyze sentiments across\nthe web, we can uncover connections regarding public opinion as a whole. The\nstudy utilizes pre-existing datasets from YouTube, Reddit, Twitter, and more.\nWe made use of popular natural language processing models like Bidirectional\nEncoder Representations from Transformers (BERT) to analyze sentiments and\nexplore relationships between comment embeddings. Additionally, we aim to\nutilize clustering and Kl-divergence to find semantic relationships within\nthese comment embeddings across various social media platforms. Our analysis\nwill enable a deeper understanding of the interconnectedness of online comments\nand will investigate the notion of the internet functioning as a large\ninterconnected brain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Olakangil_A/0/1/0/all/0/1\">Anthony Olakangil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cindy Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_J/0/1/0/all/0/1\">Justin Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qunbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jethwa_K/0/1/0/all/0/1\">Kaavya Jethwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jason Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narendra_A/0/1/0/all/0/1\">Aryan Narendra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_N/0/1/0/all/0/1\">Nishk Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajaram_A/0/1/0/all/0/1\">Arjun Rajaram</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An evolutionary model of personality traits related to cooperative behavior using a large language model. (arXiv:2310.05976v1 [physics.soc-ph])","link":"http://arxiv.org/abs/2310.05976","description":"<p>This paper aims to shed light on the evolutionary dynamics of diverse and\nsocial populations by introducing the rich expressiveness of generative models\ninto the trait expression of social agent-based evolutionary models.\nSpecifically, we focus on the evolution of personality traits in the context of\na game-theoretic relationship as a situation in which inter-individual\ninterests exert strong selection pressures. We construct an agent model in\nwhich linguistic descriptions of personality traits related to cooperative\nbehavior are used as genes. The deterministic strategies extracted from Large\nLanguage Model (LLM) that make behavioral decisions based on these personality\ntraits are used as behavioral traits. The population is evolved according to\nselection based on average payoff and mutation of genes by asking LLM to\nslightly modify the parent gene toward cooperative or selfish. Through\npreliminary experiments and analyses, we clarify that such a model can indeed\nexhibit the evolution of cooperative behavior based on the diverse and\nhigher-order representation of personality traits. We also observed the\nrepeated intrusion of cooperative and selfish personality traits through\nchanges in the expression of personality traits, and found that the emerging\nwords in the evolved gene well reflected the behavioral tendency of its\npersonality in terms of their semantics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Suzuki_R/0/1/0/all/0/1\">Reiji Suzuki</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Arita_T/0/1/0/all/0/1\">Takaya Arita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Document-level Event Argument Extraction with Contextual Clues and Role Relevance. (arXiv:2310.05991v1 [cs.CL])","link":"http://arxiv.org/abs/2310.05991","description":"<p>Document-level event argument extraction poses new challenges of long input\nand cross-sentence inference compared to its sentence-level counterpart.\nHowever, most prior works focus on capturing the relations between candidate\narguments and the event trigger in each event, ignoring two crucial points: a)\nnon-argument contextual clue information; b) the relevance among argument\nroles. In this paper, we propose a SCPRG (Span-trigger-based Contextual Pooling\nand latent Role Guidance) model, which contains two novel and effective modules\nfor the above problem. The Span-Trigger-based Contextual Pooling(STCP)\nadaptively selects and aggregates the information of non-argument clue words\nbased on the context attention weights of specific argument-trigger pairs from\npre-trained model. The Role-based Latent Information Guidance (RLIG) module\nconstructs latent role representations, makes them interact through\nrole-interactive encoding to capture semantic relevance, and merges them into\ncandidate arguments. Both STCP and RLIG introduce no more than 1% new\nparameters compared with the base model and can be easily applied to other\nevent extraction models, which are compact and transplantable. Experiments on\ntwo public datasets show that our SCPRG outperforms previous state-of-the-art\nmethods, with 1.13 F1 and 2.64 F1 improvements on RAMS and WikiEvents\nrespectively. Further analyses illustrate the interpretability of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wanlong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shaohuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Dingyi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Hong Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM for SoC Security: A Paradigm Shift. (arXiv:2310.06046v1 [cs.CR])","link":"http://arxiv.org/abs/2310.06046","description":"<p>As the ubiquity and complexity of system-on-chip (SoC) designs increase\nacross electronic devices, the task of incorporating security into an SoC\ndesign flow poses significant challenges. Existing security solutions are\ninadequate to provide effective verification of modern SoC designs due to their\nlimitations in scalability, comprehensiveness, and adaptability. On the other\nhand, Large Language Models (LLMs) are celebrated for their remarkable success\nin natural language understanding, advanced reasoning, and program synthesis\ntasks. Recognizing an opportunity, our research delves into leveraging the\nemergent capabilities of Generative Pre-trained Transformers (GPTs) to address\nthe existing gaps in SoC security, aiming for a more efficient, scalable, and\nadaptable methodology. By integrating LLMs into the SoC security verification\nparadigm, we open a new frontier of possibilities and challenges to ensure the\nsecurity of increasingly complex SoCs. This paper offers an in-depth analysis\nof existing works, showcases practical case studies, demonstrates comprehensive\nexperiments, and provides useful promoting guidelines. We also present the\nachievements, prospects, and challenges of employing LLM in different SoC\nsecurity verification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_D/0/1/0/all/0/1\">Dipayan Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarek_S/0/1/0/all/0/1\">Shams Tarek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yahyaei_K/0/1/0/all/0/1\">Katayoon Yahyaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sujan Kumar Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tehranipoor_M/0/1/0/all/0/1\">Mark Tehranipoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farahmandi_F/0/1/0/all/0/1\">Farimah Farahmandi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auditing Gender Analyzers on Text Data. (arXiv:2310.06061v1 [cs.CY])","link":"http://arxiv.org/abs/2310.06061","description":"<p>AI models have become extremely popular and accessible to the general public.\nHowever, they are continuously under the scanner due to their demonstrable\nbiases toward various sections of the society like people of color and\nnon-binary people. In this study, we audit three existing gender analyzers --\nuClassify, Readable and HackerFactor, for biases against non-binary\nindividuals. These tools are designed to predict only the cisgender binary\nlabels, which leads to discrimination against non-binary members of the\nsociety. We curate two datasets -- Reddit comments (660k) and, Tumblr posts\n(2.05M) and our experimental evaluation shows that the tools are highly\ninaccurate with the overall accuracy being ~50% on all platforms. Predictions\nfor non-binary comments on all platforms are mostly female, thus propagating\nthe societal bias that non-binary individuals are effeminate. To address this,\nwe fine-tune a BERT multi-label classifier on the two datasets in multiple\ncombinations, observe an overall performance of ~77% on the most realistically\ndeployable setting and a surprisingly higher performance of 90% for the\nnon-binary class. We also audit ChatGPT using zero-shot prompts on a small\ndataset (due to high pricing) and observe an average accuracy of 58% for Reddit\nand Tumblr combined (with overall better results for Reddit).\n</p>\n<p>Thus, we show that existing systems, including highly advanced ones like\nChatGPT are biased, and need better audits and moderation and, that such\nsocietal biases can be addressed and alleviated through simple off-the-shelf\nmodels like BERT trained on more gender inclusive datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_S/0/1/0/all/0/1\">Siddharth D Jaiswal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_A/0/1/0/all/0/1\">Ankit Kumar Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Animesh Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Multilingual Self-Supervised Pretrained Models for Sequence-to-Sequence End-to-End Spoken Language Understanding. (arXiv:2310.06103v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06103","description":"<p>A number of methods have been proposed for End-to-End Spoken Language\nUnderstanding (E2E-SLU) using pretrained models, however their evaluation often\nlacks multilingual setup and tasks that require prediction of lexical fillers,\nsuch as slot filling. In this work, we propose a unified method that integrates\nmultilingual pretrained speech and text models and performs E2E-SLU on six\ndatasets in four languages in a generative manner, including the prediction of\nlexical fillers. We investigate how the proposed method can be improved by\npretraining on widely available speech recognition data using several training\nobjectives. Pretraining on 7000 hours of multilingual data allows us to\noutperform the state-of-the-art ultimately on two SLU datasets and partly on\ntwo more SLU datasets. Finally, we examine the cross-lingual capabilities of\nthe proposed model and improve on the best known result on the\nPortMEDIA-Language dataset by almost half, achieving a Concept/Value Error Rate\nof 23.65%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Denisov_P/0/1/0/all/0/1\">Pavel Denisov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BYOC: Personalized Few-Shot Classification with Co-Authored Class Descriptions. (arXiv:2310.06111v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06111","description":"<p>Text classification is a well-studied and versatile building block for many\nNLP applications. Yet, existing approaches require either large annotated\ncorpora to train a model with or, when using large language models as a base,\nrequire carefully crafting the prompt as well as using a long context that can\nfit many examples. As a result, it is not possible for end-users to build\nclassifiers for themselves. To address this issue, we propose a novel approach\nto few-shot text classification using an LLM. Rather than few-shot examples,\nthe LLM is prompted with descriptions of the salient features of each class.\nThese descriptions are coauthored by the user and the LLM interactively: while\nthe user annotates each few-shot example, the LLM asks relevant questions that\nthe user answers. Examples, questions, and answers are summarized to form the\nclassification prompt. Our experiments show that our approach yields high\naccuracy classifiers, within 82% of the performance of models trained with\nsignificantly larger datasets while using only 1% of their training sets.\nAdditionally, in a study with 30 participants, we show that end-users are able\nto build classifiers to suit their specific needs. The personalized classifiers\nshow an average accuracy of 90%, which is 15% higher than the state-of-the-art\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bohra_A/0/1/0/all/0/1\">Arth Bohra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verkes_G/0/1/0/all/0/1\">Govert Verkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harutyunyan_A/0/1/0/all/0/1\">Artem Harutyunyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinberger_P/0/1/0/all/0/1\">Pascal Weinberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campagna_G/0/1/0/all/0/1\">Giovanni Campagna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models. (arXiv:2310.06117v1 [cs.LG])","link":"http://arxiv.org/abs/2310.06117","description":"<p>We present Step-Back Prompting, a simple prompting technique that enables\nLLMs to do abstractions to derive high-level concepts and first principles from\ninstances containing specific details. Using the concepts and principles to\nguide the reasoning steps, LLMs significantly improve their abilities in\nfollowing a correct reasoning path towards the solution. We conduct experiments\nof Step-Back Prompting with PaLM-2L models and observe substantial performance\ngains on a wide range of challenging reasoning-intensive tasks including STEM,\nKnowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting\nimproves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%,\nTimeQA by 27%, and MuSiQue by 7%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huaixiu Steven Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Heng-Tze Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed H. Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAW-coref: Conjunction-Aware Word-level Coreference Resolution. (arXiv:2310.06165v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06165","description":"<p>State-of-the-art coreference resolutions systems depend on multiple LLM calls\nper document and are thus prohibitively expensive for many use cases (e.g.,\ninformation extraction with large corpora). The leading word-level coreference\nsystem (WL-coref) attains 96.6% of these SOTA systems' performance while being\nmuch more efficient. In this work, we identify a routine yet important failure\ncase of WL-coref: dealing with conjoined mentions such as 'Tom and Mary'. We\noffer a simple yet effective solution that improves the performance on the\nOntoNotes test set by 0.9% F1, shrinking the gap between efficient word-level\ncoreference resolution and expensive SOTA approaches by 34.6%. Our\nConjunction-Aware Word-level coreference model (CAW-coref) and code is\navailable at https://github.com/KarelDO/wl-coref.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DOosterlinck_K/0/1/0/all/0/1\">Karel D&#x27;Oosterlinck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitew_S/0/1/0/all/0/1\">Semere Kiros Bitew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papineau_B/0/1/0/all/0/1\">Brandon Papineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1\">Thomas Demeester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Develder_C/0/1/0/all/0/1\">Chris Develder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Importance of Prompt Tuning for Automated Neuron Explanations. (arXiv:2310.06200v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06200","description":"<p>Recent advances have greatly increased the capabilities of large language\nmodels (LLMs), but our understanding of the models and their safety has not\nprogressed as fast. In this paper we aim to understand LLMs deeper by studying\ntheir individual neurons. We build upon previous work showing large language\nmodels such as GPT-4 can be useful in explaining what each neuron in a language\nmodel does. Specifically, we analyze the effect of the prompt used to generate\nexplanations and show that reformatting the explanation prompt in a more\nnatural way can significantly improve neuron explanation quality and greatly\nreduce computational cost. We demonstrate the effects of our new prompts in\nthree different ways, incorporating both automated and human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Justin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oikarinen_T/0/1/0/all/0/1\">Tuomas Oikarinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chantha_A/0/1/0/all/0/1\">Arjun Chantha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Keng-Chi Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yilan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_T/0/1/0/all/0/1\">Tsui-Wei Weng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compressing Context to Enhance Inference Efficiency of Large Language Models. (arXiv:2310.06201v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06201","description":"<p>Large language models (LLMs) achieved remarkable performance across various\ntasks. However, they face challenges in managing long documents and extended\nconversations, due to significantly increased computational requirements, both\nin memory and inference time, and potential context truncation when the input\nexceeds the LLM's fixed context length. This paper proposes a method called\nSelective Context that enhances the inference efficiency of LLMs by identifying\nand pruning redundancy in the input context to make the input more compact. We\ntest our approach using common data sources requiring long context processing:\narXiv papers, news articles, and long conversations, on tasks of summarisation,\nquestion answering, and response generation. Experimental results show that\nSelective Context significantly reduces memory cost and decreases generation\nlatency while maintaining comparable performance compared to that achieved when\nfull context is used. Specifically, we achieve a 50\\% reduction in context\ncost, resulting in a 36\\% reduction in inference memory usage and a 32\\%\nreduction in inference time, while observing only a minor drop of .023 in\nBERTscore and .038 in faithfulness on four downstream applications, indicating\nthat our method strikes a good balance between efficiency and performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yucheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1\">Frank Guerin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-who: An Information Density-based Machine-Generated Text Detector. (arXiv:2310.06202v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06202","description":"<p>The Uniform Information Density principle posits that humans prefer to spread\ninformation evenly during language production. In this work, we examine if the\nUID principle can help capture differences between Large Language Models (LLMs)\nand human-generated text. We propose GPT-who, the first\npsycholinguistically-aware multi-class domain-agnostic statistical-based\ndetector. This detector employs UID-based features to model the unique\nstatistical signature of each LLM and human author for accurate authorship\nattribution. We evaluate our method using 4 large-scale benchmark datasets and\nfind that GPT-who outperforms state-of-the-art detectors (both statistical- &amp;\nnon-statistical-based) such as GLTR, GPTZero, OpenAI detector, and ZeroGPT by\nover $20$% across domains. In addition to superior performance, it is\ncomputationally inexpensive and utilizes an interpretable representation of\ntext articles. We present the largest analysis of the UID-based representations\nof human and machine-generated texts (over 400k articles) to demonstrate how\nauthors distribute information differently, and in ways that enable their\ndetection using an off-the-shelf LM without any fine-tuning. We find that\nGPT-who can distinguish texts generated by very sophisticated LLMs, even when\nthe overlying text is indiscernible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venkatraman_S/0/1/0/all/0/1\">Saranya Venkatraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchendu_A/0/1/0/all/0/1\">Adaku Uchendu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongwon Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating Numbers without Regression. (arXiv:2310.06204v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06204","description":"<p>Despite recent successes in language models, their ability to represent\nnumbers is insufficient. Humans conceptualize numbers based on their\nmagnitudes, effectively projecting them on a number line; whereas subword\ntokenization fails to explicitly capture magnitude by splitting numbers into\narbitrary chunks. To alleviate this shortcoming, alternative approaches have\nbeen proposed that modify numbers at various stages of the language modeling\npipeline. These methods change either the (1) notation in which numbers are\nwritten (\\eg scientific vs decimal), the (2) vocabulary used to represent\nnumbers or the entire (3) architecture of the underlying language model, to\ndirectly regress to a desired number.\n</p>\n<p>Previous work suggests that architectural change helps achieve\nstate-of-the-art on number estimation but we find an insightful ablation:\nchanging the model's vocabulary instead (\\eg introduce a new token for numbers\nin range 10-100) is a far better trade-off. In the context of masked number\nprediction, a carefully designed tokenization scheme is both the simplest to\nimplement and sufficient, \\ie with similar performance to the state-of-the-art\napproach that requires making significant architectural changes. Finally, we\nreport similar trends on the downstream task of numerical fact estimation (for\nFermi Problems) and discuss reasons behind our findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thawani_A/0/1/0/all/0/1\">Avijit Thawani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1\">Ashwin Kalyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GeoLLM: Extracting Geospatial Knowledge from Large Language Models. (arXiv:2310.06213v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06213","description":"<p>The application of machine learning (ML) in a range of geospatial tasks is\nincreasingly common but often relies on globally available covariates such as\nsatellite imagery that can either be expensive or lack predictive power. Here\nwe explore the question of whether the vast amounts of knowledge found in\nInternet language corpora, now compressed within large language models (LLMs),\ncan be leveraged for geospatial prediction tasks. We first demonstrate that\nLLMs embed remarkable spatial information about locations, but naively querying\nLLMs using geographic coordinates alone is ineffective in predicting key\nindicators like population density. We then present GeoLLM, a novel method that\ncan effectively extract geospatial knowledge from LLMs with auxiliary map data\nfrom OpenStreetMap. We demonstrate the utility of our approach across multiple\ntasks of central interest to the international community, including the\nmeasurement of population density and economic livelihoods. Across these tasks,\nour method demonstrates a 70% improvement in performance (measured using\nPearson's $r^2$) relative to baselines that use nearest neighbors or use\ninformation directly from the prompt, and performance equal to or exceeding\nsatellite-based benchmarks in the literature. With GeoLLM, we observe that\nGPT-3.5 outperforms Llama 2 and RoBERTa by 19% and 51% respectively, suggesting\nthat the performance of our method scales well with the size of the model and\nits pretraining dataset. Our experiments reveal that LLMs are remarkably\nsample-efficient, rich in geospatial information, and robust across the globe.\nCrucially, GeoLLM shows promise in mitigating the limitations of existing\ngeospatial covariates and complementing them well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manvi_R/0/1/0/all/0/1\">Rohin Manvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanna_S/0/1/0/all/0/1\">Samar Khanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_G/0/1/0/all/0/1\">Gengchen Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burke_M/0/1/0/all/0/1\">Marshall Burke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobell_D/0/1/0/all/0/1\">David Lobell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evolution of Natural Language Processing Technology: Not Just Language Processing Towards General Purpose AI. (arXiv:2310.06228v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06228","description":"<p>Since the invention of computers, communication through natural language\n(actual human language) has been a dream technology. However, natural language\nis extremely difficult to mathematically formulate, making it difficult to\nrealize as an algorithm without considering programming. While there have been\nnumerous technological developments, one cannot say that any results allowing\nfree utilization have been achieved thus far. In the case of language learning\nin humans, for instance when learning one's mother tongue or foreign language,\none must admit that this process is similar to the adage \"practice makes\nperfect\" in principle, even though the learning method is significant up to a\npoint. Deep learning has played a central role in contemporary AI technology in\nrecent years. When applied to natural language processing (NLP), this produced\nunprecedented results. Achievements exceeding the initial predictions have been\nreported from the results of learning vast amounts of textual data using deep\nlearning. For instance, four arithmetic operations could be performed without\nexplicit learning, thereby enabling the explanation of complex images and the\ngeneration of images from corresponding explanatory texts. It is an accurate\nexample of the learner embodying the concept of \"practice makes perfect\" by\nusing vast amounts of textual data. This report provides a technological\nexplanation of how cutting-edge NLP has made it possible to realize the\n\"practice makes perfect\" principle. Additionally, examples of how this can be\napplied to business are provided. We reported in June 2022 in Japanese on the\nNLP movement from late 2021 to early 2022. We would like to summarize this as a\nmemorandum since this is just the initial movement leading to the current large\nlanguage models (LLMs).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_M/0/1/0/all/0/1\">Masahiro Yamamoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for Unbiased Question-Answering. (arXiv:2310.06238v1 [cs.CV])","link":"http://arxiv.org/abs/2310.06238","description":"<p>In recent years, there has been a growing emphasis on the intersection of\naudio, vision, and text modalities, driving forward the advancements in\nmultimodal research. However, strong bias that exists in any modality can lead\nto the model neglecting the others. Consequently, the model's ability to\neffectively reason across these diverse modalities is compromised, impeding\nfurther advancement. In this paper, we meticulously review each question type\nfrom the original dataset, selecting those with pronounced answer biases. To\ncounter these biases, we gather complementary videos and questions, ensuring\nthat no answers have outstanding skewed distribution. In particular, for binary\nquestions, we strive to ensure that both answers are almost uniformly spread\nwithin each question category. As a result, we construct a new dataset, named\nMUSIC-AVQA v2.0, which is more challenging and we believe could better foster\nthe progress of AVQA task. Furthermore, we present a novel baseline model that\ndelves deeper into the audio-visual-text interrelation. On MUSIC-AVQA v2.0,\nthis model surpasses all the existing benchmarks, improving accuracy by 2% on\nMUSIC-AVQA v2.0, setting a new state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiulong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhikang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Tuning or Prompt Tuning? A Study of Large Language Models for Clinical Concept and Relation Extraction. (arXiv:2310.06239v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06239","description":"<p>Objective To develop soft prompt-based learning algorithms for large language\nmodels (LLMs), examine the shape of prompts, prompt-tuning using\nfrozen/unfrozen LLMs, transfer learning, and few-shot learning abilities.\nMethods We developed a soft prompt-based LLM model and compared 4 training\nstrategies including (1) fine-tuning without prompts; (2) hard-prompt with\nunfrozen LLMs; (3) soft-prompt with unfrozen LLMs; and (4) soft-prompt with\nfrozen LLMs. We evaluated 7 pretrained LLMs using the 4 training strategies for\nclinical concept and relation extraction on two benchmark datasets. We\nevaluated the transfer learning ability of the prompt-based learning algorithms\nin a cross-institution setting. We also assessed the few-shot learning ability.\nResults and Conclusion When LLMs are unfrozen, GatorTron-3.9B with soft\nprompting achieves the best strict F1-scores of 0.9118 and 0.8604 for concept\nextraction, outperforming the traditional fine-tuning and hard prompt-based\nmodels by 0.6~3.1% and 1.2~2.9%, respectively; GatorTron-345M with soft\nprompting achieves the best F1-scores of 0.8332 and 0.7488 for end-to-end\nrelation extraction, outperforming the other two models by 0.2~2% and\n0.6~11.7%, respectively. When LLMs are frozen, small (i.e., 345 million\nparameters) LLMs have a big gap to be competitive with unfrozen models; scaling\nLLMs up to billions of parameters makes frozen LLMs competitive with unfrozen\nLLMs. For cross-institute evaluation, soft prompting with a frozen\nGatorTron-8.9B model achieved the best performance. This study demonstrates\nthat (1) machines can learn soft prompts better than humans, (2) frozen LLMs\nhave better few-shot learning ability and transfer learning ability to\nfacilitate muti-institution applications, and (3) frozen LLMs require large\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1\">Cheng Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kaleb E Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zehao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Aokun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"We are what we repeatedly do: Inducing and deploying habitual schemas in persona-based responses. (arXiv:2310.06245v1 [cs.AI])","link":"http://arxiv.org/abs/2310.06245","description":"<p>Many practical applications of dialogue technology require the generation of\nresponses according to a particular developer-specified persona. While a\nvariety of personas can be elicited from recent large language models, the\nopaqueness and unpredictability of these models make it desirable to be able to\nspecify personas in an explicit form. In previous work, personas have typically\nbeen represented as sets of one-off pieces of self-knowledge that are retrieved\nby the dialogue system for use in generation. However, in realistic human\nconversations, personas are often revealed through story-like narratives that\ninvolve rich habitual knowledge -- knowledge about kinds of events that an\nagent often participates in (e.g., work activities, hobbies, sporting\nactivities, favorite entertainments, etc.), including typical goals,\nsub-events, preconditions, and postconditions of those events. We capture such\nhabitual knowledge using an explicit schema representation, and propose an\napproach to dialogue generation that retrieves relevant schemas to condition a\nlarge language model to generate persona-based responses. Furthermore, we\ndemonstrate a method for bootstrapping the creation of such schemas by first\ngenerating generic passages from a set of simple facts, and then inducing\nschemas from the generated passages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kane_B/0/1/0/all/0/1\">Benjamin Kane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubert_L/0/1/0/all/0/1\">Lenhart Schubert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Get the gist? Using large language models for few-shot decontextualization. (arXiv:2310.06254v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06254","description":"<p>In many NLP applications that involve interpreting sentences within a rich\ncontext -- for instance, information retrieval systems or dialogue systems --\nit is desirable to be able to preserve the sentence in a form that can be\nreadily understood without context, for later reuse -- a process known as\n``decontextualization''. While previous work demonstrated that generative\nSeq2Seq models could effectively perform decontextualization after being\nfine-tuned on a specific dataset, this approach requires expensive human\nannotations and may not transfer to other domains. We propose a few-shot method\nof decontextualization using a large language model, and present preliminary\nresults showing that this method achieves viable performance on multiple\ndomains using only a small set of examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kane_B/0/1/0/all/0/1\">Benjamin Kane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubert_L/0/1/0/all/0/1\">Lenhart Schubert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An experiment on an automated literature survey of data-driven speech enhancement methods. (arXiv:2310.06260v1 [cs.SD])","link":"http://arxiv.org/abs/2310.06260","description":"<p>The increasing number of scientific publications in acoustics, in general,\npresents difficulties in conducting traditional literature surveys. This work\nexplores the use of a generative pre-trained transformer (GPT) model to\nautomate a literature survey of 116 articles on data-driven speech enhancement\nmethods. The main objective is to evaluate the capabilities and limitations of\nthe model in providing accurate responses to specific queries about the papers\nselected from a reference human-based survey. While we see great potential to\nautomate literature surveys in acoustics, improvements are needed to address\ntechnical questions more clearly and accurately.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santos_A/0/1/0/all/0/1\">Arthur dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_J/0/1/0/all/0/1\">Jayr Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masiero_B/0/1/0/all/0/1\">Bruno Masiero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sander_Tavallaey_S/0/1/0/all/0/1\">Shiva Sander-Tavallaey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zea_E/0/1/0/all/0/1\">Elias Zea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Mitigating Hallucination in Large Language Models via Self-Reflection. (arXiv:2310.06271v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06271","description":"<p>Large language models (LLMs) have shown promise for generative and\nknowledge-intensive tasks including question-answering (QA) tasks. However, the\npractical deployment still faces challenges, notably the issue of\n\"hallucination\", where models generate plausible-sounding but unfaithful or\nnonsensical information. This issue becomes particularly critical in the\nmedical domain due to the uncommon professional concepts and potential social\nrisks involved. This paper analyses the phenomenon of hallucination in medical\ngenerative QA systems using widely adopted LLMs and datasets. Our investigation\ncenters on the identification and comprehension of common problematic answers,\nwith a specific emphasis on hallucination. To tackle this challenge, we present\nan interactive self-reflection methodology that incorporates knowledge\nacquisition and answer generation. Through this feedback process, our approach\nsteadily enhances the factuality, consistency, and entailment of the generated\nanswers. Consequently, we harness the interactivity and multitasking ability of\nLLMs and produce progressively more precise and accurate answers. Experimental\nresults on both automatic and human evaluation demonstrate the superiority of\nour approach in hallucination reduction compared to baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Ziwei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1\">Etsuko Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Let Models Speak Ciphers: Multiagent Debate through Embeddings. (arXiv:2310.06272v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06272","description":"<p>Discussion and debate among Large Language Models (LLMs) have gained\nconsiderable attention due to their potential to enhance the reasoning ability\nof LLMs. Although natural language is an obvious choice for communication due\nto LLM's language understanding capability, the token sampling step needed when\ngenerating natural language poses a potential risk of information loss, as it\nuses only one token to represent the model's belief across the entire\nvocabulary. In this paper, we introduce a communication regime named CIPHER\n(Communicative Inter-Model Protocol Through Embedding Representation) to\naddress this issue. Specifically, we remove the token sampling step from LLMs\nand let them communicate their beliefs across the vocabulary through the\nexpectation of the raw transformer output embeddings. Remarkably, by deviating\nfrom natural language, CIPHER offers an advantage of encoding a broader\nspectrum of information without any modification to the model weights. While\nthe state-of-the-art LLM debate methods using natural language outperforms\ntraditional inference by a margin of 1.5-8%, our experiment results show that\nCIPHER debate further extends this lead by 1-3.5% across five reasoning tasks\nand multiple open-source LLMs of varying sizes. This showcases the superiority\nand robustness of embeddings as an alternative \"language\" for communication\namong LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pham_C/0/1/0/all/0/1\">Chau Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Boyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yingxiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jianbo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1\">Bryan A. Plummer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selective Demonstrations for Cross-domain Text-to-SQL. (arXiv:2310.06302v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06302","description":"<p>Large language models (LLMs) with in-context learning have demonstrated\nimpressive generalization capabilities in the cross-domain text-to-SQL task,\nwithout the use of in-domain annotations. However, incorporating in-domain\ndemonstration examples has been found to greatly enhance LLMs' performance. In\nthis paper, we delve into the key factors within in-domain examples that\ncontribute to the improvement and explore whether we can harness these benefits\nwithout relying on in-domain annotations. Based on our findings, we propose a\ndemonstration selection framework ODIS which utilizes both out-of-domain\nexamples and synthetically generated in-domain examples to construct\ndemonstrations. By retrieving demonstrations from hybrid sources, ODIS\nleverages the advantages of both, showcasing its effectiveness compared to\nbaseline methods that rely on a single data source. Furthermore, ODIS\noutperforms state-of-the-art approaches on two cross-domain text-to-SQL\ndatasets, with improvements of 1.1 and 11.8 points in execution accuracy,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuaichen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fosler_Lussier_E/0/1/0/all/0/1\">Eric Fosler-Lussier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Semantic Invariant Robust Watermark for Large Language Models. (arXiv:2310.06356v1 [cs.CR])","link":"http://arxiv.org/abs/2310.06356","description":"<p>Watermark algorithms for large language models (LLMs) have achieved extremely\nhigh accuracy in detecting text generated by LLMs. Such algorithms typically\ninvolve adding extra watermark logits to the LLM's logits at each generation\nstep. However, prior algorithms face a trade-off between attack robustness and\nsecurity robustness. This is because the watermark logits for a token are\ndetermined by a certain number of preceding tokens; a small number leads to low\nsecurity robustness, while a large number results in insufficient attack\nrobustness. In this work, we propose a semantic invariant watermarking method\nfor LLMs that provides both attack robustness and security robustness. The\nwatermark logits in our work are determined by the semantics of all preceding\ntokens. Specifically, we utilize another embedding LLM to generate semantic\nembeddings for all preceding tokens, and then these semantic embeddings are\ntransformed into the watermark logits through our trained watermark model.\nSubsequent analyses and experiments demonstrated the attack robustness of our\nmethod in semantically invariant settings: synonym substitution and text\nparaphrasing settings. Finally, we also show that our watermark possesses\nadequate security robustness. Our code and data are available at\nhttps://github.com/THU-BPM/Robust_Watermark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Leyi Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_S/0/1/0/all/0/1\">Shiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InfoCL: Alleviating Catastrophic Forgetting in Continual Text Classification from An Information Theoretic Perspective. (arXiv:2310.06362v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06362","description":"<p>Continual learning (CL) aims to constantly learn new knowledge over time\nwhile avoiding catastrophic forgetting on old tasks. We focus on continual text\nclassification under the class-incremental setting. Recent CL studies have\nidentified the severe performance decrease on analogous classes as a key factor\nfor catastrophic forgetting. In this paper, through an in-depth exploration of\nthe representation learning process in CL, we discover that the compression\neffect of the information bottleneck leads to confusion on analogous classes.\nTo enable the model learn more sufficient representations, we propose a novel\nreplay-based continual text classification method, InfoCL. Our approach\nutilizes fast-slow and current-past contrastive learning to perform mutual\ninformation maximization and better recover the previously learned\nrepresentations. In addition, InfoCL incorporates an adversarial memory\naugmentation strategy to alleviate the overfitting problem of replay.\nExperimental results demonstrate that InfoCL effectively mitigates forgetting\nand achieves state-of-the-art performance on three text classification tasks.\nThe code is publicly available at https://github.com/Yifan-Song793/InfoCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yifan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Weimin Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dawei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sujian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal Knowledge Graph Transformer Framework for Multi-Modal Entity Alignment. (arXiv:2310.06365v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06365","description":"<p>Multi-Modal Entity Alignment (MMEA) is a critical task that aims to identify\nequivalent entity pairs across multi-modal knowledge graphs (MMKGs). However,\nthis task faces challenges due to the presence of different types of\ninformation, including neighboring entities, multi-modal attributes, and entity\ntypes. Directly incorporating the above information (e.g., concatenation or\nattention) can lead to an unaligned information space. To address these\nchallenges, we propose a novel MMEA transformer, called MoAlign, that\nhierarchically introduces neighbor features, multi-modal attributes, and entity\ntypes to enhance the alignment task. Taking advantage of the transformer's\nability to better integrate multiple information, we design a hierarchical\nmodifiable self-attention block in a transformer encoder to preserve the unique\nsemantics of different information. Furthermore, we design two entity-type\nprefix injection methods to integrate entity-type information using type\nprefixes, which help to restrict the global information of entities not present\nin the MMKGs. Our extensive experiments on benchmark datasets demonstrate that\nour approach outperforms strong competitors and achieves excellent entity\nalignment performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_C/0/1/0/all/0/1\">Cheng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhaoji Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Model Selection and Decoding for Keyphrase Generation with Pre-trained Sequence-to-Sequence Models. (arXiv:2310.06374v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06374","description":"<p>Keyphrase Generation (KPG) is a longstanding task in NLP with widespread\napplications. The advent of sequence-to-sequence (seq2seq) pre-trained language\nmodels (PLMs) has ushered in a transformative era for KPG, yielding promising\nperformance improvements. However, many design decisions remain unexplored and\nare often made arbitrarily. This paper undertakes a systematic analysis of the\ninfluence of model selection and decoding strategies on PLM-based KPG. We begin\nby elucidating why seq2seq PLMs are apt for KPG, anchored by an\nattention-driven hypothesis. We then establish that conventional wisdom for\nselecting seq2seq PLMs lacks depth: (1) merely increasing model size or\nperforming task-specific adaptation is not parameter-efficient; (2) although\ncombining in-domain pre-training with task adaptation benefits KPG, it does\npartially hinder generalization. Regarding decoding, we demonstrate that while\ngreedy search delivers strong F1 scores, it lags in recall compared with\nsampling-based methods. From our insights, we propose DeSel, a likelihood-based\ndecode-select algorithm that improves greedy search by an average of 4.7%\nsemantic F1 across five datasets. Our collective findings pave the way for\ndeeper future investigations into PLM-based KPG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations. (arXiv:2310.06387v1 [cs.LG])","link":"http://arxiv.org/abs/2310.06387","description":"<p>Large Language Models (LLMs) have shown remarkable success in various tasks,\nbut concerns about their safety and the potential for generating malicious\ncontent have emerged. In this paper, we explore the power of In-Context\nLearning (ICL) in manipulating the alignment ability of LLMs. We find that by\nproviding just few in-context demonstrations without fine-tuning, LLMs can be\nmanipulated to increase or decrease the probability of jailbreaking, i.e.\nanswering malicious prompts. Based on these observations, we propose In-Context\nAttack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding\naligned language model purposes. ICA crafts malicious contexts to guide models\nin generating harmful outputs, while ICD enhances model robustness by\ndemonstrations of rejecting to answer harmful prompts. Our experiments show the\neffectiveness of ICA and ICD in increasing or reducing the success rate of\nadversarial jailbreaking attacks. Overall, we shed light on the potential of\nICL to influence LLM behavior and provide a new perspective for enhancing the\nsafety and alignment of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zeming Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"P5: Plug-and-Play Persona Prompting for Personalized Response Selection. (arXiv:2310.06390v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06390","description":"<p>The use of persona-grounded retrieval-based chatbots is crucial for\npersonalized conversations, but there are several challenges that need to be\naddressed. 1) In general, collecting persona-grounded corpus is very expensive.\n2) The chatbot system does not always respond in consideration of persona at\nreal applications. To address these challenges, we propose a plug-and-play\npersona prompting method. Our system can function as a standard open-domain\nchatbot if persona information is not available. We demonstrate that this\napproach performs well in the zero-shot setting, which reduces the dependence\non persona-ground training data. This makes it easier to expand the system to\nother languages without the need to build a persona-grounded corpus.\nAdditionally, our model can be fine-tuned for even better performance. In our\nexperiments, the zero-shot model improved the standard model by 7.71 and 1.04\npoints in the original persona and revised persona, respectively. The\nfine-tuned model improved the previous state-of-the-art system by 1.95 and 3.39\npoints in the original persona and revised persona, respectively. To the best\nof our knowledge, this is the first attempt to solve the problem of\npersonalized response selection using prompt sequences. Our code is available\non github~\\footnote{https://github.com/rungjoo/plug-and-play-prompt-persona}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joosung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1\">Minsik Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Donghun Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved prompting and process for writing user personas with LLMs, using qualitative interviews: Capturing behaviour and personality traits of users. (arXiv:2310.06391v1 [cs.HC])","link":"http://arxiv.org/abs/2310.06391","description":"<p>This draft paper presents a workflow for creating User Personas with Large\nLanguage Models, using the results of a Thematic Analysis of qualitative\ninterviews. The proposed workflow uses improved prompting and a larger pool of\nThemes, compared to previous work conducted by the author for the same task.\nThis is possible due to the capabilities of a recently released LLM which\nallows the processing of 16 thousand tokens (GPT3.5-Turbo-16k) and also due to\nthe possibility to offer a refined prompting for the creation of Personas. The\npaper offers details of performing Phase 2 and 3 of Thematic Analysis, and then\ndiscusses the improved workflow for creating Personas. The paper also offers\nsome reflections on the relationship between the proposed process and existing\napproaches to Personas such as the data-driven and qualitative Personas.\nMoreover, the paper offers reflections on the capacity of LLMs to capture user\nbehaviours and personality traits, from the underlying dataset of qualitative\ninterviews used for the analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paoli_S/0/1/0/all/0/1\">Stefano De Paoli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hexa: Self-Improving for Knowledge-Grounded Dialogue System. (arXiv:2310.06404v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06404","description":"<p>A common practice in knowledge-grounded dialogue generation is to explicitly\nutilize intermediate steps (e.g., web-search, memory retrieval) with modular\napproaches. However, data for such steps are often inaccessible compared to\nthose of dialogue responses as they are unobservable in an ordinary dialogue.\nTo fill in the absence of these data, we develop a self-improving method to\nimprove the generative performances of intermediate steps without the ground\ntruth data. In particular, we propose a novel bootstrapping scheme with a\nguided prompt and a modified loss function to enhance the diversity of\nappropriate self-generated responses. Through experiments on various benchmark\ndatasets, we empirically demonstrate that our method successfully leverages a\nself-improving mechanism in generating intermediate and final responses and\nimproves the performances on the task of knowledge-grounded dialogue\ngeneration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jo_D/0/1/0/all/0/1\">Daejin Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_D/0/1/0/all/0/1\">Daniel Wontae Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1\">Gunsoo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+On_K/0/1/0/all/0/1\">Kyoung-Woon On</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_T/0/1/0/all/0/1\">Taehwan Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rho_S/0/1/0/all/0/1\">Seungeun Rho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungwoong Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Humans and language models diverge when predicting repeating text. (arXiv:2310.06408v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06408","description":"<p>Language models that are trained on the next-word prediction task have been\nshown to accurately model human behavior in word prediction and reading speed.\nIn contrast with these findings, we present a scenario in which the performance\nof humans and LMs diverges. We collected a dataset of human next-word\npredictions for five stimuli that are formed by repeating spans of text. Human\nand GPT-2 LM predictions are strongly aligned in the first presentation of a\ntext span, but their performance quickly diverges when memory (or in-context\nlearning) begins to play a role. We traced the cause of this divergence to\nspecific attention heads in a middle layer. Adding a power-law recency bias to\nthese attention heads yielded a model that performs much more similarly to\nhumans. We hope that this scenario will spur future work in bringing LMs closer\nto human behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vaidya_A/0/1/0/all/0/1\">Aditya R. Vaidya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turek_J/0/1/0/all/0/1\">Javier Turek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huth_A/0/1/0/all/0/1\">Alexander G. Huth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models for Propaganda Detection. (arXiv:2310.06422v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06422","description":"<p>The prevalence of propaganda in our digital society poses a challenge to\nsocietal harmony and the dissemination of truth. Detecting propaganda through\nNLP in text is challenging due to subtle manipulation techniques and contextual\ndependencies. To address this issue, we investigate the effectiveness of modern\nLarge Language Models (LLMs) such as GPT-3 and GPT-4 for propaganda detection.\nWe conduct experiments using the SemEval-2020 task 11 dataset, which features\nnews articles labeled with 14 propaganda techniques as a multi-label\nclassification problem. Five variations of GPT-3 and GPT-4 are employed,\nincorporating various prompt engineering and fine-tuning strategies across the\ndifferent models. We evaluate the models' performance by assessing metrics such\nas $F1$ score, $Precision$, and $Recall$, comparing the results with the\ncurrent state-of-the-art approach using RoBERTa. Our findings demonstrate that\nGPT-4 achieves comparable results to the current state-of-the-art. Further,\nthis study analyzes the potential and challenges of LLMs in complex tasks like\npropaganda detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sprenkamp_K/0/1/0/all/0/1\">Kilian Sprenkamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_D/0/1/0/all/0/1\">Daniel Gordon Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zavolokina_L/0/1/0/all/0/1\">Liudmila Zavolokina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retromorphic Testing: A New Approach to the Test Oracle Problem. (arXiv:2310.06433v1 [cs.SE])","link":"http://arxiv.org/abs/2310.06433","description":"<p>A test oracle serves as a criterion or mechanism to assess the correspondence\nbetween software output and the anticipated behavior for a given input set. In\nautomated testing, black-box techniques, known for their non-intrusive nature\nin test oracle construction, are widely used, including notable methodologies\nlike differential testing and metamorphic testing. Inspired by the mathematical\nconcept of inverse function, we present Retromorphic Testing, a novel black-box\ntesting methodology. It leverages an auxiliary program in conjunction with the\nprogram under test, which establishes a dual-program structure consisting of a\nforward program and a backward program. The input data is first processed by\nthe forward program and then its program output is reversed to its original\ninput format using the backward program. In particular, the auxiliary program\ncan operate as either the forward or backward program, leading to different\ntesting modes. The process concludes by examining the relationship between the\ninitial input and the transformed output within the input domain. For example,\nto test the implementation of the sine function $\\sin(x)$, we can employ its\ninverse function, $\\arcsin(x)$, and validate the equation $x =\n\\sin(\\arcsin(x)+2k\\pi), \\forall k \\in \\mathbb{Z}$. In addition to the\nhigh-level concept of Retromorphic Testing, this paper presents its three\ntesting modes with illustrative use cases across diverse programs, including\nalgorithms, traditional software, and AI applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Boxi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mang_Q/0/1/0/all/0/1\">Qiuyang Mang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qingshuo Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pinjia He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition. (arXiv:2310.06434v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06434","description":"<p>We introduce a new cross-modal fusion technique designed for generative error\ncorrection in automatic speech recognition (ASR). Our methodology leverages\nboth acoustic information and external linguistic representations to generate\naccurate speech transcription contexts. This marks a step towards a fresh\nparadigm in generative error correction within the realm of n-best hypotheses.\nUnlike the existing ranking-based rescoring methods, our approach adeptly uses\ndistinct initialization techniques and parameter-efficient algorithms to boost\nASR performance derived from pre-trained speech and text models. Through\nevaluation across diverse ASR datasets, we evaluate the stability and\nreproducibility of our fusion technique, demonstrating its improved word error\nrate relative (WERR) performance in comparison to n-best hypotheses by\nrelatively 37.66%. To encourage future research, we have made our code and\npre-trained models open source at\nhttps://github.com/Srijith-rkr/Whispering-LLaMA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Radhakrishnan_S/0/1/0/all/0/1\">Srijith Radhakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Sumeer Ahmad Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Rohit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiani_N/0/1/0/all/0/1\">Narsis A. Kiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Cabrero_D/0/1/0/all/0/1\">David Gomez-Cabrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tegner_J/0/1/0/all/0/1\">Jesper N. Tegner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MemSum-DQA: Adapting An Efficient Long Document Extractive Summarizer for Document Question Answering. (arXiv:2310.06436v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06436","description":"<p>We introduce MemSum-DQA, an efficient system for document question answering\n(DQA) that leverages MemSum, a long document extractive summarizer. By\nprefixing each text block in the parsed document with the provided question and\nquestion type, MemSum-DQA selectively extracts text blocks as answers from\ndocuments. On full-document answering tasks, this approach yields a 9%\nimprovement in exact match accuracy over prior state-of-the-art baselines.\nNotably, MemSum-DQA excels in addressing questions related to\nchild-relationship understanding, underscoring the potential of extractive\nsummarization techniques for DQA tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_N/0/1/0/all/0/1\">Nianlong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yingqiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahnloser_R/0/1/0/all/0/1\">Richard H. R. Hahnloser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructive Large Language Models Alignment with Diverse Feedback. (arXiv:2310.06450v1 [cs.CL])","link":"http://arxiv.org/abs/2310.06450","description":"<p>In recent research on large language models (LLMs), there has been a growing\nemphasis on aligning these models with human values to reduce the impact of\nharmful content. However, current alignment methods often rely solely on\nsingular forms of human feedback, such as preferences, annotated labels, or\nnatural language critiques, overlooking the potential advantages of combining\nthese feedback types. This limitation leads to suboptimal performance, even\nwhen ample training data is available. In this paper, we introduce Constructive\nand Diverse Feedback (CDF) as a novel method to enhance LLM alignment, inspired\nby constructivist learning theory. Our approach involves collecting three\ndistinct types of feedback tailored to problems of varying difficulty levels\nwithin the training dataset. Specifically, we exploit critique feedback for\neasy problems, refinement feedback for medium problems, and preference feedback\nfor hard problems. By training our model with this diversified feedback, we\nachieve enhanced alignment performance while using less training data. To\nassess the effectiveness of CDF, we evaluate it against previous methods in\nthree downstream tasks: question answering, dialog generation, and text\nsummarization. Experimental results demonstrate that CDF achieves superior\nperformance even with a smaller training dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianshu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Ting-En Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuchuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memorization of Named Entities in Fine-tuned BERT Models. (arXiv:2212.03749v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.03749","description":"<p>Privacy preserving deep learning is an emerging field in machine learning\nthat aims to mitigate the privacy risks in the use of deep neural networks. One\nsuch risk is training data extraction from language models that have been\ntrained on datasets, which contain personal and privacy sensitive information.\nIn our study, we investigate the extent of named entity memorization in\nfine-tuned BERT models. We use single-label text classification as\nrepresentative downstream task and employ three different fine-tuning setups in\nour experiments, including one with Differentially Privacy (DP). We create a\nlarge number of text samples from the fine-tuned BERT models utilizing a custom\nsequential sampling strategy with two prompting strategies. We search in these\nsamples for named entities and check if they are also present in the\nfine-tuning datasets. We experiment with two benchmark datasets in the domains\nof emails and blogs. We show that the application of DP has a detrimental\neffect on the text generation capabilities of BERT. Furthermore, we show that a\nfine-tuned BERT does not generate more named entities specific to the\nfine-tuning dataset than a BERT model that is pre-trained only. This suggests\nthat BERT is unlikely to emit personal or privacy sensitive named entities.\nOverall, our results are important to understand to what extent BERT-based\nservices are prone to training data extraction attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diera_A/0/1/0/all/0/1\">Andor Diera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lell_N/0/1/0/all/0/1\">Nicolas Lell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garifullina_A/0/1/0/all/0/1\">Aygul Garifullina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1\">Ansgar Scherp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Translationese in Cross-Lingual Summarization. (arXiv:2212.07220v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.07220","description":"<p>Given a document in a source language, cross-lingual summarization (CLS) aims\nat generating a concise summary in a different target language. Unlike\nmonolingual summarization (MS), naturally occurring source-language documents\npaired with target-language summaries are rare. To collect large-scale CLS\ndata, existing datasets typically involve translation in their creation.\nHowever, the translated text is distinguished from the text originally written\nin that language, i.e., translationese. In this paper, we first confirm that\ndifferent approaches of constructing CLS datasets will lead to different\ndegrees of translationese. Then we systematically investigate how\ntranslationese affects CLS model evaluation and performance when it appears in\nsource documents or target summaries. In detail, we find that (1) the\ntranslationese in documents or summaries of test sets might lead to the\ndiscrepancy between human judgment and automatic evaluation; (2) the\ntranslationese in training sets would harm model performance in real-world\napplications; (3) though machine-translated documents involve translationese,\nthey are very useful for building CLS systems on low-resource languages under\nspecific training strategies. Lastly, we give suggestions for future CLS\nresearch including dataset and model developments. We hope that our work could\nlet researchers notice the phenomenon of translationese in CLS and take it into\naccount in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tingyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiarong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guiding Large Language Models via Directional Stimulus Prompting. (arXiv:2302.11520v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.11520","description":"<p>We introduce Directional Stimulus Prompting, a novel framework for guiding\nblack-box large language models (LLMs) toward specific desired outputs. Instead\nof directly adjusting LLMs, our method employs a small tunable policy model\n(e.g., T5) to generate an auxiliary directional stimulus prompt for each input\ninstance. These directional stimulus prompts act as nuanced, instance-specific\nhints and clues to guide LLMs in generating desired outcomes, such as including\nspecific keywords in the generated summary. Our approach sidesteps the\nchallenges of direct LLM tuning by optimizing the policy model to explore\ndirectional stimulus prompts that align LLMs with desired behaviors. The policy\nmodel can be optimized through 1) supervised fine-tuning using labeled data and\n2) reinforcement learning from offline or online rewards based on the LLM's\noutput. We assess our method across summarization, dialogue response\ngeneration, and chain-of-thought reasoning tasks. Our experiments demonstrate\nthat the framework consistently improves LLMs' (e.g., ChatGPT, Codex,\nInstructGPT) performance on these supervised tasks using minimal labeled data.\nNotably, using just 80 dialogues on the MultiWOZ dataset, our approach enhances\nChatGPT's performance by an impressive 41.4%, matching or surpassing some fully\nsupervised start-of-the-art models. Additionally, the instance-specific\nchain-of-thought prompt generated by our approach improves InstructGPT's\nreasoning accuracy compared to human-crafted or automatically generated\nprompts. The code and data are publicly available at\n\\url{https://github.com/Leezekun/Directional-Stimulus-Prompting}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1\">Michel Galley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xifeng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Speech Recognition for Language-Guided Embodied Agents. (arXiv:2302.14030v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.14030","description":"<p>Benchmarks for language-guided embodied agents typically assume text-based\ninstructions, but deployed agents will encounter spoken instructions. While\nAutomatic Speech Recognition (ASR) models can bridge the input gap, erroneous\nASR transcripts can hurt the agents' ability to complete tasks. In this work,\nwe propose training a multimodal ASR model to reduce errors in transcribing\nspoken instructions by considering the accompanying visual context. We train\nour model on a dataset of spoken instructions, synthesized from the ALFRED task\ncompletion dataset, where we simulate acoustic noise by systematically masking\nspoken words. We find that utilizing visual observations facilitates masked\nword recovery, with multimodal ASR models recovering up to 30% more masked\nwords than unimodal baselines. We also find that a text-trained embodied agent\nsuccessfully completes tasks more often by following transcribed instructions\nfrom multimodal ASR models. github.com/Cylumn/embodied-multimodal-asr\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1\">Allen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoyuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monga_A/0/1/0/all/0/1\">Aarav Monga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Seoho Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_T/0/1/0/all/0/1\">Tejas Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"N-best T5: Robust ASR Error Correction using Multiple Input Hypotheses and Constrained Decoding Space. (arXiv:2303.00456v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.00456","description":"<p>Error correction models form an important part of Automatic Speech\nRecognition (ASR) post-processing to improve the readability and quality of\ntranscriptions. Most prior works use the 1-best ASR hypothesis as input and\ntherefore can only perform correction by leveraging the context within one\nsentence. In this work, we propose a novel N-best T5 model for this task, which\nis fine-tuned from a T5 model and utilizes ASR N-best lists as model input. By\ntransferring knowledge from the pre-trained language model and obtaining richer\ninformation from the ASR decoding space, the proposed approach outperforms a\nstrong Conformer-Transducer baseline. Another issue with standard error\ncorrection is that the generation process is not well-guided. To address this a\nconstrained decoding process, either based on the N-best list or an ASR\nlattice, is used which allows additional information to be propagated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark J. F. Gales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knill_K/0/1/0/all/0/1\">Kate M. Knill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_M/0/1/0/all/0/1\">Mengjie Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reflexion: Language Agents with Verbal Reinforcement Learning. (arXiv:2303.11366v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2303.11366","description":"<p>Large language models (LLMs) have been increasingly used to interact with\nexternal environments (e.g., games, compilers, APIs) as goal-driven agents.\nHowever, it remains challenging for these language agents to quickly and\nefficiently learn from trial-and-error as traditional reinforcement learning\nmethods require extensive training samples and expensive model fine-tuning. We\npropose Reflexion, a novel framework to reinforce language agents not by\nupdating weights, but instead through linguistic feedback. Concretely,\nReflexion agents verbally reflect on task feedback signals, then maintain their\nown reflective text in an episodic memory buffer to induce better\ndecision-making in subsequent trials. Reflexion is flexible enough to\nincorporate various types (scalar values or free-form language) and sources\n(external or internally simulated) of feedback signals, and obtains significant\nimprovements over a baseline agent across diverse tasks (sequential\ndecision-making, coding, language reasoning). For example, Reflexion achieves a\n91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous\nstate-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis\nstudies using different feedback signals, feedback incorporation methods, and\nagent types, and provide insights into how they affect performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shinn_N/0/1/0/all/0/1\">Noah Shinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassano_F/0/1/0/all/0/1\">Federico Cassano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berman_E/0/1/0/all/0/1\">Edward Berman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopinath_A/0/1/0/all/0/1\">Ashwin Gopinath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1\">Shunyu Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Human Collaboration Enhance the Accuracy of Identifying LLM-Generated Deepfake Texts?. (arXiv:2304.01002v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01002","description":"<p>Advances in Large Language Models (e.g., GPT-4, LLaMA) have improved the\ngeneration of coherent sentences resembling human writing on a large scale,\nresulting in the creation of so-called deepfake texts. However, this progress\nposes security and privacy concerns, necessitating effective solutions for\ndistinguishing deepfake texts from human-written ones. Although prior works\nstudied humans' ability to detect deepfake texts, none has examined whether\n\"collaboration\" among humans improves the detection of deepfake texts. In this\nstudy, to address this gap of understanding on deepfake texts, we conducted\nexperiments with two groups: (1) nonexpert individuals from the AMT platform\nand (2) writing experts from the Upwork platform. The results demonstrate that\ncollaboration among humans can potentially improve the detection of deepfake\ntexts for both groups, increasing detection accuracies by 6.36% for non-experts\nand 12.76% for experts, respectively, compared to individuals' detection\naccuracies. We further analyze the explanations that humans used for detecting\na piece of text as deepfake text, and find that the strongest indicator of\ndeepfake texts is their lack of coherence and consistency. Our study provides\nuseful insights for future tools and framework designs to facilitate the\ncollaborative human detection of deepfake texts. The experiment datasets and\nAMT implementations are available at:\nhttps://github.com/huashen218/llm-deepfake-human-study.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uchendu_A/0/1/0/all/0/1\">Adaku Uchendu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jooyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thai Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Hao &#x27;Kenneth&#x27; Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongwon Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Interpretable Mental Health Analysis with Large Language Models. (arXiv:2304.03347v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.03347","description":"<p>The latest large language models (LLMs) such as ChatGPT, exhibit strong\ncapabilities in automated mental health analysis. However, existing relevant\nstudies bear several limitations, including inadequate evaluations, lack of\nprompting strategies, and ignorance of exploring LLMs for explainability. To\nbridge these gaps, we comprehensively evaluate the mental health analysis and\nemotional reasoning ability of LLMs on 11 datasets across 5 tasks. We explore\nthe effects of different prompting strategies with unsupervised and distantly\nsupervised emotional information. Based on these prompts, we explore LLMs for\ninterpretable mental health analysis by instructing them to generate\nexplanations for each of their decisions. We convey strict human evaluations to\nassess the quality of the generated explanations, leading to a novel dataset\nwith 163 human-assessed explanations. We benchmark existing automatic\nevaluation metrics on this dataset to guide future related works. According to\nthe results, ChatGPT shows strong in-context learning ability but still has a\nsignificant gap with advanced task-specific methods. Careful prompt engineering\nwith emotional cues and expert-written few-shot examples can also effectively\nimprove performance on mental health analysis. In addition, ChatGPT generates\nexplanations that approach human performance, showing its great potential in\nexplainable mental health analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qianqian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1\">Ziyan Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1\">Sophia Ananiadou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extractive Summarization via ChatGPT for Faithful Summary Generation. (arXiv:2304.04193v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.04193","description":"<p>Extractive summarization is a crucial task in natural language processing\nthat aims to condense long documents into shorter versions by directly\nextracting sentences. The recent introduction of large language models has\nattracted significant interest in the NLP community due to its remarkable\nperformance on a wide range of downstream tasks. This paper first presents a\nthorough evaluation of ChatGPT's performance on extractive summarization and\ncompares it with traditional fine-tuning methods on various benchmark datasets.\nOur experimental analysis reveals that ChatGPT exhibits inferior extractive\nsummarization performance in terms of ROUGE scores compared to existing\nsupervised systems, while achieving higher performance based on LLM-based\nevaluation metrics. In addition, we explore the effectiveness of in-context\nlearning and chain-of-thought reasoning for enhancing its performance.\nFurthermore, we find that applying an extract-then-generate pipeline with\nChatGPT yields significant performance improvements over abstractive baselines\nin terms of summary faithfulness. These observations highlight potential\ndirections for enhancing ChatGPT's capabilities in faithful summarization using\ntwo-stage approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summarizing Multiple Documents with Conversational Structure for Meta-Review Generation. (arXiv:2305.01498v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01498","description":"<p>We present PeerSum, a novel dataset for generating meta-reviews of scientific\npapers. The meta-reviews can be interpreted as abstractive summaries of\nreviews, multi-turn discussions and the paper abstract. These source documents\nhave rich inter-document relationships with an explicit hierarchical\nconversational structure, cross-references and (occasionally) conflicting\ninformation. To introduce the structural inductive bias into pre-trained\nlanguage models, we introduce Rammer ( Relationship-aware Multi-task\nMeta-review Generator), a model that uses sparse attention based on the\nconversational structure and a multi-task training objective that predicts\nmetadata features (e.g., review ratings). Our experimental results show that\nRammer outperforms other strong baseline models in terms of a suite of\nautomatic evaluation metrics. Further analyses, however, reveal that RAMMER and\nother models struggle to handle conflicts in source documents of PeerSum,\nsuggesting meta-review generation is a challenging task and a promising avenue\nfor further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Miao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt. (arXiv:2305.11186v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.11186","description":"<p>While the numerous parameters in Large Language Models (LLMs) contribute to\ntheir superior performance, this massive scale makes them inefficient and\nmemory-hungry. Thus, they are hard to deploy on commodity hardware, such as one\nsingle GPU. Given the memory and power constraints of such devices, model\ncompression methods are widely employed to reduce both the model size and\ninference latency, which essentially trades off model quality in return for\nimproved efficiency. Thus, optimizing this accuracy-efficiency trade-off is\ncrucial for the LLM deployment on commodity hardware. In this paper, we\nintroduce a new perspective to optimize this trade-off by prompting compressed\nmodels. Specifically, we first observe that for certain questions, the\ngeneration quality of a compressed LLM can be significantly improved by adding\ncarefully designed hard prompts, though this isn't the case for all questions.\nBased on this observation, we propose a soft prompt learning method where we\nexpose the compressed model to the prompt learning process, aiming to enhance\nthe performance of prompts. Our experimental analysis suggests our soft prompt\nstrategy greatly improves the performance of the 8x compressed LLaMA-7B model\n(with a joint 4-bit quantization and 50% weight pruning compression), allowing\nthem to match their uncompressed counterparts on popular benchmarks. Also, we\ndemonstrate that these learned prompts can be transferred across various\ndatasets, tasks, and compression levels. Hence with this transferability, we\ncan stitch the soft prompt to a newly compressed model to improve the test-time\naccuracy in an ``in-situ'' way.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhaozhuo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zirui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Beidi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yuxin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaixiong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Anshumali Shrivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction. (arXiv:2305.12660v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.12660","description":"<p>The vital role of analogical reasoning in human cognition allows us to grasp\nnovel concepts by linking them with familiar ones through shared relational\nstructures. Despite the attention previous research has given to word\nanalogies, this work suggests that Large Language Models (LLMs) often overlook\nthe structures that underpin these analogies, raising questions about the\nefficacy of word analogies as a measure of analogical reasoning skills akin to\nhuman cognition. In response to this, our paper introduces a task of analogical\nstructure abduction, grounded in cognitive psychology, designed to abduce\nstructures that form an analogy between two systems. In support of this task,\nwe establish a benchmark called SCAR, containing 400 scientific analogies from\n13 distinct fields, tailored for evaluating analogical reasoning with structure\nabduction. The empirical evidence underlines the continued challenges faced by\nLLMs, including ChatGPT and GPT-4, in mastering this task, signifying the need\nfor future exploration to enhance their abilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Siyu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiangjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1\">Xuyang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Deqing Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Interpretable Style Embeddings via Prompting LLMs. (arXiv:2305.12696v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.12696","description":"<p>Style representation learning builds content-independent representations of\nauthor style in text. Stylometry, the analysis of style in text, is often\nperformed by expert forensic linguists and no large dataset of stylometric\nannotations exists for training. Current style representation learning uses\nneural methods to disentangle style from content to create style vectors,\nhowever, these approaches result in uninterpretable representations,\ncomplicating their usage in downstream applications like authorship attribution\nwhere auditing and explainability is critical. In this work, we use prompting\nto perform stylometry on a large number of texts to create a synthetic dataset\nand train human-interpretable style representations we call LISA embeddings. We\nrelease our synthetic stylometry dataset and our interpretable style models as\nresources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Ajay Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_D/0/1/0/all/0/1\">Delip Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothary_A/0/1/0/all/0/1\">Ansh Kothary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate. (arXiv:2305.13160v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13160","description":"<p>Large language models (LLMs) such as ChatGPT and GPT-4 have shown impressive\nperformance in complex reasoning tasks. However, it is difficult to know\nwhether the models are reasoning based on deep understandings of truth and\nlogic, or leveraging their memorized patterns in a relatively superficial way.\nIn this work, we explore testing LLMs' reasoning by engaging with them in a\ndebate-like conversation, where given a question, the LLM and the user need to\ndiscuss to make the correct decision starting from opposing arguments. Upon\nmitigating the Clever Hans effect, our task requires the LLM to not only\nachieve the correct answer on its own, but also be able to hold and defend its\nbelief instead of blindly believing or getting misled by the user's (invalid)\narguments and critiques, thus testing in greater depth whether the LLM grasps\nthe essence of the reasoning required to solve the problem. Across a range of\ncomplex reasoning benchmarks spanning math, commonsense, logic and BIG-Bench\ntasks, we find that despite their impressive performance as reported in\nexisting work on generating correct step-by-step solutions in the beginning,\nLLMs like ChatGPT cannot maintain their beliefs in truth for a significant\nportion of examples when challenged by oftentimes absurdly invalid arguments.\nOur work points to danger zones of model alignment, and also suggests more\ncareful treatments and interpretations of the recent findings that LLMs can\nimprove their responses based on feedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boshi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Shared Vocabulary: Increasing Representational Word Similarities across Languages for Multilingual Machine Translation. (arXiv:2305.14189v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14189","description":"<p>Using a vocabulary that is shared across languages is common practice in\nMultilingual Neural Machine Translation (MNMT). In addition to its simple\ndesign, shared tokens play an important role in positive knowledge transfer,\nassuming that shared tokens refer to similar meanings across languages.\nHowever, when word overlap is small, especially due to different writing\nsystems, transfer is inhibited. In this paper, we define word-level information\ntransfer pathways via word equivalence classes and rely on graph networks to\nfuse word embeddings across languages. Our experiments demonstrate the\nadvantages of our approach: 1) embeddings of words with similar meanings are\nbetter aligned across languages, 2) our method achieves consistent BLEU\nimprovements of up to 2.3 points for high- and low-resource MNMT, and 3) less\nthan 1.0\\% additional trainable parameters are required with a limited increase\nin computational costs, while inference time remains identical to the baseline.\nWe release the codebase to the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monz_C/0/1/0/all/0/1\">Christof Monz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skill-Based Few-Shot Selection for In-Context Learning. (arXiv:2305.14210v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14210","description":"<p>In-context learning is the paradigm that adapts large language models to\ndownstream tasks by providing a few examples. Few-shot selection -- selecting\nappropriate examples for each test instance separately -- is important for\nin-context learning. In this paper, we propose Skill-KNN, a skill-based\nfew-shot selection method for in-context learning. The key advantages of\nSkill-KNN include: (1) it addresses the problem that existing methods based on\npre-trained embeddings can be easily biased by surface natural language\nfeatures that are not important for the target task; (2) it does not require\ntraining or fine-tuning of any models, making it suitable for frequently\nexpanding or changing example banks. The key insight is to optimize the inputs\nfed into the embedding model, rather than tuning the model itself. Technically,\nSkill-KNN generates the skill-based descriptions for each test case and\ncandidate example by utilizing a pre-processing few-shot prompting, thus\neliminating unimportant surface features. Experimental results across five\ncross-domain semantic parsing datasets and six backbone models show that\nSkill-KNN significantly outperforms existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1\">Shengnan An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zeqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training. (arXiv:2305.14342v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.14342","description":"<p>Given the massive cost of language model pre-training, a non-trivial\nimprovement of the optimization algorithm would lead to a material reduction on\nthe time and cost of training. Adam and its variants have been state-of-the-art\nfor years, and more sophisticated second-order (Hessian-based) optimizers often\nincur too much per-step overhead. In this paper, we propose Sophia,\nSecond-order Clipped Stochastic Optimization, a simple scalable second-order\noptimizer that uses a light-weight estimate of the diagonal Hessian as the\npre-conditioner. The update is the moving average of the gradients divided by\nthe moving average of the estimated Hessian, followed by element-wise clipping.\nThe clipping controls the worst-case update size and tames the negative impact\nof non-convexity and rapid change of Hessian along the trajectory. Sophia only\nestimates the diagonal Hessian every handful of iterations, which has\nnegligible average per-step time and memory overhead. On language modeling with\nGPT models of sizes ranging from 125M to 1.5B, Sophia achieves a 2x speed-up\ncompared to Adam in the number of steps, total compute, and wall-clock time,\nachieving the same perplexity with 50% fewer steps, less total compute, and\nreduced wall-clock time. Theoretically, we show that Sophia, in a much\nsimplified setting, adapts to the heterogeneous curvatures in different\nparameter dimensions, and thus has a run-time bound that does not depend on the\ncondition number of the loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_D/0/1/0/all/0/1\">David Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Editing Common Sense in Transformers. (arXiv:2305.14956v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14956","description":"<p>Editing model parameters directly in Transformers makes updating black-box\nmodels possible without re-training (Meng et al., 2023). However, these editing\nmethods have only been evaluated on statements about encyclopedic knowledge\nwith a single correct answer. Commonsense knowledge with multiple correct\nanswers, e.g., an apple can be green or red but not transparent, has not been\nstudied but is as essential for enhancing transformers' reliability and\nusefulness. In this paper, we investigate whether commonsense judgments are\ncausally associated with localized, editable parameters in Transformers, and we\nprovide an affirmative answer. We find that directly applying the MEMIT editing\nalgorithm results in sub-par performance and improve it for the commonsense\ndomain by varying edit tokens and improving the layer selection strategy, i.e.,\n$MEMIT_{CSK}$. GPT-2 Large and XL models edited using $MEMIT_{CSK}$ outperform\nbest-fine-tuned baselines by 10.97% and 10.73% F1 scores on PEP3k and 20Q\ndatasets. In addition, we propose a novel evaluation dataset, PROBE SET, that\ncontains unaffected and affected neighborhoods, affected paraphrases, and\naffected reasoning challenges. $MEMIT_{CSK}$ performs well across the metrics\nwhile fine-tuning baselines show significant trade-offs between unaffected and\naffected metrics. These results suggest a compelling future direction for\nincorporating feedback about common sense into Transformers through direct\nmodel editing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anshita Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondal_D/0/1/0/all/0/1\">Debanjan Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheshadri_A/0/1/0/all/0/1\">Akshay Krishna Sheshadri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenlong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Lorraine Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiegreffe_S/0/1/0/all/0/1\">Sarah Wiegreffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_N/0/1/0/all/0/1\">Niket Tandon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLMDet: A Third Party Large Language Models Generated Text Detection Tool. (arXiv:2305.15004v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.15004","description":"<p>Generated texts from large language models (LLMs) are remarkably close to\nhigh-quality human-authored text, raising concerns about their potential misuse\nin spreading false information and academic misconduct. Consequently, there is\nan urgent need for a highly practical detection tool capable of accurately\nidentifying the source of a given text. However, existing detection tools\ntypically rely on access to LLMs and can only differentiate between\nmachine-generated and human-authored text, failing to meet the requirements of\nfine-grained tracing, intermediary judgment, and rapid detection. Therefore, we\npropose LLMDet, a model-specific, secure, efficient, and extendable detection\ntool, that can source text from specific LLMs, such as GPT-2, OPT, LLaMA, and\nothers. In LLMDet, we record the next-token probabilities of salient n-grams as\nfeatures to calculate proxy perplexity for each LLM. By jointly analyzing the\nproxy perplexities of LLMs, we can determine the source of the generated text.\nExperimental results show that LLMDet yields impressive detection performance\nwhile ensuring speed and security, achieving 98.54% precision and x3.5 faster\nfor recognizing human-authored text. Additionally, LLMDet can effortlessly\nextend its detection capabilities to a new open-source model. We will provide\nan open-source tool at https://github.com/TrustedLLM/LLMDet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kangxi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bactrian-X: Multilingual Replicable Instruction-Following Models with Low-Rank Adaptation. (arXiv:2305.15011v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.15011","description":"<p>Instruction tuning has shown great promise in improving the performance of\nlarge language models. However, research on multilingual instruction tuning has\nbeen limited due to the scarcity of high-quality instruction-response datasets\nacross different languages. To bridge this gap, we present Bactrian-X, a\ncomprehensive multilingual parallel dataset of 3.4 million instruction-response\npairs across 52 languages. Leveraging this dataset, we train a set of adapters\nusing low-rank adaptation (LoRA), which are lightweight components that\nseamlessly integrate with large language models. These adapters have a\nsubstantially lower parameter count than the base model, making them easily\nreplaceable and usable as plug-ins for different languages or language groups.\nExtensive experiments in various multilingual evaluation settings demonstrate\nthat models derived from LoRA-based training over Bactrian-X outperform both\nthe vanilla models and existing instruction-tuned models. The code and models\nare publicly available at https://github.com/mbzuai-nlp/bactrian-x\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haonan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koto_F/0/1/0/all/0/1\">Fajri Koto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MiniSUPERB: Lightweight Benchmark for Self-supervised Speech Models. (arXiv:2305.19011v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2305.19011","description":"<p>SUPERB was proposed to evaluate the generalizability of self-supervised\nlearning (SSL) speech models across various tasks. However, it incurs high\ncomputational costs due to the large datasets and diverse tasks. In this paper,\nwe introduce MiniSUPERB, a lightweight benchmark that efficiently evaluates SSL\nspeech models with comparable results to SUPERB but lower computational costs\nsignificantly. We carefully select representative tasks, sample datasets, and\nextract model representations offline. Our approach achieves a Spearman's rank\ncorrelation of 0.954 and 0.982 with SUPERB Paper and SUPERB Challenge,\nrespectively. Additionally, we reduce the computational cost by 97% in terms of\nMultiply-ACcumulate operations (MACs). Furthermore, we evaluate SSL speech\nmodels in few-shot scenarios and observe significant variations in their\nperformance. To our knowledge, this is the first study to examine both the\ncomputational cost of the model itself and the cost of evaluating it on a\nbenchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Hsiang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Huang-Yu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsu_W/0/1/0/all/0/1\">Winston Hsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preference-grounded Token-level Guidance for Language Model Fine-tuning. (arXiv:2306.00398v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.00398","description":"<p>Aligning language models (LMs) with preferences is an important problem in\nnatural language generation. A key challenge is that preferences are typically\nprovided at the *sequence level* while LM training and generation both occur at\nthe *token level*. There is, therefore, a *granularity mismatch* between the\npreference and the LM training losses, which may complicate the learning\nproblem. In this paper, we address this issue by developing an alternate\ntraining process, where we iterate between grounding the sequence-level\npreference into token-level training guidance, and improving the LM with the\nlearned guidance. For guidance learning, we design a framework that extends the\npairwise-preference learning in imitation learning to both variable-length LM\ngeneration and the utilization of the preference among multiple generations.\nFor LM training, based on the amount of supervised data, we present two\n*minimalist* learning objectives that utilize the learned guidance. In\nexperiments, our method performs competitively on two distinct representative\nLM tasks -- discrete-prompt generation and text summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shentao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shujian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Congying Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yihao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyuan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting an Unadaptable ASR System. (arXiv:2306.01208v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2306.01208","description":"<p>As speech recognition model sizes and training data requirements grow, it is\nincreasingly common for systems to only be available via APIs from online\nservice providers rather than having direct access to models themselves. In\nthis scenario it is challenging to adapt systems to a specific target domain.\nTo address this problem we consider the recently released OpenAI Whisper ASR as\nan example of a large-scale ASR system to assess adaptation methods. An error\ncorrection based approach is adopted, as this does not require access to the\nmodel, but can be trained from either 1-best or N-best outputs that are\nnormally available via the ASR API. LibriSpeech is used as the primary target\ndomain for adaptation. The generalization ability of the system in two distinct\ndimensions are then evaluated. First, whether the form of correction model is\nportable to other speech recognition domains, and secondly whether it can be\nused for ASR models having a different architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ma_R/0/1/0/all/0/1\">Rao Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_M/0/1/0/all/0/1\">Mengjie Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gales_M/0/1/0/all/0/1\">Mark J. F. Gales</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Knill_K/0/1/0/all/0/1\">Kate M. Knill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Utilizing Longitudinal Chest X-Rays and Reports to Pre-Fill Radiology Reports. (arXiv:2306.08749v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.08749","description":"<p>Despite the reduction in turn-around times in radiology reports with the use\nof speech recognition software, persistent communication errors can\nsignificantly impact the interpretation of the radiology report. Pre-filling a\nradiology report holds promise in mitigating reporting errors, and despite\nefforts in the literature to generate medical reports, there exists a lack of\napproaches that exploit the longitudinal nature of patient visit records in the\nMIMIC-CXR dataset. To address this gap, we propose to use longitudinal\nmulti-modal data, i.e., previous patient visit CXR, current visit CXR, and\nprevious visit report, to pre-fill the 'findings' section of a current patient\nvisit report. We first gathered the longitudinal visit information for 26,625\npatients from the MIMIC-CXR dataset and created a new dataset called\nLongitudinal-MIMIC. With this new dataset, a transformer-based model was\ntrained to capture the information from longitudinal patient visit records\ncontaining multi-modal data (CXR images + reports) via a cross-attention-based\nmulti-modal fusion module and a hierarchical memory-driven decoder. In contrast\nto previous work that only uses current visit data as input to train a model,\nour work exploits the longitudinal information available to pre-fill the\n'findings' section of radiology reports. Experiments show that our approach\noutperforms several recent approaches. Code will be published at\nhttps://github.com/CelestialShine/Longitudinal-Chest-X-Ray.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qingqing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathai_T/0/1/0/all/0/1\">Tejas Sudharshan Mathai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_P/0/1/0/all/0/1\">Pritam Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Summers_R/0/1/0/all/0/1\">Ronald M. Summers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DocumentNet: Bridging the Data Gap in Document Pre-Training. (arXiv:2306.08937v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.08937","description":"<p>Document understanding tasks, in particular, Visually-rich Document Entity\nRetrieval (VDER), have gained significant attention in recent years thanks to\ntheir broad applications in enterprise AI. However, publicly available data\nhave been scarce for these tasks due to strict privacy constraints and high\nannotation costs. To make things worse, the non-overlapping entity spaces from\ndifferent datasets hinder the knowledge transfer between document types. In\nthis paper, we propose a method to collect massive-scale and weakly labeled\ndata from the web to benefit the training of VDER models. The collected\ndataset, named DocumentNet, does not depend on specific document types or\nentity sets, making it universally applicable to all VDER tasks. The current\nDocumentNet consists of 30M documents spanning nearly 400 document types\norganized in a four-level ontology. Experiments on a set of broadly adopted\nVDER tasks show significant improvements when DocumentNet is incorporated into\nthe pre-training for both classic and few-shot learning settings. With the\nrecent emergence of large language models (LLMs), DocumentNet provides a large\ndata source to extend their multi-modal capabilities for VDER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lijun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1\">Jin Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaoyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiayi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauptmann_A/0/1/0/all/0/1\">Alexander G. Hauptmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hanjun Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting an ASR Foundation Model for Spoken Language Assessment. (arXiv:2307.09378v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.09378","description":"<p>A crucial part of an accurate and reliable spoken language assessment system\nis the underlying ASR model. Recently, large-scale pre-trained ASR foundation\nmodels such as Whisper have been made available. As the output of these models\nis designed to be human readable, punctuation is added, numbers are presented\nin Arabic numeric form and abbreviations are included. Additionally, these\nmodels have a tendency to skip disfluencies and hesitations in the output.\nThough useful for readability, these attributes are not helpful for assessing\nthe ability of a candidate and providing feedback. Here a precise transcription\nof what a candidate said is needed. In this paper, we give a detailed analysis\nof Whisper outputs and propose two solutions: fine-tuning and soft prompt\ntuning. Experiments are conducted on both public speech corpora and an English\nlearner dataset. Results show that we can effectively alter the decoding\nbehaviour of Whisper to generate the exact words spoken in the response.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_M/0/1/0/all/0/1\">Mengjie Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark J. F. Gales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knill_K/0/1/0/all/0/1\">Kate M. Knill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models. (arXiv:2307.10236v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2307.10236","description":"<p>The recent performance leap of Large Language Models (LLMs) opens up new\nopportunities across numerous industrial applications and domains. However,\nerroneous generations, such as false predictions, misinformation, and\nhallucination made by LLMs, have also raised severe concerns for the\ntrustworthiness of LLMs', especially in safety-, security- and\nreliability-sensitive scenarios, potentially hindering real-world adoptions.\nWhile uncertainty estimation has shown its potential for interpreting the\nprediction risks made by general machine learning (ML) models, little is known\nabout whether and to what extent it can help explore an LLM's capabilities and\ncounteract its undesired behavior. To bridge the gap, in this paper, we\ninitiate an exploratory study on the risk assessment of LLMs from the lens of\nuncertainty. In particular, we experiment with twelve uncertainty estimation\nmethods and four LLMs on four prominent natural language processing (NLP) tasks\nto investigate to what extent uncertainty estimation techniques could help\ncharacterize the prediction risks of LLMs. Our findings validate the\neffectiveness of uncertainty estimation for revealing LLMs'\nuncertain/non-factual predictions. In addition to general NLP tasks, we\nextensively conduct experiments with four LLMs for code generation on two\ndatasets. We find that uncertainty estimation can potentially uncover buggy\nprograms generated by LLMs. Insights from our study shed light on future design\nand development for reliable LLMs, facilitating further research toward\nenhancing the trustworthiness of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiayang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shengming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huaming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio Generation with Multiple Conditional Diffusion Model. (arXiv:2308.11940v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2308.11940","description":"<p>Text-based audio generation models have limitations as they cannot encompass\nall the information in audio, leading to restricted controllability when\nrelying solely on text. To address this issue, we propose a novel model that\nenhances the controllability of existing pre-trained text-to-audio models by\nincorporating additional conditions including content (timestamp) and style\n(pitch contour and energy contour) as supplements to the text. This approach\nachieves fine-grained control over the temporal order, pitch, and energy of\ngenerated audio. To preserve the diversity of generation, we employ a trainable\ncontrol condition encoder that is enhanced by a large language model and a\ntrainable Fusion-Net to encode and fuse the additional conditions while keeping\nthe weights of the pre-trained text-to-audio model frozen. Due to the lack of\nsuitable datasets and evaluation metrics, we consolidate existing datasets into\na new dataset comprising the audio and corresponding conditions and use a\nseries of evaluation metrics to evaluate the controllability performance.\nExperimental results demonstrate that our model successfully achieves\nfine-grained control to accomplish controllable audio generation. Audio samples\nand our dataset are publicly available at\nhttps://conditionaudiogen.github.io/conditionaudiogen/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhifang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jianguo Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1\">Rui Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1\">Long Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouchi_K/0/1/0/all/0/1\">Kazushige Ouchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiangdong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Streamline Automated Machine Learning for Clinical Studies. (arXiv:2308.14120v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2308.14120","description":"<p>A knowledge gap persists between machine learning (ML) developers (e.g., data\nscientists) and practitioners (e.g., clinicians), hampering the full\nutilization of ML for clinical data analysis. We investigated the potential of\nthe ChatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this\ngap and perform ML analyses efficiently. Real-world clinical datasets and study\ndetails from large trials across various medical specialties were presented to\nChatGPT ADA without specific guidance. ChatGPT ADA autonomously developed\nstate-of-the-art ML models based on the original study's training data to\npredict clinical outcomes such as cancer development, cancer progression,\ndisease complications, or biomarkers such as pathogenic gene sequences.\nFollowing the re-implementation and optimization of the published models, the\nhead-to-head comparison of the ChatGPT ADA-crafted ML models and their\nrespective manually crafted counterparts revealed no significant differences in\ntraditional performance metrics (P&gt;0.474). Strikingly, the ChatGPT ADA-crafted\nML models often outperformed their counterparts. In conclusion, ChatGPT ADA\noffers a promising avenue to democratize ML in medicine by simplifying complex\ndata analyses, yet should enhance, not replace, specialized training and\nresources, to promote broader applications in medical research and practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arasteh_S/0/1/0/all/0/1\">Soroosh Tayebi Arasteh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tianyu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotfinia_M/0/1/0/all/0/1\">Mahshad Lotfinia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhl_C/0/1/0/all/0/1\">Christiane Kuhl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kather_J/0/1/0/all/0/1\">Jakob Nikolas Kather</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truhn_D/0/1/0/all/0/1\">Daniel Truhn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nebelung_S/0/1/0/all/0/1\">Sven Nebelung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation and Analysis of Hallucination in Large Vision-Language Models. (arXiv:2308.15126v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2308.15126","description":"<p>Large Vision-Language Models (LVLMs) have recently achieved remarkable\nsuccess. However, LVLMs are still plagued by the hallucination problem, which\nlimits the practicality in many scenarios. Hallucination refers to the\ninformation of LVLMs' responses that does not exist in the visual input, which\nposes potential risks of substantial consequences. There has been limited work\nstudying hallucination evaluation in LVLMs. In this paper, we propose\nHallucination Evaluation based on Large Language Models (HaELM), an LLM-based\nhallucination evaluation framework. HaELM achieves an approximate 95%\nperformance comparable to ChatGPT and has additional advantages including low\ncost, reproducibility, privacy preservation and local deployment. Leveraging\nthe HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we\nanalyze the factors contributing to hallucination in LVLMs and offer helpful\nsuggestions to mitigate the hallucination problem. Our training data and human\nannotation hallucination data will be made public soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guohai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Pengcheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chenlin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinghao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jihua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1\">Jitao Sang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haoyu Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pruning Large Language Models via Accuracy Predictor. (arXiv:2309.09507v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2309.09507","description":"<p>Large language models(LLMs) containing tens of billions of parameters (or\neven more) have demonstrated impressive capabilities in various NLP tasks.\nHowever, substantial model size poses challenges to training, inference, and\ndeployment so that it is necessary to compress the model. At present, most\nmodel compression for LLMs requires manual design of pruning features, which\nhas problems such as complex optimization pipeline and difficulty in retaining\nthe capabilities of certain parts of the model.Therefore, we propose a novel\npruning approach: firstly, a training set of a certain number of\narchitecture-accuracy pairs is established, and then a non-neural model is\ntrained as an accuracy predictor. Using the accuracy predictor to further\noptimize the search space and search, the optimal model can be automatically\nselected. Experiments show that our proposed approach is effective and\nefficient. Compared with the baseline, the perplexity(PPL) on Wikitext2 and PTB\ndropped by 9.48% and 5,76% respectively, and the average accuracy of MMLU\nincreased by 6.28%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yupeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yibo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiucai Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training. (arXiv:2309.10400v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.10400","description":"<p>Large Language Models (LLMs) are trained with a pre-defined context length,\nrestricting their use in scenarios requiring long inputs. Previous efforts for\nadapting LLMs to a longer length usually requires fine-tuning with this target\nlength (Full-length fine-tuning), suffering intensive training cost. To\ndecouple train length from target length for efficient context window\nextension, we propose Positional Skip-wisE (PoSE) training that smartly\nsimulates long inputs using a fixed context window. This is achieved by first\ndividing the original context window into several chunks, then designing\ndistinct skipping bias terms to manipulate the position indices of each chunk.\nThese bias terms and the lengths of each chunk are altered for every training\nexample, allowing the model to adapt to all positions within target length.\nExperimental results show that PoSE greatly reduces memory and time overhead\ncompared with Full-length fine-tuning, with minimal impact on performance.\nLeveraging this advantage, we have successfully extended the LLaMA model to\n128k tokens using a 2k training context window. Furthermore, we empirically\nconfirm that PoSE is compatible with all RoPE-based LLMs and position\ninterpolation strategies. Notably, our method can potentially support infinite\nlength, limited only by memory usage in inference. With ongoing progress for\nefficient inference, we believe PoSE can further scale the context window\nbeyond 128k.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dawei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1\">Nan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yifan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sujian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SlimPajama-DC: Understanding Data Combinations for LLM Training. (arXiv:2309.10818v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.10818","description":"<p>This paper aims to understand the impacts of various data combinations (e.g.,\nweb text, wikipedia, github, books) on the training of large language models\nusing SlimPajama. SlimPajama is a rigorously deduplicated, multi-source\ndataset, which has been refined and further deduplicated to 627B tokens from\nthe extensive 1.2T tokens RedPajama dataset contributed by Together. We've\ntermed our research as SlimPajama-DC, an empirical analysis designed to uncover\nfundamental characteristics and best practices associated with employing\nSlimPajama in the training of large language models. During our research with\nSlimPajama, two pivotal observations emerged: (1) Global deduplication vs.\nlocal deduplication. We analyze and discuss how global (across different\nsources of datasets) and local (within the single source of dataset)\ndeduplications affect the performance of trained models. (2) Proportions of\nhigh-quality/highly-deduplicated multi-source datasets in the combination. To\nstudy this, we construct six configurations of SlimPajama dataset and train\nindividual ones using 1.3B Cerebras-GPT model with Alibi and SwiGLU. Our best\nconfiguration outperforms the 1.3B model trained on RedPajama using the same\nnumber of training tokens by a significant margin. All our 1.3B models are\ntrained on Cerebras 16$\\times$ CS-2 cluster with a total of 80 PFLOP/s in bf16\nmixed precision. We further extend our discoveries (such as increasing data\ndiversity is crucial after global deduplication) on a 7B model with large\nbatch-size training. Our models and the separate SlimPajama-DC datasets are\navailable at: https://huggingface.co/MBZUAI-LLM and\nhttps://huggingface.co/datasets/cerebras/SlimPajama-627B.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhiqiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_T/0/1/0/all/0/1\">Tianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Liqun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1\">Willie Neiswanger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1\">Bowen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hestness_J/0/1/0/all/0/1\">Joel Hestness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vassilieva_N/0/1/0/all/0/1\">Natalia Vassilieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soboleva_D/0/1/0/all/0/1\">Daria Soboleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Learned Representations and Influence Functions Can Tell Us About Adversarial Examples. (arXiv:2309.10916v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2309.10916","description":"<p>Adversarial examples, deliberately crafted using small perturbations to fool\ndeep neural networks, were first studied in image processing and more recently\nin NLP. While approaches to detecting adversarial examples in NLP have largely\nrelied on search over input perturbations, image processing has seen a range of\ntechniques that aim to characterise adversarial subspaces over the learned\nrepresentations.\n</p>\n<p>In this paper, we adapt two such approaches to NLP, one based on nearest\nneighbors and influence functions and one on Mahalanobis distances. The former\nin particular produces a state-of-the-art detector when compared against\nseveral strong baselines; moreover, the novel use of influence functions\nprovides insight into how the nature of adversarial example subspaces in NLP\nrelate to those in image processing, and also how they differ depending on the\nkind of NLP task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tonni_S/0/1/0/all/0/1\">Shakila Mahjabin Tonni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dras_M/0/1/0/all/0/1\">Mark Dras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-rank Adaptation of Large Language Model Rescoring for Parameter-Efficient Speech Recognition. (arXiv:2309.15223v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.15223","description":"<p>We propose a neural language modeling system based on low-rank adaptation\n(LoRA) for speech recognition output rescoring. Although pretrained language\nmodels (LMs) like BERT have shown superior performance in second-pass\nrescoring, the high computational cost of scaling up the pretraining stage and\nadapting the pretrained models to specific domains limit their practical use in\nrescoring. Here we present a method based on low-rank decomposition to train a\nrescoring BERT model and adapt it to new domains using only a fraction (0.08%)\nof the pretrained parameters. These inserted matrices are optimized through a\ndiscriminative training objective along with a correlation-based regularization\nloss. The proposed low-rank adaptation Rescore-BERT (LoRB) architecture is\nevaluated on LibriSpeech and internal datasets with decreased training times by\nfactors between 5.4 and 3.6.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolehmainen_J/0/1/0/all/0/1\">Jari Kolehmainen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shivakumar_P/0/1/0/all/0/1\">Prashanth G. Shivakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yile Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_S/0/1/0/all/0/1\">Sungho Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1\">Roger Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Q/0/1/0/all/0/1\">Qi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gourav_A/0/1/0/all/0/1\">Aditya Gourav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_I/0/1/0/all/0/1\">I-Fan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi-Chieh Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1\">Tuan Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhe_A/0/1/0/all/0/1\">Ankur Gandhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filimonov_D/0/1/0/all/0/1\">Denis Filimonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shalini Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1\">Andreas Stolcke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastow_A/0/1/0/all/0/1\">Ariya Rastow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulyko_I/0/1/0/all/0/1\">Ivan Bulyko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Speech Recognition Error Correction with Large Language Models and Task-Activating Prompting. (arXiv:2309.15649v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.15649","description":"<p>We explore the ability of large language models (LLMs) to act as speech\nrecognition post-processors that perform rescoring and error correction. Our\nfirst focus is on instruction prompting to let LLMs perform these task without\nfine-tuning, for which we evaluate different prompting schemes, both zero- and\nfew-shot in-context learning, and a novel task activation prompting method that\ncombines causal instructions and demonstration to increase its context windows.\nNext, we show that rescoring only by in-context learning with frozen LLMs\nachieves results that are competitive with rescoring by domain-tuned LMs, using\na pretrained first-pass recognition system and rescoring output on two\nout-of-domain tasks (ATIS and WSJ). By combining prompting techniques with\nfine-tuning we achieve error rates below the N-best oracle level, showcasing\nthe generalization power of the LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yile Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi-Chieh Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shalini Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulyko_I/0/1/0/all/0/1\">Ivan Bulyko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1\">Andreas Stolcke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond. (arXiv:2309.16583v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.16583","description":"<p>With the rapid advancement of large language models (LLMs), there is a\npressing need for a comprehensive evaluation suite to assess their capabilities\nand limitations. Existing LLM leaderboards often reference scores reported in\nother papers without consistent settings and prompts, which may inadvertently\nencourage cherry-picking favored settings and prompts for better results. In\nthis work, we introduce GPT-Fathom, an open-source and reproducible LLM\nevaluation suite built on top of OpenAI Evals. We systematically evaluate 10+\nleading LLMs as well as OpenAI's legacy models on 20+ curated benchmarks across\n7 capability categories, all under aligned settings. Our retrospective study on\nOpenAI's earlier models offers valuable insights into the evolutionary path\nfrom GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3\nprogressively improves to GPT-4, including technical details like whether\nadding code data improves LLM's reasoning capability, which aspects of LLM\ncapability can be improved by SFT and RLHF, how much is the alignment tax, etc.\nOur analysis sheds light on many of these questions, aiming to improve the\ntransparency of advanced LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_C/0/1/0/all/0/1\">Chenguang Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Pengyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment. (arXiv:2310.00212v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2310.00212","description":"<p>Large Language Models (LLMs) can acquire extensive world knowledge through\npre-training on large corpora. However, due to exposure to low-quality data,\nLLMs may exhibit harmful behavior without aligning with human values. The\ndominant approach for steering LLMs towards beneficial behavior involves\nReinforcement Learning with Human Feedback (RLHF), with Proximal Policy\nOptimization (PPO) serving as the default RL optimizer. Despite its\neffectiveness, PPO has limitations when optimizing rewards trained from\ncomparison-based loss. Primarily, PPO is not invariant to equivalent reward\nfunctions containing identical preference information due to the need to\ncalibrate the reward scale. Additionally, PPO's necessity for token-wise\nupdates introduces complexity in both function approximation and algorithm\ndesign compared to trajectory-wise optimization. This paper proposes a new\nframework, reinforcement learning with relative feedback, and a novel\ntrajectory-wise policy gradient algorithm, Pairwise Proximal Policy\nOptimization (P3O) that operates directly on comparative rewards. We show\ntheoretically that P3O is invariant to equivalent rewards and avoids the\ncomplexity of PPO. Empirical evaluations demonstrate that P3O outperforms PPO\nin the KL-Reward trade-off and can align with human preferences as well as or\nbetter than prior methods. In summary, this work introduces a simpler yet\neffective approach for aligning LLMs to human preferences through relative\nfeedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Banghua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruoyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zhaojin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramchandran_K/0/1/0/all/0/1\">Kannan Ramchandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jiantao Jiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding In-Context Learning from Repetitions. (arXiv:2310.00297v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.00297","description":"<p>This paper explores the elusive mechanism underpinning in-context learning in\nLarge Language Models (LLMs). Our work provides a novel perspective by\nexamining in-context learning via the lens of surface repetitions. We\nquantitatively investigate the role of surface features in text generation, and\nempirically establish the existence of \\emph{token co-occurrence\nreinforcement}, a principle that strengthens the relationship between two\ntokens based on their contextual co-occurrences. By investigating the dual\nimpacts of these features, our research illuminates the internal workings of\nin-context learning and expounds on the reasons for its failures. This paper\nprovides an essential contribution to the understanding of in-context learning\nand its potential limitations, providing a fresh perspective on this exciting\ncapability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jianhao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chiyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yafu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language Models. (arXiv:2310.00322v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.00322","description":"<p>Deployable Large Language Models (LLMs) must conform to the criterion of\nhelpfulness and harmlessness, thereby achieving consistency between LLMs\noutputs and human values. Red-teaming techniques constitute a critical way\ntowards this criterion. Existing work rely solely on manual red team designs\nand heuristic adversarial prompts for vulnerability detection and optimization.\nThese approaches lack rigorous mathematical formulation, thus limiting the\nexploration of diverse attack strategy within quantifiable measure and\noptimization of LLMs under convergence guarantees. In this paper, we present\nRed-teaming Game (RTG), a general game-theoretic framework without manual\nannotation. RTG is designed for analyzing the multi-turn attack and defense\ninteractions between Red-team language Models (RLMs) and Blue-team Language\nModel (BLM). Within the RTG, we propose Gamified Red-teaming Solver (GRTS) with\ndiversity measure of the semantic space. GRTS is an automated red teaming\ntechnique to solve RTG towards Nash equilibrium through meta-game analysis,\nwhich corresponds to the theoretically guaranteed optimization direction of\nboth RLMs and BLM. Empirical results in multi-turn attacks with RLMs show that\nGRTS autonomously discovered diverse attack strategies and effectively improved\nsecurity of LLMs, outperforming existing heuristic red-team designs. Overall,\nRTG has established a foundational framework for red teaming tasks and\nconstructed a new scalable oversight technique for alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chengdong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Minquan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ci_H/0/1/0/all/0/1\">Hai Ci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xuehai Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaodong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEEC: A Legal Element Extraction Dataset with an Extensive Domain-Specific Label System. (arXiv:2310.01271v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.01271","description":"<p>As a pivotal task in natural language processing, element extraction has\ngained significance in the legal domain. Extracting legal elements from\njudicial documents helps enhance interpretative and analytical capacities of\nlegal cases, and thereby facilitating a wide array of downstream applications\nin various domains of law. Yet existing element extraction datasets are limited\nby their restricted access to legal knowledge and insufficient coverage of\nlabels. To address this shortfall, we introduce a more comprehensive,\nlarge-scale criminal element extraction dataset, comprising 15,831 judicial\ndocuments and 159 labels. This dataset was constructed through two main steps:\nfirst, designing the label system by our team of legal experts based on prior\nlegal research which identified critical factors driving and processes\ngenerating sentencing outcomes in criminal cases; second, employing the legal\nknowledge to annotate judicial documents according to the label system and\nannotation guideline. The Legal Element ExtraCtion dataset (LEEC) represents\nthe most extensive and domain-specific legal element extraction dataset for the\nChinese legal system. Leveraging the annotated data, we employed various SOTA\nmodels that validates the applicability of LEEC for Document Event Extraction\n(DEE) task. The LEEC dataset is available on https://github.com/THUlawtech/LEEC .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zongyue_X/0/1/0/all/0/1\">Xue Zongyue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huanghai_L/0/1/0/all/0/1\">Liu Huanghai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yiran_H/0/1/0/all/0/1\">Hu Yiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kangle_K/0/1/0/all/0/1\">Kong Kangle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chenlu_W/0/1/0/all/0/1\">Wang Chenlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_L/0/1/0/all/0/1\">Liu Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weixing_S/0/1/0/all/0/1\">Shen Weixing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation Engineering: A Top-Down Approach to AI Transparency. (arXiv:2310.01405v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2310.01405","description":"<p>In this paper, we identify and characterize the emerging area of\nrepresentation engineering (RepE), an approach to enhancing the transparency of\nAI systems that draws on insights from cognitive neuroscience. RepE places\npopulation-level representations, rather than neurons or circuits, at the\ncenter of analysis, equipping us with novel methods for monitoring and\nmanipulating high-level cognitive phenomena in deep neural networks (DNNs). We\nprovide baselines and an initial analysis of RepE techniques, showing that they\noffer simple yet effective solutions for improving our understanding and\ncontrol of large language models. We showcase how these methods can provide\ntraction on a wide range of safety-relevant problems, including honesty,\nharmlessness, power-seeking, and more, demonstrating the promise of top-down\ntransparency research. We hope that this work catalyzes further exploration of\nRepE and fosters advancements in the transparency and safety of AI systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_A/0/1/0/all/0/1\">Andy Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_L/0/1/0/all/0/1\">Long Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sarah Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_J/0/1/0/all/0/1\">James Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Phillip Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1\">Richard Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_A/0/1/0/all/0/1\">Alexander Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xuwang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazeika_M/0/1/0/all/0/1\">Mantas Mazeika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dombrowski_A/0/1/0/all/0/1\">Ann-Kathrin Dombrowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1\">Shashwat Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Nathaniel Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_M/0/1/0/all/0/1\">Michael J. Byun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallen_A/0/1/0/all/0/1\">Alex Mallen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1\">Steven Basart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1\">Sanmi Koyejo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fredrikson_M/0/1/0/all/0/1\">Matt Fredrikson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1\">J. Zico Kolter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting LLM Agents Through Communication. (arXiv:2310.01444v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.01444","description":"<p>Recent advancements in large language models (LLMs) have shown potential for\nhuman-like agents. To help these agents adapt to new tasks without extensive\nhuman supervision, we propose the Learning through Communication (LTC)\nparadigm, a novel training approach enabling LLM agents to improve continuously\nthrough interactions with their environments and other agents. Recent\nadvancements in large language models (LLMs) have shown potential for\nhuman-like agents. To help these agents adapt to new tasks without extensive\nhuman supervision, we propose the Learning through Communication (LTC)\nparadigm, a novel training approach enabling LLM agents to improve continuously\nthrough interactions with their environments and other agents. Through\niterative exploration and PPO training, LTC empowers the agent to assimilate\nshort-term experiences into long-term memory. To optimize agent interactions\nfor task-specific learning, we introduce three structured communication\npatterns: Monologue, Dialogue, and Analogue-tailored for common tasks such as\ndecision-making, knowledge-intensive reasoning, and numerical reasoning. We\nevaluated LTC on three datasets: ALFWorld (decision-making), HotpotQA\n(knowledge-intensive reasoning), and GSM8k (numerical reasoning). On ALFWorld,\nit exceeds the instruction tuning baseline by 12% in success rate. On HotpotQA,\nLTC surpasses the instruction-tuned LLaMA-7B agent by 5.1% in EM score, and it\noutperforms the instruction-tuned 9x larger PaLM-62B agent by 0.6%. On GSM8k,\nLTC outperforms the CoT-Tuning baseline by 3.6% in accuracy. The results\nshowcase the versatility and efficiency of the LTC approach across diverse\ndomains. We will open-source our code to promote further development of the\ncommunity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yadong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santacroce_M/0/1/0/all/0/1\">Michael Santacroce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Personalized Story Evaluation. (arXiv:2310.03304v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.03304","description":"<p>While large language models (LLMs) have shown impressive results for more\nobjective tasks such as QA and retrieval, it remains nontrivial to evaluate\ntheir performance on open-ended text generation for reasons including (1) data\ncontamination; (2) multi-dimensional evaluation criteria; and (3)\nsubjectiveness stemming from reviewers' personal preferences. To address such\nissues, we propose to model personalization in an uncontaminated open-ended\ngeneration assessment. We create two new datasets Per-MPST and Per-DOC for\npersonalized story evaluation, by re-purposing existing datasets with proper\nanonymization and new personalized labels. We further develop a personalized\nstory evaluation model PERSE to infer reviewer preferences and provide a\npersonalized evaluation. Specifically, given a few exemplary reviews from a\nparticular reviewer, PERSE predicts either a detailed review or fine-grained\ncomparison in several aspects (such as interestingness and surprise) for that\nreviewer on a new text input. Experimental results show that PERSE outperforms\nGPT-4 by 15.8% on Kendall correlation of story ratings, and by 13.7% on\npairwise preference prediction accuracy. Both datasets and code will be\nreleased.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Danqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hanlin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaomeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1\">Andrew Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations. (arXiv:2310.03951v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.03951","description":"<p>Large language models (LLMs) can generate fluent natural language texts when\ngiven relevant documents as background context. This ability has attracted\nconsiderable interest in developing industry applications of LLMs. However,\nLLMs are prone to generate hallucinations that are not supported by the\nprovided sources. In this paper, we propose a hierarchical framework to detect\nand mitigate such ungrounded hallucination. Our framework uses Chain of Natural\nLanguage Inference (CoNLI) for hallucination detection and hallucination\nreduction via post-editing. Our approach achieves state-of-the-art performance\non hallucination detection and enhances text quality through rewrite, using\nLLMs without any fine-tuning or domain-specific prompt engineering. We show\nthat this simple plug-and-play framework can serve as an effective choice for\nhallucination detection and reduction, achieving competitive performance across\nvarious contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_D/0/1/0/all/0/1\">Deren Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Mengya Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_V/0/1/0/all/0/1\">Vincent Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ching_E/0/1/0/all/0/1\">Emily Ching</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamal_E/0/1/0/all/0/1\">Eslam Kamal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks. (arXiv:2310.04270v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.04270","description":"<p>Recently, Large Language Models (LLM) have demonstrated impressive capability\nto solve a wide range of tasks. However, despite their success across various\ntasks, no prior work has investigated their capability in the biomedical domain\nyet. To this end, this paper aims to evaluate the performance of LLMs on\nbenchmark biomedical tasks. For this purpose, we conduct a comprehensive\nevaluation of 4 popular LLMs in 6 diverse biomedical tasks across 26 datasets.\nTo the best of our knowledge, this is the first work that conducts an extensive\nevaluation and comparison of various LLMs in the biomedical domain.\nInterestingly, we find based on our evaluation that in biomedical datasets that\nhave smaller training sets, zero-shot LLMs even outperform the current\nstate-of-the-art fine-tuned biomedical models. This suggests that pretraining\non large text corpora makes LLMs quite specialized even in the biomedical\ndomain. We also find that not a single LLM can outperform other LLMs in all\ntasks, with the performance of different LLMs may vary depending on the task.\nWhile their performance is still quite poor in comparison to the biomedical\nmodels that were fine-tuned on large training sets, our findings demonstrate\nthat LLMs have the potential to be a valuable tool for various biomedical tasks\nthat lack large annotated data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jahan_I/0/1/0/all/0/1\">Israt Jahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskar_M/0/1/0/all/0/1\">Md Tahmid Rahman Laskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1\">Chun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jimmy Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What's the Magic Word? A Control Theory of LLM Prompting. (arXiv:2310.04444v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.04444","description":"<p>Prompt engineering is effective and important in the deployment of LLMs but\nis poorly understood mathematically. Here, we formalize prompt engineering as\nan optimal control problem on LLMs -- where the prompt is considered a control\nvariable for modulating the output distribution of the LLM. Within this\nframework, we ask a simple question: given a sequence of tokens, does there\nalways exist a prompt we can prepend that will steer the LLM toward accurately\npredicting the final token? We call such an optimal prompt the magic word since\nprepending the prompt causes the LLM to output the correct answer. If magic\nwords exist, can we find them? If so, what are their properties? We offer\nanalytic analysis on the controllability of the self-attention head where we\nprove a bound on controllability as a function of the singular values of its\nweight matrices. We take inspiration from control theory to propose a metric\ncalled $k-\\epsilon$ controllability to characterize LLM steerability. We\ncompute the $k-\\epsilon$ controllability of a panel of large language models,\nincluding Falcon-7b, Llama-7b, and Falcon-40b on 5000 WikiText causal language\nmodeling tasks. Remarkably, we find that magic words of 10 tokens or less exist\nfor over 97% of WikiText instances surveyed for each model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhargava_A/0/1/0/all/0/1\">Aman Bhargava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witkowski_C/0/1/0/all/0/1\">Cameron Witkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Manav Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomson_M/0/1/0/all/0/1\">Matt Thomson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto-survey Challenge. (arXiv:2310.04480v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.04480","description":"<p>We present a novel platform for evaluating the capability of Large Language\nModels (LLMs) to autonomously compose and critique survey papers spanning a\nvast array of disciplines including sciences, humanities, education, and law.\nWithin this framework, AI systems undertake a simulated peer-review mechanism\nakin to traditional scholarly journals, with human organizers serving in an\neditorial oversight capacity. Within this framework, we organized a competition\nfor the AutoML conference 2023. Entrants are tasked with presenting stand-alone\nmodels adept at authoring articles from designated prompts and subsequently\nappraising them. Assessment criteria include clarity, reference\nappropriateness, accountability, and the substantive value of the content. This\npaper presents the design of the competition, including the implementation\nbaseline submissions and methods of evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khuong_T/0/1/0/all/0/1\">Thanh Gia Hieu Khuong</a> (TAU, LISN), <a href=\"http://arxiv.org/find/cs/1/au:+Rachmat_B/0/1/0/all/0/1\">Benedictus Kent Rachmat</a> (TAU, LISN)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ada-Instruct: Adapting Instruction Generators for Complex Reasoning. (arXiv:2310.04484v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.04484","description":"<p>Generating diverse and sophisticated instructions for downstream tasks by\nLarge Language Models (LLMs) is pivotal for advancing the effect. Current\napproaches leverage closed-source LLMs, employing in-context prompting for\ninstruction generation. However, in this paper, we found that in-context\nprompting cannot generate complex instructions with length $\\ge 100$ for tasks\nlike code completion.\n</p>\n<p>To solve this problem, we introduce Ada-Instruct, an adaptive instruction\ngenerator developed by fine-tuning open-source LLMs. Our pivotal finding\nillustrates that fine-tuning open-source LLMs with a mere ten samples generates\nlong instructions that maintain distributional consistency for complex\nreasoning tasks. We empirically validated Ada-Instruct's efficacy across\ndifferent applications, including code completion, mathematical reasoning, and\ncommonsense reasoning. The results underscore Ada-Instruct's superiority,\nevidencing its improvements over its base models, current self-instruct\nmethods, and other state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1\">Wanyun Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qianle Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DORIS-MAE: Scientific Document Retrieval using Multi-level Aspect-based Queries. (arXiv:2310.04678v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2310.04678","description":"<p>In scientific research, the ability to effectively retrieve relevant\ndocuments based on complex, multifaceted queries is critical. Existing\nevaluation datasets for this task are limited, primarily due to the high cost\nand effort required to annotate resources that effectively represent complex\nqueries. To address this, we propose a novel task, Scientific DOcument\nRetrieval using Multi-level Aspect-based quEries (DORIS-MAE), which is designed\nto handle the complex nature of user queries in scientific research. We\ndeveloped a benchmark dataset within the field of computer science, consisting\nof 100 human-authored complex query cases. For each complex query, we assembled\na collection of 100 relevant documents and produced annotated relevance scores\nfor ranking them. Recognizing the significant labor of expert annotation, we\nalso introduce Anno-GPT, a scalable framework for validating the performance of\nLarge Language Models (LLMs) on expert-level dataset annotation tasks. LLM\nannotation of the DORIS-MAE dataset resulted in a 500x reduction in cost,\nwithout compromising quality. Furthermore, due to the multi-tiered structure of\nthese complex queries, the DORIS-MAE dataset can be extended to over 4,000\nsub-query test cases without requiring additional annotation. We evaluated 17\nrecent retrieval methods on DORIS-MAE, observing notable performance drops\ncompared to traditional datasets. This highlights the need for better\napproaches to handle complex, multifaceted queries in scientific research. Our\ndataset and codebase are available at\nhttps://github.com/Real-Doris-Mae/Doris-Mae-Dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naidu_P/0/1/0/all/0/1\">Prudhviraj Naidu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergen_L/0/1/0/all/0/1\">Leon Bergen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paturi_R/0/1/0/all/0/1\">Ramamohan Paturi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Large Language Models as Zero-shot Relation Extractors. (arXiv:2310.05028v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2310.05028","description":"<p>Relation extraction (RE) consistently involves a certain degree of labeled or\nunlabeled data even if under zero-shot setting. Recent studies have shown that\nlarge language models (LLMs) transfer well to new tasks out-of-the-box simply\ngiven a natural language prompt, which provides the possibility of extracting\nrelations from text without any data and parameter tuning. This work focuses on\nthe study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors.\nOn the one hand, we analyze the drawbacks of existing RE prompts and attempt to\nincorporate recent prompt techniques such as chain-of-thought (CoT) to improve\nzero-shot RE. We propose the summarize-and-ask (\\textsc{SumAsk}) prompting, a\nsimple prompt recursively using LLMs to transform RE inputs to the effective\nquestion answering (QA) format. On the other hand, we conduct comprehensive\nexperiments on various benchmarks and settings to investigate the capabilities\nof LLMs on zero-shot RE. Specifically, we have the following findings: (i)\n\\textsc{SumAsk} consistently and significantly improves LLMs performance on\ndifferent model sizes, benchmarks and settings; (ii) Zero-shot prompting with\nChatGPT achieves competitive or superior results compared with zero-shot and\nfully supervised methods; (iii) LLMs deliver promising performance in\nextracting overlapping relations; (iv) The performance varies greatly regarding\ndifferent relations. Different from small language models, LLMs are effective\nin handling challenge none-of-the-above (NoTA) relation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guozheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_W/0/1/0/all/0/1\">Wenjun Ke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Convinced Prompting: Few-Shot Question Answering with Repeated Introspection. (arXiv:2310.05035v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.05035","description":"<p>While large language models (LLMs) such as ChatGPT and PaLM have demonstrated\nremarkable performance in various language understanding and generation tasks,\ntheir capabilities in complex reasoning and intricate knowledge utilization\nstill fall short of human-level proficiency. Recent studies have established\nthe effectiveness of prompts in steering LLMs towards generating desired\noutputs. Building on these insights, we introduce a novel framework that\nharnesses the potential of large-scale pre-trained language models, to\niteratively enhance performance of the LLMs. Our framework incorporates three\ncomponents: \\textit{Normal CoT}, a \\textit{Convincer}, and an\n\\textit{Answerer}. It processes the output of a typical few-shot\nchain-of-thought prompt, assesses the correctness of the response, scrutinizes\nthe answer, refines the reasoning, and ultimately produces a new solution.\nExperimental results on the 7 datasets of miscellaneous problems validate the\nefficacy of the Self-Convince framework, achieving substantial improvements\ncompared to the baselines. This study contributes to the burgeoning body of\nresearch focused on integrating pre-trained language models with tailored\nprompts and iterative refinement processes to augment their performance in\ncomplex tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haodi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Min Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Jason Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_R/0/1/0/all/0/1\">Rui Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kaishun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Text to Tactic: Evaluating LLMs Playing the Game of Avalon. (arXiv:2310.05036v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2310.05036","description":"<p>In this paper, we explore the potential of Large Language Models (LLMs)\nAgents in playing the strategic social deduction game, Resistance Avalon.\nPlayers in Avalon are challenged not only to make informed decisions based on\ndynamically evolving game phases, but also to engage in discussions where they\nmust deceive, deduce, and negotiate with other players. These characteristics\nmake Avalon a compelling test-bed to study the decision-making and\nlanguage-processing capabilities of LLM Agents. To facilitate research in this\nline, we introduce AvalonBench - a comprehensive game environment tailored for\nevaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game\nenvironment for Avalon, (2) rule-based bots as baseline opponents, and (3)\nReAct-style LLM agents with tailored prompts for each role. Notably, our\nevaluations based on AvalonBench highlight a clear capability gap. For\ninstance, models like ChatGPT playing good-role got a win rate of 22.2% against\nrule-based bots playing evil, while good-role bot achieves 38.2% win rate in\nthe same setting. We envision AvalonBench could be a good test-bed for\ndeveloping more advanced LLMs (with self-playing) and agent frameworks that can\neffectively model the layered complexities of such game environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Light_J/0/1/0/all/0/1\">Jonathan Light</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Min Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Ziniu Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models. (arXiv:2310.05074v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.05074","description":"<p>Chain-of-Thought (CoT) prompting has proven to be effective in enhancing the\nreasoning capabilities of Large Language Models (LLMs) with at least 100\nbillion parameters. However, it is ineffective or even detrimental when applied\nto reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion\nparameters. To address this limitation, we introduce Dialogue-guided\nChain-of-Thought (DialCoT) which employs a dialogue format to generate\nintermediate reasoning steps, guiding the model toward the final answer.\nAdditionally, we optimize the model's reasoning path selection using the\nProximal Policy Optimization (PPO) algorithm, further enhancing its reasoning\ncapabilities. Our method offers several advantages compared to previous\napproaches. Firstly, we transform the process of solving complex reasoning\nquestions by breaking them down into a series of simpler sub-questions,\nsignificantly reducing the task difficulty and making it more suitable for\nSLMs. Secondly, we optimize the model's reasoning path selection through the\nPPO algorithm. We conduct comprehensive experiments on four arithmetic\nreasoning datasets, demonstrating that our method achieves significant\nperformance improvements compared to state-of-the-art competitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chengcheng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xiaowei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Che Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Y/0/1/0/all/0/1\">Yixin Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Ming Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Baoyuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factuality Challenges in the Era of Large Language Models. (arXiv:2310.05189v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.05189","description":"<p>The emergence of tools based on Large Language Models (LLMs), such as\nOpenAI's ChatGPT, Microsoft's Bing Chat, and Google's Bard, has garnered\nimmense public attention. These incredibly useful, natural-sounding tools mark\nsignificant advances in natural language generation, yet they exhibit a\npropensity to generate false, erroneous, or misleading content -- commonly\nreferred to as \"hallucinations.\" Moreover, LLMs can be exploited for malicious\napplications, such as generating false but credible-sounding content and\nprofiles at scale. This poses a significant challenge to society in terms of\nthe potential deception of users and the increasing dissemination of inaccurate\ninformation. In light of these risks, we explore the kinds of technological\ninnovations, regulatory reforms, and AI literacy initiatives needed from\nfact-checkers, news organizations, and the broader research and policy\ncommunities. By identifying the risks, the imminent threats, and some viable\nsolutions, we seek to shed light on navigating various aspects of veracity in\nthe era of generative AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_M/0/1/0/all/0/1\">Meeyoung Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciampaglia_G/0/1/0/all/0/1\">Giovanni Luca Ciampaglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corney_D/0/1/0/all/0/1\">David Corney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DiResta_R/0/1/0/all/0/1\">Renee DiResta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrara_E/0/1/0/all/0/1\">Emilio Ferrara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1\">Scott Hale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halevy_A/0/1/0/all/0/1\">Alon Halevy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menczer_F/0/1/0/all/0/1\">Filippo Menczer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miguez_R/0/1/0/all/0/1\">Ruben Miguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheufele_D/0/1/0/all/0/1\">Dietram Scheufele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shivam Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zagni_G/0/1/0/all/0/1\">Giovanni Zagni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatRadio-Valuer: A Chat Large Language Model for Generalizable Radiology Report Generation Based on Multi-institution and Multi-system Data. (arXiv:2310.05242v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.05242","description":"<p>Radiology report generation, as a key step in medical image analysis, is\ncritical to the quantitative analysis of clinically informed decision-making\nlevels. However, complex and diverse radiology reports with cross-source\nheterogeneity pose a huge generalizability challenge to the current methods\nunder massive data volume, mainly because the style and normativity of\nradiology reports are obviously distinctive among institutions, body regions\ninspected and radiologists. Recently, the advent of large language models (LLM)\noffers great potential for recognizing signs of health conditions. To resolve\nthe above problem, we collaborate with the Second Xiangya Hospital in China and\npropose ChatRadio-Valuer based on the LLM, a tailored model for automatic\nradiology report generation that learns generalizable representations and\nprovides a basis pattern for model adaptation in sophisticated analysts' cases.\nSpecifically, ChatRadio-Valuer is trained based on the radiology reports from a\nsingle institution by means of supervised fine-tuning, and then adapted to\ndisease diagnosis tasks for human multi-system evaluation (i.e., chest,\nabdomen, muscle-skeleton, head, and maxillofacial $\\&amp;$ neck) from six different\ninstitutions in clinical-level events. The clinical dataset utilized in this\nstudy encompasses a remarkable total of \\textbf{332,673} observations. From the\ncomprehensive results on engineering indicators, clinical efficacy and\ndeployment cost metrics, it can be shown that ChatRadio-Valuer consistently\noutperforms state-of-the-art models, especially ChatGPT (GPT-3.5-Turbo) and\nGPT-4 et al., in terms of the diseases diagnosis from radiology reports.\nChatRadio-Valuer provides an effective avenue to boost model generalization\nperformance and alleviate the annotation workload of experts to enable the\npromotion of clinical AI applications in radiology reports.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_T/0/1/0/all/0/1\">Tianyang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yutong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yi Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_P/0/1/0/all/0/1\">Peixin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zuowei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kui_X/0/1/0/all/0/1\">Xiaoyan Kui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1\">Youlan Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Li Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yaonai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Longtao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Huan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_N/0/1/0/all/0/1\">Ning Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jiaqi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Ying Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhixue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Ming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Haixing Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xiaoyan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xintao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shijie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dajiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Lei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tuo Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Long-form Text Generation in Mental Health with Task-adaptive Tokenization. (arXiv:2310.05317v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.05317","description":"<p>We propose task-adaptive tokenization as a way to adapt the generation\npipeline to the specifics of a downstream task and enhance long-form generation\nin mental health. Inspired by insights from cognitive science, our\ntask-adaptive tokenizer samples variable segmentations from multiple outcomes,\nwith sampling probabilities optimized based on task-specific data. We introduce\na strategy for building a specialized vocabulary and introduce a vocabulary\nmerging protocol that allows for the integration of task-specific tokens into\nthe pre-trained model's tokenization step. Through extensive experiments on\npsychological question-answering tasks in both Chinese and English, we find\nthat our task-adaptive tokenization approach brings a significant improvement\nin generation performance while using up to 60% fewer tokens. Preliminary\nexperiments point to promising results when using our tokenization approach\nwith very large language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Siyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_N/0/1/0/all/0/1\">Naihao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabour_S/0/1/0/all/0/1\">Sahand Sabour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yilin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Multi-modal Entity Alignment via Iteratively Fusing Modality Similarity Paths. (arXiv:2310.05364v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.05364","description":"<p>The objective of Entity Alignment (EA) is to identify equivalent entity pairs\nfrom multiple Knowledge Graphs (KGs) and create a more comprehensive and\nunified KG. The majority of EA methods have primarily focused on the structural\nmodality of KGs, lacking exploration of multi-modal information. A few\nmulti-modal EA methods have made good attempts in this field. Still, they have\ntwo shortcomings: (1) inconsistent and inefficient modality modeling that\ndesigns complex and distinct models for each modality; (2) ineffective modality\nfusion due to the heterogeneous nature of modalities in EA. To tackle these\nchallenges, we propose PathFusion, consisting of two main components: (1) MSP,\na unified modeling approach that simplifies the alignment process by\nconstructing paths connecting entities and modality nodes to represent multiple\nmodalities; (2) IRF, an iterative fusion method that effectively combines\ninformation from different modalities using the path as an information carrier.\nExperimental results on real-world datasets demonstrate the superiority of\nPathFusion over state-of-the-art methods, with 22.4%-28.9% absolute improvement\non Hits@1, and 0.194-0.245 absolute improvement on MRR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bolin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xin Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Lingbing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NEFTune: Noisy Embeddings Improve Instruction Finetuning. (arXiv:2310.05914v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.05914","description":"<p>We show that language model finetuning can be improved, sometimes\ndramatically, with a simple augmentation. NEFTune adds noise to the embedding\nvectors during training. Standard finetuning of LLaMA-2-7B using Alpaca\nachieves 29.79% on AlpacaEval, which rises to 64.69% using noisy embeddings.\nNEFTune also improves over strong baselines on modern instruction datasets.\nModels trained with Evol-Instruct see a 10% improvement, with ShareGPT an 8%\nimprovement, and with OpenPlatypus an 8% improvement. Even powerful models\nfurther refined with RLHF such as LLaMA-2-Chat benefit from additional training\nwith NEFTune.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1\">Neel Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_P/0/1/0/all/0/1\">Ping-yeh Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yuxin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchenbauer_J/0/1/0/all/0/1\">John Kirchenbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_H/0/1/0/all/0/1\">Hong-Min Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somepalli_G/0/1/0/all/0/1\">Gowthami Somepalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartoldson_B/0/1/0/all/0/1\">Brian R. Bartoldson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwarzschild_A/0/1/0/all/0/1\">Avi Schwarzschild</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1\">Aniruddha Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1\">Jonas Geiping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SELF: Language-Driven Self-Evolution for Large Language Model. (arXiv:2310.00533v2 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2310.00533","description":"<p>Large Language Models (LLMs) have showcased remarkable versatility across\ndiverse domains. However, the pathway toward autonomous model development, a\ncornerstone for achieving human-level learning and advancing autonomous AI,\nremains largely uncharted. We introduce an innovative approach, termed \"SELF\"\n(Self-Evolution with Language Feedback). This methodology empowers LLMs to\nundergo continual self-evolution. Furthermore, SELF employs language-based\nfeedback as a versatile and comprehensive evaluative tool, pinpointing areas\nfor response refinement and bolstering the stability of self-evolutionary\ntraining. Initiating with meta-skill learning, SELF acquires foundational\nmeta-skills with a focus on self-feedback and self-refinement. These\nmeta-skills are critical, guiding the model's subsequent self-evolution through\na cycle of perpetual training with self-curated data, thereby enhancing its\nintrinsic abilities. Given unlabeled instructions, SELF equips the model with\nthe capability to autonomously generate and interactively refine responses.\nThis synthesized training data is subsequently filtered and utilized for\niterative fine-tuning, enhancing the model's capabilities. Experimental results\non representative benchmarks substantiate that SELF can progressively advance\nits inherent abilities without the requirement of human intervention, thereby\nindicating a viable pathway for autonomous model evolution. Additionally, SELF\ncan employ online self-refinement strategy to produce responses of superior\nquality. In essence, the SELF framework signifies a progressive step towards\nautonomous LLM development, transforming the LLM from a mere passive recipient\nof information into an active participant in its own evolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jianqiao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wanjun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenyong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Baojun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TILFA: A Unified Framework for Text, Image, and Layout Fusion in Argument Mining. (arXiv:2310.05210v1 [cs.AI] CROSS LISTED)","link":"http://arxiv.org/abs/2310.05210","description":"<p>A main goal of Argument Mining (AM) is to analyze an author's stance. Unlike\nprevious AM datasets focusing only on text, the shared task at the 10th\nWorkshop on Argument Mining introduces a dataset including both text and\nimages. Importantly, these images contain both visual elements and optical\ncharacters. Our new framework, TILFA (A Unified Framework for Text, Image, and\nLayout Fusion in Argument Mining), is designed to handle this mixed data. It\nexcels at not only understanding text but also detecting optical characters and\nrecognizing layout details in images. Our model significantly outperforms\nexisting baselines, earning our team, KnowComp, the 1st place in the\nleaderboard of Argumentative Stance Classification subtask in this shared task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zong_Q/0/1/0/all/0/1\">Qing Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Baixuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1\">Tianshi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haochen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_G/0/1/0/all/0/1\">Ginny Y. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_S/0/1/0/all/0/1\">Simon See</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-10-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}