{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-12-20T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"EffMulti: Efficiently Modeling Complex Multimodal Interactions for Emotion Analysis. (arXiv:2212.08661v1 [cs.LG])","link":"http://arxiv.org/abs/2212.08661","description":"<p>Humans are skilled in reading the interlocutor's emotion from multimodal\nsignals, including spoken words, simultaneous speech, and facial expressions.\nIt is still a challenge to effectively decode emotions from the complex\ninteractions of multimodal signals. In this paper, we design three kinds of\nmultimodal latent representations to refine the emotion analysis process and\ncapture complex multimodal interactions from different views, including a\nintact three-modal integrating representation, a modality-shared\nrepresentation, and three modality-individual representations. Then, a\nmodality-semantic hierarchical fusion is proposed to reasonably incorporate\nthese representations into a comprehensive interaction representation. The\nexperimental results demonstrate that our EffMulti outperforms the\nstate-of-the-art methods. The compelling performance benefits from its\nwell-designed framework with ease of implementation, lower computing\ncomplexity, and less trainable parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_F/0/1/0/all/0/1\">Feng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chengyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_W/0/1/0/all/0/1\">Wanzeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Impact of Symbolic Representations on In-context Learning for Few-shot Reasoning. (arXiv:2212.08686v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08686","description":"<p>Pre-trained language models (LMs) have shown remarkable reasoning performance\nusing explanations (or ``chain-of-thought'' (CoT)) for in-context learning. On\nthe other hand, these reasoning tasks are usually presumed to be more\napproachable for symbolic programming. To make progress towards understanding\nin-context learning, we curate synthetic datasets containing equivalent\n(natural, symbolic) data pairs, where symbolic examples contain first-order\nlogic rules and predicates from knowledge bases (KBs). Then we revisit\nneuro-symbolic approaches and use Language Models as Logic Programmer (LMLP)\nthat learns from demonstrations containing logic rules and corresponding\nexamples to iteratively reason over KBs, recovering Prolog's backward chaining\nalgorithm. Comprehensive experiments are included to systematically compare\nLMLP with CoT in deductive reasoning settings, showing that LMLP enjoys more\nthan 25% higher accuracy than CoT on length generalization benchmarks even with\nfewer parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi-Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Erran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"'Rarely' a problem? Language models exhibit inverse scaling in their predictions following 'few'-type quantifiers. (arXiv:2212.08700v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08700","description":"<p>Language Models appear to perform poorly on quantification. We ask how badly.\n'Few'-type quantifiers, as in 'few children like vegetables' might pose a\nparticular challenge for Language Models, since the sentence components without\nthe quantifier are likely to co-occur, and because 'few'-type quantifiers are\nrare. We present 960 sentences stimuli from two human neurolinguistic\nexperiments to 22 autoregressive transformer models of differing sizes. Not\nonly do the models perform poorly on 'few'-type quantifiers, but overall the\nlarger the model, the worse its performance. We interpret this inverse scaling\nas suggesting that larger models increasingly reflect online rather than\noffline human processing, and argue that decreasing performance of larger\nmodels may challenge uses of Language Models as the basis for Natural Language\nSystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michaelov_J/0/1/0/all/0/1\">James A. Michaelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1\">Benjamin K. Bergen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Entropy-Based Methods of Word-Level Confidence Estimation for End-To-End Automatic Speech Recognition. (arXiv:2212.08703v1 [eess.AS])","link":"http://arxiv.org/abs/2212.08703","description":"<p>This paper presents a class of new fast non-trainable entropy-based\nconfidence estimation methods for automatic speech recognition. We show how\nper-frame entropy values can be normalized and aggregated to obtain a\nconfidence measure per unit and per word for Connectionist Temporal\nClassification (CTC) and Recurrent Neural Network Transducer (RNN-T) models.\nProposed methods have similar computational complexity to the traditional\nmethod based on the maximum per-frame probability, but they are more\nadjustable, have a wider effective threshold range, and better push apart the\nconfidence distributions of correct and incorrect words. We evaluate the\nproposed confidence measures on LibriSpeech test sets, and show that they are\nup to 2 and 4 times better than confidence estimation based on the maximum\nper-frame probability at detecting incorrect words for Conformer-CTC and\nConformer-RNN-T models, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Laptev_A/0/1/0/all/0/1\">Aleksandr Laptev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ginsburg_B/0/1/0/all/0/1\">Boris Ginsburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Story Planning. (arXiv:2212.08718v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08718","description":"<p>Automated plot generation is the challenge of generating a sequence of events\nthat will be perceived by readers as the plot of a coherent story. Traditional\nsymbolic planners plan a story from a goal state and guarantee logical causal\nplot coherence but rely on a library of hand-crafted actions with their\npreconditions and effects. This closed world setting limits the length and\ndiversity of what symbolic planners can generate. On the other hand,\npre-trained neural language models can generate stories with great diversity,\nwhile being generally incapable of ending a story in a specified manner and can\nhave trouble maintaining coherence. In this paper, we present an approach to\nstory plot generation that unifies causal planning with neural language models.\nWe propose to use commonsense knowledge extracted from large language models to\nrecursively expand a story plot in a backward chaining fashion. Specifically,\nour system infers the preconditions for events in the story and then events\nthat will cause those conditions to become true. We performed automatic\nevaluation to measure narrative coherence as indicated by the ability to answer\nquestions about whether different events in the story are causally related to\nother events. Results indicate that our proposed method produces more coherent\nplotlines than several strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_A/0/1/0/all/0/1\">Anbang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Christopher Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1\">Taiwei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark O. Riedl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DuNST: Dual Noisy Self Training for Semi-Supervised Controllable Text Generation. (arXiv:2212.08724v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08724","description":"<p>Self-training (ST) has prospered again in language understanding by\naugmenting the fine-tuning of pre-trained language models when labeled data is\ninsufficient. However, it remains challenging to incorporate ST into\nattribute-controllable language generation. Augmented by only self-generated\npseudo text, generation models over-emphasize exploitation of the previously\nlearned space, suffering from a constrained generalization boundary. We revisit\nST and propose a novel method, DuNST to alleviate this problem. DuNST jointly\nmodels text generation and classification with a shared Variational AutoEncoder\nand corrupts the generated pseudo text by two kinds of flexible noise to\ndisturb the space. In this way, our model could construct and utilize both\npseudo text from given labels and pseudo labels from available unlabeled text,\nwhich are gradually refined during the ST process. We theoretically demonstrate\nthat DuNST can be regarded as enhancing exploration towards the potential real\ntext space, providing a guarantee of improved performance. Experiments on three\ncontrollable generation tasks show that DuNST could significantly boost control\naccuracy while maintaining comparable generation fluency and diversity against\nseveral strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yuxi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1\">Xiaoyuan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshmanan_L/0/1/0/all/0/1\">Laks V.S. Lakshmanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Scales Data Augmentation Approach In Natural Language Inference For Artifacts Mitigation And Pre-Trained Model Optimization. (arXiv:2212.08756v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08756","description":"<p>Machine learning models can reach high performance on benchmark natural\nlanguage processing (NLP) datasets but fail in more challenging settings. We\nstudy this issue when a pre-trained model learns dataset artifacts in natural\nlanguage inference (NLI), the topic of studying the logical relationship\nbetween a pair of text sequences. We provide a variety of techniques for\nanalyzing and locating dataset artifacts inside the crowdsourced Stanford\nNatural Language Inference (SNLI) corpus. We study the stylistic pattern of\ndataset artifacts in the SNLI. To mitigate dataset artifacts, we employ a\nunique multi-scale data augmentation technique with two distinct frameworks: a\nbehavioral testing checklist at the sentence level and lexical synonym criteria\nat the word level. Specifically, our combination method enhances our model's\nresistance to perturbation testing, enabling it to continuously outperform the\npre-trained baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhenyuan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RISE: Leveraging Retrieval Techniques for Summarization Evaluation. (arXiv:2212.08775v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08775","description":"<p>Evaluating automatically-generated text summaries is a challenging task.\nWhile there have been many interesting approaches, they still fall short of\nhuman evaluations. We present RISE, a new approach for evaluating summaries by\nleveraging techniques from information retrieval. RISE is first trained as a\nretrieval task using a dual-encoder retrieval setup, and can then be\nsubsequently utilized for evaluating a generated summary given an input\ndocument, without gold reference summaries. RISE is especially well suited when\nworking on new datasets where one may not have reference summaries available\nfor evaluation. We conduct comprehensive experiments on the SummEval benchmark\n(Fabbri et al., 2021) and the results show that RISE has higher correlation\nwith human evaluations compared to many past approaches to summarization\nevaluation. Furthermore, RISE also demonstrates data-efficiency and\ngeneralizability across languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uthus_D/0/1/0/all/0/1\">David Uthus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jianmo Ni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Cross-task Generalization of Unified Table-to-text Models with Compositional Task Configurations. (arXiv:2212.08780v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08780","description":"<p>There has been great progress in unifying various table-to-text tasks using a\nsingle encoder-decoder model trained via multi-task learning (Xie et al.,\n2022). However, existing methods typically encode task information with a\nsimple dataset name as a prefix to the encoder. This not only limits the\neffectiveness of multi-task learning, but also hinders the model's ability to\ngeneralize to new domains or tasks that were not seen during training, which is\ncrucial for real-world applications. In this paper, we propose compositional\ntask configurations, a set of prompts prepended to the encoder to improve\ncross-task generalization of unified models. We design the task configurations\nto explicitly specify the task type, as well as its input and output types. We\nshow that this not only allows the model to better learn shared knowledge\nacross different tasks at training, but also allows us to control the model by\ncomposing new configurations that apply novel input-output combinations in a\nzero-shot manner. We demonstrate via experiments over ten table-to-text tasks\nthat our method outperforms the UnifiedSKG baseline by noticeable margins in\nboth in-domain and zero-shot settings, with average improvements of +0.5 and\n+12.6 from using a T5-large backbone, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jifan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Rui Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinchi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_P/0/1/0/all/0/1\">Patrick Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiheng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Importance of Synthesizing High-quality Data for Text-to-SQL Parsing. (arXiv:2212.08785v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08785","description":"<p>Recently, there has been increasing interest in synthesizing data to improve\ndownstream text-to-SQL tasks. In this paper, we first examined the existing\nsynthesized datasets and discovered that state-of-the-art text-to-SQL\nalgorithms did not further improve on popular benchmarks when trained with\naugmented synthetic data. We observed two shortcomings: illogical synthetic SQL\nqueries from independent column sampling and arbitrary table joins. To address\nthese issues, we propose a novel synthesis framework that incorporates key\nrelationships from schema, imposes strong typing, and conducts\nschema-distance-weighted column sampling. We also adopt an intermediate\nrepresentation (IR) for the SQL-to-text task to further improve the quality of\nthe generated natural language questions. When existing powerful semantic\nparsers are pre-finetuned on our high-quality synthesized data, our experiments\nshow that these models have significant accuracy boosts on popular benchmarks,\nincluding new state-of-the-art performance on Spider.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yiyun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jiarong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yiqun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_W/0/1/0/all/0/1\">Wuwei Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Henry Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_A/0/1/0/all/0/1\">Anuj Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Alexander Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lin Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hang_C/0/1/0/all/0/1\">Chung-Wei Hang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Marvin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lilien_J/0/1/0/all/0/1\">Joe Lilien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_P/0/1/0/all/0/1\">Patrick Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiguo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castelli_V/0/1/0/all/0/1\">Vittorio Castelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relational Sentence Embedding for Flexible Semantic Matching. (arXiv:2212.08802v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08802","description":"<p>We present Relational Sentence Embedding (RSE), a new paradigm to further\ndiscover the potential of sentence embeddings. Prior work mainly models the\nsimilarity between sentences based on their embedding distance. Because of the\ncomplex semantic meanings conveyed, sentence pairs can have various relation\ntypes, including but not limited to entailment, paraphrasing, and\nquestion-answer. It poses challenges to existing embedding methods to capture\nsuch relational information. We handle the problem by learning associated\nrelational embeddings. Specifically, a relation-wise translation operation is\napplied to the source sentence to infer the corresponding target sentence with\na pre-trained Siamese-based encoder. The fine-grained relational similarity\nscores can be computed from learned embeddings. We benchmark our method on 19\ndatasets covering a wide range of tasks, including semantic textual similarity,\ntransfer, and domain-specific tasks. Experimental results show that our method\nis effective and flexible in modeling sentence relations and outperforms a\nseries of state-of-the-art sentence embedding methods.\nhttps://github.com/BinWang28/RSE\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better Datastore, Better Translation: Generating Datastores from Pre-Trained Models for Nearest Neural Machine Translation. (arXiv:2212.08822v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08822","description":"<p>Nearest Neighbor Machine Translation (kNNMT) is a simple and effective method\nof augmenting neural machine translation (NMT) with a token-level nearest\nneighbor retrieval mechanism. The effectiveness of kNNMT directly depends on\nthe quality of retrieved neighbors. However, original kNNMT builds datastores\nbased on representations from NMT models, which would result in poor retrieval\naccuracy when NMT models are not good enough, leading to sub-optimal\ntranslation performance. In this paper, we propose PRED, a framework that\nleverages Pre-trained models for Datastores in kNN-MT. Better representations\nfrom pre-trained models allow us to build datastores of better quality. We also\ndesign a novel contrastive alignment objective to mitigate the representation\ngap between the NMT model and pre-trained models, enabling the NMT model to\nretrieve from better datastores. We conduct extensive experiments on both\nbilingual and multilingual translation benchmarks, including WMT17 English\n$\\leftrightarrow$ Chinese, WMT14 English $\\leftrightarrow$ German, IWSLT14\nGerman $\\leftrightarrow$ English, and IWSLT14 multilingual datasets. Empirical\nresults demonstrate the effectiveness of PRED.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiahuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shanbo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zewei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Dense Retrieval Deserves Better Positive Pairs: Scalable Augmentation with Query Extraction and Generation. (arXiv:2212.08841v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08841","description":"<p>Dense retrievers have made significant strides in obtaining state-of-the-art\nresults on text retrieval and open-domain question answering (ODQA). Yet most\nof these achievements were made possible with the help of large annotated\ndatasets, unsupervised learning for dense retrieval models remains an open\nproblem. In this work, we explore two categories of methods for creating pseudo\nquery-document pairs, named query extraction (QExt) and transferred query\ngeneration (TQGen), to augment the retriever training in an annotation-free and\nscalable manner. Specifically, QExt extracts pseudo queries by document\nstructures or selecting salient random spans, and TQGen utilizes generation\nmodels trained for other NLP tasks (e.g., summarization) to produce pseudo\nqueries. Extensive experiments show that dense retrievers trained with\nindividual augmentation methods can perform comparably well with multiple\nstrong baselines, and combining them leads to further improvements, achieving\nstate-of-the-art performance of unsupervised dense retrieval on both BEIR and\nODQA datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_R/0/1/0/all/0/1\">Rui Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1\">Semih Yavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1\">Divyansh Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_L/0/1/0/all/0/1\">Lifu Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Ning Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianguo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_M/0/1/0/all/0/1\">Meghana Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation. (arXiv:2212.08853v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08853","description":"<p>Language models with the Transformers structure have shown great performance\nin natural language processing. However, there still poses problems when\nfine-tuning pre-trained language models on downstream tasks, such as\nover-fitting or representation collapse. In this work, we propose HyPe, a\nsimple yet effective fine-tuning technique to alleviate such problems by\nperturbing hidden representations of Transformers layers. Unlike previous works\nthat only add noise to inputs or parameters, we argue that the hidden\nrepresentations of Transformers layers convey more diverse and meaningful\nlanguage information. Therefore, making the Transformers layers more robust to\nhidden representation perturbations can further benefit the fine-tuning of PLMs\nen bloc. We conduct extensive experiments and analyses on GLUE and other\nnatural language inference datasets. Results demonstrate that HyPe outperforms\nvanilla fine-tuning and enhances generalization of hidden representations from\ndifferent layers. In addition, HyPe acquires negligible computational\noverheads, and is better than and compatible with previous state-of-the-art\nfine-tuning techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongyi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"'If you build they will come': Automatic Identification of News-Stakeholders to detect Party Preference in News Coverage. (arXiv:2212.08864v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08864","description":"<p>The coverage of different stakeholders mentioned in the news articles\nsignificantly impacts the slant or polarity detection of the concerned news\npublishers. For instance, the pro-government media outlets would give more\ncoverage to the government stakeholders to increase their accessibility to the\nnews audiences. In contrast, the anti-government news agencies would focus more\non the views of the opponent stakeholders to inform the readers about the\nshortcomings of government policies. In this paper, we address the problem of\nstakeholder extraction from news articles and thereby determine the inherent\nbias present in news reporting. Identifying potential stakeholders in\nmulti-topic news scenarios is challenging because each news topic has different\nstakeholders. The research presented in this paper utilizes both contextual\ninformation and external knowledge to identify the topic-specific stakeholders\nfrom news articles. We also apply a sequential incremental clustering algorithm\nto group the entities with similar stakeholder types. We carried out all our\nexperiments on news articles on four Indian government policies published by\nnumerous national and international news agencies. We also further generalize\nour system, and the experimental results show that the proposed model can be\nextended to other news topics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuila_A/0/1/0/all/0/1\">Alapan Kuila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Sudeshna Sarkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Rich Textual User-Product Context for Improving Sentiment Analysis. (arXiv:2212.08888v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08888","description":"<p>User and product information associated with a review is useful for sentiment\npolarity prediction. Typical approaches incorporating such information focus on\nmodeling users and products as implicitly learned representation vectors. Most\ndo not exploit the potential of historical reviews, or those that currently do\nrequire unnecessary modifications to model architecture or do not make full use\nof user/product associations. The contribution of this work is twofold: i) a\nmethod to explicitly employ historical reviews belonging to the same\nuser/product to initialize representations, and ii) efficient incorporation of\ntextual associations between users and products via a user-product\ncross-context module. Experiments on IMDb, Yelp-2013 and Yelp-2014 benchmarks\nshow that our approach substantially outperforms previous state-of-the-art.\nSince we employ BERT-base as the encoder, we additionally provide experiments\nin which our approach performs well with Span-BERT and Longformer. Furthermore,\nexperiments where the reviews of each user/product in the training data are\ndownsampled demonstrate the effectiveness of our approach under a low-resource\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1\">Chenyang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graham_Y/0/1/0/all/0/1\">Yvette Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1\">Jennifer Foster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Question Answering Performance through Manual Annotation: Costs, Benefits and Strategies. (arXiv:2212.08897v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08897","description":"<p>Recently proposed systems for open-domain question answering (OpenQA) require\nlarge amounts of training data to achieve state-of-the-art performance.\nHowever, data annotation is known to be time-consuming and therefore expensive\nto acquire. As a result, the appropriate datasets are available only for a\nhandful of languages (mainly English and Chinese). In this work, we introduce\nand publicly release PolQA, the first Polish dataset for OpenQA. It consists of\n7,000 questions, 87,525 manually labeled evidence passages, and a corpus of\nover 7,097,322 candidate passages. Each question is classified according to its\nformulation, type, as well as entity type of the answer. This resource allows\nus to evaluate the impact of different annotation choices on the performance of\nthe QA system and propose an efficient annotation strategy that increases the\npassage retrieval performance by 10.55 p.p. while reducing the annotation cost\nby 82%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rybak_P/0/1/0/all/0/1\">Piotr Rybak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Przybyla_P/0/1/0/all/0/1\">Piotr Przyby&#x142;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogrodniczuk_M/0/1/0/all/0/1\">Maciej Ogrodniczuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Know What I don't Know: Handling Ambiguous and Unanswerable Questions for Text-to-SQL. (arXiv:2212.08902v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08902","description":"<p>The task of text-to-SQL is to convert a natural language question to its\ncorresponding SQL query in the context of relational tables. Existing\ntext-to-SQL parsers generate a \"plausible\" SQL query for an arbitrary user\nquestion, thereby failing to correctly handle problematic user questions. To\nformalize this problem, we conduct a preliminary study on the observed\nambiguous and unanswerable cases in text-to-SQL and summarize them into 6\nfeature categories. Correspondingly, we identify the causes behind each\ncategory and propose requirements for handling ambiguous and unanswerable\nquestions. Following this study, we propose a simple yet effective\ncounterfactual example generation approach for the automatic generation of\nambiguous and unanswerable text-to-SQL examples. Furthermore, we propose a\nweakly supervised model DTE (Detecting-Then-Explaining) for error detection,\nlocalization, and explanation. Experimental results show that our model\nachieves the best result on both real-world examples and generated examples\ncompared with various baselines. We will release data and code for future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlling Styles in Neural Machine Translation with Activation Prompt. (arXiv:2212.08909v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08909","description":"<p>Neural machine translation(NMT) has aroused wide attention due to its\nimpressive quality. Beyond quality, controlling translation styles is also an\nimportant demand for many languages. Previous related studies mainly focus on\ncontrolling formality and gain some improvements. However, they still face two\nchallenges. The first is the evaluation limitation. Style contains abundant\ninformation including lexis, syntax, etc. But only formality is well studied.\nThe second is the heavy reliance on iterative fine-tuning when new styles are\nrequired. Correspondingly, this paper contributes in terms of the benchmark and\napproach. First, we re-visit this task and propose a multiway stylized machine\ntranslation (MSMT) benchmark, which includes multiple categories of styles in\nfour language directions to push the boundary of this task. Second, we propose\na method named style activation prompt (StyleAP) by retrieving prompts from\nstylized monolingual corpus, which needs no extra fine-tuning. Experiments show\nthat StyleAP could effectively control the style of translation and achieve\nremarkable performance. All of our data and code are released at\nhttps://github.com/IvanWang0730/StyleAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zewei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shanbo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Weiguo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaTranS: Adapting with Boundary-based Shrinking for End-to-End Speech Translation. (arXiv:2212.08911v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08911","description":"<p>To alleviate the data scarcity problem in End-to-end speech translation (ST),\npre-training on data for speech recognition and machine translation is\nconsidered as an important technique. However, the modality gap between speech\nand text prevents the ST model from efficiently inheriting knowledge from the\npre-trained models. In this work, we propose AdaTranS for end-to-end ST. It\nadapts the speech features with a new shrinking mechanism to mitigate the\nlength mismatch between speech and text features by predicting word boundaries.\nExperiments on the MUST-C dataset demonstrate that AdaTranS achieves better\nperformance than the other shrinking-based methods, with higher inference speed\nand lower memory usage. Further experiments also show that AdaTranS can be\nequipped with additional alignment losses to further improve performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xingshan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangyou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Claim Optimization in Computational Argumentation. (arXiv:2212.08913v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08913","description":"<p>An optimal delivery of arguments is key to persuasion in any debate, both for\nhumans and for AI systems. This requires the use of clear and fluent claims\nrelevant to the given debate. Prior work has studied the automatic assessment\nof argument quality extensively. Yet, no approach actually improves the quality\nso far. Our work is the first step towards filling this gap. We propose the\ntask of claim optimization: to rewrite argumentative claims to optimize their\ndelivery. As an initial approach, we first generate a candidate set of\noptimized claims using a sequence-to-sequence model, such as BART, while taking\ninto account contextual information. Our key idea is then to rerank generated\ncandidates with respect to different quality metrics to find the best\noptimization. In automatic and human evaluation, we outperform different\nreranking baselines on an English corpus, improving 60% of all claims\n(worsening 16% only). Follow-up analyses reveal that, beyond copy editing, our\napproach often specifies claims with details, whereas it adds less evidence\nthan humans do. Moreover, its capabilities generalize well to other domains,\nsuch as instructional texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Skitalinskaya_G/0/1/0/all/0/1\">Gabriella Skitalinskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spliethover_M/0/1/0/all/0/1\">Maximilian Splieth&#xf6;ver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachsmuth_H/0/1/0/all/0/1\">Henning Wachsmuth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Baseline for Beam Search Reranking. (arXiv:2212.08926v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08926","description":"<p>Reranking methods in machine translation aim to close the gap between common\nevaluation metrics (e.g. BLEU) and maximum likelihood learning and decoding\nalgorithms. Prior works address this challenge by training models to rerank\nbeam search candidates according to their predicted BLEU scores, building upon\nlarge models pretrained on massive monolingual corpora -- a privilege that was\nnever made available to the baseline translation model. In this work, we\nexamine a simple approach for training rerankers to predict translation\ncandidates' BLEU scores without introducing additional data or parameters. Our\napproach can be used as a clean baseline, decoupled from external factors, for\nfuture research in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vassertail_L/0/1/0/all/0/1\">Lior Vassertail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Information Extraction with Cross-Task and Cross-Instance High-Order Modeling. (arXiv:2212.08929v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08929","description":"<p>Prior works on Information Extraction (IE) typically predict different tasks\nand instances (e.g., event triggers, entities, roles, relations) independently,\nwhile neglecting their interactions and leading to model inefficiency. In this\nwork, we introduce a joint IE framework, HighIE, that learns and predicts\nmultiple IE tasks by integrating high-order cross-task and cross-instance\ndependencies. Specifically, we design two categories of high-order factors:\nhomogeneous factors and heterogeneous factors. Then, these factors are utilized\nto jointly predict labels of all instances. To address the intractability\nproblem of exact high-order inference, we incorporate a high-order neural\ndecoder that is unfolded from a mean-field variational inference method. The\nexperimental results show that our approach achieves consistent improvements on\nthree IE tasks compared with our baseline and prior work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zixia Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhaohui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wenjuan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zilong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards leveraging latent knowledge and Dialogue context for real-world conversational question answering. (arXiv:2212.08946v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08946","description":"<p>In many real-world scenarios, the absence of external knowledge source like\nWikipedia restricts question answering systems to rely on latent internal\nknowledge in limited dialogue data. In addition, humans often seek answers by\nasking several questions for more comprehensive information. As the dialog\nbecomes more extensive, machines are challenged to refer to previous\nconversation rounds to answer questions. In this work, we propose to leverage\nlatent knowledge in existing conversation logs via a neural Retrieval-Reading\nsystem, enhanced with a TFIDF-based text summarizer refining lengthy\nconversational history to alleviate the long context issue. Our experiments\nshow that our Retrieval-Reading system can exploit retrieved background\nknowledge to generate significantly better answers. The results also indicate\nthat our context summarizer significantly helps both the retriever and the\nreader by introducing more concise and less noisy contextual information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shaomu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paperno_D/0/1/0/all/0/1\">Denis Paperno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond the C: Retargetable Decompilation using Neural Machine Translation. (arXiv:2212.08950v1 [cs.CR])","link":"http://arxiv.org/abs/2212.08950","description":"<p>The problem of reversing the compilation process, decompilation, is an\nimportant tool in reverse engineering of computer software. Recently,\nresearchers have proposed using techniques from neural machine translation to\nautomate the process in decompilation. Although such techniques hold the\npromise of targeting a wider range of source and assembly languages, to date\nthey have primarily targeted C code. In this paper we argue that existing\nneural decompilers have achieved higher accuracy at the cost of requiring\nlanguage-specific domain knowledge such as tokenizers and parsers to build an\nabstract syntax tree (AST) for the source language, which increases the\noverhead of supporting new languages. We explore a different tradeoff that, to\nthe extent possible, treats the assembly and source languages as plain text,\nand show that this allows us to build a decompiler that is easily retargetable\nto new languages. We evaluate our prototype decompiler, Beyond The C (BTC), on\nGo, Fortran, OCaml, and C, and examine the impact of parameters such as\ntokenization and training data selection on the quality of decompilation,\nfinding that it achieves comparable decompilation results to prior work in\nneural decompilation with significantly less domain knowledge. We will release\nour training data, trained decompilation models, and code to help encourage\nfuture research into language-agnostic decompilation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_I/0/1/0/all/0/1\">Iman Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolan_Gavitt_B/0/1/0/all/0/1\">Brendan Dolan-Gavitt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language model acceptability judgements are not always robust to context. (arXiv:2212.08979v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08979","description":"<p>Targeted syntactic evaluations of language models ask whether models show\nstable preferences for syntactically acceptable content over minimal-pair\nunacceptable inputs. Most targeted syntactic evaluation datasets ask models to\nmake these judgements with just a single context-free sentence as input. This\ndoes not match language models' training regime, in which input sentences are\nalways highly contextualized by the surrounding corpus. This mismatch raises an\nimportant question: how robust are models' syntactic judgements in different\ncontexts? In this paper, we investigate the stability of language models'\nperformance on targeted syntactic evaluations as we vary properties of the\ninput context: the length of the context, the types of syntactic phenomena it\ncontains, and whether or not there are violations of grammaticality. We find\nthat model judgements are generally robust when placed in randomly sampled\nlinguistic contexts. However, they are substantially unstable for contexts\ncontaining syntactic structures matching those in the critical test content.\nAmong all tested models (GPT-2 and five variants of OPT), we significantly\nimprove models' judgements by providing contexts with matching syntactic\nstructures, and conversely significantly worsen them using unacceptable\ncontexts with matching but violated syntactic structures. This effect is\namplified by the length of the context, except for unrelated inputs. We show\nthat these changes in model performance are not explainable by simple features\nmatching the context and the test inputs, such as lexical overlap and\ndependency overlap. This sensitivity to highly specific syntactic features of\nthe context can only be explained by the models' implicit in-context learning\nabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sinha_K/0/1/0/all/0/1\">Koustuv Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gauthier_J/0/1/0/all/0/1\">Jon Gauthier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_A/0/1/0/all/0/1\">Aaron Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_K/0/1/0/all/0/1\">Kanishka Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuentes_K/0/1/0/all/0/1\">Keren Fuentes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Resource Authorship Style Transfer with In-Context Learning. (arXiv:2212.08986v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08986","description":"<p>Authorship style transfer involves altering the style of text to match the\nstyle of some target author whilst preserving the semantic meaning of the\noriginal text. Existing approaches to unsupervised authorship style transfer\nlike STRAP have largely focused on style transfer for target authors with many\nexamples of their writing style through books, speeches, or other published\nworks (Krishna et al., 2020). Due to this high-resource training data\nrequirement (often greater than 100,000 words), these approaches are often only\nuseful for style transfer to the style of published authors, politicians, or\nother well-known figures and authorship styles. In this paper, we attempt to\nperform low-resource authorship style transfer, a more challenging class of\nauthorship style transfer where only a limited amount of text in the target\nauthor's style may exist. In our experiments, we specifically choose source and\ntarget authors from Reddit to perform style transfer over their Reddit posts,\nlimiting ourselves to just 16 posts (on average $\\approx$ 500 words) of the\ntarget author's style. We then propose a method for automatic evaluation on the\nlow-resource authorship style transfer task utilizing authorship and style\nrepresentation embeddings (Rivera-Soto et al., 2021; Wegmann et al., 2022). We\nevaluate our style transferred outputs with the proposed automatic evaluation\nmethod and find that our method, STYLL, is able to outperform STRAP and a\ncomprehensive set of baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Ajay Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andrews_N/0/1/0/all/0/1\">Nicholas Andrews</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Robust Semantic Frame Parsing Pipeline on a New Complex Twitter Dataset. (arXiv:2212.08987v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08987","description":"<p>Most recent semantic frame parsing systems for spoken language understanding\n(SLU) are designed based on recurrent neural networks. These systems display\ndecent performance on benchmark SLU datasets such as ATIS or SNIPS, which\ncontain short utterances with relatively simple patterns. However, the current\nsemantic frame parsing models lack a mechanism to handle out-of-distribution\n(\\emph{OOD}) patterns and out-of-vocabulary (\\emph{OOV}) tokens. In this paper,\nwe introduce a robust semantic frame parsing pipeline that can handle both\n\\emph{OOD} patterns and \\emph{OOV} tokens in conjunction with a new complex\nTwitter dataset that contains long tweets with more \\emph{OOD} patterns and\n\\emph{OOV} tokens. The new pipeline demonstrates much better results in\ncomparison to state-of-the-art baseline SLU models on both the SNIPS dataset\nand the new Twitter dataset (Our new Twitter dataset can be downloaded from\nhttps://1drv.ms/u/s!AroHb-W6_OAlavK4begsDsMALfE?e=c8f2XX ). Finally, we also\nbuild an E2E application to demo the feasibility of our algorithm and show why\nit is useful in real application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hongxia Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PoE: a Panel of Experts for Generalized Automatic Dialogue Assessment. (arXiv:2212.08992v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08992","description":"<p>Chatbots are expected to be knowledgeable across multiple domains, e.g. for\ndaily chit-chat, exchange of information, and grounding in emotional\nsituations. To effectively measure the quality of such conversational agents, a\nmodel-based automatic dialogue evaluation metric (ADEM) is expected to perform\nwell across multiple domains. Despite significant progress, an ADEM that works\nwell in one domain does not necessarily generalize to another. This calls for a\ndedicated network architecture for domain generalization. To tackle the\nmulti-domain dialogue evaluation task, we propose a Panel of Experts (PoE), a\nmultitask network that consists of a shared transformer encoder and a\ncollection of lightweight adapters. The shared encoder captures the general\nknowledge of dialogues across domains, while each adapter specializes in one\nspecific domain and serves as a domain expert. To validate the idea, we\nconstruct a high-quality multi-domain dialogue dataset leveraging data\naugmentation and pseudo-labeling. The PoE network is comprehensively assessed\non 16 dialogue evaluation datasets spanning a wide range of dialogue domains.\nIt achieves state-of-the-art performance in terms of mean Spearman correlation\nover all the evaluation datasets. It exhibits better zero-shot generalization\nthan existing state-of-the-art ADEMs and the ability to easily adapt to new\ndomains with few-shot transfer learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DHaro_L/0/1/0/all/0/1\">Luis Fernando D&#x27;Haro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiquan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrichs_T/0/1/0/all/0/1\">Thomas Friedrichs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Impact of Sentiment Analysis in Fake Review Detection. (arXiv:2212.08995v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08995","description":"<p>Fake review identification is an important topic and has gained the interest\nof experts all around the world. Identifying fake reviews is challenging for\nresearchers, and there are several primary challenges to fake review detection.\nWe propose developing an initial research paper for investigating fake reviews\nby using sentiment analysis. Ten research papers are identified that show fake\nreviews, and they discuss currently available solutions for predicting or\ndetecting fake reviews. They also show the distribution of fake and truthful\nreviews through the analysis of sentiment. We summarize and compare previous\nstudies related to fake reviews. We highlight the most significant challenges\nin the sentiment evaluation process and demonstrate that there is a significant\nimpact on sentiment scores used to identify fake feedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yousif_A/0/1/0/all/0/1\">Amira Yousif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buckley_J/0/1/0/all/0/1\">James Buckley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence-level Feedback Generation for English Language Learners: Does Data Augmentation Help?. (arXiv:2212.08999v1 [cs.CL])","link":"http://arxiv.org/abs/2212.08999","description":"<p>In this paper, we present strong baselines for the task of Feedback Comment\nGeneration for Writing Learning. Given a sentence and an error span, the task\nis to generate a feedback comment explaining the error. Sentences and feedback\ncomments are both in English. We experiment with LLMs and also create multiple\npseudo datasets for the task, investigating how it affects the performance of\nour system. We present our results for the task along with extensive analysis\nof the generated comments with the aim of aiding future studies in feedback\ncomment generation for English language learners.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Behzad_S/0/1/0/all/0/1\">Shabnam Behzad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeldes_A/0/1/0/all/0/1\">Amir Zeldes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nathan Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Coreference Resolution based on Reinforcement Learning. (arXiv:2212.09028v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09028","description":"<p>The target of a coreference resolution system is to cluster all mentions that\nrefer to the same entity in a given context. All coreference resolution systems\nneed to solve two subtasks; one task is to detect all of the potential\nmentions, and the other is to learn the linking of an antecedent for each\npossible mention. In this paper, we propose a reinforcement learning\nactor-critic-based neural coreference resolution system, which can achieve both\nmention detection and mention clustering by leveraging an actor-critic deep\nreinforcement learning technique and a joint training algorithm. We experiment\non the BERT model to generate different input span representations. Our model\nwith the BERT span representation achieves the state-of-the-art performance\namong the models on the CoNLL-2012 Shared Task English Test Set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hongxia Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task Preferences across Languages on Community Question Answering Platforms. (arXiv:2212.09045v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09045","description":"<p>With the steady emergence of community question answering (CQA) platforms\nlike Quora, StackExchange, and WikiHow, users now have an unprecedented access\nto information on various kind of queries and tasks. Moreover, the rapid\nproliferation and localization of these platforms spanning geographic and\nlinguistic boundaries offer a unique opportunity to study the task requirements\nand preferences of users in different socio-linguistic groups. In this study,\nwe implement an entity-embedding model trained on a large longitudinal dataset\nof multi-lingual and task-oriented question-answer pairs to uncover and\nquantify the (i) prevalence and distribution of various online tasks across\nlinguistic communities, and (ii) emerging and receding trends in task\npopularity over time in these communities. Our results show that there exists\nsubstantial variance in task preference as well as popularity trends across\nlinguistic communities on the platform. Findings from this study will help Q&amp;A\nplatforms better curate and personalize content for non-English users, while\nalso offering valuable insights to businesses looking to target non-English\nspeaking communities online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santy_S/0/1/0/all/0/1\">Sebastin Santy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_P/0/1/0/all/0/1\">Prasanta Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehrotra_R/0/1/0/all/0/1\">Rishabh Mehrotra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Better Choice: Entire-space Datasets for Aspect Sentiment Triplet Extraction. (arXiv:2212.09052v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09052","description":"<p>Aspect sentiment triplet extraction (ASTE) aims to extract aspect term,\nsentiment and opinion term triplets from sentences. Since the initial datasets\nused to evaluate models on ASTE had flaws, several studies later corrected the\ninitial datasets and released new versions of the datasets independently. As a\nresult, different studies select different versions of datasets to evaluate\ntheir methods, which makes ASTE-related works hard to follow. In this paper, we\nanalyze the relation between different versions of datasets and suggest that\nthe entire-space version should be used for ASTE. Besides the sentences\ncontaining triplets and the triplets in the sentences, the entire-space version\nadditionally includes the sentences without triplets and the aspect terms which\ndo not belong to any triplets. Hence, the entire-space version is consistent\nwith real-world scenarios and evaluating models on the entire-space version can\nbetter reflect the models' performance in real-world scenarios. In addition,\nexperimental results show that evaluating models on non-entire-space datasets\ninflates the performance of existing models and models trained on the\nentire-space version can obtain better performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuncong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1\">Sheng-Hua Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Digital \"Echo Chambers\": The Role of Viewpoint Diversity in Political Discussion. (arXiv:2212.09056v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09056","description":"<p>Increasingly taking place in online spaces, modern political conversations\nare typically perceived to be unproductively affirming -- siloed in so called\n``echo chambers'' of exclusively like-minded discussants. Yet, to date we lack\nsufficient means to measure viewpoint diversity in conversations. To this end,\nin this paper, we operationalize two viewpoint metrics proposed for recommender\nsystems and adapt them to the context of social media conversations. This is\nthe first study to apply these two metrics (Representation and Fragmentation)\nto real world data and to consider the implications for online conversations\nspecifically. We apply these measures to two topics -- daylight savings time\n(DST), which serves as a control, and the more politically polarized topic of\nimmigration. We find that the diversity scores for both Fragmentation and\nRepresentation are lower for immigration than for DST. Further, we find that\nwhile pro-immigrant views receive consistent pushback on the platform,\nanti-immigrant views largely operate within echo chambers. We observe less\nsevere yet similar patterns for DST. Taken together, Representation and\nFragmentation paint a meaningful and important new picture of viewpoint\ndiversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hada_R/0/1/0/all/0/1\">Rishav Hada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fard_A/0/1/0/all/0/1\">Amir Ebrahimi Fard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shugars_S/0/1/0/all/0/1\">Sarah Shugars</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossini_P/0/1/0/all/0/1\">Patricia Rossini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tromble_R/0/1/0/all/0/1\">Rebekah Tromble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tintarev_N/0/1/0/all/0/1\">Nava Tintarev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEATs: Audio Pre-Training with Acoustic Tokenizers. (arXiv:2212.09058v1 [eess.AS])","link":"http://arxiv.org/abs/2212.09058","description":"<p>The massive growth of self-supervised learning (SSL) has been witnessed in\nlanguage, vision, speech, and audio domains over the past few years. While\ndiscrete label prediction is widely adopted for other modalities, the\nstate-of-the-art audio SSL models still employ reconstruction loss for\npre-training. Compared with reconstruction loss, semantic-rich discrete label\nprediction encourages the SSL model to abstract the high-level audio semantics\nand discard the redundant details as in human perception. However, a\nsemantic-rich acoustic tokenizer for general audio pre-training is usually not\nstraightforward to obtain, due to the continuous property of audio and\nunavailable phoneme sequences like speech. To tackle this challenge, we propose\nBEATs, an iterative audio pre-training framework to learn Bidirectional Encoder\nrepresentation from Audio Transformers, where an acoustic tokenizer and an\naudio SSL model are optimized by iterations. In the first iteration, we use\nrandom projection as the acoustic tokenizer to train an audio SSL model in a\nmask and label prediction manner. Then, we train an acoustic tokenizer for the\nnext iteration by distilling the semantic knowledge from the pre-trained or\nfine-tuned audio SSL model. The iteration is repeated with the hope of mutual\npromotion of the acoustic tokenizer and audio SSL model. The experimental\nresults demonstrate our acoustic tokenizers can generate discrete labels with\nrich audio semantics and our audio SSL models achieve state-of-the-art results\nacross various audio classification benchmarks, even outperforming previous\nmodels that use more training data and model parameters significantly.\nSpecifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for\naudio-only models without using any external data, and 98.1% accuracy on\nESC-50. The code and pre-trained models are available at https://aka.ms/beats.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1\">Sanyuan Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chengyi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tompkins_D/0/1/0/all/0/1\">Daniel Tompkins</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Let's Negotiate! A Survey of Negotiation Dialogue Systems. (arXiv:2212.09072v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09072","description":"<p>Negotiation is one of the crucial abilities in human communication, and there\nhas been a resurgent research interest in negotiation dialogue systems\nrecently, which goal is to empower intelligent agents with such ability that\ncan efficiently help humans resolve conflicts or reach beneficial agreements.\nAlthough there have been many explorations in negotiation dialogue systems, a\nsystematic review of this task has to date remained notably absent. To this\nend, we aim to fill this gap by reviewing contemporary studies in the emerging\nfield of negotiation dialogue systems, covering benchmarks, evaluations, and\nmethodologies. Furthermore, we also discuss potential future directions,\nincluding multi-modal, multi-party, and cross-cultural negotiation scenarios.\nOur goal is to provide the community with a systematic overview of negotiation\ndialogue systems and to inspire future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1\">Haolan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1\">Tao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yuncheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Suraj Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lizhen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthesis and Evaluation of a Domain-specific Large Data Set for Dungeons & Dragons. (arXiv:2212.09080v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09080","description":"<p>This paper introduces the Forgotten Realms Wiki (FRW) data set and domain\nspecific natural language generation using FRW along with related analyses.\nForgotten Realms is the de-facto default setting of the popular open ended\ntabletop fantasy role playing game, Dungeons &amp; Dragons. The data set was\nextracted from the Forgotten Realms Fandom wiki consisting of more than over\n45,200 articles. The FRW data set is constituted of 11 sub-data sets in a\nnumber of formats: raw plain text, plain text annotated by article title,\ndirected link graphs, wiki info-boxes annotated by the wiki article title,\nPoincar\\'e embedding of first link graph, multiple Word2Vec and Doc2Vec models\nof the corpus. This is the first data set of this size for the Dungeons &amp;\nDragons domain. We then present a pairwise similarity comparison benchmark\nwhich utilizes similarity measures. In addition, we perform D&amp;D domain specific\nnatural language generation using the corpus and evaluate the named entity\nclassification with respect to the lore of Forgotten Realms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peiris_A/0/1/0/all/0/1\">Akila Peiris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">Nisansa de Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PVGRU: Generating Diverse and Relevant Dialogue Responses via Pseudo-Variational Mechanism. (arXiv:2212.09086v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09086","description":"<p>We investigate response generation for multi-turn dialogue in\ngenerative-based chatbots. Existing generative models based on RNNs (Recurrent\nNeural Networks) usually employ the last hidden state to summarize the\nsequences, which makes models unable to capture the subtle variability observed\nin different dialogues and cannot distinguish the differences between dialogues\nthat are similar in composition. In this paper, we propose a Pseudo-Variational\nGated Recurrent Unit (PVGRU) component without posterior knowledge through\nintroducing a recurrent summarizing variable into the GRU, which can aggregate\nthe accumulated distribution variations of subsequences. PVGRU can perceive the\nsubtle semantic variability through summarizing variables that are optimized by\nthe devised distribution consistency and reconstruction objectives. In\naddition, we build a Pseudo-Variational Hierarchical Dialogue (PVHD) model\nbased on PVGRU. Experimental results demonstrate that PVGRU can broadly improve\nthe diversity and relevance of responses on two benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongkang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale. (arXiv:2212.09095v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09095","description":"<p>Language models have been shown to perform better with an increase in scale\non a wide variety of tasks via the in-context learning paradigm. In this paper,\nwe investigate the hypothesis that the ability of a large language model to\nin-context learn-perform a task is not uniformly spread across all of its\nunderlying components. Using a 66 billion parameter language model (OPT-66B)\nacross a diverse set of 14 downstream tasks, we find this is indeed the case:\n$\\sim$70% of attention heads and $\\sim$20% of feed forward networks can be\nremoved with minimal decline in task performance. We find substantial overlap\nin the set of attention heads (un)important for in-context learning across\ntasks and number of in-context examples. We also address our hypothesis through\na task-agnostic lens, finding that a small set of attention heads in OPT-66B\nscore highly on their ability to perform primitive induction operations\nassociated with in-context learning, namely, prefix matching and copying. These\ninduction heads overlap with task-specific important heads, suggesting that\ninduction heads are among the heads capable of more sophisticated behaviors\nassociated with in-context learning. Overall, our study provides several\ninsights that indicate large language models may be under-trained to perform\nin-context learning and opens up questions on how to pre-train language models\nto more effectively perform in-context learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1\">Hritik Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Karthik Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dingliwal_S/0/1/0/all/0/1\">Saket Dingliwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodapati_S/0/1/0/all/0/1\">Sravan Bodapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1\">Katrin Kirchhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continually Learning from Existing Models: Knowledge Accumulation for Neural Machine Translation. (arXiv:2212.09097v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09097","description":"<p>Although continually extending an existing NMT model to new domains or\nlanguages has attracted intensive interest in recent years, the equally\nvaluable problem of continually improving a given NMT model in its domain by\nleveraging knowledge from an unlimited number of existing NMT models is not\nexplored yet. To facilitate the study, we propose a formal definition for the\nproblem named knowledge accumulation for NMT (KA-NMT) with corresponding\ndatasets and evaluation metrics and develop a novel method for KA-NMT. We\ninvestigate a novel knowledge detection algorithm to identify beneficial\nknowledge from existing models at token level, and propose to learn from\nbeneficial knowledge and learn against other knowledge simultaneously to\nimprove learning efficiency. To alleviate catastrophic forgetting, we further\npropose to transfer knowledge from previous to current version of the given\nmodel. Extensive experiments show that our proposed method significantly and\nconsistently outperforms representative baselines under homogeneous,\nheterogeneous, and malicious model settings for different language pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanchi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaSQuE: Improved Zero-Shot Classification from Explanations Through Quantifier Modeling and Curriculum Learning. (arXiv:2212.09104v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09104","description":"<p>A hallmark of human intelligence is the ability to learn new concepts purely\nfrom language. Several recent approaches have explored training machine\nlearning models via natural language supervision. However, these approaches\nfall short in leveraging linguistic quantifiers (such as 'always' or 'rarely')\nand mimicking humans in compositionally learning complex tasks. Here, we\npresent LaSQuE, a method that can learn zero-shot classifiers from language\nexplanations by using three new strategies - (1) modeling the semantics of\nlinguistic quantifiers in explanations (including exploiting ordinal strength\nrelationships, such as 'always' &gt; 'likely'), (2) aggregating information from\nmultiple explanations using an attention-based mechanism, and (3) model\ntraining via curriculum learning. With these strategies, LaSQuE outperforms\nprior work, showing an absolute gain of up to 7% in generalizing to unseen\nreal-world classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sayan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_R/0/1/0/all/0/1\">Rakesh R Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Shashank Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curriculum Sampling for Dense Retrieval with Document Expansion. (arXiv:2212.09114v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09114","description":"<p>The dual-encoder has become the de facto architecture for dense retrieval.\nTypically, it computes the latent representations of the query and document\nindependently, thus failing to fully capture the interactions between the query\nand document. To alleviate this, recent work expects to get query-informed\nrepresentations of documents. During training, it expands the document with a\nreal query, while replacing the real query with a generated pseudo query at\ninference. This discrepancy between training and inference makes the dense\nretrieval model pay more attention to the query information but ignore the\ndocument when computing the document representation. As a result, it even\nperforms worse than the vanilla dense retrieval model, since its performance\ndepends heavily on the relevance between the generated queries and the real\nquery. In this paper, we propose a curriculum sampling strategy, which also\nresorts to the pseudo query at training and gradually increases the relevance\nof the generated query to the real query. In this way, the retrieval model can\nlearn to extend its attention from the document only to both the document and\nquery, hence getting high-quality query-informed document representations.\nExperimental results on several passage retrieval datasets show that our\napproach outperforms the previous dense retrieval methods1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xingwei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_A/0/1/0/all/0/1\">A-Long Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_A/0/1/0/all/0/1\">Anlei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jian Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yiu_S/0/1/0/all/0/1\">Siu Ming Yiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recall, Expand and Multi-Candidate Cross-Encode: Fast and Accurate Ultra-Fine Entity Typing. (arXiv:2212.09125v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09125","description":"<p>Ultra-fine entity typing (UFET) predicts extremely free-formed types (e.g.,\npresident, politician) of a given entity mention (e.g., Joe Biden) in context.\nState-of-the-art (SOTA) methods use the cross-encoder (CE) based architecture.\nCE concatenates the mention (and its context) with each type and feeds the\npairs into a pretrained language model (PLM) to score their relevance. It\nbrings deeper interaction between mention and types to reach better performance\nbut has to perform N (type set size) forward passes to infer types of a single\nmention. CE is therefore very slow in inference when the type set is large\n(e.g., N = 10k for UFET). To this end, we propose to perform entity typing in a\nrecall-expand-filter manner. The recall and expand stages prune the large type\nset and generate K (K is typically less than 256) most relevant type candidates\nfor each mention. At the filter stage, we use a novel model called MCCE to\nconcurrently encode and score these K candidates in only one forward pass to\nobtain the final type prediction. We investigate different variants of MCCE and\nextensive experiments show that MCCE under our paradigm reaches SOTA\nperformance on ultra-fine entity typing and is thousands of times faster than\nthe cross-encoder. We also found MCCE is very effective in fine-grained (130\ntypes) and coarse-grained (9 types) entity typing. Our code is available at\n\\url{https://github.com/modelscope/AdaSeq/tree/master/examples/MCCE}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chengyue Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_W/0/1/0/all/0/1\">Wenyang Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaobin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Discontinuous Constituency Parsing with Mildly Context-Sensitive Grammars. (arXiv:2212.09140v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09140","description":"<p>We study grammar induction with mildly context-sensitive grammars for\nunsupervised discontinuous parsing. Using the probabilistic linear context-free\nrewriting system (LCFRS) formalism, our approach fixes the rule structure in\nadvance and focuses on parameter learning with maximum likelihood. To reduce\nthe computational complexity of both parsing and parameter estimation, we\nrestrict the grammar formalism to LCFRS-2 (i.e., binary LCFRS with fan-out two)\nand further discard rules that require O(n^6) time to parse, reducing inference\nto O(n^5). We find that using a large number of nonterminals is beneficial and\nthus make use of tensor decomposition-based rank-space dynamic programming with\nan embedding-based parameterization of rule probabilities to scale up the\nnumber of nonterminals. Experiments on German and Dutch show that our approach\nis able to induce linguistically meaningful trees with continuous and\ndiscontinuous structures\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Songlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger P. Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model. (arXiv:2212.09146v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09146","description":"<p>The emergence of large pretrained models has enabled language models to\nachieve superior performance in common NLP tasks, including language modeling\nand question answering, compared to previous static word representation\nmethods. Augmenting these models with a retriever to retrieve the related text\nand documents as supporting information has shown promise in effectively\nsolving NLP problems in a more interpretable way given that the additional\nknowledge is injected explicitly rather than being captured in the models'\nparameters. In spite of the recent progress, our analysis on\nretriever-augmented language models shows that this class of language models\nstill lack reasoning over the retrieved documents. In this paper, we study the\nstrengths and weaknesses of different retriever-augmented language models such\nas REALM, kNN-LM, FiD, ATLAS, and Flan-T5 in reasoning over the selected\ndocuments in different tasks. In particular, we analyze the reasoning failures\nof each of these models and study how the models' failures in reasoning are\nrooted in the retriever module as well as the language model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+BehnamGhader_P/0/1/0/all/0/1\">Parishad BehnamGhader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miret_S/0/1/0/all/0/1\">Santiago Miret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Isotropy and Learning Dynamics of Contrastive-based Sentence Representation Learning. (arXiv:2212.09170v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09170","description":"<p>Incorporating contrastive learning objectives in sentence representation\nlearning (SRL) has yielded significant improvements on many sentence-level NLP\ntasks. However, It is not well understood why contrastive learning works for\nlearning sentence-level semantics. In this paper, we take a closer look at\ncontrastive sentence representation learning through the lens of isotropy and\nlearning dynamics. We interpret its success stories through the geometry of the\nrepresentation shifts. We show that contrastive learning brings isotropy, and\nsurprisingly learns to converge tokens to similar positions in the semantic\nspace if given the signal that they are in the same sentence. Also, what we\nformalize as \"spurious contextualization\" is mitigated for semantically\nmeaningful tokens, while augmented for functional ones. The embedding space is\npushed toward the origin during training, with more areas now better defined.\nWe ablate these findings by observing the learning dynamic with different\ntraining temperatures, batch sizes and pooling methods. With these findings, we\naim to shed light on future designs of sentence representation learning\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chenghao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moubayed_N/0/1/0/all/0/1\">Noura Al Moubayed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rainproof: An Umbrella To Shield Text Generators From Out-Of-Distribution Data. (arXiv:2212.09171v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09171","description":"<p>As more and more conversational and translation systems are deployed in\nproduction, it is essential to implement and to develop effective control\nmechanisms guaranteeing their proper functioning and security. An essential\ncomponent to ensure safe system behavior is out-of-distribution (OOD)\ndetection, which aims at detecting whether an input sample is statistically far\nfrom the training distribution. Although OOD detection is a widely covered\ntopic in classification tasks, it has received much less attention in text\ngeneration. This paper addresses the problem of OOD detection for machine\ntranslation and dialog generation from an operational perspective. Our\ncontributions include: (i) RAINPROOF a Relative informAItioN Projection ODD\ndetection framework; and (ii) a more operational evaluation setting for OOD\ndetection. Surprisingly, we find that OOD detection is not necessarily aligned\nwith task-specific measures. The OOD detector may filter out samples that are\nwell processed by the model and keep samples that are not, leading to weaker\nperformance. Our results show that RAINPROOF breaks this curse and achieve good\nresults in OOD detection while increasing performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Darrin_M/0/1/0/all/0/1\">Maxime Darrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1\">Pablo Piantanida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems. (arXiv:2212.09180v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09180","description":"<p>There has been great recent advancement in human-computer chat. However,\nproper evaluation currently requires human judgements that produce notoriously\nhigh-variance metrics due to their inherent subjectivity. Furthermore, there is\nlittle standardization in the methods and labels used for evaluation, with an\noverall lack of work to compare and assess the validity of various evaluation\napproaches. As a consequence, existing evaluation results likely leave an\nincomplete picture of the strengths and weaknesses of open-domain chatbots. We\naim towards a dimensional evaluation of human-computer chat that can reliably\nmeasure several distinct aspects of chat quality. To this end, we present our\nnovel human evaluation method that quantifies the rate of several\nquality-related chatbot behaviors. Our results demonstrate our method to be\nmore suitable for dimensional chat evaluation than alternative likert-style or\ncomparative methods. We then use our validated method and existing methods to\nevaluate four open-domain chat models from the recent literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Finch_S/0/1/0/all/0/1\">Sarah E. Finch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finch_J/0/1/0/all/0/1\">James D. Finch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent Analogical Reasoning in Large Language Models. (arXiv:2212.09196v1 [cs.AI])","link":"http://arxiv.org/abs/2212.09196","description":"<p>The recent advent of large language models - large neural networks trained on\na simple predictive objective over a massive corpus of natural language - has\nreinvigorated debate over whether human cognitive capacities might emerge in\nsuch generic models given sufficient training data. Of particular interest is\nthe ability of these models to reason about novel problems zero-shot, without\nany direct training on those problems. In human cognition, this capacity is\nclosely tied to an ability to reason by analogy. Here, we performed a direct\ncomparison between human reasoners and a large language model (GPT-3) on a\nrange of analogical tasks, including a novel text-based matrix reasoning task\nclosely modeled on Raven's Progressive Matrices. We found that GPT-3 displayed\na surprisingly strong capacity for abstract pattern induction, matching or even\nsurpassing human capabilities in most settings. Our results indicate that large\nlanguage models such as GPT-3 have acquired an emergent ability to find\nzero-shot solutions to a broad range of analogy problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Webb_T/0/1/0/all/0/1\">Taylor Webb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holyoak_K/0/1/0/all/0/1\">Keith J. Holyoak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hongjing Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OASum: Large-Scale Open Domain Aspect-based Summarization. (arXiv:2212.09233v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09233","description":"<p>Aspect or query-based summarization has recently caught more attention, as it\ncan generate differentiated summaries based on users' interests. However, the\ncurrent dataset for aspect or query-based summarization either focuses on\nspecific domains, contains relatively small-scale instances, or includes only a\nfew aspect types. Such limitations hinder further explorations in this\ndirection. In this work, we take advantage of crowd-sourcing knowledge on\nWikipedia.org and automatically create a high-quality, large-scale open-domain\naspect-based summarization dataset named OASum, which contains more than 3.7\nmillion instances with around 1 million different aspects on 2 million\nWikipedia pages. We provide benchmark results on OAsum and demonstrate its\nability for diverse aspect-based summarization generation. To overcome the data\nscarcity problem on specific domains, we also perform zero-shot, few-shot, and\nfine-tuning on seven downstream datasets. Specifically, zero/few-shot and\nfine-tuning results show that the model pre-trained on our corpus demonstrates\na strong aspect or query-focused generation ability compared with the backbone\nmodel. Our dataset and pre-trained checkpoints are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xianjun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaiqiang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sangwoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiaoman Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petzold_L/0/1/0/all/0/1\">Linda Petzold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PAL: Persona-Augmented Emotional Support Conversation Generation. (arXiv:2212.09235v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09235","description":"<p>Due to the lack of human resources for mental health support, there is an\nincreasing demand for employing conversational agents for support. Recent work\nhas demonstrated the effectiveness of dialogue models in providing emotional\nsupport. As previous studies have demonstrated that seekers' persona is an\nimportant factor for effective support, we investigate whether there are\nbenefits to modeling such information in dialogue models for support. In this\npaper, our empirical analysis verifies that persona has an important impact on\nemotional support. Therefore, we propose a framework for dynamically inferring\nand modeling seekers' persona. We first train a model for inferring the\nseeker's persona from the conversation history. Accordingly, we propose PAL, a\nmodel that leverages persona information and, in conjunction with our\nstrategy-based controllable generation method, provides personalized emotional\nsupport. Automatic and manual evaluations demonstrate that our proposed model,\nPAL, achieves state-of-the-art results, outperforming the baselines on the\nstudied benchmark. Our code and data are publicly available at\nhttps://github.com/chengjl19/PAL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jiale Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabour_S/0/1/0/all/0/1\">Sahand Sabour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation. (arXiv:2212.09246v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09246","description":"<p>Pre-trained language models, despite their rapid advancements powered by\nscale, still fall short of robust commonsense capabilities. And yet, scale\nappears to be the winning recipe; after all, the largest models seem to have\nacquired the largest amount of commonsense capabilities. Or is it?\n</p>\n<p>In this paper, we investigate the possibility of a seemingly impossible\nmatch: can smaller language models with dismal commonsense capabilities (i.e.,\nGPT-2), ever win over models that are orders of magnitude larger and better\n(i.e., GPT-3), if the smaller models are powered with novel commonsense\ndistillation algorithms? The key intellectual question we ask here is whether\nit is possible, if at all, to design a learning algorithm that does not benefit\nfrom scale, yet leads to a competitive level of commonsense acquisition. In\nthis work, we study the generative models of commonsense knowledge, focusing on\nthe task of generating generics, statements of commonsense facts about everyday\nconcepts, e.g., birds can fly.\n</p>\n<p>We introduce a novel commonsense distillation framework, I2D2, that loosely\nfollows the Symbolic Knowledge Distillation of West et al. but breaks the\ndependence on the extreme-scale models as the teacher model by two innovations:\n(1) the novel adaptation of NeuroLogic Decoding to enhance the generation\nquality of the weak, off-the-shelf language models, and (2) self-imitation\nlearning to iteratively learn from the model's own enhanced commonsense\nacquisition capabilities. Empirical results suggest that scale is not the only\nway, as novel algorithms can be a promising alternative. Moreover, our study\nleads to a new corpus of generics, Gen-A-Tomic, that is of the largest and\nhighest quality available to date.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaguchi_K/0/1/0/all/0/1\">Keisuke Sakaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language to Code Generation in Interactive Data Science Notebooks. (arXiv:2212.09248v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09248","description":"<p>Computational notebooks, such as Jupyter notebooks, are interactive computing\nenvironments that are ubiquitous among data scientists to perform data\nwrangling and analytic tasks. To measure the performance of AI pair programmers\nthat automatically synthesize programs for those tasks given natural language\n(NL) intents from users, we build ARCADE, a benchmark of 1082 code generation\nproblems using the pandas data analysis framework in data science notebooks.\nARCADE features multiple rounds of NL-to-code problems from the same notebook.\nIt requires a model to understand rich multi-modal contexts, such as existing\nnotebook cells and their execution states as well as previous turns of\ninteraction. To establish a strong baseline on this challenging task, we\ndevelop PaChiNCo, a 62B code language model (LM) for Python computational\nnotebooks, which significantly outperforms public code LMs. Finally, we explore\nfew-shot prompting strategies to elicit better code with step-by-step\ndecomposition and NL explanation, showing the potential to improve the\ndiversity and explainability of model predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Pengcheng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wen-Ding Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1\">Kefan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Abhishek Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yeming Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1\">Kensen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howland_J/0/1/0/all/0/1\">Joshua Howland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailey_P/0/1/0/all/0/1\">Paige Bailey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catasta_M/0/1/0/all/0/1\">Michele Catasta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1\">Henryk Michalewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polozov_A/0/1/0/all/0/1\">Alex Polozov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1\">Charles Sutton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Language Model Behaviors with Model-Written Evaluations. (arXiv:2212.09251v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09251","description":"<p>As language models (LMs) scale, they develop many novel behaviors, good and\nbad, exacerbating the need to evaluate how they behave. Prior work creates\nevaluations with crowdwork (which is time-consuming and expensive) or existing\ndata sources (which are not always available). Here, we automatically generate\nevaluations with LMs. We explore approaches with varying amounts of human\neffort, from instructing LMs to write yes/no questions to making complex\nWinogender schemas with multiple stages of LM-based generation and filtering.\nCrowdworkers rate the examples as highly relevant and agree with 90-100% of\nlabels, sometimes more so than corresponding human-written datasets. We\ngenerate 154 datasets and discover new cases of inverse scaling where LMs get\nworse with size. Larger LMs repeat back a dialog user's preferred answer\n(\"sycophancy\") and express greater desire to pursue concerning goals like\nresource acquisition and goal preservation. We also find some of the first\nexamples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF\nmakes LMs worse. For example, RLHF makes LMs express stronger political views\n(on gun rights and immigration) and a greater desire to avoid shut down.\nOverall, LM-written evaluations are high-quality and let us quickly discover\nmany novel LM behaviors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ringer_S/0/1/0/all/0/1\">Sam Ringer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukosiute_K/0/1/0/all/0/1\">Kamil&#x117; Luko&#x161;i&#x16b;t&#x117;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Karina Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Edwin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heiner_S/0/1/0/all/0/1\">Scott Heiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pettit_C/0/1/0/all/0/1\">Craig Pettit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsson_C/0/1/0/all/0/1\">Catherine Olsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1\">Sandipan Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1\">Saurav Kadavath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_A/0/1/0/all/0/1\">Andy Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anna Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mann_B/0/1/0/all/0/1\">Ben Mann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Israel_B/0/1/0/all/0/1\">Brian Israel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seethor_B/0/1/0/all/0/1\">Bryan Seethor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKinnon_C/0/1/0/all/0/1\">Cameron McKinnon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olah_C/0/1/0/all/0/1\">Christopher Olah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1\">Da Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amodei_D/0/1/0/all/0/1\">Daniela Amodei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amodei_D/0/1/0/all/0/1\">Dario Amodei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1\">Dawn Drain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dustin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Johnson_E/0/1/0/all/0/1\">Eli Tran-Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khundadze_G/0/1/0/all/0/1\">Guro Khundadze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kernion_J/0/1/0/all/0/1\">Jackson Kernion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landis_J/0/1/0/all/0/1\">James Landis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerr_J/0/1/0/all/0/1\">Jamie Kerr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_J/0/1/0/all/0/1\">Jared Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyun_J/0/1/0/all/0/1\">Jeeyoon Hyun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landau_J/0/1/0/all/0/1\">Joshua Landau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ndousse_K/0/1/0/all/0/1\">Kamal Ndousse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_L/0/1/0/all/0/1\">Landon Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovitt_L/0/1/0/all/0/1\">Liane Lovitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_M/0/1/0/all/0/1\">Martin Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellitto_M/0/1/0/all/0/1\">Michael Sellitto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miranda Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsland_N/0/1/0/all/0/1\">Neerav Kingsland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhage_N/0/1/0/all/0/1\">Nelson Elhage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_N/0/1/0/all/0/1\">Nicholas Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mercado_N/0/1/0/all/0/1\">Noem&#xed; Mercado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DasSarma_N/0/1/0/all/0/1\">Nova DasSarma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rausch_O/0/1/0/all/0/1\">Oliver Rausch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larson_R/0/1/0/all/0/1\">Robin Larson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCandlish_S/0/1/0/all/0/1\">Sam McCandlish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnston_S/0/1/0/all/0/1\">Scott Johnston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kravec_S/0/1/0/all/0/1\">Shauna Kravec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Showk_S/0/1/0/all/0/1\">Sheer El Showk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanham_T/0/1/0/all/0/1\">Tamera Lanham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Telleen_Lawton_T/0/1/0/all/0/1\">Timothy Telleen-Lawton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_T/0/1/0/all/0/1\">Tom Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henighan_T/0/1/0/all/0/1\">Tom Henighan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hume_T/0/1/0/all/0/1\">Tristan Hume</a>, et al. (11 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mind the Knowledge Gap: A Survey of Knowledge-enhanced Dialogue Systems. (arXiv:2212.09252v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09252","description":"<p>Many dialogue systems (DSs) lack characteristics humans have, such as emotion\nperception, factuality, and informativeness. Enhancing DSs with knowledge\nalleviates this problem, but, as many ways of doing so exist, keeping track of\nall proposed methods is difficult. Here, we present the first survey of\nknowledge-enhanced DSs. We define three categories of systems - internal,\nexternal, and hybrid - based on the knowledge they use. We survey the\nmotivation for enhancing DSs with knowledge, used datasets, and methods for\nknowledge search, knowledge encoding, and knowledge incorporation. Finally, we\npropose how to improve existing systems based on theories from linguistics and\ncognitive science.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaier_S/0/1/0/all/0/1\">Sagi Shaier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hunter_L/0/1/0/all/0/1\">Lawrence Hunter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kann_K/0/1/0/all/0/1\">Katharina Kann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven Optimization. (arXiv:2212.09254v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09254","description":"<p>Robustness evaluation against adversarial examples has become increasingly\nimportant to unveil the trustworthiness of the prevailing deep models in\nnatural language processing (NLP). However, in contrast to the computer vision\ndomain where the first-order projected gradient descent (PGD) is used as the\nbenchmark approach to generate adversarial examples for robustness evaluation,\nthere lacks a principled first-order gradient-based robustness evaluation\nframework in NLP. The emerging optimization challenges lie in 1) the discrete\nnature of textual inputs together with the strong coupling between the\nperturbation location and the actual content, and 2) the additional constraint\nthat the perturbed text should be fluent and achieve a low perplexity under a\nlanguage model. These challenges make the development of PGD-like NLP attacks\ndifficult. To bridge the gap, we propose TextGrad, a new attack generator using\ngradient-driven optimization, supporting high-accuracy and high-quality\nassessment of adversarial robustness in NLP. Specifically, we address the\naforementioned challenges in a unified optimization framework. And we develop\nan effective convex relaxation method to co-optimize the continuously-relaxed\nsite selection and perturbation variables and leverage an effective sampling\nmethod to establish an accurate mapping from the continuous optimization\nvariables to the discrete textual perturbations. Moreover, as a first-order\nattack generation method, TextGrad can be baked into adversarial training to\nfurther improve the robustness of NLP models. Extensive experiments are\nprovided to demonstrate the effectiveness of TextGrad not only in attack\ngeneration for robustness evaluation but also in adversarial defense.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1\">Bairu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jinghan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yihua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guanhua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi hash embeddings in spaCy. (arXiv:2212.09255v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09255","description":"<p>The distributed representation of symbols is one of the key technologies in\nmachine learning systems today, playing a pivotal role in modern natural\nlanguage processing. Traditional word embeddings associate a separate vector\nwith each word. While this approach is simple and leads to good performance, it\nrequires a lot of memory for representing a large vocabulary. To reduce the\nmemory footprint, the default embedding layer in spaCy is a hash embeddings\nlayer. It is a stochastic approximation of traditional embeddings that provides\nunique vectors for a large number of words without explicitly storing a\nseparate vector for each of them. To be able to compute meaningful\nrepresentations for both known and unknown words, hash embeddings represent\neach word as a summary of the normalized word form, subword information and\nword shape. Together, these features produce a multi-embedding of a word. In\nthis technical report we lay out a bit of history and introduce the embedding\nmethods in spaCy in detail. Second, we critically evaluate the hash embedding\narchitecture with multi-embeddings on Named Entity Recognition datasets from a\nvariety of domains and languages. The experiments validate most key design\nchoices behind spaCy's embedders, but we also uncover a few surprising results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miranda_L/0/1/0/all/0/1\">Lester James Miranda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadar_A/0/1/0/all/0/1\">&#xc1;kos K&#xe1;d&#xe1;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_A/0/1/0/all/0/1\">Adriane Boyd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landeghem_S/0/1/0/all/0/1\">Sofie Van Landeghem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honnibal_M/0/1/0/all/0/1\">Matthew Honnibal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptBoosting: Black-Box Text Classification with Ten Forward Passes. (arXiv:2212.09257v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09257","description":"<p>We describe PromptBoosting, a query-efficient procedure for building a text\nclassifier from a neural language model (LM) without access to the LM's\nparameters, gradients, or hidden representations. This form of \"black-box\"\nclassifier training has become increasingly important as the cost of training\nand inference in large-scale LMs grows. But existing black-box LM classifier\nlearning approaches are themselves computationally inefficient, typically\nspecializing LMs to the target task by searching in a large space of (discrete\nor continuous) prompts using zeroth-order optimization methods. Instead of\ndirectly optimizing in prompt space, PromptBoosting obtains a small pool of\nprompts via a gradient-free approach and then constructs a large pool of weak\nlearners by pairing these prompts with different elements of the LM's output\ndistribution. These weak learners are then ensembled using the AdaBoost\nalgorithm. The entire learning process requires only a small number of forward\npasses and no backward pass. Experiments show that PromptBoosting achieves\nstate-of-the-art performance in multiple black-box few-shot classification\ntasks, and matches or outperforms full fine-tuning in both few-shot and\nstandard learning paradigms, while training 10x faster than existing black-box\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1\">Bairu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnor_J/0/1/0/all/0/1\">Joe O&#x27;Connor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Statistical Dataset Evaluation: Reliability, Difficulty, and Validity. (arXiv:2212.09272v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09272","description":"<p>Datasets serve as crucial training resources and model performance trackers.\nHowever, existing datasets have exposed a plethora of problems, inducing biased\nmodels and unreliable evaluation results. In this paper, we propose a\nmodel-agnostic dataset evaluation framework for automatic dataset quality\nevaluation. We seek the statistical properties of the datasets and address\nthree fundamental dimensions: reliability, difficulty, and validity, following\na classical testing theory. Taking the Named Entity Recognition (NER) datasets\nas a case study, we introduce $9$ statistical metrics for a statistical dataset\nevaluation framework. Experimental results and human evaluation validate that\nour evaluation framework effectively assesses various aspects of the dataset\nquality. Furthermore, we study how the dataset scores on our statistical\nmetrics affect the model performance, and appeal for dataset quality evaluation\nor targeted dataset improvement before training or testing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengwen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qingxiu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaochen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haitao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MIGA: A Unified Multi-task Generation Framework for Conversational Text-to-SQL. (arXiv:2212.09278v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09278","description":"<p>Conversational text-to-SQL is designed to translate multi-turn natural\nlanguage questions into their corresponding SQL queries. Most state-of-the-art\nconversational text- to-SQL methods are incompatible with generative\npre-trained language models (PLMs), such as T5. In this paper, we present a\ntwo-stage unified MultI-task Generation frAmework (MIGA) that leverages PLMs'\nability to tackle conversational text-to-SQL. In the pre-training stage, MIGA\nfirst decomposes the main task into several related sub-tasks and then unifies\nthem into the same sequence-to-sequence (Seq2Seq) paradigm with task-specific\nnatural language prompts to boost the main task from multi-task training. Later\nin the fine-tuning stage, we propose four SQL perturbations to alleviate the\nerror propagation problem. MIGA tends to achieve state-of-the-art performance\non two benchmarks (SparC and CoSQL). We also provide extensive analyses and\ndiscussions to shed light on some new perspectives for conversational\ntext-to-SQL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yingwen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_W/0/1/0/all/0/1\">Wenjie Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yue Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning. (arXiv:2212.09282v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09282","description":"<p>Logical reasoning of text is an important ability that requires understanding\nthe information present in the text, their interconnections, and then reasoning\nthrough them to infer new conclusions. Prior works on improving the logical\nreasoning ability of language models require complex processing of training\ndata (e.g., aligning symbolic knowledge to text), yielding task-specific data\naugmentation solutions that restrict the learning of general logical reasoning\nskills. In this work, we propose APOLLO, an adaptively pretrained language\nmodel that has improved logical reasoning abilities. We select a subset of\nWikipedia, based on a set of logical inference keywords, for continued\npretraining of a language model. We use two self-supervised loss functions: a\nmodified masked language modeling loss where only specific parts-of-speech\nwords, that would likely require more reasoning than basic language\nunderstanding, are masked, and a sentence-level classification loss that\nteaches the model to distinguish between entailment and contradiction types of\nsentences. The proposed training paradigm is both simple and independent of\ntask formats. We demonstrate the effectiveness of APOLLO by comparing it with\nprior baselines on two logical reasoning datasets. APOLLO performs comparably\non ReClor and outperforms baselines on LogiQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1\">Soumya Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1\">Reid Pryzant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Investigation of Indian Native Language Phonemic Influences on L2 English Pronunciations. (arXiv:2212.09284v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09284","description":"<p>Speech systems are sensitive to accent variations. This is especially\nchallenging in the Indian context, with an abundance of languages but a dearth\nof linguistic studies characterising pronunciation variations. The growing\nnumber of L2 English speakers in India reinforces the need to study accents and\nL1-L2 interactions. We investigate the accents of Indian English (IE) speakers\nand report in detail our observations, both specific and common to all regions.\nIn particular, we observe the phonemic variations and phonotactics occurring in\nthe speakers' native languages and apply this to their English pronunciations.\nWe demonstrate the influence of 18 Indian languages on IE by comparing the\nnative language pronunciations with IE pronunciations obtained jointly from\nexisting literature studies and phonetically annotated speech of 80 speakers.\nConsequently, we are able to validate the intuitions of Indian language\ninfluences on IE pronunciations by justifying pronunciation rules from the\nperspective of Indian language phonology. We obtain a comprehensive description\nin terms of universal and region-specific characteristics of IE, which\nfacilitates accent conversion and adaptation of existing ASR and TTS systems to\ndifferent Indian accents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Shelly Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_P/0/1/0/all/0/1\">Priyanshi Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuppala_A/0/1/0/all/0/1\">Anil Vuppala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_P/0/1/0/all/0/1\">Prasanta Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yarra_C/0/1/0/all/0/1\">Chiranjeevi Yarra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT: The End of Online Exam Integrity?. (arXiv:2212.09292v1 [cs.AI])","link":"http://arxiv.org/abs/2212.09292","description":"<p>This study evaluated the ability of ChatGPT, a recently developed artificial\nintelligence (AI) agent, to perform high-level cognitive tasks and produce text\nthat is indistinguishable from human-generated text. This capacity raises\nconcerns about the potential use of ChatGPT as a tool for academic misconduct\nin online exams. The study found that ChatGPT is capable of exhibiting critical\nthinking skills and generating highly realistic text with minimal input, making\nit a potential threat to the integrity of online exams, particularly in\ntertiary education settings where such exams are becoming more prevalent.\nReturning to invigilated and oral exams could form part of the solution, while\nusing advanced proctoring techniques and AI-text output detectors may be\neffective in addressing this issue, they are not likely to be foolproof\nsolutions. Further research is needed to fully understand the implications of\nlarge language models like ChatGPT and to devise strategies for combating the\nrisk of cheating using these tools. It is crucial for educators and\ninstitutions to be aware of the possibility of ChatGPT being used for cheating\nand to investigate measures to address it in order to maintain the fairness and\nvalidity of online exams for all students.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Susnjak_T/0/1/0/all/0/1\">Teo Susnjak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SEScore2: Retrieval Augmented Pretraining for Text Generation Evaluation. (arXiv:2212.09305v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09305","description":"<p>Is it possible to leverage large scale raw and raw parallel corpora to build\na general learned metric? Existing learned metrics have gaps to human\njudgements, are model-dependent or are limited to the domains or tasks where\nhuman ratings are available. In this paper, we propose SEScore2, a model-based\nmetric pretrained over million-scale synthetic dataset constructed by our novel\nretrieval augmented data synthesis pipeline. SEScore2 achieves high correlation\nto human judgements without any human rating supervisions. Importantly, our\nunsupervised SEScore2 can outperform supervised metrics, which are trained on\nthe News human ratings, at the TED domain. We evaluate SEScore2 over four text\ngeneration tasks across three languages. SEScore2 outperforms all prior\nunsupervised evaluation metrics in machine translation, speech translation,\ndata-to-text and dialogue generation, with average Kendall improvements 0.158.\nSEScore2 even outperforms SOTA supervised BLEURT at data-to-text, dialogue\ngeneration and overall correlation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenda Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xian Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E-NER -- An Annotated Named Entity Recognition Corpus of Legal Text. (arXiv:2212.09306v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09306","description":"<p>Identifying named entities such as a person, location or organization, in\ndocuments can highlight key information to readers. Training Named Entity\nRecognition (NER) models requires an annotated data set, which can be a\ntime-consuming labour-intensive task. Nevertheless, there are publicly\navailable NER data sets for general English. Recently there has been interest\nin developing NER for legal text. However, prior work and experimental results\nreported here indicate that there is a significant degradation in performance\nwhen NER methods trained on a general English data set are applied to legal\ntext. We describe a publicly available legal NER data set, called E-NER, based\non legal company filings available from the US Securities and Exchange\nCommission's EDGAR data set. Training a number of different NER algorithms on\nthe general English CoNLL-2003 corpus but testing on our test collection\nconfirmed significant degradations in accuracy, as measured by the F1-score, of\nbetween 29.4\\% and 60.4\\%, compared to training and testing on the E-NER\ncollection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Au_T/0/1/0/all/0/1\">Ting Wai Terence Au</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cox_I/0/1/0/all/0/1\">Ingemar J. Cox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampos_V/0/1/0/all/0/1\">Vasileios Lampos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging The Gap: Entailment Fused-T5 for Open-retrieval Conversational Machine Reading Comprehension. (arXiv:2212.09353v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09353","description":"<p>Open-retrieval conversational machine reading comprehension (OCMRC) simulates\nreal-life conversational interaction scenes. Machines are required to make a\ndecision of \"Yes/No/Inquire\" or generate a follow-up question when the decision\nis \"Inquire\" based on retrieved rule texts, user scenario, user question, and\ndialogue history. Recent studies explored the methods to reduce the information\ngap between decision-making and question generation and thus improve the\nperformance of generation. However, the information gap still exists because\nthese pipeline structures are still limited in decision-making, span\nextraction, and question rephrasing three stages. Decision-making and\ngeneration are reasoning separately, and the entailment reasoning utilized in\ndecision-making is hard to share through all stages. To tackle the above\nproblem, we proposed a novel one-stage end-to-end framework, called Entailment\nFused-T5 (EFT), to bridge the information gap between decision-making and\ngeneration in a global understanding manner. The extensive experimental results\ndemonstrate that our proposed framework achieves new state-of-the-art\nperformance on the OR-ShARC benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zewen Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WACO: Word-Aligned Contrastive Learning for Speech Translation. (arXiv:2212.09359v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09359","description":"<p>End-to-end Speech Translation (E2E ST) aims to translate source speech into\ntarget translation without generating the intermediate transcript. However,\nexisting approaches for E2E ST degrade considerably when only limited ST data\nare available. We observe that an ST model's performance strongly correlates\nwith its embedding similarity from speech and transcript. In this paper, we\npropose Word-Aligned COntrastive learning (WACO), a novel method for few-shot\nspeech-to-text translation. Our key idea is bridging word-level representations\nfor both modalities via contrastive learning. We evaluate WACO and other\nmethods on the MuST-C dataset, a widely used ST benchmark. Our experiments\ndemonstrate that WACO outperforms the best baseline methods by 0.7-8.5 BLEU\npoints with only 1-hour parallel data. Code is available at\nhttps://anonymous.4open.science/r/WACO .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1\">Siqi Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1\">Rong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enriching Relation Extraction with OpenIE. (arXiv:2212.09376v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09376","description":"<p>Relation extraction (RE) is a sub-discipline of information extraction (IE)\nwhich focuses on the prediction of a relational predicate from a\nnatural-language input unit (such as a sentence, a clause, or even a short\nparagraph consisting of multiple sentences and/or clauses). Together with\nnamed-entity recognition (NER) and disambiguation (NED), RE forms the basis for\nmany advanced IE tasks such as knowledge-base (KB) population and verification.\nIn this work, we explore how recent approaches for open information extraction\n(OpenIE) may help to improve the task of RE by encoding structured information\nabout the sentences' principal units, such as subjects, objects, verbal\nphrases, and adverbials, into various forms of vectorized (and hence\nunstructured) representations of the sentences. Our main conjecture is that the\ndecomposition of long and possibly convoluted sentences into multiple smaller\nclauses via OpenIE even helps to fine-tune context-sensitive language models\nsuch as BERT (and its plethora of variants) for RE. Our experiments over two\nannotated corpora, KnowledgeNet and FewRel, demonstrate the improved accuracy\nof our enriched models compared to existing RE approaches. Our best results\nreach 92% and 71% of F1 score for KnowledgeNet and FewRel, respectively,\nproving the effectiveness of our approach on competitive benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Temperoni_A/0/1/0/all/0/1\">Alessandro Temperoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biryukov_M/0/1/0/all/0/1\">Maria Biryukov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobald_M/0/1/0/all/0/1\">Martin Theobald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt Gating: A Parameter Efficient Tuning Method for Zero-Shot Multi-Source Translation. (arXiv:2212.09387v1 [cs.CL])","link":"http://arxiv.org/abs/2212.09387","description":"<p>Multi-source translation (MST), which typically receives multiple source\nsentences of the same meaning in different languages, has been shown superior\nto single-source translation. As the quantity of multi-source parallel data is\nlimited, taking full advantage of single-source data and limited multi-source\ndata to make models perform well when receiving as many as possible sources\nremains a challenge. Unlike previous work mostly devoted to supervised\nscenarios, we focus on zero-shot MST: expecting models to be able to process\nunseen combinations of multiple sources, e.g., unseen language combinations,\nduring inference. We propose a simple yet effective parameter efficient method,\nnamed Prompt Gating, which appends prompts to the model inputs and attaches\ngates on the extended hidden states for each encoder layer. It shows strong\nzero-shot transferability (+9.0 BLEU points maximally) and remarkable\ncompositionality (+15.6 BLEU points maximally) on MST, and also shows its\nsuperiorities over baselines on lexically constrained translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuancheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zijun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-sense embeddings through a word sense disambiguation process. (arXiv:2101.08700v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.08700","description":"<p>Natural Language Understanding has seen an increasing number of publications\nin the last few years, especially after robust word embeddings models became\nprominent, when they proved themselves able to capture and represent semantic\nrelationships from massive amounts of data. Nevertheless, traditional models\noften fall short in intrinsic issues of linguistics, such as polysemy and\nhomonymy. Any expert system that makes use of natural language in its core, can\nbe affected by a weak semantic representation of text, resulting in inaccurate\noutcomes based on poor decisions. To mitigate such issues, we propose a novel\napproach called Most Suitable Sense Annotation (MSSA), that disambiguates and\nannotates each word by its specific sense, considering the semantic effects of\nits context. Our approach brings three main contributions to the semantic\nrepresentation scenario: (i) an unsupervised technique that disambiguates and\nannotates words by their senses, (ii) a multi-sense embeddings model that can\nbe extended to any traditional word embeddings algorithm, and (iii) a recurrent\nmethodology that allows our models to be re-used and their representations\nrefined. We test our approach on six different benchmarks for the word\nsimilarity task, showing that our approach can produce state-of-the-art results\nand outperforms several more complex state-of-the-art systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosky_W/0/1/0/all/0/1\">William Grosky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aizawa_A/0/1/0/all/0/1\">Akiko Aizawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced word embeddings using multi-semantic representation through lexical chains. (arXiv:2101.09023v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.09023","description":"<p>The relationship between words in a sentence often tells us more about the\nunderlying semantic content of a document than its actual words, individually.\nIn this work, we propose two novel algorithms, called Flexible Lexical Chain II\nand Fixed Lexical Chain II. These algorithms combine the semantic relations\nderived from lexical chains, prior knowledge from lexical databases, and the\nrobustness of the distributional hypothesis in word embeddings as building\nblocks forming a single system. In short, our approach has three main\ncontributions: (i) a set of techniques that fully integrate word embeddings and\nlexical chains; (ii) a more robust semantic representation that considers the\nlatent relation between words in a document; and (iii) lightweight word\nembeddings models that can be extended to any natural language task. We intend\nto assess the knowledge of pre-trained models to evaluate their robustness in\nthe document classification task. The proposed techniques are tested against\nseven word embeddings algorithms using five different machine learning\nclassifiers over six scenarios in the document classification task. Our results\nshow the integration between lexical chains and word embeddings representations\nsustain state-of-the-art results, even against more complex systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_C/0/1/0/all/0/1\">Charles Henrique Porto Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosky_W/0/1/0/all/0/1\">William Grosky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franca_F/0/1/0/all/0/1\">Fabr&#xed;cio Olivetti de Fran&#xe7;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medeiros_D/0/1/0/all/0/1\">D&#xe9;bora Maria Rossi Medeiros</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Additive Compositionality: AND, OR and NOT Operations with Word Embeddings. (arXiv:2105.08585v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.08585","description":"<p>It is well-known that typical word embedding methods such as Word2Vec and\nGloVe have the property that the meaning can be composed by adding up the\nembeddings (additive compositionality). Several theories have been proposed to\nexplain additive compositionality, but the following questions remain\nunanswered: (Q1) The assumptions of those theories do not hold for the\npractical word embedding. (Q2) Ordinary additive compositionality can be seen\nas an AND operation of word meanings, but it is not well understood how other\noperations, such as OR and NOT, can be computed by the embeddings. We address\nthese issues by the idea of frequency-weighted centering at its core. This\npaper proposes a post-processing method for bridging the gap between practical\nword embedding and the assumption of theory about additive compositionality as\nan answer to (Q1). It also gives a method for taking OR or NOT of the meaning\nby linear operation of word embedding as an answer to (Q2). Moreover, we\nconfirm experimentally that the accuracy of AND operation, i.e., the ordinary\nadditive compositionality, can be improved by our post-processing method (3.5x\nimprovement in top-100 accuracy) and that OR and NOT operations can be\nperformed correctly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naito_M/0/1/0/all/0/1\">Masahiro Naito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokoi_S/0/1/0/all/0/1\">Sho Yokoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Geewook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimodaira_H/0/1/0/all/0/1\">Hidetoshi Shimodaira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Heterogeneous Graph Attention Network for Multi-hop Machine Reading Comprehension. (arXiv:2107.00841v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.00841","description":"<p>Multi-hop machine reading comprehension is a challenging task in natural\nlanguage processing, which requires more reasoning ability across multiple\ndocuments. Spectral models based on graph convolutional networks grant\ninferring abilities and lead to competitive results. However, part of them\nstill faces the challenge of analyzing the reasoning in a human-understandable\nway. Inspired by the concept of the Grandmother Cells in cognitive\nneuroscience, a spatial graph attention framework named ClueReader was proposed\nin this paper, imitating the procedure. This model is designed to assemble the\nsemantic features in multi-level representations and automatically concentrate\nor alleviate information for reasoning via the attention mechanism. The name\nClueReader is a metaphor for the pattern of the model: regard the subjects of\nqueries as the start points of clues, take the reasoning entities as bridge\npoints, consider the latent candidate entities as the grandmother cells, and\nthe clues end up in candidate entities. The proposed model allows us to\nvisualize the reasoning graph, then analyze the importance of edges connecting\ntwo entities and the selectivity in the mention and candidate nodes, which can\nbe easier to be comprehended empirically. The official evaluations in the\nopen-domain multi-hop reading dataset WikiHop and the Drug-drug Interactions\ndataset MedHop prove the validity of our approach and show the probability of\nthe application of the model in the molecular biology domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Feng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jian-Cheng Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujita_H/0/1/0/all/0/1\">Hamido Fujita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual Open-Domain Question Answering with Answer Sentence Generation. (arXiv:2110.07150v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07150","description":"<p>Open-Domain Generative Question Answering has achieved impressive performance\nin English by combining document-level retrieval with answer generation. These\napproaches, which we refer to as GenQA, can generate complete sentences,\neffectively answering both factoid and non-factoid questions. In this paper, we\nextend GenQA to the multilingual and cross-lingual settings. For this purpose,\nwe first introduce GenTyDiQA, an extension of the TyDiQA dataset with\nwell-formed and complete answers for Arabic, Bengali, English, Japanese, and\nRussian. Based on GenTyDiQA, we design a cross-lingual generative model that\nproduces full-sentence answers by exploiting passages written in multiple\nlanguages, including languages different from the question. Our cross-lingual\ngenerative system outperforms answer sentence selection baselines for all 5\nlanguages and monolingual generative pipelines for three out of five languages\nstudied.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muller_B/0/1/0/all/0/1\">Benjamin Muller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldaini_L/0/1/0/all/0/1\">Luca Soldaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koncel_Kedziorski_R/0/1/0/all/0/1\">Rik Koncel-Kedziorski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lind_E/0/1/0/all/0/1\">Eric Lind</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moschitti_A/0/1/0/all/0/1\">Alessandro Moschitti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClimateBert: A Pretrained Language Model for Climate-Related Text. (arXiv:2110.12010v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.12010","description":"<p>Over the recent years, large pretrained language models (LM) have\nrevolutionized the field of natural language processing (NLP). However, while\npretraining on general language has been shown to work very well for common\nlanguage, it has been observed that niche language poses problems. In\nparticular, climate-related texts include specific language that common LMs can\nnot represent accurately. We argue that this shortcoming of today's LMs limits\nthe applicability of modern NLP to the broad field of text processing of\nclimate-related texts. As a remedy, we propose CLIMATEBERT, a transformer-based\nlanguage model that is further pretrained on over 2 million paragraphs of\nclimate-related texts, crawled from various sources such as common news,\nresearch articles, and climate reporting of companies. We find that CLIMATEBERT\nleads to a 48% improvement on a masked language model objective which, in turn,\nleads to lowering error rates by 3.57% to 35.71% for various climate-related\ndownstream tasks like text classification, sentiment analysis, and\nfact-checking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Webersinke_N/0/1/0/all/0/1\">Nicolas Webersinke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraus_M/0/1/0/all/0/1\">Mathias Kraus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bingler_J/0/1/0/all/0/1\">Julia Anna Bingler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leippold_M/0/1/0/all/0/1\">Markus Leippold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models. (arXiv:2201.05337v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05337","description":"<p>Controllable Text Generation (CTG) is emerging area in the field of natural\nlanguage generation (NLG). It is regarded as crucial for the development of\nadvanced text generation technologies that are more natural and better meet the\nspecific constraints in practical applications. In recent years, methods using\nlarge-scale pre-trained language models (PLMs), in particular the widely used\ntransformer-based PLMs, have become a new paradigm of NLG, allowing generation\nof more diverse and fluent text. However, due to the lower level of\ninterpretability of deep neural networks, the controllability of these methods\nneed to be guaranteed. To this end, controllable text generation using\ntransformer-based PLMs has become a rapidly growing yet challenging new\nresearch hotspot. A diverse range of approaches have emerged in the recent 3-4\nyears, targeting different CTG tasks which may require different types of\ncontrolled constraints. In this paper, we present a systematic critical review\non the common tasks, main approaches and evaluation methods in this area.\nFinally, we discuss the challenges that the field is facing, and put forward\nvarious promising future directions. To the best of our knowledge, this is the\nfirst survey paper to summarize CTG techniques from the perspective of PLMs. We\nhope it can help researchers in related fields to quickly track the academic\nfrontier, providing them with a landscape of the area and a roadmap for future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Haolin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaoyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawei Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal Knowledge Graph Construction and Application: A Survey. (arXiv:2202.05786v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2202.05786","description":"<p>Recent years have witnessed the resurgence of knowledge engineering which is\nfeatured by the fast growth of knowledge graphs. However, most of existing\nknowledge graphs are represented with pure symbols, which hurts the machine's\ncapability to understand the real world. The multi-modalization of knowledge\ngraphs is an inevitable key step towards the realization of human-level machine\nintelligence. The results of this endeavor are Multi-modal Knowledge Graphs\n(MMKGs). In this survey on MMKGs constructed by texts and images, we first give\ndefinitions of MMKGs, followed with the preliminaries on multi-modal tasks and\ntechniques. We then systematically review the challenges, progresses and\nopportunities on the construction and application of MMKGs respectively, with\ndetailed analyses of the strength and weakness of different solutions. We\nfinalize this survey with open research problems relevant to MMKGs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiangru Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaodan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xueyao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Penglei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuwu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_N/0/1/0/all/0/1\">Nicholas Jing Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GatorTron: A Large Clinical Language Model to Unlock Patient Information from Unstructured Electronic Health Records. (arXiv:2203.03540v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.03540","description":"<p>There is an increasing interest in developing artificial intelligence (AI)\nsystems to process and interpret electronic health records (EHRs). Natural\nlanguage processing (NLP) powered by pretrained language models is the key\ntechnology for medical AI systems utilizing clinical narratives. However, there\nare few clinical language models, the largest of which trained in the clinical\ndomain is comparatively small at 110 million parameters (compared with billions\nof parameters in the general domain). It is not clear how large clinical\nlanguage models with billions of parameters can help medical AI systems utilize\nunstructured EHRs. In this study, we develop from scratch a large clinical\nlanguage model - GatorTron - using &gt;90 billion words of text (including &gt;82\nbillion words of de-identified clinical text) and systematically evaluate it on\n5 clinical NLP tasks including clinical concept extraction, medical relation\nextraction, semantic textual similarity, natural language inference (NLI), and\nmedical question answering (MQA). We examine how (1) scaling up the number of\nparameters and (2) scaling up the size of the training data could benefit these\nNLP tasks. GatorTron models scale up the clinical language model from 110\nmillion to 8.9 billion parameters and improve 5 clinical NLP tasks (e.g., 9.6%\nand 9.5% improvement in accuracy for NLI and MQA), which can be applied to\nmedical AI systems to improve healthcare delivery. The GatorTron models are\npublicly available at:\nhttps://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/models/gatortron_og.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Aokun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+PourNejatian_N/0/1/0/all/0/1\">Nima PourNejatian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_H/0/1/0/all/0/1\">Hoo Chang Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kaleb E Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parisien_C/0/1/0/all/0/1\">Christopher Parisien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Compas_C/0/1/0/all/0/1\">Colin Compas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_C/0/1/0/all/0/1\">Cheryl Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flores_M/0/1/0/all/0/1\">Mona G Flores</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magoc_T/0/1/0/all/0/1\">Tanja Magoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harle_C/0/1/0/all/0/1\">Christopher A Harle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipori_G/0/1/0/all/0/1\">Gloria Lipori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_D/0/1/0/all/0/1\">Duane A Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hogan_W/0/1/0/all/0/1\">William R Hogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenkman_E/0/1/0/all/0/1\">Elizabeth A Shenkman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MVP: Multi-task Supervised Pre-training for Natural Language Generation. (arXiv:2206.12131v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.12131","description":"<p>Pre-trained language models (PLMs) have achieved remarkable success in\nnatural language generation (NLG) tasks. Up to now, most NLG-oriented PLMs are\npre-trained in an unsupervised manner using the large-scale general corpus. In\nthe meanwhile, an increasing number of models pre-trained with labeled data\n(i.e., ``supervised pre-training'') showcase superior performance compared to\nunsupervised pre-trained models. Motivated by the success of supervised\npre-training, we propose Multi-task superVised Pre-training~(MVP) for natural\nlanguage generation. We collect a large-scale natural language generation\ncorpus, MVPCorpus, from $77$ datasets over $11$ diverse NLG tasks. Then we\nunify these examples into a general text-to-text format to pre-train the text\ngeneration model MVP in a supervised manner. For each task, we further\npre-train specific soft prompts to stimulate the model's capacity to perform a\nspecific task. Extensive experiments have demonstrated the effectiveness and\ngenerality of our MVP model in a number of NLG tasks, which achieves\nstate-of-the-art performance on $13$ out of $17$ datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instruction-driven history-aware policies for robotic manipulations. (arXiv:2209.04899v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2209.04899","description":"<p>In human environments, robots are expected to accomplish a variety of\nmanipulation tasks given simple natural language instructions. Yet, robotic\nmanipulation is extremely challenging as it requires fine-grained motor\ncontrol, long-term memory as well as generalization to previously unseen tasks\nand environments. To address these challenges, we propose a unified\ntransformer-based approach that takes into account multiple inputs. In\nparticular, our transformer architecture integrates (i) natural language\ninstructions and (ii) multi-view scene observations while (iii) keeping track\nof the full history of observations and actions. Such an approach enables\nlearning dependencies between history and instructions and improves\nmanipulation precision using multiple views. We evaluate our method on the\nchallenging RLBench benchmark and on a real-world robot. Notably, our approach\nscales to 74 diverse RLBench tasks and outperforms the state of the art. We\nalso address instruction-conditioned tasks and demonstrate excellent\ngeneralization to previously unseen variations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guhur_P/0/1/0/all/0/1\">Pierre-Louis Guhur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_R/0/1/0/all/0/1\">Ricardo Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tapaswi_M/0/1/0/all/0/1\">Makarand Tapaswi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Finetuning for Robust Continual Multilingual Learning. (arXiv:2209.06767v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.06767","description":"<p>We study the underexplored problem of Continual Multilingual Learning, where\na multilingual model, already trained on task-specific data from all supported\nlanguages, is continually updated using batches of new multilingual training\ndata for the same task. We show that naively updating the multilingual model\ncan lead to losses in performance over a subset of languages although the\naggregated performance metric shows an improvement. We establish this\nphenomenon over four tasks belonging to three task families (token-level,\nsentence-level and seq2seq). We then build upon recent advances in\nparameter-efficient finetuning to develop novel finetuning strategies that\nallow us to jointly minimize language-specific forgetting while encouraging\npositive cross-lingual transfer observed in this setup. Our proposed pipeline,\nLAFT-URIEL, improves the spread of gains over the supported languages while\nreducing the magnitude of language-specific losses incurred.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Badola_K/0/1/0/all/0/1\">Kartikeya Badola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dave_S/0/1/0/all/0/1\">Shachi Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Unlearning for Mitigating Privacy Risks in Language Models. (arXiv:2210.01504v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.01504","description":"<p>Pretrained Language Models (LMs) memorize a vast amount of knowledge during\ninitial pretraining, including information that may violate the privacy of\npersonal lives and identities. Previous work addressing privacy issues for\nlanguage models has mostly focused on data preprocessing and differential\nprivacy methods, both requiring re-training the underlying LM. We propose\nknowledge unlearning as an alternative method to reduce privacy risks for LMs\npost hoc. We show that simply performing gradient ascent on target token\nsequences is effective at forgetting them with little to no degradation of\ngeneral language modeling performances for larger LMs; it sometimes even\nsubstantially improves the underlying LM with just a few iterations. We also\nfind that sequential unlearning is better than trying to unlearn all the data\nat once and that unlearning is highly dependent on which kind of data (domain)\nis forgotten. By showing comparisons with a previous data preprocessing method\nand a decoding method known to mitigate privacy risks for LMs, we show that\nunlearning can give a stronger empirical privacy guarantee in scenarios where\nthe data vulnerable to extraction attacks are known a priori while being much\nmore efficient and robust. We release the code and dataset needed to replicate\nour results at https://github.com/joeljang/knowledge-unlearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Joel Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_D/0/1/0/all/0/1\">Dongkeun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sohee Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1\">Sungmin Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Moontae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Logeswaran_L/0/1/0/all/0/1\">Lajanugen Logeswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Prompt Tuning for Text Summarization. (arXiv:2211.01837v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.01837","description":"<p>Prompts with different control signals (e.g., length, keywords, etc.) can be\nused to control text summarization. When control signals are available, they\ncan control the properties of generated summaries and potentially improve\nsummarization quality (since more information are given). Unfortunately,\ncontrol signals are not already available during inference time. In this paper,\nwe propose Lotus (shorthand for Latent Prompt Tuning for Summarization), which\nis a single model that can be applied in both controlled and uncontrolled\n(without control signals) modes. During training, Lotus learns latent prompt\nrepresentations from prompts with gold control signals using a contrastive\nlearning objective. Experiments show Lotus in uncontrolled mode consistently\nimproves upon strong (uncontrollable) summarization models across four\ndifferent summarization datasets. We also demonstrate generated summaries can\nbe controlled using prompts with user specified control tokens.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yubo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si-qing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LMentry: A Language Model Benchmark of Elementary Language Tasks. (arXiv:2211.02069v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.02069","description":"<p>As the performance of large language models rapidly improves, benchmarks are\ngetting larger and more complex as well. We present LMentry, a benchmark that\navoids this \"arms race\" by focusing on a compact set of tasks that are trivial\nto humans, e.g. writing a sentence containing a specific word, identifying\nwhich words in a list belong to a specific category, or choosing which of two\nwords is longer. LMentry is specifically designed to provide quick and\ninterpretable insights into the capabilities and robustness of large language\nmodels. Our experiments reveal a wide variety of failure cases that, while\nimmediately obvious to humans, pose a considerable challenge for large language\nmodels, including OpenAI's latest 175B-parameter instruction-tuned model,\nTextDavinci002. LMentry complements contemporary evaluation approaches of large\nlanguage models, providing a quick, automatic, and easy-to-run \"unit test\",\nwithout resorting to large benchmark suites of complex tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Efrat_A/0/1/0/all/0/1\">Avia Efrat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honovich_O/0/1/0/all/0/1\">Or Honovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reranking Overgenerated Responses for End-to-End Task-Oriented Dialogue Systems. (arXiv:2211.03648v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.03648","description":"<p>End-to-end (E2E) task-oriented dialogue (ToD) systems are prone to fall into\nthe so-called \"likelihood trap\", resulting in generated responses which are\ndull, repetitive, and often inconsistent with dialogue history. Comparing\nranked lists of multiple generated responses against the \"gold response\" (from\nevaluation data) reveals a wide diversity in response quality, with many good\nresponses placed lower in the ranked list. The main challenge, addressed in\nthis work, is then how to reach beyond greedily generated system responses,\nthat is, how to obtain and select such high-quality responses from the list of\novergenerated responses at inference without availability of the gold response.\nTo this end, we propose a simple yet effective reranking method which aims to\nselect high-quality items from the lists of responses initially overgenerated\nby the system. The idea is to use any sequence-level (similarity) scoring\nfunction to divide the semantic space of responses into high-scoring versus\nlow-scoring partitions. At training, the high-scoring partition comprises all\ngenerated responses whose similarity to the gold response is higher than the\nsimilarity of the greedy response to the gold response. At inference, the aim\nis to estimate the probability that each overgenerated response belongs to the\nhigh-scoring partition, given only previous dialogue history. We validate the\nrobustness and versatility of our proposed method on the standard MultiWOZ\ndataset: our methods improve a state-of-the-art E2E ToD system by 2.0 BLEU, 1.6\nROUGE, and 1.3 METEOR scores, achieving new peak results. Additional\nexperiments on the BiTOD dataset and human evaluation further ascertain the\ngeneralisability and effectiveness of the proposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What would Harry say? Building Dialogue Agents for Characters in a Story. (arXiv:2211.06869v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.06869","description":"<p>We have a Christmas gift for Harry Potter fans all over the world. In this\npaper, we present Harry Potter Dialogue (HPD), a dataset that helps train Harry\nPotter-like dialogue agents. Such a task is typically viewed as a variant of\npersonalized dialogue agents, but they differ significantly in three respects:\n1) Harry lived in a virtual world of wizards, thus, real-world commonsense may\nnot apply to Harry's conversations; 2) Harry's behavior is strongly linked to\nbackground information in conversations: the scene, its attributes and its\nrelationship to other speakers; and 3) Such backgrounds are dynamically altered\nas the storyline goes on. The HPD dataset, as the first dataset to facilitate\nthe study of dialogue agent construction for characters within a story,\nprovides rich contextual information about each dialogue session such as\nscenes, character attributes, and relations. More importantly, all the\nbackground information will change over the course of the story. In addition,\nHPD could support both dialogue generation and retrieval tasks. We evaluate\nbaselines such as Dialog-GPT and BOB to determine the extent to which they can\ngenerate Harry Potter-like responses. The experimental results disappoint us in\nthat although the generated responses are fluent, they still seem out of\ncharacter for Harry. Besides, we validate the current most robust dialogue\nagent, ChatGPT, which also can't generate plausible Harry-Potter-like responses\nin some cases, either. Our results suggest that there is much scope for future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ziyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting PaLM for Translation: Assessing Strategies and Performance. (arXiv:2211.09102v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.09102","description":"<p>Large language models (LLMs) that have been trained on multilingual but not\nparallel text exhibit a remarkable ability to translate between languages. We\nprobe this ability in an in-depth study of the pathways language model (PaLM),\nwhich has demonstrated the strongest machine translation (MT) performance among\nsimilarly-trained LLMs to date. We investigate various strategies for choosing\ntranslation examples for few-shot prompting, concluding that example quality is\nthe most important factor. Using optimized prompts, we revisit previous\nassessments of PaLM's MT capabilities with more recent test sets, modern MT\nmetrics, and human evaluation, and find that its performance, while impressive,\nstill lags that of state-of-the-art supervised systems. We conclude by\nproviding an analysis of PaLM's MT output which reveals some interesting\nproperties and prospects for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vilar_D/0/1/0/all/0/1\">David Vilar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1\">Markus Freitag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherry_C/0/1/0/all/0/1\">Colin Cherry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiaming Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratnakar_V/0/1/0/all/0/1\">Viresh Ratnakar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_G/0/1/0/all/0/1\">George Foster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniSumm: Unified Few-shot Summarization with Multi-Task Pre-Training and Prefix-Tuning. (arXiv:2211.09783v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.09783","description":"<p>The diverse demands of different summarization tasks and their high\nannotation costs are driving a need for few-shot summarization. However,\ndespite the emergence of many summarization tasks and datasets, the current\ntraining paradigm for few-shot summarization systems ignores potentially\nshareable knowledge in heterogeneous datasets. To this end, we propose\n\\textsc{UniSumm}, a unified few-shot summarization model pre-trained with\nmultiple summarization tasks and can be prefix-tuned to excel at any few-shot\nsummarization datasets. Meanwhile, to better evaluate few-shot summarization\nsystems, under the principles of diversity and robustness, we assemble and\npublicize a new benchmark \\textsc{SummZoo}. It consists of $8$ diverse\nsummarization tasks with multiple sets of few-shot samples for each task,\ncovering both monologue and dialogue domains. Experimental results and ablation\nstudies show that \\textsc{UniSumm} outperforms strong baseline systems by a\nlarge margin across all tasks in \\textsc{SummZoo} under both automatic and\nhuman evaluations. We release our code and benchmark at\n\\url{https://github.com/microsoft/UniSumm}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruochen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context Variance Evaluation of Pretrained Language Models for Prompt-based Biomedical Knowledge Probing. (arXiv:2211.10265v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.10265","description":"<p>Pretrained language models (PLMs) have motivated research on what kinds of\nknowledge these models learn. Fill-in-the-blanks problem (e.g., cloze tests) is\na natural approach for gauging such knowledge. BioLAMA generates prompts for\nbiomedical factual knowledge triples and uses the Top-k accuracy metric to\nevaluate different PLMs' knowledge. However, existing research has shown that\nsuch prompt-based knowledge probing methods can only probe a lower bound of\nknowledge. Many factors like prompt-based probing biases make the LAMA\nbenchmark unreliable and unstable. This problem is more prominent in BioLAMA.\nThe severe long-tailed distribution in vocabulary and large-N-M relation make\nthe performance gap between LAMA and BioLAMA remain notable. To address these,\nwe introduce context variance into the prompt generation and propose a new\nrank-change-based evaluation metric. Different from the previous known-unknown\nevaluation criteria, we propose the concept of \"Misunderstand\" in LAMA for the\nfirst time. Through experiments on 12 PLMs, our context variance prompts and\nUnderstand-Confuse-Misunderstand (UCM) metric makes BioLAMA more friendly to\nlarge-N-M relations and rare relations. We also conducted a set of control\nexperiments to disentangle \"understand\" from just \"read and copy\".\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zonghai Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Refined Semantic Enhancement towards Frequency Diffusion for Video Captioning. (arXiv:2211.15076v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.15076","description":"<p>Video captioning aims to generate natural language sentences that describe\nthe given video accurately. Existing methods obtain favorable generation by\nexploring richer visual representations in encode phase or improving the\ndecoding ability. However, the long-tailed problem hinders these attempts at\nlow-frequency tokens, which rarely occur but carry critical semantics, playing\na vital role in the detailed generation. In this paper, we introduce a novel\nRefined Semantic enhancement method towards Frequency Diffusion (RSFD), a\ncaptioning model that constantly perceives the linguistic representation of the\ninfrequent tokens. Concretely, a Frequency-Aware Diffusion (FAD) module is\nproposed to comprehend the semantics of low-frequency tokens to break through\ngeneration limitations. In this way, the caption is refined by promoting the\nabsorption of tokens with insufficient occurrence. Based on FAD, we design a\nDivergent Semantic Supervisor (DSS) module to compensate for the information\nloss of high-frequency tokens brought by the diffusion process, where the\nsemantics of low-frequency tokens is further emphasized to alleviate the\nlong-tailed problem. Extensive experiments indicate that RSFD outperforms the\nstate-of-the-art methods on two benchmark datasets, i.e., MSR-VTT and MSVD,\ndemonstrate that the enhancement of low-frequency tokens semantics can obtain a\ncompetitive generation effect. Code is available at\nhttps://github.com/lzp870/RSFD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1\">Xian Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zipeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuqin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kui Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Mang Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AIONER: All-in-one scheme-based biomedical named entity recognition using deep learning. (arXiv:2211.16944v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.16944","description":"<p>Biomedical named entity recognition (BioNER) seeks to automatically recognize\nbiomedical entities in natural language text, serving as a necessary foundation\nfor downstream text mining tasks and applications such as information\nextraction and question answering. Manually labeling training data for the\nBioNER task is costly, however, due to the significant domain expertise\nrequired for accurate annotation. The resulting data scarcity causes current\nBioNER approaches to be prone to overfitting, to suffer from limited\ngeneralizability, and to address a single entity type at a time (e.g., gene or\ndisease). We therefore propose a novel all-in-one (AIO) scheme that uses\nexternal data from existing annotated resources to improve generalization. We\nfurther present AIONER, a general-purpose BioNER tool based on cutting-edge\ndeep learning and our AIO schema. We evaluate AIONER on 14 BioNER benchmark\ntasks and show that AIONER is effective, robust, and compares favorably to\nother state-of-the-art approaches such as multi-task learning. We further\ndemonstrate the practical utility of AIONER in three independent tasks to\nrecognize entity types not previously seen in training data, as well as the\nadvantages of AIONER over existing methods for processing biomedical text at a\nlarge scale (e.g., the entire PubMed data).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Ling Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chih-Hsuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_P/0/1/0/all/0/1\">Po-Ting Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leaman_R/0/1/0/all/0/1\">Robert Leaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Glyph Phonetic Information for Chinese Spell Checking: What Works and What's Next. (arXiv:2212.04068v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.04068","description":"<p>While pre-trained Chinese language models have demonstrated impressive\nperformance on a wide range of NLP tasks, the Chinese Spell Checking (CSC) task\nremains a challenge. Previous research has explored using information such as\nglyphs and phonetics to improve the ability to distinguish misspelled\ncharacters, with good results. However, the generalization ability of these\nmodels is not well understood: it is unclear whether they incorporate\nglyph-phonetic information and, if so, whether this information is fully\nutilized. In this paper, we aim to better understand the role of glyph-phonetic\ninformation in the CSC task and suggest directions for improvement.\nAdditionally, we propose a new, more challenging, and practical setting for\ntesting the generalizability of CSC models. All code is made publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaotian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yanjun Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature-Level Debiased Natural Language Understanding. (arXiv:2212.05421v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.05421","description":"<p>Natural language understanding (NLU) models often rely on dataset biases\nrather than intended task-relevant features to achieve high performance on\nspecific datasets. As a result, these models perform poorly on datasets outside\nthe training distribution. Some recent studies address this issue by reducing\nthe weights of biased samples during the training process. However, these\nmethods still encode biased latent features in representations and neglect the\ndynamic nature of bias, which hinders model prediction. We propose an NLU\ndebiasing method, named debiasing contrastive learning (DCT), to simultaneously\nalleviate the above problems based on contrastive learning. We devise a\ndebiasing, positive sampling strategy to mitigate biased latent features by\nselecting the least similar biased positive samples. We also propose a dynamic\nnegative sampling strategy to capture the dynamic influence of biases by\nemploying a bias-only model to dynamically select the most similar biased\nnegative samples. We conduct experiments on three NLU benchmark datasets.\nExperimental results show that DCT outperforms state-of-the-art baselines on\nout-of-distribution datasets while maintaining in-distribution performance. We\nalso verify that DCT can reduce biased latent features from the model's\nrepresentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yougang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yechang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yukun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dawei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Generalization of Pre-trained Language Models via Stochastic Weight Averaging. (arXiv:2212.05956v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.05956","description":"<p>Knowledge Distillation (KD) is a commonly used technique for improving the\ngeneralization of compact Pre-trained Language Models (PLMs) on downstream\ntasks. However, such methods impose the additional burden of training a\nseparate teacher model for every new dataset. Alternatively, one may directly\nwork on the improvement of the optimization procedure of the compact model\ntoward better generalization. Recent works observe that the flatness of the\nlocal minimum correlates well with better generalization. In this work, we\nadapt Stochastic Weight Averaging (SWA), a method encouraging convergence to a\nflatter minimum, to fine-tuning PLMs. We conduct extensive experiments on\nvarious NLP tasks (text classification, question answering, and generation) and\ndifferent model architectures and demonstrate that our adaptation improves the\ngeneralization without extra computation cost. Moreover, we observe that this\nsimple optimization technique is able to outperform the state-of-the-art KD\nmethods for compact models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Peng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobyzev_I/0/1/0/all/0/1\">Ivan Kobyzev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_A/0/1/0/all/0/1\">Ahmad Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlais_P/0/1/0/all/0/1\">Philippe Langlais</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lisan: Yemeni, Iraqi, Libyan, and Sudanese Arabic Dialect Copora with Morphological Annotations. (arXiv:2212.06468v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.06468","description":"<p>This article presents morphologically-annotated Yemeni, Sudanese, Iraqi, and\nLibyan Arabic dialects Lisan corpora. Lisan features around 1.2 million tokens.\nWe collected the content of the corpora from several social media platforms.\nThe Yemeni corpus (~ 1.05M tokens) was collected automatically from Twitter.\nThe corpora of the other three dialects (~ 50K tokens each) came manually from\nFacebook and YouTube posts and comments.\n</p>\n<p>Thirty five (35) annotators who are native speakers of the target dialects\ncarried out the annotations. The annotators segemented all words in the four\ncorpora into prefixes, stems and suffixes and labeled each with different\nmorphological features such as part of speech, lemma, and a gloss in English.\nAn Arabic Dialect Annotation Toolkit ADAT was developped for the purpose of the\nannation. The annotators were trained on a set of guidelines and on how to use\nADAT. We developed ADAT to assist the annotators and to ensure compatibility\nwith SAMA and Curras tagsets. The tool is open source, and the four corpora are\nalso available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jarrar_M/0/1/0/all/0/1\">Mustafa Jarrar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaraket_F/0/1/0/all/0/1\">Fadi A Zaraket</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammouda_T/0/1/0/all/0/1\">Tymaa Hammouda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alavi_D/0/1/0/all/0/1\">Daanish Masood Alavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waahlisch_M/0/1/0/all/0/1\">Martin Waahlisch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Azimuth: Systematic Error Analysis for Text Classification. (arXiv:2212.08216v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2212.08216","description":"<p>We present Azimuth, an open-source and easy-to-use tool to perform error\nanalysis for text classification. Compared to other stages of the ML\ndevelopment cycle, such as model training and hyper-parameter tuning, the\nprocess and tooling for the error analysis stage are less mature. However, this\nstage is critical for the development of reliable and trustworthy AI systems.\nTo make error analysis more systematic, we propose an approach comprising\ndataset analysis and model quality assessment, which Azimuth facilitates. We\naim to help AI practitioners discover and address areas where the model does\nnot generalize by leveraging and integrating a range of ML techniques, such as\nsaliency maps, similarity, uncertainty, and behavioral analyses, all in one\ntool. Our code and documentation are available at\ngithub.com/servicenow/azimuth.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gauthier_Melancon_G/0/1/0/all/0/1\">Gabrielle Gauthier-Melan&#xe7;on</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayala_O/0/1/0/all/0/1\">Orlando Marquez Ayala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brin_L/0/1/0/all/0/1\">Lindsay Brin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyler_C/0/1/0/all/0/1\">Chris Tyler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Branchaud_Charron_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Branchaud-Charron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marinier_J/0/1/0/all/0/1\">Joseph Marinier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grande_K/0/1/0/all/0/1\">Karine Grande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Di Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teaching Small Language Models to Reason. (arXiv:2212.08410v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08410","description":"<p>Chain of thought prompting successfully improves the reasoning capabilities\nof large language models, achieving state of the art results on a range of\ndatasets. However, these reasoning capabilities only appear to emerge in models\nwith a size of over 100 billion parameters. In this paper, we explore the\ntransfer of such reasoning capabilities to models with less than 100 billion\nparameters via knowledge distillation. Specifically, we finetune a student\nmodel on the chain of thought outputs generated by a larger teacher model. Our\nexperiments show that the proposed method improves task performance across\narithmetic, commonsense and symbolic reasoning datasets. For example, the\naccuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned on\nPaLM-540B generated chains of thought.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Magister_L/0/1/0/all/0/1\">Lucie Charlotte Magister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallinson_J/0/1/0/all/0/1\">Jonathan Mallinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adamek_J/0/1/0/all/0/1\">Jakub Adamek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malmi_E/0/1/0/all/0/1\">Eric Malmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Severyn_A/0/1/0/all/0/1\">Aliaksei Severyn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SalKG: Learning From Knowledge Graph Explanations for Commonsense Reasoning. (arXiv:2104.08793v5 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2104.08793","description":"<p>Augmenting pre-trained language models with knowledge graphs (KGs) has\nachieved success on various commonsense reasoning tasks. However, for a given\ntask instance, the KG, or certain parts of the KG, may not be useful. Although\nKG-augmented models often use attention to focus on specific KG components, the\nKG is still always used, and the attention mechanism is never explicitly taught\nwhich KG components should be used. Meanwhile, saliency methods can measure how\nmuch a KG feature (e.g., graph, node, path) influences the model to make the\ncorrect prediction, thus explaining which KG features are useful. This paper\nexplores how saliency explanations can be used to improve KG-augmented models'\nperformance. First, we propose to create coarse (Is the KG useful?) and fine\n(Which nodes/paths in the KG are useful?) saliency explanations. Second, to\nmotivate saliency-based supervision, we analyze oracle KG-augmented models\nwhich directly use saliency explanations as extra inputs for guiding their\nattention. Third, we propose SalKG, a framework for KG-augmented models to\nlearn from coarse and/or fine saliency explanations. Given saliency\nexplanations created from a task's training set, SalKG jointly trains the model\nto predict the explanations, then solve the task by attending to KG features\nhighlighted by the predicted explanations. On three commonsense QA benchmarks\n(CSQA, OBQA, CODAH) and a range of KG-augmented models, we show that SalKG can\nyield considerable performance gains -- up to 2.76% absolute improvement on\nCSQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Aaron Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiashu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Boyuan Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1\">Soumya Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1\">Tanishq Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNIREX: A Unified Learning Framework for Language Model Rationale Extraction. (arXiv:2112.08802v2 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2112.08802","description":"<p>An extractive rationale explains a language model's (LM's) prediction on a\ngiven task instance by highlighting the text inputs that most influenced the\nprediction. Ideally, rationale extraction should be faithful (reflective of\nLM's actual behavior) and plausible (convincing to humans), without\ncompromising the LM's (i.e., task model's) task performance. Although\nattribution algorithms and select-predict pipelines are commonly used in\nrationale extraction, they both rely on certain heuristics that hinder them\nfrom satisfying all three desiderata. In light of this, we propose UNIREX, a\nflexible learning framework which generalizes rationale extractor optimization\nas follows: (1) specify architecture for a learned rationale extractor; (2)\nselect explainability objectives (i.e., faithfulness and plausibility\ncriteria); and (3) jointly the train task model and rationale extractor on the\ntask using selected objectives. UNIREX enables replacing prior works' heuristic\ndesign choices with a generic learned rationale extractor in (1) and optimizing\nit for all three desiderata in (2)-(3). To facilitate comparison between\nmethods w.r.t. multiple desiderata, we introduce the Normalized Relative Gain\n(NRG) metric. Across five text classification datasets, our best UNIREX\nconfiguration outperforms baselines by an average of 32.9% NRG. Plus, we find\nthat UNIREX-trained rationale extractors can even generalize to unseen datasets\nand tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Aaron Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanjabi_M/0/1/0/all/0/1\">Maziar Sanjabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathias_L/0/1/0/all/0/1\">Lambert Mathias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Liang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_S/0/1/0/all/0/1\">Shaoliang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiaochang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1\">Hamed Firooz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Swing Distillation: A Privacy-Preserving Knowledge Distillation Framework. (arXiv:2212.08349v1 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2212.08349","description":"<p>Knowledge distillation (KD) has been widely used for model compression and\nknowledge transfer. Typically, a big teacher model trained on sufficient data\ntransfers knowledge to a small student model. However, despite the success of\nKD, little effort has been made to study whether KD leaks the training data of\nthe teacher model. In this paper, we experimentally reveal that KD suffers from\nthe risk of privacy leakage. To alleviate this issue, we propose a novel\nknowledge distillation method, swing distillation, which can effectively\nprotect the private information of the teacher model from flowing to the\nstudent model. In our framework, the temperature coefficient is dynamically and\nadaptively adjusted according to the degree of private information contained in\nthe data, rather than a predefined constant hyperparameter. It assigns\ndifferent temperatures to tokens according to the likelihood that a token in a\nposition contains private information. In addition, we inject noise into soft\ntargets provided to the student model, in order to avoid unshielded knowledge\ntransfer. Experiments on multiple datasets and tasks demonstrate that the\nproposed swing distillation can significantly reduce (by over 80% in terms of\ncanary exposure) the risk of privacy leakage in comparison to KD with\ncompetitive or better performance. Furthermore, swing distillation is robust\nagainst the increasing privacy budget.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junzhuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xinwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">Weilong Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuangzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_C/0/1/0/all/0/1\">Chao Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Deyi Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-12-19T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}