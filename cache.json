{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-10-26T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Adaptive Label Smoothing with Self-Knowledge in Natural Language Generation. (arXiv:2210.13459v1 [cs.LG])","link":"http://arxiv.org/abs/2210.13459","description":"<p>Overconfidence has been shown to impair generalization and calibration of a\nneural network. Previous studies remedy this issue by adding a regularization\nterm to a loss function, preventing a model from making a peaked distribution.\nLabel smoothing smoothes target labels with a pre-defined prior label\ndistribution; as a result, a model is learned to maximize the likelihood of\npredicting the soft label. Nonetheless, the amount of smoothing is the same in\nall samples and remains fixed in training. In other words, label smoothing does\nnot reflect the change in probability distribution mapped by a model over the\ncourse of training. To address this issue, we propose a regularization scheme\nthat brings dynamic nature into the smoothing parameter by taking model\nprobability distribution into account, thereby varying the parameter per\ninstance. A model in training self-regulates the extent of smoothing on the fly\nduring forward propagation. Furthermore, inspired by recent work in bridging\nlabel smoothing and knowledge distillation, our work utilizes self-knowledge as\na prior label distribution in softening target labels, and presents theoretical\nsupport for the regularization effect by knowledge distillation and the dynamic\nsmoothing parameter. Our regularizer is validated comprehensively, and the\nresult illustrates marked improvements in model generalization and calibration,\nenhancing robustness and trustworthiness of a model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongkyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_K/0/1/0/all/0/1\">Ka Chun Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Nevin L. Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ExPUNations: Augmenting Puns with Keywords and Explanations. (arXiv:2210.13513v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13513","description":"<p>The tasks of humor understanding and generation are challenging and\nsubjective even for humans, requiring commonsense and real-world knowledge to\nmaster. Puns, in particular, add the challenge of fusing that knowledge with\nthe ability to interpret lexical-semantic ambiguity. In this paper, we present\nthe ExPUNations (ExPUN) dataset, in which we augment an existing dataset of\npuns with detailed crowdsourced annotations of keywords denoting the most\ndistinctive words that make the text funny, pun explanations describing why the\ntext is funny, and fine-grained funniness ratings. This is the first humor\ndataset with such extensive and fine-grained annotations specifically for puns.\nBased on these annotations, we propose two tasks: explanation generation to aid\nwith pun classification and keyword-conditioned pun generation, to challenge\nthe current state-of-the-art natural language understanding and generation\nmodels' ability to understand and generate humor. We showcase that the\nannotated keywords we collect are helpful for generating better novel humorous\ntexts in human evaluation, and that our natural language explanations can be\nleveraged to improve both the accuracy and robustness of humor classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_Chen_A/0/1/0/all/0/1\">Anjali Narayan-Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oraby_S/0/1/0/all/0/1\">Shereen Oraby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cervone_A/0/1/0/all/0/1\">Alessandra Cervone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1\">Tagyoung Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Situated Pun Generation. (arXiv:2210.13522v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13522","description":"<p>Previous work on pun generation commonly begins with a given pun word (a pair\nof homophones for heterographic pun generation and a polyseme for homographic\npun generation) and seeks to generate an appropriate pun. While this may enable\nefficient pun generation, we believe that a pun is most entertaining if it fits\nappropriately within a given context, e.g., a given situation or dialogue. In\nthis work, we propose a new task, context-situated pun generation, where a\nspecific context represented by a set of keywords is provided, and the task is\nto first identify suitable pun words that are appropriate for the context, then\ngenerate puns based on the context keywords and the identified pun words. We\ncollect CUP (Context-sitUated Pun), containing 4.5k tuples of context words and\npun pairs. Based on the new data and setup, we propose a pipeline system for\ncontext-situated pun generation, including a pun word retrieval module that\nidentifies suitable pun words for a given context, and a generation module that\ngenerates puns from context keywords and pun words. Human evaluation shows that\n69% of our top retrieved pun words can be used to generate context-situated\npuns, and our generation module yields successful puns 31% of the time given a\nplausible tuple of context words and pun pair, almost tripling the yield of a\nstate-of-the-art pun generation model. With an end-to-end evaluation, our\npipeline system with the top-1 retrieved pun pair for a given context can\ngenerate successful puns 40% of the time, better than all other modeling\nvariations but 32% lower than the human success rate. This highlights the\ndifficulty of the task, and encourages more research in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_Chen_A/0/1/0/all/0/1\">Anjali Narayan-Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oraby_S/0/1/0/all/0/1\">Shereen Oraby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shuyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1\">Tagyoung Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computational Inference in Cognitive Science: Operational, Societal and Ethical Considerations. (arXiv:2210.13526v1 [q-bio.NC])","link":"http://arxiv.org/abs/2210.13526","description":"<p>Emerging research frontiers and computational advances have gradually\ntransformed cognitive science into a multidisciplinary and data-driven field.\nAs a result, there is a proliferation of cognitive theories investigated and\ninterpreted from different academic lens and in different levels of\nabstraction. We formulate this applied aspect of this challenge as the\ncomputational cognitive inference, and describe the major routes of\ncomputational approaches. To balance the potential optimism alongside the speed\nand scale of the data-driven era of cognitive science, we propose to inspect\nthis trend in more empirical terms by identifying the operational challenges,\nsocietal impacts and ethical guidelines in conducting research and interpreting\nresults from the computational inference in cognitive science.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Lin_B/0/1/0/all/0/1\">Baihan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of Misinformation in New Articles using Natural Language Processing and a Recurrent Neural Network. (arXiv:2210.13534v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13534","description":"<p>This paper seeks to address the classification of misinformation in news\narticles using a Long Short Term Memory Recurrent Neural Network. Articles were\ntaken from 2018; a year that was filled with reporters writing about President\nDonald Trump, Special Counsel Robert Mueller, the Fifa World Cup, and Russia.\nThe model presented successfully classifies these articles with an accuracy\nscore of 0.779944. We consider this to be successful because the model was\ntrained on articles that included languages other than English as well as\nincomplete, or fragmented, articles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cunha_B/0/1/0/all/0/1\">Brendan Cunha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manikonda_L/0/1/0/all/0/1\">Lydia Manikonda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Pre-Training Objectives for Transformer-based Autoencoders. (arXiv:2210.13536v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13536","description":"<p>In this paper, we study trade-offs between efficiency, cost and accuracy when\npre-training Transformer encoders with different pre-training objectives. For\nthis purpose, we analyze features of common objectives and combine them to\ncreate new effective pre-training approaches. Specifically, we designed light\ntoken generators based on a straightforward statistical approach, which can\nreplace ELECTRA computationally heavy generators, thus highly reducing cost.\nOur experiments also show that (i) there are more efficient alternatives to\nBERT's MLM, and (ii) it is possible to efficiently pre-train Transformer-based\nmodels using lighter generators without a significant drop in performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liello_L/0/1/0/all/0/1\">Luca Di Liello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabburo_M/0/1/0/all/0/1\">Matteo Gabburo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moschitti_A/0/1/0/all/0/1\">Alessandro Moschitti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterizing Verbatim Short-Term Memory in Neural Language Models. (arXiv:2210.13569v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13569","description":"<p>When a language model is trained to predict natural language sequences, its\nprediction at each moment depends on a representation of prior context. What\nkind of information about the prior context can language models retrieve? We\ntested whether language models could retrieve the exact words that occurred\npreviously in a text. In our paradigm, language models (transformers and an\nLSTM) processed English text in which a list of nouns occurred twice. We\noperationalized retrieval as the reduction in surprisal from the first to the\nsecond list. We found that the transformers retrieved both the identity and\nordering of nouns from the first list. Further, the transformers' retrieval was\nmarkedly enhanced when they were trained on a larger corpus and with greater\nmodel depth. Lastly, their ability to index prior tokens was dependent on\nlearned attention patterns. In contrast, the LSTM exhibited less precise\nretrieval, which was limited to list-initial tokens and to short intervening\ntexts. The LSTM's retrieval was not sensitive to the order of nouns and it\nimproved when the list was semantically coherent. We conclude that transformers\nimplemented something akin to a working memory system that could flexibly\nretrieve individual token representations across arbitrary delays; conversely,\nthe LSTM maintained a coarser and more rapidly-decaying semantic gist of prior\ntokens, weighted toward the earliest items.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Armeni_K/0/1/0/all/0/1\">Kristijan Armeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honey_C/0/1/0/all/0/1\">Christopher Honey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Self-Rationalization Improve Robustness to Spurious Correlations?. (arXiv:2210.13575v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13575","description":"<p>Rationalization is fundamental to human reasoning and learning. NLP models\ntrained to produce rationales along with predictions, called\nself-rationalization models, have been investigated for their interpretability\nand utility to end-users. However, the extent to which training with\nhuman-written rationales facilitates learning remains an under-explored\nquestion. We ask whether training models to self-rationalize can aid in their\nlearning to solve tasks for the right reasons. Specifically, we evaluate how\ntraining self-rationalization models with free-text rationales affects\nrobustness to spurious correlations in fine-tuned encoder-decoder and\ndecoder-only models of six different sizes. We evaluate robustness to spurious\ncorrelations by measuring performance on 1) manually annotated challenge\ndatasets and 2) subsets of original test sets where reliance on spurious\ncorrelations would fail to produce correct answers. We find that while\nself-rationalization can improve robustness to spurious correlations in\nlow-resource settings, it tends to hurt robustness in higher-resource settings.\nFurthermore, these effects depend on model family and size, as well as on\nrationale content. Together, our results suggest that explainability can come\nat the cost of robustness; thus, appropriate care should be taken when training\nself-rationalizing models with the goal of creating more trustworthy models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1\">Alexis Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1\">Matthew E. Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marasovic_A/0/1/0/all/0/1\">Ana Marasovi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speeding Up Question Answering Task of Language Models via Inverted Index. (arXiv:2210.13578v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13578","description":"<p>Natural language processing applications, such as conversational agents and\ntheir question-answering capabilities, are widely used in the real world.\nDespite the wide popularity of large language models (LLMs), few real-world\nconversational agents take advantage of LLMs. Extensive resources consumed by\nLLMs disable developers from integrating them into end-user applications. In\nthis study, we leverage an inverted indexing mechanism combined with LLMs to\nimprove the efficiency of question-answering models for closed-domain\nquestions. Our experiments show that using the index improves the average\nresponse time by 97.44%. In addition, due to the reduced search scope, the\naverage BLEU score improved by 0.23 while using the inverted index.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sungu_Eryilmaz_Y/0/1/0/all/0/1\">Yesim Sungu-Eryilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momeni_E/0/1/0/all/0/1\">Elaheh Momeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawassizadeh_R/0/1/0/all/0/1\">Reza Rawassizadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LANS: Large-scale Arabic News Summarization Corpus. (arXiv:2210.13600v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13600","description":"<p>Text summarization has been intensively studied in many languages, and some\nlanguages have reached advanced stages. Yet, Arabic Text Summarization (ATS) is\nstill in its developing stages. Existing ATS datasets are either small or lack\ndiversity. We build, LANS, a large-scale and diverse dataset for Arabic Text\nSummarization task. LANS offers 8.4 million articles and their summaries\nextracted from newspapers websites metadata between 1999 and 2019. The\nhigh-quality and diverse summaries are written by journalists from 22 major\nArab newspapers, and include an eclectic mix of at least more than 7 topics\nfrom each source. We conduct an intrinsic evaluation on LANS by both automatic\nand human evaluations. Human evaluation of 1000 random samples reports 95.4%\naccuracy for our collected summaries, and automatic evaluation quantifies the\ndiversity and abstractness of the summaries. The dataset is publicly available\nupon request.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alhamadani_A/0/1/0/all/0/1\">Abdulaziz Alhamadani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chang-Tien Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapters for Enhanced Modeling of Multilingual Knowledge and Text. (arXiv:2210.13617v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13617","description":"<p>Large language models appear to learn facts from the large text corpora they\nare trained on. Such facts are encoded implicitly within their many parameters,\nmaking it difficult to verify or manipulate what knowledge has been learned.\nLanguage models have recently been extended to multilingual language models\n(MLLMs), enabling knowledge to be learned across hundreds of languages.\nMeanwhile, knowledge graphs contain facts in an explicit triple format, which\nrequire careful and costly curation and are only available in a few\nhigh-resource languages, restricting their research and application. To address\nthese issues, we propose to enhance MLLMs with knowledge from multilingual\nknowledge graphs (MLKGs) so as to tackle language and knowledge graph tasks\nacross many languages, including low-resource ones. Specifically, we introduce\na lightweight adapter set to enhance MLLMs with cross-lingual entity alignment\nand facts from MLKGs for many languages. Experiments on common benchmarks show\nthat such enhancement benefits both MLLMs and MLKGs, achieving: (1) comparable\nor improved performance for knowledge graph completion and entity alignment\nrelative to baselines, especially for low-resource languages (for which\nknowledge graphs are unavailable); and (2) improved MLLM performance on\nlanguage understanding tasks that require multilingual factual knowledge; all\nwhile maintaining performance on other general language tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yifan Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenxiang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Meizhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allen_C/0/1/0/all/0/1\">Carl Allen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook. (arXiv:2210.13623v1 [cs.AI])","link":"http://arxiv.org/abs/2210.13623","description":"<p>In recent years, reinforcement learning and bandits have transformed a wide\nrange of real-world applications including healthcare, finance, recommendation\nsystems, robotics, and last but not least, the speech and natural language\nprocessing. While most speech and language applications of reinforcement\nlearning algorithms are centered around improving the training of deep neural\nnetworks with its flexible optimization properties, there are still many\ngrounds to explore to utilize the benefits of reinforcement learning, such as\nits reward-driven adaptability, state representations, temporal structures and\ngeneralizability. In this survey, we present an overview of recent advancements\nof reinforcement learning and bandits, and discuss how they can be effectively\nemployed to solve speech and natural language processing problems with models\nthat are adaptive, interactive and scalable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Baihan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge. (arXiv:2210.13626v1 [cs.CV])","link":"http://arxiv.org/abs/2210.13626","description":"<p>There has been a growing interest in solving Visual Question Answering (VQA)\ntasks that require the model to reason beyond the content present in the image.\nIn this work, we focus on questions that require commonsense reasoning. In\ncontrast to previous methods which inject knowledge from static knowledge\nbases, we investigate the incorporation of contextualized knowledge using\nCommonsense Transformer (COMET), an existing knowledge model trained on\nhuman-curated knowledge bases. We propose a method to generate, select, and\nencode external commonsense knowledge alongside visual and textual cues in a\nnew pre-trained Vision-Language-Commonsense transformer model, VLC-BERT.\nThrough our evaluation on the knowledge-intensive OK-VQA and A-OKVQA datasets,\nwe show that VLC-BERT is capable of outperforming existing models that utilize\nstatic knowledge bases. Furthermore, through a detailed analysis, we explain\nwhich questions benefit, and which don't, from contextualized commonsense\nknowledge from COMET.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1\">Sahithya Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinchure_A/0/1/0/all/0/1\">Aditya Chinchure</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1\">Leonid Sigal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1\">Renjie Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1\">Vered Shwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Long-Term Citations from Short-Term Linguistic Influence. (arXiv:2210.13628v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13628","description":"<p>A standard measure of the influence of a research paper is the number of\ntimes it is cited. However, papers may be cited for many reasons, and citation\ncount offers limited information about the extent to which a paper affected the\ncontent of subsequent publications. We therefore propose a novel method to\nquantify linguistic influence in timestamped document collections. There are\ntwo main steps: first, identify lexical and semantic changes using contextual\nembeddings and word frequencies; second, aggregate information about these\nchanges into per-document influence scores by estimating a high-dimensional\nHawkes process with a low-rank parameter matrix. We show that this measure of\nlinguistic influence is predictive of $\\textit{future}$ citations: the estimate\nof linguistic influence from the two years after a paper's publication is\ncorrelated with and predictive of its citation count in the following three\nyears. This is demonstrated using an online evaluation with incremental\ntemporal training/test splits, in comparison with a strong baseline that\nincludes predictors for initial citation counts, topics, and lexical features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soni_S/0/1/0/all/0/1\">Sandeep Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamman_D/0/1/0/all/0/1\">David Bamman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1\">Jacob Eisenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward an Intelligent Tutoring System for Argument Mining in Legal Texts. (arXiv:2210.13635v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13635","description":"<p>We propose an adaptive environment (CABINET) to support caselaw analysis\n(identifying key argument elements) based on a novel cognitive computing\nframework that carefully matches various machine learning (ML) capabilities to\nthe proficiency of a user. CABINET supports law students in their learning as\nwell as professionals in their work. The results of our experiments focused on\nthe feasibility of the proposed framework are promising. We show that the\nsystem is capable of identifying a potential error in the analysis with very\nlow false positives rate (2.0-3.5%), as well as of predicting the key argument\nelement type (e.g., an issue or a holding) with a reasonably high F1-score\n(0.74).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Westermann_H/0/1/0/all/0/1\">Hannes Westermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savelka_J/0/1/0/all/0/1\">Jaromir Savelka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_V/0/1/0/all/0/1\">Vern R. Walker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashley_K/0/1/0/all/0/1\">Kevin D. Ashley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benyekhlef_K/0/1/0/all/0/1\">Karim Benyekhlef</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReaRev: Adaptive Reasoning for Question Answering over Knowledge Graphs. (arXiv:2210.13650v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13650","description":"<p>Knowledge Graph Question Answering (KGQA) involves retrieving entities as\nanswers from a Knowledge Graph (KG) using natural language queries. The\nchallenge is to learn to reason over question-relevant KG facts that traverse\nKG entities and lead to the question answers. To facilitate reasoning, the\nquestion is decoded into instructions, which are dense question representations\nused to guide the KG traversals. However, if the derived instructions do not\nexactly match the underlying KG information, they may lead to reasoning under\nirrelevant context. Our method, termed ReaRev, introduces a new way to KGQA\nreasoning with respect to both instruction decoding and execution. To improve\ninstruction decoding, we perform reasoning in an adaptive manner, where\nKG-aware information is used to iteratively update the initial instructions. To\nimprove instruction execution, we emulate breadth-first search (BFS) with graph\nneural networks (GNNs). The BFS strategy treats the instructions as a set and\nallows our method to decide on their execution order on the fly. Experimental\nresults on three KGQA benchmarks demonstrate the ReaRev's effectiveness\ncompared with previous state-of-the-art, especially when the KG is incomplete\nor when we tackle complex questions. Our code is publicly available at\nhttps://github.com/cmavro/ReaRev_KGQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mavromatis_C/0/1/0/all/0/1\">Costas Mavromatis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1\">George Karypis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Help me write a poem: Instruction Tuning as a Vehicle for Collaborative Poetry Writing. (arXiv:2210.13669v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13669","description":"<p>Recent work in training large language models (LLMs) to follow natural\nlanguage instructions has opened up exciting opportunities for natural language\ninterface design. Building on the prior success of LLMs in the realm of\ncomputer-assisted creativity, we aim to study if LLMs can improve the quality\nof user-generated content through collaboration. We present CoPoet, a\ncollaborative poetry writing system. In contrast to auto-completing a user's\ntext, CoPoet is controlled by user instructions that specify the attributes of\nthe desired text, such as Write a sentence about `love' or Write a sentence\nending in `fly'. The core component of our system is a language model\nfine-tuned on a diverse collection of instructions for poetry writing. Our\nmodel is not only competitive with publicly available LLMs trained on\ninstructions (InstructGPT), but is also capable of satisfying unseen\ncompositional instructions. A study with 15 qualified crowdworkers shows that\nusers successfully write poems with CoPoet on diverse topics ranging from\nMonarchy to Climate change. Further, the collaboratively written poems are\npreferred by third-party evaluators over those written without the system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_V/0/1/0/all/0/1\">Vishakh Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Parameter Efficient Learning for Generation. (arXiv:2210.13673v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13673","description":"<p>Parameter efficient learning methods (PERMs) have recently gained significant\nattention as they provide an efficient way for pre-trained language models\n(PLMs) to adapt to a downstream task. However, these conclusions are mostly\ndrawn from in-domain evaluations over the full training set. In this paper, we\npresent comparisons between PERMs and finetuning from three new perspectives:\n(1) the effect of sample and model size to in-domain evaluations, (2)\ngeneralization to unseen domains and new datasets, and (3) the faithfulness of\ngenerations. Our results show that for in-domain settings (a) there is a cross\npoint of sample size for which PERMs will perform better than finetuning when\ntraining with fewer samples, and (b) larger PLMs have larger cross points. For\ncross-domain and cross-dataset cases, we show that (a) Adapter (Houlsby et al.,\n2019) performs the best amongst all the PERMs studied here, and (b) it\noutperforms finetuning if the task dataset is below a certain size. We also\ncompare the faithfulness of generations and show that PERMs can achieve better\nfaithfulness score than finetuning, especially for small training set, by as\nmuch as 6%. Finally, we apply Adapter to MT-NLG 530b (Smith et al., 2022) and\nachieve new state-of-the-art results on Xsum (Narayan et al., 2018) for all\nROUGE scores (ROUGE-1 49.17, ROUGE-2 27.20, ROUGE-L 40.98).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwary_M/0/1/0/all/0/1\">Mostofa Patwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhumoye_S/0/1/0/all/0/1\">Shrimai Prabhumoye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_V/0/1/0/all/0/1\">Virginia Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prenger_R/0/1/0/all/0/1\">Ryan J. Prenger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging the Training-Inference Gap for Dense Phrase Retrieval. (arXiv:2210.13678v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13678","description":"<p>Building dense retrievers requires a series of standard procedures, including\ntraining and validating neural models and creating indexes for efficient\nsearch. However, these procedures are often misaligned in that training\nobjectives do not exactly reflect the retrieval scenario at inference time. In\nthis paper, we explore how the gap between training and inference in dense\nretrieval can be reduced, focusing on dense phrase retrieval (Lee et al., 2021)\nwhere billions of representations are indexed at inference. Since validating\nevery dense retriever with a large-scale index is practically infeasible, we\npropose an efficient way of validating dense retrievers using a small subset of\nthe entire corpus. This allows us to validate various training strategies\nincluding unifying contrastive loss terms and using hard negatives for phrase\nretrieval, which largely reduces the training-inference discrepancy. As a\nresult, we improve top-1 phrase retrieval accuracy by 2~3 points and top-20\npassage retrieval accuracy by 2~4 points for open-domain question answering.\nOur work urges modeling dense retrievers with careful consideration of training\nand inference via efficient validation while advancing phrase retrieval as a\ngeneral solution for dense retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyuwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinhyuk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas Oguz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Wenhan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XRICL: Cross-lingual Retrieval-Augmented In-Context Learning for Cross-lingual Text-to-SQL Semantic Parsing. (arXiv:2210.13693v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13693","description":"<p>In-context learning using large language models has recently shown surprising\nresults for semantic parsing tasks such as Text-to-SQL translation. Prompting\nGPT-3 or Codex using several examples of question-SQL pairs can produce\nexcellent results, comparable to state-of-the-art finetuning-based models.\nHowever, existing work primarily focuses on English datasets, and it is unknown\nwhether large language models can serve as competitive semantic parsers for\nother languages. To bridge this gap, our work focuses on cross-lingual\nText-to-SQL semantic parsing for translating non-English utterances into SQL\nqueries based on an English schema. We consider a zero-shot transfer learning\nsetting with the assumption that we do not have any labeled examples in the\ntarget language (but have annotated examples in English). This work introduces\nthe XRICL framework, which learns to retrieve relevant English exemplars for a\ngiven query to construct prompts. We also include global translation exemplars\nfor a target language to facilitate the translation process for large language\nmodels. To systematically evaluate our model, we construct two new benchmark\ndatasets, XSpider and XKaggle-dbqa, which include questions in Chinese,\nVietnamese, Farsi, and Hindi. Our experiments show that XRICL effectively\nleverages large pre-trained language models to outperform existing baselines.\nData and code are publicly available at https://github.com/Impavidity/XRICL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Peng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">He Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Joint Training Really Help Cascaded Speech Translation?. (arXiv:2210.13700v1 [eess.AS])","link":"http://arxiv.org/abs/2210.13700","description":"<p>Currently, in speech translation, the straightforward approach - cascading a\nrecognition system with a translation system - delivers state-of-the-art\nresults. However, fundamental challenges such as error propagation from the\nautomatic speech recognition system still remain. To mitigate these problems,\nrecently, people turn their attention to direct data and propose various joint\ntraining methods. In this work, we seek to answer the question of whether joint\ntraining really helps cascaded speech translation. We review recent papers on\nthe topic and also investigate a joint training criterion by marginalizing the\ntranscription posterior probabilities. Our findings show that a strong cascaded\nbaseline can diminish any improvements obtained using joint training, and we\nsuggest alternatives to joint training. We hope this work can serve as a\nrefresher of the current speech translation landscape, and motivate research in\nfinding more efficient and creative ways to utilize the direct data for speech\ntranslation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tran_V/0/1/0/all/0/1\">Viet Anh Khoa Tran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thulke_D/0/1/0/all/0/1\">David Thulke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yingbo Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herold_C/0/1/0/all/0/1\">Christian Herold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence. (arXiv:2210.13701v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13701","description":"<p>Question answering models can use rich knowledge sources -- up to one hundred\nretrieved passages and parametric knowledge in the large-scale language model\n(LM). Prior work assumes information in such knowledge sources is consistent\nwith each other, paying little attention to how models blend information stored\nin their LM parameters with that from retrieved evidence documents. In this\npaper, we simulate knowledge conflicts (i.e., where parametric knowledge\nsuggests one answer and different passages suggest different answers) and\nexamine model behaviors. We find retrieval performance heavily impacts which\nsources models rely on, and current models mostly rely on non-parametric\nknowledge in their best-performing settings. We discover a troubling trend that\ncontradictions among knowledge sources affect model confidence only marginally.\nTo address this issue, we present a new calibration study, where models are\ndiscouraged from presenting any single answer when presented with multiple\nconflicting answer candidates in retrieved evidences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hung-Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Michael J.Q. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Legal Domain Adaptation. (arXiv:2210.13712v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13712","description":"<p>Seeking legal advice is often expensive. Recent advancement in machine\nlearning for solving complex problems can be leveraged to help make legal\nservices more accessible to the public. However, real-life applications\nencounter significant challenges. State-of-the-art language models are growing\nincreasingly large, making parameter-efficient learning increasingly important.\nUnfortunately, parameter-efficient methods perform poorly with small amounts of\ndata, which are common in the legal domain (where data labelling costs are\nhigh). To address these challenges, we propose parameter-efficient legal domain\nadaptation, which uses vast unsupervised legal data from public legal forums to\nperform legal pre-training. This method exceeds or matches the fewshot\nperformance of existing models such as LEGAL-BERT on various legal tasks while\ntuning only approximately 0.1% of model parameters. Additionally, we show that\nour method can achieve calibration comparable to existing methods across\nseveral tasks. To the best of our knowledge, this work is among the first to\nexplore parameter-efficient methods of tuning language models toward the legal\ndomain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jonathan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhambhoria_R/0/1/0/all/0/1\">Rohan Bhambhoria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PALT: Parameter-Lite Transfer of Language Models for Knowledge Graph Completion. (arXiv:2210.13715v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13715","description":"<p>This paper presents a parameter-lite transfer learning approach of pretrained\nlanguage models (LM) for knowledge graph (KG) completion. Instead of\nfinetuning, which modifies all LM parameters, we only tune a few new parameters\nwhile keeping the original LM parameters fixed. We establish this via\nreformulating KG completion as a \"fill-in-the-blank\" task, and introducing a\nparameter-lite encoder on top of the original LMs. We show that, by tuning far\nfewer parameters than finetuning, LMs transfer non-trivially to most tasks and\nreach competitiveness with prior state-of-the-art approaches. For instance, we\noutperform the fully finetuning approaches on a KG completion benchmark by\ntuning only 1% of the parameters. The code and datasets are available at\n\\url{https://github.com/yuanyehome/PALT}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianhao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_K/0/1/0/all/0/1\">Koushik Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Reinforced Medical Report Generation with M-Linear Attention and Repetition Penalty. (arXiv:2210.13729v1 [cs.AI])","link":"http://arxiv.org/abs/2210.13729","description":"<p>To reduce doctors' workload, deep-learning-based automatic medical report\ngeneration has recently attracted more and more research efforts, where deep\nconvolutional neural networks (CNNs) are employed to encode the input images,\nand recurrent neural networks (RNNs) are used to decode the visual features\ninto medical reports automatically. However, these state-of-the-art methods\nmainly suffer from three shortcomings: (i) incomprehensive optimization, (ii)\nlow-order and unidimensional attention mechanisms, and (iii) repeated\ngeneration. In this article, we propose a hybrid reinforced medical report\ngeneration method with m-linear attention and repetition penalty mechanism\n(HReMRG-MR) to overcome these problems. Specifically, a hybrid reward with\ndifferent weights is employed to remedy the limitations of single-metric-based\nrewards. We also propose a search algorithm with linear complexity to\napproximate the best weight combination. Furthermore, we use m-linear attention\nmodules to explore high-order feature interactions and to achieve multi-modal\nreasoning, while a repetition penalty applies penalties to repeated terms\nduring the model's training process. Extensive experimental studies on two\npublic datasets show that HReMRG-MR greatly outperforms the state-of-the-art\nbaselines in terms of all metrics. We also conducted a series of ablation\nexperiments to prove the effectiveness of all our proposed components. We also\nperformed a reward search toy experiment to give evidence that our proposed\nsearch approach can significantly reduce the search time while approximating\nthe best performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenting Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhenghua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1\">Chang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better Few-Shot Relation Extraction with Label Prompt Dropout. (arXiv:2210.13733v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13733","description":"<p>Few-shot relation extraction aims to learn to identify the relation between\ntwo entities based on very limited training examples. Recent efforts found that\ntextual labels (i.e., relation names and relation descriptions) could be\nextremely useful for learning class representations, which will benefit the\nfew-shot learning task. However, what is the best way to leverage such label\ninformation in the learning process is an important research question. Existing\nworks largely assume such textual labels are always present during both\nlearning and prediction. In this work, we argue that such approaches may not\nalways lead to optimal results. Instead, we present a novel approach called\nlabel prompt dropout, which randomly removes label descriptions in the learning\nprocess. Our experiments show that our approach is able to lead to improved\nclass representations, yielding significantly better results on the few-shot\nrelation extraction task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DEMETR: Diagnosing Evaluation Metrics for Translation. (arXiv:2210.13746v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13746","description":"<p>While machine translation evaluation metrics based on string overlap (e.g.,\nBLEU) have their limitations, their computations are transparent: the BLEU\nscore assigned to a particular candidate translation can be traced back to the\npresence or absence of certain words. The operations of newer learned metrics\n(e.g., BLEURT, COMET), which leverage pretrained language models to achieve\nhigher correlations with human quality judgments than BLEU, are opaque in\ncomparison. In this paper, we shed light on the behavior of these learned\nmetrics by creating DEMETR, a diagnostic dataset with 31K English examples\n(translated from 10 source languages) for evaluating the sensitivity of MT\nevaluation metrics to 35 different linguistic perturbations spanning semantic,\nsyntactic, and morphological error categories. All perturbations were carefully\ndesigned to form minimal pairs with the actual translation (i.e., differ in\nonly one aspect). We find that learned metrics perform substantially better\nthan string-based metrics on DEMETR. Additionally, learned metrics differ in\ntheir sensitivity to various phenomena (e.g., BERTScore is sensitive to\nuntranslated words but relatively insensitive to gender manipulation, while\nCOMET is much more sensitive to word repetition than to aspectual changes). We\npublicly release DEMETR to spur more informed future development of machine\ntranslation evaluation metrics\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karpinska_M/0/1/0/all/0/1\">Marzena Karpinska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_N/0/1/0/all/0/1\">Nishant Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thai_K/0/1/0/all/0/1\">Katherine Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yixiao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ankita Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AugCSE: Contrastive Sentence Embedding with Diverse Augmentations. (arXiv:2210.13749v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13749","description":"<p>Data augmentation techniques have been proven useful in many applications in\nNLP fields. Most augmentations are task-specific, and cannot be used as a\ngeneral-purpose tool. In our work, we present AugCSE, a unified framework to\nutilize diverse sets of data augmentations to achieve a better, general\npurpose, sentence embedding model. Building upon the latest sentence embedding\nmodels, our approach uses a simple antagonistic discriminator that\ndifferentiates the augmentation types. With the finetuning objective borrowed\nfrom domain adaptation, we show that diverse augmentations, which often lead to\nconflicting contrastive signals, can be tamed to produce a better and more\nrobust sentence representation. Our methods achieve state-of-the-art results on\ndownstream transfer tasks and perform competitively on semantic textual\nsimilarity tasks, using only unsupervised data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zilu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocyigit_M/0/1/0/all/0/1\">Muhammed Yusuf Kocyigit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijaya_D/0/1/0/all/0/1\">Derry Wijaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SciFact-Open: Towards open-domain scientific claim verification. (arXiv:2210.13777v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13777","description":"<p>While research on scientific claim verification has led to the development of\npowerful systems that appear to approach human performance, these approaches\nhave yet to be tested in a realistic setting against large corpora of\nscientific literature. Moving to this open-domain evaluation setting, however,\nposes unique challenges; in particular, it is infeasible to exhaustively\nannotate all evidence documents. In this work, we present SciFact-Open, a new\ntest collection designed to evaluate the performance of scientific claim\nverification systems on a corpus of 500K research abstracts. Drawing upon\npooling techniques from information retrieval, we collect evidence for\nscientific claims by pooling and annotating the top predictions of four\nstate-of-the-art scientific claim verification models. We find that systems\ndeveloped on smaller corpora struggle to generalize to SciFact-Open, exhibiting\nperformance drops of at least 15 F1. In addition, analysis of the evidence in\nSciFact-Open reveals interesting phenomena likely to appear when claim\nverification systems are deployed in practice, e.g., cases where the evidence\nsupports only a special case of the claim. Our dataset is available at\nhttps://github.com/dwadden/scifact-open.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wadden_D/0/1/0/all/0/1\">David Wadden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1\">Bailey Kuehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lucy Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IDK-MRC: Unanswerable Questions for Indonesian Machine Reading Comprehension. (arXiv:2210.13778v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13778","description":"<p>Machine Reading Comprehension (MRC) has become one of the essential tasks in\nNatural Language Understanding (NLU) as it is often included in several NLU\nbenchmarks (Liang et al., 2020; Wilie et al., 2020). However, most MRC datasets\nonly have answerable question type, overlooking the importance of unanswerable\nquestions. MRC models trained only on answerable questions will select the span\nthat is most likely to be the answer, even when the answer does not actually\nexist in the given passage (Rajpurkar et al., 2018). This problem especially\nremains in medium- to low-resource languages like Indonesian. Existing\nIndonesian MRC datasets (Purwarianti et al., 2007; Clark et al., 2020) are\nstill inadequate because of the small size and limited question types, i.e.,\nthey only cover answerable questions. To fill this gap, we build a new\nIndonesian MRC dataset called I(n)don'tKnow- MRC (IDK-MRC) by combining the\nautomatic and manual unanswerable question generation to minimize the cost of\nmanual dataset construction while maintaining the dataset quality. Combined\nwith the existing answerable questions, IDK-MRC consists of more than 10K\nquestions in total. Our analysis shows that our dataset significantly improves\nthe performance of Indonesian MRC models, showing a large improvement for\nunanswerable questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Putri_R/0/1/0/all/0/1\">Rifki Afina Putri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topical Segmentation of Spoken Narratives: A Test Case on Holocaust Survivor Testimonies. (arXiv:2210.13783v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13783","description":"<p>The task of topical segmentation is well studied, but previous work has\nmostly addressed it in the context of structured, well-defined segments, such\nas segmentation into paragraphs, chapters, or segmenting text that originated\nfrom multiple sources. We tackle the task of segmenting running (spoken)\nnarratives, which poses hitherto unaddressed challenges. As a test case, we\naddress Holocaust survivor testimonies, given in English. Other than the\nimportance of studying these testimonies for Holocaust research, we argue that\nthey provide an interesting test case for topical segmentation, due to their\nunstructured surface level, relative abundance (tens of thousands of such\ntestimonies were collected), and the relatively confined domain that they\ncover. We hypothesize that boundary points between segments correspond to low\nmutual information between the sentences proceeding and following the boundary.\nBased on this hypothesis, we explore a range of algorithmic approaches to the\ntask, building on previous work on segmentation that uses generative Bayesian\nmodeling and state-of-the-art neural machinery. Compared to manually annotated\nreferences, we find that the developed approaches show considerable\nimprovements over previous work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wagner_E/0/1/0/all/0/1\">Eitan Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keydar_R/0/1/0/all/0/1\">Renana Keydar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinchevski_A/0/1/0/all/0/1\">Amit Pinchevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Referee: Reference-Free Sentence Summarization with Sharper Controllability through Symbolic Knowledge Distillation. (arXiv:2210.13800v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13800","description":"<p>We present Referee, a novel framework for sentence summarization that can be\ntrained reference-free (i.e., requiring no gold summaries for supervision),\nwhile allowing direct control for compression ratio. Our work is the first to\ndemonstrate that reference-free, controlled sentence summarization is feasible\nvia the conceptual framework of Symbolic Knowledge Distillation (West et al.,\n2022), where latent knowledge in pre-trained language models is distilled via\nexplicit examples sampled from the teacher models, further purified with three\ntypes of filters: length, fidelity, and Information Bottleneck. Moreover, we\nuniquely propose iterative distillation of knowledge, where student models from\nthe previous iteration of distillation serve as teacher models in the next\niteration. Starting off from a relatively modest set of GPT3-generated\nsummaries, we demonstrate how iterative knowledge distillation can lead to\nconsiderably smaller, but better summarizers with sharper controllability. A\nuseful by-product of this iterative distillation process is a high-quality\ndataset of sentence-summary pairs with varying degrees of compression ratios.\nEmpirical results demonstrate that the final student models vastly outperform\nthe much larger GPT3-Instruct model in terms of the controllability of\ncompression ratios, without compromising the quality of resulting\nsummarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sclar_M/0/1/0/all/0/1\">Melanie Sclar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sachin Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapitch: Adaption Multi-Speaker Text-to-Speech Conditioned on Pitch Disentangling with Untranscribed Data. (arXiv:2210.13803v1 [cs.SD])","link":"http://arxiv.org/abs/2210.13803","description":"<p>In this paper, we proposed Adapitch, a multi-speaker TTS method that makes\nadaptation of the supervised module with untranscribed data. We design two self\nsupervised modules to train the text encoder and mel decoder separately with\nuntranscribed data to enhance the representation of text and mel. To better\nhandle the prosody information in a synthesized voice, a supervised TTS module\nis designed conditioned on content disentangling of pitch, text, and speaker.\nThe training phase was separated into two parts, pretrained and fixed the text\nencoder and mel decoder with unsupervised mode, then the supervised mode on the\ndisentanglement of TTS. Experiment results show that the Adaptich achieved much\nbetter quality than baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xulong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Ning Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Speech Representation Learning via Speech-level and Phoneme-level Masking Approach. (arXiv:2210.13805v1 [cs.SD])","link":"http://arxiv.org/abs/2210.13805","description":"<p>Recovering the masked speech frames is widely applied in speech\nrepresentation learning. However, most of these models use random masking in\nthe pre-training. In this work, we proposed two kinds of masking approaches:\n(1) speech-level masking, making the model to mask more speech segments than\nsilence segments, (2) phoneme-level masking, forcing the model to mask the\nwhole frames of the phoneme, instead of phoneme pieces. We pre-trained the\nmodel via these two approaches, and evaluated on two downstream tasks, phoneme\nclassification and speaker recognition. The experiments demonstrated that the\nproposed masking approaches are beneficial to improve the performance of speech\nrepresentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xulong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Ning Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kexin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaSpeech: Speech Effects Switch Along with Environment for Metaverse. (arXiv:2210.13811v1 [cs.SD])","link":"http://arxiv.org/abs/2210.13811","description":"<p>Metaverse expands the physical world to a new dimension, and the physical\nenvironment and Metaverse environment can be directly connected and entered.\nVoice is an indispensable communication medium in the real world and Metaverse.\nFusion of the voice with environment effects is important for user immersion in\nMetaverse. In this paper, we proposed using the voice conversion based method\nfor the conversion of target environment effect speech. The proposed method was\nnamed MetaSpeech, which introduces an environment effect module containing an\neffect extractor to extract the environment information and an effect encoder\nto encode the environment effect condition, in which gradient reversal layer\nwas used for adversarial training to keep the speech content and speaker\ninformation while disentangling the environmental effects. From the experiment\nresults on the public dataset of LJSpeech with four environment effects, the\nproposed model could complete the specific environment effect conversion and\noutperforms the baseline methods from the voice conversion task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xulong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Ning Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Chinese Spelling Check Framework Based on Reverse Contrastive Learning. (arXiv:2210.13823v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13823","description":"<p>Chinese spelling check is a task to detect and correct spelling mistakes in\nChinese text. Existing research aims to enhance the text representation and use\nmulti-source information to improve the detection and correction capabilities\nof models, but does not pay too much attention to improving their ability to\ndistinguish between confusable words. Contrastive learning, whose aim is to\nminimize the distance in representation space between similar sample pairs, has\nrecently become a dominant technique in natural language processing. Inspired\nby contrastive learning, we present a novel framework for Chinese spelling\nchecking, which consists of three modules: language representation, spelling\ncheck and reverse contrastive learning. Specifically, we propose a reverse\ncontrastive learning strategy, which explicitly forces the model to minimize\nthe agreement between the similar examples, namely, the phonetically and\nvisually confusable characters. Experimental results show that our framework is\nmodel-agnostic and could be combined with existing Chinese spelling check\nmodels to yield state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1\">Sihui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiaotian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Aimin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information Filter upon Diversity-Improved Decoding for Diversity-Faithfulness Tradeoff in NLG. (arXiv:2210.13829v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13829","description":"<p>Some Natural Language Generation (NLG) tasks require both faithfulness and\ndiversity. The decoding strategy is intensively related to the quality of the\ngenerated text. Strategies such as beam search, greedy search, etc., perform\nwith low diversity and high repetition. On the other hand, guided decoding, the\nsolution towards diversity, may generate unfaithful expressions. To this end,\nthis paper presents Information Filter upon Diversity-Improved Decoding (IFDID)\nto obtain the tradeoff between diversity and faithfulness. IFDID is a two-stage\ndecoding strategy leveraging the proposed Enhance-Filter framework, which\nachieves the tradeoff by increasing the probabilities of some typical tokens\nbeing selected and subsequently filtering them by their information amount. To\nverify the effectiveness, we compare our method with other baselines on related\nCommonGEN, RocStories and AdGen benchmarks, which cover Chinese and English\ndatasets. Our numerical experimental results and human evaluation outcomes\nverify the effectiveness of the proposed approach, as our approach achieves a\n1.24 higher ROUGE score describing faithfulness as well as higher diversity\nrepresented by 62.5% higher upon Dist-2 than traditional approaches,\ndemonstrating that IFDID is a novel SOTA decoding strategy for the tradeoff\nbetween diversity and faithfulness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Han Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaosong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zexing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Feng Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FineD-Eval: Fine-grained Automatic Dialogue-Level Evaluation. (arXiv:2210.13832v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13832","description":"<p>Recent model-based reference-free metrics for open-domain dialogue evaluation\nexhibit promising correlations with human judgment. However, they either\nperform turn-level evaluation or look at a single dialogue quality dimension.\nOne would expect a good evaluation metric to assess multiple quality dimensions\nat the dialogue level. To this end, we are motivated to propose a\nmulti-dimensional dialogue-level metric, which consists of three sub-metrics\nwith each targeting a specific dimension. The sub-metrics are trained with\nnovel self-supervised objectives and exhibit strong correlations with human\njudgment for their respective dimensions. Moreover, we explore two approaches\nto combine the sub-metrics: metric ensemble and multitask learning. Both\napproaches yield a holistic metric that significantly outperforms individual\nsub-metrics. Compared to the existing state-of-the-art metric, the combined\nmetrics achieve around 16% relative improvement on average across three\nhigh-quality dialogue-level evaluation benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DHaro_L/0/1/0/all/0/1\">Luis Fernando D&#x27;Haro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiquan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrichs_T/0/1/0/all/0/1\">Thomas Friedrichs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deconfounding Legal Judgment Prediction for European Court of Human Rights Cases Towards Better Alignment with Experts. (arXiv:2210.13836v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13836","description":"<p>This work demonstrates that Legal Judgement Prediction systems without\nexpert-informed adjustments can be vulnerable to shallow, distracting surface\nsignals that arise from corpus construction, case distribution, and confounding\nfactors. To mitigate this, we use domain expertise to strategically identify\nstatistically predictive but legally irrelevant information. We adopt\nadversarial training to prevent the system from relying on it. We evaluate our\ndeconfounded models by employing interpretability techniques and comparing to\nexpert annotations. Quantitative experiments and qualitative analysis show that\nour deconfounded model consistently aligns better with expert rationales than\nbaselines trained for prediction only. We further contribute a set of reference\nexpert annotations to the validation and testing partitions of an existing\nbenchmark dataset of European Court of Human Rights cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santosh_T/0/1/0/all/0/1\">T.Y.S.S Santosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shanshan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichim_O/0/1/0/all/0/1\">Oana Ichim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grabmair_M/0/1/0/all/0/1\">Matthias Grabmair</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Relation Classification via Efficient and Effective Prompting. (arXiv:2210.13838v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13838","description":"<p>Prompting pre-trained language models has achieved impressive performance on\nvarious NLP tasks, especially in low data regimes. Despite the success of\nprompting in monolingual settings, applying prompt-based methods in\nmultilingual scenarios has been limited to a narrow set of tasks, due to the\nhigh cost of handcrafting multilingual prompts. In this paper, we present the\nfirst work on prompt-based multilingual relation classification (RC), by\nintroducing an efficient and effective method that constructs prompts from\nrelation triples and involves only minimal translation for the class labels. We\nevaluate its performance in fully supervised, few-shot and zero-shot scenarios,\nand analyze its effectiveness across 14 languages, prompt variants, and\nEnglish-task training in cross-lingual settings. We find that in both fully\nsupervised and few-shot scenarios, our prompt method beats competitive\nbaselines: fine-tuning XLM-R_EM and null prompts. It also outperforms the\nrandom baseline by a large margin in zero-shot experiments. Our method requires\nlittle in-language knowledge and can be used as a strong baseline for similar\nmultilingual classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harbecke_D/0/1/0/all/0/1\">David Harbecke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennig_L/0/1/0/all/0/1\">Leonhard Hennig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialogConv: A Lightweight Fully Convolutional Network for Multi-view Response Selection. (arXiv:2210.13845v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13845","description":"<p>Current end-to-end retrieval-based dialogue systems are mainly based on\nRecurrent Neural Networks or Transformers with attention mechanisms. Although\npromising results have been achieved, these models often suffer from slow\ninference or huge number of parameters. In this paper, we propose a novel\nlightweight fully convolutional architecture, called DialogConv, for response\nselection. DialogConv is exclusively built on top of convolution to extract\nmatching features of context and response. Dialogues are modeled in 3D views,\nwhere DialogConv performs convolution operations on embedding view, word view\nand utterance view to capture richer semantic information from multiple\ncontextual views. On the four benchmark datasets, compared with\nstate-of-the-art baselines, DialogConv is on average about 8.5x smaller in\nsize, and 79.39x and 10.64x faster on CPU and GPU devices, respectively. At the\nsame time, DialogConv achieves the competitive effectiveness of response\nselection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongkang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Missing Counter-Evidence Renders NLP Fact-Checking Unrealistic for Misinformation. (arXiv:2210.13865v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13865","description":"<p>Misinformation emerges in times of uncertainty when credible information is\nlimited. This is challenging for NLP-based fact-checking as it relies on\ncounter-evidence, which may not yet be available. Despite increasing interest\nin automatic fact-checking, it is still unclear if automated approaches can\nrealistically refute harmful real-world misinformation. Here, we contrast and\ncompare NLP fact-checking with how professional fact-checkers combat\nmisinformation in the absence of counter-evidence. In our analysis, we show\nthat, by design, existing NLP task definitions for fact-checking cannot refute\nmisinformation as professional fact-checkers do for the majority of claims. We\nthen define two requirements that the evidence in datasets must fulfill for\nrealistic fact-checking: It must be (1) sufficient to refute the claim and (2)\nnot leaked from existing fact-checking articles. We survey existing\nfact-checking datasets and find that all of them fail to satisfy both criteria.\nFinally, we perform experiments to demonstrate that models trained on a\nlarge-scale fact-checking dataset rely on leaked evidence, which makes them\nunsuitable in real-world scenarios. Taken together, we show that current NLP\nfact-checking cannot realistically combat real-world misinformation because it\ndepends on unrealistic assumptions about counter-evidence in the data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glockner_M/0/1/0/all/0/1\">Max Glockner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yufang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentially Private Language Models for Secure Data Sharing. (arXiv:2210.13918v1 [cs.LG])","link":"http://arxiv.org/abs/2210.13918","description":"<p>To protect the privacy of individuals whose data is being shared, it is of\nhigh importance to develop methods allowing researchers and companies to\nrelease textual data while providing formal privacy guarantees to its\noriginators. In the field of NLP, substantial efforts have been directed at\nbuilding mechanisms following the framework of local differential privacy,\nthereby anonymizing individual text samples before releasing them. In practice,\nthese approaches are often dissatisfying in terms of the quality of their\noutput language due to the strong noise required for local differential\nprivacy. In this paper, we approach the problem at hand using global\ndifferential privacy, particularly by training a generative language model in a\ndifferentially private manner and consequently sampling data from it. Using\nnatural language prompts and a new prompt-mismatch loss, we are able to create\nhighly accurate and fluent textual datasets taking on specific desired\nattributes such as sentiment or topic and resembling statistical properties of\nthe training data. We perform thorough experiments indicating that our\nsynthetic datasets do not leak information from our original data and are of\nhigh language quality and highly suitable for training models for further\nanalysis on real-world data. Notably, we also demonstrate that training\nclassifiers on private synthetic data outperforms directly training classifiers\non real data with DP-SGD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mattern_J/0/1/0/all/0/1\">Justus Mattern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weggenmann_B/0/1/0/all/0/1\">Benjamin Weggenmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoelkopf_B/0/1/0/all/0/1\">Bernhard Schoelkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Mechanism Priming Effects in Hindi Word Order. (arXiv:2210.13938v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13938","description":"<p>Word order choices during sentence production can be primed by preceding\nsentences. In this work, we test the DUAL MECHANISM hypothesis that priming is\ndriven by multiple different sources. Using a Hindi corpus of text productions,\nwe model lexical priming with an n-gram cache model and we capture more\nabstract syntactic priming with an adaptive neural language model. We permute\nthe preverbal constituents of corpus sentences, and then use a logistic\nregression model to predict which sentences actually occurred in the corpus\nagainst artificially generated meaning-equivalent variants. Our results\nindicate that lexical priming and lexically-independent syntactic priming\naffect complementary sets of verb classes. By showing that different priming\ninfluences are separable from one another, our results support the hypothesis\nthat multiple different cognitive mechanisms underlie priming.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ranjan_S/0/1/0/all/0/1\">Sidharth Ranjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schijndel_M/0/1/0/all/0/1\">Marten van Schijndel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sumeet Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajkumar_R/0/1/0/all/0/1\">Rajakrishnan Rajkumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discourse Context Predictability Effects in Hindi Word Order. (arXiv:2210.13940v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13940","description":"<p>We test the hypothesis that discourse predictability influences Hindi\nsyntactic choice. While prior work has shown that a number of factors (e.g.,\ninformation status, dependency length, and syntactic surprisal) influence Hindi\nword order preferences, the role of discourse predictability is underexplored\nin the literature. Inspired by prior work on syntactic priming, we investigate\nhow the words and syntactic structures in a sentence influence the word order\nof the following sentences. Specifically, we extract sentences from the\nHindi-Urdu Treebank corpus (HUTB), permute the preverbal constituents of those\nsentences, and build a classifier to predict which sentences actually occurred\nin the corpus against artificially generated distractors. The classifier uses a\nnumber of discourse-based features and cognitive features to make its\npredictions, including dependency length, surprisal, and information status. We\nfind that information status and LSTM-based discourse predictability influence\nword order choices, especially for non-canonical object-fronted orders. We\nconclude by situating our results within the broader syntactic priming\nliterature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ranjan_S/0/1/0/all/0/1\">Sidharth Ranjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schijndel_M/0/1/0/all/0/1\">Marten van Schijndel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sumeet Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajkumar_R/0/1/0/all/0/1\">Rajakrishnan Rajkumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity Divider with Language Grounding in Multi-Agent Reinforcement Learning. (arXiv:2210.13942v1 [cs.LG])","link":"http://arxiv.org/abs/2210.13942","description":"<p>We investigate the use of natural language to drive the generalization of\npolicies in multi-agent settings. Unlike single-agent settings, the\ngeneralization of policies should also consider the influence of other agents.\nBesides, with the increasing number of entities in multi-agent settings, more\nagent-entity interactions are needed for language grounding, and the enormous\nsearch space could impede the learning process. Moreover, given a simple\ngeneral instruction,e.g., beating all enemies, agents are required to decompose\nit into multiple subgoals and figure out the right one to focus on. Inspired by\nprevious work, we try to address these issues at the entity level and propose a\nnovel framework for language grounding in multi-agent reinforcement learning,\nentity divider (EnDi). EnDi enables agents to independently learn subgoal\ndivision at the entity level and act in the environment based on the associated\nentities. The subgoal division is regularized by opponent modeling to avoid\nsubgoal conflicts and promote coordinated strategies. Empirically, EnDi\ndemonstrates the strong generalization ability to unseen games with new\ndynamics and expresses the superiority over existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Ziluo Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wanpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_J/0/1/0/all/0/1\">Junpeng Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiangjun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zongqing Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KnowGL: Knowledge Generation and Linking from Text. (arXiv:2210.13952v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13952","description":"<p>We propose KnowGL, a tool that allows converting text into structured\nrelational data represented as a set of ABox assertions compliant with the TBox\nof a given Knowledge Graph (KG), such as Wikidata. We address this problem as a\nsequence generation task by leveraging pre-trained sequence-to-sequence\nlanguage models, e.g. BART. Given a sentence, we fine-tune such models to\ndetect pairs of entity mentions and jointly generate a set of facts consisting\nof the full set of semantic annotations for a KG, such as entity labels, entity\ntypes, and their relationships. To showcase the capabilities of our tool, we\nbuild a web application consisting of a set of UI widgets that help users to\nnavigate through the semantic data extracted from a given input text. We make\nthe KnowGL model available at https://huggingface.co/ibm/knowgl-large.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rossiello_G/0/1/0/all/0/1\">Gaetano Rossiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_F/0/1/0/all/0/1\">Faisal Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1\">Nandana Mihindukulasooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornec_O/0/1/0/all/0/1\">Owen Cornec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Gliozzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Reuse Distractors to support Multiple Choice Question Generation in Education. (arXiv:2210.13964v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13964","description":"<p>Multiple choice questions (MCQs) are widely used in digital learning systems,\nas they allow for automating the assessment process. However, due to the\nincreased digital literacy of students and the advent of social media\nplatforms, MCQ tests are widely shared online, and teachers are continuously\nchallenged to create new questions, which is an expensive and time-consuming\ntask. A particularly sensitive aspect of MCQ creation is to devise relevant\ndistractors, i.e., wrong answers that are not easily identifiable as being\nwrong. This paper studies how a large existing set of manually created answers\nand distractors for questions over a variety of domains, subjects, and\nlanguages can be leveraged to help teachers in creating new MCQs, by the smart\nreuse of existing distractors. We built several data-driven models based on\ncontext-aware question and distractor representations, and compared them with\nstatic feature-based models. The proposed models are evaluated with automated\nmetrics and in a realistic user test with teachers. Both automatic and human\nevaluations indicate that context-aware models consistently outperform a static\nfeature-based approach. For our best-performing context-aware model, on average\n3 distractors out of the 10 shown to teachers were rated as high-quality\ndistractors. We create a performance benchmark, and make it public, to enable\ncomparison between different approaches and to introduce a more standardized\nevaluation of the task. The benchmark contains a test of 298 educational\nquestions covering multiple subjects &amp; languages and a 77k multilingual pool of\ndistractor vocabulary for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bitew_S/0/1/0/all/0/1\">Semere Kiros Bitew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadifar_A/0/1/0/all/0/1\">Amir Hadifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sterckx_L/0/1/0/all/0/1\">Lucas Sterckx</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deleu_J/0/1/0/all/0/1\">Johannes Deleu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Develder_C/0/1/0/all/0/1\">Chris Develder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1\">Thomas Demeester</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-learning Pathologies from Radiology Reports using Variance Aware Prototypical Networks. (arXiv:2210.13979v1 [cs.LG])","link":"http://arxiv.org/abs/2210.13979","description":"<p>Large pretrained Transformer-based language models like BERT and GPT have\nchanged the landscape of Natural Language Processing (NLP). However, fine\ntuning such models still requires a large number of training examples for each\ntarget task, thus annotating multiple datasets and training these models on\nvarious downstream tasks becomes time consuming and expensive. In this work, we\npropose a simple extension of the Prototypical Networks for few-shot text\nclassification. Our main idea is to replace the class prototypes by Gaussians\nand introduce a regularization term that encourages the examples to be\nclustered near the appropriate class centroids. Experimental results show that\nour method outperforms various strong baselines on 13 public and 4 internal\ndatasets. Furthermore, we use the class distributions as a tool for detecting\npotential out-of-distribution (OOD) data points during deployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sehanobish_A/0/1/0/all/0/1\">Arijit Sehanobish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannan_K/0/1/0/all/0/1\">Kawshik Kannan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abraham_N/0/1/0/all/0/1\">Nabila Abraham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Anasuya Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Odry_B/0/1/0/all/0/1\">Benjamin Odry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"This joke is [MASK]: Recognizing Humor and Offense with Prompting. (arXiv:2210.13985v1 [cs.CL])","link":"http://arxiv.org/abs/2210.13985","description":"<p>Humor is a magnetic component in everyday human interactions and\ncommunications. Computationally modeling humor enables NLP systems to entertain\nand engage with users. We investigate the effectiveness of prompting, a new\ntransfer learning paradigm for NLP, for humor recognition. We show that\nprompting performs similarly to finetuning when numerous annotations are\navailable, but gives stellar performance in low-resource humor recognition. The\nrelationship between humor and offense is also inspected by applying influence\nfunctions to prompting; we show that models could rely on offense to determine\nhumor during transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengjie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yubo Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maronikolakis_A/0/1/0/all/0/1\">Antonis Maronikolakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_P/0/1/0/all/0/1\">Pearl Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are All Spurious Features in Natural Language Alike? An Analysis through a Causal Lens. (arXiv:2210.14011v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14011","description":"<p>The term `spurious correlations' has been used in NLP to informally denote\nany undesirable feature-label correlations. However, a correlation can be\nundesirable because (i) the feature is irrelevant to the label (e.g.\npunctuation in a review), or (ii) the feature's effect on the label depends on\nthe context (e.g. negation words in a review), which is ubiquitous in language\ntasks. In case (i), we want the model to be invariant to the feature, which is\nneither necessary nor sufficient for prediction. But in case (ii), even an\nideal model (e.g. humans) must rely on the feature, since it is necessary (but\nnot sufficient) for prediction. Therefore, a more fine-grained treatment of\nspurious features is needed to specify the desired model behavior. We formalize\nthis distinction using a causal model and probabilities of necessity and\nsufficiency, which delineates the causal relations between a feature and a\nlabel. We then show that this distinction helps explain results of existing\ndebiasing methods on different spurious features, and demystifies surprising\nresults such as the encoding of spurious features in model representations\nafter debiasing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Nitish Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Softmax for Uncertainty Approximation in Text Classification. (arXiv:2210.14037v1 [cs.LG])","link":"http://arxiv.org/abs/2210.14037","description":"<p>Uncertainty approximation in text classification is an important area with\napplications in domain adaptation and interpretability. The most widely used\nuncertainty approximation method is Monte Carlo Dropout, which is\ncomputationally expensive as it requires multiple forward passes through the\nmodel. A cheaper alternative is to simply use a softmax to estimate model\nuncertainty. However, prior work has indicated that the softmax can generate\noverconfident uncertainty estimates and can thus be tricked into producing\nincorrect predictions. In this paper, we perform a thorough empirical analysis\nof both methods on five datasets with two base neural architectures in order to\nreveal insight into the trade-offs between the two. We compare the methods'\nuncertainty approximations and downstream text classification performance,\nwhile weighing their performance against their computational complexity as a\ncost-benefit analysis, by measuring runtime (cost) and the downstream\nperformance (benefit). We find that, while Monte Carlo produces the best\nuncertainty approximations, using a simple softmax leads to competitive\nuncertainty estimation for text classification at a much lower computational\ncost, suggesting that softmax can in fact be a sufficient uncertainty estimate\nwhen computational resources are a concern.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Holm_A/0/1/0/all/0/1\">Andreas Nugaard Holm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wright_D/0/1/0/all/0/1\">Dustin Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From exemplar to copy: the scribal appropriation of a Hadewijch manuscript computationally explored. (arXiv:2210.14061v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14061","description":"<p>This study is devoted to two of the oldest known manuscripts in which the\noeuvre of the medieval mystical author Hadewijch has been preserved: Brussels,\nKBR, 2879-2880 (ms. A) and Brussels, KBR, 2877-2878 (ms. B). On the basis of\ncodicological and contextual arguments, it is assumed that the scribe who\nproduced B used A as an exemplar. While the similarities in both layout and\ncontent between the two manuscripts are striking, the present article seeks to\nidentify the differences. After all, regardless of the intention to produce a\ncopy that closely follows the exemplar, subtle linguistic variation is\napparent. Divergences relate to spelling conventions, but also to the way in\nwhich words are abbreviated (and the extent to which abbreviations occur). The\npresent study investigates the spelling profiles of the scribes who produced\nmss. A and B in a computational way. In the first part of this study, we will\npresent both manuscripts in more detail, after which we will consider prior\nresearch carried out on scribal profiling. The current study both builds and\nexpands on Kestemont (2015). Next, we outline the methodology used to analyse\nand measure the degree of scribal appropriation that took place when ms. B was\ncopied off the exemplar ms. A. After this, we will discuss the results\nobtained, focusing on the scribal variation that can be found both at the level\nof individual words and n-grams. To this end, we use machine learning to\nidentify the most distinctive features that separate manuscript A from B.\nFinally, we look at possible diachronic trends in the appropriation by B's\nscribe of his exemplar. We argue that scribal takeovers in the exemplar impacts\nthe practice of the copying scribe, while transitions to a different content\nmatter cause little to no effect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haverals_W/0/1/0/all/0/1\">Wouter Haverals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kestemont_M/0/1/0/all/0/1\">Mike Kestemont</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Mode Connectivity for Pre-trained Language Models. (arXiv:2210.14102v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14102","description":"<p>Recent years have witnessed the prevalent application of pre-trained language\nmodels (PLMs) in NLP. From the perspective of parameter space, PLMs provide\ngeneric initialization, starting from which high-performance minima could be\nfound. Although plenty of works have studied how to effectively and efficiently\nadapt PLMs to high-performance minima, little is known about the connection of\nvarious minima reached under different adaptation configurations. In this\npaper, we investigate the geometric connections of different minima through the\nlens of mode connectivity, which measures whether two minima can be connected\nwith a low-loss path. We conduct empirical analyses to investigate three\nquestions: (1) how could hyperparameters, specific tuning methods, and training\ndata affect PLM's mode connectivity? (2) How does mode connectivity change\nduring pre-training? (3) How does the PLM's task knowledge change along the\npath connecting two minima? In general, exploring the mode connectivity of PLMs\nconduces to understanding the geometric connection of different minima, which\nmay help us fathom the inner workings of PLM downstream adaptation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Cheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jing Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weize Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IELM: An Open Information Extraction Benchmark for Pre-Trained Language Models. (arXiv:2210.14128v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14128","description":"<p>We introduce a new open information extraction (OIE) benchmark for\npre-trained language models (LM). Recent studies have demonstrated that\npre-trained LMs, such as BERT and GPT, may store linguistic and relational\nknowledge. In particular, LMs are able to answer ``fill-in-the-blank''\nquestions when given a pre-defined relation category. Instead of focusing on\npre-defined relations, we create an OIE benchmark aiming to fully examine the\nopen relational information present in the pre-trained LMs. We accomplish this\nby turning pre-trained LMs into zero-shot OIE systems. Surprisingly,\npre-trained LMs are able to obtain competitive performance on both standard OIE\ndatasets (CaRB and Re-OIE2016) and two new large-scale factual OIE datasets\n(TAC KBP-OIE and Wikidata-OIE) that we establish via distant supervision. For\ninstance, the zero-shot pre-trained LMs outperform the F1 score of the\nstate-of-the-art supervised OIE methods on our factual OIE datasets without\nneeding to use any training sets. Our code and datasets are available at\nhttps://github.com/cgraywang/IELM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PolyHope: Dataset Creation for a Two-Level Hope Speech Detection Task from Tweets. (arXiv:2210.14136v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14136","description":"<p>Hope is characterized as openness of spirit toward the future, a desire,\nexpectation, and wish for something to happen or to be true that remarkably\naffects human's state of mind, emotions, behaviors, and decisions. Hope is\nusually associated with concepts of desired expectations and\npossibility/probability concerning the future. Despite its importance, hope has\nrarely been studied as a social media analysis task. This paper presents a hope\nspeech dataset that classifies each tweet first into \"Hope\" and \"Not Hope\",\nthen into three fine-grained hope categories: \"Generalized Hope\", \"Realistic\nHope\", and \"Unrealistic Hope\" (along with \"Not Hope\"). English tweets in the\nfirst half of 2022 were collected to build this dataset. Furthermore, we\ndescribe our annotation process and guidelines in detail and discuss the\nchallenges of classifying hope and the limitations of the existing hope speech\ndetection corpora. In addition, we reported several baselines based on\ndifferent learning approaches, such as traditional machine learning, deep\nlearning, and transformers, to benchmark our dataset. We evaluated our\nbaselines using weighted-averaged and macro-averaged F1-scores. Observations\nshow that a strict process for annotator selection and detailed annotation\nguidelines enhanced the dataset's quality. This strict annotation process\nresulted in promising performance for simple machine learning classifiers with\nonly bi-grams; however, binary and multiclass hope speech detection results\nreveal that contextual embedding models have higher performance in this\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balouchzahi_F/0/1/0/all/0/1\">Fazlourrahman Balouchzahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidorov_G/0/1/0/all/0/1\">Grigori Sidorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1\">Alexander Gelbukh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Search Is What You Need For Neural Text Generation. (arXiv:2210.14140v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14140","description":"<p>Generating text with autoregressive language models (LMs) is of great\nimportance to many natural language processing (NLP) applications. Previous\nsolutions for this task often produce text that contains degenerative\nexpressions or lacks semantic consistency. Recently, Su et al. introduced a new\ndecoding method, contrastive search, based on the isotropic representation\nspace of the language model and obtained new state of the art on various\nbenchmarks. Additionally, Su et al. argued that the representations of\nautoregressive LMs (e.g. GPT-2) are intrinsically anisotropic which is also\nshared by previous study. Therefore, to ensure the language model follows an\nisotropic distribution, Su et al. proposed a contrastive learning scheme,\nSimCTG, which calibrates the language model's representations through\nadditional training.\n</p>\n<p>In this study, we first answer the question: \"Are autoregressive LMs really\nanisotropic?\". To this end, we extensively evaluate the isotropy of LMs across\n16 major languages. Surprisingly, we find that the anisotropic problem only\nexists in the two specific English GPT-2-small/medium models. On the other\nhand, all other evaluated LMs are naturally isotropic which is in contrast to\nthe conclusion drawn by previous studies. Based on our findings, we further\nassess the contrastive search decoding method using off-the-shelf LMs on four\ngeneration tasks across 16 languages. Our experimental results demonstrate that\ncontrastive search significantly outperforms previous decoding methods without\nany additional training. More notably, on 12 out of 16 evaluated languages,\ncontrastive search performs comparably with human-level performances as judged\nby human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding. (arXiv:2210.14169v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14169","description":"<p>Dialogue understanding tasks often necessitate abundant annotated data to\nachieve good performance and that presents challenges in low-resource settings.\nTo alleviate this barrier, we explore few-shot data augmentation for dialogue\nunderstanding by prompting large pre-trained language models and present a\nnovel approach that iterates on augmentation quality by applying\nweakly-supervised filters. We evaluate our methods on the emotion and act\nclassification tasks in DailyDialog and the intent classification task in\nFacebook Multilingual Task-Oriented Dialogue. Models fine-tuned on our\naugmented data mixed with few-shot ground truth data are able to approach or\nsurpass existing state-of-the-art performance on both datasets. For DailyDialog\nspecifically, using 10% of the ground truth data we outperform the current\nstate-of-the-art model which uses 100% of the data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Maximillian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papangelis_A/0/1/0/all/0/1\">Alexandros Papangelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chenyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenbaum_A/0/1/0/all/0/1\">Andy Rosenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Interpretable Summary Evaluation via Allocation of Contextual Embeddings to Reference Text Topics. (arXiv:2210.14174v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14174","description":"<p>Despite extensive recent advances in summary generation models, evaluation of\nauto-generated summaries still widely relies on single-score systems\ninsufficient for transparent assessment and in-depth qualitative analysis.\nTowards bridging this gap, we propose the multifaceted interpretable summary\nevaluation method (MISEM), which is based on allocation of a summary's\ncontextual token embeddings to semantic topics identified in the reference\ntext. We further contribute an interpretability toolbox for automated summary\nevaluation and interactive visual analysis of summary scoring, topic\nidentification, and token-topic allocation. MISEM achieves a promising .404\nPearson correlation with human judgment on the TAC'08 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schaper_B/0/1/0/all/0/1\">Ben Schaper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lohse_C/0/1/0/all/0/1\">Christopher Lohse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Streile_M/0/1/0/all/0/1\">Marcell Streile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giovannini_A/0/1/0/all/0/1\">Andrea Giovannini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osuala_R/0/1/0/all/0/1\">Richard Osuala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Influence Functions for Sequence Tagging Models. (arXiv:2210.14177v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14177","description":"<p>Many language tasks (e.g., Named Entity Recognition, Part-of-Speech tagging,\nand Semantic Role Labeling) are naturally framed as sequence tagging problems.\nHowever, there has been comparatively little work on interpretability methods\nfor sequence tagging models. In this paper, we extend influence functions -\nwhich aim to trace predictions back to the training points that informed them -\nto sequence tagging tasks. We define the influence of a training instance\nsegment as the effect that perturbing the labels within this segment has on a\ntest segment level prediction. We provide an efficient approximation to compute\nthis, and show that it tracks with the true segment influence, measured\nempirically. We show the practical utility of segment influence by using the\nmethod to identify systematic annotation errors in two named entity recognition\ncorpora. Code to reproduce our results is available at\nhttps://github.com/successar/Segment_Influence_Functions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sarthak Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manjunatha_V/0/1/0/all/0/1\">Varun Manjunatha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenkova_A/0/1/0/all/0/1\">Ani Nenkova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrisisLTLSum: A Benchmark for Local Crisis Event Timeline Extraction and Summarization. (arXiv:2210.14190v1 [cs.CL])","link":"http://arxiv.org/abs/2210.14190","description":"<p>Social media has increasingly played a key role in emergency response: first\nresponders can use public posts to better react to ongoing crisis events and\ndeploy the necessary resources where they are most needed. Timeline extraction\nand abstractive summarization are critical technical tasks to leverage large\nnumbers of social media posts about events. Unfortunately, there are few\ndatasets for benchmarking technical approaches for those tasks. This paper\npresents CrisisLTLSum, the largest dataset of local crisis event timelines\navailable to date. CrisisLTLSum contains 1,000 crisis event timelines across\nfour domains: wildfires, local fires, traffic, and storms. We built\nCrisisLTLSum using a semi-automated cluster-then-refine approach to collect\ndata from the public Twitter stream. Our initial experiments indicate a\nsignificant gap between the performance of strong baselines compared to the\nhuman performance on both tasks. Our dataset, code, and models are publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faghihi_H/0/1/0/all/0/1\">Hossein Rajaby Faghihi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhafni_B/0/1/0/all/0/1\">Bashar Alhafni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Ke Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ran_S/0/1/0/all/0/1\">Shihao Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tetreault_J/0/1/0/all/0/1\">Joel Tetreault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaimes_A/0/1/0/all/0/1\">Alejandro Jaimes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Differentiable Relaxation of Graph Segmentation and Alignment for AMR Parsing. (arXiv:2010.12676v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.12676","description":"<p>Abstract Meaning Representations (AMR) are a broad-coverage semantic\nformalism which represents sentence meaning as a directed acyclic graph. To\ntrain most AMR parsers, one needs to segment the graph into subgraphs and align\neach such subgraph to a word in a sentence; this is normally done at\npreprocessing, relying on hand-crafted rules. In contrast, we treat both\nalignment and segmentation as latent variables in our model and induce them as\npart of end-to-end training.\n</p>\n<p>As marginalizing over the structured latent variables is infeasible, we use\nthe variational autoencoding framework.\n</p>\n<p>To ensure end-to-end differentiable optimization, we introduce a\ndifferentiable relaxation of the segmentation and alignment problems. We\nobserve that inducing segmentation yields substantial gains over using a\n`greedy' segmentation heuristic. The performance of our method also approaches\nthat of a model that relies on the segmentation rules of\n\\citet{lyu-titov-2018-amr}, which were hand-crafted to handle individual AMR\nconstructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1\">Chunchuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Shay B. Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1\">Ivan Titov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advanced Semantics for Commonsense Knowledge Extraction. (arXiv:2011.00905v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2011.00905","description":"<p>Commonsense knowledge (CSK) about concepts and their properties is useful for\nAI applications such as robust chatbots. Prior works like ConceptNet, TupleKB\nand others compiled large CSK collections, but are restricted in their\nexpressiveness to subject-predicate-object (SPO) triples with simple concepts\nfor S and monolithic strings for P and O. Also, these projects have either\nprioritized precision or recall, but hardly reconcile these complementary\ngoals. This paper presents a methodology, called Ascent, to automatically build\na large-scale knowledge base (KB) of CSK assertions, with advanced\nexpressiveness and both better precision and recall than prior works. Ascent\ngoes beyond triples by capturing composite concepts with subgroups and aspects,\nand by refining assertions with semantic facets. The latter are important to\nexpress temporal and spatial validity of assertions and further qualifiers.\nAscent combines open information extraction with judicious cleaning using\nlanguage models. Intrinsic evaluation shows the superior size and quality of\nthe Ascent KB, and an extrinsic evaluation for QA-support tasks underlines the\nbenefits of Ascent. A web interface, data and code can be found at\nhttps://ascent.mpi-inf.mpg.de/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan-Phong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1\">Simon Razniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Interpretations for Explainable Natural Language Processing: A Survey. (arXiv:2103.11072v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.11072","description":"<p>As the use of deep learning techniques has grown across various fields over\nthe past decade, complaints about the opaqueness of the black-box models have\nincreased, resulting in an increased focus on transparency in deep learning\nmodels. This work investigates various methods to improve the interpretability\nof deep neural networks for natural language processing (NLP) tasks, including\nmachine translation and sentiment analysis. We provide a comprehensive\ndiscussion on the definition of the term \\textit{interpretability} and its\nvarious aspects at the beginning of this work. The methods collected and\nsummarised in this survey are only associated with local interpretation and are\ndivided into three categories: 1) explaining the model's predictions through\nrelated input features; 2) explaining through natural language explanation; 3)\nprobing the hidden states of models and word representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Siwen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivison_H/0/1/0/all/0/1\">Hamish Ivison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Caren Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_J/0/1/0/all/0/1\">Josiah Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Temporal Event Relation with Syntax-guided Graph Transformer. (arXiv:2104.09570v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.09570","description":"<p>Extracting temporal relations (e.g., before, after, and simultaneous) among\nevents is crucial to natural language understanding. One of the key challenges\nof this problem is that when the events of interest are far away in text, the\ncontext in-between often becomes complicated, making it challenging to resolve\nthe temporal relationship between them. This paper thus proposes a new\nSyntax-guided Graph Transformer network (SGT) to mitigate this issue, by (1)\nexplicitly exploiting the connection between two events based on their\ndependency parsing trees, and (2) automatically locating temporal cues between\ntwo events via a novel syntax-guided attention mechanism. Experiments on two\nbenchmark datasets, MATRES and TB-Dense, show that our approach significantly\noutperforms previous state-of-the-art methods on both end-to-end temporal\nrelation extraction and temporal relation classification; This improvement also\nproves to be robust on the contrast set of MATRES. The code is publicly\navailable at https://github.com/VT-NLP/Syntax-Guided-Graph-Transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuaicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_Q/0/1/0/all/0/1\">Qiang Ning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sampling-Based Approximations to Minimum Bayes Risk Decoding for Neural Machine Translation. (arXiv:2108.04718v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.04718","description":"<p>In NMT we search for the mode of the model distribution to form predictions.\nThe mode and other high-probability translations found by beam search have been\nshown to often be inadequate in a number of ways. This prevents improving\ntranslation quality through better search, as these idiosyncratic translations\nend up selected by the decoding algorithm, a problem known as the beam search\ncurse. Recently, an approximation to minimum Bayes risk (MBR) decoding has been\nproposed as an alternative decision rule that would likely not suffer from the\nsame problems. We analyse this approximation and establish that it has no\nequivalent to the beam search curse. We then design approximations that\ndecouple the cost of exploration from the cost of robust estimation of expected\nutility. This allows for much larger hypothesis spaces, which we show to be\nbeneficial. We also show that mode-seeking strategies can aid in constructing\ncompact sets of promising hypotheses and that MBR is effective in identifying\ngood translations in them. We conduct experiments on three language pairs\nvarying in amounts of resources available: English into and from German,\nRomanian, and Nepali.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eikema_B/0/1/0/all/0/1\">Bryan Eikema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1\">Wilker Aziz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Neural Networks for Nomination and Representation Learning of Web Elements. (arXiv:2111.02168v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.02168","description":"<p>This paper tackles the under-explored problem of DOM element nomination and\nrepresentation learning with three important contributions. First, we present a\nlarge-scale and realistic dataset of webpages, far richer and more diverse than\nother datasets proposed for element representation learning, classification and\nnomination on the web. The dataset contains $51,701$ manually labeled product\npages from $8,175$ real e-commerce websites. Second, we adapt several Graph\nNeural Network (GNN) architectures to website DOM trees and benchmark their\nperformance on a diverse set of element nomination tasks using our proposed\ndataset. In element nomination, a single element on a page is selected for a\ngiven class. We show that on our challenging dataset a simple Convolutional GNN\noutperforms state-of-the-art methods on web element nomination. Finally, we\npropose a new training method that further boosts the element nomination\naccuracy. In nomination for the web, classification (assigning a class to a\ngiven element) is usually used as a surrogate objective for nomination during\ntraining. Our novel training methodology steers the classification objective\ntowards the more complex and useful nomination objective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hotti_A/0/1/0/all/0/1\">Alexandra Hotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risuleo_R/0/1/0/all/0/1\">Riccardo Sven Risuleo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magureanu_S/0/1/0/all/0/1\">Stefan Magureanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_A/0/1/0/all/0/1\">Aref Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lagergren_J/0/1/0/all/0/1\">Jens Lagergren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAGMA -- Multimodal Augmentation of Generative Models through Adapter-based Finetuning. (arXiv:2112.05253v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05253","description":"<p>Large-scale pretraining is fast becoming the norm in Vision-Language (VL)\nmodeling. However, prevailing VL approaches are limited by the requirement for\nlabeled data and the use of complex multi-step pretraining objectives. We\npresent MAGMA - a simple method for augmenting generative language models with\nadditional modalities using adapter-based finetuning. Building on Frozen, we\ntrain a series of VL models that autoregressively generate text from arbitrary\ncombinations of visual and textual input. The pretraining is entirely\nend-to-end using a single language modeling objective, simplifying optimization\ncompared to previous approaches. Importantly, the language model weights remain\nunchanged during training, allowing for transfer of encyclopedic knowledge and\nin-context learning abilities from language pretraining. MAGMA outperforms\nFrozen on open-ended generative tasks, achieving state of the art results on\nthe OKVQA benchmark and competitive results on a range of other popular VL\nbenchmarks, while pretraining on 0.2% of the number of samples used to train\nSimVLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eichenberg_C/0/1/0/all/0/1\">Constantin Eichenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_S/0/1/0/all/0/1\">Sidney Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinbach_S/0/1/0/all/0/1\">Samuel Weinbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parcalabescu_L/0/1/0/all/0/1\">Letitia Parcalabescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1\">Anette Frank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Student Knows All Experts Know: From Sparse to Dense. (arXiv:2201.10890v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.10890","description":"<p>Human education system trains one student by multiple experts.\nMixture-of-experts (MoE) is a powerful sparse architecture including multiple\nexperts. However, sparse MoE model is easy to overfit, hard to deploy, and not\nhardware-friendly for practitioners. In this work, inspired by the human\neducation model, we propose a novel task, knowledge integration, to obtain a\ndense student model (OneS) as knowledgeable as one sparse MoE. We investigate\nthis task by proposing a general training framework including knowledge\ngathering and knowledge distillation. Specifically, to gather key knowledge\nfrom different pre-trained experts, we first investigate four different\npossible knowledge gathering methods, \\ie summation, averaging, Top-K Knowledge\nGathering (Top-KG), and Singular Value Decomposition Knowledge Gathering\n(SVD-KG) proposed in this paper. We then refine the dense student model by\nknowledge distillation to offset the noise from gathering. On ImageNet, our\nOneS preserves $61.7\\%$ benefits from MoE and achieves $78.4\\%$ top-1 accuracy\nImageNet with only $15$M parameters. On four natural language processing\ndatasets, OneS obtains $88.2\\%$ MoE benefits and outperforms the best baseline\nby $51.7\\%$ using the same architecture and training data. In addition,\ncompared with the MoE counterpart, OneS can achieve $3.7 \\times$ inference\nspeedup due to less computation and hardware-friendly architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaoxin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yuxuan Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Has Been Enhanced in my Knowledge-Enhanced Language Model?. (arXiv:2202.00964v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00964","description":"<p>We show that existing model interpretation methods such as linear probes and\nprompts have some key limitations in answering these questions. We revisit KI\nfrom an information-theoretic view and propose a new theoretically sound probe\ncalled Graph Convolution Simulator (GCS) for KI interpretation. GCS uses graph\nattention on the corresponding knowledge graph for interpretation. In our\nexperiments we verify that GCS can provide reasonable interpretation results\nfor two well-known knowledge-enhanced LMs: ERNIE and K-Adapter. We also find\nthat only a marginal amount of knowledge is successfully integrated in these\nmodels, and simply increasing the size of the KI corpus may not lead to better\nknowledge-enhanced LMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yifan Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1\">Guoji Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transforming Sequence Tagging Into A Seq2Seq Task. (arXiv:2203.08378v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08378","description":"<p>Pretrained, large, generative language models (LMs) have had great success in\na wide range of sequence tagging and structured prediction tasks. Casting a\nsequence tagging task as a Seq2Seq one requires deciding the formats of the\ninput and output sequences. However, we lack a principled understanding of the\ntrade-offs associated with these formats (such as the effect on model accuracy,\nsequence length, multilingual generalization, hallucination). In this paper, we\nrigorously study different formats one could use for casting input text\nsentences and their output labels into the input and target (i.e., output) of a\nSeq2Seq model. Along the way, we introduce a new format, which we show to to be\nboth simpler and more effective. Additionally the new format demonstrates\nsignificant gains in the multilingual settings -- both zero-shot transfer\nlearning and joint training. Lastly, we find that the new format is more robust\nand almost completely devoid of hallucination -- an issue we find common in\nexisting formats. With well over a 1000 experiments studying 14 different\nformats, over 7 diverse public benchmarks -- including 3 multilingual datasets\nspanning 7 languages -- we believe our findings provide a strong empirical\nbasis in understanding how we should tackle sequence tagging tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raman_K/0/1/0/all/0/1\">Karthik Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naim_I/0/1/0/all/0/1\">Iftekhar Naim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiecao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1\">Kazuma Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yalasangi_K/0/1/0/all/0/1\">Kiran Yalasangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_K/0/1/0/all/0/1\">Krishna Srinivasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Transformer-based Models for Long Document Classification. (arXiv:2204.06683v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06683","description":"<p>The recent literature in text classification is biased towards short text\nsequences (e.g., sentences or paragraphs). In real-world applications,\nmulti-page multi-paragraph documents are common and they cannot be efficiently\nencoded by vanilla Transformer-based models. We compare different\nTransformer-based Long Document Classification (TrLDC) approaches that aim to\nmitigate the computational overhead of vanilla transformers to encode much\nlonger text, namely sparse attention and hierarchical encoding methods. We\nexamine several aspects of sparse attention (e.g., size of local attention\nwindow, use of global attention) and hierarchical (e.g., document splitting\nstrategy) transformers on four document classification datasets covering\ndifferent domains. We observe a clear benefit from being able to process longer\ntext, and, based on our results, we derive practical advice of applying\nTransformer-based models on long document classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darkner_S/0/1/0/all/0/1\">Sune Darkner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1\">Desmond Elliott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label-aware Multi-level Contrastive Learning for Cross-lingual Spoken Language Understanding. (arXiv:2205.03656v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.03656","description":"<p>Despite the great success of spoken language understanding (SLU) in\nhigh-resource languages, it remains challenging in low-resource languages\nmainly due to the lack of labeled training data. The recent multilingual\ncode-switching approach achieves better alignments of model representations\nacross languages by constructing a mixed-language context in zero-shot\ncross-lingual SLU. However, current code-switching methods are limited to\nimplicit alignment and disregard the inherent semantic structure in SLU, i.e.,\nthe hierarchical inclusion of utterances, slots, and words. In this paper, we\npropose to model the utterance-slot-word structure by a multi-level contrastive\nlearning framework at the utterance, slot, and word levels to facilitate\nexplicit alignment. Novel code-switching schemes are introduced to generate\nhard negative examples for our contrastive learning framework. Furthermore, we\ndevelop a label-aware joint model leveraging label semantics to enhance the\nimplicit alignment and feed to contrastive learning. Our experimental results\nshow that our proposed methods significantly improve the performance compared\nwith the strong baselines on two zero-shot cross-lingual SLU benchmark\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shining Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_L/0/1/0/all/0/1\">Linjun Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Ming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wanli Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xianglin Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not to Overfit or Underfit the Source Domains? An Empirical Study of Domain Generalization in Question Answering. (arXiv:2205.07257v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.07257","description":"<p>Machine learning models are prone to overfitting their training (source)\ndomains, which is commonly believed to be the reason why they falter in novel\ntarget domains. Here we examine the contrasting view that multi-source domain\ngeneralization (DG) is first and foremost a problem of mitigating source domain\nunderfitting: models not adequately learning the signal already present in\ntheir multi-domain training data. Experiments on a reading comprehension DG\nbenchmark show that as a model learns its source domains better -- using\nfamiliar methods such as knowledge distillation (KD) from a bigger model -- its\nzero-shot out-of-domain utility improves at an even faster pace. Improved\nsource domain learning also demonstrates superior out-of-domain generalization\nover three popular existing DG approaches that aim to limit overfitting. Our\nimplementation of KD-based domain generalization is available via PrimeQA at:\nhttps://ibm.biz/domain-generalization-with-kd.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sultan_M/0/1/0/all/0/1\">Md Arafat Sultan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sil_A/0/1/0/all/0/1\">Avirup Sil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florian_R/0/1/0/all/0/1\">Radu Florian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Tracing Factual Knowledge in Language Models Back to the Training Data. (arXiv:2205.11482v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11482","description":"<p>Language models (LMs) have been shown to memorize a great deal of factual\nknowledge contained in their training data. But when an LM generates an\nassertion, it is often difficult to determine where it learned this information\nand whether it is true. In this paper, we propose the problem of fact tracing:\nidentifying which training examples taught an LM to generate a particular\nfactual assertion. Prior work on training data attribution (TDA) may offer\neffective tools for identifying such examples, known as \"proponents\". We\npresent the first quantitative benchmark to evaluate this. We compare two\npopular families of TDA methods -- gradient-based and embedding-based -- and\nfind that much headroom remains. For example, both methods have lower\nproponent-retrieval precision than an information retrieval baseline (BM25)\nthat does not have access to the LM at all. We identify key challenges that may\nbe necessary for further improvement such as overcoming the problem of gradient\nsaturation, and also show how several nuanced implementation details of\nexisting neural TDA methods can significantly improve overall fact tracing\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akyurek_E/0/1/0/all/0/1\">Ekin Aky&#xfc;rek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolukbasi_T/0/1/0/all/0/1\">Tolga Bolukbasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Frederick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_B/0/1/0/all/0/1\">Binbin Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenney_I/0/1/0/all/0/1\">Ian Tenney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guu_K/0/1/0/all/0/1\">Kelvin Guu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations. (arXiv:2205.11822v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11822","description":"<p>Despite their impressive capabilities, large pre-trained language models\n(LMs) struggle with consistent reasoning; recently, prompting LMs to generate\nexplanations that self-guide the inference has emerged as a promising direction\nto amend this. However, these approaches are fundamentally bounded by the\ncorrectness of explanations, which themselves are often noisy and inconsistent.\nIn this work, we develop Maieutic Prompting, which infers a correct answer to a\nquestion even from the noisy and inconsistent generations of LM. Maieutic\nPrompting induces a tree of explanations abductively (e.g. X is true, because\n...) and recursively, then frames the inference as a satisfiability problem\nover these explanations and their logical relations. We test Maieutic Prompting\nfor true/false QA on three challenging benchmarks that require complex\ncommonsense reasoning. Maieutic Prompting achieves up to 20% better accuracy\nthan state-of-the-art prompting methods, and as a fully unsupervised approach,\nperforms competitively with supervised models. We also show that Maieutic\nPrompting improves robustness in inference while providing interpretable\nrationales.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1\">Jaehun Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Lianhui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1\">Sean Welleck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyper-X: A Unified Hypernetwork for Multi-Task Multilingual Transfer. (arXiv:2205.12148v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12148","description":"<p>Massively multilingual models are promising for transfer learning across\ntasks and languages. However, existing methods are unable to fully leverage\ntraining data when it is available in different task-language combinations. To\nexploit such heterogeneous supervision, we propose Hyper-X, a single\nhypernetwork that unifies multi-task and multilingual learning with efficient\nadaptation. This model generates weights for adapter modules conditioned on\nboth tasks and language embeddings. By learning to combine task and\nlanguage-specific knowledge, our model enables zero-shot transfer for unseen\nlanguages and task-language combinations. Our experiments on a diverse set of\nlanguages demonstrate that Hyper-X achieves the best or competitive gain when a\nmixture of multiple resources is available, while being on par with strong\nbaselines in the standard scenario. Hyper-X is also considerably more efficient\nin terms of parameters and resources compared to methods that train separate\nadapters. Finally, Hyper-X consistently produces strong results in few-shot\nscenarios for new languages, showing the versatility of our approach beyond\nzero-shot transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ustun_A/0/1/0/all/0/1\">Ahmet &#xdc;st&#xfc;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisazza_A/0/1/0/all/0/1\">Arianna Bisazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouma_G/0/1/0/all/0/1\">Gosse Bouma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noord_G/0/1/0/all/0/1\">Gertjan van Noord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing. (arXiv:2205.12253v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12253","description":"<p>Despite their strong performance on many tasks, pre-trained language models\nhave been shown to struggle on out-of-distribution compositional\ngeneralization. Meanwhile, recent work has shown considerable improvements on\nmany NLP tasks from model scaling. Can scaling up model size also improve\ncompositional generalization in semantic parsing? We evaluate encoder-decoder\nmodels up to 11B parameters and decoder-only models up to 540B parameters, and\ncompare model scaling curves for three different methods for applying a\npre-trained language model to a new task: fine-tuning all parameters, prompt\ntuning, and in-context learning. We observe that fine-tuning generally has flat\nor negative scaling curves on out-of-distribution compositional generalization\nin semantic parsing evaluations. In-context learning has positive scaling\ncurves, but is generally outperformed by much smaller fine-tuned models.\nPrompt-tuning can outperform fine-tuning, suggesting further potential\nimprovements from scaling as it exhibits a more positive scaling curve.\nAdditionally, we identify several error trends that vary with model scale. For\nexample, larger models are generally better at modeling the syntax of the\noutput space, but are also more prone to certain types of overfitting. Overall,\nour study highlights limitations of current techniques for effectively\nleveraging model scale for compositional generalization, while our analysis\nalso suggests promising directions for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Linlu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_P/0/1/0/all/0/1\">Peter Shaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasupat_P/0/1/0/all/0/1\">Panupong Pasupat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1\">Tianze Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1\">Jonathan Herzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pitler_E/0/1/0/all/0/1\">Emily Pitler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1\">Fei Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toutanova_K/0/1/0/all/0/1\">Kristina Toutanova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenges and Opportunities in Information Manipulation Detection: An Examination of Wartime Russian Media. (arXiv:2205.12382v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12382","description":"<p>NLP research on public opinion manipulation campaigns has primarily focused\non detecting overt strategies such as fake news and disinformation. However,\ninformation manipulation in the ongoing Russia-Ukraine war exemplifies how\ngovernments and media also employ more nuanced strategies. We release a new\ndataset, VoynaSlov, containing 38M+ posts from Russian media outlets on Twitter\nand VKontakte, as well as public activity and responses, immediately preceding\nand during the 2022 Russia-Ukraine war. We apply standard and\nrecently-developed NLP models on VoynaSlov to examine agenda setting, framing,\nand priming, several strategies underlying information manipulation, and reveal\nvariation across media outlet control, social media platform, and time. Our\nexamination of these media effects and extensive discussion of current\napproaches' limitations encourage further development of NLP models for\nunderstanding information manipulation in emerging crises, as well as other\nreal-world and interdisciplinary tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chan Young Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendelsohn_J/0/1/0/all/0/1\">Julia Mendelsohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Field_A/0/1/0/all/0/1\">Anjalie Field</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly Supervised Text Classification. (arXiv:2205.12528v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12528","description":"<p>Weakly supervised text classification methods typically train a deep neural\nclassifier based on pseudo-labels. The quality of pseudo-labels is crucial to\nfinal performance but they are inevitably noisy due to their heuristic nature,\nso selecting the correct ones has a huge potential for performance boost. One\nstraightforward solution is to select samples based on the softmax probability\nscores in the neural classifier corresponding to their pseudo-labels. However,\nwe show through our experiments that such solutions are ineffective and\nunstable due to the erroneously high-confidence predictions from poorly\ncalibrated models. Recent studies on the memorization effects of deep neural\nmodels suggest that these models first memorize training samples with clean\nlabels and then those with noisy labels. Inspired by this observation, we\npropose a novel pseudo-label selection method LOPS that takes learning order of\nsamples into consideration. We hypothesize that the learning order reflects the\nprobability of wrong annotation in terms of ranking, and therefore, propose to\nselect the samples that are learnt earlier. LOPS can be viewed as a strong\nperformance-boost plug-in to most of existing weakly-supervised text\nclassification methods, as confirmed in extensive experiments on four\nreal-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mekala_D/0/1/0/all/0/1\">Dheeraj Mekala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chengyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging QA Datasets to Improve Generative Data Augmentation. (arXiv:2205.12604v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12604","description":"<p>The ability of generative language models (GLMs) to generate text has\nimproved considerably in the last few years, enabling their use for generative\ndata augmentation. In this work, we propose CONDA, an approach to further\nimprove GLMs' ability to generate synthetic data by reformulating data\ngeneration as context generation for a given question-answer (QA) pair and\nleveraging QA datasets for training context generators. Then, we cast\ndownstream tasks into the same question answering format and adapt the\nfine-tuned context generators to the target task domain. Finally, we use the\nfine-tuned GLM to generate relevant contexts, which are in turn used as\nsynthetic training data for their corresponding tasks. We perform extensive\nexperiments on multiple classification datasets and demonstrate substantial\nimprovements in performance for both few- and zero-shot settings. Our analysis\nreveals that QA datasets that require high-level reasoning abilities (e.g.,\nabstractive and common-sense QA datasets) tend to give the best boost in\nperformance in both few-shot and zero-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mekala_D/0/1/0/all/0/1\">Dheeraj Mekala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tu Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schick_T/0/1/0/all/0/1\">Timo Schick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Language Models with Memory Augmentation. (arXiv:2205.12674v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12674","description":"<p>Recent work has improved language models (LMs) remarkably by equipping them\nwith a non-parametric memory component. However, most existing approaches only\nintroduce mem-ories at testing time or represent them using a separately\ntrained encoder, resulting in suboptimal training of the language model. In\nthis work, we present TRIME, a novel yet simple training approach designed for\ntraining LMs with memory augmentation. Our approach uses a training objective\nthat directly takes in-batch examples as accessible memory. We also present new\nmethods for memory construction and data batching, which are used for adapting\nto different sets of memories--local, long-term, and external memory--at\ntesting time. We evaluate TRIME on multiple language modeling and machine\ntranslation benchmarks and show that it is able to achieve significant\nimprovements across all the settings. Concretely, TRIME reduces the perplexity\nfrom 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory\nset from the training corpus. Compared to standard LM training, TRIME adds\nnegligible computational overhead and is compatible with different neural\narchitectures, making it a versatile solution for training memory-augmented\nLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zexuan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1\">Tao Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProsocialDialog: A Prosocial Backbone for Conversational Agents. (arXiv:2205.12688v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12688","description":"<p>Most existing dialogue systems fail to respond properly to potentially unsafe\nuser utterances by either ignoring or passively agreeing with them. To address\nthis issue, we introduce ProsocialDialog, the first large-scale multi-turn\ndialogue dataset to teach conversational agents to respond to problematic\ncontent following social norms. Covering diverse unethical, problematic,\nbiased, and toxic situations, ProsocialDialog contains responses that encourage\nprosocial behavior, grounded in commonsense social rules (i.e., rules-of-thumb,\nRoTs). Created via a human-AI collaborative framework, ProsocialDialog consists\nof 58K dialogues, with 331K utterances, 160K unique RoTs, and 497K dialogue\nsafety labels accompanied by free-form rationales.\n</p>\n<p>With this dataset, we introduce a dialogue safety detection module, Canary,\ncapable of generating RoTs given conversational context, and a\nsocially-informed dialogue agent, Prost. Empirical results show that Prost\ngenerates more socially acceptable dialogues compared to other state-of-the-art\nlanguage and dialogue models in both in-domain and out-of-domain settings.\nAdditionally, Canary effectively guides conversational agents and off-the-shelf\nlanguage models to generate significantly more prosocial responses. Our work\nhighlights the promise and importance of creating and steering conversational\nAI to be socially responsible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gunhee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting DocRED -- Addressing the False Negative Problem in Relation Extraction. (arXiv:2205.12696v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12696","description":"<p>The DocRED dataset is one of the most popular and widely used benchmarks for\ndocument-level relation extraction (RE). It adopts a recommend-revise\nannotation scheme so as to have a large-scale annotated dataset. However, we\nfind that the annotation of DocRED is incomplete, i.e., false negative samples\nare prevalent. We analyze the causes and effects of the overwhelming false\nnegative problem in the DocRED dataset. To address the shortcoming, we\nre-annotate 4,053 documents in the DocRED dataset by adding the missed relation\ntriples back to the original DocRED. We name our revised DocRED dataset\nRe-DocRED. We conduct extensive experiments with state-of-the-art neural models\non both datasets, and the experimental results show that the models trained and\nevaluated on our Re-DocRED achieve performance improvements of around 13 F1\npoints. Moreover, we conduct a comprehensive analysis to identify the potential\nareas for further improvement. Our dataset is publicly available at\nhttps://github.com/tonytan48/Re-DocRED.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Q/0/1/0/all/0/1\">Qingyu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_H/0/1/0/all/0/1\">Hwee Tou Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aljunied_S/0/1/0/all/0/1\">Sharifah Mahani Aljunied</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Can Secondary Predictions Tell Us? An Exploration on Question-Answering with SQuAD-v2.0. (arXiv:2206.14348v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.14348","description":"<p>Performance in natural language processing, and specifically for the\nquestion-answer task, is typically measured by comparing a model\\'s most\nconfident (primary) prediction to golden answers (the ground truth). We are\nmaking the case that it is also useful to quantify how close a model came to\npredicting a correct answer even for examples that failed. We define the Golden\nRank (GR) of an example as the rank of its most confident prediction that\nexactly matches a ground truth, and show why such a match always exists. For\nthe 16 transformer models we analyzed, the majority of exactly matched golden\nanswers in secondary prediction space hover very close to the top rank. We\nrefer to secondary predictions as those ranking above 0 in descending\nconfidence probability order. We demonstrate how the GR can be used to classify\nquestions and visualize their spectrum of difficulty, from persistent near\nsuccesses to persistent extreme failures. We derive a new aggregate statistic\nover entire test sets, named the Golden Rank Interpolated Median (GRIM) that\nquantifies the proximity of failed predictions to the top choice made by the\nmodel. To develop some intuition and explore the applicability of these metrics\nwe use the Stanford Question Answering Dataset (SQuAD-2) and a few popular\ntransformer models from the Hugging Face hub. We first demonstrate that the\nGRIM is not directly correlated with the F1 and exact match (EM) scores. We\nthen calculate and visualize these scores for various transformer\narchitectures, probe their applicability in error analysis by clustering failed\npredictions, and compare how they relate to other training diagnostics such as\nthe EM and F1 scores. We finally suggest various research goals, such as\nbroadening data collection for these metrics and their possible use in\nadversarial training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamfonas_M/0/1/0/all/0/1\">Michael Kamfonas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alon_G/0/1/0/all/0/1\">Gabriel Alon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TweetNLP: Cutting-Edge Natural Language Processing for Social Media. (arXiv:2206.14774v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.14774","description":"<p>In this paper we present TweetNLP, an integrated platform for Natural\nLanguage Processing (NLP) in social media. TweetNLP supports a diverse set of\nNLP tasks, including generic focus areas such as sentiment analysis and named\nentity recognition, as well as social media-specific tasks such as emoji\nprediction and offensive language identification. Task-specific systems are\npowered by reasonably-sized Transformer-based language models specialized on\nsocial media text (in particular, Twitter) which can be run without the need\nfor dedicated hardware or cloud services. The main contributions of TweetNLP\nare: (1) an integrated Python library for a modern toolkit supporting social\nmedia analysis using our various task-specific models adapted to the social\ndomain; (2) an interactive online demo for codeless experimentation using our\nmodels; and (3) a tutorial covering a wide variety of typical social media\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezaee_K/0/1/0/all/0/1\">Kiamehr Rezaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riahi_T/0/1/0/all/0/1\">Talayeh Riahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ushio_A/0/1/0/all/0/1\">Asahi Ushio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loureiro_D/0/1/0/all/0/1\">Daniel Loureiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antypas_D/0/1/0/all/0/1\">Dimosthenis Antypas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boisson_J/0/1/0/all/0/1\">Joanne Boisson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espinosa_Anke_L/0/1/0/all/0/1\">Luis Espinosa-Anke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Camara_E/0/1/0/all/0/1\">Eugenio Mart&#xed;nez-C&#xe1;mara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medina_G/0/1/0/all/0/1\">Gonzalo Medina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buhrmann_T/0/1/0/all/0/1\">Thomas Buhrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neves_L/0/1/0/all/0/1\">Leonardo Neves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbieri_F/0/1/0/all/0/1\">Francesco Barbieri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Confident Adaptive Language Modeling. (arXiv:2207.07061v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.07061","description":"<p>Recent advances in Transformer-based large language models (LLMs) have led to\nsignificant performance improvements across many tasks. These gains come with a\ndrastic increase in the models' size, potentially leading to slow and costly\nuse at inference time. In practice, however, the series of generations made by\nLLMs is composed of varying levels of difficulty. While certain predictions\ntruly benefit from the models' full capacity, other continuations are more\ntrivial and can be solved with reduced compute. In this work, we introduce\nConfident Adaptive Language Modeling (CALM), a framework for dynamically\nallocating different amounts of compute per input and generation timestep.\nEarly exit decoding involves several challenges that we address here, such as:\n(1) what confidence measure to use; (2) connecting sequence-level constraints\nto local per-token exit decisions; and (3) attending back to missing hidden\nrepresentations due to early exits in previous tokens. Through theoretical\nanalysis and empirical experiments on three diverse text generation tasks, we\ndemonstrate the efficacy of our framework in reducing compute -- potential\nspeedup of up to $\\times 3$ -- while provably maintaining high performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisch_A/0/1/0/all/0/1\">Adam Fisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1\">Jai Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Co-learning of Uncurated Images and Reports Enables Oversight AI in Radiology. (arXiv:2208.05140v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2208.05140","description":"<p>Oversight AI is an emerging concept in radiology where the AI forms a\nsymbiosis with radiologists by continuously supporting radiologists in their\ndecision-making. Recent advances in vision-language pre-training sheds a light\non the long-standing problems of the oversight AI by the understanding of both\nvisual and textual concepts and their semantic correspondences. However, there\nhave been limited successes in the application of vision-language pre-training\nin the medical domain, as the current vision-language models and learning\nstrategies for photographic images and captions are not optimal to process the\nmedical data that are usually insufficient in the amount and the diversity. To\naddress this, here we present medical X-VL, a self-supervised model tailored\nfor efficient vision-language pre-training that exploits cross attention in the\nradiological images and reports' common feature space in a symmetric manner. We\nexperimentally demonstrate that the pre-trained medical X-VL model outperforms\nthe current state-of-the-art models in various vision-language tasks in medical\ndomains. We finally demonstrate practical clinical usages of our oversight AI\nfor monitoring human errors and in the diagnosis of newly emerging diseases,\nwhich suggests the potential of an oversight AI model for widespread\napplicability in different medical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1\">Sangjoon Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_E/0/1/0/all/0/1\">Eun Sun Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shin_K/0/1/0/all/0/1\">Kyung Sook Shin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Jeong Eun Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"F-coref: Fast, Accurate and Easy to Use Coreference Resolution. (arXiv:2209.04280v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.04280","description":"<p>We introduce fastcoref, a python package for fast, accurate, and easy-to-use\nEnglish coreference resolution. The package is pip-installable, and allows two\nmodes: an accurate mode based on the LingMess architecture, providing\nstate-of-the-art coreference accuracy, and a substantially faster model,\nF-coref, which is the focus of this work. F-coref allows to process 2.8K\nOntoNotes documents in 25 seconds on a V100 GPU (compared to 6 minutes for the\nLingMess model, and to 12 minutes of the popular AllenNLP coreference model)\nwith only a modest drop in accuracy. The fast speed is achieved through a\ncombination of distillation of a compact model from the LingMess model, and an\nefficient batching implementation using a technique we call leftover batching.\nOur code is available at https://github.com/shon-otmazgin/fastcoref\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Otmazgin_S/0/1/0/all/0/1\">Shon Otmazgin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cattan_A/0/1/0/all/0/1\">Arie Cattan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Examining Large Pre-Trained Language Models for Machine Translation: What You Don't Know About It. (arXiv:2209.07417v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.07417","description":"<p>Pre-trained language models (PLMs) often take advantage of the monolingual\nand multilingual dataset that is freely available online to acquire general or\nmixed domain knowledge before deployment into specific tasks. Extra-large PLMs\n(xLPLMs) are proposed very recently to claim supreme performances over\nsmaller-sized PLMs such as in machine translation (MT) tasks. These xLPLMs\ninclude Meta-AI's wmt21-dense-24-wide-en-X (2021) and NLLB (2022). In this\nwork, we examine if xLPLMs are absolutely superior to smaller-sized PLMs in\nfine-tuning toward domain-specific MTs. We use two different in-domain data of\ndifferent sizes: commercial automotive in-house data and clinical shared task\ndata from the ClinSpEn2022 challenge at WMT2022. We choose popular Marian\nHelsinki as smaller sized PLM and two massive-sized Mega-Transformers from\nMeta-AI as xLPLMs.\n</p>\n<p>Our experimental investigation shows that 1) on smaller-sized in-domain\ncommercial automotive data, xLPLM wmt21-dense-24-wide-en-X indeed shows much\nbetter evaluation scores using SacreBLEU and hLEPOR metrics than smaller-sized\nMarian, even though its score increase rate is lower than Marian after\nfine-tuning; 2) on relatively larger-size well prepared clinical data\nfine-tuning, the xLPLM NLLB tends to lose its advantage over smaller-sized\nMarian on two sub-tasks (clinical terms and ontology concepts) using ClinSpEn\noffered metrics METEOR, COMET, and ROUGE-L, and totally lost to Marian on\nTask-1 (clinical cases) on all official metrics including SacreBLEU and BLEU;\n3) metrics do not always agree with each other on the same tasks using the same\nmodel outputs; 4) clinic-Marian ranked No.2 on Task- 1 (via SACREBLEU/BLEU) and\nTask-3 (via METEOR and ROUGE) among all submissions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lifeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erofeev_G/0/1/0/all/0/1\">Gleb Erofeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorokina_I/0/1/0/all/0/1\">Irina Sorokina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gladkoff_S/0/1/0/all/0/1\">Serge Gladkoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1\">Goran Nenadic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpeechCLIP: Integrating Speech with Pre-Trained Vision and Language Model. (arXiv:2210.00705v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.00705","description":"<p>Data-driven speech processing models usually perform well with a large amount\nof text supervision, but collecting transcribed speech data is costly.\nTherefore, we propose SpeechCLIP, a novel framework bridging speech and text\nthrough images to enhance speech models without transcriptions. We leverage\nstate-of-the-art pre-trained HuBERT and CLIP, aligning them via paired images\nand spoken captions with minimal fine-tuning. SpeechCLIP outperforms prior\nstate-of-the-art on image-speech retrieval and performs zero-shot speech-text\nretrieval without direct supervision from transcriptions. Moreover, SpeechCLIP\ncan directly retrieve semantically related keywords from speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shih_Y/0/1/0/all/0/1\">Yi-Jen Shih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsuan-Fu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berry_L/0/1/0/all/0/1\">Layne Berry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Dialogue Simulation with In-Context Learning. (arXiv:2210.04185v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04185","description":"<p>Building dialogue systems requires a large corpus of annotated dialogues.\nSuch datasets are usually created via crowdsourcing, which is expensive and\ntime-consuming. In this paper, we propose \\textsc{Dialogic}, a novel dialogue\nsimulation method based on large language model in-context learning to automate\ndataset creation. Seeded with a few annotated dialogues, \\textsc{Dialogic}\nautomatically selects in-context examples for demonstration and prompts GPT-3\nto generate new dialogues and annotations in a controllable way. Our method can\nrapidly expand a small set of dialogue data with minimum or zero \\textit{human\ninvolvement} and \\textit{parameter update} and is thus much more cost-efficient\nand time-saving than crowdsourcing. Experimental results on the MultiWOZ\ndataset demonstrate that training a model on the simulated dialogues leads to\neven better performance than using the same amount of human-generated dialogues\nunder the challenging low-resource settings, with as few as 85 dialogues as a\nseed. When the full training set is given, our method can still serve as an\neffective data augmentation method to further improve performance. Human\nevaluation results show that our simulated dialogues have near-human fluency\nand annotation accuracy. The code and data are available at\n\\textbf{\\url{https://github.com/Leezekun/dialogic}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shiyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jing Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xifeng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Compressing Sequences for Self-Supervised Speech Models. (arXiv:2210.07189v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07189","description":"<p>Compressing self-supervised models has become increasingly necessary, as\nself-supervised models become larger. While previous approaches have primarily\nfocused on compressing the model size, shortening sequences is also effective\nin reducing the computational cost. In this work, we study fixed-length and\nvariable-length subsampling along the time axis in self-supervised learning. We\nexplore how individual downstream tasks are sensitive to input frame rates.\nSubsampling while training self-supervised models not only improves the overall\nperformance on downstream tasks under certain frame rates, but also brings\nsignificant speed-up in inference. Variable-length subsampling performs\nparticularly well under low frame rates. In addition, if we have access to\nphonetic boundaries, we find no degradation in performance for an average frame\nrate as low as 10 Hz.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yen Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hsuan-Jui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_P/0/1/0/all/0/1\">Paola Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"G-Augment: Searching for the Meta-Structure of Data Augmentation Policies for ASR. (arXiv:2210.10879v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.10879","description":"<p>Data augmentation is a ubiquitous technique used to provide robustness to\nautomatic speech recognition (ASR) training. However, even as so much of the\nASR training process has become automated and more \"end-to-end\", the data\naugmentation policy (what augmentation functions to use, and how to apply them)\nremains hand-crafted. We present Graph-Augment, a technique to define the\naugmentation space as directed acyclic graphs (DAGs) and search over this space\nto optimize the augmentation policy itself. We show that given the same\ncomputational budget, policies produced by G-Augment are able to perform better\nthan SpecAugment policies obtained by random search on fine-tuning tasks on\nCHiME-6 and AMI. G-Augment is also able to establish a new state-of-the-art ASR\nperformance on the CHiME-6 evaluation set (30.7% WER). We further demonstrate\nthat G-Augment policies show better transfer properties across warm-start to\ncold-start training and model size compared to random-searched SpecAugment\npolicies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gary Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cubuk_E/0/1/0/all/0/1\">Ekin D.Cubuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_A/0/1/0/all/0/1\">Andrew Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shuyang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_R/0/1/0/all/0/1\">Ron J. Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramabhadran_B/0/1/0/all/0/1\">Bhuvana Ramabhadran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_P/0/1/0/all/0/1\">Pedro J. Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Daniel S. Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balanced Adversarial Training: Balancing Tradeoffs between Fickleness and Obstinacy in NLP Models. (arXiv:2210.11498v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.11498","description":"<p>Traditional (fickle) adversarial examples involve finding a small\nperturbation that does not change an input's true label but confuses the\nclassifier into outputting a different prediction. Conversely, obstinate\nadversarial examples occur when an adversary finds a small perturbation that\npreserves the classifier's prediction but changes the true label of an input.\nAdversarial training and certified robust training have shown some\neffectiveness in improving the robustness of machine learnt models to fickle\nadversarial examples. We show that standard adversarial training methods\nfocused on reducing vulnerability to fickle adversarial examples may make a\nmodel more vulnerable to obstinate adversarial examples, with experiments for\nboth natural language inference and paraphrase identification tasks. To counter\nthis phenomenon, we introduce Balanced Adversarial Training, which incorporates\ncontrastive learning to increase robustness against both fickle and obstinate\nadversarial examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hannah Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_D/0/1/0/all/0/1\">David Evans</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Can Self-Improve. (arXiv:2210.11610v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.11610","description":"<p>Large Language Models (LLMs) have achieved excellent performances in various\ntasks. However, fine-tuning an LLM requires extensive supervision. Human, on\nthe other hand, may improve their reasoning abilities by self-thinking without\nexternal inputs. In this work, we demonstrate that an LLM is also capable of\nself-improving with only unlabeled datasets. We use a pre-trained LLM to\ngenerate \"high-confidence\" rationale-augmented answers for unlabeled questions\nusing Chain-of-Thought prompting and self-consistency, and fine-tune the LLM\nusing those self-generated solutions as target outputs. We show that our\napproach improves the general reasoning ability of a 540B-parameter LLM\n(74.4%-&gt;82.1% on GSM8K, 78.2%-&gt;83.0% on DROP, 90.0%-&gt;94.4% on OpenBookQA, and\n63.4%-&gt;67.9% on ANLI-A3) and achieves state-of-the-art-level performance,\nwithout any ground truth label. We conduct ablation studies and show that\nfine-tuning on reasoning is critical for self-improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shixiang Shane Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Le Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuexin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AfroLID: A Neural Language Identification Tool for African Languages. (arXiv:2210.11744v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.11744","description":"<p>Language identification (LID) is a crucial precursor for NLP, especially for\nmining web data. Problematically, most of the world's 7000+ languages today are\nnot covered by LID technologies. We address this pressing issue for Africa by\nintroducing AfroLID, a neural LID toolkit for $517$ African languages and\nvarieties. AfroLID exploits a multi-domain web dataset manually curated from\nacross 14 language families utilizing five orthographic systems. When evaluated\non our blind Test set, AfroLID achieves 95.89 F_1-score. We also compare\nAfroLID to five existing LID tools that each cover a small number of African\nlanguages, finding it to outperform them on most languages. We further show the\nutility of AfroLID in the wild by testing it on the acutely under-served\nTwitter domain. Finally, we offer a number of controlled case studies and\nperform a linguistically-motivated error analysis that allow us to both\nshowcase AfroLID's powerful capabilities and limitations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adebara_I/0/1/0/all/0/1\">Ife Adebara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elmadany_A/0/1/0/all/0/1\">AbdelRahim Elmadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inciarte_A/0/1/0/all/0/1\">Alcides Alcoba Inciarte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PATS: Sensitivity-aware Noisy Learning for Pretrained Language Models. (arXiv:2210.12403v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12403","description":"<p>A wide range of NLP tasks benefit from the fine-tuning of pretrained language\nmodels (PLMs). However, a number of redundant parameters which contribute less\nto the downstream task are observed in a directly fine-tuned model. We consider\nthe gap between pretraining and downstream tasks hinders the training of these\nredundant parameters, and results in a suboptimal performance of the overall\nmodel. In this paper, we present PATS (Perturbation According To Sensitivity),\na noisy training mechanism which considers each parameter's importance in the\ndownstream task to help fine-tune PLMs. The main idea of PATS is to add bigger\nnoise to parameters with lower sensitivity and vice versa, in order to activate\nmore parameters' contributions to downstream tasks without affecting the\nsensitive ones much. Extensive experiments conducted on different tasks of the\nGLUE benchmark show PATS can consistently empower the fine-tuning of different\nsizes of PLMs, and the parameters in the well-performing models always have\nmore concentrated distributions of sensitivities, which experimentally proves\nthe effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yupeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure-Unified M-Tree Coding Solver for MathWord Problem. (arXiv:2210.12432v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12432","description":"<p>As one of the challenging NLP tasks, designing math word problem (MWP)\nsolvers has attracted increasing research attention for the past few years. In\nprevious work, models designed by taking into account the properties of the\nbinary tree structure of mathematical expressions at the output side have\nachieved better performance. However, the expressions corresponding to a MWP\nare often diverse (e.g., $n_1+n_2 \\times n_3-n_4$, $n_3\\times n_2-n_4+n_1$,\netc.), and so are the corresponding binary trees, which creates difficulties in\nmodel learning due to the non-deterministic output space. In this paper, we\npropose the Structure-Unified M-Tree Coding Solver (SUMC-Solver), which applies\na tree with any M branches (M-tree) to unify the output structures. To learn\nthe M-tree, we use a mapping to convert the M-tree into the M-tree codes, where\ncodes store the information of the paths from tree root to leaf nodes and the\ninformation of leaf nodes themselves, and then devise a Sequence-to-Code\n(seq2code) model to generate the codes. Experimental results on the widely used\nMAWPS and Math23K datasets have demonstrated that SUMC-Solver not only\noutperforms several state-of-the-art models under similar experimental settings\nbut also performs much better under low-resource conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_J/0/1/0/all/0/1\">Jiangzhou Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xinyu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lexical Generalization Improves with Larger Models and Longer Training. (arXiv:2210.12673v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12673","description":"<p>While fine-tuned language models perform well on many tasks, they were also\nshown to rely on superficial surface features such as lexical overlap.\nExcessive utilization of such heuristics can lead to failure on challenging\ninputs. We analyze the use of lexical overlap heuristics in natural language\ninference, paraphrase detection, and reading comprehension (using a novel\ncontrastive dataset), and find that larger models are much less susceptible to\nadopting lexical overlap heuristics. We also find that longer training leads\nmodels to abandon lexical overlap heuristics. Finally, we provide evidence that\nthe disparity between models size has its source in the pre-trained model\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bandel_E/0/1/0/all/0/1\">Elron Bandel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1\">Yanai Elazar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Essay Scoring using Transformers. (arXiv:2210.12809v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12809","description":"<p>Despite being investigated for over five decades, the task of automated essay\nscoring continues to draw a lot of attention in the NLP community, in part\nbecause of its commercial and educational values as well as the associated\nresearch challenges. Large pre-trained models have made remarkable progress in\nNLP. Data augmentation techniques have also helped build state-of-the-art\nmodels for automated essay scoring. Many works in the past have attempted to\nsolve this problem by using RNNs, LSTMs, etc. This work examines the\ntransformer models like BERT, RoBERTa, etc. We empirically demonstrate the\neffectiveness of transformer models and data augmentation for automated essay\ngrading across many topics using a single model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Kshitij Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DALL-E 2 Fails to Reliably Capture Common Syntactic Processes. (arXiv:2210.12889v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12889","description":"<p>Machine intelligence is increasingly being linked to claims about sentience,\nlanguage processing, and an ability to comprehend and transform natural\nlanguage into a range of stimuli. We systematically analyze the ability of\nDALL-E 2 to capture 8 grammatical phenomena pertaining to compositionality that\nare widely discussed in linguistics and pervasive in human language: binding\nprinciples and coreference, passives, word order, coordination, comparatives,\nnegation, ellipsis, and structural ambiguity. Whereas young children routinely\nmaster these phenomena, learning systematic mappings between syntax and\nsemantics, DALL-E 2 is unable to reliably infer meanings that are consistent\nwith the syntax. These results challenge recent claims concerning the capacity\nof such systems to understand of human language. We make available the full set\nof test materials as a benchmark for future testing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leivada_E/0/1/0/all/0/1\">Evelina Leivada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphy_E/0/1/0/all/0/1\">Elliot Murphy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcus_G/0/1/0/all/0/1\">Gary Marcus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Auxiliary Tasks Training: Bridging the Gap between Languages for Zero-Shot Transfer of Hate Speech Detection Models. (arXiv:2210.13029v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.13029","description":"<p>Zero-shot cross-lingual transfer learning has been shown to be highly\nchallenging for tasks involving a lot of linguistic specificities or when a\ncultural gap is present between languages, such as in hate speech detection. In\nthis paper, we highlight this limitation for hate speech detection in several\ndomains and languages using strict experimental settings. Then, we propose to\ntrain on multilingual auxiliary tasks -- sentiment analysis, named entity\nrecognition, and tasks relying on syntactic information -- to improve zero-shot\ntransfer of hate speech detection models across languages. We show how hate\nspeech detection models benefit from a cross-lingual knowledge proxy brought by\nauxiliary tasks fine-tuning and highlight these tasks' positive impact on\nbridging the hate speech linguistic and cultural gap between languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Montariol_S/0/1/0/all/0/1\">Syrielle Montariol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riabi_A/0/1/0/all/0/1\">Arij Riabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seddah_D/0/1/0/all/0/1\">Djam&#xe9; Seddah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task. (arXiv:2210.13382v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.13382","description":"<p>Language models show a surprising range of capabilities, but the source of\ntheir apparent competence is unclear. Do these networks just memorize a\ncollection of surface statistics, or do they rely on internal representations\nof the process that generates the sequences they see? We investigate this\nquestion by applying a variant of the GPT model to the task of predicting legal\nmoves in a simple board game, Othello. Although the network has no a priori\nknowledge of the game or its rules, we uncover evidence of an emergent\nnonlinear internal representation of the board state. Interventional\nexperiments indicate this representation can be used to control the output of\nthe network and create \"latent saliency maps\" that can help explain predictions\nin human terms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kenneth Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hopkins_A/0/1/0/all/0/1\">Aspen K. Hopkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1\">David Bau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viegas_F/0/1/0/all/0/1\">Fernanda Vi&#xe9;gas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenberg_M/0/1/0/all/0/1\">Martin Wattenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-10-25T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/"}}]}]}