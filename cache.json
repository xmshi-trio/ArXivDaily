{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-07-07T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding. (arXiv:2307.02499v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02499","description":"<p>Document understanding refers to automatically extract, analyze and\ncomprehend information from various types of digital documents, such as a web\npage. Existing Multi-model Large Language Models (MLLMs), including mPLUG-Owl,\nhave demonstrated promising zero-shot capabilities in shallow OCR-free text\nrecognition, indicating their potential for OCR-free document understanding.\nNevertheless, without in-domain training, these models tend to ignore\nfine-grained OCR features, such as sophisticated tables or large blocks of\ntext, which are essential for OCR-free document understanding. In this paper,\nwe propose mPLUG-DocOwl based on mPLUG-Owl for OCR-free document understanding.\nSpecifically, we first construct a instruction tuning dataset featuring a wide\nrange of visual-text understanding tasks. Then, we strengthen the OCR-free\ndocument understanding ability by jointly train the model on language-only,\ngeneral vision-and-language, and document instruction tuning dataset with our\nunified instruction tuning strategy. We also build an OCR-free document\ninstruction understanding evaluation set LLMDoc to better compare models'\ncapabilities on instruct compliance and document understanding. Experimental\nresults show that our model outperforms existing multi-modal models,\ndemonstrating its strong ability of document understanding. Besides, without\nspecific fine-tuning, mPLUG-DocOwl generalizes well on various downstream\ntasks. Our code, models, training data and evaluation set are available at\nhttps://github.com/X-PLUG/mPLUG-DocOwl.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiabo Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1\">Anwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinghao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dan_Y/0/1/0/all/0/1\">Yuhao Dan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chenlin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guohai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Junfeng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1\">Qian Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Math Agents: Computational Infrastructure, Mathematical Embedding, and Genomics. (arXiv:2307.02502v1 [q-bio.OT])","link":"http://arxiv.org/abs/2307.02502","description":"<p>The advancement in generative AI could be boosted with more accessible\nmathematics. Beyond human-AI chat, large language models (LLMs) are emerging in\nprogramming, algorithm discovery, and theorem proving, yet their genomics\napplication is limited. This project introduces Math Agents and mathematical\nembedding as fresh entries to the \"Moore's Law of Mathematics\", using a\nGPT-based workflow to convert equations from literature into LaTeX and Python\nformats. While many digital equation representations exist, there's a lack of\nautomated large-scale evaluation tools. LLMs are pivotal as linguistic user\ninterfaces, providing natural language access for human-AI chat and formal\nlanguages for large-scale AI-assisted computational infrastructure. Given the\ninfinite formal possibility spaces, Math Agents, which interact with math,\ncould potentially shift us from \"big data\" to \"big math\". Math, unlike the more\nflexible natural language, has properties subject to proof, enabling its use\nbeyond traditional applications like high-validation math-certified icons for\nAI alignment aims. This project aims to use Math Agents and mathematical\nembeddings to address the ageing issue in information systems biology by\napplying multiscalar physics mathematics to disease models and genomic data.\nGenerative AI with episodic memory could help analyse causal relations in\nlongitudinal health records, using SIR Precision Health models. Genomic data is\nsuggested for addressing the unsolved Alzheimer's disease problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Swan_M/0/1/0/all/0/1\">Melanie Swan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kido_T/0/1/0/all/0/1\">Takashi Kido</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Roland_E/0/1/0/all/0/1\">Eric Roland</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Santos_R/0/1/0/all/0/1\">Renato P. dos Santos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review. (arXiv:2307.02503v1 [cs.SE])","link":"http://arxiv.org/abs/2307.02503","description":"<p>This paper provides a comprehensive review of the literature concerning the\nutilization of Natural Language Processing (NLP) techniques, with a particular\nfocus on transformer-based large language models (LLMs) trained using Big Code,\nwithin the domain of AI-assisted programming tasks. LLMs, augmented with\nsoftware naturalness, have played a crucial role in facilitating AI-assisted\nprogramming applications, including code generation, code completion, code\ntranslation, code refinement, code summarization, defect detection, and clone\ndetection. Notable examples of such applications include the GitHub Copilot\npowered by OpenAI's Codex and DeepMind AlphaCode. This paper presents an\noverview of the major LLMs and their applications in downstream tasks related\nto AI-assisted programming. Furthermore, it explores the challenges and\nopportunities associated with incorporating NLP techniques with software\nnaturalness in these applications, with a discussion on extending AI-assisted\nprogramming capabilities to Apple's Xcode for mobile software development. This\npaper also presents the challenges of and opportunities for incorporating NLP\ntechniques with software naturalness, empowering developers with advanced\ncoding assistance and streamlining the software development process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wong_M/0/1/0/all/0/1\">Man Fai Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shangxin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hang_C/0/1/0/all/0/1\">Ching Nam Hang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_S/0/1/0/all/0/1\">Siu Wai Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chee Wei Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing the Performance of ChatGPT in Cardiology and Vascular Pathologies. (arXiv:2307.02518v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02518","description":"<p>The article aims to analyze the performance of ChatGPT, a large language\nmodel developed by OpenAI, in the context of cardiology and vascular\npathologies. The study evaluated the accuracy of ChatGPT in answering\nchallenging multiple-choice questions (QCM) using a dataset of 190 questions\nfrom the Siamois-QCM platform. The goal was to assess ChatGPT potential as a\nvaluable tool in medical education compared to two well-ranked students of\nmedicine. The results showed that ChatGPT outperformed the students, scoring\n175 out of 190 correct answers with a percentage of 92.10\\%, while the two\nstudents achieved scores of 163 and 159 with percentages of 85.78\\% and\n82.63\\%, respectively. These results showcase how ChatGPT has the potential to\nbe highly effective in the fields of cardiology and vascular pathologies by\nproviding accurate answers to relevant questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hariri_W/0/1/0/all/0/1\">Walid Hariri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Named Entity Inclusion in Abstractive Text Summarization. (arXiv:2307.02570v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02570","description":"<p>We address the named entity omission - the drawback of many current\nabstractive text summarizers. We suggest a custom pretraining objective to\nenhance the model's attention on the named entities in a text. At first, the\nnamed entity recognition model RoBERTa is trained to determine named entities\nin the text. After that, this model is used to mask named entities in the text\nand the BART model is trained to reconstruct them. Next, the BART model is\nfine-tuned on the summarization task. Our experiments showed that this\npretraining approach improves named entity inclusion precision and recall\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berezin_S/0/1/0/all/0/1\">Sergey Berezin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batura_T/0/1/0/all/0/1\">Tatiana Batura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ODD: A Benchmark Dataset for the NLP-based Opioid Related Aberrant Behavior Detection. (arXiv:2307.02591v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02591","description":"<p>Opioid related aberrant behaviors (ORAB) present novel risk factors for\nopioid overdose. Previously, ORAB have been mainly assessed by survey results\nand by monitoring drug administrations. Such methods however, cannot scale up\nand do not cover the entire spectrum of aberrant behaviors. On the other hand,\nORAB are widely documented in electronic health record notes. This paper\nintroduces a novel biomedical natural language processing benchmark dataset\nnamed ODD, for ORAB Detection Dataset. ODD is an expert-annotated dataset\ncomprising of more than 750 publicly available EHR notes. ODD has been designed\nto identify ORAB from patients' EHR notes and classify them into nine\ncategories; 1) Confirmed Aberrant Behavior, 2) Suggested Aberrant Behavior, 3)\nOpioids, 4) Indication, 5) Diagnosed opioid dependency, 6) Benzodiapines, 7)\nMedication Changes, 8) Central Nervous System-related, and 9) Social\nDeterminants of Health. We explored two state-of-the-art natural language\nprocessing (NLP) models (finetuning pretrained language models and\nprompt-tuning approaches) to identify ORAB. Experimental results show that the\nprompt-tuning models outperformed the finetuning models in most cateogories and\nthe gains were especially higher among uncommon categories (Suggested aberrant\nbehavior, Diagnosed opioid dependency and Medication change). Although the best\nmodel achieved the highest 83.92\\% on area under precision recall curve,\nuncommon classes (Suggested Aberrant Behavior, Diagnosed Opioid Dependence, and\nMedication Change) still have a large room for performance improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1\">Sunjae Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weisong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Druhl_E/0/1/0/all/0/1\">Emily Druhl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1\">Minhee L. Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reisman_J/0/1/0/all/0/1\">Joel I. Reisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerns_R/0/1/0/all/0/1\">Robert D. Kerns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becker_W/0/1/0/all/0/1\">William Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evade ChatGPT Detectors via A Single Space. (arXiv:2307.02599v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02599","description":"<p>ChatGPT brings revolutionary social value but also raises concerns about the\nmisuse of AI-generated content. Consequently, an important question is how to\ndetect whether content is generated by ChatGPT or by human. Existing detectors\nare built upon the assumption that there are distributional gaps between\nhuman-generated and AI-generated content. These gaps are typically identified\nusing statistical information or classifiers. Our research challenges the\ndistributional gap assumption in detectors. We find that detectors do not\neffectively discriminate the semantic and stylistic gaps between\nhuman-generated and AI-generated content. Instead, the \"subtle differences\",\nsuch as an extra space, become crucial for detection. Based on this discovery,\nwe propose the SpaceInfi strategy to evade detection. Experiments demonstrate\nthe effectiveness of this strategy across multiple benchmarks and detectors. We\nalso provide a theoretical explanation for why SpaceInfi is successful in\nevading perplexity-based detection. Our findings offer new insights and\nchallenges for understanding and constructing more applicable ChatGPT\ndetectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shuyang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1\">Wanyun Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Inspired Progressive Alignment and Comparative Learning for Grounded Word Acquisition. (arXiv:2307.02615v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02615","description":"<p>Human language acquisition is an efficient, supervised, and continual\nprocess. In this work, we took inspiration from how human babies acquire their\nfirst language, and developed a computational process for word acquisition\nthrough comparative learning. Motivated by cognitive findings, we generated a\nsmall dataset that enables the computation models to compare the similarities\nand differences of various attributes, learn to filter out and extract the\ncommon information for each shared linguistic label. We frame the acquisition\nof words as not only the information filtration process, but also as\nrepresentation-symbol mapping. This procedure does not involve a fixed\nvocabulary size, nor a discriminative objective, and allows the models to\ncontinually learn more concepts efficiently. Our results in controlled\nexperiments have shown the potential of this approach for efficient continual\nlearning of grounded words.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yuwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lattimer_B/0/1/0/all/0/1\">Barrett Martin Lattimer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference. (arXiv:2307.02628v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02628","description":"<p>Autoregressive large language models (LLMs) have made remarkable progress in\nvarious natural language generation tasks. However, they incur high computation\ncost and latency resulting from the autoregressive token-by-token generation.\nTo address this issue, several approaches have been proposed to reduce\ncomputational cost using early-exit strategies. These strategies enable faster\ntext generation using reduced computation without applying the full computation\ngraph to each token. While existing token-level early exit methods show\npromising results for online inference, they cannot be readily applied for\nbatch inferencing and Key-Value caching. This is because they have to wait\nuntil the last token in a batch exits before they can stop computing. This\nseverely limits the practical application of such techniques. In this paper, we\npropose a simple and effective token-level early exit method, SkipDecode,\ndesigned to work seamlessly with batch inferencing and KV caching. It overcomes\nprior constraints by setting up a singular exit point for every token in a\nbatch at each sequence position. It also guarantees a monotonic decrease in\nexit points, thereby eliminating the need to recompute KV Caches for preceding\ntokens. Rather than terminating computation prematurely as in prior works, our\napproach bypasses lower to middle layers, devoting most of the computational\nresources to upper layers, allowing later tokens to benefit from the compute\nexpenditure by earlier tokens. Our experimental results show that SkipDecode\ncan obtain 2x to 5x inference speedups with negligible regression across a\nvariety of tasks. This is achieved using OPT models of 1.3 billion and 6.7\nbillion parameters, all the while being directly compatible with batching and\nKV caching optimization techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Corro_L/0/1/0/all/0/1\">Luciano Del Corro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giorno_A/0/1/0/all/0/1\">Allie Del Giorno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sahaj Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Sentiment Analysis of Plastic Surgery Social Media Posts. (arXiv:2307.02640v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02640","description":"<p>The massive collection of user posts across social media platforms is\nprimarily untapped for artificial intelligence (AI) use cases based on the\nsheer volume and velocity of textual data. Natural language processing (NLP) is\na subfield of AI that leverages bodies of documents, known as corpora, to train\ncomputers in human-like language understanding. Using a word ranking method,\nterm frequency-inverse document frequency (TF-IDF), to create features across\ndocuments, it is possible to perform unsupervised analytics, machine learning\n(ML) that can group the documents without a human manually labeling the data.\nFor large datasets with thousands of features, t-distributed stochastic\nneighbor embedding (t-SNE), k-means clustering and Latent Dirichlet allocation\n(LDA) are employed to learn top words and generate topics for a Reddit and\nTwitter combined corpus. Using extremely simple deep learning models, this\nstudy demonstrates that the applied results of unsupervised analysis allow a\ncomputer to predict either negative, positive, or neutral user sentiment\ntowards plastic surgery based on a tweet or subreddit post with almost 90%\naccuracy. Furthermore, the model is capable of achieving higher accuracy on the\nunsupervised sentiment task than on a rudimentary supervised document\nclassification task. Therefore, unsupervised learning may be considered a\nviable option in labeling social media documents for NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramnarine_A/0/1/0/all/0/1\">Alexandrea K. Ramnarine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment. (arXiv:2307.02682v1 [cs.CV])","link":"http://arxiv.org/abs/2307.02682","description":"<p>Dense video captioning, a task of localizing meaningful moments and\ngenerating relevant captions for videos, often requires a large, expensive\ncorpus of annotated video segments paired with text. In an effort to minimize\nthe annotation cost, we propose ZeroTA, a novel method for dense video\ncaptioning in a zero-shot manner. Our method does not require any videos or\nannotations for training; instead, it localizes and describes events within\neach input video at test time by optimizing solely on the input. This is\naccomplished by introducing a soft moment mask that represents a temporal\nsegment in the video and jointly optimizing it with the prefix parameters of a\nlanguage model. This joint optimization aligns a frozen language generation\nmodel (i.e., GPT-2) with a frozen vision-language contrastive model (i.e.,\nCLIP) by maximizing the matching score between the generated text and a moment\nwithin the video. We also introduce a pairwise temporal IoU loss to let a set\nof soft moment masks capture multiple distinct events within the video. Our\nmethod effectively discovers diverse significant events within the video, with\nthe resulting captions appropriately describing these events. The empirical\nresults demonstrate that ZeroTA surpasses zero-shot baselines and even\noutperforms the state-of-the-art few-shot method on the widely-used benchmark\nActivityNet Captions. Moreover, our method shows greater robustness compared to\nsupervised methods when evaluated in out-of-domain scenarios. This research\nprovides insight into the potential of aligning widely-used models, such as\nlanguage generation models and vision-language models, to unlock a new\ncapability: understanding temporal aspects of videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jo_Y/0/1/0/all/0/1\">Yongrae Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seongyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Aiden SJ Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyunji Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1\">Hanseok Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Symbolic Rules over Abstract Meaning Representations for Textual Reinforcement Learning. (arXiv:2307.02689v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02689","description":"<p>Text-based reinforcement learning agents have predominantly been neural\nnetwork-based models with embeddings-based representation, learning\nuninterpretable policies that often do not generalize well to unseen games. On\nthe other hand, neuro-symbolic methods, specifically those that leverage an\nintermediate formal representation, are gaining significant attention in\nlanguage understanding tasks. This is because of their advantages ranging from\ninherent interpretability, the lesser requirement of training data, and being\ngeneralizable in scenarios with unseen data. Therefore, in this paper, we\npropose a modular, NEuro-Symbolic Textual Agent (NESTA) that combines a generic\nsemantic parser with a rule induction system to learn abstract interpretable\nrules as policies. Our experiments on established text-based game benchmarks\nshow that the proposed NESTA method outperforms deep reinforcement\nlearning-based techniques by achieving better generalization to unseen test\ngames and learning from fewer training interactions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhury_S/0/1/0/all/0/1\">Subhajit Chaudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swaminathan_S/0/1/0/all/0/1\">Sarathkrishna Swaminathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimura_D/0/1/0/all/0/1\">Daiki Kimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_P/0/1/0/all/0/1\">Prithviraj Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murugesan_K/0/1/0/all/0/1\">Keerthiram Murugesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uceda_Sosa_R/0/1/0/all/0/1\">Rosario Uceda-Sosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tatsubori_M/0/1/0/all/0/1\">Michiaki Tatsubori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fokoue_A/0/1/0/all/0/1\">Achille Fokoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapanipathi_P/0/1/0/all/0/1\">Pavan Kapanipathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munawar_A/0/1/0/all/0/1\">Asim Munawar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gray_A/0/1/0/all/0/1\">Alexander Gray</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling In-Context Demonstrations with Structured Attention. (arXiv:2307.02690v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02690","description":"<p>The recent surge of large language models (LLMs) highlights their ability to\nperform in-context learning, i.e., \"learning\" to perform a task from a few\ndemonstrations in the context without any parameter updates. However, their\ncapabilities of in-context learning are limited by the model architecture: 1)\nthe use of demonstrations is constrained by a maximum sentence length due to\npositional embeddings; 2) the quadratic complexity of attention hinders users\nfrom using more demonstrations efficiently; 3) LLMs are shown to be sensitive\nto the order of the demonstrations. In this work, we tackle these challenges by\nproposing a better architectural design for in-context learning. We propose\nSAICL (Structured Attention for In-Context Learning), which replaces the\nfull-attention by a structured attention mechanism designed for in-context\nlearning, and removes unnecessary dependencies between individual\ndemonstrations, while making the model invariant to the permutation of\ndemonstrations. We evaluate SAICL in a meta-training framework and show that\nSAICL achieves comparable or better performance than full attention while\nobtaining up to 3.4x inference speed-up. SAICL also consistently outperforms a\nstrong Fusion-in-Decoder (FiD) baseline which processes each demonstration\nindependently. Finally, thanks to its linear nature, we demonstrate that SAICL\ncan easily scale to hundreds of demonstrations with continuous performance\ngains with scaling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Tianle Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaixuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jason D. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengdi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Statistical Mechanics of Strahler Number via Random and Natural Language Sentences. (arXiv:2307.02697v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02697","description":"<p>The Strahler number was originally proposed to characterize the complexity of\nriver bifurcation and has found various applications. This article proposes\ncomputation of the Strahler number's upper and lower limits for natural\nlanguage sentence tree structures, which are available in a large dataset\nallowing for statistical mechanics analysis.\n</p>\n<p>Through empirical measurements across grammatically annotated data, the\nStrahler number of natural language sentences is shown to be almost always 3 or\n4, similar to the case of river bifurcation as reported by Strahler (1957) and\nHorton (1945).\n</p>\n<p>From the theory behind the number, we show that it is the lower limit of the\namount of memory required to process sentences under a particular model. A\nmathematical analysis of random trees provides a further conjecture on the\nnature of the Strahler number, revealing that it is not a constant but grows\nlogarithmically. This finding uncovers the statistical basics behind the\nStrahler number as a characteristic of a general tree structure target.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_Ishii_K/0/1/0/all/0/1\">Kumiko Tanaka-Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_A/0/1/0/all/0/1\">Akira Tanaka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CFSum: A Coarse-to-Fine Contribution Network for Multimodal Summarization. (arXiv:2307.02716v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02716","description":"<p>Multimodal summarization usually suffers from the problem that the\ncontribution of the visual modality is unclear. Existing multimodal\nsummarization approaches focus on designing the fusion methods of different\nmodalities, while ignoring the adaptive conditions under which visual\nmodalities are useful. Therefore, we propose a novel Coarse-to-Fine\ncontribution network for multimodal Summarization (CFSum) to consider different\ncontributions of images for summarization. First, to eliminate the interference\nof useless images, we propose a pre-filter module to abandon useless images.\nSecond, to make accurate use of useful images, we propose two levels of visual\ncomplement modules, word level and phrase level. Specifically, image\ncontributions are calculated and are adopted to guide the attention of both\ntextual and visual modalities. Experimental results have shown that CFSum\nsignificantly outperforms multiple strong baselines on the standard benchmark.\nFurthermore, the analysis verifies that useful images can even help generate\nnon-visual words which are implicitly represented in the image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1\">Min Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Junnan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haitao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_C/0/1/0/all/0/1\">Chengqing Zong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On-Device Constrained Self-Supervised Speech Representation Learning for Keyword Spotting via Knowledge Distillation. (arXiv:2307.02720v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02720","description":"<p>Large self-supervised models are effective feature extractors, but their\napplication is challenging under on-device budget constraints and biased\ndataset collection, especially in keyword spotting. To address this, we\nproposed a knowledge distillation-based self-supervised speech representation\nlearning (S3RL) architecture for on-device keyword spotting. Our approach used\na teacher-student framework to transfer knowledge from a larger, more complex\nmodel to a smaller, light-weight model using dual-view cross-correlation\ndistillation and the teacher's codebook as learning objectives. We evaluated\nour model's performance on an Alexa keyword spotting detection task using a\n16.6k-hour in-house dataset. Our technique showed exceptional performance in\nnormal and noisy conditions, demonstrating the efficacy of knowledge\ndistillation methods in constructing self-supervised models for keyword\nspotting tasks while working within on-device resource constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Gene-Ping Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yue Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1\">Qingming Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1\">Dongsu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuzong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Alignment Is An Efficient Unified Model for Massive NLP Tasks. (arXiv:2307.02729v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02729","description":"<p>Large language models (LLMs), typically designed as a function of next-word\nprediction, have excelled across extensive NLP tasks. Despite the generality,\nnext-word prediction is often not an efficient formulation for many of the\ntasks, demanding an extreme scale of model parameters (10s or 100s of billions)\nand sometimes yielding suboptimal performance. In practice, it is often\ndesirable to build more efficient models -- despite being less versatile, they\nstill apply to a substantial subset of problems, delivering on par or even\nsuperior performance with much smaller model sizes. In this paper, we propose\ntext alignment as an efficient unified model for a wide range of crucial tasks\ninvolving text entailment, similarity, question answering (and answerability),\nfactual consistency, and so forth. Given a pair of texts, the model measures\nthe degree of alignment between their information. We instantiate an alignment\nmodel (Align) through lightweight finetuning of RoBERTa (355M parameters) using\n5.9M examples from 28 datasets. Despite its compact size, extensive experiments\nshow the model's efficiency and strong performance: (1) On over 20 datasets of\naforementioned diverse tasks, the model matches or surpasses FLAN-T5 models\nthat have around 2x or 10x more parameters; the single unified model also\noutperforms task-specific models finetuned on individual datasets; (2) When\napplied to evaluate factual consistency of language generation on 23 datasets,\nour model improves over various baselines, including the much larger GPT-3.5\n(ChatGPT) and sometimes even GPT-4; (3) The lightweight model can also serve as\nan add-on component for LLMs such as GPT-3.5 in question answering tasks,\nimproving the average exact match (EM) score by 17.94 and F1 score by 15.05\nthrough identifying unanswerable questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zha_Y/0/1/0/all/0/1\">Yuheng Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yichi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruichen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RecallM: An Architecture for Temporal Context Understanding and Question Answering. (arXiv:2307.02738v1 [cs.AI])","link":"http://arxiv.org/abs/2307.02738","description":"<p>The ideal long-term memory mechanism for Large Language Model (LLM) based\nchatbots, would lay the foundation for continual learning, complex reasoning\nand allow sequential and temporal dependencies to be learnt. Creating this type\nof memory mechanism is an extremely challenging problem. In this paper we\nexplore different methods of achieving the effect of long-term memory. We\npropose a new architecture focused on creating adaptable and updatable\nlong-term memory for AGI systems. We demonstrate through various experiments\nthe benefits of the RecallM architecture, particularly the improved temporal\nunderstanding it provides.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kynoch_B/0/1/0/all/0/1\">Brandon Kynoch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Latapie_H/0/1/0/all/0/1\">Hugo Latapie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Retrieval Adaptation using Target Domain Description. (arXiv:2307.02740v1 [cs.IR])","link":"http://arxiv.org/abs/2307.02740","description":"<p>In information retrieval (IR), domain adaptation is the process of adapting a\nretrieval model to a new domain whose data distribution is different from the\nsource domain. Existing methods in this area focus on unsupervised domain\nadaptation where they have access to the target document collection or\nsupervised (often few-shot) domain adaptation where they additionally have\naccess to (limited) labeled data in the target domain. There also exists\nresearch on improving zero-shot performance of retrieval models with no\nadaptation. This paper introduces a new category of domain adaptation in IR\nthat is as-yet unexplored. Here, similar to the zero-shot setting, we assume\nthe retrieval model does not have access to the target document collection. In\ncontrast, it does have access to a brief textual description that explains the\ntarget domain. We define a taxonomy of domain attributes in retrieval tasks to\nunderstand different properties of a source domain that can be adapted to a\ntarget domain. We introduce a novel automatic data construction pipeline that\nproduces a synthetic document collection, query set, and pseudo relevance\nlabels, given a textual domain description. Extensive experiments on five\ndiverse target domains show that adapting dense retrieval models using the\nconstructed synthetic data leads to effective retrieval performance on the\ntarget domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hashemi_H/0/1/0/all/0/1\">Helia Hashemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yong Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothur_S/0/1/0/all/0/1\">Sachith Sri Ram Kothur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_S/0/1/0/all/0/1\">Srivas Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meij_E/0/1/0/all/0/1\">Edgar Meij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Croft_W/0/1/0/all/0/1\">W. Bruce Croft</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Linguistic Style Matching in Online Communities: The Role of Social Context and Conversation Dynamics. (arXiv:2307.02758v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02758","description":"<p>Linguistic style matching (LSM) in conversations can be reflective of several\naspects of social influence such as power or persuasion. However, how LSM\nrelates to the outcomes of online communication on platforms such as Reddit is\nan unknown question. In this study, we analyze a large corpus of two-party\nconversation threads in Reddit where we identify all occurrences of LSM using\ntwo types of style: the use of function words and formality. Using this\nframework, we examine how levels of LSM differ in conversations depending on\nseveral social factors within Reddit: post and subreddit features, conversation\ndepth, user tenure, and the controversiality of a comment. Finally, we measure\nthe change of LSM following loss of status after community banning. Our\nfindings reveal the interplay of LSM in Reddit conversations with several\ncommunity metrics, suggesting the importance of understanding conversation\nengagement when understanding community dynamics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ananthasubramaniam_A/0/1/0/all/0/1\">Aparna Ananthasubramaniam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jason Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alkiek_K/0/1/0/all/0/1\">Kenan Alkiek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jiaxin Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seth_A/0/1/0/all/0/1\">Agrima Seth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunagan_L/0/1/0/all/0/1\">Lavinia Dunagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1\">Minje Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litterer_B/0/1/0/all/0/1\">Benjamin Litterer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations. (arXiv:2307.02762v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02762","description":"<p>Nowadays, the quality of responses generated by different modern large\nlanguage models (LLMs) are hard to evaluate and compare automatically. Recent\nstudies suggest and predominantly use LLMs as a reference-free metric for\nopen-ended question answering. More specifically, they use the recognized\n\"strongest\" LLM as the evaluator, which conducts pairwise comparisons of\ncandidate models' answers and provides a ranking score. However, this intuitive\nmethod has multiple problems, such as bringing in self-enhancement (favoring\nits own answers) and positional bias. We draw insights and lessons from the\neducational domain (Cho and MacArthur, 2011; Walsh, 2014) to improve LLM-based\nevaluations. Specifically, we propose the (1) peer rank (PR) algorithm that\ntakes into account each peer LLM's pairwise preferences of all answer pairs,\nand outputs a final ranking of models; and (2) peer discussion (PD), where we\nprompt two LLMs to discuss and try to reach a mutual agreement on preferences\nof two answers. We conduct experiments on two benchmark datasets. We find that\nour approaches achieve higher accuracy and align better with human judgments,\nrespectively. Interestingly, PR can induce a relatively accurate self-ranking\nof models under the anonymous setting, where each model's name is unrevealed.\nOur work provides space to explore evaluating models that are hard to compare\nfor humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruosen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_T/0/1/0/all/0/1\">Teerth Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xinya Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Your spouse needs professional help: Determining the Contextual Appropriateness of Messages through Modeling Social Relationships. (arXiv:2307.02763v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02763","description":"<p>Understanding interpersonal communication requires, in part, understanding\nthe social context and norms in which a message is said. However, current\nmethods for identifying offensive content in such communication largely operate\nindependent of context, with only a few approaches considering community norms\nor prior conversation as context. Here, we introduce a new approach to\nidentifying inappropriate communication by explicitly modeling the social\nrelationship between the individuals. We introduce a new dataset of\ncontextually-situated judgments of appropriateness and show that large language\nmodels can readily incorporate relationship information to accurately identify\nappropriateness in a given context. Using data from online conversations and\nmovie dialogues, we provide insight into how the relationships themselves\nfunction as implicit norms and quantify the degree to which context-sensitivity\nis needed in different conversation settings. Further, we also demonstrate that\ncontextual-appropriateness judgments are predictive of other social factors\nexpressed in language such as condescension and politeness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seth_A/0/1/0/all/0/1\">Agrima Seth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sargent_J/0/1/0/all/0/1\">Jackson Sargent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghighi_A/0/1/0/all/0/1\">Athena Aghighi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geraci_M/0/1/0/all/0/1\">Michael Geraci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Models to Generate, Recognize, and Reframe Unhelpful Thoughts. (arXiv:2307.02768v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02768","description":"<p>Many cognitive approaches to well-being, such as recognizing and reframing\nunhelpful thoughts, have received considerable empirical support over the past\ndecades, yet still lack truly widespread adoption in self-help format. A\nbarrier to that adoption is a lack of adequately specific and diverse dedicated\npractice material. This work examines whether current language models can be\nleveraged to both produce a virtually unlimited quantity of practice material\nillustrating standard unhelpful thought patterns matching specific given\ncontexts, and generate suitable positive reframing proposals. We propose\nPATTERNREFRAME, a novel dataset of about 10k examples of thoughts containing\nunhelpful thought patterns conditioned on a given persona, accompanied by about\n27k positive reframes. By using this dataset to train and/or evaluate current\nmodels, we show that existing models can already be powerful tools to help\ngenerate an abundance of tailored practice material and hypotheses, with no or\nminimal additional model training required.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maddela_M/0/1/0/all/0/1\">Mounica Maddela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ung_M/0/1/0/all/0/1\">Megan Ung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foran_H/0/1/0/all/0/1\">Heather Foran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boureau_Y/0/1/0/all/0/1\">Y-Lan Boureau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Should Data Science Education Do with Large Language Models?. (arXiv:2307.02792v1 [cs.CY])","link":"http://arxiv.org/abs/2307.02792","description":"<p>The rapid advances of large language models (LLMs), such as ChatGPT, are\nrevolutionizing data science and statistics. These state-of-the-art tools can\nstreamline complex processes. As a result, it reshapes the role of data\nscientists. We argue that LLMs are transforming the responsibilities of data\nscientists, shifting their focus from hands-on coding, data-wrangling and\nconducting standard analyses to assessing and managing analyses performed by\nthese automated AIs. This evolution of roles is reminiscent of the transition\nfrom a software engineer to a product manager. We illustrate this transition\nwith concrete data science case studies using LLMs in this paper. These\ndevelopments necessitate a meaningful evolution in data science education.\nPedagogy must now place greater emphasis on cultivating diverse skillsets among\nstudents, such as LLM-informed creativity, critical thinking, AI-guided\nprogramming. LLMs can also play a significant role in the classroom as\ninteractive teaching and learning tools, contributing to personalized\neducation. This paper discusses the opportunities, resources and open\nchallenges for each of these directions. As with any transformative technology,\nintegrating LLMs into education calls for careful consideration. While LLMs can\nperform repetitive tasks efficiently, it's crucial to remember that their role\nis to supplement human intelligence and creativity, not to replace it.\nTherefore, the new era of data science education should balance the benefits of\nLLMs while fostering complementary human expertise and innovations. In\nconclusion, the rise of LLMs heralds a transformative period for data science\nand its education. This paper seeks to shed light on the emerging trends,\npotential opportunities, and challenges accompanying this paradigm shift,\nhoping to spark further discourse and investigation into this exciting,\nuncharted territory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_X/0/1/0/all/0/1\">Xinming Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weijie J. Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linjun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VerifAI: Verified Generative AI. (arXiv:2307.02796v1 [cs.DB])","link":"http://arxiv.org/abs/2307.02796","description":"<p>Generative AI has made significant strides, yet concerns about the accuracy\nand reliability of its outputs continue to grow. Such inaccuracies can have\nserious consequences such as inaccurate decision-making, the spread of false\ninformation, privacy violations, legal liabilities, and more. Although efforts\nto address these risks are underway, including explainable AI and responsible\nAI practices such as transparency, privacy protection, bias mitigation, and\nsocial and environmental responsibility, misinformation caused by generative AI\nwill remain a significant challenge. We propose that verifying the outputs of\ngenerative AI from a data management perspective is an emerging issue for\ngenerative AI. This involves analyzing the underlying data from multi-modal\ndata lakes, including text files, tables, and knowledge graphs, and assessing\nits quality and consistency. By doing so, we can establish a stronger\nfoundation for evaluating the outputs of generative AI models. Such an approach\ncan ensure the correctness of generative AI, promote transparency, and enable\ndecision-making with greater confidence. Our vision is to promote the\ndevelopment of verifiable generative AI and contribute to a more trustworthy\nand responsible use of AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_N/0/1/0/all/0/1\">Nan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chenyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Ju Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Lei Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Zero-Shot Prompt Learning for Cross-Domain Slot Filling with Inverse Prompting. (arXiv:2307.02830v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02830","description":"<p>Zero-shot cross-domain slot filling aims to transfer knowledge from the\nlabeled source domain to the unlabeled target domain. Existing models either\nencode slot descriptions and examples or design handcrafted question templates\nusing heuristic rules, suffering from poor generalization capability or\nrobustness. In this paper, we propose a generative zero-shot prompt learning\nframework for cross-domain slot filling, both improving generalization and\nrobustness than previous work. Besides, we introduce a novel inverse prompting\nstrategy to distinguish different slot types to avoid the multiple prediction\nproblem, and an efficient prompt-tuning strategy to boost higher performance by\nonly training fewer prompt parameters. Experiments and analysis demonstrate the\neffectiveness of our proposed framework, especially huge improvements (+13.44%\nF1) on the unseen slots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuefeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guanting Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Keqing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jinzheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_H/0/1/0/all/0/1\">Hao Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiran Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing LLM with Evolutionary Fine Tuning for News Summary Generation. (arXiv:2307.02839v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02839","description":"<p>News summary generation is an important task in the field of intelligence\nanalysis, which can provide accurate and comprehensive information to help\npeople better understand and respond to complex real-world events. However,\ntraditional news summary generation methods face some challenges, which are\nlimited by the model itself and the amount of training data, as well as the\ninfluence of text noise, making it difficult to generate reliable information\naccurately. In this paper, we propose a new paradigm for news summary\ngeneration using LLM with powerful natural language understanding and\ngenerative capabilities. We use LLM to extract multiple structured event\npatterns from the events contained in news paragraphs, evolve the event pattern\npopulation with genetic algorithm, and select the most adaptive event pattern\nto input into the LLM to generate news summaries. A News Summary Generator\n(NSG) is designed to select and evolve the event pattern populations and\ngenerate news summaries. The experimental results show that the news summary\ngenerator is able to generate accurate and reliable news summaries with some\ngeneralization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Le Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaolin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NatLogAttack: A Framework for Attacking Natural Language Inference Models with Natural Logic. (arXiv:2307.02849v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02849","description":"<p>Reasoning has been a central topic in artificial intelligence from the\nbeginning. The recent progress made on distributed representation and neural\nnetworks continues to improve the state-of-the-art performance of natural\nlanguage inference. However, it remains an open question whether the models\nperform real reasoning to reach their conclusions or rely on spurious\ncorrelations. Adversarial attacks have proven to be an important tool to help\nevaluate the Achilles' heel of the victim models. In this study, we explore the\nfundamental problem of developing attack models based on logic formalism. We\npropose NatLogAttack to perform systematic attacks centring around natural\nlogic, a classical logic formalism that is traceable back to Aristotle's\nsyllogism and has been closely developed for natural language inference. The\nproposed framework renders both label-preserving and label-flipping attacks. We\nshow that compared to the existing attack models, NatLogAttack generates better\nadversarial examples with fewer visits to the victim models. The victim models\nare found to be more vulnerable under the label-flipping setting. NatLogAttack\nprovides a tool to probe the existing and future NLI models' capacity from a\nkey viewpoint and we hope more logic-based attacks will be further explored for\nunderstanding the desired property of reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zi&#x27;ou Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ValiTex -- a uniform validation framework for computational text-based measures of social science constructs. (arXiv:2307.02863v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02863","description":"<p>Guidance on how to validate computational text-based measures of social\nscience constructs is fragmented. Whereas scholars are generally acknowledging\nthe importance of validating their text-based measures, they often lack common\nterminology and a unified framework to do so. This paper introduces a new\nvalidation framework called ValiTex, designed to assist scholars to measure\nsocial science constructs based on textual data. The framework draws on a\nlong-established tradition within psychometrics while extending the framework\nfor the purpose of computational text analysis. ValiTex consists of two\ncomponents, a conceptual model, and a dynamic checklist. Whereas the conceptual\nmodel provides a general structure along distinct phases on how to approach\nvalidation, the dynamic checklist defines specific validation steps and\nprovides guidance on which steps might be considered recommendable (i.e.,\nproviding relevant and necessary validation evidence) or optional (i.e., useful\nfor providing additional supporting validation evidence. The utility of the\nframework is demonstrated by applying it to a use case of detecting sexism from\nsocial media data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Birkenmaier_L/0/1/0/all/0/1\">Lukas Birkenmaier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lechner_C/0/1/0/all/0/1\">Clemens Lechner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_C/0/1/0/all/0/1\">Claudia Wagner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrast Is All You Need. (arXiv:2307.02882v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02882","description":"<p>In this study, we analyze data-scarce classification scenarios, where\navailable labeled legal data is small and imbalanced, potentially hurting the\nquality of the results. We focused on two finetuning objectives; SetFit\n(Sentence Transformer Finetuning), a contrastive learning setup, and a vanilla\nfinetuning setup on a legal provision classification task. Additionally, we\ncompare the features that are extracted with LIME (Local Interpretable\nModel-agnostic Explanations) to see which particular features contributed to\nthe model's classification decisions. The results show that a contrastive setup\nwith SetFit performed better than vanilla finetuning while using a fraction of\nthe training samples. LIME results show that the contrastive learning approach\nhelps boost both positive and negative features which are legally informative\nand contribute to the classification results. Thus a model finetuned with a\ncontrastive objective seems to base its decisions more confidently on legally\ninformative features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kilic_B/0/1/0/all/0/1\">Burak Kilic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bex_F/0/1/0/all/0/1\">Florix Bex</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1\">Albert Gatt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Relationship Between Speech Features Changes When You Get Depressed: Feature Correlations for Improving Speed and Performance of Depression Detection. (arXiv:2307.02892v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02892","description":"<p>This work shows that depression changes the correlation between features\nextracted from speech. Furthermore, it shows that using such an insight can\nimprove the training speed and performance of depression detectors based on\nSVMs and LSTMs. The experiments were performed over the Androids Corpus, a\npublicly available dataset involving 112 speakers, including 58 people\ndiagnosed with depression by professional psychiatrists. The results show that\nthe models used in the experiments improve in terms of training speed and\nperformance when fed with feature correlation matrices rather than with feature\nvectors. The relative reduction of the error rate ranges between 23.1% and\n26.6% depending on the model. The probable explanation is that feature\ncorrelation matrices appear to be more variable in the case of depressed\nspeakers. Correspondingly, such a phenomenon can be thought of as a depression\nmarker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_F/0/1/0/all/0/1\">Fuxiang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1\">Xuri Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esposito_A/0/1/0/all/0/1\">Anna Esposito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinciarelli_A/0/1/0/all/0/1\">Alessandro Vinciarelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Agentivit\\`a e telicit\\`a in GilBERTo: implicazioni cognitive. (arXiv:2307.02910v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02910","description":"<p>The goal of this study is to investigate whether a Transformer-based neural\nlanguage model infers lexical semantics and use this information for the\ncompletion of morphosyntactic patterns. The semantic properties considered are\ntelicity (also combined with definiteness) and agentivity. Both act at the\ninterface between semantics and morphosyntax: they are semantically determined\nand syntactically encoded. The tasks were submitted to both the computational\nmodel and a group of Italian native speakers. The comparison between the two\ngroups of data allows us to investigate to what extent neural language models\ncapture significant aspects of human semantic competence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lombardi_A/0/1/0/all/0/1\">Agnese Lombardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenci_A/0/1/0/all/0/1\">Alessandro Lenci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEA: Improving Sentence Similarity Robustness to Typos Using Lexical Attention Bias. (arXiv:2307.02912v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02912","description":"<p>Textual noise, such as typos or abbreviations, is a well-known issue that\npenalizes vanilla Transformers for most downstream tasks. We show that this is\nalso the case for sentence similarity, a fundamental task in multiple domains,\ne.g. matching, retrieval or paraphrasing. Sentence similarity can be approached\nusing cross-encoders, where the two sentences are concatenated in the input\nallowing the model to exploit the inter-relations between them. Previous works\naddressing the noise issue mainly rely on data augmentation strategies, showing\nimproved robustness when dealing with corrupted samples that are similar to the\nones used for training. However, all these methods still suffer from the token\ndistribution shift induced by typos. In this work, we propose to tackle textual\nnoise by equipping cross-encoders with a novel LExical-aware Attention module\n(LEA) that incorporates lexical similarities between words in both sentences.\nBy using raw text similarities, our approach avoids the tokenization shift\nproblem obtaining improved robustness. We demonstrate that the attention bias\nintroduced by LEA helps cross-encoders to tackle complex scenarios with textual\nnoise, specially in domains with short-text descriptions and limited context.\nExperiments using three popular Transformer encoders in five e-commerce\ndatasets for product matching show that LEA consistently boosts performance\nunder the presence of noise, while remaining competitive on the original\n(clean) splits. We also evaluate our approach in two datasets for textual\nentailment and paraphrasing showing that LEA is robust to typos in domains with\nlonger sentences and more natural context. Additionally, we thoroughly analyze\nseveral design choices in our approach, providing insights about the impact of\nthe decisions made and fostering future research in cross-encoders dealing with\ntypos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Almagro_M/0/1/0/all/0/1\">Mario Almagro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almazan_E/0/1/0/all/0/1\">Emilio Almaz&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortego_D/0/1/0/all/0/1\">Diego Ortego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jimenez_D/0/1/0/all/0/1\">David Jim&#xe9;nez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Cultural Gap in Text-to-Image Generation. (arXiv:2307.02971v1 [cs.CV])","link":"http://arxiv.org/abs/2307.02971","description":"<p>One challenge in text-to-image (T2I) generation is the inadvertent reflection\nof culture gaps present in the training data, which signifies the disparity in\ngenerated image quality when the cultural elements of the input text are rarely\ncollected in the training set. Although various T2I models have shown\nimpressive but arbitrary examples, there is no benchmark to systematically\nevaluate a T2I model's ability to generate cross-cultural images. To bridge the\ngap, we propose a Challenging Cross-Cultural (C3) benchmark with comprehensive\nevaluation criteria, which can assess how well-suited a model is to a target\nculture. By analyzing the flawed images generated by the Stable Diffusion model\non the C3 benchmark, we find that the model often fails to generate certain\ncultural objects. Accordingly, we propose a novel multi-modal metric that\nconsiders object-text alignment to filter the fine-tuning data in the target\nculture, which is used to fine-tune a T2I model to improve cross-cultural\ngeneration. Experimental results show that our multi-modal metric provides\nstronger data selection performance on the C3 benchmark than existing metrics,\nin which the object-text alignment is crucial. We release the benchmark, data,\ncode, and generated images to facilitate future research on culturally diverse\nT2I generation (https://github.com/longyuewangdcu/C3-Bench).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bingshuai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1\">Chenyang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Semiring-Weighted Earley Parsing. (arXiv:2307.02982v1 [cs.CL])","link":"http://arxiv.org/abs/2307.02982","description":"<p>This paper provides a reference description, in the form of a deduction\nsystem, of Earley's (1970) context-free parsing algorithm with various\nspeed-ups. Our presentation includes a known worst-case runtime improvement\nfrom Earley's $O (N^3|G||R|)$, which is unworkable for the large grammars that\narise in natural language processing, to $O (N^3|G|)$, which matches the\nruntime of CKY on a binarized version of the grammar $G$. Here $N$ is the\nlength of the sentence, $|R|$ is the number of productions in $G$, and $|G|$ is\nthe total length of those productions. We also provide a version that achieves\nruntime of $O (N^3|M|)$ with $|M| \\leq |G|$ when the grammar is represented\ncompactly as a single finite-state automaton $M$ (this is partly novel). We\ncarefully treat the generalization to semiring-weighted deduction,\npreprocessing the grammar like Stolcke (1995) to eliminate deduction cycles,\nand further generalize Stolcke's method to compute the weights of sentence\nprefixes. We also provide implementation details for efficient execution,\nensuring that on a preprocessed grammar, the semiring-weighted versions of our\nmethods have the same asymptotic runtime and space requirements as the\nunweighted methods, including sub-cubic runtime on some grammars.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Opedal_A/0/1/0/all/0/1\">Andreas Opedal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zmigrod_R/0/1/0/all/0/1\">Ran Zmigrod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vieira_T/0/1/0/all/0/1\">Tim Vieira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisner_J/0/1/0/all/0/1\">Jason Eisner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Style Over Substance: Evaluation Biases for Large Language Models. (arXiv:2307.03025v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03025","description":"<p>As large language models (LLMs) continue to advance, accurately and\ncomprehensively evaluating their performance becomes increasingly challenging.\nConventionally, human evaluations are considered the gold standard in natural\nlanguage generation. Recent advancements incorporate state-of-the-art LLMs as\nproxies for human judges in evaluation processes. Nonetheless, the extent to\nwhich humans and LLMs are capable evaluators remains uncertain. This study aims\nto investigate the behavior of both crowd-sourced human and LLM-based judges\nwhen comparing outputs from different models. To accomplish this, we curate a\ndataset comprising intentionally flawed machine-generated answers. Our findings\nindicate that despite the potentially greater danger posed by factual errors,\nanswers with factual errors were still rated more favorably compared to answers\nthat were too short or contained grammatical errors. This highlights a\nconcerning bias in the evaluation process. To address this issue, we propose to\nindependently evaluate machine-generated text across multiple dimensions,\nrather than merging all the evaluation aspects into a single score. We\ninstantiate this idea with the Elo rating system, resulting in the Multi-Elo\nRating System. Empirical results from our study reveal that this proposed\napproach significantly enhances the quality of LLM-based evaluations,\nparticularly in terms of factual accuracy. However, notable improvement is not\nobserved in crowd-sourced-based evaluations, suggesting the need for further\ninvestigation and refinement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Retrieval-Augmented Large Language Models via Data Importance Learning. (arXiv:2307.03027v1 [cs.LG])","link":"http://arxiv.org/abs/2307.03027","description":"<p>Retrieval augmentation enables large language models to take advantage of\nexternal knowledge, for example on tasks like question answering and data\nimputation. However, the performance of such retrieval-augmented models is\nlimited by the data quality of their underlying retrieval corpus. In this\npaper, we propose an algorithm based on multilinear extension for evaluating\nthe data importance of retrieved data points. There are exponentially many\nterms in the multilinear extension, and one key contribution of this paper is a\npolynomial time algorithm that computes exactly, given a retrieval-augmented\nmodel with an additive utility function and a validation set, the data\nimportance of data points in the retrieval corpus using the multilinear\nextension of the model's utility function. We further proposed an even more\nefficient ({\\epsilon}, {\\delta})-approximation algorithm. Our experimental\nresults illustrate that we can enhance the performance of large language models\nby only pruning or reweighting the retrieval corpus, without requiring further\ntraining. For some tasks, this even allows a small model (e.g., GPT-JT),\naugmented with a search engine API, to outperform GPT-3.5 (without retrieval\naugmentation). Moreover, we show that weights based on multilinear extension\ncan be computed efficiently in practice (e.g., in less than ten minutes for a\ncorpus with 100 million elements).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1\">Xiaozhong Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grafberger_S/0/1/0/all/0/1\">Stefan Grafberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biegel_S/0/1/0/all/0/1\">Samantha Biegel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1\">Shaopeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Meng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schelter_S/0/1/0/all/0/1\">Sebastian Schelter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain. (arXiv:2307.03042v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03042","description":"<p>Adapting pretrained language models to novel domains, such as clinical\napplications, traditionally involves retraining their entire set of parameters.\nHowever, this approach is increasingly proven to be impractical owing to the\nsubstantial computational requirements associated with training such large\nlanguage models. To address this issue, Parameter-Efficient Fine-Tuning (PEFT)\ntechniques offer a viable solution by selectively fine-tuning a small subset of\nadditional parameters, significantly reducing the computational requirements\nfor domain adaptation. In this study, we propose Clinical LLaMA-LoRA, a PEFT\nadapter layer built upon the open-sourced LLaMA model. Clinical LLaMA-LoRA is\ntrained using clinical notes obtained from the MIMIC-IV database, thereby\ncreating a specialised adapter designed for the clinical domain. Additionally,\nwe propose a two-step PEFT framework which fuses Clinical LLaMA-LoRA with\nDownstream LLaMA-LoRA, another PEFT adapter specialised for downstream tasks.\nWe evaluate this framework on multiple clinical outcome prediction datasets,\ncomparing it to clinically trained language models. Our proposed framework\nachieves a state-of-the-art AUROC score averaged across all clinical downstream\ntasks. We observe substantial improvements of 6-9% AUROC score in the\nlarge-scale multilabel classification tasks, such as diagnoses and procedures\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gema_A/0/1/0/all/0/1\">Aryo Gema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daines_L/0/1/0/all/0/1\">Luke Daines</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1\">Pasquale Minervini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alex_B/0/1/0/all/0/1\">Beatrice Alex</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepOnto: A Python Package for Ontology Engineering with Deep Learning. (arXiv:2307.03067v1 [cs.AI])","link":"http://arxiv.org/abs/2307.03067","description":"<p>Applying deep learning techniques, particularly language models (LMs), in\nontology engineering has raised widespread attention. However, deep learning\nframeworks like PyTorch and Tensorflow are predominantly developed for Python\nprogramming, while widely-used ontology APIs, such as the OWL API and Jena, are\nprimarily Java-based. To facilitate seamless integration of these frameworks\nand APIs, we present Deeponto, a Python package designed for ontology\nengineering. The package encompasses a core ontology processing module founded\non the widely-recognised and reliable OWL API, encapsulating its fundamental\nfeatures in a more \"Pythonic\" manner and extending its capabilities to include\nother essential components including reasoning, verbalisation, normalisation,\nprojection, and more. Building on this module, Deeponto offers a suite of\ntools, resources, and algorithms that support various ontology engineering\ntasks, such as ontology alignment and completion, by harnessing deep learning\nmethodologies, primarily pre-trained LMs. In this paper, we also demonstrate\nthe practical utility of Deeponto through two use-cases: the Digital Health\nCoaching in Samsung Research UK and the Bio-ML track of the Ontology Alignment\nEvaluation Initiative (OAEI).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horrocks_I/0/1/0/all/0/1\">Ian Horrocks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allocca_C/0/1/0/all/0/1\">Carlo Allocca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sapkota_B/0/1/0/all/0/1\">Brahmananda Sapkota</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenDelta: A Plug-and-play Library for Parameter-efficient Adaptation of Pre-trained Models. (arXiv:2307.03084v1 [cs.LG])","link":"http://arxiv.org/abs/2307.03084","description":"<p>The scale of large pre-trained models (PTMs) poses significant challenges in\nadapting to downstream tasks due to the high optimization overhead and storage\ncosts associated with full-parameter fine-tuning. To address this, many studies\nexplore parameter-efficient tuning methods, also framed as \"delta tuning\",\nwhich updates only a small subset of parameters, known as \"delta modules\",\nwhile keeping the backbone model's parameters fixed. However, the practicality\nand flexibility of delta tuning have been limited due to existing\nimplementations that directly modify the code of the backbone PTMs and\nhard-code specific delta tuning methods for each PTM. In this paper, we present\nOpenDelta, an open-source library that overcomes these limitations by providing\na plug-and-play implementation of various delta tuning methods. Our novel\ntechniques eliminate the need to modify the backbone PTMs' code, making\nOpenDelta compatible with different, even novel PTMs. OpenDelta is designed to\nbe simple, modular, and extensible, providing a comprehensive platform for\nresearchers and practitioners to adapt large PTMs efficiently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengding Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1\">Xingtai Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Domain Adaptation of Sentence Embeddings using Adapters. (arXiv:2307.03104v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03104","description":"<p>Sentence embeddings enable us to capture the semantic similarity of short\ntexts. Most sentence embedding models are trained for general semantic textual\nsimilarity (STS) tasks. Therefore, to use sentence embeddings in a particular\ndomain, the model must be adapted to it in order to achieve good results.\nUsually, this is done by fine-tuning the entire sentence embedding model for\nthe domain of interest. While this approach yields state-of-the-art results,\nall of the model's weights are updated during fine-tuning, making this method\nresource-intensive. Therefore, instead of fine-tuning entire sentence embedding\nmodels for each target domain individually, we propose to train lightweight\nadapters. These domain-specific adapters do not require fine-tuning all\nunderlying sentence embedding model parameters. Instead, we only train a small\nnumber of additional parameters while keeping the weights of the underlying\nsentence embedding model fixed. Training domain-specific adapters allows always\nusing the same base model and only exchanging the domain-specific adapters to\nadapt sentence embeddings to a specific domain. We show that using adapters for\nparameter-efficient domain adaptation of sentence embeddings yields competitive\nperformance within 1% of a domain-adapted, entirely fine-tuned sentence\nembedding model while only training approximately 3.6% of the parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schopf_T/0/1/0/all/0/1\">Tim Schopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_D/0/1/0/all/0/1\">Dennis Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthes_F/0/1/0/all/0/1\">Florian Matthes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03109","description":"<p>Large language models (LLMs) are gaining increasing popularity in both\nacademia and industry, owing to their unprecedented performance in various\napplications. As LLMs continue to play a vital role in both research and daily\nuse, their evaluation becomes increasingly critical, not only at the task\nlevel, but also at the society level for better understanding of their\npotential risks. Over the past years, significant efforts have been made to\nexamine LLMs from various perspectives. This paper presents a comprehensive\nreview of these evaluation methods for LLMs, focusing on three key dimensions:\nwhat to evaluate, where to evaluate, and how to evaluate. Firstly, we provide\nan overview from the perspective of evaluation tasks, encompassing general\nnatural language processing tasks, reasoning, medical usage, ethics,\neducations, natural and social sciences, agent applications, and other areas.\nSecondly, we answer the `where' and `how' questions by diving into the\nevaluation methods and benchmarks, which serve as crucial components in\nassessing performance of LLMs. Then, we summarize the success and failure cases\nof LLMs in different tasks. Finally, we shed light on several future challenges\nthat lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to\nresearchers in the realm of LLMs evaluation, thereby aiding the development of\nmore proficient LLMs. Our key point is that evaluation should be treated as an\nessential discipline to better assist the development of LLMs. We consistently\nmaintain the related open-source materials at:\nhttps://github.com/MLGroupJLU/LLM-eval-survey.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yupeng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kaijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1\">Xiaoyuan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cunxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yi Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KoRC: Knowledge oriented Reading Comprehension Benchmark for Deep Text Understanding. (arXiv:2307.03115v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03115","description":"<p>Deep text understanding, which requires the connections between a given\ndocument and prior knowledge beyond its text, has been highlighted by many\nbenchmarks in recent years. However, these benchmarks have encountered two\nmajor limitations. On the one hand, most of them require human annotation of\nknowledge, which leads to limited knowledge coverage. On the other hand, they\nusually use choices or spans in the texts as the answers, which results in\nnarrow answer space. To overcome these limitations, we build a new challenging\nbenchmark named KoRc in this paper. Compared with previous benchmarks, KoRC has\ntwo advantages, i.e., broad knowledge coverage and flexible answer format.\nSpecifically, we utilize massive knowledge bases to guide annotators or large\nlanguage models (LLMs) to construct knowledgable questions. Moreover, we use\nlabels in knowledge bases rather than spans or choices as the final answers. We\ntest state-of-the-art models on KoRC and the experimental results show that the\nstrongest baseline only achieves 68.3% and 30.0% F1 measure in the\nin-distribution and out-of-distribution test set, respectively. These results\nindicate that deep text understanding is still an unsolved challenge. The\nbenchmark dataset, leaderboard, and baseline methods are released in\nhttps://github.com/THU-KEG/KoRC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zijun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yantao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1\">Xin Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shulin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jifan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Multi-valued Relations from Language Models. (arXiv:2307.03122v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03122","description":"<p>The widespread usage of latent language representations via pre-trained\nlanguage models (LMs) suggests that they are a promising source of structured\nknowledge. However, existing methods focus only on a single object per\nsubject-relation pair, even though often multiple objects are correct. To\novercome this limitation, we analyze these representations for their potential\nto yield materialized multi-object relational knowledge. We formulate the\nproblem as a rank-then-select task. For ranking candidate objects, we evaluate\nexisting prompting techniques and propose new ones incorporating domain\nknowledge. Among the selection methods, we find that choosing objects with a\nlikelihood above a learned relation-specific threshold gives a 49.5% F1 score.\nOur results highlight the difficulty of employing LMs for the multi-valued\nslot-filling task and pave the way for further research on extracting\nrelational knowledge from latent language representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singhania_S/0/1/0/all/0/1\">Sneha Singhania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1\">Simon Razniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisKoP: Visual Knowledge oriented Programming for Interactive Knowledge Base Question Answering. (arXiv:2307.03130v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03130","description":"<p>We present Visual Knowledge oriented Programming platform (VisKoP), a\nknowledge base question answering (KBQA) system that integrates human into the\nloop to edit and debug the knowledge base (KB) queries. VisKoP not only\nprovides a neural program induction module, which converts natural language\nquestions into knowledge oriented program language (KoPL), but also maps KoPL\nprograms into graphical elements. KoPL programs can be edited with simple\ngraphical operators, such as dragging to add knowledge operators and slot\nfilling to designate operator arguments. Moreover, VisKoP provides\nauto-completion for its knowledge base schema and users can easily debug the\nKoPL program by checking its intermediate results. To facilitate the practical\nKBQA on a million-entity-level KB, we design a highly efficient KoPL execution\nengine for the back-end. Experiment results show that VisKoP is highly\nefficient and user interaction can fix a large portion of wrong KoPL programs\nto acquire the correct answer. The VisKoP online demo\nhttps://demoviskop.xlore.cn (Stable release of this paper) and\nhttps://viskop.xlore.cn (Beta release with new features), highly efficient KoPL\nengine https://pypi.org/project/kopl-engine, and screencast video\nhttps://youtu.be/zAbJtxFPTXo are now publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zijun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanyong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1\">Xin Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shulin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_A/0/1/0/all/0/1\">Amy Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jifan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jianjun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BLEURT Has Universal Translations: An Analysis of Automatic Metrics by Minimum Risk Training. (arXiv:2307.03131v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03131","description":"<p>Automatic metrics play a crucial role in machine translation. Despite the\nwidespread use of n-gram-based metrics, there has been a recent surge in the\ndevelopment of pre-trained model-based metrics that focus on measuring sentence\nsemantics. However, these neural metrics, while achieving higher correlations\nwith human evaluations, are often considered to be black boxes with potential\nbiases that are difficult to detect. In this study, we systematically analyze\nand compare various mainstream and cutting-edge automatic metrics from the\nperspective of their guidance for training machine translation systems. Through\nMinimum Risk Training (MRT), we find that certain metrics exhibit robustness\ndefects, such as the presence of universal adversarial translations in BLEURT\nand BARTScore. In-depth analysis suggests two main causes of these robustness\ndeficits: distribution biases in the training datasets, and the tendency of the\nmetric paradigm. By incorporating token-level constraints, we enhance the\nrobustness of evaluation metrics, which in turn leads to an improvement in the\nperformance of machine translation systems. Codes are available at\n\\url{https://github.com/powerpuffpomelo/fairseq_mrt}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yiming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chengqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"T-MARS: Improving Visual Representations by Circumventing Text Feature Learning. (arXiv:2307.03132v1 [cs.CV])","link":"http://arxiv.org/abs/2307.03132","description":"<p>Large web-sourced multimodal datasets have powered a slew of new methods for\nlearning general-purpose visual representations, advancing the state of the art\nin computer vision and revolutionizing zero- and few-shot recognition. One\ncrucial decision facing practitioners is how, if at all, to curate these\never-larger datasets. For example, the creators of the LAION-5B dataset chose\nto retain only image-caption pairs whose CLIP similarity score exceeded a\ndesignated threshold. In this paper, we propose a new state-of-the-art data\nfiltering approach motivated by our observation that nearly 40% of LAION's\nimages contain text that overlaps significantly with the caption. Intuitively,\nsuch data could be wasteful as it incentivizes models to perform optical\ncharacter recognition rather than learning visual features. However, naively\nremoving all such data could also be wasteful, as it throws away images that\ncontain visual features (in addition to overlapping text). Our simple and\nscalable approach, T-MARS (Text Masking and Re-Scoring), filters out only those\npairs where the text dominates the remaining visual features -- by first\nmasking out the text and then filtering out those with a low CLIP similarity\nscore of the masked image. Experimentally, T-MARS outperforms the top-ranked\nmethod on the \"medium scale\" of DataComp (a data filtering benchmark) by a\nmargin of 6.5% on ImageNet and 4.7% on VTAB. Additionally, our systematic\nevaluation on various data pool sizes from 2M to 64M shows that the accuracy\ngains enjoyed by T-MARS linearly increase as data and compute are scaled\nexponentially. Code is available at https://github.com/locuslab/T-MARS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maini_P/0/1/0/all/0/1\">Pratyush Maini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_S/0/1/0/all/0/1\">Sachin Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1\">J. Zico Kolter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1\">Aditi Raghunathan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Large Vision-Language Model with Out-of-Distribution Generalizability. (arXiv:2307.03135v1 [cs.CV])","link":"http://arxiv.org/abs/2307.03135","description":"<p>Large vision-language models have achieved outstanding performance, but their\nsize and computational requirements make their deployment on\nresource-constrained devices and time-sensitive tasks impractical. Model\ndistillation, the process of creating smaller, faster models that maintain the\nperformance of larger models, is a promising direction towards the solution.\nThis paper investigates the distillation of visual representations in large\nteacher vision-language models into lightweight student models using a small-\nor mid-scale dataset. Notably, this study focuses on open-vocabulary\nout-of-distribution (OOD) generalization, a challenging problem that has been\noverlooked in previous model distillation literature. We propose two principles\nfrom vision and language modality perspectives to enhance student's OOD\ngeneralization: (1) by better imitating teacher's visual representation space,\nand carefully promoting better coherence in vision-language alignment with the\nteacher; (2) by enriching the teacher's language representations with\ninformative and finegrained semantic attributes to effectively distinguish\nbetween different labels. We propose several metrics and conduct extensive\nexperiments to investigate their techniques. The results demonstrate\nsignificant improvements in zero-shot and few-shot student performance on\nopen-vocabulary out-of-distribution classification, highlighting the\neffectiveness of our proposed approaches. Our code will be released at\nhttps://github.com/xuanlinli17/large_vlm_distillation_ood\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yunhao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minghua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhan Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhuowen Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focused Transformer: Contrastive Training for Context Scaling. (arXiv:2307.03170v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03170","description":"<p>Large language models have an exceptional capability to incorporate new\ninformation in a contextual manner. However, the full potential of such an\napproach is often restrained due to a limitation in the effective context\nlength. One solution to this issue is to endow an attention layer with access\nto an external memory, which comprises of (key, value) pairs. Yet, as the\nnumber of documents increases, the proportion of relevant keys to irrelevant\nones decreases, leading the model to focus more on the irrelevant keys. We\nidentify a significant challenge, dubbed the distraction issue, where keys\nlinked to different semantic values might overlap, making them hard to\ndistinguish. To tackle this problem, we introduce the Focused Transformer\n(FoT), a technique that employs a training process inspired by contrastive\nlearning. This novel approach enhances the structure of the (key, value) space,\nenabling an extension of the context length. Our method allows for fine-tuning\npre-existing, large-scale models to lengthen their effective context. This is\ndemonstrated by our fine-tuning of $3B$ and $7B$ OpenLLaMA checkpoints. The\nresulting models, which we name LongLLaMA, exhibit advancements in tasks\nrequiring a long context. We further illustrate that our LongLLaMA models\nadeptly manage a $256 k$ context length for passkey retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tworkowski_S/0/1/0/all/0/1\">Szymon Tworkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staniszewski_K/0/1/0/all/0/1\">Konrad Staniszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacek_M/0/1/0/all/0/1\">Miko&#x142;aj Pacek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuhuai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1\">Henryk Michalewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milos_P/0/1/0/all/0/1\">Piotr Mi&#x142;o&#x15b;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lost in the Middle: How Language Models Use Long Contexts. (arXiv:2307.03172v1 [cs.CL])","link":"http://arxiv.org/abs/2307.03172","description":"<p>While recent language models have the ability to take long contexts as input,\nrelatively little is known about how well the language models use longer\ncontext. We analyze language model performance on two tasks that require\nidentifying relevant information within their input contexts: multi-document\nquestion answering and key-value retrieval. We find that performance is often\nhighest when relevant information occurs at the beginning or end of the input\ncontext, and significantly degrades when models must access relevant\ninformation in the middle of long contexts. Furthermore, performance\nsubstantially decreases as the input context grows longer, even for explicitly\nlong-context models. Our analysis provides a better understanding of how\nlanguage models use their input context and provides new evaluation protocols\nfor future long-context models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nelson F. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kevin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hewitt_J/0/1/0/all/0/1\">John Hewitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paranjape_A/0/1/0/all/0/1\">Ashwin Paranjape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bevilacqua_M/0/1/0/all/0/1\">Michele Bevilacqua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1\">Fabio Petroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond. (arXiv:2204.09269v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.09269","description":"<p>Non-autoregressive (NAR) generation, which is first proposed in neural\nmachine translation (NMT) to speed up inference, has attracted much attention\nin both machine learning and natural language processing communities. While NAR\ngeneration can significantly accelerate inference speed for machine\ntranslation, the speedup comes at the cost of sacrificed translation accuracy\ncompared to its counterpart, autoregressive (AR) generation. In recent years,\nmany new models and algorithms have been designed/proposed to bridge the\naccuracy gap between NAR generation and AR generation. In this paper, we\nconduct a systematic survey with comparisons and discussions of various\nnon-autoregressive translation (NAT) models from different aspects.\nSpecifically, we categorize the efforts of NAT into several groups, including\ndata manipulation, modeling methods, training criterion, decoding algorithms,\nand the benefit from pre-trained models. Furthermore, we briefly review other\napplications of NAR models beyond machine translation, such as grammatical\nerror correction, text summarization, text style transfer, dialogue, semantic\nparsing, automatic speech recognition, and so on. In addition, we also discuss\npotential directions for future exploration, including releasing the dependency\nof KD, reasonable training objectives, pre-training for NAR, and wider\napplications, etc. We hope this survey can help researchers capture the latest\nprogress in NAR generation, inspire the design of advanced NAR models and\nalgorithms, and enable industry practitioners to choose appropriate solutions\nfor their applications. The web page of this survey is at\n\\url{https://github.com/LitterBrother-Xiao/Overview-of-Non-autoregressive-Applications}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yisheng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lijun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Junliang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Androids Laugh at Electric Sheep? Humor \"Understanding\" Benchmarks from The New Yorker Caption Contest. (arXiv:2209.06293v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.06293","description":"<p>Large neural networks can now generate jokes, but do they really \"understand\"\nhumor? We challenge AI models with three tasks derived from the New Yorker\nCartoon Caption Contest: matching a joke to a cartoon, identifying a winning\ncaption, and explaining why a winning caption is funny. These tasks encapsulate\nprogressively more sophisticated aspects of \"understanding\" a cartoon; key\nelements are the complex, often surprising relationships between images and\ncaptions and the frequent inclusion of indirect and playful allusions to human\nexperience and culture. We investigate both multimodal and language-only\nmodels: the former are challenged with the cartoon images directly, while the\nlatter are given multifaceted descriptions of the visual scene to simulate\nhuman-level visual understanding. We find that both types of models struggle at\nall three tasks. For example, our best multimodal models fall 30 accuracy\npoints behind human performance on the matching task, and, even when provided\nground-truth visual scene descriptors, human-authored explanations are\npreferred head-to-head over the best machine-authored ones (few-shot GPT-4) in\nmore than 2/3 of cases. We release models, code, leaderboard, and corpus, which\nincludes newly-gathered annotations describing the image's locations/entities,\nwhat's unusual in the scene, and an explanation of the joke.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marasovic_A/0/1/0/all/0/1\">Ana Marasovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1\">Lillian Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Da_J/0/1/0/all/0/1\">Jeff Da</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1\">Rowan Zellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mankoff_R/0/1/0/all/0/1\">Robert Mankoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Chinese Spelling Check Framework Based on Reverse Contrastive Learning. (arXiv:2210.13823v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.13823","description":"<p>Chinese spelling check is a task to detect and correct spelling mistakes in\nChinese text. Existing research aims to enhance the text representation and use\nmulti-source information to improve the detection and correction capabilities\nof models, but does not pay too much attention to improving their ability to\ndistinguish between confusable words. Contrastive learning, whose aim is to\nminimize the distance in representation space between similar sample pairs, has\nrecently become a dominant technique in natural language processing. Inspired\nby contrastive learning, we present a novel framework for Chinese spelling\nchecking, which consists of three modules: language representation, spelling\ncheck and reverse contrastive learning. Specifically, we propose a reverse\ncontrastive learning strategy, which explicitly forces the model to minimize\nthe agreement between the similar examples, namely, the phonetically and\nvisually confusable characters. Experimental results show that our framework is\nmodel-agnostic and could be combined with existing Chinese spelling check\nmodels to yield state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hongyan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1\">Sihui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Aimin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Weakly-Supervised Streaming Multilingual Speech Model with Truly Zero-Shot Capability. (arXiv:2211.02499v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.02499","description":"<p>In this paper, we introduce our work of building a Streaming Multilingual\nSpeech Model (SM2), which can transcribe or translate multiple spoken languages\ninto texts of the target language. The backbone of SM2 is Transformer\nTransducer, which has high streaming capability. Instead of human labeled\nspeech translation (ST) data, SM2 models are trained using weakly supervised\ndata generated by converting the transcriptions in speech recognition corpora\nwith a machine translation service. With 351 thousand hours of anonymized\nspeech training data from 25 languages, SM2 models achieve comparable or even\nbetter ST quality than some recent popular large-scale non-streaming speech\nmodels. More importantly, we show that SM2 has the truly zero-shot capability\nwhen expanding to new target languages, yielding high quality ST results for\n{source-speech, target-text} pairs that are not seen during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jian Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_E/0/1/0/all/0/1\">Eric Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Effective Employment of Contrastive Learning in Multi-label Text Classification. (arXiv:2212.00552v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.00552","description":"<p>The effectiveness of contrastive learning technology in natural language\nprocessing tasks is yet to be explored and analyzed. How to construct positive\nand negative samples correctly and reasonably is the core challenge of\ncontrastive learning. It is even harder to discover contrastive objects in\nmulti-label text classification tasks. There are very few contrastive losses\nproposed previously. In this paper, we investigate the problem from a different\nangle by proposing five novel contrastive losses for multi-label text\nclassification tasks. These are Strict Contrastive Loss (SCL), Intra-label\nContrastive Loss (ICL), Jaccard Similarity Contrastive Loss (JSCL), Jaccard\nSimilarity Probability Contrastive Loss (JSPCL), and Stepwise Label Contrastive\nLoss (SLCL). We explore the effectiveness of contrastive learning for\nmulti-label text classification tasks by the employment of these novel losses\nand provide a set of baseline models for deploying contrastive learning\ntechniques on specific tasks. We further perform an interpretable analysis of\nour approach to show how different components of contrastive learning losses\nplay their roles. The experimental results show that our proposed contrastive\nlosses can bring improvement to multi-label text classification tasks. Our work\nalso explores how contrastive learning should be adapted for multi-label text\nclassification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_G/0/1/0/all/0/1\">Guanqiu Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jigang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Aimin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dong Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model. (arXiv:2212.09811v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09811","description":"<p>Compared to conventional bilingual translation systems, massively\nmultilingual machine translation is appealing because a single model can\ntranslate into multiple languages and benefit from knowledge transfer for low\nresource languages. On the other hand, massively multilingual models suffer\nfrom the curse of multilinguality, unless scaling their size massively, which\nincreases their training and inference costs. Sparse Mixture-of-Experts models\nare a way to drastically increase model capacity without the need for a\nproportional amount of computing. The recently released NLLB-200 is an example\nof such a model. It covers 202 languages but requires at least four 32GB GPUs\njust for inference. In this work, we propose a pruning method that allows the\nremoval of up to 80\\% of experts with a negligible loss in translation quality,\nwhich makes it feasible to run the model on a single 32GB GPU. Further analysis\nsuggests that our pruning metrics allow to identify language-specific experts\nand prune non-relevant experts for a given language pair.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koishekenov_Y/0/1/0/all/0/1\">Yeskendir Koishekenov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1\">Vassilina Nikoulina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berard_A/0/1/0/all/0/1\">Alexandre Berard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computer says \"No\": The Case Against Empathetic Conversational AI. (arXiv:2212.10983v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10983","description":"<p>Emotions are an integral part of human cognition and they guide not only our\nunderstanding of the world but also our actions within it. As such, whether we\nsoothe or flame an emotion is not inconsequential. Recent work in\nconversational AI has focused on responding empathetically to users, validating\nand soothing their emotions without a real basis. This AI-aided emotional\nregulation can have negative consequences for users and society, tending\ntowards a one-noted happiness defined as only the absence of \"negative\"\nemotions. We argue that we must carefully consider whether and how to respond\nto users' emotions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Curry_A/0/1/0/all/0/1\">Alba Curry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curry_A/0/1/0/all/0/1\">Amanda Cercas Curry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generation of Highlights from Research Papers Using Pointer-Generator Networks and SciBERT Embeddings. (arXiv:2302.07729v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.07729","description":"<p>Nowadays many research articles are prefaced with research highlights to\nsummarize the main findings of the paper. Highlights not only help researchers\nprecisely and quickly identify the contributions of a paper, they also enhance\nthe discoverability of the article via search engines. We aim to automatically\nconstruct research highlights given certain segments of a research paper. We\nuse a pointer-generator network with coverage mechanism and a contextual\nembedding layer at the input that encodes the input tokens into SciBERT\nembeddings. We test our model on a benchmark dataset, CSPubSum, and also\npresent MixSub, a new multi-disciplinary corpus of papers for automatic\nresearch highlight generation. For both CSPubSum and MixSub, we have observed\nthat the proposed model achieves the best performance compared to related\nvariants and other models proposed in the literature. On the CSPubSum dataset,\nour model achieves the best performance when the input is only the abstract of\na paper as opposed to other segments of the paper. It produces ROUGE-1, ROUGE-2\nand ROUGE-L F1-scores of 38.26, 14.26 and 35.51, respectively, METEOR score of\n32.62, and BERTScore F1 of 86.65 which outperform all other baselines. On the\nnew MixSub dataset, where only the abstract is the input, our proposed model\n(when trained on the whole training corpus without distinguishing between the\nsubject categories) achieves ROUGE-1, ROUGE-2 and ROUGE-L F1-scores of 31.78,\n9.76 and 29.3, respectively, METEOR score of 24.00, and BERTScore F1 of 85.25.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rehman_T/0/1/0/all/0/1\">Tohida Rehman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_D/0/1/0/all/0/1\">Debarshi Kumar Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1\">Samiran Chattopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhowmick_P/0/1/0/all/0/1\">Plaban Kumar Bhowmick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Partha Pratim Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unstructured and structured data: Can we have the best of both worlds with large language models?. (arXiv:2304.13010v2 [cs.DB] UPDATED)","link":"http://arxiv.org/abs/2304.13010","description":"<p>This paper presents an opinion on the potential of using large language\nmodels to query on both unstructured and structured data. It also outlines some\nresearch challenges related to the topic of building question-answering systems\nfor both types of data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wang-Chiew Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models. (arXiv:2305.01645v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01645","description":"<p>Fine-tuning large models is highly effective, however, inference can be\nexpensive and produces carbon emissions. Knowledge distillation has been shown\nto be a practical solution to reduce inference costs, but the distillation\nprocess itself requires significant computational resources. Rather than buying\nor renting GPUs to fine-tune, then distill a large model, an NLP practitioner\nmight instead choose to allocate the available budget to hire annotators and\nmanually label additional fine-tuning data. In this paper, we investigate how\nto most efficiently use a fixed budget to build a compact model. Through\nextensive experiments on six diverse tasks, we show that distilling from T5-XXL\n(11B) to T5-Small (60M) is almost always a cost-efficient strategy compared to\nannotating more data to directly train a compact model (T5-Small). We further\ninvestigate how the optimal budget allocated towards computation varies across\nscenarios. We will make our code, datasets, annotation cost estimates, and\nbaseline models available as a benchmark to support further work on\ncost-efficient training of compact models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Junmo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models. (arXiv:2305.08283v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08283","description":"<p>Language models (LMs) are pretrained on diverse data sources, including news,\ndiscussion forums, books, and online encyclopedias. A significant portion of\nthis data includes opinions and perspectives which, on one hand, celebrate\ndemocracy and diversity of ideas, and on the other hand are inherently socially\nbiased. Our work develops new methods to (1) measure political biases in LMs\ntrained on such corpora, along social and economic axes, and (2) measure the\nfairness of downstream NLP models trained on top of politically biased LMs. We\nfocus on hate speech and misinformation detection, aiming to empirically\nquantify the effects of political (social, economic) biases in pretraining data\non the fairness of high-stakes social-oriented tasks. Our findings reveal that\npretrained LMs do have political leanings that reinforce the polarization\npresent in pretraining corpora, propagating social biases into hate speech\npredictions and misinformation detectors. We discuss the implications of our\nfindings for NLP research and propose future directions to mitigate unfairness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shangbin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chan Young Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuhan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised representations in speech-based depression detection. (arXiv:2305.12263v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.12263","description":"<p>This paper proposes handling training data sparsity in speech-based automatic\ndepression detection (SDD) using foundation models pre-trained with\nself-supervised learning (SSL). An analysis of SSL representations derived from\ndifferent layers of pre-trained foundation models is first presented for SDD,\nwhich provides insight to suitable indicator for depression detection.\nKnowledge transfer is then performed from automatic speech recognition (ASR)\nand emotion recognition to SDD by fine-tuning the foundation models. Results\nshow that the uses of oracle and ASR transcriptions yield similar SDD\nperformance when the hidden representations of the ASR model is incorporated\nalong with the ASR textual information. By integrating representations from\nmultiple foundation models, state-of-the-art SDD results based on real ASR were\nachieved on the DAIC-WOZ dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibration of Transformer-based Models for Identifying Stress and Depression in Social Media. (arXiv:2305.16797v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16797","description":"<p>In today's fast-paced world, the rates of stress and depression present a\nsurge. Social media provide assistance for the early detection of mental health\nconditions. Existing methods mainly introduce feature extraction approaches and\ntrain shallow machine learning classifiers. Other researches use deep neural\nnetworks or transformers. Despite the fact that transformer-based models\nachieve noticeable improvements, they cannot often capture rich factual\nknowledge. Although there have been proposed a number of studies aiming to\nenhance the pretrained transformer-based models with extra information or\nadditional modalities, no prior work has exploited these modifications for\ndetecting stress and depression through social media. In addition, although the\nreliability of a machine learning model's confidence in its predictions is\ncritical for high-risk applications, there is no prior work taken into\nconsideration the model calibration. To resolve the above issues, we present\nthe first study in the task of depression and stress detection in social media,\nwhich injects extra linguistic information in transformer-based models, namely\nBERT and MentalBERT. Specifically, the proposed approach employs a Multimodal\nAdaptation Gate for creating the combined embeddings, which are given as input\nto a BERT (or MentalBERT) model. For taking into account the model calibration,\nwe apply label smoothing. We test our proposed approaches in three publicly\navailable datasets and demonstrate that the integration of linguistic features\ninto transformer-based models presents a surge in the performance. Also, the\nusage of label smoothing contributes to both the improvement of the model's\nperformance and the calibration of the model. We finally perform a linguistic\nanalysis of the posts and show differences in language between stressful and\nnon-stressful texts, as well as depressive and non-depressive posts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ilias_L/0/1/0/all/0/1\">Loukas Ilias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouzakitis_S/0/1/0/all/0/1\">Spiros Mouzakitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askounis_D/0/1/0/all/0/1\">Dimitris Askounis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18486","description":"<p>The development of large language models (LLMs) such as ChatGPT has brought a\nlot of attention recently. However, their evaluation in the benchmark academic\ndatasets remains under-explored due to the difficulty of evaluating the\ngenerative outputs produced by this model against the ground truth. In this\npaper, we aim to present a thorough evaluation of ChatGPT's performance on\ndiverse academic datasets, covering tasks like question-answering, text\nsummarization, code generation, commonsense reasoning, mathematical\nproblem-solving, machine translation, bias detection, and ethical\nconsiderations. Specifically, we evaluate ChatGPT across 140 tasks and analyze\n255K responses it generates in these datasets. This makes our work the largest\nevaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate\nthe strengths and weaknesses of ChatGPT in various tasks and provide insights\nfor future research using LLMs. We also report a new emergent ability to follow\nmulti-query instructions that we mostly found in ChatGPT and other\ninstruction-tuned models. Our extensive evaluation shows that even though\nChatGPT is capable of performing a wide variety of tasks, and may obtain\nimpressive performance in several benchmark datasets, it is still far from\nachieving the ability to reliably solve many challenging tasks. By providing a\nthorough assessment of ChatGPT's performance across diverse NLP tasks, this\npaper sets the stage for a targeted deployment of ChatGPT-like LLMs in\nreal-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laskar_M/0/1/0/all/0/1\">Md Tahmid Rahman Laskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1\">M Saiful Bari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mizanur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhuiyan_M/0/1/0/all/0/1\">Md Amran Hossen Bhuiyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jimmy Xiangji Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers. (arXiv:2306.04504v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.04504","description":"<p>ChatGPT is a large language model developed by OpenAI. Despite its impressive\nperformance across various tasks, no prior work has investigated its capability\nin the biomedical domain yet. To this end, this paper aims to evaluate the\nperformance of ChatGPT on various benchmark biomedical tasks, such as relation\nextraction, document classification, question answering, and summarization. To\nthe best of our knowledge, this is the first work that conducts an extensive\nevaluation of ChatGPT in the biomedical domain. Interestingly, we find based on\nour evaluation that in biomedical datasets that have smaller training sets,\nzero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative\ntransformer models, such as BioGPT and BioBART. This suggests that ChatGPT's\npre-training on large text corpora makes it quite specialized even in the\nbiomedical domain. Our findings demonstrate that ChatGPT has the potential to\nbe a valuable tool for various tasks in the biomedical domain that lack large\nannotated data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jahan_I/0/1/0/all/0/1\">Israt Jahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskar_M/0/1/0/all/0/1\">Md Tahmid Rahman Laskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1\">Chun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jimmy Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection. (arXiv:2306.04637v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2306.04637","description":"<p>Neural sequence models based on the transformer architecture have\ndemonstrated remarkable \\emph{in-context learning} (ICL) abilities, where they\ncan perform new tasks when prompted with training and test examples, without\nany parameter update to the model. This work first provides a comprehensive\nstatistical theory for transformers to perform ICL. Concretely, we show that\ntransformers can implement a broad class of standard machine learning\nalgorithms in context, such as least squares, ridge regression, Lasso, learning\ngeneralized linear models, and gradient descent on two-layer neural networks,\nwith near-optimal predictive power on various in-context data distributions.\nUsing an efficient implementation of in-context gradient descent as the\nunderlying mechanism, our transformer constructions admit mild size bounds, and\ncan be learned with polynomially many pretraining sequences.\n</p>\n<p>Building on these ``base'' ICL algorithms, intriguingly, we show that\ntransformers can implement more complex ICL procedures involving\n\\emph{in-context algorithm selection}, akin to what a statistician can do in\nreal life -- A \\emph{single} transformer can adaptively select different base\nICL algorithms -- or even perform qualitatively different tasks -- on different\ninput sequences, without any explicit prompting of the right algorithm or task.\nWe both establish this in theory by explicit constructions, and also observe\nthis phenomenon experimentally. In theory, we construct two general mechanisms\nfor algorithm selection with concrete examples: pre-ICL testing, and post-ICL\nvalidation. As an example, we use the post-ICL validation mechanism to\nconstruct a transformer that can perform nearly Bayes-optimal ICL on a\nchallenging task -- noisy linear models with mixed noise levels.\nExperimentally, we demonstrate the strong in-context algorithm selection\ncapabilities of standard transformer architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1\">Song Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Impact of ChatGPT and LLMs on Medical Imaging Stakeholders: Perspectives and Use Cases. (arXiv:2306.06767v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2306.06767","description":"<p>This study investigates the transformative potential of Large Language Models\n(LLMs), such as OpenAI ChatGPT, in medical imaging. With the aid of public\ndata, these models, which possess remarkable language understanding and\ngeneration capabilities, are augmenting the interpretive skills of\nradiologists, enhancing patient-physician communication, and streamlining\nclinical workflows. The paper introduces an analytic framework for presenting\nthe complex interactions between LLMs and the broader ecosystem of medical\nimaging stakeholders, including businesses, insurance entities, governments,\nresearch institutions, and hospitals (nicknamed BIGR-H). Through detailed\nanalyses, illustrative use cases, and discussions on the broader implications\nand future directions, this perspective seeks to raise discussion in strategic\nplanning and decision-making in the era of AI-enabled healthcare.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jiancheng Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hongwei Bran Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_D/0/1/0/all/0/1\">Donglai Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models. (arXiv:2306.14096v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.14096","description":"<p>Entity-level fine-grained sentiment analysis in the financial domain is a\ncrucial subtask of sentiment analysis and currently faces numerous challenges.\nThe primary challenge stems from the lack of high-quality and large-scale\nannotated corpora specifically designed for financial text sentiment analysis,\nwhich in turn limits the availability of data necessary for developing\neffective text processing techniques. Recent advancements in large language\nmodels (LLMs) have yielded remarkable performance in natural language\nprocessing tasks, primarily centered around language pattern matching. In this\npaper, we propose a novel and extensive Chinese fine-grained financial\nsentiment analysis dataset, FinChina SA, for enterprise early warning. We\nthoroughly evaluate and experiment with well-known existing open-source LLMs\nusing our dataset. We firmly believe that our dataset will serve as a valuable\nresource to advance the exploration of real-world financial sentiment analysis\ntasks, which should be the focus of future research. Our dataset and all code\nto replicate the experimental results will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yinyu Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanru Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Weiqiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Youhao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Singing Voice Conversion Challenge 2023. (arXiv:2306.14422v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2306.14422","description":"<p>We present the latest iteration of the voice conversion challenge (VCC)\nseries, a bi-annual scientific event aiming to compare and understand different\nvoice conversion (VC) systems based on a common dataset. This year we shifted\nour focus to singing voice conversion (SVC), thus named the challenge the\nSinging Voice Conversion Challenge (SVCC). A new database was constructed for\ntwo tasks, namely in-domain and cross-domain SVC. The challenge was run for two\nmonths, and in total we received 26 submissions, including 2 baselines. Through\na large-scale crowd-sourced listening test, we observed that for both tasks,\nalthough human-level naturalness was achieved by the top system, no team was\nable to obtain a similarity score as high as the target speakers. Also, as\nexpected, cross-domain SVC is harder than in-domain SVC, especially in the\nsimilarity aspect. We also investigated whether existing objective measurements\nwere able to predict perceptual performance, and found that only few of them\ncould reach a significant correlation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wen-Chin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Violeta_L/0/1/0/all/0/1\">Lester Phillip Violeta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songxiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM Calibration and Automatic Hallucination Detection via Pareto Optimal Self-supervision. (arXiv:2306.16564v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.16564","description":"<p>Large language models (LLMs) have demonstrated remarkable capabilities out of\nbox for a wide range of applications, yet accuracy still remains a major growth\narea, especially in mission-critical domains such as biomedicine. An effective\nmethod to calibrate the confidence level on LLM responses is essential to\nautomatically detect errors and facilitate human-in-the-loop verification. An\nimportant source of calibration signals stems from expert-stipulated\nprogrammatic supervision, which is often available at low cost but has its own\nlimitations such as noise and coverage. In this paper, we introduce a Pareto\noptimal self-supervision framework that can leverage available programmatic\nsupervision to systematically calibrate LLM responses by producing a risk score\nfor every response, without any additional manual efforts. This is accomplished\nby learning a harmonizer model to align LLM output with other available\nsupervision sources, which would assign higher risk scores to more uncertain\nLLM responses and facilitate error correction. Experiments on standard relation\nextraction tasks in biomedical and general domains demonstrate the promise of\nthis approach, with our proposed risk scores highly correlated with the real\nerror rate of LLMs. For the most uncertain test instances, dynamic prompting\nbased on our proposed risk scores results in significant accuracy improvement\nfor off-the-shelf LLMs, boosting GPT-3 results past state-of-the-art (SOTA)\nweak supervision and GPT-4 results past SOTA supervised results on challenging\nevaluation datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Theodore Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Mu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preston_J/0/1/0/all/0/1\">J. Samuel Preston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biomedical Language Models are Robust to Sub-optimal Tokenization. (arXiv:2306.17649v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.17649","description":"<p>As opposed to general English, many concepts in biomedical terminology have\nbeen designed in recent history by biomedical professionals with the goal of\nbeing precise and concise. This is often achieved by concatenating meaningful\nbiomedical morphemes to create new semantic units. Nevertheless, most modern\nbiomedical language models (LMs) are pre-trained using standard domain-specific\ntokenizers derived from large scale biomedical corpus statistics without\nexplicitly leveraging the agglutinating nature of biomedical language. In this\nwork, we first find that standard open-domain and biomedical tokenizers are\nlargely unable to segment biomedical terms into meaningful components.\nTherefore, we hypothesize that using a tokenizer which segments biomedical\nterminology more accurately would enable biomedical LMs to improve their\nperformance on downstream biomedical NLP tasks, especially ones which involve\nbiomedical terms directly such as named entity recognition (NER) and entity\nlinking. Surprisingly, we find that pre-training a biomedical LM using a more\naccurate biomedical tokenizer does not improve the entity representation\nquality of a language model as measured by several intrinsic and extrinsic\nmeasures such as masked language modeling prediction (MLM) accuracy as well as\nNER and entity linking performance. These quantitative findings, along with a\ncase study which explores entity representation quality more directly, suggest\nthat the biomedical pre-training process is quite robust to instances of\nsub-optimal tokenization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_B/0/1/0/all/0/1\">Bernal Jim&#xe9;nez Guti&#xe9;rrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Matters: A New Dataset and Empirical Study for Multimodal Hyperbole Detection. (arXiv:2307.00209v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2307.00209","description":"<p>Hyperbole, or exaggeration, is a common linguistic phenomenon. The detection\nof hyperbole is an important part of understanding human expression. There have\nbeen several studies on hyperbole detection, but most of which focus on text\nmodality only. However, with the development of social media, people can create\nhyperbolic expressions with various modalities, including text, images, videos,\netc. In this paper, we focus on multimodal hyperbole detection. We create a\nmultimodal detection dataset\\footnote{The dataset will be released to the\ncommunity.} from Weibo (a Chinese social media) and carry out some studies on\nit. We treat the text and image from a piece of weibo as two modalities and\nexplore the role of text and image for hyperbole detection. Different\npre-trained multimodal encoders are also evaluated on this downstream task to\nshow their performance. Besides, since this dataset is constructed from five\ndifferent topics, we also evaluate the cross-domain performance of different\nmodels. These studies can serve as a benchmark and point out the direction of\nfurther study on multimodal hyperbole detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huixuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework. (arXiv:2307.01715v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.01715","description":"<p>Connectionist Temporal Classification (CTC) is a widely used criterion for\ntraining supervised sequence-to-sequence (seq2seq) models. It enables learning\nthe relations between input and output sequences, termed alignments, by\nmarginalizing over perfect alignments (that yield the ground truth), at the\nexpense of imperfect alignments. This binary differentiation of perfect and\nimperfect alignments falls short of capturing other essential alignment\nproperties that hold significance in other real-world applications. Here we\npropose $\\textit{Align With Purpose}$, a $\\textbf{general Plug-and-Play\nframework}$ for enhancing a desired property in models trained with the CTC\ncriterion. We do that by complementing the CTC with an additional loss term\nthat prioritizes alignments according to a desired property. Our method does\nnot require any intervention in the CTC loss function, enables easy\noptimization of a variety of properties, and allows differentiation between\nboth perfect and imperfect alignments. We apply our framework in the domain of\nAutomatic Speech Recognition (ASR) and show its generality in terms of property\nselection, architectural choice, and scale of training dataset (up to 280,000\nhours). To demonstrate the effectiveness of our framework, we apply it to two\nunrelated properties: emission time and word error rate (WER). For the former,\nwe report an improvement of up to 570ms in latency optimization with a minor\nreduction in WER, and for the latter, we report a relative improvement of 4.5%\nWER over the baseline models. To the best of our knowledge, these applications\nhave never been demonstrated to work on a scale of data as large as ours.\nNotably, our method can be implemented using only a few lines of code, and can\nbe extended to other alignment-free loss functions and to domains other than\nASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Segev_E/0/1/0/all/0/1\">Eliya Segev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alroy_M/0/1/0/all/0/1\">Maya Alroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsir_R/0/1/0/all/0/1\">Ronen Katsir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wies_N/0/1/0/all/0/1\">Noam Wies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenhav_A/0/1/0/all/0/1\">Ayana Shenhav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Oren_Y/0/1/0/all/0/1\">Yael Ben-Oren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zar_D/0/1/0/all/0/1\">David Zar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadmor_O/0/1/0/all/0/1\">Oren Tadmor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitterman_J/0/1/0/all/0/1\">Jacob Bitterman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1\">Amnon Shashua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenwein_T/0/1/0/all/0/1\">Tal Rosenwein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformed Protoform Reconstruction. (arXiv:2307.01896v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.01896","description":"<p>Protoform reconstruction is the task of inferring what morphemes or words\nappeared like in the ancestral languages of a set of daughter languages. Meloni\net al. (2021) achieved the state-of-the-art on Latin protoform reconstruction\nwith an RNN-based encoder-decoder with attention model. We update their model\nwith the state-of-the-art seq2seq model: the Transformer. Our model outperforms\ntheir model on a suite of different metrics on two different datasets: their\nRomance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou\n2004) of 800+ cognates spanning 39 varieties. We also probe our model for\npotential phylogenetic signal contained in the model. Our code is publicly\navailable at https://github.com/cmu-llab/acl-2023.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Min Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kalvin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Chenxuan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortensen_D/0/1/0/all/0/1\">David Mortensen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Utilizing ChatGPT Generated Data to Retrieve Depression Symptoms from Social Media. (arXiv:2307.02313v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.02313","description":"<p>In this work, we present the contribution of the BLUE team in the eRisk Lab\ntask on searching for symptoms of depression. The task consists of retrieving\nand ranking Reddit social media sentences that convey symptoms of depression\nfrom the BDI-II questionnaire. Given that synthetic data provided by LLMs have\nbeen proven to be a reliable method for augmenting data and fine-tuning\ndownstream models, we chose to generate synthetic data using ChatGPT for each\nof the symptoms of the BDI-II questionnaire. We designed a prompt such that the\ngenerated data contains more richness and semantic diversity than the BDI-II\nresponses for each question and, at the same time, contains emotional and\nanecdotal experiences that are specific to the more intimate way of sharing\nexperiences on Reddit. We perform semantic search and rank the sentences'\nrelevance to the BDI-II symptoms by cosine similarity. We used two\nstate-of-the-art transformer-based models (MentalRoBERTa and a variant of\nMPNet) for embedding the social media posts, the original and generated\nresponses of the BDI-II. Our results show that using sentence embeddings from a\nmodel designed for semantic search outperforms the approach using embeddings\nfrom a model pre-trained on mental health data. Furthermore, the generated\nsynthetic data were proved too specific for this task, the approach simply\nrelying on the BDI-II responses had the best performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1\">Ana-Maria Bucur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deductive Additivity for Planning of Natural Language Proofs. (arXiv:2307.02472v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.02472","description":"<p>Current natural language systems designed for multi-step claim validation\ntypically operate in two phases: retrieve a set of relevant premise statements\nusing heuristics (planning), then generate novel conclusions from those\nstatements using a large language model (deduction). The planning step often\nrequires expensive Transformer operations and does not scale to arbitrary\nnumbers of premise statements. In this paper, we investigate whether an\nefficient planning heuristic is possible via embedding spaces compatible with\ndeductive reasoning. Specifically, we evaluate whether embedding spaces exhibit\na property we call deductive additivity: the sum of premise statement\nembeddings should be close to embeddings of conclusions based on those\npremises. We explore multiple sources of off-the-shelf dense embeddings in\naddition to fine-tuned embeddings from GPT3 and sparse embeddings from BM25. We\nstudy embedding models both intrinsically, evaluating whether the property of\ndeductive additivity holds, and extrinsically, using them to assist planning in\nnatural language proof generation. Lastly, we create a dataset, Single-Step\nReasoning Contrast (SSRC), to further probe performance on various reasoning\ntypes. Our findings suggest that while standard embedding methods frequently\nembed conclusions near the sums of their premises, they fall short of being\neffective heuristics and lack the ability to model certain categories of\nreasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sprague_Z/0/1/0/all/0/1\">Zayne Sprague</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bostrom_K/0/1/0/all/0/1\">Kaj Bostrom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Swarat Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-07-06T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}