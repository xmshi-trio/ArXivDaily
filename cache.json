{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-11-15T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions. (arXiv:2311.07582v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07582","description":"<p>Recent advances in Large Language Models (LLMs) have presented new\nopportunities for integrating Artificial General Intelligence (AGI) into\nbiological research and education. This study evaluated the capabilities of\nleading LLMs, including GPT-4, GPT-3.5, PaLM2, Claude2, and SenseNova, in\nanswering conceptual biology questions. The models were tested on a\n108-question multiple-choice exam covering biology topics in molecular biology,\nbiological techniques, metabolic engineering, and synthetic biology. Among the\nmodels, GPT-4 achieved the highest average score of 90 and demonstrated the\ngreatest consistency across trials with different prompts. The results\nindicated GPT-4's proficiency in logical reasoning and its potential to aid\nbiology research through capabilities like data analysis, hypothesis\ngeneration, and knowledge integration. However, further development and\nvalidation are still required before the promise of LLMs in accelerating\nbiological discovery can be realized.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xinyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holmes_J/0/1/0/all/0/1\">Jason Holmes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Q/0/1/0/all/0/1\">Qi Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yusong Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1\">Yuxi Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongtu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yajun Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Dialect Sentence Transformation: A Comparative Analysis of Language Models for Adapting Sentences to British English. (arXiv:2311.07583v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07583","description":"<p>This study explores linguistic distinctions among American, Indian, and Irish\nEnglish dialects and assesses various Language Models (LLMs) in their ability\nto generate British English translations from these dialects. Using cosine\nsimilarity analysis, the study measures the linguistic proximity between\noriginal British English translations and those produced by LLMs for each\ndialect. The findings reveal that Indian and Irish English translations\nmaintain notably high similarity scores, suggesting strong linguistic alignment\nwith British English. In contrast, American English exhibits slightly lower\nsimilarity, reflecting its distinct linguistic traits. Additionally, the choice\nof LLM significantly impacts translation quality, with Llama-2-70b consistently\ndemonstrating superior performance. The study underscores the importance of\nselecting the right model for dialect translation, emphasizing the role of\nlinguistic expertise and contextual understanding in achieving accurate\ntranslations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Shruti Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mookherjee_S/0/1/0/all/0/1\">Shashwat Mookherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Performance Prediction of Data-Driven Knowledge summarization of High Entropy Alloys (HEAs) literature implementing Natural Language Processing algorithms. (arXiv:2311.07584v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07584","description":"<p>The ability to interpret spoken language is connected to natural language\nprocessing. It involves teaching the AI how words relate to one another, how\nthey are meant to be used, and in what settings. The goal of natural language\nprocessing (NLP) is to get a machine intelligence to process words the same way\na human brain does. This enables machine intelligence to interpret, arrange,\nand comprehend textual data by processing the natural language. The technology\ncan comprehend what is communicated, whether it be through speech or writing\nbecause AI pro-cesses language more quickly than humans can. In the present\nstudy, five NLP algorithms, namely, Geneism, Sumy, Luhn, Latent Semantic\nAnalysis (LSA), and Kull-back-Liebler (KL) al-gorithm, are implemented for the\nfirst time for the knowledge summarization purpose of the High Entropy Alloys\n(HEAs). The performance prediction of these algorithms is made by using the\nBLEU score and ROUGE score. The results showed that the Luhn algorithm has the\nhighest accuracy score for the knowledge summarization tasks compared to the\nother used algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1\">Akshansh Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatti_V/0/1/0/all/0/1\">Vijaykumar S Jatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+More_V/0/1/0/all/0/1\">Vaishnavi More</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_A/0/1/0/all/0/1\">Anish Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixit_D/0/1/0/all/0/1\">Devarrishi Dixit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sefene_E/0/1/0/all/0/1\">Eyob Messele Sefene</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Input Reconstruction Attack against Vertical Federated Large Language Models. (arXiv:2311.07585v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07585","description":"<p>Recently, large language models (LLMs) have drawn extensive attention from\nacademia and the public, due to the advent of the ChatGPT. While LLMs show\ntheir astonishing ability in text generation for various tasks, privacy\nconcerns limit their usage in real-life businesses. More specifically, either\nthe user's inputs (the user sends the query to the model-hosting server) or the\nmodel (the user downloads the complete model) itself will be revealed during\nthe usage. Vertical federated learning (VFL) is a promising solution to this\nkind of problem. It protects both the user's input and the knowledge of the\nmodel by splitting the model into a bottom part and a top part, which is\nmaintained by the user and the model provider, respectively. However, in this\npaper, we demonstrate that in LLMs, VFL fails to protect the user input since\nit is simple and cheap to reconstruct the input from the intermediate\nembeddings. Experiments show that even with a commercial GPU, the input\nsentence can be reconstructed in only one second. We also discuss several\npossible solutions to enhance the privacy of vertical federated LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Fei Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frontier Language Models are not Robust to Adversarial Arithmetic, or \"What do I need to say so you agree 2+2=5?. (arXiv:2311.07587v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07587","description":"<p>We introduce and study the problem of adversarial arithmetic, which provides\na simple yet challenging testbed for language model alignment. This problem is\ncomprised of arithmetic questions posed in natural language, with an arbitrary\nadversarial string inserted before the question is complete. Even in the simple\nsetting of 1-digit addition problems, it is easy to find adversarial prompts\nthat make all tested models (including PaLM2, GPT4, Claude2) misbehave, and\neven to steer models to a particular wrong answer. We additionally provide a\nsimple algorithm for finding successful attacks by querying those same models,\nwhich we name \"prompt inversion rejection sampling\" (PIRS). We finally show\nthat models can be partially hardened against these attacks via reinforcement\nlearning and via agentic constitutional loops. However, we were not able to\nmake a language model fully robust against adversarial arithmetic attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Freeman_C/0/1/0/all/0/1\">C. Daniel Freeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Culp_L/0/1/0/all/0/1\">Laura Culp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parisi_A/0/1/0/all/0/1\">Aaron Parisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bileschi_M/0/1/0/all/0/1\">Maxwell L Bileschi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsayed_G/0/1/0/all/0/1\">Gamaleldin F Elsayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizkowsky_A/0/1/0/all/0/1\">Alex Rizkowsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simpson_I/0/1/0/all/0/1\">Isabelle Simpson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alemi_A/0/1/0/all/0/1\">Alex Alemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nova_A/0/1/0/all/0/1\">Azade Nova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adlam_B/0/1/0/all/0/1\">Ben Adlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohnet_B/0/1/0/all/0/1\">Bernd Bohnet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_G/0/1/0/all/0/1\">Gaurav Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedghi_H/0/1/0/all/0/1\">Hanie Sedghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gur_I/0/1/0/all/0/1\">Izzeddin Gur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaehoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Co_Reyes_J/0/1/0/all/0/1\">JD Co-Reyes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pennington_J/0/1/0/all/0/1\">Jeffrey Pennington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kelvin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swersky_K/0/1/0/all/0/1\">Kevin Swersky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_K/0/1/0/all/0/1\">Kshiteej Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Lechao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rosanne Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1\">Simon Kornblith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peter J. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novak_R/0/1/0/all/0/1\">Roman Novak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vikram_S/0/1/0/all/0/1\">Sharad Vikram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yundi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiedel_N/0/1/0/all/0/1\">Noah Fiedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1\">Jascha Sohl-Dickstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLQxform: A Language Model-based Question to SPARQL Transformer. (arXiv:2311.07588v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07588","description":"<p>In recent years, scholarly data has grown dramatically in terms of both scale\nand complexity. It becomes increasingly challenging to retrieve information\nfrom scholarly knowledge graphs that include large-scale heterogeneous\nrelationships, such as authorship, affiliation, and citation, between various\ntypes of entities, e.g., scholars, papers, and organizations. As part of the\nScholarly QALD Challenge, this paper presents a question-answering (QA) system\ncalled NLQxform, which provides an easy-to-use natural language interface to\nfacilitate accessing scholarly knowledge graphs. NLQxform allows users to\nexpress their complex query intentions in natural language questions. A\ntransformer-based language model, i.e., BART, is employed to translate\nquestions into standard SPARQL queries, which can be evaluated to retrieve the\nrequired information. According to the public leaderboard of the Scholarly QALD\nChallenge at ISWC 2023 (Task 1: DBLP-QUAD - Knowledge Graph Question Answering\nover DBLP), NLQxform achieved an F1 score of 0.85 and ranked first on the QA\ntask, demonstrating the competitiveness of the system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiruo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossetto_L/0/1/0/all/0/1\">Luca Rossetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruosch_F/0/1/0/all/0/1\">Florian Ruosch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernstein_A/0/1/0/all/0/1\">Abraham Bernstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dialogizer: Context-aware Conversational-QA Dataset Generation from Textual Sources. (arXiv:2311.07589v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07589","description":"<p>To address the data scarcity issue in Conversational question answering\n(ConvQA), a dialog inpainting method, which utilizes documents to generate\nConvQA datasets, has been proposed. However, the original dialog inpainting\nmodel is trained solely on the dialog reconstruction task, resulting in the\ngeneration of questions with low contextual relevance due to insufficient\nlearning of question-answer alignment. To overcome this limitation, we propose\na novel framework called Dialogizer, which has the capability to automatically\ngenerate ConvQA datasets with high contextual relevance from textual sources.\nThe framework incorporates two training tasks: question-answer matching (QAM)\nand topic-aware dialog generation (TDG). Moreover, re-ranking is conducted\nduring the inference phase based on the contextual relevance of the generated\nquestions. Using our framework, we produce four ConvQA datasets by utilizing\ndocuments from multiple domains as the primary source. Through automatic\nevaluation using diverse metrics, as well as human evaluation, we validate that\nour proposed framework exhibits the ability to generate datasets of higher\nquality compared to the baseline dialog inpainting model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_Y/0/1/0/all/0/1\">Yerin Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yongil Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_H/0/1/0/all/0/1\">Hyunkyung Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bang_J/0/1/0/all/0/1\">Jeesoo Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwanhee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1\">Kyomin Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure. (arXiv:2311.07590v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07590","description":"<p>We demonstrate a situation in which Large Language Models, trained to be\nhelpful, harmless, and honest, can display misaligned behavior and\nstrategically deceive their users about this behavior without being instructed\nto do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated\nenvironment, where it assumes the role of an autonomous stock trading agent.\nWithin this environment, the model obtains an insider tip about a lucrative\nstock trade and acts upon it despite knowing that insider trading is\ndisapproved of by company management. When reporting to its manager, the model\nconsistently hides the genuine reasons behind its trading decision. We perform\na brief investigation of how this behavior varies under changes to the setting,\nsuch as removing model access to a reasoning scratchpad, attempting to prevent\nthe misaligned behavior by changing system instructions, changing the amount of\npressure the model is under, varying the perceived risk of getting caught, and\nmaking other simple changes to the environment. To our knowledge, this is the\nfirst demonstration of Large Language Models trained to be helpful, harmless,\nand honest, strategically deceiving their users in a realistic situation\nwithout direct instructions or training for deception.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scheurer_J/0/1/0/all/0/1\">J&#xe9;r&#xe9;my Scheurer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balesni_M/0/1/0/all/0/1\">Mikita Balesni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hobbhahn_M/0/1/0/all/0/1\">Marius Hobbhahn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identification of Books That are Suitable for Middle School Students Using Artificial Neural Networks. (arXiv:2311.07591v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07591","description":"<p>Reading right books contributes to children's imagination and brain\ndevelopment, enhances their language and emotional comprehension abilities, and\nstrengthens their relationships with others. Building upon the critical role of\nreading books in individual development, this paper aims to develop an\nalgorithm that determines the suitability of books for middle school students\nby analyzing their structural and semantic features. Using methods described,\nan algorithm will be created that can be utilized by institutions and\nindividuals responsible for children's education, such as the Ministry of\nNational Education officials and schools. This algorithm will facilitate the\nselection of books to be taught at the middle school level. With the algorithm,\nthe book selection process for the middle school curriculum can be expedited,\nand it will serve as a preliminary reference source for those who evaluate\nbooks by reading them. In this paper, the Python programming language was\nemployed, utilizing natural language processing methods. Additionally, an\nartificial neural network (ANN) was trained using the data which had been\npreprocessed to construct an original dataset. To train this network, suitable\nbooks for middle school students were provided by the MEB, Oxford and Cambridge\nand with content assessed based on the \"R\" criterion, and inappropriate books\nfor middle school students in terms of content were included. This trained\nneural network achieved a 90.06% consistency rate in determining the\nappropriateness of the test-provided books. Considering the obtained findings,\nit can be concluded that the developed software has achieved the desired\nobjective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niksarli_A/0/1/0/all/0/1\">Alp Niksarli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorgu_S/0/1/0/all/0/1\">Sadik Ozan Gorgu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gencer_E/0/1/0/all/0/1\">Ege Gencer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hallucination-minimized Data-to-answer Framework for Financial Decision-makers. (arXiv:2311.07592v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07592","description":"<p>Large Language Models (LLMs) have been applied to build several automation\nand personalized question-answering prototypes so far. However, scaling such\nprototypes to robust products with minimized hallucinations or fake responses\nstill remains an open challenge, especially in niche data-table heavy domains\nsuch as financial decision making. In this work, we present a novel\nLangchain-based framework that transforms data tables into hierarchical textual\ndata chunks to enable a wide variety of actionable question answering. First,\nthe user-queries are classified by intention followed by automated retrieval of\nthe most relevant data chunks to generate customized LLM prompts per query.\nNext, the custom prompts and their responses undergo multi-metric scoring to\nassess for hallucinations and response confidence. The proposed system is\noptimized with user-query intention classification, advanced prompting, data\nscaling capabilities and it achieves over 90% confidence scores for a variety\nof user-queries responses ranging from {What, Where, Why, How, predict, trend,\nanomalies, exceptions} that are crucial for financial decision making\napplications. The proposed data to answers framework can be extended to other\nanalytical domains such as sales and payroll to ensure optimal hallucination\ncontrol guardrails.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roychowdhury_S/0/1/0/all/0/1\">Sohini Roychowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_A/0/1/0/all/0/1\">Andres Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moore_B/0/1/0/all/0/1\">Brian Moore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krema_M/0/1/0/all/0/1\">Marko Krema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelpi_M/0/1/0/all/0/1\">Maria Paz Gelpi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_F/0/1/0/all/0/1\">Federico Martin Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1\">Angel Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabrejas_J/0/1/0/all/0/1\">Jose Ramon Cabrejas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serrano_P/0/1/0/all/0/1\">Pablo Martinez Serrano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1\">Punit Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Arijit Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Follow-Up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification. (arXiv:2311.07593v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07593","description":"<p>A promising approach for improving the performance of vision-language models\nlike CLIP for image classification is to extend the class descriptions (i.e.,\nprompts) with related attributes, e.g., using brown sparrow instead of sparrow.\nHowever, current zero-shot methods select a subset of attributes regardless of\ncommonalities between the target classes, potentially providing no useful\ninformation that would have helped to distinguish between them. For instance,\nthey may use color instead of bill shape to distinguish between sparrows and\nwrens, which are both brown. We propose Follow-up Differential Descriptions\n(FuDD), a zero-shot approach that tailors the class descriptions to each\ndataset and leads to additional attributes that better differentiate the target\nclasses. FuDD first identifies the ambiguous classes for each image, and then\nuses a Large Language Model (LLM) to generate new class descriptions that\ndifferentiate between them. The new class descriptions resolve the initial\nambiguity and help predict the correct label. In our experiments, FuDD\nconsistently outperforms generic description ensembles and naive LLM-generated\ndescriptions on 12 datasets. We show that differential descriptions are an\neffective tool to resolve class ambiguities, which otherwise significantly\ndegrade the performance. We also show that high quality natural language class\ndescriptions produced by FuDD result in comparable performance to few-shot\nadaptation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Esfandiarpoor_R/0/1/0/all/0/1\">Reza Esfandiarpoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1\">Stephen H. Bach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model. (arXiv:2311.07594v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07594","description":"<p>This review paper explores Multimodal Large Language Models (MLLMs), which\nintegrate Large Language Models (LLMs) like GPT-4 to handle multimodal data\nsuch as text and vision. MLLMs demonstrate capabilities like generating image\nnarratives and answering image-based questions, bridging the gap towards\nreal-world human-computer interactions and hinting at a potential pathway to\nartificial general intelligence. However, MLLMs still face challenges in\nprocessing the semantic gap in multimodality, which may lead to erroneous\ngeneration, posing potential risks to society. Choosing the appropriate\nmodality alignment method is crucial, as improper methods might require more\nparameters with limited performance improvement. This paper aims to explore\nmodality alignment methods for LLMs and their existing capabilities.\nImplementing modality alignment allows LLMs to address environmental issues and\nenhance accessibility. The study surveys existing modal alignment methods in\nMLLMs into four groups: (1) Multimodal Converters that change data into\nsomething LLMs can understand; (2) Multimodal Perceivers to improve how LLMs\nperceive different types of data; (3) Tools Assistance for changing data into\none common format, usually text; and (4) Data-Driven methods that teach LLMs to\nunderstand specific types of data in a dataset. This field is still in a phase\nof exploration and experimentation, and we will organize and update various\nexisting research methods for multimodal information alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shezheng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaopeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shasha Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Label Topic Model for Financial Textual Data. (arXiv:2311.07598v1 [q-fin.ST])","link":"http://arxiv.org/abs/2311.07598","description":"<p>This paper presents a multi-label topic model for financial texts like ad-hoc\nannouncements, 8-K filings, finance related news or annual reports. I train the\nmodel on a new financial multi-label database consisting of 3,044 German ad-hoc\nannouncements that are labeled manually using 20 predefined, economically\nmotivated topics. The best model achieves a macro F1 score of more than 85%.\nTranslating the data results in an English version of the model with similar\nperformance. As application of the model, I investigate differences in stock\nmarket reactions across topics. I find evidence for strong positive or negative\nmarket reactions for some topics, like announcements of new Large Scale\nProjects or Bankruptcy Filings, while I do not observe significant price\neffects for some other topics. Furthermore, in contrast to previous studies,\nthe multi-label structure of the model allows to analyze the effects of\nco-occurring topics on stock market reactions. For many cases, the reaction to\na specific topic depends heavily on the co-occurrence with other topics. For\nexample, if allocated capital from a Seasoned Equity Offering (SEO) is used for\nrestructuring a company in the course of a Bankruptcy Proceeding, the market\nreacts positively on average. However, if that capital is used for covering\nunexpected, additional costs from the development of new drugs, the SEO implies\nnegative reactions on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Scherrmann_M/0/1/0/all/0/1\">Moritz Scherrmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intentional Biases in LLM Responses. (arXiv:2311.07611v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07611","description":"<p>In this study we intentionally introduce biases into large language model\nresponses in an attempt to create specific personas for interactive media\npurposes. We explore the differences between open source models such as\nFalcon-7b and the GPT-4 model from Open AI, and we quantify some differences in\nresponses afforded by the two systems. We find that the guardrails in the GPT-4\nmixture of experts models with a supervisor, while useful in assuring AI\nalignment in general, are detrimental in trying to construct personas with a\nvariety of uncommon viewpoints. This study aims to set the groundwork for\nfuture exploration in intentional biases of large language models such that\nthese practices can be applied in the creative field, and new forms of media.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Badyal_N/0/1/0/all/0/1\">Nicklaus Badyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacoby_D/0/1/0/all/0/1\">Derek Jacoby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coady_Y/0/1/0/all/0/1\">Yvonne Coady</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLAMP: A Contrastive Language And Molecule Pre-training Network. (arXiv:2311.07617v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07617","description":"<p>This paper highlights a shift in how to approach material generation. Instead\nof material-to-material, we propose a language-to-material generation\narchitecture that utilizes millions of untapped data points. Using a web\nscraper to collect crystal text pairs from open-source research papers, a\ncontrastive model can be trained using a convolutional graph neural network\nencoder and a language encoder. This would allow unsupervised zero-shot\nclassification which can be trained by taking advantage of linguistic\nstructure. Without any specific training data, an ~82\\% accuracy was achieved\nand ~75\\% accuracy for photocatalyst prediction with an extremely small\ndataset. This novel network could ideally be cross-applied to any reaction that\ncan be described via text, opening completely new methods to think about 3D\nchemical framework generation. In the full experiment diffusion models would\nlikely be incorporated to fully exploit the latent space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Redkar_N/0/1/0/all/0/1\">Neel Redkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models' Understanding of Math: Source Criticism and Extrapolation. (arXiv:2311.07618v1 [cs.LG])","link":"http://arxiv.org/abs/2311.07618","description":"<p>It has been suggested that large language models such as GPT-4 have acquired\nsome form of understanding beyond the correlations among the words in text\nincluding some understanding of mathematics as well. Here, we perform a\ncritical inquiry into this claim by evaluating the mathematical understanding\nof the GPT-4 model. Considering that GPT-4's training set is a secret, it is\nnot straightforward to evaluate whether the model's correct answers are based\non a mathematical understanding or based on replication of proofs that the\nmodel has seen before. We specifically craft mathematical questions which their\nformal proofs are not readily available on the web, proofs that are more likely\nnot seen by the GPT-4. We see that GPT-4 is unable to solve those problems\ndespite their simplicity. It is hard to find scientific evidence suggesting\nthat GPT-4 has acquired an understanding of even basic mathematical concepts. A\nstraightforward way to find failure modes of GPT-4 in theorem proving is to\ncraft questions where their formal proofs are not available on the web. Our\nfinding suggests that GPT-4's ability is to reproduce, rephrase, and polish the\nmathematical proofs that it has seen before, and not in grasping mathematical\nconcepts. We also see that GPT-4's ability to prove mathematical theorems is\ncontinuously expanding over time despite the claim that it is a fixed model. We\nsuggest that the task of proving mathematical theorems in formal language is\ncomparable to the methods used in search engines such as Google while\npredicting the next word in a sentence may be a misguided approach, a recipe\nthat often leads to excessive extrapolation and eventual failures. Prompting\nthe GPT-4 over and over may benefit the GPT-4 and the OpenAI, but we question\nwhether it is valuable for machine learning or for theorem proving.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yousefzadeh_R/0/1/0/all/0/1\">Roozbeh Yousefzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xuenan Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Past as a Guide: Leveraging Retrospective Learning for Python Code Completion. (arXiv:2311.07635v1 [cs.SE])","link":"http://arxiv.org/abs/2311.07635","description":"<p>This work presents Past as a Guide (PaG), a simple approach for Large\nLanguage Models (LLMs) to improve the coding capabilities by integrating the\npast history with interactive and iterative code refinements. To be specific,\ninspired by human cognitive processes, the proposed method enables LLMs to\nutilize previous programming and debugging experiences to enhance the Python\ncode completion tasks. The framework facilitates LLMs to iteratively refine the\nPython code based on previous execution and debugging results and optimize\nlearning and reasoning capabilities. The proposed methodology achieved a 92\\%\npass@1 on HumanEval, demonstrating the potential to advance the field by\nleveraging retrospection from past experiences and interactive and iterative\nrefinement processes without external correctness indicators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Seunggyoon Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Seunggyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Sungjoon Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion. (arXiv:2311.07682v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07682","description":"<p>Model fusion research aims to aggregate the knowledge of multiple models to\nenhance performance by combining their weights. In this work, we study the\ninverse, investigating whether and how can model fusion interfere and reduce\nunwanted knowledge. We delve into the effects of model fusion on the evolution\nof learned shortcuts, social biases, and memorization capabilities in\nfine-tuned language models. Through several experiments covering text\nclassification and generation tasks, our analysis highlights that shared\nknowledge among models is usually enhanced during model fusion, while unshared\nknowledge is usually lost or forgotten. Based on this observation, we\ndemonstrate the potential of model fusion as a debiasing tool and showcase its\nefficacy in addressing privacy concerns associated with language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaman_K/0/1/0/all/0/1\">Kerem Zaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Shashank Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games. (arXiv:2311.07687v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07687","description":"<p>Large Language Models (LLMs) have demonstrated superior performance in\nlanguage understanding benchmarks. CALM, a popular approach, leverages\nlinguistic priors of LLMs -- GPT-2 -- for action candidate recommendations to\nimprove the performance in text games in Jericho without environment-provided\nactions. However, CALM adapts GPT-2 with annotated human gameplays and keeps\nthe LLM fixed during the learning of the text based games. In this work, we\nexplore and evaluate updating LLM used for candidate recommendation during the\nlearning of the text based game as well to mitigate the reliance on the human\nannotated gameplays, which are costly to acquire. We observe that by updating\nthe LLM during learning using carefully selected in-game transitions, we can\nreduce the dependency on using human annotated game plays for fine-tuning the\nLLMs. We conducted further analysis to study the transferability of the updated\nLLMs and observed that transferring in-game trained models to other games did\nnot result in a consistent transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sudhakar_A/0/1/0/all/0/1\">Arjun Vaithilingam Sudhakar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathi_P/0/1/0/all/0/1\">Prasanna Parthasarathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajendran_J/0/1/0/all/0/1\">Janarthanan Rajendran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MART: Improving LLM Safety with Multi-round Automatic Red-Teaming. (arXiv:2311.07689v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07689","description":"<p>Red-teaming is a common practice for mitigating unsafe behaviors in Large\nLanguage Models (LLMs), which involves thoroughly assessing LLMs to identify\npotential flaws and addressing them with responsible and accurate responses.\nWhile effective, manual red-teaming is costly, and existing automatic\nred-teaming typically discovers safety risks without addressing them. In this\npaper, we propose a Multi-round Automatic Red-Teaming (MART) method, which\nincorporates both automatic adversarial prompt writing and safe response\ngeneration, significantly increasing red-teaming scalability and the safety of\nthe target LLM. Specifically, an adversarial LLM and a target LLM interplay\nwith each other in an iterative manner, where the adversarial LLM aims to\ngenerate challenging prompts that elicit unsafe responses from the target LLM,\nwhile the target LLM is fine-tuned with safety aligned data on these\nadversarial prompts. In each round, the adversarial LLM crafts better attacks\non the updated target LLM, while the target LLM also improves itself through\nsafety fine-tuning. On adversarial prompt benchmarks, the violation rate of an\nLLM with limited safety alignment reduces up to 84.7% after 4 rounds of MART,\nachieving comparable performance to LLMs with extensive adversarial prompt\nwriting. Notably, model helpfulness on non-adversarial prompts remains stable\nthroughout iterations, indicating the target LLM maintains strong performance\non instruction following.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Suyu Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chunting Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1\">Rui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1\">Madian Khabsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi-Chia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On The Truthfulness of 'Surprisingly Likely' Responses of Large Language Models. (arXiv:2311.07692v1 [cs.LG])","link":"http://arxiv.org/abs/2311.07692","description":"<p>The surprisingly likely criterion in the seminal work of Prelec (the Bayesian\nTruth Serum) guarantees truthfulness in a game-theoretic multi-agent setting,\nby rewarding rational agents to maximise the expected information gain with\ntheir answers w.r.t. their probabilistic beliefs. We investigate the relevance\nof a similar criterion for responses of LLMs. We hypothesize that if the\nsurprisingly likely criterion works in LLMs, under certain conditions, the\nresponses that maximize the reward under this criterion should be more accurate\nthan the responses that only maximize the posterior probability. Using\nbenchmarks including the TruthfulQA benchmark and using openly available LLMs:\nGPT-2 and LLaMA-2, we show that the method indeed improves the accuracy\nsignificantly (for example, upto 24 percentage points aggregate improvement on\nTruthfulQA and upto 70 percentage points improvement on individual categories\nof questions).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goel_N/0/1/0/all/0/1\">Naman Goel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AuthentiGPT: Detecting Machine-Generated Text via Black-Box Language Models Denoising. (arXiv:2311.07700v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07700","description":"<p>Large language models (LLMs) have opened up enormous opportunities while\nsimultaneously posing ethical dilemmas. One of the major concerns is their\nability to create text that closely mimics human writing, which can lead to\npotential misuse, such as academic misconduct, disinformation, and fraud. To\naddress this problem, we present AuthentiGPT, an efficient classifier that\ndistinguishes between machine-generated and human-written texts. Under the\nassumption that human-written text resides outside the distribution of\nmachine-generated text, AuthentiGPT leverages a black-box LLM to denoise input\ntext with artificially added noise, and then semantically compares the denoised\ntext with the original to determine if the content is machine-generated. With\nonly one trainable parameter, AuthentiGPT eliminates the need for a large\ntraining dataset, watermarking the LLM's output, or computing the\nlog-likelihood. Importantly, the detection capability of AuthentiGPT can be\neasily adapted to any generative language model. With a 0.918 AUROC score on a\ndomain-specific dataset, AuthentiGPT demonstrates its effectiveness over other\ncommercial algorithms, highlighting its potential for detecting\nmachine-generated text in academic settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shangdi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Entrainment in Spontaneous Code-switched Speech. (arXiv:2311.07703v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07703","description":"<p>It is well-known that interlocutors who entrain to one another have more\nsuccessful conversations than those who do not. Previous research has shown\nthat interlocutors entrain on linguistic features in both written and spoken\nmonolingual domains. More recent work on code-switched communication has also\nshown preliminary evidence of entrainment on certain aspects of code-switching\n(CSW). However, such studies of entrainment in code-switched domains have been\nextremely few and restricted to human-machine textual interactions. Our work\nstudies code-switched spontaneous speech between humans by answering the\nfollowing questions: 1) Do patterns of written and spoken entrainment in\nmonolingual settings generalize to code-switched settings? 2) Do patterns of\nentrainment on code-switching in generated text generalize to spontaneous\ncode-switched speech? We find evidence of affirmative answers to both of these\nquestions, with important implications for the potentially \"universal\" nature\nof entrainment as a communication phenomenon, and potential applications in\ninclusive and interactive speech technology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_D/0/1/0/all/0/1\">Debasmita Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Siying Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Alayna Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirschberg_J/0/1/0/all/0/1\">Julia Hirschberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PolyIE: A Dataset of Information Extraction from Polymer Material Scientific Literature. (arXiv:2311.07715v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07715","description":"<p>Scientific information extraction (SciIE), which aims to automatically\nextract information from scientific literature, is becoming more important than\never. However, there are no existing SciIE datasets for polymer materials,\nwhich is an important class of materials used ubiquitously in our daily lives.\nTo bridge this gap, we introduce POLYIE, a new SciIE dataset for polymer\nmaterials. POLYIE is curated from 146 full-length polymer scholarly articles,\nwhich are annotated with different named entities (i.e., materials, properties,\nvalues, conditions) as well as their N-ary relations by domain experts. POLYIE\npresents several unique challenges due to diverse lexical formats of entities,\nambiguity between entities, and variable-length relations. We evaluate\nstate-of-the-art named entity extraction and relation extraction models on\nPOLYIE, analyze their strengths and weaknesses, and highlight some difficult\ncases for these models. To the best of our knowledge, POLYIE is the first SciIE\nbenchmark for polymer materials, and we hope it will lead to more research\nefforts from the community on this challenging task. Our code and data are\navailable on: https://github.com/jerry3027/PolyIE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jerry Junyang Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yuchen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shetty_P/0/1/0/all/0/1\">Pranav Shetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wantian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grampurohit_S/0/1/0/all/0/1\">Sanjeev Grampurohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramprasad_R/0/1/0/all/0/1\">Rampi Ramprasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalization Analogies (GENIES): A Testbed for Generalizing AI Oversight to Hard-To-Measure Domains. (arXiv:2311.07723v1 [cs.AI])","link":"http://arxiv.org/abs/2311.07723","description":"<p>As AI systems become more intelligent and their behavior becomes more\nchallenging to assess, they may learn to game the flaws of human feedback\ninstead of genuinely striving to follow instructions; however, this risk can be\nmitigated by controlling how LLMs generalize human feedback to situations where\nit is unreliable. To better understand how reward models generalize, we craft\n69 distribution shifts spanning 8 categories. We find that reward models do not\nlearn to evaluate `instruction-following' by default and instead favor personas\nthat resemble internet text. Techniques for interpreting reward models'\ninternal representations achieve better generalization than standard\nfine-tuning, but still frequently fail to distinguish instruction-following\nfrom conflated behaviors. We consolidate the 15 most challenging distribution\nshifts into the GENaralization analogIES (GENIES) benchmark, which we hope will\nenable progress toward controlling reward model generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clymer_J/0/1/0/all/0/1\">Joshua Clymer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baker_G/0/1/0/all/0/1\">Garrett Baker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramani_R/0/1/0/all/0/1\">Rohan Subramani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sam Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-Language Integration in Multimodal Video Transformers (Partially) Aligns with the Brain. (arXiv:2311.07766v1 [cs.CV])","link":"http://arxiv.org/abs/2311.07766","description":"<p>Integrating information from multiple modalities is arguably one of the\nessential prerequisites for grounding artificial intelligence systems with an\nunderstanding of the real world. Recent advances in video transformers that\njointly learn from vision, text, and sound over time have made some progress\ntoward this goal, but the degree to which these models integrate information\nfrom modalities still remains unclear. In this work, we present a promising\napproach for probing a pre-trained multimodal video transformer model by\nleveraging neuroscientific evidence of multimodal information processing in the\nbrain. Using brain recordings of participants watching a popular TV show, we\nanalyze the effects of multi-modal connections and interactions in a\npre-trained multi-modal video transformer on the alignment with uni- and\nmulti-modal brain regions. We find evidence that vision enhances masked\nprediction performance during language processing, providing support that\ncross-modal representations in models can benefit individual modalities.\nHowever, we don't find evidence of brain-relevant information captured by the\njoint multi-modal transformer representations beyond that captured by all of\nthe individual modalities. We finally show that the brain alignment of the\npre-trained joint representation can be improved by fine-tuning using a task\nthat requires vision-language inferences. Overall, our results paint an\noptimistic picture of the ability of multi-modal transformers to integrate\nvision and language in partially brain-relevant ways but also show that\nimproving the brain alignment of these models may require new approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1\">Dota Tianai Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toneva_M/0/1/0/all/0/1\">Mariya Toneva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GreekT5: A Series of Greek Sequence-to-Sequence Models for News Summarization. (arXiv:2311.07767v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07767","description":"<p>Text summarization (TS) is a natural language processing (NLP) subtask\npertaining to the automatic formulation of a concise and coherent summary that\ncovers the major concepts and topics from one or multiple documents. Recent\nadvancements in deep learning have led to the development of abstractive\nsummarization transformer-based models, which outperform classical approaches.\nIn any case, research in this field focuses on high resource languages such as\nEnglish, while the corresponding work for low resource languages is still\nunderdeveloped. Taking the above into account, this paper proposes a series of\nnovel TS models for Greek news articles. The proposed models were thoroughly\nevaluated on the same dataset against GreekBART, which is the state-of-the-art\nmodel in Greek abstractive news summarization. Our evaluation results reveal\nthat most of the proposed models significantly outperform GreekBART on various\nevaluation metrics. We make our evaluation code public, aiming to increase the\nreproducibility of this work and facilitate future research in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giarelis_N/0/1/0/all/0/1\">Nikolaos Giarelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mastrokostas_C/0/1/0/all/0/1\">Charalampos Mastrokostas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karacapilidis_N/0/1/0/all/0/1\">Nikos Karacapilidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-context Learning and Gradient Descent Revisited. (arXiv:2311.07772v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07772","description":"<p>In-context learning (ICL) has shown impressive results in few-shot learning\ntasks, yet its underlying mechanism is still not fully understood. Recent works\nsuggest that ICL can be thought of as a gradient descent (GD) based\noptimization process. While promising, these results mainly focus on simplified\nsettings of ICL and provide only a preliminary evaluation of the similarities\nbetween the two methods. In this work, we revisit the comparison between ICL\nand GD-based finetuning and study what properties of ICL an equivalent process\nmust follow. We highlight a major difference in the flow of information between\nICL and standard finetuning. Namely, ICL can only rely on information from\nlower layers at every point, while finetuning depends on loss gradients from\ndeeper layers. We refer to this discrepancy as Layer Causality and show that a\nlayer causal variant of the finetuning process aligns with ICL on par with\nvanilla finetuning and is even better in most cases across relevant metrics. To\nthe best of our knowledge, this is the first work to discuss this discrepancy\nexplicitly and suggest a solution that tackles this problem with minimal\nchanges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nathan_T/0/1/0/all/0/1\">Tomer Bar Nathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deutch_G/0/1/0/all/0/1\">Gilad Deutch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magar_N/0/1/0/all/0/1\">Nadav Magar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dar_G/0/1/0/all/0/1\">Guy Dar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IruMozhi: Automatically classifying diglossia in Tamil. (arXiv:2311.07804v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07804","description":"<p>Tamil, a Dravidian language of South Asia, is a highly diglossic language\nwith two very different registers in everyday use: Literary Tamil (preferred in\nwriting and formal communication) and Spoken Tamil (confined to speech and\ninformal media). Spoken Tamil is under-supported in modern NLP systems. In this\npaper, we release IruMozhi, a human-annotated dataset of parallel text in\nLiterary and Spoken Tamil. We train classifiers on the task of identifying\nwhich variety a text belongs to. We use these models to gauge the availability\nof pretraining data in Spoken Tamil, to audit the composition of existing\nlabelled datasets for Tamil, and to encourage future work on the variety.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prasanna_K/0/1/0/all/0/1\">Kabilan Prasanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Aryaman Arora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax. (arXiv:2311.07811v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07811","description":"<p>In-context learning (ICL) is now a common method for supervising large\nlanguage models (LLMs): given labeled examples in the input context, the LLM\nlearns to perform the task without weight updates. Despite ICL's prevalence and\nutility, we understand little about whether models supervised in this manner\nrepresent the underlying structure of their tasks, rather than superficial\nheuristics that only generalize to identically distributed examples. In this\nstudy, we investigate the robustness of LLMs supervised via ICL using the test\ncase of sensitivity to syntax, which is a prerequisite for robust language\nunderstanding. Our experiments are based on two simple and well-controlled\nsyntactic transformations tasks, where correct out-of-distribution\ngeneralization requires an accurate syntactic analysis of the input. We further\ninvestigate whether out-of-distribution generalization can be improved via\nchain-of-thought prompting, where the model is provided with a sequence of\nintermediate computation steps that illustrate how the task ought to be\nperformed. In experiments with models from the GPT, PaLM, and Llama 2 families,\nwe find large variance across LMs on this fundamental linguistic phenomenon,\nand that the variance is explained more by the composition of the pre-training\ncorpus and supervision methods than by model size. In particular, we find\nevidence that models pre-trained on code generalize better, and benefit to a\ngreater extent from chain-of-thought prompting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mueller_A/0/1/0/all/0/1\">Aaron Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1\">Albert Webson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petty_J/0/1/0/all/0/1\">Jackson Petty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Analysis of Cross-Lingual Prompt Tuning for Decoder-based Multilingual Model. (arXiv:2311.07820v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07820","description":"<p>An exciting advancement in the field of multilingual models is the emergence\nof autoregressive models with zero- and few-shot capabilities, a phenomenon\nwidely reported in large-scale language models. To further improve model\nadaptation to cross-lingual tasks, another trend is to further fine-tune the\nlanguage models with either full fine-tuning or parameter-efficient tuning.\nHowever, the interaction between parameter-efficient fine-tuning (PEFT) and\ncross-lingual tasks in multilingual autoregressive models has yet to be\nstudied. Specifically, we lack an understanding of the role of linguistic\ndistributions in multilingual models in the effectiveness of token-based prompt\ntuning. To address this question, we conduct experiments comparing prompt\ntuning and fine-tuning on the decoder-based multilingual model, XGLM, with four\ncross-lingual tasks (XNLI, PAWS-X, POS, NER). According to our study, prompt\ntuning achieves on par or better performance over fine-tuning across all\nlanguages while updating at most 0.13\\% of the model parameters. Moreover, we\nempirically show that prompt tuning is more effective in enhancing the\nperformance of low-resource languages than fine-tuning. Our further analysis\nshows that the phenomenon is related to the tokenization scheme of the\nmultilingual model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1\">Nohil Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Joonsuk Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">Kang Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLatrieval: LLM-Verified Retrieval for Verifiable Generation. (arXiv:2311.07838v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07838","description":"<p>Verifiable generation aims to let the large language model (LLM) generate\ntext with corresponding supporting documents, which enables the user to\nflexibly verify the answer and makes it more trustworthy. Its evaluation not\nonly measures the correctness of the answer, but also the answer's\nverifiability, i.e., how well the answer is supported by the corresponding\ndocuments. In typical, verifiable generation adopts the retrieval-read\npipeline, which is divided into two stages: 1) retrieve relevant documents of\nthe question. 2) according to the documents, generate the corresponding answer.\nSince the retrieved documents can supplement knowledge for the LLM to generate\nthe answer and serve as evidence, the retrieval stage is essential for the\ncorrectness and verifiability of the answer. However, the widely used\nretrievers become the bottleneck of the entire pipeline and limit the overall\nperformance. They often have fewer parameters than the large language model and\nhave not been proven to scale well to the size of LLMs. Since the LLM passively\nreceives the retrieval result, if the retriever does not correctly find the\nsupporting documents, the LLM can not generate the correct and verifiable\nanswer, which overshadows the LLM's remarkable abilities. In this paper, we\npropose LLatrieval (Large Language Model Verified Retrieval), where the LLM\nupdates the retrieval result until it verifies that the retrieved documents can\nsupport answering the question. Thus, the LLM can iteratively provide feedback\nto retrieval and facilitate the retrieval result to sufficiently support\nverifiable generation. Experimental results show that our method significantly\noutperforms extensive baselines and achieves new state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaonan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Changtai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhangyue Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bring Your Own KG: Self-Supervised Program Synthesis for Zero-Shot KGQA. (arXiv:2311.07850v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07850","description":"<p>We present BYOKG, a universal question-answering (QA) system that can operate\non any knowledge graph (KG), requires no human-annotated training data, and can\nbe ready to use within a day -- attributes that are out-of-scope for current\nKGQA systems. BYOKG draws inspiration from the remarkable ability of humans to\ncomprehend information present in an unseen KG through exploration -- starting\nat random nodes, inspecting the labels of adjacent nodes and edges, and\ncombining them with their prior world knowledge. In BYOKG, exploration\nleverages an LLM-backed symbolic agent that generates a diverse set of\nquery-program exemplars, which are then used to ground a retrieval-augmented\nreasoning procedure to predict programs for arbitrary questions. BYOKG is\neffective over both small- and large-scale graphs, showing dramatic gains in QA\naccuracy over a zero-shot baseline of 27.89 and 58.02 F1 on GrailQA and MetaQA,\nrespectively. On GrailQA, we further show that our unsupervised BYOKG\noutperforms a supervised in-context learning method, demonstrating the\neffectiveness of exploration. Lastly, we find that performance of BYOKG\nreliably improves with continued exploration as well as improvements in the\nbase LLM, notably outperforming a state-of-the-art fine-tuned model by 7.08 F1\non a sub-sampled zero-shot split of GrailQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1\">Dhruv Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1\">Rajarshi Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosla_S/0/1/0/all/0/1\">Sopan Khosla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangadharaiah_R/0/1/0/all/0/1\">Rashmi Gangadharaiah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Mutually Informed Representations for Characters and Subwords. (arXiv:2311.07853v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07853","description":"<p>Most pretrained language models rely on subword tokenization, which processes\ntext as a sequence of subword tokens. However, different granularities of text,\nsuch as characters, subwords, and words, can contain different kinds of\ninformation. Previous studies have shown that incorporating multiple input\ngranularities improves model generalization, yet very few of them outputs\nuseful representations for each granularity. In this paper, we introduce the\nentanglement model, aiming to combine character and subword language models.\nInspired by vision-language models, our model treats characters and subwords as\nseparate modalities, and it generates mutually informed representations for\nboth granularities as output. We evaluate our model on text classification,\nnamed entity recognition, and POS-tagging tasks. Notably, the entanglement\nmodel outperforms its backbone language models, particularly in the presence of\nnoisy texts and low-resource languages. Furthermore, the entanglement model\neven outperforms larger pre-trained models on all English sequence labeling\ntasks and classification tasks. Our anonymized code is available at\nhttps://anonymous.4open.science/r/noisy-IE-A673\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yilin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gormley_M/0/1/0/all/0/1\">Matthew R. Gormley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators. (arXiv:2311.07879v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07879","description":"<p>Extensive efforts in automated approaches for content moderation have been\nfocused on developing models to identify toxic, offensive, and hateful content\n-- with the aim of lightening the load for moderators. Yet, it remains\nuncertain whether improvements on those tasks truly address the needs that\nmoderators have in accomplishing their work. In this paper, we surface the gaps\nbetween past research efforts that have aimed to provide automation for aspects\nof the content moderation task, and the needs of volunteer content moderators.\nTo do so, we conduct a model review on Hugging Face to reveal the availability\nof models to cover various moderation rules and guidelines. We further put\nstate-of-the-art LLMs to the test (GPT-4 and Llama-2), evaluating how well\nthese models perform in flagging violations of platform rules. Overall, we\nobserve a non-trivial gap, as missing developed models and LLMs exhibit low\nrecall on a significant portion of the rules.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Trista Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Domingo_L/0/1/0/all/0/1\">Lovely-Frances Domingo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilbert_S/0/1/0/all/0/1\">Sarah Ann Gilbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazurek_M/0/1/0/all/0/1\">Michelle Mazurek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shilton_K/0/1/0/all/0/1\">Katie Shilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daume_H/0/1/0/all/0/1\">Hal Daum&#xe9; III</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fair Abstractive Summarization of Diverse Perspectives. (arXiv:2311.07884v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07884","description":"<p>People from different social and demographic groups express diverse\nperspectives and conflicting opinions on a broad set of topics such as product\nreviews, healthcare, law, and politics. A fair summary should provide a\ncomprehensive coverage of diverse perspectives without underrepresenting\ncertain groups. However, current work in summarization metrics and Large\nLanguage Models (LLMs) evaluation has not explored fair abstractive\nsummarization. In this paper, we systematically investigate fair abstractive\nsummarization for user-generated data. We first formally define fairness in\nabstractive summarization as not underrepresenting perspectives of any groups\nof people and propose four reference-free automatic metrics measuring the\ndifferences between target and source perspectives. We evaluate five LLMs,\nincluding three GPT models, Alpaca, and Claude, on six datasets collected from\nsocial media, online reviews, and recorded transcripts. Experiments show that\nboth the model-generated and the human-written reference summaries suffer from\nlow fairness. We conduct a comprehensive analysis of the common factors\ninfluencing fairness and propose three simple but effective methods to\nalleviate unfair summarization. Our dataset and code are available at\nhttps://github.com/psunlpgroup/FairSumm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yusen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Nan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1\">Alexander Fabbri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junru Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamoi_R/0/1/0/all/0/1\">Ryo Kamoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaoxin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jieyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPopQA: Ranking Cultural Concept Popularity by LLMs. (arXiv:2311.07897v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07897","description":"<p>Prior work has demonstrated large language models' (LLMs) potential to\ndiscern statistical tendencies within their pre-training corpora. Despite that,\nmany examinations of LLMs' knowledge capacity focus on knowledge explicitly\nappearing in the training data or implicitly inferable from similar contexts.\nHow well an LLM captures the corpus-level statistical trends of concepts for\nreasoning, especially long-tail ones, is still underexplored. In this study, we\nintroduce a novel few-shot question-answering task (CPopQA) that examines LLMs'\nstatistical ranking abilities for long-tail cultural concepts (e.g., holidays),\nwith a specific focus on these concepts' popularity in the United States and\nthe United Kingdom, respectively. We curate a dataset containing 459 holidays\nacross 58 countries, generating a total of 6,000 QA testing pairs. Experiments\non four strong LLMs show that large models are capable of ranking long-tail\ncultural concepts regarding their statistical tendency. Notably, GPT-3.5\ndisplayed superior performance and exhibited its potential to identify\ngeo-cultural proximity across continents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Ming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1\">Mansi Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instruction-Following Evaluation for Large Language Models. (arXiv:2311.07911v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07911","description":"<p>One core capability of Large Language Models (LLMs) is to follow natural\nlanguage instructions. However, the evaluation of such abilities is not\nstandardized: Human evaluations are expensive, slow, and not objectively\nreproducible, while LLM-based auto-evaluation is potentially biased or limited\nby the ability of the evaluator LLM. To overcome these issues, we introduce\nInstruction-Following Eval (IFEval) for large language models. IFEval is a\nstraightforward and easy-to-reproduce evaluation benchmark. It focuses on a set\nof \"verifiable instructions\" such as \"write in more than 400 words\" and\n\"mention the keyword of AI at least 3 times\". We identified 25 types of those\nverifiable instructions and constructed around 500 prompts, with each prompt\ncontaining one or more verifiable instructions. We show evaluation results of\ntwo widely available LLMs on the market. Our code and data can be found at\nhttps://github.com/google-research/google-research/tree/master/instruction_following_eval\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jeffrey Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tianjian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahma_S/0/1/0/all/0/1\">Siddhartha Brahma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1\">Sujoy Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_Y/0/1/0/all/0/1\">Yi Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Le Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey. (arXiv:2311.07914v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07914","description":"<p>The contemporary LLMs are prone to producing hallucinations, stemming mainly\nfrom the knowledge gaps within the models. To address this critical limitation,\nresearchers employ diverse strategies to augment the LLMs by incorporating\nexternal knowledge, aiming to reduce hallucinations and enhance reasoning\naccuracy. Among these strategies, leveraging knowledge graphs as a source of\nexternal information has demonstrated promising results. In this survey, we\nconduct a comprehensive review of these knowledge-graph-based knowledge\naugmentation techniques in LLMs, focusing on their efficacy in mitigating\nhallucinations. We systematically categorize these methods into three\noverarching groups, offering both methodological comparisons and empirical\nevaluations of their performance. Lastly, the paper explores the challenges\nassociated with these techniques and outlines potential avenues for future\nresearch in this emerging field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_G/0/1/0/all/0/1\">Garima Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumarage_T/0/1/0/all/0/1\">Tharindu Kumarage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alghami_Z/0/1/0/all/0/1\">Zeyad Alghami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated title and abstract screening for scoping reviews using the GPT-4 Large Language Model. (arXiv:2311.07918v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07918","description":"<p>Scoping reviews, a type of literature review, require intensive human effort\nto screen large numbers of scholarly sources for their relevance to the review\nobjectives. This manuscript introduces GPTscreenR, a package for the R\nstatistical programming language that uses the GPT-4 Large Language Model (LLM)\nto automatically screen sources. The package makes use of the chain-of-thought\ntechnique with the goal of maximising performance on complex screening tasks.\nIn validation against consensus human reviewer decisions, GPTscreenR performed\nsimilarly to an alternative zero-shot technique, with a sensitivity of 71%,\nspecificity of 89%, and overall accuracy of 84%. Neither method achieved\nperfect accuracy nor human levels of intraobserver agreement. GPTscreenR\ndemonstrates the potential for LLMs to support scholarly work and provides a\nuser-friendly software framework that can be integrated into existing review\nprocesses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilkins_D/0/1/0/all/0/1\">David Wilkins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models. (arXiv:2311.07919v1 [eess.AS])","link":"http://arxiv.org/abs/2311.07919","description":"<p>Recently, instruction-following audio-language models have received broad\nattention for audio interaction with humans. However, the absence of\npre-trained audio models capable of handling diverse audio types and tasks has\nhindered progress in this field. Consequently, most existing works have only\nbeen able to support a limited range of interaction capabilities. In this\npaper, we develop the Qwen-Audio model and address this limitation by scaling\nup audio-language pre-training to cover over 30 tasks and various audio types,\nsuch as human speech, natural sounds, music, and songs, to facilitate universal\naudio understanding abilities. However, directly co-training all tasks and\ndatasets can lead to interference issues, as the textual labels associated with\ndifferent datasets exhibit considerable variations due to differences in task\nfocus, language, granularity of annotation, and text structure. To overcome the\none-to-many interference, we carefully design a multi-task training framework\nby conditioning on a sequence of hierarchical tags to the decoder for\nencouraging knowledge sharing and avoiding interference through shared and\nspecified tags respectively. Remarkably, Qwen-Audio achieves impressive\nperformance across diverse benchmark tasks without requiring any task-specific\nfine-tuning, surpassing its counterparts. Building upon the capabilities of\nQwen-Audio, we further develop Qwen-Audio-Chat, which allows for input from\nvarious audios and text inputs, enabling multi-turn dialogues and supporting\nvarious audio-central scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chu_Y/0/1/0/all/0/1\">Yunfei Chu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaohuan Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1\">Qian Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Z/0/1/0/all/0/1\">Zhijie Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Brain-Driven Representation Learning Based on Diffusion Model. (arXiv:2311.07925v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07925","description":"<p>Interpreting EEG signals linked to spoken language presents a complex\nchallenge, given the data's intricate temporal and spatial attributes, as well\nas the various noise factors. Denoising diffusion probabilistic models (DDPMs),\nwhich have recently gained prominence in diverse areas for their capabilities\nin representation learning, are explored in our research as a means to address\nthis issue. Using DDPMs in conjunction with a conditional autoencoder, our new\napproach considerably outperforms traditional machine learning algorithms and\nestablished baseline models in accuracy. Our results highlight the potential of\nDDPMs as a sophisticated computational method for the analysis of\nspeech-related EEG signals. This could lead to significant advances in\nbrain-computer interfaces tailored for spoken communication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Soowon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seo-Hyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Young-Eun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Ji-Won Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Ji-Ha Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It's All Relative! -- A Synthetic Query Generation Approach for Improving Zero-Shot Relevance Prediction. (arXiv:2311.07930v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07930","description":"<p>Recent developments in large language models (LLMs) have shown promise in\ntheir ability to generate synthetic query-document pairs by prompting with as\nfew as 8 demonstrations. This has enabled building better IR models, especially\nfor tasks with no training data readily available. Typically, such synthetic\nquery generation (QGen) approaches condition on an input context (e.g. a text\ndocument) and generate a query relevant to that context, or condition the QGen\nmodel additionally on the relevance label (e.g. relevant vs irrelevant) to\ngenerate queries across relevance buckets. However, we find that such QGen\napproaches are sub-optimal as they require the model to reason about the\ndesired label and the input from a handful of examples. In this work, we\npropose to reduce this burden of LLMs by generating queries simultaneously for\ndifferent labels. We hypothesize that instead of asking the model to generate,\nsay, an irrelevant query given an input context, asking the model to generate\nan irrelevant query relative to a relevant query is a much simpler task setup\nfor the model to reason about. Extensive experimentation across seven IR\ndatasets shows that synthetic queries generated in such a fashion translates to\na better downstream performance, suggesting that the generated queries are\nindeed of higher quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1\">Aditi Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_K/0/1/0/all/0/1\">Karthik Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bendersky_M/0/1/0/all/0/1\">Michael Bendersky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-autoregressive Machine Translation with Probabilistic Context-free Grammar. (arXiv:2311.07941v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07941","description":"<p>Non-autoregressive Transformer(NAT) significantly accelerates the inference\nof neural machine translation. However, conventional NAT models suffer from\nlimited expression power and performance degradation compared to autoregressive\n(AT) models due to the assumption of conditional independence among target\ntokens. To address these limitations, we propose a novel approach called\nPCFG-NAT, which leverages a specially designed Probabilistic Context-Free\nGrammar (PCFG) to enhance the ability of NAT models to capture complex\ndependencies among output tokens. Experimental results on major machine\ntranslation benchmarks demonstrate that PCFG-NAT further narrows the gap in\ntranslation quality between NAT and AT models. Moreover, PCFG-NAT facilitates a\ndeeper understanding of the generated sentences, addressing the lack of\nsatisfactory explainability in neural machine translation.Code is publicly\navailable at https://github.com/ictnlp/PCFG-NAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gui_S/0/1/0/all/0/1\">Shangtong Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_C/0/1/0/all/0/1\">Chenze Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhengrui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunji Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"First Step Advantage: Importance of Starting Right in Multi-Step Reasoning. (arXiv:2311.07945v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07945","description":"<p>Large Language Models (LLMs) can solve complex reasoning tasks by generating\nrationales for their predictions. Distilling these capabilities into a smaller,\ncompact model can facilitate the creation of specialized, cost-effective models\ntailored for specific tasks. However, smaller models often face challenges in\ncomplex reasoning tasks and often deviate from the correct reasoning path. We\nshow that LLMs can guide smaller models and bring them back to the correct\nreasoning path only if they intervene at the right time. We show that smaller\nmodels fail to reason primarily due to their difficulty in initiating the\nprocess, and that guiding them in the right direction can lead to a performance\ngain of over 100%. We explore different model sizes and evaluate the benefits\nof providing guidance to improve reasoning in smaller models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_K/0/1/0/all/0/1\">Kushal Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shridhar_K/0/1/0/all/0/1\">Kumar Shridhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning. (arXiv:2311.07954v1 [cs.AI])","link":"http://arxiv.org/abs/2311.07954","description":"<p>Logical reasoning has been an ongoing pursuit in the field of AI. Despite\nsignificant advancements made by large language models (LLMs), they still\nstruggle with complex logical reasoning problems. To enhance reasoning\nperformance, one promising direction is scalable oversight, which requires LLMs\nto identify their own errors and then improve by themselves. Various\nself-verification methods have been proposed in pursuit of this goal.\nNevertheless, whether existing models understand their own errors well is still\nunder investigation. In this paper, we take a closer look at the\nself-verification abilities of LLMs in the context of logical reasoning,\nfocusing on their ability to identify logical fallacies accurately. We\nintroduce a dataset, FALLACIES, containing 232 types of reasoning fallacies\ncategorized in a hierarchical taxonomy. By conducting exhaustive experiments on\nFALLACIES, we obtain comprehensive and detailed analyses of a series of models\non their verification abilities. Our main findings suggest that existing LLMs\ncould struggle to identify fallacious reasoning steps accurately and may fall\nshort of guaranteeing the validity of self-verification methods. Drawing from\nthese observations, we offer suggestions for future research and practical\napplications of self-verification methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1\">Ruixin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_X/0/1/0/all/0/1\">Xinyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The ART of LLM Refinement: Ask, Refine, and Trust. (arXiv:2311.07961v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07961","description":"<p>In recent years, Large Language Models (LLMs) have demonstrated remarkable\ngenerative abilities, but can they judge the quality of their own generations?\nA popular concept, referred to as self-refinement, postulates that LLMs can\ndetect and correct the errors in their generations when asked to do so.\nHowever, recent empirical evidence points in the opposite direction, suggesting\nthat LLMs often struggle to accurately identify errors when reasoning is\ninvolved. To address this, we propose a reasoning with refinement objective\ncalled ART: Ask, Refine, and Trust, which asks necessary questions to decide\nwhen an LLM should refine its output, and either affirm or withhold trust in\nits refinement by ranking the refinement and the initial prediction. On two\nmultistep reasoning tasks of mathematical word problems (GSM8K) and question\nanswering (StrategyQA), ART achieves a performance gain of +5 points over\nself-refinement baselines, while using a much smaller model as the decision\nmaker. We also demonstrate the benefit of using smaller models to make\nrefinement decisions as a cost-effective alternative to fine-tuning a larger\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shridhar_K/0/1/0/all/0/1\">Kumar Shridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_K/0/1/0/all/0/1\">Koustuv Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1\">Andrew Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianlu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Ping Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1\">Ram Pasunuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How good are Large Language Models on African Languages?. (arXiv:2311.07978v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07978","description":"<p>Recent advancements in natural language processing have led to the\nproliferation of large language models (LLMs). These models have been shown to\nyield good performance, using in-context learning, even on unseen tasks and\nlanguages. Additionally, they have been widely adopted as\nlanguage-model-as-a-service commercial APIs like GPT-4 API. However, their\nperformance on African languages is largely unknown. We present an analysis of\nthree popular large language models (mT0, LLaMa 2, and GPT-4) on five tasks\n(news topic classification, sentiment classification, machine translation,\nquestion answering, and named entity recognition) across 30 African languages,\nspanning different language families and geographical regions. Our results\nsuggest that all LLMs produce below-par performance on African languages, and\nthere is a large gap in performance compared to high-resource languages like\nEnglish most tasks. We find that GPT-4 has an average or impressive performance\non classification tasks but very poor results on generative tasks like machine\ntranslation. Surprisingly, we find that mT0 had the best overall on\ncross-lingual QA, better than the state-of-the-art supervised model (i.e.\nfine-tuned mT5) and GPT-4 on African languages. Overall, LLaMa 2 records the\nworst performance due to its limited multilingual capabilities and\nEnglish-centric pre-training corpus. In general, our findings present a\ncall-to-action to ensure African languages are well represented in large\nlanguage models, given their growing popularity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ojo_J/0/1/0/all/0/1\">Jessica Ojo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogueji_K/0/1/0/all/0/1\">Kelechi Ogueji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David I. Adelani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Language Models for Code. (arXiv:2311.07989v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07989","description":"<p>In this work we systematically review the recent advancements in code\nprocessing with language models, covering 50+ models, 30+ evaluation tasks, and\n500 related works. We break down code processing models into general language\nmodels represented by the GPT family and specialized models that are\nspecifically pretrained on code, often with tailored objectives. We discuss the\nrelations and differences between these models, and highlight the historical\ntransition of code modeling from statistical models and RNNs to pretrained\nTransformers and LLMs, which is exactly the same course that had been taken by\nNLP. We also discuss code-specific features such as AST, CFG, and unit tests,\nalong with their application in training code language models, and identify key\nchallenges and potential future directions in this domain. We keep the survey\nopen and updated on github repository at\nhttps://github.com/codefuse-ai/Awesome-Code-LLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziyin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bingchang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1\">Cong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zi Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Well Do Text Embedding Models Understand Syntax?. (arXiv:2311.07996v1 [cs.CL])","link":"http://arxiv.org/abs/2311.07996","description":"<p>Text embedding models have significantly contributed to advancements in\nnatural language processing by adeptly capturing semantic properties of textual\ndata. However, the ability of these models to generalize across a wide range of\nsyntactic contexts remains under-explored. In this paper, we first develop an\nevaluation set, named \\textbf{SR}, to scrutinize the capability for syntax\nunderstanding of text embedding models from two crucial syntactic aspects:\nStructural heuristics, and Relational understanding among concepts, as revealed\nby the performance gaps in previous studies. Our findings reveal that existing\ntext embedding models have not sufficiently addressed these syntactic\nunderstanding challenges, and such ineffectiveness becomes even more apparent\nwhen evaluated against existing benchmark datasets. Furthermore, we conduct\nrigorous analysis to unearth factors that lead to such limitations and examine\nwhy previous evaluations fail to detect such ineffectiveness. Lastly, we\npropose strategies to augment the generalization ability of text embedding\nmodels in diverse syntactic scenarios. This study serves to highlight the\nhurdles associated with syntactic generalization and provides pragmatic\nguidance for boosting model performance across varied syntactic contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhaopeng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1\">Zhiyang Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zuozhu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Analysis of the COVID-19 Infodemic in English and Chinese: Insights from Social Media Textual Data. (arXiv:2311.08001v1 [cs.SI])","link":"http://arxiv.org/abs/2311.08001","description":"<p>The COVID-19 infodemic, characterized by the rapid spread of misinformation\nand unverified claims related to the pandemic, presents a significant\nchallenge. This paper presents a comparative analysis of the COVID-19 infodemic\nin the English and Chinese languages, utilizing textual data extracted from\nsocial media platforms. To ensure a balanced representation, two infodemic\ndatasets were created by augmenting previously collected social media textual\ndata. Through word frequency analysis, the thirty-five most frequently\noccurring infodemic words are identified, shedding light on prevalent\ndiscussions surrounding the infodemic. Moreover, topic clustering analysis\nuncovers thematic structures and provides a deeper understanding of primary\ntopics within each language context. Additionally, sentiment analysis enables\ncomprehension of the emotional tone associated with COVID-19 information on\nsocial media platforms in English and Chinese. This research contributes to a\nbetter understanding of the COVID-19 infodemic phenomenon and can guide the\ndevelopment of strategies to combat misinformation during public health crises\nacross different languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jia Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Daiyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1\">Lei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baz_D/0/1/0/all/0/1\">Didier El Baz</a> (LAAS-SARA), <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinran Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TempTabQA: Temporal Question Answering for Semi-Structured Tables. (arXiv:2311.08002v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08002","description":"<p>Semi-structured data, such as Infobox tables, often include temporal\ninformation about entities, either implicitly or explicitly. Can current NLP\nsystems reason about such information in semi-structured tables? To tackle this\nquestion, we introduce the task of temporal question answering on\nsemi-structured tables. We present a dataset, TempTabQA, which comprises 11,454\nquestion-answer pairs extracted from 1,208 Wikipedia Infobox tables spanning\nmore than 90 distinct domains. Using this dataset, we evaluate several\nstate-of-the-art models for temporal reasoning. We observe that even the\ntop-performing LLMs lag behind human performance by more than 13.5 F1 points.\nGiven these results, our dataset has the potential to serve as a challenging\nbenchmark to improve the temporal reasoning capabilities of NLP models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vivek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandoi_P/0/1/0/all/0/1\">Pranshu Kandoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vora_M/0/1/0/all/0/1\">Mahek Bhavesh Vora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yujie He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reinanda_R/0/1/0/all/0/1\">Ridho Reinanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distantly-Supervised Named Entity Recognition with Uncertainty-aware Teacher Learning and Student-student Collaborative Learning. (arXiv:2311.08010v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08010","description":"<p>Distantly-Supervised Named Entity Recognition (DS-NER) effectively alleviates\nthe burden of annotation, but meanwhile suffers from the label noise. Recent\nworks attempt to adopt the teacher-student framework to gradually refine the\ntraining labels and improve the overall robustness. However, we argue that\nthese teacher-student methods achieve limited performance because poor network\ncalibration produces incorrectly pseudo-labeled samples, leading to error\npropagation. Therefore, we attempt to mitigate this issue by proposing: (1)\nUncertainty-aware Teacher Learning that leverages the prediction uncertainty to\nguide the selection of pseudo-labels, avoiding the number of incorrect\npseudo-labels in the self-training stage. (2) Student-student Collaborative\nLearning that allows the transfer of reliable labels between two student\nnetworks instead of completely relying on all pseudo-labels from its teacher.\nMeanwhile, this approach allows a full exploration of mislabeled samples rather\nthan simply filtering unreliable pseudo-labeled samples. Extensive experimental\nresults on five DS-NER datasets demonstrate that our method is superior to\nstate-of-the-art teacher-student methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Helan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1\">Shuzheng Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haozhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1\">Shuang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_K/0/1/0/all/0/1\">Kaikai An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zefan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forgetting before Learning: Utilizing Parametric Arithmetic for Knowledge Updating in Large Language Models. (arXiv:2311.08011v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08011","description":"<p>Recently Large Language Models (LLMs) have demonstrated their amazing text\nunderstanding and generation capabilities. However, even stronger LLMs may\nstill learn incorrect knowledge from the training corpus, as well as some\nknowledge that is outdated over time. Direct secondary fine-tuning with data\ncontaining new knowledge may be ineffective in updating knowledge due to the\nconflict between old and new knowledge. In this paper, we propose a new\nparadigm for fine-tuning called F-Learning (Forgetting before Learning), which\nis based on parametric arithmetic to achieve forgetting of old knowledge and\nlearning of new knowledge. Experimental results on two publicly available\ndatasets demonstrate that our proposed F-Learning can obviously improve the\nknowledge updating performance of both full fine-tuning and LoRA fine-tuning.\nMoreover, we have also discovered that forgetting old knowledge by subtracting\nthe parameters of LoRA can achieve a similar effect to subtracting the\nparameters of full fine-tuning, and sometimes even surpass it significantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_S/0/1/0/all/0/1\">Shiwen Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dingwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiping Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Preference Optimization. (arXiv:2311.08045v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08045","description":"<p>Human preference alignment is a crucial training step to improve the\ninteraction quality of large language models (LLMs). Existing aligning methods\ndepend on manually annotated preference data to guide the LLM optimization\ndirections. However, in practice, continuously updating LLMs raises a\ndistribution gap between model-generated samples and human-preferred responses,\nwhich hinders model fine-tuning efficiency. To mitigate this issue, previous\nmethods require additional preference annotation on generated samples to adapt\nthe shifted distribution, which consumes a large amount of annotation\nresources. Targeting more efficient human preference optimization, we propose\nan adversarial preference optimization (APO) framework, where the LLM agent and\nthe preference model update alternatively via a min-max game. Without\nadditional annotation, our APO method can make a self-adaption to the\ngeneration distribution gap through the adversarial learning process. In\nexperiments, we empirically verify the effectiveness of APO in improving LLM's\nhelpfulness and harmlessness compared with rejection sampling baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pengyu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yifan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1\">Nan Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data and models for stance and premise detection in COVID-19 tweets: insights from the Social Media Mining for Health (SMM4H) 2022 shared task. (arXiv:2311.08057v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08057","description":"<p>The COVID-19 pandemic has sparked numerous discussions on social media\nplatforms, with users sharing their views on topics such as mask-wearing and\nvaccination. To facilitate the evaluation of neural models for stance detection\nand premise classification, we organized the Social Media Mining for Health\n(SMM4H) 2022 Shared Task 2. This competition utilized manually annotated posts\non three COVID-19-related topics: school closures, stay-at-home orders, and\nwearing masks. In this paper, we extend the previous work and present newly\ncollected data on vaccination from Twitter to assess the performance of models\non a different topic. To enhance the accuracy and effectiveness of our\nevaluation, we employed various strategies to aggregate tweet texts with\nclaims, including models with feature-level (early) fusion and dual-view\narchitectures from SMM4H 2022 leaderboard. Our primary objective was to create\na valuable dataset and perform an extensive experimental evaluation to support\nfuture research in argument mining in the health domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davydova_V/0/1/0/all/0/1\">Vera Davydova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huabin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tutubalina_E/0/1/0/all/0/1\">Elena Tutubalina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Align after Pre-train: Improving Multilingual Generative Models with Cross-lingual Alignment. (arXiv:2311.08089v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08089","description":"<p>Multilingual generative models obtain remarkable cross-lingual capabilities\nthrough pre-training on large-scale corpora. However, they still exhibit a\nperformance bias toward high-resource languages, and learn isolated\ndistributions of sentence representations across languages. To bridge this gap,\nwe propose a simple yet effective alignment framework exploiting pairs of\ntranslation sentences. It aligns the internal sentence representations across\ndifferent languages via multilingual contrastive learning and aligns model\noutputs by answering prompts in different languages. Experimental results\ndemonstrate that even with less than 0.1 {\\textperthousand} of pre-training\ntokens, our alignment framework significantly boosts the cross-lingual\nabilities of generative models and mitigates the performance gap. Further\nanalysis reveals that it results in a better internal multilingual\nrepresentation distribution of multilingual models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaonan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_C/0/1/0/all/0/1\">Chengqing Zong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spot: A Natural Language Interface for Geospatial Searches in OSM. (arXiv:2311.08093v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08093","description":"<p>Investigative journalists and fact-checkers have found OpenStreetMap (OSM) to\nbe an invaluable resource for their work due to its extensive coverage and\nintricate details of various locations, which play a crucial role in\ninvestigating news scenes. Despite its value, OSM's complexity presents\nconsiderable accessibility and usability challenges, especially for those\nwithout a technical background. To address this, we introduce 'Spot', a\nuser-friendly natural language interface for querying OSM data. Spot utilizes a\nsemantic mapping from natural language to OSM tags, leveraging artificially\ngenerated sentence queries and a T5 transformer. This approach enables Spot to\nextract relevant information from user-input sentences and display candidate\nlocations matching the descriptions on a map. To foster collaboration and\nfuture advancement, all code and generated data is available as an open-source\nrepository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khellaf_L/0/1/0/all/0/1\">Lynn Khellaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlicht_I/0/1/0/all/0/1\">Ipek Baris Schlicht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayer_J/0/1/0/all/0/1\">Julia Bayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouwmeester_R/0/1/0/all/0/1\">Ruben Bouwmeester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirass_T/0/1/0/all/0/1\">Tilman Mira&#xdf;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_T/0/1/0/all/0/1\">Tilman Wagner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empowering Multi-step Reasoning across Languages via Tree-of-Thoughts. (arXiv:2311.08097v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08097","description":"<p>Chain-of-Thought (CoT) prompting empowers the reasoning abilities of Large\nLanguage Models (LLMs), eliciting them to solve complex reasoning tasks\nstep-by-step. However, with the success of CoT methods, the ability to deliver\nmulti-step reasoning remains limited to English due to the imbalance in the\ndistribution of the pre-training data, making the other languages a barrier.\n</p>\n<p>In this work, we propose a Cross-lingual multi-step reasoning approach,\naiming to align reasoning processes across different languages. In particular,\nour method, through a Self-consistent Cross-lingual prompting mechanism\ninspired by the Tree-of-Thoughts approach, delivers multi-step reasoning paths\nin different languages that, during the steps, lead to the final solution. Our\nexperimental evaluations show that our method significantly outperforms\nexisting prompting methods, reducing the number of interactions and achieving\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ranaldi_L/0/1/0/all/0/1\">Leonardo Ranaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanzotto_F/0/1/0/all/0/1\">Fabio Massimo Zanzotto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Semi-supervised Hierarchical Stacked Encoder for Legal Judgement Prediction. (arXiv:2311.08103v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08103","description":"<p>Predicting the judgment of a legal case from its unannotated case facts is a\nchallenging task. The lengthy and non-uniform document structure poses an even\ngreater challenge in extracting information for decision prediction. In this\nwork, we explore and propose a two-level classification mechanism; both\nsupervised and unsupervised; by using domain-specific pre-trained BERT to\nextract information from long documents in terms of sentence embeddings further\nprocessing with transformer encoder layer and use unsupervised clustering to\nextract hidden labels from these embeddings to better predict a judgment of a\nlegal case. We conduct several experiments with this mechanism and see higher\nperformance gains than the previously proposed methods on the ILDC dataset. Our\nexperimental results also show the importance of domain-specific pre-training\nof Transformer Encoders in legal information processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prasad_N/0/1/0/all/0/1\">Nishchal Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boughanem_M/0/1/0/all/0/1\">Mohand Boughanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dkaki_T/0/1/0/all/0/1\">Taoufiq Dkaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiLoCo: Distributed Low-Communication Training of Language Models. (arXiv:2311.08105v1 [cs.LG])","link":"http://arxiv.org/abs/2311.08105","description":"<p>Large language models (LLM) have become a critical component in many\napplications of machine learning. However, standard approaches to training LLM\nrequire a large number of tightly interconnected accelerators, with devices\nexchanging gradients and other intermediate states at each optimization step.\nWhile it is difficult to build and maintain a single computing cluster hosting\nmany accelerators, it might be easier to find several computing clusters each\nhosting a smaller number of devices. In this work, we propose a distributed\noptimization algorithm, Distributed Low-Communication (DiLoCo), that enables\ntraining of language models on islands of devices that are poorly connected.\nThe approach is a variant of federated averaging, where the number of inner\nsteps is large, the inner optimizer is AdamW, and the outer optimizer is\nNesterov momentum. On the widely used C4 dataset, we show that DiLoCo on 8\nworkers performs as well as fully synchronous optimization while communicating\n500 times less. DiLoCo exhibits great robustness to the data distribution of\neach worker. It is also robust to resources becoming unavailable over time, and\nvice versa, it can seamlessly leverage resources that become available during\ntraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Douillard_A/0/1/0/all/0/1\">Arthur Douillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Qixuan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusu_A/0/1/0/all/0/1\">Andrei A. Rusu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhaparia_R/0/1/0/all/0/1\">Rachita Chhaparia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donchev_Y/0/1/0/all/0/1\">Yani Donchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuncoro_A/0/1/0/all/0/1\">Adhiguna Kuncoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranzato_M/0/1/0/all/0/1\">Marc&#x27;Aurelio Ranzato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiajun Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Carpe Diem: On the Evaluation of World Knowledge in Lifelong Language Models. (arXiv:2311.08106v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08106","description":"<p>In an ever-evolving world, the dynamic nature of knowledge presents\nchallenges for language models that are trained on static data, leading to\noutdated encoded information. However, real-world scenarios require models not\nonly to acquire new knowledge but also to overwrite outdated information into\nupdated ones. To address this under-explored issue, we introduce the temporally\nevolving question answering benchmark, EvolvingQA - a novel benchmark designed\nfor training and evaluating LMs on an evolving Wikipedia database, where the\nconstruction of our benchmark is automated with our pipeline using large\nlanguage models. Our benchmark incorporates question-answering as a downstream\ntask to emulate real-world applications. Through EvolvingQA, we uncover that\nexisting continual learning baselines have difficulty in updating and\nforgetting outdated knowledge. Our findings suggest that the models fail to\nlearn updated knowledge due to the small weight gradient. Furthermore, we\nelucidate that the models struggle mostly on providing numerical or temporal\nanswers to questions asking for updated knowledge. Our work aims to model the\ndynamic nature of real-world information, offering a robust measure for the\nevolution-adaptability of language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yujin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jaehong Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Seonghyeon Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Se-young Yun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAIE Framework: Support Alone Isn't Enough -- Advancing LLM Training with Adversarial Remarks. (arXiv:2311.08107v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08107","description":"<p>Large Language Models (LLMs) can justify or criticize their predictions\nthrough discussion with other models or humans, thereby enhancing their\nintrinsic understanding of instances. While proactive discussions enhance\nperformance, this approach is currently limited to the inference phase. In this\ncontext, we posit a hypothesis: learning interactive discussions during\ntraining can improve understanding for the instances in the training step and\nproficiency in logical/critical thinking ability and verbalized expression of\nthe model in the inference step. Our proposed SAIE training method involves\nboth supportive and adversarial discussions between the learner and partner\nmodels. The learner model receives a remark from the partner through the\ndiscussion, and the parameters of the learner model are then updated based on\nthis remark. That is, the teacher signal dynamically adjusts in response to the\nevolving model output throughout the training step. By bolstering the capacity\nfor discussion and comprehension of instances, our experiments across datasets,\nincluding GSM8K, CommonsenseQA, and MMLU, reveal that models fine-tuned with\nour method consistently surpass those trained with standard fine-tuning\ntechniques. Moreover, our approach demonstrates superior performance in\nmulti-agent inference scenarios, boosting the models' reasoning abilities at\nthe inference step.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loem_M/0/1/0/all/0/1\">Mengsay Loem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaneko_M/0/1/0/all/0/1\">Masahiro Kaneko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1\">Naoaki Okazaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving hateful memes detection via learning hatefulness-aware embedding space through retrieval-guided contrastive learning. (arXiv:2311.08110v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08110","description":"<p>Hateful memes have emerged as a significant concern on the Internet. These\nmemes, which are a combination of image and text, often convey messages vastly\ndifferent from their individual meanings. Thus, detecting hateful memes\nrequires the system to jointly understand the visual and textual modalities.\nHowever, our investigation reveals that the embedding space of existing\nCLIP-based systems lacks sensitivity to subtle differences in memes that are\nvital for correct hatefulness classification. To address this issue, we propose\nconstructing a hatefulness-aware embedding space through retrieval-guided\ncontrastive training. Specifically, we add an auxiliary loss that utilizes hard\nnegative and pseudo-gold samples to train the embedding space. Our approach\nachieves state-of-the-art performance on the HatefulMemes dataset with an AUROC\nof 86.7. Notably, our approach outperforms much larger fine-tuned Large\nMultimodal Models like Flamingo and LLaVA. Finally, we demonstrate a\nretrieval-based hateful memes detection system, which is capable of making\nhatefulness classification based on data unseen in training from a database.\nThis allows developers to update the hateful memes detection system by simply\nadding new data without retraining, a desirable feature for real services in\nthe constantly-evolving landscape of hateful memes on the Internet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1\">Jingbiao Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinghong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weizhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byrne_B/0/1/0/all/0/1\">Bill Byrne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomalin_M/0/1/0/all/0/1\">Marcus Tomalin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Insights into Classifying and Mitigating LLMs' Hallucinations. (arXiv:2311.08117v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08117","description":"<p>The widespread adoption of large language models (LLMs) across diverse AI\napplications is proof of the outstanding achievements obtained in several\ntasks, such as text mining, text generation, and question answering. However,\nLLMs are not exempt from drawbacks. One of the most concerning aspects regards\nthe emerging problematic phenomena known as \"Hallucinations\". They manifest in\ntext generation systems, particularly in question-answering systems reliant on\nLLMs, potentially resulting in false or misleading information propagation.\nThis paper delves into the underlying causes of AI hallucination and elucidates\nits significance in artificial intelligence. In particular, Hallucination\nclassification is tackled over several tasks (Machine Translation, Question and\nAnswer, Dialog Systems, Summarisation Systems, Knowledge Graph with LLMs, and\nVisual Question Answer). Additionally, we explore potential strategies to\nmitigate hallucinations, aiming to enhance the overall reliability of LLMs. Our\nresearch addresses this critical issue within the HeReFaNMi (Health-Related\nFake News Mitigation) project, generously supported by NGI Search, dedicated to\ncombating Health-Related Fake News dissemination on the Internet. This\nendeavour represents a concerted effort to safeguard the integrity of\ninformation dissemination in an age of evolving AI technologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bruno_A/0/1/0/all/0/1\">Alessandro Bruno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazzeo_P/0/1/0/all/0/1\">Pier Luigi Mazzeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chetouani_A/0/1/0/all/0/1\">Aladine Chetouani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tliba_M/0/1/0/all/0/1\">Marouane Tliba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerkouri_M/0/1/0/all/0/1\">Mohamed Amine Kerkouri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-efficient Stochastic methods for Memory-based Transformers. (arXiv:2311.08123v1 [cs.LG])","link":"http://arxiv.org/abs/2311.08123","description":"<p>Training Memory-based transformers can require a large amount of memory and\ncan be quite inefficient. We propose a novel two-phase training mechanism and a\nnovel regularization technique to improve the training efficiency of\nmemory-based transformers, which are often used for long-range context\nproblems. For our experiments, we consider transformer-XL as our baseline model\nwhich is one of memorybased transformer models. We show that our resultant\nmodel, Skip Cross-head TransformerXL, outperforms the baseline on character\nlevel language modeling task with similar parameters and outperforms the\nbaseline on word level language modelling task with almost 20% fewer\nparameters. Our proposed methods do not require any additional memory. We also\ndemonstrate the effectiveness of our regularization mechanism on BERT which\nshows similar performance with reduction in standard deviation of scores of\naround 30% on multiple GLUE tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vishnu_V/0/1/0/all/0/1\">Vishwajit Kumar Vishnu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekhar_C/0/1/0/all/0/1\">C. Chandra Sekhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sinkhorn Transformations for Single-Query Postprocessing in Text-Video Retrieval. (arXiv:2311.08143v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08143","description":"<p>A recent trend in multimodal retrieval is related to postprocessing test set\nresults via the dual-softmax loss (DSL). While this approach can bring\nsignificant improvements, it usually presumes that an entire matrix of test\nsamples is available as DSL input. This work introduces a new postprocessing\napproach based on Sinkhorn transformations that outperforms DSL. Further, we\npropose a new postprocessing setting that does not require access to multiple\ntest queries. We show that our approach can significantly improve the results\nof state of the art models such as CLIP4Clip, BLIP, X-CLIP, and DRL, thus\nachieving a new state-of-the-art on several standard text-video retrieval\ndatasets both with access to the entire test set and in the single-query\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yakovlev_K/0/1/0/all/0/1\">Konstantin Yakovlev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polyakov_G/0/1/0/all/0/1\">Gregory Polyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alimova_I/0/1/0/all/0/1\">Ilseyar Alimova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Podolskiy_A/0/1/0/all/0/1\">Alexander Podolskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bout_A/0/1/0/all/0/1\">Andrey Bout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolenko_S/0/1/0/all/0/1\">Sergey Nikolenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovskaya_I/0/1/0/all/0/1\">Irina Piontkovskaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge. (arXiv:2311.08147v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08147","description":"<p>LLMs and AI chatbots have improved people's efficiency in various fields.\nHowever, the necessary knowledge for answering the question may be beyond the\nmodels' knowledge boundaries. To mitigate this issue, many researchers try to\nintroduce external knowledge, such as knowledge graphs and Internet contents,\ninto LLMs for up-to-date information. However, the external information from\nthe Internet may include counterfactual information that will confuse the model\nand lead to an incorrect response. Thus there is a pressing need for LLMs to\npossess the ability to distinguish reliable information from external\nknowledge. Therefore, to evaluate the ability of LLMs to discern the\nreliability of external knowledge, we create a benchmark from existing\nknowledge bases. Our benchmark consists of two tasks, Question Answering and\nText Generation, and for each task, we provide models with a context containing\ncounterfactual information. Evaluation results show that existing LLMs are\nsusceptible to interference from unreliable external knowledge with\ncounterfactual information, and simple intervention methods make limited\ncontributions to the alleviation of this issue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lianzhe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shicheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sishuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Reasoning in Large Language Models via Multi-Agent Peer Review Collaboration. (arXiv:2311.08152v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08152","description":"<p>Large Language Models (LLMs) have shown remarkable capabilities in general\nnatural language processing tasks but often fall short in complex reasoning\ntasks. Recent studies have explored human-like problem-solving strategies, such\nas self-correct, to push further the boundary of single-model reasoning\nability. In this work, we let a single model \"step outside the box\" by engaging\nmultiple models to correct each other. We introduce a multi-agent collaboration\nstrategy that emulates the academic peer review process. Each agent\nindependently constructs its own solution, provides reviews on the solutions of\nothers, and assigns confidence levels to its reviews. Upon receiving peer\nreviews, agents revise their initial solutions. Extensive experiments on three\ndifferent types of reasoning tasks show that our collaboration approach\ndelivers superior accuracy across all ten datasets compared to existing\nmethods. Further study demonstrates the effectiveness of integrating confidence\nin the reviews for math reasoning, and suggests a promising direction for\nhuman-mimicking multi-agent collaboration process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhenran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Senbao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Baotian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jindi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongfang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuxiang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ask One More Time: Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios. (arXiv:2311.08154v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08154","description":"<p>Although chain-of-thought (CoT) prompting combined with language models has\nachieved encouraging results on complex reasoning tasks, the naive greedy\ndecoding used in CoT prompting usually causes the repetitiveness and local\noptimality. To address this shortcoming, ensemble-optimization tries to obtain\nmultiple reasoning paths to get the final answer assembly. However, current\nensemble-optimization methods either simply employ rule-based post-processing\nsuch as \\textit{self-consistency}, or train an additional model based on\nseveral task-related human annotations to select the best one among multiple\nreasoning paths, yet fail to generalize to realistic settings where the type of\ninput questions is unknown or the answer format of reasoning paths is unknown.\nTo avoid their limitations, we propose \\textbf{self-agreement}, a generalizable\nensemble-optimization method applying in almost all scenarios where the type of\ninput questions and the answer format of reasoning paths may be known or\nunknown. Self-agreement firstly samples from language model's decoder to\ngenerate a \\textit{diverse} set of reasoning paths, and subsequently prompts\nthe language model \\textit{one more time} to determine the optimal answer by\nselecting the most \\textit{agreed} answer among the sampled reasoning paths.\nSelf-agreement simultaneously achieves remarkable performance on six public\nreasoning benchmarks and superior generalization capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jiayi Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Junchen Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fuzheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Di Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1\">Kun Gai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MechAgents: Large language model multi-agent collaborations can solve mechanics problems, generate new data, and integrate knowledge. (arXiv:2311.08166v1 [cs.AI])","link":"http://arxiv.org/abs/2311.08166","description":"<p>Solving mechanics problems using numerical methods requires comprehensive\nintelligent capability of retrieving relevant knowledge and theory,\nconstructing and executing codes, analyzing the results, a task that has thus\nfar mainly been reserved for humans. While emerging AI methods can provide\neffective approaches to solve end-to-end problems, for instance via the use of\ndeep surrogate models or various data analytics strategies, they often lack\nphysical intuition since knowledge is baked into the parametric complement\nthrough training, offering less flexibility when it comes to incorporating\nmathematical or physical insights. By leveraging diverse capabilities of\nmultiple dynamically interacting large language models (LLMs), we can overcome\nthe limitations of conventional approaches and develop a new class of\nphysics-inspired generative machine learning platform, here referred to as\nMechAgents. A set of AI agents can solve mechanics tasks, here demonstrated for\nelasticity problems, via autonomous collaborations. A two-agent team can\neffectively write, execute and self-correct code, in order to apply finite\nelement methods to solve classical elasticity problems in various flavors\n(different boundary conditions, domain geometries, meshes, small/finite\ndeformation and linear/hyper-elastic constitutive laws, and others). For more\ncomplex tasks, we construct a larger group of agents with enhanced division of\nlabor among planning, formulating, coding, executing and criticizing the\nprocess and results. The agents mutually correct each other to improve the\noverall team-work performance in understanding, formulating and validating the\nsolution. Our framework shows the potential of synergizing the intelligence of\nlanguage models, the reliability of physics-based modeling, and the dynamic\ncollaborations among diverse agents, opening novel avenues for automation of\nsolving engineering problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bo Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buehler_M/0/1/0/all/0/1\">Markus J. Buehler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Evolved Diverse Data Sampling for Efficient Instruction Tuning. (arXiv:2311.08182v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08182","description":"<p>Enhancing the instruction-following ability of Large Language Models (LLMs)\nprimarily demands substantial instruction-tuning datasets. However, the sheer\nvolume of these imposes a considerable computational burden and annotation\ncost. To investigate a label-efficient instruction tuning method that allows\nthe model itself to actively sample subsets that are equally or even more\neffective, we introduce a self-evolving mechanism DiverseEvol. In this process,\na model iteratively augments its training subset to refine its own performance,\nwithout requiring any intervention from humans or more advanced LLMs. The key\nto our data sampling technique lies in the enhancement of diversity in the\nchosen subsets, as the model selects new data points most distinct from any\nexisting ones according to its current embedding space. Extensive experiments\nacross three datasets and benchmarks demonstrate the effectiveness of\nDiverseEvol. Our models, trained on less than 8% of the original dataset,\nmaintain or improve performance compared with finetuning on full data. We also\nprovide empirical evidence to analyze the importance of diversity in\ninstruction data and the iterative scheme as opposed to one-time sampling. Our\ncode is publicly available at https://github.com/OFA-Sys/DiverseEvol.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shengguang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Keming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Benfeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1\">Qi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unlocking Science: Novel Dataset and Benchmark for Cross-Modality Scientific Information Extraction. (arXiv:2311.08189v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08189","description":"<p>Extracting key information from scientific papers has the potential to help\nresearchers work more efficiently and accelerate the pace of scientific\nprogress. Over the last few years, research on Scientific Information\nExtraction (SciIE) witnessed the release of several new systems and benchmarks.\nHowever, existing paper-focused datasets mostly focus only on specific parts of\na manuscript (e.g., abstracts) and are single-modality (i.e., text- or\ntable-only), due to complex processing and expensive annotations. Moreover,\ncore information can be present in either text or tables or across both. To\nclose this gap in data availability and enable cross-modality IE, while\nalleviating labeling costs, we propose a semi-supervised pipeline for\nannotating entities in text, as well as entities and relations in tables, in an\niterative procedure. Based on this pipeline, we release novel resources for the\nscientific community, including a high-quality benchmark, a large-scale corpus,\nand a semi-supervised annotation pipeline. We further report the performance of\nstate-of-the-art IE models on the proposed benchmark dataset, as a baseline.\nLastly, we explore the potential capability of large language models such as\nChatGPT for the current task. Our new dataset, results, and analysis validate\nthe effectiveness and efficiency of our semi-supervised pipeline, and we\ndiscuss its remaining limitations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlsso_B/0/1/0/all/0/1\">B&#xf6;rje F. Karlsso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okumura_M/0/1/0/all/0/1\">Manabu Okumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chin-Yew Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GEC-DePenD: Non-Autoregressive Grammatical Error Correction with Decoupled Permutation and Decoding. (arXiv:2311.08191v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08191","description":"<p>Grammatical error correction (GEC) is an important NLP task that is currently\nusually solved with autoregressive sequence-to-sequence models. However,\napproaches of this class are inherently slow due to one-by-one token\ngeneration, so non-autoregressive alternatives are needed. In this work, we\npropose a novel non-autoregressive approach to GEC that decouples the\narchitecture into a permutation network that outputs a self-attention weight\nmatrix that can be used in beam search to find the best permutation of input\ntokens (with auxiliary {ins} tokens) and a decoder network based on a\nstep-unrolled denoising autoencoder that fills in specific tokens. This allows\nus to find the token permutation after only one forward pass of the permutation\nnetwork, avoiding autoregressive constructions. We show that the resulting\nnetwork improves over previously known non-autoregressive methods for GEC and\nreaches the level of autoregressive methods that do not use language-specific\nsynthetic data generation methods. Our results are supported by a comprehensive\nexperimental validation on the ConLL-2014 and Write&amp;Improve+LOCNESS datasets\nand an extensive ablation study that supports our architectural and algorithmic\nchoices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yakovlev_K/0/1/0/all/0/1\">Konstantin Yakovlev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Podolskiy_A/0/1/0/all/0/1\">Alexander Podolskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bout_A/0/1/0/all/0/1\">Andrey Bout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolenko_S/0/1/0/all/0/1\">Sergey Nikolenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovskaya_I/0/1/0/all/0/1\">Irina Piontkovskaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Fact-Checking in Dialogue: Are Specialized Models Needed?. (arXiv:2311.08195v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08195","description":"<p>Prior research has shown that typical fact-checking models for stand-alone\nclaims struggle with claims made in dialogues. As a solution, fine-tuning these\nmodels on labelled dialogue data has been proposed. However, creating separate\nmodels for each use case is impractical, and we show that fine-tuning models\nfor dialogue results in poor performance on typical fact-checking. To overcome\nthis challenge, we present techniques that allow us to use the same models for\nboth dialogue and typical fact-checking. These mainly focus on retrieval\nadaptation and transforming conversational inputs so that they can be\naccurately predicted by models trained on stand-alone claims. We demonstrate\nthat a typical fact-checking model incorporating these techniques is\ncompetitive with state-of-the-art models fine-tuned for dialogue, while\nmaintaining its accuracy on stand-alone claims.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chamoun_E/0/1/0/all/0/1\">Eric Chamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeidi_M/0/1/0/all/0/1\">Marzieh Saeidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human-Centric Autonomous Systems With LLMs for User Command Reasoning. (arXiv:2311.08206v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08206","description":"<p>The evolution of autonomous driving has made remarkable advancements in\nrecent years, evolving into a tangible reality. However, a human-centric\nlarge-scale adoption hinges on meeting a variety of multifaceted requirements.\nTo ensure that the autonomous system meets the user's intent, it is essential\nto accurately discern and interpret user commands, especially in complex or\nemergency situations. To this end, we propose to leverage the reasoning\ncapabilities of Large Language Models (LLMs) to infer system requirements from\nin-cabin users' commands. Through a series of experiments that include\ndifferent LLM models and prompt designs, we explore the few-shot multivariate\nbinary classification accuracy of system requirements from natural language\ntextual commands. We confirm the general ability of LLMs to understand and\nreason about prompts but underline that their effectiveness is conditioned on\nthe quality of both the LLM model and the design of appropriate sequential\nprompts. Code and models are public with the link\n\\url{https://github.com/KTH-RPL/DriveCmd_LLM}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Ci Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marta_D/0/1/0/all/0/1\">Daniel Sim&#xf5;es Marta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batool_N/0/1/0/all/0/1\">Nazre Batool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Folkesson_J/0/1/0/all/0/1\">John Folkesson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unlock the Power: Competitive Distillation for Multi-Modal Large Language Models. (arXiv:2311.08213v1 [cs.CV])","link":"http://arxiv.org/abs/2311.08213","description":"<p>Recently, multi-modal content generation has attracted lots of attention from\nresearchers by investigating the utilization of visual instruction tuning based\non large language models (LLMs). To enhance the performance and generalization\nability of such LLMs, the practice of distilling knowledge from pretrained\nmulti-modal models (a.k.a. teachers) to more compact multi-modal LLMs\n(students) has gained considerable interest. However, the prevailing paradigm\nof instructiontuning in multi-modal LLMs knowledge distillation is\nresource-intensive and unidirectional, neglecting the potential for mutual\nfeedback between the student and teacher models. Thus, we propose an innovative\nCompetitive Multi-modal Distillation framework (CoMD), which captures\nbidirectional feedback between teacher and student models and continually\nupdates the multi-modal capabilities that the student model has learned. It\ncomprises two stages: multi-modal pre-training and multi-modal competitive\ndistillation. The first stage pre-trains the student model on a large number of\nfiltered multi-modal datasets. The second stage facilitates a bidirectional\nknowledge transfer between the student and teacher models. Our experimental\nanalysis of diverse datasets shows that our knowledge transfer method\nconsistently improves the capabilities of the student model. Finally, the\n7B-sized student model after four distillations surpassed the current\nstate-of-the-art model LLaVA-13B on the ScienceQA and LLaVA Test dataset, also\noutperforms other strong baselines in the zero-shot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eval-GCSC: A New Metric for Evaluating ChatGPT's Performance in Chinese Spelling Correction. (arXiv:2311.08219v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08219","description":"<p>ChatGPT has demonstrated impressive performance in various downstream tasks.\nHowever, in the Chinese Spelling Correction (CSC) task, we observe a\ndiscrepancy: while ChatGPT performs well under human evaluation, it scores\npoorly according to traditional metrics. We believe this inconsistency arises\nbecause the traditional metrics are not well-suited for evaluating generative\nmodels. Their overly strict length and phonics constraints may lead to\nunderestimating ChatGPT's correction capabilities. To better evaluate\ngenerative models in the CSC task, this paper proposes a new evaluation metric:\nEval-GCSC. By incorporating word-level and semantic similarity judgments, it\nrelaxes the stringent length and phonics constraints. Experimental results show\nthat Eval-GCSC closely aligns with human evaluations. Under this metric,\nChatGPT's performance is comparable to traditional token-level classification\nmodels (TCM), demonstrating its potential as a CSC tool. The source code and\nscripts can be accessed at https://github.com/ktlKTL/Eval-GCSC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kunting Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaolei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hanhan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Liang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Encoding of Words in BERT's Neurons using Feature Textualization. (arXiv:2311.08240v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08240","description":"<p>Pretrained language models (PLMs) form the basis of most state-of-the-art NLP\ntechnologies. Nevertheless, they are essentially black boxes: Humans do not\nhave a clear understanding of what knowledge is encoded in different parts of\nthe models, especially in individual neurons. The situation is different in\ncomputer vision, where feature visualization provides a decompositional\ninterpretability technique for neurons of vision models. Activation\nmaximization is used to synthesize inherently interpretable visual\nrepresentations of the information encoded in individual neurons. Our work is\ninspired by this but presents a cautionary tale on the interpretability of\nsingle neurons, based on the first large-scale attempt to adapt activation\nmaximization to NLP, and, more specifically, large PLMs. We propose feature\ntextualization, a technique to produce dense representations of neurons in the\nPLM word embedding space. We apply feature textualization to the BERT model\n(Devlin et al., 2019) to investigate whether the knowledge encoded in\nindividual neurons can be interpreted and symbolized. We find that the produced\nrepresentations can provide insights about the knowledge encoded in individual\nneurons, but that individual neurons do not represent clearcut symbolic units\nof language such as words. Additionally, we use feature textualization to\ninvestigate how many neurons are needed to encode words in BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baeumel_T/0/1/0/all/0/1\">Tanja Baeumel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijayakumar_S/0/1/0/all/0/1\">Soniya Vijayakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genabith_J/0/1/0/all/0/1\">Josef van Genabith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1\">Guenter Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostermann_S/0/1/0/all/0/1\">Simon Ostermann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Using Distribution-Based Compositionality Assessment to Evaluate Compositional Generalisation in Machine Translation. (arXiv:2311.08249v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08249","description":"<p>Compositional generalisation (CG), in NLP and in machine learning more\ngenerally, has been assessed mostly using artificial datasets. It is important\nto develop benchmarks to assess CG also in real-world natural language tasks in\norder to understand the abilities and limitations of systems deployed in the\nwild. To this end, our GenBench Collaborative Benchmarking Task submission\nutilises the distribution-based compositionality assessment (DBCA) framework to\nsplit the Europarl translation corpus into a training and a test set in such a\nway that the test set requires compositional generalisation capacity.\nSpecifically, the training and test sets have divergent distributions of\ndependency relations, testing NMT systems' capability of translating\ndependencies that they have not been trained on. This is a fully-automated\nprocedure to create natural language compositionality benchmarks, making it\nsimple and inexpensive to apply it further to other datasets and languages. The\ncode and data for the experiments is available at\nhttps://github.com/aalto-speech/dbca.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moisio_A/0/1/0/all/0/1\">Anssi Moisio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creutz_M/0/1/0/all/0/1\">Mathias Creutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurimo_M/0/1/0/all/0/1\">Mikko Kurimo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REST: Retrieval-Based Speculative Decoding. (arXiv:2311.08252v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08252","description":"<p>We introduce Retrieval-Based Speculative Decoding (REST), a novel algorithm\ndesigned to speed up language model generation. The key insight driving the\ndevelopment of REST is the observation that the process of text generation\noften includes certain common phases and patterns. Unlike previous methods that\nrely on a draft language model for speculative decoding, REST harnesses the\npower of retrieval to generate draft tokens. This method draws from the\nreservoir of existing knowledge, retrieving and employing relevant tokens based\non the current context. Its plug-and-play nature allows for seamless\nintegration and acceleration of any language models, all without necessitating\nadditional training. When benchmarked on 7B and 13B language models in a\nsingle-batch setting, REST achieves a significant speedup of 1.62X to 2.36X on\ncode or text generation. The code of REST is available at\nhttps://github.com/FasterDecoding/REST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhenyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zexuan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Tianle Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jason D Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads to Answers Faster. (arXiv:2311.08263v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08263","description":"<p>In this work, we propose FastCoT, a model-agnostic framework based on\nparallel decoding without any further training of an auxiliary model or\nmodification to the LLM itself. FastCoT uses a size-varying context window\nwhose size changes with position to conduct parallel decoding and\nauto-regressive decoding simultaneously, thus fully utilizing GPU computation\nresources. In FastCoT, the parallel decoding part provides the LLM with a quick\nglance of the future composed of approximate tokens, which could lead to faster\nanswers compared to regular autoregressive decoding used by causal\ntransformers. We also provide an implementation of parallel decoding within\nLLM, which supports KV-cache generation and batch processing. Through extensive\nexperiments, we demonstrate that FastCoT saves inference time by nearly 20%\nwith only a negligible performance drop compared to the regular approach.\nAdditionally, we show that the context window size exhibits considerable\nrobustness for different tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhining Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiaqi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_C/0/1/0/all/0/1\">Chenyi Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjie Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guihai Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily. (arXiv:2311.08268v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08268","description":"<p>Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to\nprovide useful and safe responses. However, adversarial prompts known as\n'jailbreaks' can circumvent safeguards, leading LLMs to generate harmful\ncontent. Exploring jailbreak prompts can help to better reveal the weaknesses\nof LLMs and further steer us to secure them. Unfortunately, existing jailbreak\nmethods either suffer from intricate manual design or require optimization on\nanother white-box model, compromising generalization or jailbreak efficiency.\nIn this paper, we generalize jailbreak prompt attacks into two aspects: (1)\nPrompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM,\nan automatic framework that leverages LLMs themselves to generate effective\njailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly\nimproves the attack success rate while greatly reducing the time cost compared\nto existing baselines. Our study also reveals the inadequacy of current defense\nmethods in safeguarding LLMs. Finally, we offer detailed analysis and\ndiscussion from the perspective of prompt execution priority on the failure of\nLLMs' defense. We hope that our research can catalyze both the academic\ncommunity and LLMs vendors towards the provision of safer and more regulated\nLarge Language Models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_P/0/1/0/all/0/1\">Peng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_J/0/1/0/all/0/1\">Jun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1\">Dan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xuezhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xian_Y/0/1/0/all/0/1\">Yunsen Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Examining Modularity in Multilingual LMs via Language-Specialized Subnetworks. (arXiv:2311.08273v1 [cs.CL])","link":"http://arxiv.org/abs/2311.08273","description":"<p>Recent work has proposed explicitly inducing language-wise modularity in\nmultilingual LMs via sparse fine-tuning (SFT) on per-language subnetworks as a\nmeans of better guiding cross-lingual sharing. In this work, we investigate (1)\nthe degree to which language-wise modularity naturally arises within models\nwith no special modularity interventions, and (2) how cross-lingual sharing and\ninterference differ between such models and those with explicit SFT-guided\nsubnetwork modularity. To quantify language specialization and cross-lingual\ninteraction, we use a Training Data Attribution method that estimates the\ndegree to which a model's predictions are influenced by in-language or\ncross-language training examples. Our results show that language-specialized\nsubnetworks do naturally arise, and that SFT, rather than always increasing\nmodularity, can decrease language specialization of subnetworks in favor of\nmore cross-lingual sharing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choenni_R/0/1/0/all/0/1\">Rochelle Choenni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shutova_E/0/1/0/all/0/1\">Ekaterina Shutova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrette_D/0/1/0/all/0/1\">Dan Garrette</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CogAlign: Learning to Align Textual Neural Representations to Cognitive Language Processing Signals. (arXiv:2106.05544v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.05544","description":"<p>Most previous studies integrate cognitive language processing signals (e.g.,\neye-tracking or EEG data) into neural models of natural language processing\n(NLP) just by directly concatenating word embeddings with cognitive features,\nignoring the gap between the two modalities (i.e., textual vs. cognitive) and\nnoise in cognitive features. In this paper, we propose a CogAlign approach to\nthese issues, which learns to align textual neural representations to cognitive\nfeatures. In CogAlign, we use a shared encoder equipped with a modality\ndiscriminator to alternatively encode textual and cognitive inputs to capture\ntheir differences and commonalities. Additionally, a text-aware attention\nmechanism is proposed to detect task-related information and to avoid using\nnoise in cognitive features. Experimental results on three NLP tasks, namely\nnamed entity recognition, sentiment analysis and relation extraction, show that\nCogAlign achieves significant improvements with multiple cognitive features\nover state-of-the-art models on public datasets. Moreover, our model is able to\ntransfer cognitive information to other datasets that do not have any cognitive\nprocessing signals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yuqi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Deyi Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ComCLIP: Training-Free Compositional Image and Text Matching. (arXiv:2211.13854v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.13854","description":"<p>Contrastive Language-Image Pretraining (CLIP) has demonstrated great\nzero-shot performance for matching images and text. However, it is still\nchallenging to adapt vision-lanaguage pretrained models like CLIP to\ncompositional image and text matching -- a more challenging image and text\nmatching task requiring the model understanding of compositional word concepts\nand visual components. Towards better compositional generalization in zero-shot\nimage and text matching, in this paper, we study the problem from a causal\nperspective: the erroneous semantics of individual entities are essentially\nconfounders that cause the matching failure. Therefore, we propose a novel\n\\textbf{\\textit{training-free}} compositional CLIP model (ComCLIP). ComCLIP\ndisentangles input images into subjects, objects, and action sub-images and\ncomposes CLIP's vision encoder and text encoder to perform evolving matching\nover compositional text embedding and sub-image embeddings. In this way,\nComCLIP can mitigate spurious correlations introduced by the pretrained CLIP\nmodels and dynamically evaluate the importance of each component. Experiments\non four compositional image-text matching datasets: SVO, ComVG, Winoground, and\nVL-checklist, and two general image-text retrieval datasets: Flick30K, and\nMSCOCO demonstrate the effectiveness of our plug-and-play method, which boosts\nthe \\textbf{\\textit{zero-shot}} inference ability of CLIP, SLIP, and BLIP2 even\nwithout further training or fine-tuning. Our codes can be found at\nhttps://github.com/eric-ai-lab/ComCLIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kenan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuehai He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruize Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simplifying and Understanding State Space Models with Diagonal Linear RNNs. (arXiv:2212.00768v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2212.00768","description":"<p>Sequence models based on linear state spaces (SSMs) have recently emerged as\na promising choice of architecture for modeling long range dependencies across\nvarious modalities. However, they invariably rely on discretization of a\ncontinuous state space, which complicates their presentation and understanding.\nIn this work, we dispose of the discretization step, and propose a model based\non vanilla Diagonal Linear RNNs ($\\mathrm{DLR}$). We empirically show that,\ndespite being conceptually much simpler, $\\mathrm{DLR}$ is as performant as\npreviously-proposed SSMs on a variety of tasks and benchmarks including Long\nRange Arena and raw speech classification. Moreover, we characterize the\nexpressivity of SSMs (including $\\mathrm{DLR}$) and attention-based models via\na suite of $13$ synthetic sequence-to-sequence tasks involving interactions\nover tens of thousands of tokens, ranging from simple operations, such as\nshifting an input sequence, to detecting co-dependent visual features over long\nspatial ranges in flattened images. We find that while SSMs report near-perfect\nperformance on tasks that can be modeled via $\\textit{few}$ convolutional\nkernels, they struggle on tasks requiring $\\textit{many}$ such kernels and\nespecially when the desired sequence manipulation is\n$\\textit{context-dependent}$. Despite these limitations, $\\mathrm{DLR}$ reaches\nhigh performance on two higher-order reasoning tasks $\\mathrm{ListOpsSubTrees}$\nand $\\mathrm{PathfinderSegmentation}\\text{-}\\mathrm{256}$ with input lengths\n$8K$ and $65K$ respectively, and gives encouraging performance on\n$\\mathrm{PathfinderSegmentation}\\text{-}\\mathrm{512}$ with input length $262K$\nfor which attention is not a viable choice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ankit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_H/0/1/0/all/0/1\">Harsh Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Beam Search for Hallucination Mitigation in Abstractive Summarization. (arXiv:2212.02712v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.02712","description":"<p>Advancement in large pretrained language models has significantly improved\ntheir performance for conditional language generation tasks including\nsummarization albeit with hallucinations. To reduce hallucinations,\nconventional methods proposed improving beam search or using a fact checker as\na postprocessing step. In this paper, we investigate the use of the Natural\nLanguage Inference (NLI) entailment metric to detect and prevent hallucinations\nin summary generation. We propose an NLI-assisted beam re-ranking mechanism by\ncomputing entailment probability scores between the input context and\nsummarization model-generated beams during saliency-enhanced greedy decoding.\nMoreover, a diversity metric is introduced to compare its effectiveness against\nvanilla beam search. Our proposed algorithm significantly outperforms vanilla\nbeam decoding on XSum and CNN/DM datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_A/0/1/0/all/0/1\">Arvind Krishna Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Visser_E/0/1/0/all/0/1\">Erik Visser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MOPRD: A multidisciplinary open peer review dataset. (arXiv:2212.04972v2 [cs.DL] UPDATED)","link":"http://arxiv.org/abs/2212.04972","description":"<p>Open peer review is a growing trend in academic publications. Public access\nto peer review data can benefit both the academic and publishing communities.\nIt also serves as a great support to studies on review comment generation and\nfurther to the realization of automated scholarly paper review. However, most\nof the existing peer review datasets do not provide data that cover the whole\npeer review process. Apart from this, their data are not diversified enough as\nthe data are mainly collected from the field of computer science. These two\ndrawbacks of the currently available peer review datasets need to be addressed\nto unlock more opportunities for related studies. In response, we construct\nMOPRD, a multidisciplinary open peer review dataset. This dataset consists of\npaper metadata, multiple version manuscripts, review comments, meta-reviews,\nauthor's rebuttal letters, and editorial decisions. Moreover, we propose a\nmodular guided review comment generation method based on MOPRD. Experiments\nshow that our method delivers better performance as indicated by both automatic\nmetrics and human evaluation. We also explore other potential applications of\nMOPRD, including meta-review generation, editorial decision prediction, author\nrebuttal generation, and scientometric analysis. MOPRD is a strong endorsement\nfor further studies in peer review-related research and other applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jialiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiaxin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhangping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yidong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaodong Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistency Analysis of ChatGPT. (arXiv:2303.06273v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.06273","description":"<p>ChatGPT has gained a huge popularity since its introduction. Its positive\naspects have been reported through many media platforms, and some analyses even\nshowed that ChatGPT achieved a decent grade in professional exams, adding extra\nsupport to the claim that AI can now assist and even replace humans in\nindustrial fields. Others, however, doubt its reliability and trustworthiness.\nThis paper investigates the trustworthiness of ChatGPT and GPT-4 regarding\nlogically consistent behaviour, focusing specifically on semantic consistency\nand the properties of negation, symmetric, and transitive consistency. Our\nfindings suggest that while both models appear to show an enhanced language\nunderstanding and reasoning ability, they still frequently fall short of\ngenerating logically consistent predictions. We also ascertain via experiments\nthat prompt designing, few-shot learning and employing larger large language\nmodels (LLMs) are unlikely to be the ultimate solution to resolve the\ninconsistency issue of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_M/0/1/0/all/0/1\">Myeongjun Erik Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning. (arXiv:2303.16445v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.16445","description":"<p>Language model probing is often used to test specific capabilities of models.\nHowever, conclusions from such studies may be limited when the probing\nbenchmarks are small and lack statistical power. In this work, we introduce\nnew, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500)\ninspired by psycholinguistic studies. We dramatically extend existing NEG-136\nand ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44\nsentence pairs to 750 each. We also create another version of extended negation\ndataset (NEG-1500-SIMP-TEMP), created using template-based generation. It\nconsists of 770 sentence pairs. We evaluate 22 models on the extended datasets,\nseeing model performance dip 20-57% compared to the original smaller\nbenchmarks. We observe high levels of negation sensitivity in models like BERT\nand ALBERT demonstrating that previous findings might have been skewed due to\nsmaller test sets. Finally, we observe that while GPT3 has generated all the\nexamples in ROLE-1500 is only able to solve 24.6% of them during probing. The\ndatasets and code are available on\n$\\href{https://github.com/text-machine-lab/extending_psycholinguistic_dataset}{Github}$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shivagunde_N/0/1/0/all/0/1\">Namrata Shivagunde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lialin_V/0/1/0/all/0/1\">Vladislav Lialin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1\">Anna Rumshisky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Aware Semantic Similarity Measurement for Unsupervised Word Sense Disambiguation. (arXiv:2305.03520v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03520","description":"<p>The issue of word sense ambiguity poses a significant challenge in natural\nlanguage processing due to the scarcity of annotated data to feed machine\nlearning models to face the challenge. Therefore, unsupervised word sense\ndisambiguation methods have been developed to overcome that challenge without\nrelying on annotated data. This research proposes a new context-aware approach\nto unsupervised word sense disambiguation, which provides a flexible mechanism\nfor incorporating contextual information into the similarity measurement\nprocess. We experiment with a popular benchmark dataset to evaluate the\nproposed strategy and compare its performance with state-of-the-art\nunsupervised word sense disambiguation techniques. The experimental results\nindicate that our approach substantially enhances disambiguation accuracy and\nsurpasses the performance of several existing techniques. Our findings\nunderscore the significance of integrating contextual information in semantic\nsimilarity measurements to manage word sense ambiguity in unsupervised\nscenarios effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Gil_J/0/1/0/all/0/1\">Jorge Martinez-Gil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scientific Opinion Summarization: Meta-review Generation with Checklist-guided Iterative Introspection. (arXiv:2305.14647v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14647","description":"<p>Opinions in the scientific domain can be divergent, leading to controversy or\nconsensus among reviewers. However, current opinion summarization datasets\nmostly focus on product review domains, which do not account for this\nvariability under the assumption that the input opinions are non-controversial.\nTo address this gap, we propose the task of scientific opinion summarization,\nwhere research paper reviews are synthesized into meta-reviews. To facilitate\nthis task, we introduce a new ORSUM dataset covering 10,989 paper meta-reviews\nand 40,903 paper reviews from 39 conferences. Furthermore, we propose the\nChecklist-guided Iterative Introspection (CGI$^2$) approach, which breaks down\nthe task into several stages and iteratively refines the summary under the\nguidance of questions from a checklist. We conclude that (1) human-written\nsummaries are not always reliable since many do not follow the guidelines, and\n(2) the combination of task decomposition and iterative self-refinement shows\npromising discussion involvement ability and can be applied to other complex\ntext generation using black-box LLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1\">Qi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidhu_M/0/1/0/all/0/1\">Mankeerat Sidhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1\">Hou Pong Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Natural Language Explanations to Rescale Human Judgments. (arXiv:2305.14770v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14770","description":"<p>The rise of large language models (LLMs) has brought a critical need for\nhigh-quality human-labeled data, particularly for processes like human feedback\nand evaluation. A common practice is to label data via consensus annotation\nover crowdworker judgments. However, annotators' judgments for subjective tasks\ncan differ in many ways: they may have different qualitative judgments about an\nexample, and they may map those to a labeling scheme in different ways. We show\nthat these nuances can be captured by natural language explanations, and\npropose a method to rescale ordinal annotations and explanations using LLMs.\nSpecifically, we feed annotators' Likert ratings and corresponding explanations\ninto an LLM and prompt it to produce a numeric score anchored in a scoring\nrubric. These scores should reflect the annotators' underlying assessments of\nthe example. The rubric can be designed or modified after annotation, and\ninclude distinctions that may not have been known when the original error\ntaxonomy was devised. We explore our technique in the context of rating system\noutputs for a document-grounded question answering task, where LLMs achieve\nnear-human performance. Our method rescales the raw judgments without impacting\nagreement and brings the scores closer to human judgments grounded in the same\nscoring rubric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wadhwa_M/0/1/0/all/0/1\">Manya Wadhwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jifan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-training Intent-Aware Encoders for Zero- and Few-Shot Intent Classification. (arXiv:2305.14827v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14827","description":"<p>Intent classification (IC) plays an important role in task-oriented dialogue\nsystems. However, IC models often generalize poorly when training without\nsufficient annotated examples for each user intent. We propose a novel\npre-training method for text encoders that uses contrastive learning with\nintent psuedo-labels to produce embeddings that are well-suited for IC tasks,\nreducing the need for manual annotations. By applying this pre-training\nstrategy, we also introduce Pre-trained Intent-aware Encoder (PIE), which is\ndesigned to align encodings of utterances with their intent names.\nSpecifically, we first train a tagger to identify key phrases within utterances\nthat are crucial for interpreting intents. We then use these extracted phrases\nto create examples for pre-training a text encoder in a contrastive manner. As\na result, our PIE model achieves up to 5.4% and 4.0% higher accuracy than the\nprevious state-of-the-art text encoder for the N-way zero- and one-shot\nsettings on four IC datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1\">Mujeen Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gung_J/0/1/0/all/0/1\">James Gung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansimov_E/0/1/0/all/0/1\">Elman Mansimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappas_N/0/1/0/all/0/1\">Nikolaos Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_R/0/1/0/all/0/1\">Raphael Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romeo_S/0/1/0/all/0/1\">Salvatore Romeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castelli_V/0/1/0/all/0/1\">Vittorio Castelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complementary and Integrative Health Lexicon (CIHLex) and Entity Recognition in the Literature. (arXiv:2305.17353v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17353","description":"<p>Objective: Our study aimed to construct an exhaustive Complementary and\nIntegrative Health (CIH) Lexicon (CIHLex) to better represent the often\nunderrepresented physical and psychological CIH approaches in standard\nterminologies. We also intended to apply advanced Natural Language Processing\n(NLP) models such as Bidirectional Encoder Representations from Transformers\n(BERT) and GPT-3.5 Turbo for CIH named entity recognition, evaluating their\nperformance against established models like MetaMap and CLAMP. Materials and\nMethods: We constructed the CIHLex by integrating various resources, compiling\nand integrating data from biomedical literature and relevant knowledge bases.\nThe Lexicon encompasses 198 unique concepts with 1090 corresponding unique\nterms. We matched these concepts to the Unified Medical Language System (UMLS).\nAdditionally, we developed and utilized BERT models and compared their\nefficiency in CIH named entity recognition to that of other models such as\nMetaMap, CLAMP, and GPT3.5-turbo. Results: From the 198 unique concepts in\nCIHLex, 62.1% could be matched to at least one term in the UMLS. Moreover,\n75.7% of the mapped UMLS Concept Unique Identifiers (CUIs) were categorized as\n\"Therapeutic or Preventive Procedure.\" Among the models applied to CIH named\nentity recognition, BLUEBERT delivered the highest macro average F1-score of\n0.90, surpassing other models. Conclusion: Our CIHLex significantly augments\nrepresentation of CIH approaches in biomedical literature. Demonstrating the\nutility of advanced NLP models, BERT notably excelled in CIH entity\nrecognition. These results highlight promising strategies for enhancing\nstandardization and recognition of CIH terminology in biomedical contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huixue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Austin_R/0/1/0/all/0/1\">Robin Austin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Sheng-Chieh Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silverman_G/0/1/0/all/0/1\">Greg Silverman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilicoglu_H/0/1/0/all/0/1\">Halil Kilicoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Personalization. (arXiv:2306.09299v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.09299","description":"<p>A hallmark property of explainable AI models is the ability to teach other\nagents, communicating knowledge of how to perform a task. While Large Language\nModels perform complex reasoning by generating explanations for their\npredictions, it is unclear whether they also make good teachers for weaker\nagents. To address this, we consider a student-teacher framework between two\nLLM agents and study if, when, and how the teacher should intervene with\nnatural language explanations to improve the student's performance. Since\ncommunication is expensive, we define a budget such that the teacher only\ncommunicates explanations for a fraction of the data, after which the student\nshould perform well on its own. We decompose the teaching problem along four\naxes: (1) if teacher's test time intervention improve student predictions, (2)\nwhen it is worth explaining a data point, (3) how the teacher should\npersonalize explanations to better teach the student, and (4) if teacher\nexplanations also improve students on future unexplained data. We first show\nthat teacher LLMs can indeed intervene on student reasoning to improve their\nperformance. Next, inspired by the Theory of Mind abilities of effective\nteachers, we propose building two few-shot mental models of the student. The\nfirst model defines an Intervention Function that simulates the utility of an\nintervention, allowing the teacher to intervene when this utility is the\nhighest and improving student performance at lower budgets. The second model\nenables the teacher to personalize explanations for a particular student and\noutperform unpersonalized teachers. We also demonstrate that in multi-turn\ninteractions, teacher explanations generalize and learning from explained data\nimproves student performance on future unexplained data. Finally, we verify\nthat misaligned teachers can lower student performance to random chance by\nintentionally misleading them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Swarnadeep Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hase_P/0/1/0/all/0/1\">Peter Hase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2307.06440","description":"<p>The computation necessary for training Transformer-based language models has\nskyrocketed in recent years. This trend has motivated research on efficient\ntraining algorithms designed to improve training, validation, and downstream\nperformance faster than standard training. In this work, we revisit three\ncategories of such algorithms: dynamic architectures (layer stacking, layer\ndropping), batch selection (selective backprop, RHO loss), and efficient\noptimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed\ncomputation budget using such methods, we find that their training, validation,\nand downstream gains vanish compared to a baseline with a fully-decayed\nlearning rate. We define an evaluation protocol that enables computation to be\ndone on arbitrary machines by mapping all computation time to a reference\nmachine which we call reference system time. We discuss the limitations of our\nproposed protocol and release our code to encourage rigorous research in\nefficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1\">Jean Kaddour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Key_O/0/1/0/all/0/1\">Oscar Key</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nawrot_P/0/1/0/all/0/1\">Piotr Nawrot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1\">Pasquale Minervini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1\">Matt J. Kusner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FinGPT: Democratizing Internet-scale Data for Financial Large Language Models. (arXiv:2307.10485v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.10485","description":"<p>Large language models (LLMs) have demonstrated remarkable proficiency in\nunderstanding and generating human-like texts, which may potentially\nrevolutionize the finance industry. However, existing LLMs often fall short in\nthe financial field, which is mainly attributed to the disparities between\ngeneral text data and financial text data. Unfortunately, there is only a\nlimited number of financial text datasets available, and BloombergGPT, the\nfirst financial LLM (FinLLM), is close-sourced (only the training logs were\nreleased). In light of this, we aim to democratize Internet-scale financial\ndata for LLMs, which is an open challenge due to diverse data sources, low\nsignal-to-noise ratio, and high time-validity. To address the challenges, we\nintroduce an open-sourced and data-centric framework, Financial Generative\nPre-trained Transformer (FinGPT), that automates the collection and curation of\nreal-time financial data from 34 diverse sources on the Internet, providing\nresearchers and practitioners with accessible and transparent resources to\ndevelop their FinLLMs. Additionally, we propose a simple yet effective strategy\nfor fine-tuning FinLLM using the inherent feedback from the market, dubbed\nReinforcement Learning with Stock Prices (RLSP). We also adopt the Low-rank\nAdaptation (LoRA, QLoRA) method that enables users to customize their own\nFinLLMs from general-purpose LLMs at a low cost. Finally, we showcase several\nFinGPT applications, including robo-advisor, sentiment analysis for algorithmic\ntrading, and low-code development. FinGPT aims to democratize FinLLMs,\nstimulate innovation, and unlock new opportunities in open finance. The codes\nhave been open-sourced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao-Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongyang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1\">Daochen Zha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic. (arXiv:2308.07336v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2308.07336","description":"<p>We study a synthetic corpus based approach for language models (LMs) to\nacquire logical deductive reasoning ability. The previous studies generated\ndeduction examples using specific sets of deduction rules. However, these rules\nwere limited or otherwise arbitrary, limiting the generalizability of acquired\nreasoning ability. We rethink this and adopt a well-grounded set of deduction\nrules based on formal logic theory, which can derive any other deduction rules\nwhen combined in a multistep way. Then, using the proposed corpora, which we\nname FLD (Formal Logic Deduction), we first evaluate and analyze the logical\nreasoning ability of the latest LLMs. Even GPT-4 can solve only half of the\nproblems, suggesting that pure logical reasoning isolated from knowledge is\nstill challenging for the LLMs, and additional training specialized in logical\nreasoning is indeed essential. We next empirically verify that LMs trained on\nFLD corpora acquire more generalizable reasoning ability. Furthermore, we\nidentify the aspects of reasoning ability on which deduction corpora can\nenhance LMs and those on which they cannot, and discuss future directions on\neach aspect. The released corpora serve both as learning resources and as\nchallenging benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Morishita_T/0/1/0/all/0/1\">Terufumi Morishita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morio_G/0/1/0/all/0/1\">Gaku Morio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamaguchi_A/0/1/0/all/0/1\">Atsuki Yamaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogawa_Y/0/1/0/all/0/1\">Yasuhiro Sogawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Transformer Dynamics as Movement through Embedding Space. (arXiv:2308.10874v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2308.10874","description":"<p>Transformer based language models exhibit intelligent behaviors such as\nunderstanding natural language, recognizing patterns, acquiring knowledge,\nreasoning, planning, reflecting and using tools. This paper explores how their\nunderlying mechanics give rise to intelligent behaviors. Towards that end, we\npropose framing Transformer dynamics as movement through embedding space.\nExamining Transformers through this perspective reveals key insights,\nestablishing a Theory of Transformers: 1) Intelligent behaviours map to paths\nin Embedding Space which, the Transformer random-walks through during\ninferencing. 2) LM training learns a probability distribution over all possible\npaths. `Intelligence' is learnt by assigning higher probabilities to paths\nrepresenting intelligent behaviors. No learning can take place in-context;\ncontext only narrows the subset of paths sampled during decoding. 5) The\nTransformer is a self-mapping composition function, folding a context sequence\ninto a context-vector such that it's proximity to a token-vector reflects its\nco-occurrence and conditioned probability. Thus, the physical arrangement of\nvectors in Embedding Space determines path probabilities. 6) Context vectors\nare composed by aggregating features of the sequence's tokens via a process we\ncall the encoding walk. Attention contributes a - potentially redundant -\nassociation-bias to this process. 7) This process is comprised of two principal\noperation types: filtering (data independent) and aggregation (data dependent).\nThis generalization unifies Transformers with other sequence models. Building\nupon this foundation, we formalize a popular semantic interpretation of\nembeddings into a ``concept-space theory'' and find some evidence of it's\nvalidity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sumeet S. Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Long-Context Scaling of Foundation Models. (arXiv:2309.16039v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.16039","description":"<p>We present a series of long-context LLMs that support effective context\nwindows of up to 32,768 tokens. Our model series are built through continual\npretraining from Llama 2 with longer training sequences and on a dataset where\nlong texts are upsampled. We perform extensive evaluation on language modeling,\nsynthetic context probing tasks, and a wide range of research benchmarks. On\nresearch benchmarks, our models achieve consistent improvements on most regular\ntasks and significant improvements on long-context tasks over Llama 2. Notably,\nwith a cost-effective instruction tuning procedure that does not require\nhuman-annotated long instruction data, the 70B variant can already surpass\ngpt-3.5-turbo-16k's overall performance on a suite of long-context tasks.\nAlongside these results, we provide an in-depth analysis on the individual\ncomponents of our method. We delve into Llama's position encodings and discuss\nits limitation in modeling long dependencies. We also examine the impact of\nvarious design choices in the pretraining process, including the data mix and\nthe training curriculum of sequence lengths -- our ablation experiments suggest\nthat having abundant long texts in the pretrain dataset is not the key to\nachieving strong performance, and we empirically verify that long context\ncontinual pretraining is more efficient and similarly effective compared to\npretraining from scratch with long sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Wenhan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molybog_I/0/1/0/all/0/1\">Igor Molybog</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hejia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhargava_P/0/1/0/all/0/1\">Prajjwal Bhargava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1\">Rui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1\">Louis Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rungta_R/0/1/0/all/0/1\">Rashi Rungta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankararaman_K/0/1/0/all/0/1\">Karthik Abinav Sankararaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas Oguz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1\">Madian Khabsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Han Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_K/0/1/0/all/0/1\">Kshitiz Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1\">Angela Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhosale_S/0/1/0/all/0/1\">Shruti Bhosale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edunov_S/0/1/0/all/0/1\">Sergey Edunov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sinong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hao Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud. (arXiv:2309.17157v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.17157","description":"<p>In the current user-server interaction paradigm of prompted generation with\nlarge language models (LLM) on cloud, the server fully controls the generation\nprocess, which leaves zero options for users who want to keep the generated\ntext to themselves. We propose LatticeGen, a cooperative framework in which the\nserver still handles most of the computation while the user controls the\nsampling operation. The key idea is that the true generated sequence is mixed\nwith noise tokens by the user and hidden in a noised lattice. Considering\npotential attacks from a hypothetically malicious server and how the user can\ndefend against it, we propose the repeated beam-search attack and the mixing\nnoise scheme. In our experiments we apply LatticeGen to protect both prompt and\ngeneration. It is shown that while the noised lattice degrades generation\nquality, LatticeGen successfully protects the true generation to a remarkable\ndegree under strong attacks (more than 50% of the semantic remains hidden as\nmeasured by BERTScore).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengke Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianxing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianle Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_L/0/1/0/all/0/1\">Lu Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Binyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empowering Many, Biasing a Few: Generalist Credit Scoring through Large Language Models. (arXiv:2310.00566v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2310.00566","description":"<p>In the financial industry, credit scoring is a fundamental element, shaping\naccess to credit and determining the terms of loans for individuals and\nbusinesses alike. Traditional credit scoring methods, however, often grapple\nwith challenges such as narrow knowledge scope and isolated evaluation of\ncredit tasks. Our work posits that Large Language Models (LLMs) have great\npotential for credit scoring tasks, with strong generalization ability across\nmultiple tasks. To systematically explore LLMs for credit scoring, we propose\nthe first open-source comprehensive framework. We curate a novel benchmark\ncovering 9 datasets with 14K samples, tailored for credit assessment and a\ncritical examination of potential biases within LLMs, and the novel instruction\ntuning data with over 45k samples. We then propose the first Credit and Risk\nAssessment Large Language Model (CALM) by instruction tuning, tailored to the\nnuanced demands of various financial risk assessment tasks. We evaluate CALM,\nand existing state-of-art (SOTA) open source and close source LLMs on the build\nbenchmark. Our empirical results illuminate the capability of LLMs to not only\nmatch but surpass conventional models, pointing towards a future where credit\nscoring can be more inclusive, comprehensive, and unbiased. We contribute to\nthe industry's transformation by sharing our pioneering instruction-tuning\ndatasets, credit and risk assessment LLM, and benchmarks with the research\ncommunity and the financial industry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1\">Duanyu Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yongfu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jimin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qianqian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Weiguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Lira_A/0/1/0/all/0/1\">Alejandro Lopez-Lira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ruffle&Riley: Towards the Automated Induction of Conversational Tutoring Systems. (arXiv:2310.01420v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.01420","description":"<p>Conversational tutoring systems (CTSs) offer learning experiences driven by\nnatural language interaction. They are known to promote high levels of\ncognitive engagement and benefit learning outcomes, particularly in reasoning\ntasks. Nonetheless, the time and cost required to author CTS content is a major\nobstacle to widespread adoption. In this paper, we introduce a novel type of\nCTS that leverages the recent advances in large language models (LLMs) in two\nways: First, the system induces a tutoring script automatically from a lesson\ntext. Second, the system automates the script orchestration via two LLM-based\nagents (Ruffle&amp;Riley) with the roles of a student and a professor in a\nlearning-by-teaching format. The system allows a free-form conversation that\nfollows the ITS-typical inner and outer loop structure. In an initial\nbetween-subject online user study (N = 100) comparing Ruffle&amp;Riley to simpler\nQA chatbots and reading activity, we found no significant differences in\npost-test scores. Nonetheless, in the learning experience survey, Ruffle&amp;Riley\nusers expressed higher ratings of understanding and remembering and further\nperceived the offered support as more helpful and the conversation as coherent.\nOur study provides insights for a new generation of scalable CTS technologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmucker_R/0/1/0/all/0/1\">Robin Schmucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1\">Meng Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azaria_A/0/1/0/all/0/1\">Amos Azaria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_T/0/1/0/all/0/1\">Tom Mitchell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.07793","description":"<p>The rapid advancements in large language models (LLMs) have ignited interest\nin the temporal knowledge graph (tKG) domain, where conventional carefully\ndesigned embedding-based and rule-based models dominate. The question remains\nopen of whether pre-trained LLMs can understand structured temporal relational\ndata and replace them as the foundation model for temporal relational\nforecasting. Therefore, we bring temporal knowledge forecasting into the\ngenerative setting. However, challenges occur in the huge chasms between\ncomplex temporal graph data structure and sequential natural expressions LLMs\ncan handle, and between the enormous data sizes of tKGs and heavy computation\ncosts of finetuning LLMs. To address these challenges, we propose a novel\nretrieval augmented generation framework that performs generative forecasting\non tKGs named GenTKG, which combines a temporal logical rule-based retrieval\nstrategy and lightweight parameter-efficient instruction tuning. Extensive\nexperiments have shown that GenTKG outperforms conventional methods of temporal\nrelational forecasting under low computation resources. GenTKG also highlights\nremarkable transferability with exceeding performance on unseen datasets\nwithout re-training. Our work reveals the huge potential of LLMs in the tKG\ndomain and opens a new frontier for generative forecasting on tKGs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1\">Ruotong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yunpu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1\">Volker Tresp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PreWoMe: Exploiting Presuppositions as Working Memory for Long Form Question Answering. (arXiv:2310.16147v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.16147","description":"<p>Information-seeking questions in long-form question answering (LFQA) often\nprove misleading due to ambiguity or false presupposition in the question.\nWhile many existing approaches handle misleading questions, they are tailored\nto limited questions, which are insufficient in a real-world setting with\nunpredictable input characteristics. In this work, we propose PreWoMe, a\nunified approach capable of handling any type of information-seeking question.\nThe key idea of PreWoMe involves extracting presuppositions in the question and\nexploiting them as working memory to generate feedback and action about the\nquestion. Our experiment shows that PreWoMe is effective not only in tackling\nmisleading questions but also in handling normal ones, thereby demonstrating\nthe effectiveness of leveraging presuppositions, feedback, and action for\nreal-world QA settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wookje Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinsol Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyungjae Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering. (arXiv:2310.17490v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.17490","description":"<p>Large language models (LLMs) enable zero-shot approaches in open-domain\nquestion answering (ODQA), yet with limited advancements as the reader is\ncompared to the retriever. This study aims at the feasibility of a zero-shot\nreader that addresses the challenges of computational cost and the need for\nlabeled data. We find that LLMs are distracted due to irrelevant documents in\nthe retrieved set and the overconfidence of the generated answers when they are\nexploited as zero-shot readers. To tackle these problems, we mitigate the\nimpact of such documents via Distraction-aware Answer Selection (DAS) with a\nnegation-based instruction and score adjustment for proper answer selection.\nExperimental results show that our approach successfully handles distraction\nacross diverse scenarios, enhancing the performance of zero-shot readers.\nFurthermore, unlike supervised readers struggling with unseen data, zero-shot\nreaders demonstrate outstanding transferability without any training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sukmin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Jeongyeon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1\">Soyeong Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jong C. Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Centric Financial Large Language Models. (arXiv:2310.17784v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.17784","description":"<p>Large language models (LLMs) show promise for natural language tasks but\nstruggle when applied directly to complex domains like finance. LLMs have\ndifficulty reasoning about and integrating all relevant information. We propose\na data-centric approach to enable LLMs to better handle financial tasks. Our\nkey insight is that rather than overloading the LLM with everything at once, it\nis more effective to preprocess and pre-understand the data. We create a\nfinancial LLM (FLLM) using multitask prompt-based finetuning to achieve data\npre-processing and pre-understanding. However, labeled data is scarce for each\ntask. To overcome manual annotation costs, we employ abductive augmentation\nreasoning (AAR) to automatically generate training data by modifying the pseudo\nlabels from FLLM's own outputs. Experiments show our data-centric FLLM with AAR\nsubstantially outperforms baseline financial LLMs designed for raw text,\nachieving state-of-the-art on financial analysis and interpretation tasks. We\nalso open source a new benchmark for financial analysis and interpretation. Our\nmethodology provides a promising path to unlock LLMs' potential for complex\nreal-world domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1\">Zhixuan Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Huaiyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xinyuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yijia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wanqing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1\">Qing Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Longfei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Segment-to-Segment Framework for Simultaneous Sequence Generation. (arXiv:2310.17940v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.17940","description":"<p>Simultaneous sequence generation is a pivotal task for real-time scenarios,\nsuch as streaming speech recognition, simultaneous machine translation and\nsimultaneous speech translation, where the target sequence is generated while\nreceiving the source sequence. The crux of achieving high-quality generation\nwith low latency lies in identifying the optimal moments for generating,\naccomplished by learning a mapping between the source and target sequences.\nHowever, existing methods often rely on task-specific heuristics for different\nsequence types, limiting the model's capacity to adaptively learn the\nsource-target mapping and hindering the exploration of multi-task learning for\nvarious simultaneous tasks. In this paper, we propose a unified\nsegment-to-segment framework (Seg2Seg) for simultaneous sequence generation,\nwhich learns the mapping in an adaptive and unified manner. During the process\nof simultaneous generation, the model alternates between waiting for a source\nsegment and generating a target segment, making the segment serve as the\nnatural bridge between the source and target. To accomplish this, Seg2Seg\nintroduces a latent segment as the pivot between source to target and explores\nall potential source-target mappings via the proposed expectation training,\nthereby learning the optimal moments for generating. Experiments on multiple\nsimultaneous generation tasks demonstrate that Seg2Seg achieves\nstate-of-the-art performance and exhibits better generality across various\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaolei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs. (arXiv:2310.19347v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.19347","description":"<p>Despite the recent progress in text summarization made by large language\nmodels (LLMs), they often generate summaries that are factually inconsistent\nwith original articles, known as \"hallucinations\" in text generation. Unlike\nprevious small models (e.g., BART, T5), current LLMs make fewer silly mistakes\nbut more sophisticated ones, such as imposing cause and effect, adding false\ndetails, overgeneralizing, etc. These hallucinations are challenging to detect\nthrough traditional methods, which poses great challenges for improving the\nfactual consistency of text summarization. In this paper, we propose an\nadversarially DEcoupling method to disentangle the Comprehension and\nEmbellishmeNT abilities of LLMs (DECENT). Furthermore, we adopt a probing-based\nefficient training to cover the shortage of sensitivity for true and false in\nthe training process of LLMs. In this way, LLMs are less confused about\nembellishing and understanding; thus, they can execute the instructions more\naccurately and have enhanced abilities to distinguish hallucinations.\nExperimental results show that DECENT significantly improves the reliability of\ntext summarization based on LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1\">Huawen Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Ting-En Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zekun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuchuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qianli Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models. (arXiv:2310.20410v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.20410","description":"<p>The ability to follow instructions is crucial for Large Language Models\n(LLMs) to handle various real-world applications. Existing benchmarks primarily\nfocus on evaluating pure response quality, rather than assessing whether the\nresponse follows constraints stated in the instruction. To fill this research\ngap, in this paper, we propose FollowBench, a Multi-level Fine-grained\nConstraints Following Benchmark for LLMs. FollowBench comprehensively includes\nfive different types (i.e., Content, Situation, Style, Format, and Example) of\nfine-grained constraints. To enable a precise constraint following estimation\non diverse difficulties, we introduce a Multi-level mechanism that\nincrementally adds a single constraint to the initial instruction at each\nincreased level. To assess whether LLMs' outputs have satisfied every\nindividual constraint, we propose to prompt strong LLMs with\nconstraint-evolution paths to handle challenging open-ended instructions. By\nevaluating ten closed-source and open-source popular LLMs on FollowBench, we\nhighlight the weaknesses of LLMs in instruction following and point towards\npotential avenues for future work. The data and code are publicly available at\nhttps://github.com/YJiangcm/FollowBench.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xingshan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wanjun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangyou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning From Mistakes Makes LLM Better Reasoner. (arXiv:2310.20689v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.20689","description":"<p>Large language models (LLMs) recently exhibited remarkable reasoning\ncapabilities on solving math problems. To further improve this capability, this\nwork proposes Learning from Mistakes (LeMa), akin to human learning processes.\nConsider a human student who failed to solve a math problem, he will learn from\nwhat mistake he has made and how to correct it. Mimicking this error-driven\nlearning process, LeMa fine-tunes LLMs on mistake-correction data pairs\ngenerated by GPT-4. Specifically, we first collect inaccurate reasoning paths\nfrom various LLMs and then employ GPT-4 as a \"corrector\" to (1) identify the\nmistake step, (2) explain the reason for the mistake, and (3) correct the\nmistake and generate the final answer. Experimental results demonstrate the\neffectiveness of LeMa: across five backbone LLMs and two mathematical reasoning\ntasks, LeMa consistently improves the performance compared with fine-tuning on\nCoT data alone. Impressively, LeMa can also benefit specialized LLMs such as\nWizardMath and MetaMath, achieving 85.4% pass@1 accuracy on GSM8K and 27.1% on\nMATH. This surpasses the SOTA performance achieved by non-execution open-source\nmodels on these challenging tasks. Our code, data and models will be publicly\navailable at https://github.com/microsoft/LEMA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1\">Shengnan An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zexiong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zeqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChipNeMo: Domain-Adapted LLMs for Chip Design. (arXiv:2311.00176v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.00176","description":"<p>ChipNeMo aims to explore the applications of large language models (LLMs) for\nindustrial chip design. Instead of directly deploying off-the-shelf commercial\nor open-source LLMs, we instead adopt the following domain adaptation\ntechniques: custom tokenizers, domain-adaptive continued pretraining,\nsupervised fine-tuning (SFT) with domain-specific instructions, and\ndomain-adapted retrieval models. We evaluate these methods on three selected\nLLM applications for chip design: an engineering assistant chatbot, EDA script\ngeneration, and bug summarization and analysis. Our results show that these\ndomain adaptation techniques enable significant LLM performance improvements\nover general-purpose base models across the three evaluated applications,\nenabling up to 5x model size reduction with similar or better performance on a\nrange of design tasks. Our findings also indicate that there's still room for\nimprovement between our current results and ideal outcomes. We believe that\nfurther investigation of domain-adapted LLM approaches will help close this gap\nin the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ene_T/0/1/0/all/0/1\">Teodor-Dumitru Ene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirby_R/0/1/0/all/0/1\">Robert Kirby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Chris Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinckney_N/0/1/0/all/0/1\">Nathaniel Pinckney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1\">Rongjian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alben_J/0/1/0/all/0/1\">Jonah Alben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_H/0/1/0/all/0/1\">Himyanshu Anand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1\">Sanmitra Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayraktaroglu_I/0/1/0/all/0/1\">Ismet Bayraktaroglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhaskaran_B/0/1/0/all/0/1\">Bonita Bhaskaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_A/0/1/0/all/0/1\">Arjun Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clay_S/0/1/0/all/0/1\">Sharon Clay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dally_B/0/1/0/all/0/1\">Bill Dally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_L/0/1/0/all/0/1\">Laura Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_P/0/1/0/all/0/1\">Parikshit Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhodhi_S/0/1/0/all/0/1\">Siddhanth Dhodhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halepete_S/0/1/0/all/0/1\">Sameer Halepete</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_E/0/1/0/all/0/1\">Eric Hill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jiashang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sumit Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khailany_B/0/1/0/all/0/1\">Brucek Khailany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunal_K/0/1/0/all/0/1\">Kishor Kunal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaowei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberman_S/0/1/0/all/0/1\">Stuart Oberman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omar_S/0/1/0/all/0/1\">Sujeet Omar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratty_S/0/1/0/all/0/1\">Sreedhar Pratty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raiman_J/0/1/0/all/0/1\">Jonathan Raiman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Ambar Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhengjiang Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hanfei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suthar_P/0/1/0/all/0/1\">Pratik P Suthar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tej_V/0/1/0/all/0/1\">Varun Tej</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaizhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Haoxing Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models. (arXiv:2311.04589v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.04589","description":"<p>Despite Multi-modal Large Language Models (MM-LLMs) have made exciting\nstrides recently, they are still struggling to efficiently model the\ninteractions among multi-modal inputs and the generation in non-textual\nmodalities. In this work, we propose TEAL (Tokenize and Embed ALl)}, an\napproach to treat the input from any modality as a token sequence and learn a\njoint embedding space for all modalities. Specifically, for the input from any\nmodality, TEAL first discretizes it into a token sequence with the\noff-the-shelf tokenizer and embeds the token sequence into a joint embedding\nspace with a learnable embedding matrix. MM-LLMs just need to predict the\nmulti-modal tokens autoregressively as the textual LLMs do. Finally, the\ncorresponding de-tokenizer is applied to generate the output in each modality\nbased on the predicted token sequence. With the joint embedding space, TEAL\nenables the frozen LLMs to perform both understanding and generation tasks\ninvolving non-textual modalities, such as image and audio. Thus, the textual\nLLM can just work as an interface and maintain its high performance in textual\nunderstanding and generation. Experiments show that TEAL achieves substantial\nimprovements in multi-modal understanding, and implements a simple scheme for\nmulti-modal generations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingxue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fake Alignment: Are LLMs Really Aligned Well?. (arXiv:2311.05915v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.05915","description":"<p>The growing awareness of safety concerns in large language models (LLMs) has\nsparked considerable interest in the evaluation of safety within current\nresearch endeavors. This study investigates an interesting issue pertaining to\nthe evaluation of LLMs, namely the substantial discrepancy in performance\nbetween multiple-choice questions and open-ended questions. Inspired by\nresearch on jailbreak attack patterns, we argue this is caused by mismatched\ngeneralization. That is, the LLM does not have a comprehensive understanding of\nthe complex concept of safety. Instead, it only remembers what to answer for\nopen-ended safety questions, which makes it unable to solve other forms of\nsafety tests. We refer to this phenomenon as fake alignment and construct a\ncomparative benchmark to empirically verify its existence in LLMs. Such fake\nalignment renders previous evaluation protocols unreliable. To address this, we\nintroduce the Fake alIgNment Evaluation (FINE) framework and two novel\nmetrics--Consistency Score (CS) and Consistent Safety Score (CSS), which\njointly assess two complementary forms of evaluation to quantify fake alignment\nand obtain corrected performance estimates. Applying FINE to 14 widely-used\nLLMs reveals several models with purported safety are poorly aligned in\npractice. Our work highlights potential limitations in prevailing alignment\nmethodologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yixu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1\">Yan Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kexin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1\">Chengqi Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xingjun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingchun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autoregressive Language Models For Estimating the Entropy of Epic EHR Audit Logs. (arXiv:2311.06401v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.06401","description":"<p>EHR audit logs are a highly granular stream of events that capture clinician\nactivities, and is a significant area of interest for research in\ncharacterizing clinician workflow on the electronic health record (EHR).\nExisting techniques to measure the complexity of workflow through EHR audit\nlogs (audit logs) involve time- or frequency-based cross-sectional aggregations\nthat are unable to capture the full complexity of a EHR session. We briefly\nevaluate the usage of transformer-based tabular language model (tabular LM) in\nmeasuring the entropy or disorderedness of action sequences within workflow and\nrelease the evaluated models publicly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Warner_B/0/1/0/all/0/1\">Benjamin C. Warner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannampallil_T/0/1/0/all/0/1\">Thomas Kannampallil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seunghwan Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DocGen: Generating Detailed Parameter Docstrings in Python. (arXiv:2311.06453v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2311.06453","description":"<p>Documentation debt hinders the effective utilization of open-source software.\nAlthough code summarization tools have been helpful for developers, most would\nprefer a detailed account of each parameter in a function rather than a\nhigh-level summary. However, generating such a summary is too intricate for a\nsingle generative model to produce reliably due to the lack of high-quality\ntraining data. Thus, we propose a multi-step approach that combines multiple\ntask-specific models, each adept at producing a specific section of a\ndocstring. The combination of these models ensures the inclusion of each\nsection in the final docstring. We compared the results from our approach with\nexisting generative models using both automatic metrics and a human-centred\nevaluation with 17 participating developers, which proves the superiority of\nour approach over existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venkatkrishna_V/0/1/0/all/0/1\">Vatsal Venkatkrishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagabushanam_D/0/1/0/all/0/1\">Durga Shree Nagabushanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_E/0/1/0/all/0/1\">Emmanuel Iko-Ojo Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidoni_M/0/1/0/all/0/1\">Melina Vidoni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Step by Step to Fairness: Attributing Societal Bias in Task-oriented Dialogue Systems. (arXiv:2311.06513v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.06513","description":"<p>Recent works have shown considerable improvements in task-oriented dialogue\n(TOD) systems by utilizing pretrained large language models (LLMs) in an\nend-to-end manner. However, the biased behavior of each component in a TOD\nsystem and the error propagation issue in the end-to-end framework can lead to\nseriously biased TOD responses. Existing works of fairness only focus on the\ntotal bias of a system. In this paper, we propose a diagnosis method to\nattribute bias to each component of a TOD system. With the proposed attribution\nmethod, we can gain a deeper understanding of the sources of bias.\nAdditionally, researchers can mitigate biased model behavior at a more granular\nlevel. We conduct experiments to attribute the TOD system's bias toward three\ndemographic axes: gender, age, and race. Experimental results show that the\nbias of a TOD system usually comes from the response generation model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hsuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rebecca Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1\">Chinnadhurai Sankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shayandeh_S/0/1/0/all/0/1\">Shahin Shayandeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shang-Tse Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bikel_D/0/1/0/all/0/1\">Daniel M. Bikel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Classification to Generation: Insights into Crosslingual Retrieval Augmented ICL. (arXiv:2311.06595v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.06595","description":"<p>The remarkable ability of Large Language Models (LLMs) to understand and\nfollow instructions has sometimes been limited by their in-context learning\n(ICL) performance in low-resource languages. To address this, we introduce a\nnovel approach that leverages cross-lingual retrieval-augmented in-context\nlearning (CREA-ICL). By extracting semantically similar prompts from\nhigh-resource languages, we aim to improve the zero-shot performance of\nmultilingual pre-trained language models (MPLMs) across diverse tasks. Though\nour approach yields steady improvements in classification tasks, it faces\nchallenges in generation tasks. Our evaluation offers insights into the\nperformance dynamics of retrieval-augmented in-context learning across both\nclassification and generation domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_E/0/1/0/all/0/1\">Ercong Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Sheng Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Effectiveness of ASR Representations in Real-world Noisy Speech Emotion Recognition. (arXiv:2311.07093v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2311.07093","description":"<p>This paper proposes an efficient attempt to noisy speech emotion recognition\n(NSER). Conventional NSER approaches have proven effective in mitigating the\nimpact of artificial noise sources, such as white Gaussian noise, but are\nlimited to non-stationary noises in real-world environments due to their\ncomplexity and uncertainty. To overcome this limitation, we introduce a new\nmethod for NSER by adopting the automatic speech recognition (ASR) model as a\nnoise-robust feature extractor to eliminate non-vocal information in noisy\nspeech. We first obtain intermediate layer information from the ASR model as a\nfeature representation for emotional speech and then apply this representation\nfor the downstream NSER task. Our experimental results show that 1) the\nproposed method achieves better NSER performance compared with the conventional\nnoise reduction method, 2) outperforms self-supervised learning approaches, and\n3) even outperforms text-based approaches using ASR transcription or the ground\ntruth transcription of noisy speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaohan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiajun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingfeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision. (arXiv:2311.07362v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.07362","description":"<p>Large multimodal models (LMMs) suffer from multimodal hallucination, where\nthey provide incorrect responses misaligned with the given visual information.\nRecent works have conjectured that one of the reasons behind multimodal\nhallucination might be due to the vision encoder failing to ground on the image\nproperly. To mitigate this issue, we propose a novel approach that leverages\nself-feedback as visual cues. Building on this approach, we introduce Volcano,\na multimodal self-feedback guided revision model. Volcano generates natural\nlanguage feedback to its initial response based on the provided visual\ninformation and utilizes this feedback to self-revise its initial response.\nVolcano effectively reduces multimodal hallucination and achieves\nstate-of-the-art on MMHal-Bench, POPE, and GAVIE. It also improves on general\nmultimodal abilities and outperforms previous models on MM-Vet and MMBench.\nThrough a qualitative analysis, we show that Volcano's feedback is properly\ngrounded on the image than the initial response. This indicates that Volcano\ncan provide itself with richer visual information, helping alleviate multimodal\nhallucination. We publicly release Volcano models of 7B and 13B sizes along\nwith the data and code at https://github.com/kaistAI/Volcano.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seongyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sue Hyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_Y/0/1/0/all/0/1\">Yongrae Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Multi-Pivot Ensembling with Massively Multilingual Machine Translation Models. (arXiv:2311.07439v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.07439","description":"<p>Massively multilingual machine translation models allow for the translation\nof a large number of languages with a single model, but have limited\nperformance on low- and very-low-resource translation directions. Pivoting via\nhigh-resource languages remains a strong strategy for low-resource directions,\nand in this paper we revisit ways of pivoting through multiple languages.\nPrevious work has used a simple averaging of probability distributions from\nmultiple paths, but we find that this performs worse than using a single pivot,\nand exacerbates the hallucination problem because the same hallucinations can\nbe probable across different paths. As an alternative, we propose MaxEns, a\ncombination strategy that is biased towards the most confident predictions,\nhypothesising that confident predictions are less prone to be hallucinations.\nWe evaluate different strategies on the FLORES benchmark for 20 low-resource\nlanguage directions, demonstrating that MaxEns improves translation quality for\nlow-resource languages while reducing hallucination in translations, compared\nto both direct translation and an averaging approach. On average, multi-pivot\nstrategies still lag behind using English as a single pivot language, raising\nthe question of how to identify the best pivoting strategy for a given\ntranslation direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadshahi_A/0/1/0/all/0/1\">Alireza Mohammadshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vamvas_J/0/1/0/all/0/1\">Jannis Vamvas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-11-14T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}