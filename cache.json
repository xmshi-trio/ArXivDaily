{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-12-15T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Deep Image Style Transfer from Freeform Text. (arXiv:2212.06868v1 [cs.CV])","link":"http://arxiv.org/abs/2212.06868","description":"<p>This paper creates a novel method of deep neural style transfer by generating\nstyle images from freeform user text input. The language model and style\ntransfer model form a seamless pipeline that can create output images with\nsimilar losses and improved quality when compared to baseline style transfer\nmethods. The language model returns a closely matching image given a style text\nand description input, which is then passed to the style transfer model with an\ninput content image to create a final output. A proof-of-concept tool is also\ndeveloped to integrate the models and demonstrate the effectiveness of deep\nimage style transfer from freeform text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santanam_T/0/1/0/all/0/1\">Tejas Santanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiangyue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhaodong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Envisioning a Human-AI collaborative system to transform policies into decision models. (arXiv:2212.06882v1 [cs.AI])","link":"http://arxiv.org/abs/2212.06882","description":"<p>Regulations govern many aspects of citizens' daily lives. Governments and\nbusinesses routinely automate these in the form of coded rules (e.g., to check\na citizen's eligibility for specific benefits). However, the path to automation\nis long and challenging. To address this, recent global initiatives for digital\ngovernment, proposing to simultaneously express policy in natural language for\nhuman consumption as well as computationally amenable rules or code, are\ngathering broad public-sector interest. We introduce the problem of\nsemi-automatically building decision models from eligibility policies for\nsocial services, and present an initial emerging approach to shorten the route\nfrom policy documents to executable, interpretable and standardised decision\nmodels using AI, NLP and Knowledge Graphs. Despite the many open domain\nchallenges, in this position paper we explore the enormous potential of AI to\nassist government agencies and policy experts in scaling the production of both\nhuman-readable and machine executable policy rules, while improving\ntransparency, interpretability, traceability and accountability of the decision\nmaking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lopez_V/0/1/0/all/0/1\">Vanessa Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picco_G/0/1/0/all/0/1\">Gabriele Picco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vejsbjerg_I/0/1/0/all/0/1\">Inge Vejsbjerg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1\">Thanh Lam Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yufang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sbodio_M/0/1/0/all/0/1\">Marco Luca Sbodio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segrave_Daly_J/0/1/0/all/0/1\">John Segrave-Daly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moga_D/0/1/0/all/0/1\">Denisa Moga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swords_S/0/1/0/all/0/1\">Sean Swords</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Miao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carroll_E/0/1/0/all/0/1\">Eoin Carroll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paraphrase Identification with Deep Learning: A Review of Datasets and Methods. (arXiv:2212.06933v1 [cs.CL])","link":"http://arxiv.org/abs/2212.06933","description":"<p>The rapid advancement of AI technology has made text generation tools like\nGPT-3 and ChatGPT increasingly accessible, scalable, and effective. This can\npose serious threat to the credibility of various forms of media if these\ntechnologies are used for plagiarism, including scientific literature and news\nsources. Despite the development of automated methods for paraphrase\nidentification, detecting this type of plagiarism remains a challenge due to\nthe disparate nature of the datasets on which these methods are trained. In\nthis study, we review traditional and current approaches to paraphrase\nidentification and propose a refined typology of paraphrases. We also\ninvestigate how this typology is represented in popular datasets and how\nunder-representation of certain types of paraphrases impacts detection\ncapabilities. Finally, we outline new directions for future research and\ndatasets in the pursuit of more effective paraphrase detection using AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chao Zhou</a> (Department of Computer Science, Syracuse University), <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_C/0/1/0/all/0/1\">Cheng Qiu</a> (School of Arts and Science, Vanderbilt University), <a href=\"http://arxiv.org/find/cs/1/au:+Acuna_D/0/1/0/all/0/1\">Daniel E. Acuna</a> (Department of Computer Science, University of Colorado at Boulder)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-trained Language Models can be Fully Zero-Shot Learners. (arXiv:2212.06950v1 [cs.CL])","link":"http://arxiv.org/abs/2212.06950","description":"<p>How can we extend a pre-trained model to many language understanding tasks,\nwithout labeled or additional unlabeled data? Pre-trained language models\n(PLMs) have been effective for a wide range of NLP tasks. However, existing\napproaches either require fine-tuning on downstream labeled datasets or\nmanually constructing proper prompts. In this paper, we propose nonparametric\nprompting PLM (NPPrompt) for fully zero-shot language understanding. Unlike\nprevious methods, NPPrompt uses only pre-trained language models and does not\nrequire any labeled data or additional raw corpus for further fine-tuning, nor\ndoes it rely on humans to construct a comprehensive set of prompt label words.\nWe evaluate NPPrompt against previous major few-shot and zero-shot learning\nmethods on diverse NLP tasks: including text classification, text entailment,\nsimilar text retrieval, and paraphrasing. Experimental results demonstrate that\nour NPPrompt outperforms the previous best fully zero-shot method by big\nmargins, with absolute gains of 12.8% in accuracy on text classification and\n18.9% on the GLUE benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xuandong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1\">Siqi Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiguo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Ming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Find Someone Who: Visual Commonsense Understanding in Human-Centric Grounding. (arXiv:2212.06971v1 [cs.CV])","link":"http://arxiv.org/abs/2212.06971","description":"<p>From a visual scene containing multiple people, human is able to distinguish\neach individual given the context descriptions about what happened before,\ntheir mental/physical states or intentions, etc. Above ability heavily relies\non human-centric commonsense knowledge and reasoning. For example, if asked to\nidentify the \"person who needs healing\" in an image, we need to first know that\nthey usually have injuries or suffering expressions, then find the\ncorresponding visual clues before finally grounding the person. We present a\nnew commonsense task, Human-centric Commonsense Grounding, that tests the\nmodels' ability to ground individuals given the context descriptions about what\nhappened before, and their mental/physical states or intentions. We further\ncreate a benchmark, HumanCog, a dataset with 130k grounded commonsensical\ndescriptions annotated on 67k images, covering diverse types of commonsense and\nvisual scenes. We set up a context-object-aware method as a strong baseline\nthat outperforms previous pre-trained and non-pretrained models. Further\nanalysis demonstrates that rich visual commonsense and powerful integration of\nmulti-modal commonsense are essential, which sheds light on future works. Data\nand code will be available https://github.com/Hxyou/HumanCog.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1\">Haoxuan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Rui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhecan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling Prosody Representations with Unsupervised Speech Reconstruction. (arXiv:2212.06972v1 [cs.SD])","link":"http://arxiv.org/abs/2212.06972","description":"<p>Human speech can be characterized by different components, including semantic\ncontent, speaker identity and prosodic information. Significant progress has\nbeen made in disentangling representations for semantic content and speaker\nidentity in Automatic Speech Recognition (ASR) and speaker verification tasks\nrespectively. However, it is still an open challenging research question to\nextract prosodic information because of the intrinsic association of different\nattributes, such as timbre and rhythm, and because of the need for unsupervised\ntraining schemes to achieve robust large-scale and speaker-independent ASR. The\naim of this paper is to address the disentanglement of emotional prosody from\nspeech based on unsupervised reconstruction. Specifically, we identify, design,\nimplement and integrate three crucial components in our proposed speech\nreconstruction model Prosody2Vec: (1) a unit encoder that transforms speech\nsignals into discrete units for semantic content, (2) a pretrained speaker\nverification model to generate speaker identity embeddings, and (3) a trainable\nprosody encoder to learn prosody representations. We first pretrain the\nProsody2Vec representations on unlabelled emotional speech corpora, then\nfine-tune the model on specific datasets to perform Speech Emotion Recognition\n(SER) and Emotional Voice Conversion (EVC) tasks. Both objective and subjective\nevaluations on the EVC task suggest that Prosody2Vec effectively captures\ngeneral prosodic features that can be smoothly transferred to other emotional\nspeech. In addition, our SER experiments on the IEMOCAP dataset reveal that the\nprosody features learned by Prosody2Vec are complementary and beneficial for\nthe performance of widely used speech pretraining models and surpass the\nstate-of-the-art methods when combining Prosody2Vec with HuBERT\nrepresentations. Some audio samples can be found on our demo website.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Leyuan Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Taihao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1\">Cornelius Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pekarek_Rosin_T/0/1/0/all/0/1\">Theresa Pekarek-Rosin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_F/0/1/0/all/0/1\">Fuji Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AsPOS: Assamese Part of Speech Tagger using Deep Learning Approach. (arXiv:2212.07043v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07043","description":"<p>Part of Speech (POS) tagging is crucial to Natural Language Processing (NLP).\nIt is a well-studied topic in several resource-rich languages. However, the\ndevelopment of computational linguistic resources is still in its infancy\ndespite the existence of numerous languages that are historically and literary\nrich. Assamese, an Indian scheduled language, spoken by more than 25 million\npeople, falls under this category. In this paper, we present a Deep Learning\n(DL)-based POS tagger for Assamese. The development process is divided into two\nstages. In the first phase, several pre-trained word embeddings are employed to\ntrain several tagging models. This allows us to evaluate the performance of the\nword embeddings in the POS tagging task. The top-performing model from the\nfirst phase is employed to annotate another set of new sentences. In the second\nphase, the model is trained further using the fresh dataset. Finally, we attain\na tagging accuracy of 86.52% in F1 score. The model may serve as a baseline for\nfurther study on DL-based Assamese POS tagging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Dhrubajyoti Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandi_S/0/1/0/all/0/1\">Sukumar Nandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarmah_P/0/1/0/all/0/1\">Priyankoo Sarmah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMSMix: Sense-Maintained Sentence Mixup for Word Sense Disambiguation. (arXiv:2212.07072v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07072","description":"<p>Word Sense Disambiguation (WSD) is an NLP task aimed at determining the\ncorrect sense of a word in a sentence from discrete sense choices. Although\ncurrent systems have attained unprecedented performances for such tasks, the\nnonuniform distribution of word senses during training generally results in\nsystems performing poorly on rare senses. To this end, we consider data\naugmentation to increase the frequency of these least frequent senses (LFS) to\nreduce the distributional bias of senses during training. We propose\nSense-Maintained Sentence Mixup (SMSMix), a novel word-level mixup method that\nmaintains the sense of a target word. SMSMix smoothly blends two sentences\nusing mask prediction while preserving the relevant span determined by saliency\nscores to maintain a specific word's sense. To the best of our knowledge, this\nis the first attempt to apply mixup in NLP while preserving the meaning of a\nspecific word. With extensive experiments, we validate that our augmentation\nmethod can effectively give more information about rare senses during training\nwith maintained target sense label.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1\">Hee Suk Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_E/0/1/0/all/0/1\">Eunseop Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harvill_J/0/1/0/all/0/1\">John Harvill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sunjae Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasegawa_Johnson_M/0/1/0/all/0/1\">Mark Hasegawa-Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1\">Chang D. Yoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modal Similarity-Based Curriculum Learning for Image Captioning. (arXiv:2212.07075v1 [cs.CV])","link":"http://arxiv.org/abs/2212.07075","description":"<p>Image captioning models require the high-level generalization ability to\ndescribe the contents of various images in words. Most existing approaches\ntreat the image-caption pairs equally in their training without considering the\ndifferences in their learning difficulties. Several image captioning approaches\nintroduce curriculum learning methods that present training data with\nincreasing levels of difficulty. However, their difficulty measurements are\neither based on domain-specific features or prior model training. In this\npaper, we propose a simple yet efficient difficulty measurement for image\ncaptioning using cross-modal similarity calculated by a pretrained\nvision-language model. Experiments on the COCO and Flickr30k datasets show that\nour proposed approach achieves superior performance and competitive convergence\nspeed to baselines without requiring heuristics or incurring additional\ntraining costs. Moreover, the higher model performance on difficult examples\nand unseen data also demonstrates the generalization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongkuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugawara_S/0/1/0/all/0/1\">Saku Sugawara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aizawa_A/0/1/0/all/0/1\">Akiko Aizawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Lei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasano_R/0/1/0/all/0/1\">Ryohei Sasano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1\">Koichi Takeda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialogQAE: N-to-N Question Answer Pair Extraction from Customer Service Chatlog. (arXiv:2212.07112v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07112","description":"<p>Harvesting question-answer (QA) pairs from customer service chatlog in the\nwild is an efficient way to enrich the knowledge base for customer service\nchatbots in the cold start or continuous integration scenarios. Prior work\nattempts to obtain 1-to-1 QA pairs from growing customer service chatlog, which\nfails to integrate the incomplete utterances from the dialog context for\ncomposite QA retrieval. In this paper, we propose N-to-N QA extraction task in\nwhich the derived questions and corresponding answers might be separated across\ndifferent utterances. We introduce a suite of generative/discriminative tagging\nbased methods with end-to-end and two-stage variants that perform well on 5\ncustomer service datasets and for the first time setup a benchmark for N-to-N\nDialogQAE with utterance and session level evaluation metrics. With a deep dive\ninto extracted QA pairs, we find that the relations between and inside the QA\npairs can be indicators to analyze the dialogue structure, e.g. information\nseeking, clarification, barge-in and elaboration. We also show that the\nproposed models can adapt to different domains and languages, and reduce the\nlabor cost of knowledge accumulation in the real-world product dialogue\nplatform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Haoran Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yufan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_M/0/1/0/all/0/1\">Mengliang Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Binghuai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainability of Text Processing and Retrieval Methods: A Critical Survey. (arXiv:2212.07126v1 [cs.IR])","link":"http://arxiv.org/abs/2212.07126","description":"<p>Deep Learning and Machine Learning based models have become extremely popular\nin text processing and information retrieval. However, the non-linear\nstructures present inside the networks make these models largely inscrutable. A\nsignificant body of research has focused on increasing the transparency of\nthese models. This article provides a broad overview of research on the\nexplainability and interpretability of natural language processing and\ninformation retrieval methods. More specifically, we survey approaches that\nhave been applied to explain word embeddings, sequence modeling, attention\nmodules, transformers, BERT, and document ranking. The concluding section\nsuggests some possible directions for future research on this topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sourav Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_D/0/1/0/all/0/1\">Debapriyo Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_M/0/1/0/all/0/1\">Mandar Mitra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards mapping the contemporary art world with ArtLM: an art-specific NLP model. (arXiv:2212.07127v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07127","description":"<p>With an increasing amount of data in the art world, discovering artists and\nartworks suitable to collectors' tastes becomes a challenge. It is no longer\nenough to use visual information, as contextual information about the artist\nhas become just as important in contemporary art. In this work, we present a\ngeneric Natural Language Processing framework (called ArtLM) to discover the\nconnections among contemporary artists based on their biographies. In this\napproach, we first continue to pre-train the existing general English language\nmodels with a large amount of unlabelled art-related data. We then fine-tune\nthis new pre-trained model with our biography pair dataset manually annotated\nby a team of professionals in the art industry. With extensive experiments, we\ndemonstrate that our ArtLM achieves 85.6% accuracy and 84.0% F1 score and\noutperforms other baseline models. We also provide a visualisation and a\nqualitative analysis of the artist network built from ArtLM's outputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qinkai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Mennaoui_M/0/1/0/all/0/1\">Mohamed El-Mennaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fosset_A/0/1/0/all/0/1\">Antoine Fosset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rebei_A/0/1/0/all/0/1\">Amine Rebei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Haoyang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OBeirne_C/0/1/0/all/0/1\">Christy E&#xf3;in O&#x27;Beirne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shevchenko_S/0/1/0/all/0/1\">Sasha Shevchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenbaum_M/0/1/0/all/0/1\">Mathieu Rosenbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MIST: a Large-Scale Annotated Resource and Neural Models for Functions of Modal Verbs in English Scientific Text. (arXiv:2212.07156v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07156","description":"<p>Modal verbs (e.g., \"can\", \"should\", or \"must\") occur highly frequently in\nscientific articles. Decoding their function is not straightforward: they are\noften used for hedging, but they may also denote abilities and restrictions.\nUnderstanding their meaning is important for various NLP tasks such as writing\nassistance or accurate information extraction from scientific text.\n</p>\n<p>To foster research on the usage of modals in this genre, we introduce the\nMIST (Modals In Scientific Text) dataset, which contains 3737 modal instances\nin five scientific domains annotated for their semantic, pragmatic, or\nrhetorical function. We systematically evaluate a set of competitive neural\narchitectures on MIST. Transfer experiments reveal that leveraging\nnon-scientific data is of limited benefit for modeling the distinctions in\nMIST. Our corpus analysis provides evidence that scientific communities differ\nin their usage of modal verbs, yet, classifiers trained on scientific data\ngeneralize to some extent to unseen scientific domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Henning_S/0/1/0/all/0/1\">Sophie Henning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macher_N/0/1/0/all/0/1\">Nicole Macher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grunewald_S/0/1/0/all/0/1\">Stefan Gr&#xfc;newald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_A/0/1/0/all/0/1\">Annemarie Friedrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-task Learning for Cross-Lingual Sentiment Analysis. (arXiv:2212.07160v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07160","description":"<p>This paper presents a cross-lingual sentiment analysis of news articles using\nzero-shot and few-shot learning. The study aims to classify the Croatian news\narticles with positive, negative, and neutral sentiments using the Slovene\ndataset. The system is based on a trilingual BERT-based model trained in three\nlanguages: English, Slovene, Croatian. The paper analyses different setups\nusing datasets in two languages and proposes a simple multi-task model to\nperform sentiment classification. The evaluation is performed using the\nfew-shot and zero-shot scenarios in single-task and multi-task experiments for\nCroatian and Slovene.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_G/0/1/0/all/0/1\">Gaurish Thakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preradovic_N/0/1/0/all/0/1\">Nives Mikelic Preradovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadic_M/0/1/0/all/0/1\">Marko Tadic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building and Evaluating Universal Named-Entity Recognition English corpus. (arXiv:2212.07162v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07162","description":"<p>This article presents the application of the Universal Named Entity framework\nto generate automatically annotated corpora. By using a workflow that extracts\nWikipedia data and meta-data and DBpedia information, we generated an English\ndataset which is described and evaluated. Furthermore, we conducted a set of\nexperiments to improve the annotations in terms of precision, recall, and\nF1-measure. The final dataset is available and the established workflow can be\napplied to any language with existing Wikipedia and DBpedia. As part of future\nresearch, we intend to continue improving the annotation process and extend it\nto other languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alves_D/0/1/0/all/0/1\">Diego Alves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_G/0/1/0/all/0/1\">Gaurish Thakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadic_M/0/1/0/all/0/1\">Marko Tadi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech and Natural Language Processing Technologies for Pseudo-Pilot Simulator. (arXiv:2212.07164v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07164","description":"<p>This paper describes a simple yet efficient repetition-based modular system\nfor speeding up air-traffic controllers (ATCos) training. E.g., a human pilot\nis still required in EUROCONTROL's ESCAPE lite simulator (see\nhttps://www.eurocontrol.int/simulator/escape) during ATCo training. However,\nthis need can be substituted by an automatic system that could act as a pilot.\nIn this paper, we aim to develop and integrate a pseudo-pilot agent into the\nATCo training pipeline by merging diverse artificial intelligence (AI) powered\nmodules. The system understands the voice communications issued by the ATCo,\nand, in turn, it generates a spoken prompt that follows the pilot's phraseology\nto the initial communication. Our system mainly relies on open-source AI tools\nand air traffic control (ATC) databases, thus, proving its simplicity and ease\nof replicability. The overall pipeline is composed of the following: (1) a\nsubmodule that receives and pre-processes the input stream of raw audio, (2) an\nautomatic speech recognition (ASR) system that transforms audio into a sequence\nof words; (3) a high-level ATC-related entity parser, which extracts relevant\ninformation from the communication, i.e., callsigns and commands, and finally,\n(4) a speech synthesizer submodule that generates responses based on the\nhigh-level ATC entities previously extracted. Overall, we show that this system\ncould pave the way toward developing a real proof-of-concept pseudo-pilot\nsystem. Hence, speeding up the training of ATCos while drastically reducing its\noverall cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1\">Amrutha Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1\">Juan Zuluaga-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarfjoo_S/0/1/0/all/0/1\">Saeed Sarfjoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nigmatulina_I/0/1/0/all/0/1\">Iuliia Nigmatulina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vesely_K/0/1/0/all/0/1\">Karel Vesely</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quotations, Coreference Resolution, and Sentiment Annotations in Croatian News Articles: An Exploratory Study. (arXiv:2212.07172v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07172","description":"<p>This paper presents a corpus annotated for the task of direct-speech\nextraction in Croatian. The paper focuses on the annotation of the quotation,\nco-reference resolution, and sentiment annotation in SETimes news corpus in\nCroatian and on the analysis of its language-specific differences compared to\nEnglish. From this, a list of the phenomena that require special attention when\nperforming these annotations is derived. The generated corpus with quotation\nfeatures annotations can be used for multiple tasks in the field of Natural\nLanguage Processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarajlic_J/0/1/0/all/0/1\">Jelena Sarajli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_G/0/1/0/all/0/1\">Gaurish Thakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alves_D/0/1/0/all/0/1\">Diego Alves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preradovic_N/0/1/0/all/0/1\">Nives Mikelic Preradovi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Negative Style Transfer in Hybrid Dialogue System. (arXiv:2212.07183v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07183","description":"<p>As the functionality of dialogue systems evolves, hybrid dialogue systems\nthat accomplish user-specific goals and participate in open-topic chitchat with\nusers are attracting growing attention. Existing research learns both tasks\nconcurrently utilizing a multi-task fusion technique but ignores the negative\ntransfer phenomenon induced by the unique textual style differences. Therefore,\ncontrastive learning based on the latent variable model is used to decouple the\nvarious textual genres in the latent space. We devise supervised and\nself-supervised positive and negative sample constructions for diverse\ndatasets. In addition, to capitalize on the style information contained in the\ndecoupled latent variables, we employ a style prefix that incorporates latent\nvariables further to control the generation of responses with varying styles.\nWe performed extensive experiments on three dialogue datasets, including a\nhybrid dialogue dataset and two task-oriented dialogue datasets. The\nexperimental results demonstrate that our method can mitigate the negative\nstyle transfer issue and achieves state-of-the-art performance on multiple\ndialogue datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shimin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1\">Qinyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VTCC-NLP at NL4Opt competition subtask 1: An Ensemble Pre-trained language models for Named Entity Recognition. (arXiv:2212.07219v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07219","description":"<p>We propose a combined three pre-trained language models (XLM-R, BART, and\nDeBERTa-V3) as an empower of contextualized embedding for named entity\nrecognition. Our model achieves a 92.9% F1 score on the test set and ranks 5th\non the leaderboard at NL4Opt competition subtask 1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doan_X/0/1/0/all/0/1\">Xuan-Dung Doan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Translationese in Cross-Lingual Summarization. (arXiv:2212.07220v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07220","description":"<p>Given a document in a source language, cross-lingual summarization (CLS) aims\nat generating a concise summary in a different target language. Unlike\nmonolingual summarization (MS), naturally occurring source-language documents\npaired with target-language summaries are rare. To collect large-scale CLS\nsamples, existing datasets typically involve translation in their creation.\nHowever, the translated text is distinguished from the text originally written\nin that language, i.e., translationese. Though many efforts have been devoted\nto CLS, none of them notice the phenomenon of translationese. In this paper, we\nfirst confirm that the different approaches to constructing CLS datasets will\nlead to different degrees of translationese. Then we design systematic\nexperiments to investigate how translationese affects CLS model evaluation and\nperformance when it appears in source documents or target summaries. In detail,\nwe find that (1) the translationese in documents or summaries of test sets\nmight lead to the discrepancy between human judgment and automatic evaluation;\n(2) the translationese in training sets would harm model performance in the\nreal scene; (3) though machine-translated documents involve translationese,\nthey are very useful for building CLS systems on low-resource languages under\nspecific training strategies. Furthermore, we give suggestions for future CLS\nresearch including dataset and model developments. We hope that our work could\nlet researchers notice the phenomenon of translationese in CLS and take it into\naccount in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tingyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiarong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Byte and Wordpiece Level Models for Massively Multilingual Semantic Parsing. (arXiv:2212.07223v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07223","description":"<p>Token free approaches have been successfully applied to a series of word and\nspan level tasks. In this work, we compare a byte-level (ByT5) and a wordpiece\nbased (mT5) sequence to sequence model on the 51 languages of the MASSIVE\nmultilingual semantic parsing dataset. We examine multiple experimental\nsettings: (i) zero-shot, (ii) full gold data and (iii) zero-shot with synthetic\ndata. By leveraging a state-of-the-art label projection method for machine\ntranslated examples, we are able to reduce the gap in exact match accuracy to\nonly 5 points with respect to a model trained on gold data from all the\nlanguages. We additionally provide insights on the cross-lingual transfer of\nByT5 and show how the model compares with respect to mT5 across all parameter\nsizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nicosia_M/0/1/0/all/0/1\">Massimo Nicosia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccinno_F/0/1/0/all/0/1\">Francesco Piccinno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"APOLLO: An Optimized Training Approach for Long-form Numerical Reasoning. (arXiv:2212.07249v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07249","description":"<p>Long-form numerical reasoning in financial analysis aims to generate a\nreasoning program to calculate the correct answer for a given question.\nPrevious work followed a retriever-generator framework, where the retriever\nselects key facts from a long-form document, and the generator generates a\nreasoning program based on retrieved facts. However, they treated all facts\nequally without considering the different contributions of facts with and\nwithout numbers. Meanwhile, the program consistency were ignored under\nsupervised training, resulting in lower training accuracy and diversity. To\nsolve these problems, we proposed APOLLO to improve the long-form numerical\nreasoning framework. For the retriever, we adopt a number-aware negative\nsampling strategy to enable the retriever to be more discriminative on key\nnumerical facts. For the generator, we design consistency-based reinforcement\nlearning and target program augmentation strategy based on the consistency of\nprogram execution results. Experimental results on the FinQA and ConvFinQA\nleaderboard verify the effectiveness of our proposed method, achieving the new\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiashuo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jian Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling. (arXiv:2212.07284v1 [cs.CL])","link":"http://arxiv.org/abs/2212.07284","description":"<p>Static subword tokenization algorithms have been an essential component of\nrecent works on language modeling. However, their static nature results in\nimportant flaws that degrade the models' downstream performance and robustness.\nIn this work, we propose MANTa, a Module for Adaptive Neural TokenizAtion.\nMANTa is a differentiable tokenizer trained end-to-end with the language model.\nThe resulting system offers a trade-off between the expressiveness of\nbyte-level models and the speed of models trained using subword tokenization.\nIn addition, our tokenizer is highly explainable since it produces an explicit\nsegmentation of sequences into blocks. We evaluate our pre-trained model on\nseveral English datasets from different domains as well as on synthetic noise.\nWe find that MANTa improves robustness to character perturbations and\nout-of-domain data. We then show that MANTa performs comparably to other models\non the general-domain GLUE benchmark. Finally, we show that it is considerably\nfaster than strictly byte-level models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Godey_N/0/1/0/all/0/1\">Nathan Godey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castagne_R/0/1/0/all/0/1\">Roman Castagn&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clergerie_E/0/1/0/all/0/1\">&#xc9;ric de la Clergerie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Beno&#xee;t Sagot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Emotion-Cause Pair Extraction via Learning to Link. (arXiv:2002.10710v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2002.10710","description":"<p>Emotion-cause pair extraction (ECPE), as an emergent natural language\nprocessing task, aims at jointly investigating emotions and their underlying\ncauses in documents. It extends the previous emotion cause extraction (ECE)\ntask, yet without requiring a set of pre-given emotion clauses as in ECE.\nExisting approaches to ECPE generally adopt a two-stage method, i.e., (1)\nemotion and cause detection, and then (2) pairing the detected emotions and\ncauses. Such pipeline method, while intuitive, suffers from two critical\nissues, including error propagation across stages that may hinder the\neffectiveness, and high computational cost that would limit the practical\napplication of the method. To tackle these issues, we propose a multi-task\nlearning model that can extract emotions, causes and emotion-cause pairs\nsimultaneously in an end-to-end manner. Specifically, our model regards pair\nextraction as a link prediction task, and learns to link from emotion clauses\nto cause clauses, i.e., the links are directional. Emotion extraction and cause\nextraction are incorporated into the model as auxiliary tasks, which further\nboost the pair extraction. Experiments are conducted on an ECPE benchmarking\ndataset. The results show that our proposed model outperforms a range of\nstate-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Haolin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiuchi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawei Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-Scale Chemical Language Representations Capture Molecular Structure and Properties. (arXiv:2106.09553v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.09553","description":"<p>Models based on machine learning can enable accurate and fast molecular\nproperty predictions, which is of interest in drug discovery and material\ndesign. Various supervised machine learning models have demonstrated promising\nperformance, but the vast chemical space and the limited availability of\nproperty labels make supervised learning challenging. Recently, unsupervised\ntransformer-based language models pretrained on a large unlabelled corpus have\nproduced state-of-the-art results in many downstream natural language\nprocessing tasks. Inspired by this development, we present molecular embeddings\nobtained by training an efficient transformer encoder model, MoLFormer, which\nuses rotary positional embeddings. This model employs a linear attention\nmechanism, coupled with highly distributed training, on SMILES sequences of 1.1\nbillion unlabelled molecules from the PubChem and ZINC datasets. We show that\nthe learned molecular representation outperforms existing baselines, including\nsupervised and self-supervised graph neural networks and language models, on\nseveral downstream tasks from ten benchmark datasets. They perform\ncompetitively on two others. Further analyses, specifically through the lens of\nattention, demonstrate that MoLFormer trained on chemical SMILES indeed learns\nthe spatial relationships between atoms within a molecule. These results\nprovide encouraging evidence that large-scale molecular language models can\ncapture sufficient chemical and structural information to predict various\ndistinct molecular properties, including quantum-chemical properties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ross_J/0/1/0/all/0/1\">Jerret Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belgodere_B/0/1/0/all/0/1\">Brian Belgodere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chenthamarakshan_V/0/1/0/all/0/1\">Vijil Chenthamarakshan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padhi_I/0/1/0/all/0/1\">Inkit Padhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mroueh_Y/0/1/0/all/0/1\">Youssef Mroueh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Payel Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grammar Based Speaker Role Identification for Air Traffic Control Speech Recognition. (arXiv:2108.12175v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12175","description":"<p>Automatic Speech Recognition (ASR) for air traffic control is generally\ntrained by pooling Air Traffic Controller (ATCO) and pilot data into one set.\nThis is motivated by the fact that pilot's voice communications are more scarce\nthan ATCOs. Due to this data imbalance and other reasons (e.g., varying\nacoustic conditions), the speech from ATCOs is usually recognized more\naccurately than from pilots. Automatically identifying the speaker roles is a\nchallenging task, especially in the case of the noisy voice recordings\ncollected using Very High Frequency (VHF) receivers or due to the\nunavailability of the push-to-talk (PTT) signal, i.e., both audio channels are\nmixed. In this work, we propose to (1) automatically segment the ATCO and pilot\ndata based on an intuitive approach exploiting ASR transcripts and (2)\nsubsequently consider an automatic recognition of ATCOs' and pilots' voice as\ntwo separate tasks. Our work is performed on VHF audio data with high noise\nlevels, i.e., signal-to-noise (SNR) ratios below 15 dB, as this data is\nrecognized to be helpful for various speech-based machine-learning tasks.\nSpecifically, for the speaker role identification task, the module is\nrepresented by a simple yet efficient knowledge-based system exploiting a\ngrammar defined by the International Civil Aviation Organization (ICAO). The\nsystem accepts text as the input, either manually verified annotations or\nautomatically generated transcripts. The developed approach provides an average\naccuracy in speaker role identification of about 83%. Finally, we show that\ntraining an acoustic model for ASR tasks separately (i.e., separate models for\nATCOs and pilots) or using a multitask approach is well suited for the noisy\ndata and outperforms the traditional ASR system where all data is pooled\ntogether.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1\">Amrutha Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1\">Juan Zuluaga-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarfjoo_S/0/1/0/all/0/1\">Saeed Sarfjoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nigmatulina_I/0/1/0/all/0/1\">Iuliia Nigmatulina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohneiser_O/0/1/0/all/0/1\">Oliver Ohneiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helmke_H/0/1/0/all/0/1\">Hartmut Helmke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STEREO: Scientific Text Reuse in Open Access Publications. (arXiv:2112.11800v3 [cs.DL] UPDATED)","link":"http://arxiv.org/abs/2112.11800","description":"<p>We present the Webis-STEREO-21 dataset, a massive collection of Scientific\nText Reuse in Open-access publications. It contains more than 91 million cases\nof reused text passages found in 4.2 million unique open-access publications.\nFeaturing a high coverage of scientific disciplines and varieties of reuse, as\nwell as comprehensive metadata to contextualize each case, our dataset\naddresses the most salient shortcomings of previous ones on scientific writing.\nWebis-STEREO-21 allows for tackling a wide range of research questions from\ndifferent scientific backgrounds, facilitating both qualitative and\nquantitative analysis of the phenomenon as well as a first-time grounding on\nthe base rate of text reuse in scientific publications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gienapp_L/0/1/0/all/0/1\">Lukas Gienapp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kircheis_W/0/1/0/all/0/1\">Wolfgang Kircheis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sievers_B/0/1/0/all/0/1\">Bjarne Sievers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_B/0/1/0/all/0/1\">Benno Stein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Multi-Granularity Summarization. (arXiv:2201.12502v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.12502","description":"<p>Text summarization is a user-preference based task, i.e., for one document,\nusers often have different priorities for summary. As a key aspect of\ncustomization in summarization, granularity is used to measure the semantic\ncoverage between the summary and source document. However, developing systems\nthat can generate summaries with customizable semantic coverage is still an\nunder-explored topic. In this paper, we propose the first unsupervised\nmulti-granularity summarization framework, GranuSum. We take events as the\nbasic semantic units of the source documents and propose to rank these events\nby their salience. We also develop a model to summarize input documents with\ngiven events as anchors and hints. By inputting different numbers of events,\nGranuSum is capable of producing multi-granular summaries in an unsupervised\nmanner. Meanwhile, we annotate a new benchmark GranuDUC that contains multiple\nsummaries at different granularities for each document cluster. Experimental\nresults confirm the substantial superiority of GranuSum on multi-granularity\nsummarization over strong baselines. Further, by exploiting the event\ninformation, GranuSum also exhibits state-of-the-art performance under the\nconventional unsupervised abstractive setting. Dataset for this paper can be\nfound at: https://github.com/maszhongming/GranuDUC\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Suyu Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yizhu Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FREDA: Flexible Relation Extraction Data Annotation. (arXiv:2204.07150v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07150","description":"<p>To effectively train accurate Relation Extraction models, sufficient and\nproperly labeled data is required. Adequately labeled data is difficult to\nobtain and annotating such data is a tricky undertaking. Previous works have\nshown that either accuracy has to be sacrificed or the task is extremely\ntime-consuming, if done accurately. We are proposing an approach in order to\nproduce high-quality datasets for the task of Relation Extraction quickly.\nNeural models, trained to do Relation Extraction on the created datasets,\nachieve very good results and generalize well to other datasets. In our study,\nwe were able to annotate 10,022 sentences for 19 relations in a reasonable\namount of time, and trained a commonly used baseline model for each relation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Strobl_M/0/1/0/all/0/1\">Michael Strobl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trabelsi_A/0/1/0/all/0/1\">Amine Trabelsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar Zaiane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DR.BENCH: Diagnostic Reasoning Benchmark for Clinical Natural Language Processing. (arXiv:2209.14901v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.14901","description":"<p>The meaningful use of electronic health records (EHR) continues to progress\nin the digital era with clinical decision support systems augmented by\nartificial intelligence. A priority in improving provider experience is to\novercome information overload and reduce the cognitive burden so fewer medical\nerrors and cognitive biases are introduced during patient care. One major type\nof medical error is diagnostic error due to systematic or predictable errors in\njudgment that rely on heuristics. The potential for clinical natural language\nprocessing (cNLP) to model diagnostic reasoning in humans with forward\nreasoning from data to diagnosis and potentially reduce the cognitive burden\nand medical error has not been investigated. Existing tasks to advance the\nscience in cNLP have largely focused on information extraction and named entity\nrecognition through classification tasks. We introduce a novel suite of tasks\ncoined as Diagnostic Reasoning Benchmarks, DR.BENCH, as a new benchmark for\ndeveloping and evaluating cNLP models with clinical diagnostic reasoning\nability. The suite includes six tasks from ten publicly available datasets\naddressing clinical text understanding, medical knowledge reasoning, and\ndiagnosis generation. DR.BENCH is the first clinical suite of tasks designed to\nbe a natural language generation framework to evaluate pre-trained language\nmodels. Experiments with state-of-the-art pre-trained generative language\nmodels using large general domain models and models that were continually\ntrained on a medical corpus demonstrate opportunities for improvement when\nevaluated in DR. BENCH. We share DR. BENCH as a publicly available GitLab\nrepository with a systematic approach to load and evaluate models for the cNLP\ncommunity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yanjun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dligach_D/0/1/0/all/0/1\">Dmitriy Dligach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_T/0/1/0/all/0/1\">Timothy Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caskey_J/0/1/0/all/0/1\">John Caskey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_B/0/1/0/all/0/1\">Brihat Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Churpek_M/0/1/0/all/0/1\">Matthew M Churpek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afshar_M/0/1/0/all/0/1\">Majid Afshar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MCP: Self-supervised Pre-training for Personalized Chatbots with Multi-level Contrastive Sampling. (arXiv:2210.08753v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08753","description":"<p>Personalized chatbots focus on endowing the chatbots with a consistent\npersonality to behave like real users and further act as personal assistants.\nPrevious studies have explored generating implicit user profiles from the\nuser's dialogue history for building personalized chatbots. However, these\nstudies only use the response generation loss to train the entire model, thus\nit is prone to suffer from the problem of data sparsity. Besides, they\noveremphasize the final generated response's quality while ignoring the\ncorrelations and fusions between the user's dialogue history, leading to rough\ndata representations and performance degradation. To tackle these problems, we\npropose a self-supervised learning framework MCP for capturing better\nrepresentations from users' dialogue history for personalized chatbots.\nSpecifically, we apply contrastive sampling methods to leverage the supervised\nsignals hidden in user dialog history, and generate the pre-training samples\nfor enhancing the model. We design three pre-training tasks based on three\ntypes of contrastive pairs from user dialogue history, namely response pairs,\nsequence augmentation pairs, and user pairs. We pre-train the utterance encoder\nand the history encoder towards the contrastive objectives and use these\npre-trained encoders for generating user profiles while personalized response\ngeneration. Experimental results on two real-world datasets show a significant\nimprovement in our proposed model MCP compared with the existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhaoheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yutao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhengyi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event-Centric Question Answering via Contrastive Learning and Invertible Event Transformation. (arXiv:2210.12902v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12902","description":"<p>Human reading comprehension often requires reasoning of event semantic\nrelations in narratives, represented by Event-centric Question-Answering (QA).\nTo address event-centric QA, we propose a novel QA model with contrastive\nlearning and invertible event transformation, call TranCLR. Our proposed model\nutilizes an invertible transformation matrix to project semantic vectors of\nevents into a common event embedding space, trained with contrastive learning,\nand thus naturally inject event semantic knowledge into mainstream QA\npipelines. The transformation matrix is fine-tuned with the annotated event\nrelation types between events that occurred in questions and those in answers,\nusing event-aware question vectors. Experimental results on the Event Semantic\nRelation Reasoning (ESTER) dataset show significant improvements in both\ngenerative and extractive settings compared to the existing strong baselines,\nachieving over 8.4% gain in the token-level F1 score and 3.0% gain in Exact\nMatch (EM) score under the multi-answer setting. Qualitative analysis reveals\nthe high quality of the generated answers by TranCLR, demonstrating the\nfeasibility of injecting event knowledge into QA model learning. Our code and\nmodels can be found at https://github.com/LuJunru/TranCLR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Junru Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xingwei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pergola_G/0/1/0/all/0/1\">Gabriele Pergola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1\">Lin Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Composition, Attention, or Both?. (arXiv:2210.12958v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12958","description":"<p>In this paper, we propose a novel architecture called Composition Attention\nGrammars (CAGs) that recursively compose subtrees into a single vector\nrepresentation with a composition function, and selectively attend to previous\nstructural information with a self-attention mechanism. We investigate whether\nthese components -- the composition function and the self-attention mechanism\n-- can both induce human-like syntactic generalization. Specifically, we train\nlanguage models (LMs) with and without these two components with the model\nsizes carefully controlled, and evaluate their syntactic generalization\nperformance against six test circuits on the SyntaxGym benchmark. The results\ndemonstrated that the composition function and the self-attention mechanism\nboth play an important role to make LMs more human-like, and closer inspection\nof linguistic phenomenon implied that the composition function allowed\nsyntactic features, but not semantic features, to percolate into subtree\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoshida_R/0/1/0/all/0/1\">Ryo Yoshida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oseki_Y/0/1/0/all/0/1\">Yohei Oseki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Associations Between Natural Language Processing (NLP) Enriched Social Determinants of Health and Suicide Death among US Veterans. (arXiv:2212.05546v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.05546","description":"<p>Importance: Social determinants of health (SDOH) are known to be associated\nwith increased risk of suicidal behaviors, but few studies utilized SDOH from\nunstructured electronic health record (EHR) notes.\n</p>\n<p>Objective: To investigate associations between suicide and recent SDOH,\nidentified using structured and unstructured data.\n</p>\n<p>Design: Nested case-control study.\n</p>\n<p>Setting: EHR data from the US Veterans Health Administration (VHA).\n</p>\n<p>Participants: 6,122,785 Veterans who received care in the US VHA between\nOctober 1, 2010, and September 30, 2015.\n</p>\n<p>Exposures: Occurrence of SDOH over a maximum span of two years compared with\nno occurrence of SDOH.\n</p>\n<p>Main Outcomes and Measures: Cases of suicide deaths were matched with 4\ncontrols on birth year, cohort entry date, sex, and duration of follow-up. We\ndeveloped an NLP system to extract SDOH from unstructured notes. Structured\ndata, NLP on unstructured data, and combining them yielded seven, eight and\nnine SDOH respectively. Adjusted odds ratios (aORs) and 95% confidence\nintervals (CIs) were estimated using conditional logistic regression.\n</p>\n<p>Results: In our cohort, 8,821 Veterans committed suicide during 23,725,382\nperson-years of follow-up (incidence rate 37.18 /100,000 person-years). Our\ncohort was mostly male (92.23%) and white (76.99%). Across the six common SDOH\nas covariates, NLP-extracted SDOH, on average, covered 84.38% of all SDOH\noccurrences. All SDOH, measured by structured data and NLP, were significantly\nassociated with increased risk of suicide. The SDOH with the largest effects\nwas legal problems (aOR=2.67, 95% CI=2.46-2.89), followed by violence\n(aOR=2.26, 95% CI=2.11-2.43). NLP-extracted and structured SDOH were also\nassociated with suicide.\n</p>\n<p>Conclusions and Relevance: NLP-extracted SDOH were always significantly\nassociated with increased risk of suicide among Veterans, suggesting the\npotential of NLP in public health studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1\">Avijit Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pradhan_R/0/1/0/all/0/1\">Richeek Pradhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melamed_R/0/1/0/all/0/1\">Rachel D Melamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoaglin_D/0/1/0/all/0/1\">David C Hoaglin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tucker_K/0/1/0/all/0/1\">Katherine L Tucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reisman_J/0/1/0/all/0/1\">Joel I Reisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weisong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_J/0/1/0/all/0/1\">Jack Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Technical Report -- Competition Solution for Prompt Tuning using Pretrained Language Model. (arXiv:2212.06369v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.06369","description":"<p>Prompt tuning recently becomes a hot-spot in the applications of large\npretrained language models on specific downstream tasks. Regarding the Language\nModel as a Service (LMaaS), black-box tuning using derivative-free optimization\n(DFO) provides a novel approach to expand the practical scenarios of pretrained\nmodels and enrich the researches of few-shot learning. In this report, we\npresent our solution in this competition that is based on the LMaaS scenario.\nOur solution consists of several modifications to BBTv2, including multiple\nlabel words, selection of P0, rolling update strategy, multi-task loss from MLP\nclassifier, and finally using the ensemble method to further improve\ngeneralization ability. We also shared some strategies that we tried but didn't\nuse in the final submission for further discussion. In the end we raised a\nquestion about the SNLI dataset and the impact on the results, as well as our\nconcerns about the competition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiang-Long Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1\">Wu-He Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Feng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiao-Lei Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Text-based Personality Computing: Challenges and Future Directions. (arXiv:2212.06711v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.06711","description":"<p>Text-based personality computing (TPC) has gained many research interests in\nNLP. In this paper, we describe 15 challenges that we consider deserving the\nattention of the research community. These challenges are organized by the\nfollowing topics: personality taxonomies, measurement quality, datasets,\nperformance evaluation, modelling choices, as well as ethics and fairness. When\naddressing each challenge, not only do we combine perspectives from both NLP\nand social sciences, but also offer concrete suggestions towards more valid and\nreliable TPC research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qixiang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giachanou_A/0/1/0/all/0/1\">Anastasia Giachanou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagheri_A/0/1/0/all/0/1\">Ayoub Bagheri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boeschoten_L/0/1/0/all/0/1\">Laura Boeschoten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kesteren_E/0/1/0/all/0/1\">Erik-Jan van Kesteren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamalabad_M/0/1/0/all/0/1\">Mahdi Shafiee Kamalabad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberski_D/0/1/0/all/0/1\">Daniel L Oberski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-12-14T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}