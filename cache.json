{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-02-23T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"GLUECons: A Generic Benchmark for Learning Under Constraints. (arXiv:2302.10914v1 [cs.LG])","link":"http://arxiv.org/abs/2302.10914","description":"<p>Recent research has shown that integrating domain knowledge into deep\nlearning architectures is effective -- it helps reduce the amount of required\ndata, improves the accuracy of the models' decisions, and improves the\ninterpretability of models. However, the research community is missing a\nconvened benchmark for systematically evaluating knowledge integration methods.\nIn this work, we create a benchmark that is a collection of nine tasks in the\ndomains of natural language processing and computer vision. In all cases, we\nmodel external knowledge as constraints, specify the sources of the constraints\nfor each task, and implement various models that use these constraints. We\nreport the results of these models using a new set of extended evaluation\ncriteria in addition to the task performances for a more in-depth analysis.\nThis effort provides a framework for a more comprehensive and systematic\ncomparison of constraint integration techniques and for identifying related\nresearch challenges. It will facilitate further research for alleviating some\nproblems of state-of-the-art neural models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faghihi_H/0/1/0/all/0/1\">Hossein Rajaby Faghihi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nafar_A/0/1/0/all/0/1\">Aliakbar Nafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirzaee_R/0/1/0/all/0/1\">Roshanak Mirzaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uszok_A/0/1/0/all/0/1\">Andrzej Uszok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_A/0/1/0/all/0/1\">Alexander Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Premsri_T/0/1/0/all/0/1\">Tanawan Premsri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kordjamshidi_P/0/1/0/all/0/1\">Parisa Kordjamshidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conformers are All You Need for Visual Speech Recogntion. (arXiv:2302.10915v1 [cs.LG])","link":"http://arxiv.org/abs/2302.10915","description":"<p>Visual speech recognition models extract visual features in a hierarchical\nmanner. At the lower level, there is a visual front-end with a limited temporal\nreceptive field that processes the raw pixels depicting the lips or faces. At\nthe higher level, there is an encoder that attends to the embeddings produced\nby the front-end over a large temporal receptive field. Previous work has\nfocused on improving the visual front-end of the model to extract more useful\nfeatures for speech recognition. Surprisingly, our work shows that complex\nvisual front-ends are not necessary. Instead of allocating resources to a\nsophisticated visual front-end, we find that a linear visual front-end paired\nwith a larger Conformer encoder results in lower latency, more efficient memory\nusage, and improved WER performance. We achieve a new state-of-the-art of\n$12.8\\%$ WER for visual speech recognition on the TED LRS3 dataset, which\nrivals the performance of audio-only models from just four years ago.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_O/0/1/0/all/0/1\">Oscar Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1\">Hank Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serdyuk_D/0/1/0/all/0/1\">Dmitriy Serdyuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Ankit Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siohan_O/0/1/0/all/0/1\">Olivier Siohan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Retrieve Engaging Follow-Up Queries. (arXiv:2302.10978v1 [cs.CL])","link":"http://arxiv.org/abs/2302.10978","description":"<p>Open domain conversational agents can answer a broad range of targeted\nqueries. However, the sequential nature of interaction with these systems makes\nknowledge exploration a lengthy task which burdens the user with asking a chain\nof well phrased questions. In this paper, we present a retrieval based system\nand associated dataset for predicting the next questions that the user might\nhave. Such a system can proactively assist users in knowledge exploration\nleading to a more engaging dialog. The retrieval system is trained on a dataset\nwhich contains ~14K multi-turn information-seeking conversations with a valid\nfollow-up question and a set of invalid candidates. The invalid candidates are\ngenerated to simulate various syntactic and semantic confounders such as\nparaphrases, partial entity match, irrelevant entity, and ASR errors. We use\nconfounder specific techniques to simulate these negative examples on the\nOR-QuAC dataset and develop a dataset called the Follow-up Query Bank\n(FQ-Bank). Then, we train ranking models on FQ-Bank and present results\ncomparing supervised and unsupervised approaches. The results suggest that we\ncan retrieve the valid follow-ups by ranking them in higher positions compared\nto confounders, but further knowledge grounding can improve ranking\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Richardson_C/0/1/0/all/0/1\">Christopher Richardson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_S/0/1/0/all/0/1\">Sudipta Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Anjishnu Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_A/0/1/0/all/0/1\">Anand Ramachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_O/0/1/0/all/0/1\">Omar Zia Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raeesy_Z/0/1/0/all/0/1\">Zeynab Raeesy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethy_A/0/1/0/all/0/1\">Abhinav Sethy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Orcas Have Semantic Language? Machine Learning to Predict Orca Behaviors Using Partially Labeled Vocalization Data. (arXiv:2302.10983v1 [cs.SD])","link":"http://arxiv.org/abs/2302.10983","description":"<p>Orcinus orca (killer whales) exhibit complex calls. They last about a second.\nIn a call, an orca typically uses multiple frequencies simultaneously, varies\nthe frequencies, and varies their volumes. Behavior data is hard to obtain\nbecause orcas live under water and travel quickly. Sound data is relatively\neasy to capture. As a science goal, we would like to know whether orca\nvocalizations constitute a semantic language. We do this by studying whether\nmachine learning can predict behavior from vocalizations. Such prediction would\nalso help scientific research and safety applications because one would like to\npredict behavior while only having to capture sound. A significant challenge in\nthis process is lack of labeled data. We work with recent recordings of McMurdo\nSound orcas [Wellard et al. 2020] where each recording is labeled with the\nbehaviors observed during the recording. This yields a dataset where sound\nsegments - continuous vocalizations that can be thought of as call sequences or\nmore general structures - within the recordings are labeled with superfluous\nbehaviors. Despite that, with a careful combination of recent machine learning\ntechniques, we achieve 96.4% classification accuracy. This suggests that orcas\ndo use a semantic language. It is also promising for research and applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sandholm_S/0/1/0/all/0/1\">Sophia Sandholm</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-context Example Selection with Influences. (arXiv:2302.11042v1 [cs.CL])","link":"http://arxiv.org/abs/2302.11042","description":"<p>In-context learning (ICL) is a powerful paradigm emerged from large language\nmodels (LLMs). Despite its promises, ICL performance is known to be highly\nsensitive to input examples. In this work, we use in-context influences to\nanalyze few-shot ICL performance directly from the in-context examples. Our\nproposed influence-based example selection method outperforms most baselines\nwhen evaluated on 10 SuperGlue tasks and stably scales with increasing k-shot.\nThe analysis finds up to a 22.2% performance gap between the most positively\nand negatively influential examples. In a case study, we apply our\ninfluence-based framework to quantify the phenomena of recency bias in example\nordering for few-shot ICL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_E/0/1/0/all/0/1\">Eric Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Edgeformers: Graph-Empowered Transformers for Representation Learning on Textual-Edge Networks. (arXiv:2302.11050v1 [cs.LG])","link":"http://arxiv.org/abs/2302.11050","description":"<p>Edges in many real-world social/information networks are associated with rich\ntext information (e.g., user-user communications or user-product reviews).\nHowever, mainstream network representation learning models focus on propagating\nand aggregating node attributes, lacking specific designs to utilize text\nsemantics on edges. While there exist edge-aware graph neural networks, they\ndirectly initialize edge attributes as a feature vector, which cannot fully\ncapture the contextualized text semantics of edges. In this paper, we propose\nEdgeformers, a framework built upon graph-enhanced Transformers, to perform\nedge and node representation learning by modeling texts on edges in a\ncontextualized way. Specifically, in edge representation learning, we inject\nnetwork information into each Transformer layer when encoding edge texts; in\nnode representation learning, we aggregate edge representations through an\nattention mechanism within each node's ego-graph. On five public datasets from\nthree different domains, Edgeformers consistently outperform state-of-the-art\nbaselines in edge classification and link prediction, demonstrating the\nefficacy in learning edge and node representations, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1\">Bowen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversational Text-to-SQL: An Odyssey into State-of-the-Art and Challenges Ahead. (arXiv:2302.11054v1 [cs.CL])","link":"http://arxiv.org/abs/2302.11054","description":"<p>Conversational, multi-turn, text-to-SQL (CoSQL) tasks map natural language\nutterances in a dialogue to SQL queries. State-of-the-art (SOTA) systems use\nlarge, pre-trained and finetuned language models, such as the T5-family, in\nconjunction with constrained decoding. With multi-tasking (MT) over coherent\ntasks with discrete prompts during training, we improve over specialized\ntext-to-SQL T5-family models. Based on Oracle analyses over n-best hypotheses,\nwe apply a query plan model and a schema linking algorithm as rerankers.\nCombining MT and reranking, our results using T5-3B show absolute accuracy\nimprovements of 1.0% in exact match and 3.4% in execution match over a SOTA\nbaseline on CoSQL. While these gains consistently manifest at turn level,\ncontext dependent turns are considerably harder. We conduct studies to tease\napart errors attributable to domain and compositional generalization, with the\nlatter remaining a challenge for multi-turn conversations, especially in\ngenerating SQL with unseen parse trees.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathi_S/0/1/0/all/0/1\">Sree Hari Krishnan Parthasarathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1\">Lu Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preventing Catastrophic Forgetting in Continual Learning of New Natural Language Tasks. (arXiv:2302.11074v1 [cs.CL])","link":"http://arxiv.org/abs/2302.11074","description":"<p>Multi-Task Learning (MTL) is widely-accepted in Natural Language Processing\nas a standard technique for learning multiple related tasks in one model.\nTraining an MTL model requires having the training data for all tasks available\nat the same time. As systems usually evolve over time, (e.g., to support new\nfunctionalities), adding a new task to an existing MTL model usually requires\nretraining the model from scratch on all the tasks and this can be\ntime-consuming and computationally expensive. Moreover, in some scenarios, the\ndata used to train the original training may be no longer available, for\nexample, due to storage or privacy concerns. In this paper, we approach the\nproblem of incrementally expanding MTL models' capability to solve new tasks\nover time by distilling the knowledge of an already trained model on n tasks\ninto a new one for solving n+1 tasks. To avoid catastrophic forgetting, we\npropose to exploit unlabeled data from the same distributions of the old tasks.\nOur experiments on publicly available benchmarks show that such a technique\ndramatically benefits the distillation by preserving the already acquired\nknowledge (i.e., preventing up to 20% performance drops on old tasks) while\nobtaining good performance on the incrementally added tasks. Further, we also\nshow that our approach is beneficial in practical settings by using data from a\nleading voice assistant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kar_S/0/1/0/all/0/1\">Sudipta Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castellucci_G/0/1/0/all/0/1\">Giuseppe Castellucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filice_S/0/1/0/all/0/1\">Simone Filice</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malmasi_S/0/1/0/all/0/1\">Shervin Malmasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rokhlenko_O/0/1/0/all/0/1\">Oleg Rokhlenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distribution Normalization: An \"Effortless\" Test-Time Augmentation for Contrastively Learned Visual-language Models. (arXiv:2302.11084v1 [cs.LG])","link":"http://arxiv.org/abs/2302.11084","description":"<p>Advances in the field of visual-language contrastive learning have made it\npossible for many downstream applications to be carried out efficiently and\naccurately by simply taking the dot product between image and text\nrepresentations. One of the most representative approaches proposed recently\nknown as CLIP has quickly garnered widespread adoption due to its\neffectiveness. CLIP is trained with an InfoNCE loss that takes into account\nboth positive and negative samples to help learn a much more robust\nrepresentation space. This paper however reveals that the common downstream\npractice of taking a dot product is only a zeroth-order approximation of the\noptimization goal, resulting in a loss of information during test-time.\nIntuitively, since the model has been optimized based on the InfoNCE loss,\ntest-time procedures should ideally also be in alignment. The question lies in\nhow one can retrieve any semblance of negative samples information during\ninference. We propose Distribution Normalization (DN), where we approximate the\nmean representation of a batch of test samples and use such a mean to represent\nwhat would be analogous to negative samples in the InfoNCE loss. DN requires no\nretraining or fine-tuning and can be effortlessly applied during inference.\nExtensive experiments on a wide variety of downstream tasks exhibit a clear\nadvantage of DN over the dot product.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yifei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Juntao Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fengyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zabih_R/0/1/0/all/0/1\">Ramin Zabih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GTRL: An Entity Group-Aware Temporal Knowledge Graph Representation Learning Method. (arXiv:2302.11091v1 [cs.LG])","link":"http://arxiv.org/abs/2302.11091","description":"<p>Temporal Knowledge Graph (TKG) representation learning embeds entities and\nevent types into a continuous low-dimensional vector space by integrating the\ntemporal information, which is essential for downstream tasks, e.g., event\nprediction and question answering. Existing methods stack multiple graph\nconvolution layers to model the influence of distant entities, leading to the\nover-smoothing problem. To alleviate the problem, recent studies infuse\nreinforcement learning to obtain paths that contribute to modeling the\ninfluence of distant entities. However, due to the limited number of hops,\nthese studies fail to capture the correlation between entities that are far\napart and even unreachable. To this end, we propose GTRL, an entity Group-aware\nTemporal knowledge graph Representation Learning method. GTRL is the first work\nthat incorporates the entity group modeling to capture the correlation between\nentities by stacking only a finite number of layers. Specifically, the entity\ngroup mapper is proposed to generate entity groups from entities in a learning\nway. Based on entity groups, the implicit correlation encoder is introduced to\ncapture implicit correlations between any pairwise entity groups. In addition,\nthe hierarchical GCNs are exploited to accomplish the message aggregation and\nrepresentation updating on the entity group graph and the entity graph.\nFinally, GRUs are employed to capture the temporal dependency in TKGs.\nExtensive experiments on three real-world datasets demonstrate that GTRL\nachieves the state-of-the-art performances on the event prediction task,\noutperforming the best baseline by an average of 13.44%, 9.65%, 12.15%, and\n15.12% in MRR, Hits@1, Hits@3, and Hits@10, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xing Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Ling Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities. (arXiv:2302.11154v1 [cs.CV])","link":"http://arxiv.org/abs/2302.11154","description":"<p>Large-scale multi-modal pre-training models such as CLIP and PaLI exhibit\nstrong generalization on various visual domains and tasks. However, existing\nimage classification benchmarks often evaluate recognition on a specific domain\n(e.g., outdoor images) or a specific task (e.g., classifying plant species),\nwhich falls short of evaluating whether pre-trained foundational models are\nuniversal visual recognizers. To address this, we formally present the task of\nOpen-domain Visual Entity recognitioN (OVEN), where a model need to link an\nimage onto a Wikipedia entity with respect to a text query. We construct\nOVEN-Wiki by re-purposing 14 existing datasets with all labels grounded onto\none single label space: Wikipedia entities. OVEN challenges models to select\namong six million possible Wikipedia entities, making it a general visual\nrecognition benchmark with the largest number of labels. Our study on\nstate-of-the-art pre-trained models reveals large headroom in generalizing to\nthe massive-scale label space. We show that a PaLI-based auto-regressive visual\nrecognition model performs surprisingly well, even on Wikipedia entities that\nhave never been seen during fine-tuning. We also find existing pretrained\nmodels yield different strengths: while PaLI-based models obtain higher overall\nperformance, CLIP-based models are better at recognizing tail entities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hexiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_Y/0/1/0/all/0/1\">Yi Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_U/0/1/0/all/0/1\">Urvashi Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1\">Mandar Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kenton Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toutanova_K/0/1/0/all/0/1\">Kristina Toutanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FiNER: Financial Named Entity Recognition Dataset and Weak-Supervision Model. (arXiv:2302.11157v1 [cs.CL])","link":"http://arxiv.org/abs/2302.11157","description":"<p>The development of annotated datasets over the 21st century has helped us\ntruly realize the power of deep learning. Most of the datasets created for the\nnamed-entity-recognition (NER) task are not domain specific. Finance domain\npresents specific challenges to the NER task and a domain specific dataset\nwould help push the boundaries of finance research. In our work, we develop the\nfirst high-quality NER dataset for the finance domain. To set the benchmark for\nthe dataset, we develop and test a weak-supervision-based framework for the NER\ntask. We extend the current weak-supervision framework to make it employable\nfor span-level classification. Our weak-ner framework and the dataset are\npublicly available on GitHub and Hugging Face.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Agam Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vithani_R/0/1/0/all/0/1\">Ruchit Vithani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gullapalli_A/0/1/0/all/0/1\">Abhinav Gullapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chava_S/0/1/0/all/0/1\">Sudheer Chava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UML: A Universal Monolingual Output Layer for Multilingual ASR. (arXiv:2302.11186v1 [eess.AS])","link":"http://arxiv.org/abs/2302.11186","description":"<p>Word-piece models (WPMs) are commonly used subword units in state-of-the-art\nend-to-end automatic speech recognition (ASR) systems. For multilingual ASR,\ndue to the differences in written scripts across languages, multilingual WPMs\nbring the challenges of having overly large output layers and scaling to more\nlanguages. In this work, we propose a universal monolingual output layer (UML)\nto address such problems. Instead of one output node for only one WPM, UML\nre-associates each output node with multiple WPMs, one for each language, and\nresults in a smaller monolingual output layer shared across languages.\nConsequently, the UML enables to switch in the interpretation of each output\nnode depending on the language of the input speech. Experimental results on an\n11-language voice search task demonstrated the feasibility of using UML for\nhigh-quality and high-efficiency multilingual streaming ASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_S/0/1/0/all/0/1\">Shuo-yiin Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Contextual Spelling Correction by External Acoustics Attention and Semantic Aware Data Augmentation. (arXiv:2302.11192v1 [cs.SD])","link":"http://arxiv.org/abs/2302.11192","description":"<p>We previously proposed contextual spelling correction (CSC) to correct the\noutput of end-to-end (E2E) automatic speech recognition (ASR) models with\ncontextual information such as name, place, etc. Although CSC has achieved\nreasonable improvement in the biasing problem, there are still two drawbacks\nfor further accuracy improvement. First, due to information limitation in text\nonly hypothesis or weak performance of ASR model on rare domains, the CSC model\nmay fail to correct phrases with similar pronunciation or anti-context cases\nwhere all biasing phrases are not present in the utterance. Second, there is a\ndiscrepancy between the training and inference of CSC. The bias list in\ntraining is randomly selected but in inference there may be more similarity\nbetween ground truth phrase and other phrases. To solve above limitations, in\nthis paper we propose an improved non-autoregressive (NAR) spelling correction\nmodel for contextual biasing in E2E neural transducer-based ASR systems to\nimprove the previous CSC model from two perspectives: Firstly, we incorporate\nacoustics information with an external attention as well as text hypotheses\ninto CSC to better distinguish target phrase from dissimilar or irrelevant\nphrases. Secondly, we design a semantic aware data augmentation schema in\ntraining phrase to reduce the mismatch between training and inference to\nfurther boost the biasing accuracy. Experiments show that the improved method\noutperforms the baseline ASR+Biasing system by as much as 20.3% relative name\nrecall gain and achieves stable improvement compared to the previous CSC method\nover different bias list name coverage ratio.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Structured Policy Learning for Multi-Domain and Multi-Task Dialogues. (arXiv:2302.11199v1 [cs.CL])","link":"http://arxiv.org/abs/2302.11199","description":"<p>Reinforcement learning has been widely adopted to model dialogue managers in\ntask-oriented dialogues. However, the user simulator provided by\nstate-of-the-art dialogue frameworks are only rough approximations of human\nbehaviour. The ability to learn from a small number of human interactions is\nhence crucial, especially on multi-domain and multi-task environments where the\naction space is large. We therefore propose to use structured policies to\nimprove sample efficiency when learning on these kinds of environments. We also\nevaluate the impact of learning from human vs simulated experts. Among the\ndifferent levels of structure that we tested, the graph neural networks (GNNs)\nshow a remarkable superiority by reaching a success rate above 80% with only 50\ndialogues, when learning from simulated experts. They also show superiority\nwhen learning from human experts, although a performance drop was observed,\nindicating a possible difficulty in capturing the variability of human\nstrategies. We therefore suggest to concentrate future research efforts on\nbridging the gap between human data, simulators and automatic evaluators in\ndialogue frameworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cordier_T/0/1/0/all/0/1\">Thibault Cordier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urvoy_T/0/1/0/all/0/1\">Tanguy Urvoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lefevre_F/0/1/0/all/0/1\">Fabrice Lefevre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rojas_Barahona_L/0/1/0/all/0/1\">Lina M. Rojas-Barahona</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MADI: Inter-domain Matching and Intra-domain Discrimination for Cross-domain Speech Recognition. (arXiv:2302.11224v1 [cs.CL])","link":"http://arxiv.org/abs/2302.11224","description":"<p>End-to-end automatic speech recognition (ASR) usually suffers from\nperformance degradation when applied to a new domain due to domain shift.\nUnsupervised domain adaptation (UDA) aims to improve the performance on the\nunlabeled target domain by transferring knowledge from the source to the target\ndomain. To improve transferability, existing UDA approaches mainly focus on\nmatching the distributions of the source and target domains globally and/or\nlocally, while ignoring the model discriminability. In this paper, we propose a\nnovel UDA approach for ASR via inter-domain MAtching and intra-domain\nDIscrimination (MADI), which improves the model transferability by fine-grained\ninter-domain matching and discriminability by intra-domain contrastive\ndiscrimination simultaneously. Evaluations on the Libri-Adapt dataset\ndemonstrate the effectiveness of our approach. MADI reduces the relative word\nerror rate (WER) on cross-device and cross-environment ASR by 17.7% and 22.8%,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiaming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shiwan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Ning Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoqing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yong Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Multiple Sources for Data-to-Text and Text-to-Data. (arXiv:2302.11269v1 [cs.LG])","link":"http://arxiv.org/abs/2302.11269","description":"<p>Data-to-text (D2T) and text-to-data (T2D) are dual tasks that convert\nstructured data, such as graphs or tables into fluent text, and vice versa.\nThese tasks are usually handled separately and use corpora extracted from a\nsingle source. Current systems leverage pre-trained language models fine-tuned\non D2T or T2D tasks. This approach has two main limitations: first, a separate\nsystem has to be tuned for each task and source; second, learning is limited by\nthe scarcity of available corpora. This paper considers a more general scenario\nwhere data are available from multiple heterogeneous sources. Each source, with\nits specific data format and semantic domain, provides a non-parallel corpus of\ntext and structured data. We introduce a variational auto-encoder model with\ndisentangled style and content variables that allows us to represent the\ndiversity that stems from multiple sources of text and data. Our model is\ndesigned to handle the tasks of D2T and T2D jointly. We evaluate our model on\nseveral datasets, and show that by learning from multiple sources, our model\ncloses the performance gap with its supervised single-source counterpart and\noutperforms it in some cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duong_S/0/1/0/all/0/1\">Song Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lumbreras_A/0/1/0/all/0/1\">Alberto Lumbreras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gartrell_M/0/1/0/all/0/1\">Mike Gartrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic-switch adapted Japanese Dialogue System based on PLATO-2. (arXiv:2302.11280v1 [cs.CL])","link":"http://arxiv.org/abs/2302.11280","description":"<p>Large-scale open-domain dialogue systems such as PLATO-2 have achieved\nstate-of-the-art scores in both English and Chinese. However, little work\nexplores whether such dialogue systems also work well in the Japanese language.\nIn this work, we create a large-scale Japanese dialogue dataset,\nDialogue-Graph, which contains 1.656 million dialogue data in a tree structure\nfrom News, TV subtitles, and Wikipedia corpus. Then, we train PLATO-2 using\nDialogue-Graph to build a large-scale Japanese dialogue system, PLATO-JDS. In\naddition, to improve the PLATO-JDS in the topic switch issue, we introduce a\ntopic-switch algorithm composed of a topic discriminator to switch to a new\ntopic when user input differs from the previous topic. We evaluate the user\nexperience by using our model with respect to four metrics, namely, coherence,\ninformativeness, engagingness, and humanness. As a result, our proposed\nPLATO-JDS achieves an average score of 1.500 for the human evaluation with\nhuman-bot chat strategy, which is close to the maximum score of 2.000 and\nsuggests the high-quality dialogue generation capability of PLATO-2 in\nJapanese. Furthermore, our proposed topic-switch algorithm achieves an average\nscore of 1.767 and outperforms PLATO-JDS by 0.267, indicating its effectiveness\nin improving the user experience of our system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Donghuo Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsumoto_K/0/1/0/all/0/1\">Kazunori Matsumoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hattori_G/0/1/0/all/0/1\">Gen Hattori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikeda_K/0/1/0/all/0/1\">Kazushi Ikeda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Impact of Subword Pooling Strategy for Cross-lingual Event Detection. (arXiv:2302.11365v1 [cs.CL])","link":"http://arxiv.org/abs/2302.11365","description":"<p>Pre-trained multilingual language models (e.g., mBERT, XLM-RoBERTa) have\nsignificantly advanced the state-of-the-art for zero-shot cross-lingual\ninformation extraction. These language models ubiquitously rely on word\nsegmentation techniques that break a word into smaller constituent subwords.\nTherefore, all word labeling tasks (e.g. named entity recognition, event\ndetection, etc.), necessitate a pooling strategy that takes the subword\nrepresentations as input and outputs a representation for the entire word.\nTaking the task of cross-lingual event detection as a motivating example, we\nshow that the choice of pooling strategy can have a significant impact on the\ntarget language performance. For example, the performance varies by up to 16\nabsolute $f_{1}$ points depending on the pooling strategy when training in\nEnglish and testing in Arabic on the ACE task. We carry out our analysis with\nfive different pooling strategies across nine languages in diverse\nmulti-lingual datasets. Across configurations, we find that the canonical\nstrategy of taking just the first subword to represent the entire word is\nusually sub-optimal. On the other hand, we show that attention pooling is\nrobust to language and dataset variations by being either the best or close to\nthe optimal strategy. For reproducibility, we make our code available at\nhttps://github.com/isi-boston/ed-pooling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Shantanu Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fincke_S/0/1/0/all/0/1\">Steven Fincke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenkins_C/0/1/0/all/0/1\">Chris Jenkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_S/0/1/0/all/0/1\">Scott Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boschee_E/0/1/0/all/0/1\">Elizabeth Boschee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation for Neural NLP. (arXiv:2302.11412v1 [cs.CL])","link":"http://arxiv.org/abs/2302.11412","description":"<p>Data scarcity is a problem that occurs in languages and tasks where we do not\nhave large amounts of labeled data but want to use state-of-the-art models.\nSuch models are often deep learning models that require a significant amount of\ndata to train. Acquiring data for various machine learning problems is\naccompanied by high labeling costs. Data augmentation is a low-cost approach\nfor tackling data scarcity. This paper gives an overview of current\nstate-of-the-art data augmentation methods used for natural language\nprocessing, with an emphasis on methods for neural and transformer-based\nmodels. Furthermore, it discusses the practical challenges of data\naugmentation, possible mitigations, and directions for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pluscec_D/0/1/0/all/0/1\">Domagoj Plu&#x161;&#x10d;ec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snajder_J/0/1/0/all/0/1\">Jan &#x160;najder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advancements in Federated Learning: Models, Methods, and Privacy. (arXiv:2302.11466v1 [cs.AI])","link":"http://arxiv.org/abs/2302.11466","description":"<p>Federated learning (FL) is a promising technique for addressing the rising\nprivacy and security issues. Its main ingredient is to cooperatively learn the\nmodel among the distributed clients without uploading any sensitive data. In\nthis paper, we conducted a thorough review of the related works, following the\ndevelopment context and deeply mining the key technologies behind FL from both\ntheoretical and practical perspectives. Specifically, we first classify the\nexisting works in FL architecture based on the network topology of FL systems\nwith detailed analysis and summarization. Next, we abstract the current\napplication problems, summarize the general techniques and frame the\napplication problems into the general paradigm of FL base models. Moreover, we\nprovide our proposed solutions for model training via FL. We have summarized\nand analyzed the existing FedOpt algorithms, and deeply revealed the\nalgorithmic development principles of many first-order algorithms in depth,\nproposing a more generalized algorithm design framework. Based on these\nframeworks, we have instantiated FedOpt algorithms. As privacy and security is\nthe fundamental requirement in FL, we provide the existing attack scenarios and\nthe defense methods. To the best of our knowledge, we are among the first tier\nto review the theoretical methodology and propose our strategies since there\nare very few works surveying the theoretical approaches. Our survey targets\nmotivating the development of high-performance, privacy-preserving, and secure\nmethods to integrate FL into real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huandong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Depeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guiding Large Language Models via Directional Stimulus Prompting. (arXiv:2302.11520v1 [cs.CL])","link":"http://arxiv.org/abs/2302.11520","description":"<p>We introduce a new framework, Directional Stimulus Prompting, that uses a\ntuneable language model (LM) to provide guidance for the black-box frozen large\nlanguage model (LLM) on downstream tasks. Unlike prior work that manually or\nautomatically finds the optimal prompt for each task, we train a policy LM to\ngenerate discrete tokens as ``directional stimulus'' of each input, which is a\nhint/cue such as keywords of an article for summarization. The directional\nstimulus is then combined with the original input and fed into the LLM to guide\nits generation toward the desired target. The policy LM can be trained through\n1) supervised learning from annotated data and 2) reinforcement learning from\noffline and online rewards to explore directional stimulus that better aligns\nLLMs with human preferences. This framework is flexibly applicable to various\nLMs and tasks. To verify its effectiveness, we apply our framework to\nsummarization and dialogue response generation tasks. Experimental results\ndemonstrate that it can significantly improve LLMs' performance with a small\ncollection of training data: a T5 (780M) trained with 2,000 samples from the\nCNN/Daily Mail dataset improves Codex (175B)'s performance by 7.2% in ROUGE-Avg\nscores; 500 dialogues boost the combined score by 52.5%, achieving comparable\nor even better performance than fully trained models on the MultiWOZ dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1\">Michel Galley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xifeng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Does In-Context Learning Help Prompt Tuning?. (arXiv:2302.11521v1 [cs.CL])","link":"http://arxiv.org/abs/2302.11521","description":"<p>Fine-tuning large language models is becoming ever more impractical due to\ntheir rapidly-growing scale. This motivates the use of parameter-efficient\nadaptation methods such as prompt tuning (PT), which adds a small number of\ntunable embeddings to an otherwise frozen model, and in-context learning (ICL),\nin which demonstrations of the task are provided to the model in natural\nlanguage without any additional training. Recently, Singhal et al. (2022)\npropose ``instruction prompt tuning'' (IPT), which combines PT with ICL by\nconcatenating a natural language demonstration with learned prompt embeddings.\nWhile all of these methods have proven effective on different tasks, how they\ninteract with each other remains unexplored. In this paper, we empirically\nstudy when and how in-context examples improve prompt tuning by measuring the\neffectiveness of ICL, PT, and IPT on five text generation tasks with multiple\nbase language models. We observe that (1) IPT does \\emph{not} always outperform\nPT, and in fact requires the in-context demonstration to be semantically\nsimilar to the test input to yield improvements; (2) PT is unstable and\nexhibits high variance, but combining PT and ICL (into IPT) consistently\nreduces variance across all five tasks; and (3) prompts learned for a specific\nsource task via PT exhibit positive transfer when paired with in-context\nexamples of a different target task. Our results offer actionable insights on\nchoosing a suitable parameter-efficient adaptation method for a given task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Simeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iter_D/0/1/0/all/0/1\">Dan Iter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Robot Learning with Semantically Imagined Experience. (arXiv:2302.11550v1 [cs.RO])","link":"http://arxiv.org/abs/2302.11550","description":"<p>Recent advances in robot learning have shown promise in enabling robots to\nperform a variety of manipulation tasks and generalize to novel scenarios. One\nof the key contributing factors to this progress is the scale of robot data\nused to train the models. To obtain large-scale datasets, prior approaches have\nrelied on either demonstrations requiring high human involvement or\nengineering-heavy autonomous data collection schemes, both of which are\nchallenging to scale. To mitigate this issue, we propose an alternative route\nand leverage text-to-image foundation models widely used in computer vision and\nnatural language processing to obtain meaningful data for robot learning\nwithout requiring additional robot data. We term our method Robot Learning with\nSemantically Imagened Experience (ROSIE). Specifically, we make use of the\nstate of the art text-to-image diffusion models and perform aggressive data\naugmentation on top of our existing robotic manipulation datasets via\ninpainting various unseen objects for manipulation, backgrounds, and\ndistractors with text guidance. Through extensive real-world experiments, we\nshow that manipulation policies trained on data augmented this way are able to\nsolve completely unseen tasks with new objects and can behave more robustly\nw.r.t. novel distractors. In addition, we find that we can improve the\nrobustness and generalization of high-level robot learning tasks such as\nsuccess detection through training with the diffusion-based data augmentation.\nThe project's website and videos can be found at diffusion-rosie.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianhe Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Ted Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_A/0/1/0/all/0/1\">Austin Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tompson_J/0/1/0/all/0/1\">Jonathan Tompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brohan_A/0/1/0/all/0/1\">Anthony Brohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Su Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Jaspiar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Clayton Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+M_D/0/1/0/all/0/1\">Dee M</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peralta_J/0/1/0/all/0/1\">Jodilyn Peralta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1\">Brian Ichter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1\">Karol Hausman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Denoising Diffusion Models in Discrete State-Spaces. (arXiv:2107.03006v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.03006","description":"<p>Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown\nimpressive results on image and waveform generation in continuous state spaces.\nHere, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs),\ndiffusion-like generative models for discrete data that generalize the\nmultinomial diffusion model of Hoogeboom et al. 2021, by going beyond\ncorruption processes with uniform transition probabilities. This includes\ncorruption with transition matrices that mimic Gaussian kernels in continuous\nspace, matrices based on nearest neighbors in embedding space, and matrices\nthat introduce absorbing states. The third allows us to draw a connection\nbetween diffusion models and autoregressive and mask-based generative models.\nWe show that the choice of transition matrix is an important design decision\nthat leads to improved results in image and text domains. We also introduce a\nnew loss function that combines the variational lower bound with an auxiliary\ncross entropy loss. For text, this model class achieves strong results on\ncharacter-level text generation while scaling to large vocabularies on LM1B. On\nthe image dataset CIFAR-10, our models approach the sample quality and exceed\nthe log-likelihood of the continuous-space DDPM model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1\">Jacob Austin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_D/0/1/0/all/0/1\">Daniel D. Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarlow_D/0/1/0/all/0/1\">Daniel Tarlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_R/0/1/0/all/0/1\">Rianne van den Berg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Cognitive Architecture for Machine Consciousness and Artificial Superintelligence: Updating Working Memory Iteratively. (arXiv:2203.17255v3 [q-bio.NC] UPDATED)","link":"http://arxiv.org/abs/2203.17255","description":"<p>This article examines how to construct human-like working memory and thought\nprocesses within a computer. The focus is on simulating the mammalian working\nmemory system. There should be two interacting working memory stores, one\nanalogous to sustained firing lending the system a focus of attention, and\nanother analogous to synaptic potentiation lending the system a short-term\nmemory. These working memory stores retain and coactivate representations,\nusing them to search long-term memory for appropriate updates. The working\nmemory stores should be updated continuously, and in an iterative fashion,\nmeaning that, in the next state, some proportion of the coactive items should\nalways be retained. Thus, the set of concepts coactive in working memory will\nevolve gradually and incrementally over time. This makes each state a revised\niteration of the preceding state and causes successive states to overlap and\nblend with respect to the set of representations they contain. As new\nrepresentations are added and old ones are subtracted, some remain active for\nseveral seconds over the course of these changes. This persistent activity,\nsimilar to that used in contemporary artificial recurrent neural networks, is\nused to spread activation energy throughout the global workspace to search for\nthe next associative update. The result is a chain of associatively linked\nintermediate states that are capable of advancing toward a solution or goal.\nIterative updating is conceptualized here as an information processing\nstrategy, a computational and neurophysiological determinant of the stream of\nthought, and an algorithm for designing and programming artificial general\nintelligence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Reser_J/0/1/0/all/0/1\">Jared Edward Reser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Faithful Model Explanation in NLP: A Survey. (arXiv:2209.11326v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.11326","description":"<p>End-to-end neural Natural Language Processing (NLP) models are notoriously\ndifficult to understand. This has given rise to numerous efforts towards model\nexplainability in recent years. One desideratum of model explanation is\nfaithfulness, i.e. an explanation should accurately represent the reasoning\nprocess behind the model's prediction. In this survey, we review over 110 model\nexplanation methods in NLP through the lens of faithfulness. We first discuss\nthe definition and evaluation of faithfulness, as well as its significance for\nexplainability. We then introduce recent advances in faithful explanation,\ngrouping existing approaches into five categories: similarity methods, analysis\nof model-internal structures, backpropagation-based methods, counterfactual\nintervention, and self-explanatory models. For each category, we synthesize its\nrepresentative studies, strengths, and weaknesses. Finally, we summarize their\ncommon virtues and remaining challenges, and reflect on future work directions\ntowards faithful explainability in NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apidianaki_M/0/1/0/all/0/1\">Marianna Apidianaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Interdisciplinary Topic Detection Model for Research Proposal Classification. (arXiv:2209.13519v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2209.13519","description":"<p>The peer merit review of research proposals has been the major mechanism for\ndeciding grant awards. However, research proposals have become increasingly\ninterdisciplinary. It has been a longstanding challenge to assign\ninterdisciplinary proposals to appropriate reviewers, so proposals are fairly\nevaluated. One of the critical steps in reviewer assignment is to generate\naccurate interdisciplinary topic labels for proposal-reviewer matching.\nExisting systems mainly collect topic labels manually generated by principal\ninvestigators. However, such human-reported labels can be non-accurate,\nincomplete, labor intensive, and time costly. What role can AI play in\ndeveloping a fair and precise proposal reviewer assignment system? In this\nstudy, we collaborate with the National Science Foundation of China to address\nthe task of automated interdisciplinary topic path detection. For this purpose,\nwe develop a deep Hierarchical Interdisciplinary Research Proposal\nClassification Network (HIRPCN). Specifically, we first propose a hierarchical\ntransformer to extract the textual semantic information of proposals. We then\ndesign an interdisciplinary graph and leverage GNNs for learning\nrepresentations of each discipline in order to extract interdisciplinary\nknowledge. After extracting the semantic and interdisciplinary knowledge, we\ndesign a level-wise prediction component to fuse the two types of knowledge\nrepresentations and detect interdisciplinary topic paths for each proposal. We\nconduct extensive experiments and expert evaluations on three real-world\ndatasets to demonstrate the effectiveness of our proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1\">Meng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1\">Ziyue Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanjie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuanchun Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Methods for Addressing Class Imbalance in Deep-Learning Based Natural Language Processing. (arXiv:2210.04675v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04675","description":"<p>Many natural language processing (NLP) tasks are naturally imbalanced, as\nsome target categories occur much more frequently than others in the real\nworld. In such scenarios, current NLP models still tend to perform poorly on\nless frequent classes. Addressing class imbalance in NLP is an active research\ntopic, yet, finding a good approach for a particular task and imbalance\nscenario is difficult.\n</p>\n<p>With this survey, the first overview on class imbalance in deep-learning\nbased NLP, we provide guidance for NLP researchers and practitioners dealing\nwith imbalanced data. We first discuss various types of controlled and\nreal-world class imbalance. Our survey then covers approaches that have been\nexplicitly proposed for class-imbalanced NLP tasks or, originating in the\ncomputer vision community, have been evaluated on them. We organize the methods\nby whether they are based on sampling, data augmentation, choice of loss\nfunction, staged learning, or model design. Finally, we discuss open problems\nsuch as dealing with multi-label scenarios, and propose systematic benchmarking\nand reporting in order to move forward on this problem as a community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Henning_S/0/1/0/all/0/1\">Sophie Henning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beluch_W/0/1/0/all/0/1\">William Beluch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_A/0/1/0/all/0/1\">Annemarie Friedrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SQA3D: Situated Question Answering in 3D Scenes. (arXiv:2210.07474v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.07474","description":"<p>We propose a new task to benchmark scene understanding of embodied agents:\nSituated Question Answering in 3D Scenes (SQA3D). Given a scene context (e.g.,\n3D scan), SQA3D requires the tested agent to first understand its situation\n(position, orientation, etc.) in the 3D scene as described by text, then reason\nabout its surrounding environment and answer a question under that situation.\nBased upon 650 scenes from ScanNet, we provide a dataset centered around 6.8k\nunique situations, along with 20.4k descriptions and 33.4k diverse reasoning\nquestions for these situations. These questions examine a wide spectrum of\nreasoning capabilities for an intelligent agent, ranging from spatial relation\ncomprehension to commonsense understanding, navigation, and multi-hop\nreasoning. SQA3D imposes a significant challenge to current multi-modal\nespecially 3D reasoning models. We evaluate various state-of-the-art approaches\nand find that the best one only achieves an overall score of 47.20%, while\namateur human participants can reach 90.06%. We believe SQA3D could facilitate\nfuture embodied AI research with stronger situation understanding and reasoning\ncapability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yong_S/0/1/0/all/0/1\">Silong Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zilong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yitao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyuan Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey. (arXiv:2210.07700v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07700","description":"<p>Recent advances in the capacity of large language models to generate\nhuman-like text have resulted in their increased adoption in user-facing\nsettings. In parallel, these improvements have prompted a heated discourse\naround the risks of societal harms they introduce, whether inadvertent or\nmalicious. Several studies have explored these harms and called for their\nmitigation via development of safer, fairer models. Going beyond enumerating\nthe risks of harms, this work provides a survey of practical methods for\naddressing potential threats and societal harms from language generation\nmodels. We draw on several prior works' taxonomies of language model risks to\npresent a structured overview of strategies for detecting and ameliorating\ndifferent kinds of risks/harms of language generators. Bridging diverse strands\nof research, this survey aims to serve as a practical guide for both LM\nresearchers and practitioners, with explanations of different mitigation\nstrategies' motivations, their limitations, and open problems for future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sachin Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balachandran_V/0/1/0/all/0/1\">Vidhisha Balachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Njoo_L/0/1/0/all/0/1\">Lucille Njoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Effective Distillation of Self-Supervised Speech Models for Automatic Speech Recognition. (arXiv:2210.15631v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2210.15631","description":"<p>Recent years have witnessed great strides in self-supervised learning (SSL)\non the speech processing. The SSL model is normally pre-trained on a great\nvariety of unlabelled data and a large model size is preferred to increase the\nmodeling capacity. However, this might limit its potential applications due to\nthe expensive computation and memory costs introduced by the oversize model.\nMiniaturization for SSL models has become an important research direction of\npractical value. To this end, we explore the effective distillation of\nHuBERT-based SSL models for automatic speech recognition (ASR). First, in order\nto establish a strong baseline, a comprehensive study on different student\nmodel structures is conducted. On top of this, as a supplement to the\nregression loss widely adopted in previous works, a discriminative loss is\nintroduced for HuBERT to enhance the distillation performance, especially in\nlow-resource scenarios. In addition, we design a simple and effective algorithm\nto distill the front-end input from waveform to Fbank feature, resulting in 17%\nparameter reduction and doubling inference speed, at marginal performance\ndegradation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yujin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_C/0/1/0/all/0/1\">Changli Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_Z/0/1/0/all/0/1\">Ziyang Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhisheng Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xie Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Wei-Qiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PUnifiedNER: A Prompting-based Unified NER System for Diverse Datasets. (arXiv:2211.14838v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.14838","description":"<p>Much of named entity recognition (NER) research focuses on developing\ndataset-specific models based on data from the domain of interest, and a\nlimited set of related entity types. This is frustrating as each new dataset\nrequires a new model to be trained and stored. In this work, we present a\n``versatile'' model -- the Prompting-based Unified NER system (PUnifiedNER) --\nthat works with data from different domains and can recognise up to 37 entity\ntypes simultaneously, and theoretically it could be as many as possible. By\nusing prompt learning, PUnifiedNER is a novel approach that is able to jointly\ntrain across multiple corpora, implementing intelligent on-demand entity\nrecognition. Experimental results show that PUnifiedNER leads to significant\nprediction benefits compared to dataset-specific models with impressively\nreduced model deployment costs. Furthermore, the performance of PUnifiedNER can\nachieve competitive or even better performance than state-of-the-art\ndomain-specific methods for some datasets. We also perform comprehensive pilot\nand ablation studies to support in-depth analysis of each component in\nPUnifiedNER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jinghui Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1\">Brian Mac Namee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1\">Fei Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Bidirectional Action-Language Translation with Limited Supervision and Incongruent Input. (arXiv:2301.03353v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.03353","description":"<p>Human infant learning happens during exploration of the environment, by\ninteraction with objects, and by listening to and repeating utterances\ncasually, which is analogous to unsupervised learning. Only occasionally, a\nlearning infant would receive a matching verbal description of an action it is\ncommitting, which is similar to supervised learning. Such a learning mechanism\ncan be mimicked with deep learning. We model this weakly supervised learning\nparadigm using our Paired Gated Autoencoders (PGAE) model, which combines an\naction and a language autoencoder. After observing a performance drop when\nreducing the proportion of supervised training, we introduce the Paired\nTransformed Autoencoders (PTAE) model, using Transformer-based crossmodal\nattention. PTAE achieves significantly higher accuracy in language-to-action\nand action-to-language translations, particularly in realistic but difficult\ncases when only few supervised training samples are available. We also test\nwhether the trained model behaves realistically with conflicting multimodal\ninput. In accordance with the concept of incongruence in psychology, conflict\ndeteriorates the model output. Conflicting action input has a more severe\nimpact than conflicting language input, and more conflicting features lead to\nlarger interference. PTAE can be trained on mostly unlabelled data where\nlabeled data is scarce, and it behaves plausibly when tested with incongruent\ninput.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ozdemir_O/0/1/0/all/0/1\">Ozan &#xd6;zdemir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerzel_M/0/1/0/all/0/1\">Matthias Kerzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1\">Cornelius Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jae Hee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hafez_M/0/1/0/all/0/1\">Muhammad Burhan Hafez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruns_P/0/1/0/all/0/1\">Patrick Bruns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring AI Ethics of ChatGPT: A Diagnostic Analysis. (arXiv:2301.12867v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.12867","description":"<p>Recent breakthroughs in natural language processing (NLP) have permitted the\nsynthesis and comprehension of coherent text in an open-ended way, therefore\ntranslating the theoretical algorithms into practical applications. The large\nlanguage-model (LLM) has significantly impacted businesses such as report\nsummarization softwares and copywriters. Observations indicate, however, that\nLLMs may exhibit social prejudice and toxicity, posing ethical and societal\ndangers of consequences resulting from irresponsibility. Large-scale benchmarks\nfor accountable LLMs should consequently be developed. Although several\nempirical investigations reveal the existence of a few ethical difficulties in\nadvanced LLMs, there is no systematic examination and user study of the ethics\nof current LLMs use. To further educate future efforts on constructing ethical\nLLMs responsibly, we perform a qualitative research method on OpenAI's ChatGPT\nto better understand the practical features of ethical dangers in recent LLMs.\nWe analyze ChatGPT comprehensively from four perspectives: 1) \\textit{Bias} 2)\n\\textit{Reliability} 3) \\textit{Robustness} 4) \\textit{Toxicity}. In accordance\nwith our stated viewpoints, we empirically benchmark ChatGPT on multiple sample\ndatasets. We find that a significant number of ethical risks cannot be\naddressed by existing benchmarks, and hence illustrate them via additional case\nstudies. In addition, we examine the implications of our findings on the AI\nethics of ChatGPT, as well as future problems and practical design\nconsiderations for LLMs. We believe that our findings may give light on future\nefforts to determine and mitigate the ethical hazards posed by machines in LLM\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1\">Terry Yue Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yujin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chunyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1\">Zhenchang Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study on the Transferability of Transformer Modules in Parameter-Efficient Fine-Tuning. (arXiv:2302.00378v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.00378","description":"<p>Parameter-efficient fine-tuning approaches have recently garnered a lot of\nattention. Having considerably lower number of trainable weights, these methods\ncan bring about scalability and computational effectiveness. In this paper, we\nlook for optimal sub-networks and investigate the capability of different\ntransformer modules in transferring knowledge from a pre-trained model to a\ndownstream task. Our empirical results suggest that every transformer module in\nBERT can act as a winning ticket: fine-tuning each specific module while\nkeeping the rest of the network frozen can lead to comparable performance to\nthe full fine-tuning. Among different modules, LayerNorms exhibit the best\ncapacity for knowledge transfer with limited trainable weights, to the extent\nthat, with only 0.003% of all parameters in the layer-wise analysis, they show\nacceptable performance on various target tasks. On the reasons behind their\neffectiveness, we argue that their notable performance could be attributed to\ntheir high-magnitude weights compared to that of the other modules in the\npre-trained BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akbar_Tajari_M/0/1/0/all/0/1\">Mohammad Akbar-Tajari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajaee_S/0/1/0/all/0/1\">Sara Rajaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilehvar_M/0/1/0/all/0/1\">Mohammad Taher Pilehvar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAB: Empathetic Dialogue Generation with Cognition, Affection and Behavior. (arXiv:2302.01935v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.01935","description":"<p>Empathy is an important characteristic to be considered when building a more\nintelligent and humanized dialogue agent. However, existing methods did not\nfully comprehend empathy as a complex process involving three aspects:\ncognition, affection and behavior. In this paper, we propose CAB, a novel\nframework that takes a comprehensive perspective of cognition, affection and\nbehavior to generate empathetic responses. For cognition, we build paths\nbetween critical keywords in the dialogue by leveraging external knowledge.\nThis is because keywords in a dialogue are the core of sentences. Building the\nlogic relationship between keywords, which is overlooked by the majority of\nexisting works, can improve the understanding of keywords and contextual logic,\nthus enhance the cognitive ability. For affection, we capture the emotional\ndependencies with dual latent variables that contain both interlocutors'\nemotions. The reason is that considering both interlocutors' emotions\nsimultaneously helps to learn the emotional dependencies. For behavior, we use\nappropriate dialogue acts to guide the dialogue generation to enhance the\nempathy expression. Extensive experiments demonstrate that our\nmulti-perspective model outperforms the state-of-the-art models in both\nautomatic and manual evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Pan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Donghong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Rui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuejiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zikun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Semantic Approach to Negation Detection and Word Disambiguation with Natural Language Processing. (arXiv:2302.02291v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.02291","description":"<p>This study aims to demonstrate the methods for detecting negations in a\nsentence by uniquely evaluating the lexical structure of the text via\nword-sense disambiguation. The proposed framework examines all the unique\nfeatures in the various expressions within a text to resolve the contextual\nusage of all tokens and decipher the effect of negation on sentiment analysis.\nThe application of popular expression detectors skips this important step,\nthereby neglecting the root words caught in the web of negation and making text\nclassification difficult for machine learning and sentiment analysis. This\nstudy adopts the Natural Language Processing (NLP) approach to discover and\nantonimize words that were negated for better accuracy in text classification\nusing a knowledge base provided by an NLP library called WordHoard. Early\nresults show that our initial analysis improved on traditional sentiment\nanalysis, which sometimes neglects negations or assigns an inverse polarity\nscore. The SentiWordNet analyzer was improved by 35%, the Vader analyzer by 20%\nand the TextBlob by 6%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Okpala_I/0/1/0/all/0/1\">Izunna Okpala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_G/0/1/0/all/0/1\">Guillermo Romera Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tapia_A/0/1/0/all/0/1\">Andrea Tapia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halse_S/0/1/0/all/0/1\">Shane Halse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kropczynski_J/0/1/0/all/0/1\">Jess Kropczynski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RESDSQL: Decoupling Schema Linking and Skeleton Parsing for Text-to-SQL. (arXiv:2302.05965v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.05965","description":"<p>One of the recent best attempts at Text-to-SQL is the pre-trained language\nmodel. Due to the structural property of the SQL queries, the seq2seq model\ntakes the responsibility of parsing both the schema items (i.e., tables and\ncolumns) and the skeleton (i.e., SQL keywords). Such coupled targets increase\nthe difficulty of parsing the correct SQL queries especially when they involve\nmany schema items and logic operators. This paper proposes a ranking-enhanced\nencoding and skeleton-aware decoding framework to decouple the schema linking\nand the skeleton parsing. Specifically, for a seq2seq encoder-decode model, its\nencoder is injected by the most relevant schema items instead of the whole\nunordered ones, which could alleviate the schema linking effort during SQL\nparsing, and its decoder first generates the skeleton and then the actual SQL\nquery, which could implicitly constrain the SQL parsing. We evaluate our\nproposed framework on Spider and its three robustness variants: Spider-DK,\nSpider-Syn, and Spider-Realistic. The experimental results show that our\nframework delivers promising performance and robustness. Our code is available\nat https://github.com/RUCKBReasoning/RESDSQL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cuiping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What happens before and after: Multi-Event Commonsense in Event Coreference Resolution. (arXiv:2302.09715v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.09715","description":"<p>Event coreference models cluster event mentions pertaining to the same\nreal-world event. Recent models rely on contextualized representations to\nrecognize coreference among lexically or contextually similar mentions.\nHowever, models typically fail to leverage commonsense inferences, which is\nparticularly limiting for resolving lexically-divergent mentions. We propose a\nmodel that extends event mentions with temporal commonsense inferences. Given a\ncomplex sentence with multiple events, e.g., \"The man killed his wife and got\narrested\", with the target event \"arrested\", our model generates plausible\nevents that happen before the target event - such as \"the police arrived\", and\nafter it, such as \"he was sentenced\". We show that incorporating such\ninferences into an existing event coreference model improves its performance,\nand we analyze the coreferences in which such temporal knowledge is required.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1\">Sahithya Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanner_C/0/1/0/all/0/1\">Chris Tanner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_R/0/1/0/all/0/1\">Raymond Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1\">Vered Shwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-02-22T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}