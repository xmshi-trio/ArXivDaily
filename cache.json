{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-09-26T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Applying BioBERT to Extract Germline Gene-Disease Associations for Building a Knowledge Graph from the Biomedical Literature. (arXiv:2309.13061v1 [cs.CL])","link":"http://arxiv.org/abs/2309.13061","description":"<p>Published biomedical information has and continues to rapidly increase. The\nrecent advancements in Natural Language Processing (NLP), have generated\nconsiderable interest in automating the extraction, normalization, and\nrepresentation of biomedical knowledge about entities such as genes and\ndiseases. Our study analyzes germline abstracts in the construction of\nknowledge graphs of the of the immense work that has been done in this area for\ngenes and diseases. This paper presents SimpleGermKG, an automatic knowledge\ngraph construction approach that connects germline genes and diseases. For the\nextraction of genes and diseases, we employ BioBERT, a pre-trained BERT model\non biomedical corpora. We propose an ontology-based and rule-based algorithm to\nstandardize and disambiguate medical terms. For semantic relationships between\narticles, genes, and diseases, we implemented a part-whole relation approach to\nconnect each entity with its data source and visualize them in a graph-based\nknowledge representation. Lastly, we discuss the knowledge graph applications,\nlimitations, and challenges to inspire the future research of germline corpora.\nOur knowledge graph contains 297 genes, 130 diseases, and 46,747 triples.\nGraph-based visualizations are used to show the results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_A/0/1/0/all/0/1\">Armando D. Diaz Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1\">Songhui Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_S/0/1/0/all/0/1\">Sean T. Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_K/0/1/0/all/0/1\">Kevin S. Hughes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Large Language Models to Generate, Validate, and Apply User Intent Taxonomies. (arXiv:2309.13063v1 [cs.IR])","link":"http://arxiv.org/abs/2309.13063","description":"<p>Log data can reveal valuable information about how users interact with web\nsearch services, what they want, and how satisfied they are. However, analyzing\nuser intents in log data is not easy, especially for new forms of web search\nsuch as AI-driven chat. To understand user intents from log data, we need a way\nto label them with meaningful categories that capture their diversity and\ndynamics. Existing methods rely on manual or ML-based labeling, which are\neither expensive or inflexible for large and changing datasets. We propose a\nnovel solution using large language models (LLMs), which can generate rich and\nrelevant concepts, descriptions, and examples for user intents. However, using\nLLMs to generate a user intent taxonomy and apply it to do log analysis can be\nproblematic for two main reasons: such a taxonomy is not externally validated,\nand there may be an undesirable feedback loop. To overcome these issues, we\npropose a new methodology with human experts and assessors to verify the\nquality of the LLM-generated taxonomy. We also present an end-to-end pipeline\nthat uses an LLM with human-in-the-loop to produce, refine, and use labels for\nuser intent analysis in log data. Our method offers a scalable and adaptable\nway to analyze user intents in web-scale log data with minimal human effort. We\ndemonstrate its effectiveness by uncovering new insights into user intents from\nsearch and chat logs from Bing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_C/0/1/0/all/0/1\">Chirag Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_R/0/1/0/all/0/1\">Ryen W. White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andersen_R/0/1/0/all/0/1\">Reid Andersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buscher_G/0/1/0/all/0/1\">Georg Buscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Counts_S/0/1/0/all/0/1\">Scott Counts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sarkar Snigdha Sarathi Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montazer_A/0/1/0/all/0/1\">Ali Montazer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manivannan_S/0/1/0/all/0/1\">Sathish Manivannan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neville_J/0/1/0/all/0/1\">Jennifer Neville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_X/0/1/0/all/0/1\">Xiaochuan Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangan_N/0/1/0/all/0/1\">Nagu Rangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safavi_T/0/1/0/all/0/1\">Tara Safavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suri_S/0/1/0/all/0/1\">Siddharth Suri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_M/0/1/0/all/0/1\">Mengting Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Leijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Longqi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personality Profiling: How informative are social media profiles in predicting personal information?. (arXiv:2309.13065v1 [cs.CL])","link":"http://arxiv.org/abs/2309.13065","description":"<p>Personality profiling has been utilised by companies for targeted\nadvertising, political campaigns and vaccine campaigns. However, the accuracy\nand versatility of such models still remains relatively unknown. Consequently,\nwe aim to explore the extent to which peoples' online digital footprints can be\nused to profile their Myers-Briggs personality type. We analyse and compare the\nresults of four models: logistic regression, naive Bayes, support vector\nmachines (SVMs) and random forests. We discover that a SVM model achieves the\nbest accuracy of 20.95% for predicting someones complete personality type.\nHowever, logistic regression models perform only marginally worse and are\nsignificantly faster to train and perform predictions. We discover that many\nlabelled datasets present substantial class imbalances of personal\ncharacteristics on social media, including our own. As a result, we highlight\nthe need for attentive consideration when reporting model performance on these\ndatasets and compare a number of methods for fixing the class-imbalance\nproblems. Moreover, we develop a statistical framework for assessing the\nimportance of different sets of features in our models. We discover some\nfeatures to be more informative than others in the Intuitive/Sensory (p =\n0.032) and Thinking/Feeling (p = 0.019) models. While we apply these methods to\nMyers-Briggs personality profiling, they could be more generally used for any\nlabelling of individuals on social media.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Watt_J/0/1/0/all/0/1\">Joshua Watt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuke_J/0/1/0/all/0/1\">Jonathan Tuke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_L/0/1/0/all/0/1\">Lewis Mitchell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning Technique Based Fake News Detection. (arXiv:2309.13069v1 [cs.CL])","link":"http://arxiv.org/abs/2309.13069","description":"<p>False news has received attention from both the general public and the\nscholarly world. Such false information has the ability to affect public\nperception, giving nefarious groups the chance to influence the results of\npublic events like elections. Anyone can share fake news or facts about anyone\nor anything for their personal gain or to cause someone trouble. Also,\ninformation varies depending on the part of the world it is shared on. Thus, in\nthis paper, we have trained a model to classify fake and true news by utilizing\nthe 1876 news data from our collected dataset. We have preprocessed the data to\nget clean and filtered texts by following the Natural Language Processing\napproaches. Our research conducts 3 popular Machine Learning (Stochastic\ngradient descent, Na\\\"ive Bayes, Logistic Regression,) and 2 Deep Learning\n(Long-Short Term Memory, ASGD Weight-Dropped LSTM, or AWD-LSTM) algorithms.\nAfter we have found our best Naive Bayes classifier with 56% accuracy and an\nF1-macro score of an average of 32%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sutradhar_B/0/1/0/all/0/1\">Biplob Kumar Sutradhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonaid_M/0/1/0/all/0/1\">Md. Zonaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ria_N/0/1/0/all/0/1\">Nushrat Jahan Ria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noori_S/0/1/0/all/0/1\">Sheak Rashed Haider Noori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Reasoning by Neuro-Symbolic Approaches. (arXiv:2309.13072v1 [cs.CL])","link":"http://arxiv.org/abs/2309.13072","description":"<p>Deep learning has largely improved the performance of various natural\nlanguage processing (NLP) tasks. However, most deep learning models are\nblack-box machinery, and lack explicit interpretation. In this chapter, we will\nintroduce our recent progress on neuro-symbolic approaches to NLP, which\ncombines different schools of AI, namely, symbolism and connectionism.\nGenerally, we will design a neural system with symbolic latent structures for\nan NLP task, and apply reinforcement learning or its relaxation to perform\nweakly supervised reasoning in the downstream task. Our framework has been\nsuccessfully applied to various tasks, including table query reasoning,\nsyntactic structure reasoning, information extraction reasoning, and rule\nreasoning. For each application, we will introduce the background, our\napproach, and experimental results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianggen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhengdong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lili Mou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCREWS: A Modular Framework for Reasoning with Revisions. (arXiv:2309.13075v1 [cs.AI])","link":"http://arxiv.org/abs/2309.13075","description":"<p>Large language models (LLMs) can improve their accuracy on various tasks\nthrough iteratively refining and revising their output based on feedback. We\nobserve that these revisions can introduce errors, in which case it is better\nto roll back to a previous result. Further, revisions are typically\nhomogeneous: they use the same reasoning method that produced the initial\nanswer, which may not correct errors. To enable exploration in this space, we\npresent SCREWS, a modular framework for reasoning with revisions. It is\ncomprised of three main modules: Sampling, Conditional Resampling, and\nSelection, each consisting of sub-modules that can be hand-selected per task.\nWe show that SCREWS not only unifies several previous approaches under a common\nframework, but also reveals several novel strategies for identifying improved\nreasoning chains. We evaluate our framework with state-of-the-art LLMs (ChatGPT\nand GPT-4) on a diverse set of reasoning tasks and uncover useful new reasoning\nstrategies for each: arithmetic word problems, multi-hop question answering,\nand code debugging. Heterogeneous revision strategies prove to be important, as\ndoes selection between original and revised candidates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shridhar_K/0/1/0/all/0/1\">Kumar Shridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jhamtani_H/0/1/0/all/0/1\">Harsh Jhamtani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Hao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisner_J/0/1/0/all/0/1\">Jason Eisner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_P/0/1/0/all/0/1\">Patrick Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MiChao-HuaFen 1.0: A Specialized Pre-trained Corpus Dataset for Domain-specific Large Models. (arXiv:2309.13079v1 [cs.CL])","link":"http://arxiv.org/abs/2309.13079","description":"<p>With the advancement of deep learning technologies, general-purpose large\nmodels such as GPT-4 have demonstrated exceptional capabilities across various\ndomains. Nevertheless, there remains a demand for high-quality, domain-specific\noutputs in areas like healthcare, law, and finance. This paper first evaluates\nthe existing large models for specialized domains and discusses their\nlimitations. To cater to the specific needs of certain domains, we introduce\nthe ``MiChao-HuaFen 1.0'' pre-trained corpus dataset, tailored for the news and\ngovernmental sectors. The dataset, sourced from publicly available internet\ndata from 2022, underwent multiple rounds of cleansing and processing to ensure\nhigh quality and reliable origins, with provisions for consistent and stable\nupdates. This dataset not only supports the pre-training of large models for\nChinese vertical domains but also aids in propelling deep learning research and\napplications in related fields.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yidong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Conghui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Conghui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1\">FuKai Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Rui Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPICED: News Similarity Detection Dataset with Multiple Topics and Complexity Levels. (arXiv:2309.13080v1 [cs.CL])","link":"http://arxiv.org/abs/2309.13080","description":"<p>Nowadays, the use of intelligent systems to detect redundant information in\nnews articles has become especially prevalent with the proliferation of news\nmedia outlets in order to enhance user experience. However, the heterogeneous\nnature of news can lead to spurious findings in these systems: Simple\nheuristics such as whether a pair of news are both about politics can provide\nstrong but deceptive downstream performance. Segmenting news similarity\ndatasets into topics improves the training of these models by forcing them to\nlearn how to distinguish salient characteristics under more narrow domains.\nHowever, this requires the existence of topic-specific datasets, which are\ncurrently lacking. In this article, we propose a new dataset of similar news,\nSPICED, which includes seven topics: Crime &amp; Law, Culture &amp; Entertainment,\nDisasters &amp; Accidents, Economy &amp; Business, Politics &amp; Conflicts, Science &amp;\nTechnology, and Sports. Futhermore, we present four distinct approaches for\ngenerating news pairs, which are used in the creation of datasets specifically\ndesigned for news similarity detection task. We benchmarked the created\ndatasets using MinHash, BERT, SBERT, and SimCSE models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shushkevich_E/0/1/0/all/0/1\">Elena Shushkevich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_L/0/1/0/all/0/1\">Long Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loureiro_M/0/1/0/all/0/1\">Manuel V. Loureiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derby_S/0/1/0/all/0/1\">Steven Derby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijaya_T/0/1/0/all/0/1\">Tri Kurniawan Wijaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Lexical Analysis of Dog Vocalizations via Online Videos. (arXiv:2309.13086v1 [cs.SD])","link":"http://arxiv.org/abs/2309.13086","description":"<p>Deciphering the semantics of animal language has been a grand challenge. This\nstudy presents a data-driven investigation into the semantics of dog\nvocalizations via correlating different sound types with consistent semantics.\nWe first present a new dataset of Shiba Inu sounds, along with contextual\ninformation such as location and activity, collected from YouTube with a\nwell-constructed pipeline. The framework is also applicable to other animal\nspecies. Based on the analysis of conditioned probability between dog\nvocalizations and corresponding location and activity, we discover supporting\nevidence for previous heuristic research on the semantic meaning of various dog\nsounds. For instance, growls can signify interactions. Furthermore, our study\nyields new insights that existing word types can be subdivided into\nfiner-grained subtypes and minimal semantic unit for Shiba Inu is word-related.\nFor example, whimper can be subdivided into two types, attention-seeking and\ndiscomfort.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chunhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jieyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mengyue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kenny Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cardiovascular Disease Risk Prediction via Social Media. (arXiv:2309.13147v1 [cs.CL])","link":"http://arxiv.org/abs/2309.13147","description":"<p>Researchers utilize Twitter and sentiment analysis to forecast the risk of\nCardiovascular Disease (CVD). We have introduced a novel CVD-related keyword\ndictionary by scrutinizing the emotions conveyed in tweets. We gathered tweets\nfrom eighteen U.S. states, encompassing the Appalachian region. Employing the\nVADER model for sentiment analysis, we categorized users as potentially at risk\nfor CVD. Machine Learning (ML) models were employed to assess individuals' CVD\nrisk and were subsequently applied to a CDC dataset containing demographic\ninformation for comparison. We considered various performance evaluation\nmetrics, including Test Accuracy, Precision, Recall, F1 score, Mathew's\nCorrelation Coefficient (MCC), and Cohen's Kappa (CK) score. Our findings\ndemonstrate that analyzing the emotional content of tweets outperforms the\npredictive capabilities of demographic data alone, enabling the identification\nof individuals at potential risk of developing CVD. This research underscores\nthe potential of Natural Language Processing (NLP) and ML techniques in\nleveraging tweets to identify individuals with CVD risks, offering an\nalternative approach to traditional demographic information for public health\nmonitoring.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Habib_A/0/1/0/all/0/1\">Al Zadid Sultan Bin Habib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Syed_M/0/1/0/all/0/1\">Md Asif Bin Syed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Tanvirul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adjeroh_D/0/1/0/all/0/1\">Donald A. Adjeroh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Are Also Good Prototypical Commonsense Reasoners. (arXiv:2309.13165v1 [cs.CL])","link":"http://arxiv.org/abs/2309.13165","description":"<p>Commonsense reasoning is a pivotal skill for large language models, yet it\npresents persistent challenges in specific tasks requiring this competence.\nTraditional fine-tuning approaches can be resource-intensive and potentially\ncompromise a model's generalization capacity. Furthermore, state-of-the-art\nlanguage models like GPT-3.5 and Claude are primarily accessible through API\ncalls, which makes fine-tuning models challenging. To address these challenges,\nwe draw inspiration from the outputs of large models for tailored tasks and\nsemi-automatically developed a set of novel prompts from several perspectives,\nincluding task-relevance, supportive evidence generation (e.g. chain-of-thought\nand knowledge), diverse path decoding to aid the model. Experimental results on\nProtoQA dataset demonstrate that with better designed prompts we can achieve\nthe new state-of-art(SOTA) on the ProtoQA leaderboard, improving the Max\nAnswer@1 score by 8%, Max Incorrect@1 score by 4% (breakthrough 50% for the\nfirst time) compared to the previous SOTA model and achieved an improvement on\nStrategyQA and CommonsenseQA2.0 (3% and 1%, respectively). Furthermore, with\nthe generated Chain-of-Thought and knowledge, we can improve the\ninterpretability of the model while also surpassing the previous SOTA models.\nWe hope that our work can provide insight for the NLP community to develop\nbetter prompts and explore the potential of large language models for more\ncomplex reasoning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qianglong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Hongxiang Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BenLLMEval: A Comprehensive Evaluation into the Potentials and Pitfalls of Large Language Models on Bengali NLP. (arXiv:2309.13173v1 [cs.CL])","link":"http://arxiv.org/abs/2309.13173","description":"<p>Large Language Models (LLMs) have emerged as one of the most important\nbreakthroughs in natural language processing (NLP) for their impressive skills\nin language generation and other language-specific tasks. Though LLMs have been\nevaluated in various tasks, mostly in English, they have not yet undergone\nthorough evaluation in under-resourced languages such as Bengali (Bangla). In\nthis paper, we evaluate the performance of LLMs for the low-resourced Bangla\nlanguage. We select various important and diverse Bangla NLP tasks, such as\nabstractive summarization, question answering, paraphrasing, natural language\ninference, text classification, and sentiment analysis for zero-shot evaluation\nwith ChatGPT, LLaMA-2, and Claude-2 and compare the performance with\nstate-of-the-art fine-tuned models. Our experimental results demonstrate an\ninferior performance of LLMs for different Bangla NLP tasks, calling for\nfurther effort to develop better understanding of LLMs in low-resource\nlanguages like Bangla.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Mohsinul Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mohammed Saidul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskar_M/0/1/0/all/0/1\">Md Tahmid Rahman Laskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayeem_M/0/1/0/all/0/1\">Mir Tafseer Nayeem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1\">M Saiful Bari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoque_E/0/1/0/all/0/1\">Enamul Hoque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Distillation of Table-based Reasoning Ability from LLMs. (arXiv:2309.13182v1 [cs.CL])","link":"http://arxiv.org/abs/2309.13182","description":"<p>Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of natural language processing tasks. However, their remarkable\nparameter size and their impressive high requirement of computing resources\npose challenges for their practical deployment. Recent research has revealed\nthat specific capabilities of LLMs, such as numerical reasoning, can be\ntransferred to smaller models through distillation. Some studies explore the\npotential of leveraging LLMs to perform table-based reasoning. Nevertheless,\nprior to our work, there has been no investigation into the prospect of\nspecialising table reasoning skills in smaller models specifically tailored for\ntable-to-text generation tasks. In this paper, we propose a novel table-based\nreasoning distillation, with the aim of distilling distilling LLMs into\ntailored, smaller models specifically designed for table-based reasoning task.\nExperimental results have shown that a 0.22 billion parameter model\n(Flan-T5-base) fine-tuned using distilled data, not only achieves a significant\nimprovement compared to traditionally fine-tuned baselines but also surpasses\nspecific LLMs like gpt-3.5-turbo on the scientific table-to-text generation\ndataset (SciGen). The code and data are released in\nhttps://github.com/Bernard-Yang/TableDistill.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bohao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chenghao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Document Understanding for Healthcare Referrals. (arXiv:2309.13184v1 [cs.CL])","link":"http://arxiv.org/abs/2309.13184","description":"<p>Reliance on scanned documents and fax communication for healthcare referrals\nleads to high administrative costs and errors that may affect patient care. In\nthis work we propose a hybrid model leveraging LayoutLMv3 along with\ndomain-specific rules to identify key patient, physician, and exam-related\nentities in faxed referral documents. We explore some of the challenges in\napplying a document understanding model to referrals, which have formats\nvarying by medical practice, and evaluate model performance using MUC-5 metrics\nto obtain appropriate metrics for the practical use case. Our analysis shows\nthe addition of domain-specific rules to the transformer model yields greatly\nincreased precision and F1 scores, suggesting a hybrid model trained on a\ncurated dataset can increase efficiency in referral management.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mistry_J/0/1/0/all/0/1\">Jimit Mistry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arzeno_N/0/1/0/all/0/1\">Natalia M. Arzeno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models and Control Mechanisms Improve Text Readability of Biomedical Abstracts. (arXiv:2309.13202v1 [cs.CL])","link":"http://arxiv.org/abs/2309.13202","description":"<p>Biomedical literature often uses complex language and inaccessible\nprofessional terminologies. That is why simplification plays an important role\nin improving public health literacy. Applying Natural Language Processing (NLP)\nmodels to automate such tasks allows for quick and direct accessibility for lay\nreaders. In this work, we investigate the ability of state-of-the-art large\nlanguage models (LLMs) on the task of biomedical abstract simplification, using\nthe publicly available dataset for plain language adaptation of biomedical\nabstracts (\\textbf{PLABA}). The methods applied include domain fine-tuning and\nprompt-based learning (PBL) on: 1) Encoder-decoder models (T5, SciFive, and\nBART), 2) Decoder-only GPT models (GPT-3.5 and GPT-4) from OpenAI and BioGPT,\nand 3) Control-token mechanisms on BART-based models. We used a range of\nautomatic evaluation metrics, including BLEU, ROUGE, SARI, and BERTscore, and\nalso conducted human evaluations. BART-Large with Control Token (BART-L-w-CT)\nmechanisms reported the highest SARI score of 46.54 and T5-base reported the\nhighest BERTscore 72.62. In human evaluation, BART-L-w-CTs achieved a better\nsimplicity score over T5-Base (2.9 vs. 2.2), while T5-Base achieved a better\nmeaning preservation score over BART-L-w-CTs (3.1 vs. 2.6). We also categorised\nthe system outputs with examples, hoping this will shed some light for future\nresearch on this task. Our code, fine-tuned models, and data splits are\navailable at \\url{https://github.com/HECTA-UoM/PLABA-MU}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zihao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belkadi_S/0/1/0/all/0/1\">Samuel Belkadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Micheletti_N/0/1/0/all/0/1\">Nicolo Micheletti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lifeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shardlow_M/0/1/0/all/0/1\">Matthew Shardlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1\">Goran Nenadic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Practical Survey on Zero-shot Prompt Design for In-context Learning. (arXiv:2309.13205v1 [cs.CL])","link":"http://arxiv.org/abs/2309.13205","description":"<p>The remarkable advancements in large language models (LLMs) have brought\nabout significant improvements in Natural Language Processing(NLP) tasks. This\npaper presents a comprehensive review of in-context learning techniques,\nfocusing on different types of prompts, including discrete, continuous,\nfew-shot, and zero-shot, and their impact on LLM performance. We explore\nvarious approaches to prompt design, such as manual design, optimization\nalgorithms, and evaluation methods, to optimize LLM performance across diverse\ntasks. Our review covers key research studies in prompt engineering, discussing\ntheir methodologies and contributions to the field. We also delve into the\nchallenges faced in evaluating prompt performance, given the absence of a\nsingle \"best\" prompt and the importance of considering multiple metrics. In\nconclusion, the paper highlights the critical role of prompt design in\nharnessing the full potential of LLMs and provides insights into the\ncombination of manual design, optimization techniques, and rigorous evaluation\nfor more effective and efficient use of LLMs in various NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hindi to English: Transformer-Based Neural Machine Translation. (arXiv:2309.13222v1 [cs.CL])","link":"http://arxiv.org/abs/2309.13222","description":"<p>Machine Translation (MT) is one of the most prominent tasks in Natural\nLanguage Processing (NLP) which involves the automatic conversion of texts from\none natural language to another while preserving its meaning and fluency.\nAlthough the research in machine translation has been going on since multiple\ndecades, the newer approach of integrating deep learning techniques in natural\nlanguage processing has led to significant improvements in the translation\nquality. In this paper, we have developed a Neural Machine Translation (NMT)\nsystem by training the Transformer model to translate texts from Indian\nLanguage Hindi to English. Hindi being a low resource language has made it\ndifficult for neural networks to understand the language thereby leading to a\nslow growth in the development of neural machine translators. Thus, to address\nthis gap, we implemented back-translation to augment the training data and for\ncreating the vocabulary, we experimented with both word and subword level\ntokenization using Byte Pair Encoding (BPE) thereby ending up training the\nTransformer in 10 different configurations. This led us to achieve a\nstate-of-the-art BLEU score of 24.53 on the test set of IIT Bombay\nEnglish-Hindi Corpus in one of the configurations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gangar_K/0/1/0/all/0/1\">Kavit Gangar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruparel_H/0/1/0/all/0/1\">Hardik Ruparel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lele_S/0/1/0/all/0/1\">Shreyas Lele</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NJUNLP's Participation for the WMT2023 Quality Estimation Shared Task. (arXiv:2309.13230v1 [cs.CL])","link":"http://arxiv.org/abs/2309.13230","description":"<p>We introduce the submissions of the NJUNLP team to the WMT 2023 Quality\nEstimation (QE) shared task. Our team submitted predictions for the\nEnglish-German language pair on all two sub-tasks: (i) sentence- and word-level\nquality prediction; and (ii) fine-grained error span detection. This year, we\nfurther explore pseudo data methods for QE based on NJUQE framework\n(https://github.com/NJUNLP/njuqe). We generate pseudo MQM data using parallel\ndata from the WMT translation task. We pre-train the XLMR large model on pseudo\nQE data, then fine-tune it on real QE data. At both stages, we jointly learn\nsentence-level scores and word-level tags. Empirically, we conduct experiments\nto find the key hyper-parameters that improve the performance. Technically, we\npropose a simple method that covert the word-level outputs to fine-grained\nerror span results. Overall, our models achieved the best results in\nEnglish-German for both word-level and fine-grained error span detection\nsub-tasks by a considerable margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiang Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Z/0/1/0/all/0/1\">Zhejian Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Shimin Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"User Simulation with Large Language Models for Evaluating Task-Oriented Dialogue. (arXiv:2309.13233v1 [cs.CL])","link":"http://arxiv.org/abs/2309.13233","description":"<p>One of the major impediments to the development of new task-oriented dialogue\n(TOD) systems is the need for human evaluation at multiple stages and\niterations of the development process. In an effort to move toward automated\nevaluation of TOD, we propose a novel user simulator built using recently\ndeveloped large pretrained language models (LLMs). In order to increase the\nlinguistic diversity of our system relative to the related previous work, we do\nnot fine-tune the LLMs used by our system on existing TOD datasets; rather we\nuse in-context learning to prompt the LLMs to generate robust and\nlinguistically diverse output with the goal of simulating the behavior of human\ninterlocutors. Unlike previous work, which sought to maximize goal success rate\n(GSR) as the primary metric of simulator performance, our goal is a system\nwhich achieves a GSR similar to that observed in human interactions with TOD\nsystems. Using this approach, our current simulator is effectively able to\ninteract with several TOD systems, especially on single-intent conversational\ngoals, while generating lexically and syntactically diverse output relative to\nprevious simulators that rely upon fine-tuned models. Finally, we collect a\nHuman2Bot dataset of humans interacting with the same TOD systems with which we\nexperimented in order to better quantify these achievements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davidson_S/0/1/0/all/0/1\">Sam Davidson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romeo_S/0/1/0/all/0/1\">Salvatore Romeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_R/0/1/0/all/0/1\">Raphael Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gung_J/0/1/0/all/0/1\">James Gung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Arshit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_S/0/1/0/all/0/1\">Saab Mansour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChEDDAR: Student-ChatGPT Dialogue in EFL Writing Education. (arXiv:2309.13243v1 [cs.CL])","link":"http://arxiv.org/abs/2309.13243","description":"<p>The integration of generative AI in education is expanding, yet empirical\nanalyses of large-scale, real-world interactions between students and AI\nsystems still remain limited. In this study, we present ChEDDAR, ChatGPT &amp; EFL\nLearner's Dialogue Dataset As Revising an essay, which is collected from a\nsemester-long longitudinal experiment involving 212 college students enrolled\nin English as Foreign Langauge (EFL) writing courses. The students were asked\nto revise their essays through dialogues with ChatGPT. ChEDDAR includes a\nconversation log, utterance-level essay edit history, self-rated satisfaction,\nand students' intent, in addition to session-level pre-and-post surveys\ndocumenting their objectives and overall experiences. We analyze students'\nusage patterns and perceptions regarding generative AI with respect to their\nintent and satisfaction. As a foundational step, we establish baseline results\nfor two pivotal tasks in task-oriented dialogue systems within educational\ncontexts: intent detection and satisfaction estimation. We finally suggest\nfurther research to refine the integration of generative AI into education\nsettings, outlining potential scenarios utilizing ChEDDAR. ChEDDAR is publicly\navailable at https://github.com/zeunie/ChEDDAR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jieun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_H/0/1/0/all/0/1\">Haneul Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myung_J/0/1/0/all/0/1\">Junho Myung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tak Yeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">So-Yeon Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Document-Level Information Extraction. (arXiv:2309.13249v1 [cs.CL])","link":"http://arxiv.org/abs/2309.13249","description":"<p>Document-level information extraction (IE) is a crucial task in natural\nlanguage processing (NLP). This paper conducts a systematic review of recent\ndocument-level IE literature. In addition, we conduct a thorough error analysis\nwith current state-of-the-art algorithms and identify their limitations as well\nas the remaining challenges for the task of document-level IE. According to our\nfindings, labeling noises, entity coreference resolution, and lack of\nreasoning, severely affect the performance of document-level IE. The objective\nof this survey paper is to provide more insights and help NLP researchers to\nfurther enhance document-level IE performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hanwen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sijia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Processing for Requirements Formalization: How to Derive New Approaches?. (arXiv:2309.13272v1 [cs.SE])","link":"http://arxiv.org/abs/2309.13272","description":"<p>It is a long-standing desire of industry and research to automate the\nsoftware development and testing process as much as possible. In this process,\nrequirements engineering (RE) plays a fundamental role for all other steps that\nbuild on it. Model-based design and testing methods have been developed to\nhandle the growing complexity and variability of software systems. However,\nmajor effort is still required to create specification models from a large set\nof functional requirements provided in natural language. Numerous approaches\nbased on natural language processing (NLP) have been proposed in the literature\nto generate requirements models using mainly syntactic properties. Recent\nadvances in NLP show that semantic quantities can also be identified and used\nto provide better assistance in the requirements formalization process. In this\nwork, we present and discuss principal ideas and state-of-the-art methodologies\nfrom the field of NLP in order to guide the readers on how to create a set of\nrules and methods for the semi-automated formalization of requirements\naccording to their specific use case and needs. We discuss two different\napproaches in detail and highlight the iterative development of rule sets. The\nrequirements models are represented in a human- and machine-readable format in\nthe form of pseudocode. The presented methods are demonstrated on two\nindustrial use cases from the automotive and railway domains. It shows that\nusing current pre-trained NLP models requires less effort to create a set of\nrules and can be easily adapted to specific use cases and domains. In addition,\nfindings and shortcomings of this research area are highlighted and an outlook\non possible future developments is given.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sudhi_V/0/1/0/all/0/1\">Viju Sudhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutty_L/0/1/0/all/0/1\">Libin Kutty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gropler_R/0/1/0/all/0/1\">Robin Gr&#xf6;pler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OATS: Opinion Aspect Target Sentiment Quadruple Extraction Dataset for Aspect-Based Sentiment Analysis. (arXiv:2309.13297v1 [cs.CL])","link":"http://arxiv.org/abs/2309.13297","description":"<p>Aspect-based sentiment Analysis (ABSA) delves into understanding sentiments\nspecific to distinct elements within textual content. It aims to analyze\nuser-generated reviews to determine a) the target entity being reviewed, b) the\nhigh-level aspect to which it belongs, c) the sentiment words used to express\nthe opinion, and d) the sentiment expressed toward the targets and the aspects.\nWhile various benchmark datasets have fostered advancements in ABSA, they often\ncome with domain limitations and data granularity challenges. Addressing these,\nwe introduce the OATS dataset, which encompasses three fresh domains and\nconsists of 20,000 sentence-level quadruples and 13,000 review-level tuples.\nOur initiative seeks to bridge specific observed gaps: the recurrent focus on\nfamiliar domains like restaurants and laptops, limited data for intricate\nquadruple extraction tasks, and an occasional oversight of the synergy between\nsentence and review-level sentiments. Moreover, to elucidate OATS's potential\nand shed light on various ABSA subtasks that OATS can solve, we conducted\nin-domain and cross-domain experiments, establishing initial baselines. We hope\nthe OATS dataset augments current resources, paving the way for an encompassing\nexploration of ABSA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chebolu_S/0/1/0/all/0/1\">Siva Uday Sampreeth Chebolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1\">Nedim Lipka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solorio_T/0/1/0/all/0/1\">Thamar Solorio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibrating LLM-Based Evaluator. (arXiv:2309.13308v1 [cs.CL])","link":"http://arxiv.org/abs/2309.13308","description":"<p>Recent advancements in large language models (LLMs) on language modeling and\nemergent capabilities make them a promising reference-free evaluator of natural\nlanguage generation quality, and a competent alternative to human evaluation.\nHowever, hindered by the closed-source or high computational demand to host and\ntune, there is a lack of practice to further calibrate an off-the-shelf\nLLM-based evaluator towards better human alignment. In this work, we propose\nAutoCalibrate, a multi-stage, gradient-free approach to automatically calibrate\nand align an LLM-based evaluator toward human preference. Instead of explicitly\nmodeling human preferences, we first implicitly encompass them within a set of\nhuman labels. Then, an initial set of scoring criteria is drafted by the\nlanguage model itself, leveraging in-context learning on different few-shot\nexamples. To further calibrate this set of criteria, we select the best\nperformers and re-draft them with self-refinement. Our experiments on multiple\ntext quality evaluation datasets illustrate a significant improvement in\ncorrelation with expert evaluation through calibration. Our comprehensive\nqualitative analysis conveys insightful intuitions and observations on the\nessence of effective scoring criteria.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuxuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianchi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haizhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Weiwei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Feng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BLM-17m: A Large-Scale Dataset for Black Lives Matter Topic Detection on Twitter. (arXiv:2105.01331v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.01331","description":"<p>Protection of human rights is one of the most important problems of our\nworld. In this paper, our aim is to provide a dataset which covers one of the\nmost significant human rights contradiction in recent months affected the whole\nworld, George Floyd incident. We propose a labeled dataset for topic detection\nthat contains 17 million tweets. These Tweets are collected from 25 May 2020 to\n21 August 2020 that covers 89 days from start of this incident. We labeled the\ndataset by monitoring most trending news topics from global and local\nnewspapers. Apart from that, we present two baselines, TF-IDF and LDA. We\nevaluated the results of these two methods with three different k values for\nmetrics of precision, recall and f1-score. The collected dataset is available\nat https://github.com/MeysamAsgariC/BLMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kemik_H/0/1/0/all/0/1\">Hasan Kemik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozates_N/0/1/0/all/0/1\">Nusret &#xd6;zate&#x15f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asgari_Chenaghlu_M/0/1/0/all/0/1\">Meysam Asgari-Chenaghlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crime Hot-Spot Modeling via Topic Modeling and Relative Density Estimation. (arXiv:2202.04176v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.04176","description":"<p>We present a method to capture groupings of similar calls and determine their\nrelative spatial distribution from a collection of crime record narratives. We\nfirst obtain a topic distribution for each narrative, and then propose a\nnearest neighbors relative density estimation (kNN-RDE) approach to obtain\nspatial relative densities per topic. Experiments over a large corpus\n($n=475,019$) of narrative documents from the Atlanta Police Department\ndemonstrate the viability of our method in capturing geographic hot-spot trends\nwhich call dispatchers do not initially pick up on and which go unnoticed due\nto conflation with elevated event density in general.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jonathan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huestis_Mitchell_S/0/1/0/all/0/1\">Sarah Huestis-Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiuyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yao Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Actuarial Applications of Natural Language Processing Using Transformers: Case Studies for Using Text Features in an Actuarial Context. (arXiv:2206.02014v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.02014","description":"<p>This tutorial demonstrates workflows to incorporate text data into actuarial\nclassification and regression tasks. The main focus is on methods employing\ntransformer-based models. A dataset of car accident descriptions with an\naverage length of 400 words, available in English and German, and a dataset\nwith short property insurance claims descriptions are used to demonstrate these\ntechniques. The case studies tackle challenges related to a multi-lingual\nsetting and long input sequences. They also show ways to interpret model\noutput, to assess and improve model performance, by fine-tuning the models to\nthe domain of application or to a specific prediction task. Finally, the\ntutorial provides practical approaches to handle classification tasks in\nsituations with no or only few labeled data, including but not limited to\nChatGPT. The results achieved by using the language-understanding skills of\noff-the-shelf natural language processing (NLP) models with only minimal\npre-processing and fine-tuning clearly demonstrate the power of transfer\nlearning for practical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Troxler_A/0/1/0/all/0/1\">Andreas Troxler</a> (AT Analytics), <a href=\"http://arxiv.org/find/cs/1/au:+Schelldorfer_J/0/1/0/all/0/1\">J&#xfc;rg Schelldorfer</a> (Swiss Re)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable and High-Performance Hate and Offensive Speech Detection. (arXiv:2206.12983v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.12983","description":"<p>The spread of information through social media platforms can create\nenvironments possibly hostile to vulnerable communities and silence certain\ngroups in society. To mitigate such instances, several models have been\ndeveloped to detect hate and offensive speech. Since detecting hate and\noffensive speech in social media platforms could incorrectly exclude\nindividuals from social media platforms, which can reduce trust, there is a\nneed to create explainable and interpretable models. Thus, we build an\nexplainable and interpretable high performance model based on the XGBoost\nalgorithm, trained on Twitter data. For unbalanced Twitter data, XGboost\noutperformed the LSTM, AutoGluon, and ULMFiT models on hate speech detection\nwith an F1 score of 0.75 compared to 0.38 and 0.37, and 0.38 respectively. When\nwe down-sampled the data to three separate classes of approximately 5000\ntweets, XGBoost performed better than LSTM, AutoGluon, and ULMFiT; with F1\nscores for hate speech detection of 0.79 vs 0.69, 0.77, and 0.66 respectively.\nXGBoost also performed better than LSTM, AutoGluon, and ULMFiT in the\ndown-sampled version for offensive speech detection with F1 score of 0.83 vs\n0.88, 0.82, and 0.79 respectively. We use Shapley Additive Explanations (SHAP)\non our XGBoost models' outputs to makes it explainable and interpretable\ncompared to LSTM, AutoGluon and ULMFiT that are black-box models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Babaeianjelodar_M/0/1/0/all/0/1\">Marzieh Babaeianjelodar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prudhvi_G/0/1/0/all/0/1\">Gurram Poorna Prudhvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenz_S/0/1/0/all/0/1\">Stephen Lorenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Keyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondal_S/0/1/0/all/0/1\">Sumona Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1\">Soumyabrata Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1\">Navin Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CC-Riddle: A Question Answering Dataset of Chinese Character Riddles. (arXiv:2206.13778v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.13778","description":"<p>The Chinese character riddle is a unique form of cultural entertainment\nspecific to the Chinese language. It typically comprises two parts: the riddle\ndescription and the solution. The solution to the riddle is a single character,\nwhile the riddle description primarily describes the glyph of the solution,\noccasionally supplemented with its explanation and pronunciation. Solving\nChinese character riddles is a challenging task that demands understanding of\ncharacter glyph, general knowledge, and a grasp of figurative language. In this\npaper, we construct a \\textbf{C}hinese \\textbf{C}haracter riddle dataset named\nCC-Riddle, which covers the majority of common simplified Chinese characters.\nThe construction process is a combination of web crawling, language model\ngeneration and manual filtering. In generation stage, we input the Chinese\nphonetic alphabet, glyph and meaning of the solution character into the\ngeneration model, which then produces multiple riddle descriptions. The\ngenerated riddles are then manually filtered and the final CC-Riddle dataset is\ncomposed of both human-written riddles and these filtered, generated riddles.\nIn order to assess the performance of language models on the task of solving\ncharacter riddles, we use retrieval-based, generative and multiple-choice QA\nstrategies to test three language models: BERT, ChatGPT and ChatGLM. The test\nresults reveal that current language models still struggle to solve Chinese\ncharacter riddles. CC-Riddle is publicly available at\n\\url{https://github.com/pku0xff/CC-Riddle}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control. (arXiv:2211.05750v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05750","description":"<p>Pretrained language models have demonstrated extraordinary capabilities in\nlanguage generation. However, real-world tasks often require controlling the\ndistribution of generated text in order to mitigate bias, promote fairness, and\nachieve personalization. Existing techniques for controlling the distribution\nof generated text only work with quantified distributions, which require\npre-defined categories, proportions of the distribution, or an existing corpus\nfollowing the desired distributions. However, many important distributions,\nsuch as personal preferences, are unquantified. In this work, we tackle the\nproblem of generating text following arbitrary distributions (quantified and\nunquantified) by proposing Nano, a few-shot human-in-the-loop training\nalgorithm that continuously learns from human feedback. Nano achieves\nstate-of-the-art results on single topic/attribute as well as quantified\ndistribution control compared to previous works. We also show that Nano is able\nto learn unquantified distributions, achieves personalization, and captures\ndifferences between different individuals' personal preferences with high\nsample efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust and Explainable Identification of Logical Fallacies in Natural Language Arguments. (arXiv:2212.07425v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.07425","description":"<p>The spread of misinformation, propaganda, and flawed argumentation has been\namplified in the Internet era. Given the volume of data and the subtlety of\nidentifying violations of argumentation norms, supporting information analytics\ntasks, like content moderation, with trustworthy methods that can identify\nlogical fallacies is essential. In this paper, we formalize prior theoretical\nwork on logical fallacies into a comprehensive three-stage evaluation framework\nof detection, coarse-grained, and fine-grained classification. We adapt\nexisting evaluation datasets for each stage of the evaluation. We employ three\nfamilies of robust and explainable methods based on prototype reasoning,\ninstance-based reasoning, and knowledge injection. The methods combine language\nmodels with background knowledge and explainable mechanisms. Moreover, we\naddress data sparsity with strategies for data augmentation and curriculum\nlearning. Our three-stage framework natively consolidates prior datasets and\nmethods from existing tasks, like propaganda detection, serving as an\noverarching evaluation testbed. We extensively evaluate these methods on our\ndatasets, focusing on their robustness and explainability. Our results provide\ninsight into the strengths and weaknesses of the methods on different\ncomponents and fallacy classes, indicating that fallacy identification is a\nchallenging task that may require specialized forms of reasoning to capture\nvarious classes. We share our open-source code and data on GitHub to support\nfurther work on logical fallacy identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sourati_Z/0/1/0/all/0/1\">Zhivar Sourati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_V/0/1/0/all/0/1\">Vishnu Priya Prasanna Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_D/0/1/0/all/0/1\">Darshan Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawlani_H/0/1/0/all/0/1\">Himanshu Rawlani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandlin_H/0/1/0/all/0/1\">H&#xf4;ng-&#xc2;n Sandlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mermoud_A/0/1/0/all/0/1\">Alain Mermoud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A comprehensive review of automatic text summarization techniques: method, data, evaluation and coding. (arXiv:2301.03403v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.03403","description":"<p>We provide a literature review about Automatic Text Summarization (ATS)\nsystems. We consider a citation-based approach. We start with some popular and\nwell-known papers that we have in hand about each topic we want to cover and we\nhave tracked the \"backward citations\" (papers that are cited by the set of\npapers we knew beforehand) and the \"forward citations\" (newer papers that cite\nthe set of papers we knew beforehand). In order to organize the different\nmethods, we present the diverse approaches to ATS guided by the mechanisms they\nuse to generate a summary. Besides presenting the methods, we also present an\nextensive review of the datasets available for summarization tasks and the\nmethods used to evaluate the quality of the summaries. Finally, we present an\nempirical exploration of these methods using the CNN Corpus dataset that\nprovides golden summaries for extractive and abstractive methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cajueiro_D/0/1/0/all/0/1\">Daniel O. Cajueiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nery_A/0/1/0/all/0/1\">Arthur G. Nery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavares_I/0/1/0/all/0/1\">Igor Tavares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_M/0/1/0/all/0/1\">Ma&#xed;sa K. De Melo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reis_S/0/1/0/all/0/1\">Silvia A. dos Reis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weigang_L/0/1/0/all/0/1\">Li Weigang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celestino_V/0/1/0/all/0/1\">Victor R. R. Celestino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data. (arXiv:2301.05843v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2301.05843","description":"<p>Large language models (LLMs) provide a new way to build chatbots by accepting\nnatural language prompts. Yet, it is unclear how to design prompts to power\nchatbots to carry on naturalistic conversations while pursuing a given goal,\nsuch as collecting self-report data from users. We explore what design factors\nof prompts can help steer chatbots to talk naturally and collect data reliably.\nTo this aim, we formulated four prompt designs with different structures and\npersonas. Through an online study (N = 48) where participants conversed with\nchatbots driven by different designs of prompts, we assessed how prompt designs\nand conversation topics affected the conversation flows and users' perceptions\nof chatbots. Our chatbots covered 79% of the desired information slots during\nconversations, and the designs of prompts and topics significantly influenced\nthe conversation flows and the data collection performance. We discuss the\nopportunities and challenges of building chatbots with LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jing Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungdong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">Hyunhoon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Ho Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HL Dataset: Visually-grounded Description of Scenes, Actions and Rationales. (arXiv:2302.12189v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.12189","description":"<p>Current captioning datasets focus on object-centric captions, describing the\nvisible objects in the image, e.g. \"people eating food in a park\". Although\nthese datasets are useful to evaluate the ability of Vision &amp; Language models\nto recognize and describe visual content, they do not support controlled\nexperiments involving model testing or fine-tuning, with more high-level\ncaptions, which humans find easy and natural to produce. For example, people\noften describe images based on the type of scene they depict ('people at a\nholiday resort') and the actions they perform ('people having a picnic'). Such\ndescriptions draw on personal experience and commonsense assumptions. We\npresent the High-Level Dataset a dataset extending 14997 images from the COCO\ndataset, aligned with a new set of 134,973 human-annotated (high-level)\ncaptions collected along three axes: scenes, actions, and rationales. We\nfurther extend this dataset with confidence scores collected from an\nindependent set of readers, as well as a set of narrative captions generated\nsynthetically, by combining each of the three axes. We describe this dataset\nand analyse it extensively. We also present baseline results for the High-Level\nCaptioning task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cafagna_M/0/1/0/all/0/1\">Michele Cafagna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deemter_K/0/1/0/all/0/1\">Kees van Deemter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1\">Albert Gatt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages. (arXiv:2303.01037v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.01037","description":"<p>We introduce the Universal Speech Model (USM), a single large model that\nperforms automatic speech recognition (ASR) across 100+ languages. This is\nachieved by pre-training the encoder of the model on a large unlabeled\nmultilingual dataset of 12 million (M) hours spanning over 300 languages, and\nfine-tuning on a smaller labeled dataset. We use multilingual pre-training with\nrandom-projection quantization and speech-text modality matching to achieve\nstate-of-the-art performance on downstream multilingual ASR and speech-to-text\ntranslation tasks. We also demonstrate that despite using a labeled training\nset 1/7-th the size of that used for the Whisper model, our model exhibits\ncomparable or better performance on both in-domain and out-of-domain speech\nrecognition tasks across many languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">James Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhehuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nanxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Axelrod_V/0/1/0/all/0/1\">Vera Axelrod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gary Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Ke Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_A/0/1/0/all/0/1\">Andrew Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhavalkar_R/0/1/0/all/0/1\">Rohit Prabhavalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Daniel S. Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haghani_P/0/1/0/all/0/1\">Parisa Haghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riesa_J/0/1/0/all/0/1\">Jason Riesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perng_G/0/1/0/all/0/1\">Ginger Perng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltau_H/0/1/0/all/0/1\">Hagen Soltau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramabhadran_B/0/1/0/all/0/1\">Bhuvana Ramabhadran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_P/0/1/0/all/0/1\">Pedro Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1\">Chung-Cheng Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schalkwyk_J/0/1/0/all/0/1\">Johan Schalkwyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Text Generation with Cooperative Training. (arXiv:2303.09075v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.09075","description":"<p>Recently, there has been a surge in the use of generated data to enhance the\nperformance of downstream models, largely due to the advancements in\npre-trained language models. However, most prevailing methods trained\ngenerative and discriminative models in isolation, which left them unable to\nadapt to changes in each other. These approaches lead to generative models that\nare prone to deviating from the true data distribution and providing limited\nbenefits to discriminative models. While some works have proposed jointly\ntraining generative and discriminative language models, their methods remain\nchallenging due to the non-differentiable nature of discrete data. To overcome\nthese issues, we introduce a \\textit{self-consistent learning} framework in the\ntext field that involves training a discriminator and generator cooperatively\nin a closed-loop manner until a scoring consensus is reached. By learning\ndirectly from selected samples, our framework are able to mitigate training\ninstabilities such as mode collapse and non-convergence. Extensive experiments\non four downstream benchmarks, including AFQMC, CHIP-STS, QQP, and MRPC,\ndemonstrate the efficacy of the proposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhongshen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-Guided Audio-Visual Source Separation via Trimodal Consistency. (arXiv:2303.16342v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2303.16342","description":"<p>We propose a self-supervised approach for learning to perform audio source\nseparation in videos based on natural language queries, using only unlabeled\nvideo and audio pairs as training data. A key challenge in this task is\nlearning to associate the linguistic description of a sound-emitting object to\nits visual features and the corresponding components of the audio waveform, all\nwithout access to annotations during training. To overcome this challenge, we\nadapt off-the-shelf vision-language foundation models to provide pseudo-target\nsupervision via two novel loss functions and encourage a stronger alignment\nbetween the audio, visual and natural language modalities. During inference,\nour approach can separate sounds given text, video and audio input, or given\ntext and audio input alone. We demonstrate the effectiveness of our\nself-supervised approach on three audio-visual separation datasets, including\nMUSIC, SOLOS and AudioSet, where we outperform state-of-the-art strongly\nsupervised approaches despite not using object detectors or text labels during\ntraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1\">Reuben Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1\">Arijit Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burns_A/0/1/0/all/0/1\">Andrea Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1\">Bryan A. Plummer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salamon_J/0/1/0/all/0/1\">Justin Salamon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nieto_O/0/1/0/all/0/1\">Oriol Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_B/0/1/0/all/0/1\">Bryan Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Textual Inversion for Personalized Text-to-Image Generation. (arXiv:2304.05265v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2304.05265","description":"<p>The recent large-scale generative modeling has attained unprecedented\nperformance especially in producing high-fidelity images driven by text\nprompts. Text inversion (TI), alongside the text-to-image model backbones, is\nproposed as an effective technique in personalizing the generation when the\nprompts contain user-defined, unseen or long-tail concept tokens. Despite that,\nwe find and show that the deployment of TI remains full of \"dark-magics\" -- to\nname a few, the harsh requirement of additional datasets, arduous human efforts\nin the loop and lack of robustness. In this work, we propose a much-enhanced\nversion of TI, dubbed Controllable Textual Inversion (COTI), in resolving all\nthe aforementioned problems and in turn delivering a robust, data-efficient and\neasy-to-use framework. The core to COTI is a theoretically-guided loss\nobjective instantiated with a comprehensive and novel weighted scoring\nmechanism, encapsulated by an active-learning paradigm. The extensive results\nshow that COTI significantly outperforms the prior TI-related approaches with a\n26.05 decrease in the FID score and a 23.00% boost in the R-precision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haobo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_R/0/1/0/all/0/1\">Ruixuan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Sai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Gang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junbo Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArguGPT: evaluating, understanding and identifying argumentative essays generated by GPT models. (arXiv:2304.07666v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.07666","description":"<p>AI generated content (AIGC) presents considerable challenge to educators\naround the world. Instructors need to be able to detect such text generated by\nlarge language models, either with the naked eye or with the help of some\ntools. There is also growing need to understand the lexical, syntactic and\nstylistic features of AIGC. To address these challenges in English language\nteaching, we first present ArguGPT, a balanced corpus of 4,038 argumentative\nessays generated by 7 GPT models in response to essay prompts from three\nsources: (1) in-class or homework exercises, (2) TOEFL and (3) GRE writing\ntasks. Machine-generated texts are paired with roughly equal number of\nhuman-written essays with three score levels matched in essay prompts. We then\nhire English instructors to distinguish machine essays from human ones. Results\nshow that when first exposed to machine-generated essays, the instructors only\nhave an accuracy of 61% in detecting them. But the number rises to 67% after\none round of minimal self-training. Next, we perform linguistic analyses of\nthese essays, which show that machines produce sentences with more complex\nsyntactic structures while human essays tend to be lexically more complex.\nFinally, we test existing AIGC detectors and build our own detectors using SVMs\nand RoBERTa. Results suggest that a RoBERTa fine-tuned with the training set of\nArguGPT achieves above 90% accuracy in both essay- and sentence-level\nclassification. To the best of our knowledge, this is the first comprehensive\nanalysis of argumentative essays produced by generative large language models.\nMachine-authored essays in ArguGPT and our models will be made publicly\navailable at https://github.com/huhailinguist/ArguGPT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yikang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziyin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wanyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1\">Shisen Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaojing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xinyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hai Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GenQ: Automated Question Generation to Support Caregivers While Reading Stories with Children. (arXiv:2305.16809v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16809","description":"<p>When caregivers ask open--ended questions to motivate dialogue with children,\nit facilitates the child's reading comprehension skills.Although there is scope\nfor use of technological tools, referred here as \"intelligent tutoring\nsystems\", to scaffold this process, it is currently unclear whether existing\nintelligent systems that generate human--language like questions is beneficial.\nAdditionally, training data used in the development of these automated question\ngeneration systems is typically sourced without attention to demographics, but\npeople with different cultural backgrounds may ask different questions. As a\npart of a broader project to design an intelligent reading support app for\nLatinx children, we crowdsourced questions from Latinx caregivers and\nnoncaregivers as well as caregivers and noncaregivers from other demographics.\nWe examine variations in question--asking within this dataset mediated by\nindividual, cultural, and contextual factors. We then design a system that\nautomatically extracts templates from this data to generate open--ended\nquestions that are representative of those asked by Latinx caregivers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_A/0/1/0/all/0/1\">Arun Balajiee Lekshmi Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_L/0/1/0/all/0/1\">Ligia E. Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_M/0/1/0/all/0/1\">Martha Michelle Soto Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tri Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blais_C/0/1/0/all/0/1\">Chris Blais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Restrepo_M/0/1/0/all/0/1\">M. Adelaida Restrepo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glenberg_A/0/1/0/all/0/1\">Art Glenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Auxiliary Domain Parallel Data in Intermediate Task Fine-tuning for Low-resource Translation. (arXiv:2306.01382v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.01382","description":"<p>NMT systems trained on Pre-trained Multilingual Sequence-Sequence (PMSS)\nmodels flounder when sufficient amounts of parallel data is not available for\nfine-tuning. This specifically holds for languages missing/under-represented in\nthese models. The problem gets aggravated when the data comes from different\ndomains. In this paper, we show that intermediate-task fine-tuning (ITFT) of\nPMSS models is extremely beneficial for domain-specific NMT, especially when\ntarget domain data is limited/unavailable and the considered languages are\nmissing or under-represented in the PMSS model. We quantify the domain-specific\nresults variations using a domain-divergence test, and show that ITFT can\nmitigate the impact of domain divergence to some extent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_S/0/1/0/all/0/1\">Shravan Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranathunga_S/0/1/0/all/0/1\">Surangika Ranathunga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thillainathan_S/0/1/0/all/0/1\">Sarubi Thillainathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_R/0/1/0/all/0/1\">Rikki Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rinaldi_A/0/1/0/all/0/1\">Anthony Rinaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yining Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mackey_J/0/1/0/all/0/1\">Jonah Mackey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_A/0/1/0/all/0/1\">Andrew Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1\">En-Shiun Annie Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D-Speaker: A Large-Scale Multi-Device, Multi-Distance, and Multi-Dialect Corpus for Speech Representation Disentanglement. (arXiv:2306.15354v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.15354","description":"<p>Disentangling uncorrelated information in speech utterances is a crucial\nresearch topic within speech community. Different speech-related tasks focus on\nextracting distinct speech representations while minimizing the affects of\nother uncorrelated information. We present a large-scale speech corpus to\nfacilitate the research of speech representation disentanglement. 3D-Speaker\ncontains over 10,000 speakers, each of whom are simultaneously recorded by\nmultiple Devices, locating at different Distances, and some speakers are\nspeaking multiple Dialects. The controlled combinations of multi-dimensional\naudio data yield a matrix of a diverse blend of speech representation\nentanglement, thereby motivating intriguing methods to untangle them. The\nmulti-domain nature of 3D-Speaker also makes it a suitable resource to evaluate\nlarge universal speech models and experiment methods of out-of-domain learning\nand self-supervised learning. https://3dspeaker.github.io/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Siqi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Luyao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yafeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care. (arXiv:2307.01458v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.01458","description":"<p>The recent advances in natural language processing (NLP), have led to a new\ntrend of applying large language models (LLMs) to real-world scenarios. While\nthe latest LLMs are astonishingly fluent when interacting with humans, they\nsuffer from the misinformation problem by unintentionally generating factually\nfalse statements. This can lead to harmful consequences, especially when\nproduced within sensitive contexts, such as healthcare. Yet few previous works\nhave focused on evaluating misinformation in the long-form (LF) generation of\nLLMs, especially for knowledge-intensive topics. Moreover, although LLMs have\nbeen shown to perform well in different languages, misinformation evaluation\nhas been mostly conducted in English. To this end, we present a benchmark,\nCARE-MI, for evaluating LLM misinformation in: 1) a sensitive topic,\nspecifically the maternity and infant care domain; and 2) a language other than\nEnglish, namely Chinese. Most importantly, we provide an innovative paradigm\nfor building LF generation evaluation benchmarks that can be transferred to\nother knowledge-intensive domains and low-resourced languages. Our proposed\nbenchmark fills the gap between the extensive usage of LLMs and the lack of\ndatasets for assessing the misinformation generated by these models. It\ncontains 1,612 expert-checked questions, accompanied with human-selected\nreferences. Using our benchmark, we conduct extensive experiments and found\nthat current Chinese LLMs are far from perfect in the topic of maternity and\ninfant care. In an effort to minimize the reliance on human resources for\nperformance evaluation, we offer off-the-shelf judgment models for\nautomatically assessing the LF output of LLMs given benchmark questions.\nMoreover, we compare potential solutions for LF generation evaluation and\nprovide insights for building better automated metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tong Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wangyue Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_M/0/1/0/all/0/1\">Mingbai Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Lu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bowen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1\">Noa Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Domain Adaptation of Sentence Embeddings Using Adapters. (arXiv:2307.03104v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.03104","description":"<p>Sentence embeddings enable us to capture the semantic similarity of short\ntexts. Most sentence embedding models are trained for general semantic textual\nsimilarity tasks. Therefore, to use sentence embeddings in a particular domain,\nthe model must be adapted to it in order to achieve good results. Usually, this\nis done by fine-tuning the entire sentence embedding model for the domain of\ninterest. While this approach yields state-of-the-art results, all of the\nmodel's weights are updated during fine-tuning, making this method\nresource-intensive. Therefore, instead of fine-tuning entire sentence embedding\nmodels for each target domain individually, we propose to train lightweight\nadapters. These domain-specific adapters do not require fine-tuning all\nunderlying sentence embedding model parameters. Instead, we only train a small\nnumber of additional parameters while keeping the weights of the underlying\nsentence embedding model fixed. Training domain-specific adapters allows always\nusing the same base model and only exchanging the domain-specific adapters to\nadapt sentence embeddings to a specific domain. We show that using adapters for\nparameter-efficient domain adaptation of sentence embeddings yields competitive\nperformance within 1% of a domain-adapted, entirely fine-tuned sentence\nembedding model while only training approximately 3.6% of the parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schopf_T/0/1/0/all/0/1\">Tim Schopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_D/0/1/0/all/0/1\">Dennis N. Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthes_F/0/1/0/all/0/1\">Florian Matthes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Named entity recognition using GPT for identifying comparable companies. (arXiv:2307.07420v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.07420","description":"<p>For both public and private firms, comparable companies' analysis is widely\nused as a method for company valuation. In particular, the method is of great\nvalue for valuation of private equity companies. The several approaches to the\ncomparable companies' method usually rely on a qualitative approach to\nidentifying similar peer companies, which tend to use established industry\nclassification schemes and/or analyst intuition and knowledge. However, more\nquantitative methods have started being used in the literature and in the\nprivate equity industry, in particular, machine learning clustering, and\nnatural language processing (NLP). For NLP methods, the process consists of\nextracting product entities from e.g., the company's website or company\ndescriptions from some financial database system and then to perform similarity\nanalysis. Here, using companies' descriptions/summaries from publicly available\ncompanies' Wikipedia websites, we show that using large language models (LLMs),\nsuch as GPT from OpenAI, has a much higher precision and success rate than\nusing the standard named entity recognition (NER) methods which use manual\nannotation. We demonstrate quantitatively a higher precision rate, and show\nthat, qualitatively, it can be used to create appropriate comparable companies\npeer groups which could then be used for equity valuation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Covas_E/0/1/0/all/0/1\">Eurico Covas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AspectCSE: Sentence Embeddings for Aspect-based Semantic Textual Similarity Using Contrastive Learning and Structured Knowledge. (arXiv:2307.07851v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.07851","description":"<p>Generic sentence embeddings provide a coarse-grained approximation of\nsemantic textual similarity but ignore specific aspects that make texts\nsimilar. Conversely, aspect-based sentence embeddings provide similarities\nbetween texts based on certain predefined aspects. Thus, similarity predictions\nof texts are more targeted to specific requirements and more easily\nexplainable. In this paper, we present AspectCSE, an approach for aspect-based\ncontrastive learning of sentence embeddings. Results indicate that AspectCSE\nachieves an average improvement of 3.97% on information retrieval tasks across\nmultiple aspects compared to the previous best results. We also propose using\nWikidata knowledge graph properties to train models of multi-aspect sentence\nembeddings in which multiple specific aspects are simultaneously considered\nduring similarity predictions. We demonstrate that multi-aspect embeddings\noutperform single-aspect embeddings on aspect-specific information retrieval\ntasks. Finally, we examine the aspect-based sentence embedding space and\ndemonstrate that embeddings of semantically similar aspect labels are often\nclose, even without explicit similarity training between different aspect\nlabels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schopf_T/0/1/0/all/0/1\">Tim Schopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerber_E/0/1/0/all/0/1\">Emanuel Gerber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostendorff_M/0/1/0/all/0/1\">Malte Ostendorff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthes_F/0/1/0/all/0/1\">Florian Matthes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Landscape of Natural Language Processing Research. (arXiv:2307.10652v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.10652","description":"<p>As an efficient approach to understand, generate, and process natural\nlanguage texts, research in natural language processing (NLP) has exhibited a\nrapid spread and wide adoption in recent years. Given the increasing research\nwork in this area, several NLP-related approaches have been surveyed in the\nresearch community. However, a comprehensive study that categorizes established\ntopics, identifies trends, and outlines areas for future research remains\nabsent. Contributing to closing this gap, we have systematically classified and\nanalyzed research papers in the ACL Anthology. As a result, we present a\nstructured overview of the research landscape, provide a taxonomy of fields of\nstudy in NLP, analyze recent developments in NLP, summarize our findings, and\nhighlight directions for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schopf_T/0/1/0/all/0/1\">Tim Schopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arabi_K/0/1/0/all/0/1\">Karim Arabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthes_F/0/1/0/all/0/1\">Florian Matthes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MASR: Multi-label Aware Speech Representation. (arXiv:2307.10982v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2307.10982","description":"<p>In the recent years, speech representation learning is constructed primarily\nas a self-supervised learning (SSL) task, using the raw audio signal alone,\nwhile ignoring the side-information that is often available for a given speech\nrecording. In this paper, we propose MASR, a Multi-label Aware Speech\nRepresentation learning framework, which addresses the aforementioned\nlimitations. MASR enables the inclusion of multiple external knowledge sources\nto enhance the utilization of meta-data information. The external knowledge\nsources are incorporated in the form of sample-level pair-wise similarity\nmatrices that are useful in a hard-mining loss. A key advantage of the MASR\nframework is that it can be combined with any choice of SSL method. Using MASR\nrepresentations, we perform evaluations on several downstream tasks such as\nlanguage identification, speech recognition and other non-semantic tasks such\nas speaker and emotion recognition. In these experiments, we illustrate\nsignificant performance improvements for the MASR over other established\nbenchmarks. We perform a detailed analysis on the language identification task\nto provide insights on how the proposed loss function enables the\nrepresentations to separate closely related languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raj_A/0/1/0/all/0/1\">Anjali Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_S/0/1/0/all/0/1\">Shikhar Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganapathy_S/0/1/0/all/0/1\">Sriram Ganapathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Min Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vashishth_S/0/1/0/all/0/1\">Shikhar Vashishth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenging the Machinery of Generative AI with Fact-Checking: Ontology-Driven Biological Graphs for Verifying Human Disease-Gene Links. (arXiv:2308.03929v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2308.03929","description":"<p>Background: Since the launch of various generative AI tools, scientists have\nbeen striving to evaluate their capabilities and contents, in the hope of\nestablishing trust in their generative abilities. Regulations and guidelines\nare emerging to verify generated contents and identify novel uses. Objective:\nwe aspire to demonstrate how ChatGPT claims are checked computationally using\nthe rigor of network models. We aim to achieve fact-checking of the knowledge\nembedded in biological graphs that were contrived from ChatGPT contents at the\naggregate level. Methods: We adopted a biological networks approach that\nenables the systematic interrogation of ChatGPT's linked entities. We designed\nan ontology-driven fact-checking algorithm that compares biological graphs\nconstructed from approximately 200,000 PubMed abstracts with counterparts\nconstructed from a dataset generated using the ChatGPT-3.5 Turbo model.\nResults: in 10-samples of 250 randomly selected records a ChatGPT dataset of\n1000 \"simulated\" articles, the fact-checking link accuracy ranged from 70% to\n86%. The computational process was followed by a manual process using IntAct\nInteraction database and the Gene regulatory network database (GRNdb) to\nconfirm the validity of the links identified computationally. We also found\nthat the proximity of the edges of ChatGPT graphs were significantly shorter\n(90 -- 153) while literature distances were (236 -- 765). This pattern held\ntrue in all 10-samples. Conclusion: This study demonstrated high accuracy of\naggregate disease-gene links relationships found in ChatGPT-generated texts.\nThe strikingly consistent pattern offers an illuminate new biological pathways\nthat may open the door for new research opportunities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamed_A/0/1/0/all/0/1\">Ahmed Abdeen Hamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Byung Suk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crimi_A/0/1/0/all/0/1\">Alessandro Crimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misiak_M/0/1/0/all/0/1\">Magdalena M. Misiak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Objective Evaluation of Socially-Situated Conversational Robots: Assessing Human-Likeness through Multimodal User Behaviors. (arXiv:2308.11020v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.11020","description":"<p>This paper tackles the challenging task of evaluating socially situated\nconversational robots and presents a novel objective evaluation approach that\nrelies on multimodal user behaviors. In this study, our main focus is on\nassessing the human-likeness of the robot as the primary evaluation metric.\nWhile previous research often relied on subjective evaluations from users, our\napproach aims to evaluate the robot's human-likeness based on observable user\nbehaviors indirectly, thus enhancing objectivity and reproducibility. To begin,\nwe created an annotated dataset of human-likeness scores, utilizing user\nbehaviors found in an attentive listening dialogue corpus. We then conducted an\nanalysis to determine the correlation between multimodal user behaviors and\nhuman-likeness scores, demonstrating the feasibility of our proposed\nbehavior-based evaluation method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inoue_K/0/1/0/all/0/1\">Koji Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lala_D/0/1/0/all/0/1\">Divesh Lala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochi_K/0/1/0/all/0/1\">Keiko Ochi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawahara_T/0/1/0/all/0/1\">Tatsuya Kawahara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skantze_G/0/1/0/all/0/1\">Gabriel Skantze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Style Transfer Evaluation Using Large Language Models. (arXiv:2308.13577v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.13577","description":"<p>Evaluating Text Style Transfer (TST) is a complex task due to its\nmultifaceted nature. The quality of the generated text is measured based on\nchallenging factors, such as style transfer accuracy, content preservation, and\noverall fluency. While human evaluation is considered to be the gold standard\nin TST assessment, it is costly and often hard to reproduce. Therefore,\nautomated metrics are prevalent in these domains. Nevertheless, it remains\nunclear whether these automated metrics correlate with human evaluations.\nRecent strides in Large Language Models (LLMs) have showcased their capacity to\nmatch and even exceed average human performance across diverse, unseen tasks.\nThis suggests that LLMs could be a feasible alternative to human evaluation and\nother automated metrics in TST evaluation. We compare the results of different\nLLMs in TST using multiple input prompts. Our findings highlight a strong\ncorrelation between (even zero-shot) prompting and human evaluation, showing\nthat LLMs often outperform traditional automated metrics. Furthermore, we\nintroduce the concept of prompt ensembling, demonstrating its ability to\nenhance the robustness of TST evaluation. This research contributes to the\nongoing evaluation of LLMs in diverse tasks, offering insights into successful\noutcomes and areas of limitation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ostheimer_P/0/1/0/all/0/1\">Phil Ostheimer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagda_M/0/1/0/all/0/1\">Mayank Nagda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kloft_M/0/1/0/all/0/1\">Marius Kloft</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fellenz_S/0/1/0/all/0/1\">Sophie Fellenz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPSP: Learning Speech Concepts From Phoneme Supervision. (arXiv:2309.00424v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2309.00424","description":"<p>For fine-grained generation and recognition tasks such as\nminimally-supervised text-to-speech (TTS), voice conversion (VC), and automatic\nspeech recognition (ASR), the intermediate representations extracted from\nspeech should serve as a \"bridge\" between text and acoustic information,\ncontaining information from both modalities. The semantic content is\nemphasized, while the paralinguistic information such as speaker identity and\nacoustic details should be de-emphasized. However, existing methods for\nextracting fine-grained intermediate representations from speech suffer from\nissues of excessive redundancy and dimension explosion. Contrastive learning is\na good method for modeling intermediate representations from two modalities.\nHowever, existing contrastive learning methods in the audio field focus on\nextracting global descriptive information for downstream audio classification\ntasks, making them unsuitable for TTS, VC, and ASR tasks. To address these\nissues, we propose a method named \"Contrastive Token-Acoustic Pretraining\n(CTAP)\", which uses two encoders to bring phoneme and speech into a joint\nmultimodal space, learning how to connect phoneme and speech at the frame\nlevel. The CTAP model is trained on 210k speech and phoneme text pairs,\nachieving minimally-supervised TTS, VC, and ASR. The proposed CTAP method\noffers a promising solution for fine-grained generation and recognition\ndownstream tasks in speech processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Qiang_C/0/1/0/all/0/1\">Chunyu Qiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_Y/0/1/0/all/0/1\">Yixin Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_R/0/1/0/all/0/1\">Ruibo Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Longbiao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dang_J/0/1/0/all/0/1\">Jianwu Dang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models. (arXiv:2309.01219v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.01219","description":"<p>While large language models (LLMs) have demonstrated remarkable capabilities\nacross a range of downstream tasks, a significant concern revolves around their\npropensity to exhibit hallucinations: LLMs occasionally generate content that\ndiverges from the user input, contradicts previously generated context, or\nmisaligns with established world knowledge. This phenomenon poses a substantial\nchallenge to the reliability of LLMs in real-world scenarios. In this paper, we\nsurvey recent efforts on the detection, explanation, and mitigation of\nhallucination, with an emphasis on the unique challenges posed by LLMs. We\npresent taxonomies of the LLM hallucination phenomena and evaluation\nbenchmarks, analyze existing approaches aiming at mitigating LLM hallucination,\nand discuss potential directions for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yafu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Leyang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1\">Tingchen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinting Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_E/0/1/0/all/0/1\">Enbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1\">Anh Tuan Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1\">Wei Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1\">Freda Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models. (arXiv:2309.02706v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.02706","description":"<p>Large Language Models (LLMs) trained on massive corpora demonstrate\nimpressive capabilities in a wide range of tasks. While there are ongoing\nefforts to adapt these models to languages beyond English, the attention given\nto their evaluation methodologies remains limited. Current multilingual\nbenchmarks often rely on back translations or re-implementations of English\ntests, limiting their capacity to capture unique cultural and linguistic\nnuances. To bridge this gap for the Korean language, we introduce HAE-RAE\nBench, a dataset curated to challenge models lacking Korean cultural and\ncontextual depth. The dataset encompasses six downstream tasks across four\ndomains: vocabulary, history, general knowledge, and reading comprehension.\nContrary to traditional evaluation suites focused on token or sequence\nclassification and specific mathematical or logical reasoning, HAE-RAE Bench\nemphasizes a model's aptitude for recalling Korean-specific knowledge and\ncultural contexts. Comparative analysis with prior Korean benchmarks indicates\nthat the HAE-RAE Bench presents a greater challenge to non-native models, by\ndisturbing abilities and knowledge learned from English being transferred.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Son_G/0/1/0/all/0/1\">Guijin Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hanwool Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Suwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Huiseo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaecheol Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeom_J/0/1/0/all/0/1\">Je Won Yeom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1\">Jihyu Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jung Woo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Songseong Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models as Black-Box Optimizers for Vision-Language Models. (arXiv:2309.05950v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.05950","description":"<p>Vision-language models (VLMs) pre-trained on web-scale datasets have\ndemonstrated remarkable capabilities across a variety of vision and multimodal\ntasks. Currently, fine-tuning methods for VLMs mainly operate in a white-box\nsetting, requiring access to model parameters for backpropagation. However,\nmany VLMs rely on proprietary data and are not open-source, which restricts the\nuse of white-box approaches for fine-tuning. Given that popular private large\nlanguage models (LLMs) like ChatGPT still offer a language-based user\ninterface, we aim to develop a novel fine-tuning approach for VLMs through\nnatural language prompts, thereby avoiding the need to access model parameters,\nfeature embeddings, or output logits. In this setup, we propose employing\nchat-based LLMs as black-box optimizers to search for the best text prompt on\nthe illustrative task of few-shot image classification using CLIP.\nSpecifically, we adopt an automatic \"hill-climbing\" procedure that converges on\nan effective prompt by evaluating the accuracy of current prompts and asking\nLLMs to refine them based on textual feedback, all within a conversational\nprocess without human-in-the-loop. In a challenging 1-shot learning setup, our\nsimple approach surpasses the white-box continuous prompting method (CoOp) by\nan average of 1.5% across 11 datasets including ImageNet. Our approach also\noutperforms OpenAI's manually crafted prompts. Additionally, we highlight the\nadvantage of conversational feedback that incorporates both positive and\nnegative prompts, suggesting that LLMs can utilize the implicit \"gradient\"\ndirection in textual feedback for a more efficient search. Lastly, we find that\nthe text prompts generated through our strategy are not only more interpretable\nbut also transfer well across different CLIP architectures in a black-box\nmanner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shihong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Samuel Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhiqiu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Action Co-occurrence in Lifestyle Vlogs using Graph Link Prediction. (arXiv:2309.06219v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2309.06219","description":"<p>We introduce the task of automatic human action co-occurrence identification,\ni.e., determine whether two human actions can co-occur in the same interval of\ntime. We create and make publicly available the ACE (Action Co-occurrencE)\ndataset, consisting of a large graph of ~12k co-occurring pairs of visual\nactions and their corresponding video clips. We describe graph link prediction\nmodels that leverage visual and textual information to automatically infer if\ntwo actions are co-occurring. We show that graphs are particularly well suited\nto capture relations between human actions, and the learned graph\nrepresentations are effective for our task and capture novel and relevant\ninformation across different data domains. The ACE dataset and the code\nintroduced in this paper are publicly available at\nhttps://github.com/MichiganNLP/vlog_action_co-occurrence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ignat_O/0/1/0/all/0/1\">Oana Ignat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_S/0/1/0/all/0/1\">Santiago Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DBLPLink: An Entity Linker for the DBLP Scholarly Knowledge Graph. (arXiv:2309.07545v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.07545","description":"<p>In this work, we present a web application named DBLPLink, which performs\nentity linking over the DBLP scholarly knowledge graph. DBLPLink uses\ntext-to-text pre-trained language models, such as T5, to produce entity label\nspans from an input text question. Entity candidates are fetched from a\ndatabase based on the labels, and an entity re-ranker sorts them based on\nentity embeddings, such as TransE, DistMult and ComplEx. The results are\ndisplayed so that users may compare and contrast the results between T5-small,\nT5-base and the different KG embeddings used. The demo can be accessed at\nhttps://ltdemos.informatik.uni-hamburg.de/dblplink/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1\">Debayan Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arefa/0/1/0/all/0/1\">Arefa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usbeck_R/0/1/0/all/0/1\">Ricardo Usbeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biemann_C/0/1/0/all/0/1\">Chris Biemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions. (arXiv:2309.07875v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.07875","description":"<p>Training large language models to follow instructions makes them perform\nbetter on a wide range of tasks, generally becoming more helpful. However, a\nperfectly helpful model will follow even the most malicious instructions and\nreadily generate harmful content. In this paper, we raise concerns over the\nsafety of models that only emphasize helpfulness, not safety, in their\ninstruction-tuning. We show that several popular instruction-tuned models are\nhighly unsafe. Moreover, we show that adding just 3% safety examples (a few\nhundred demonstrations) in the training set when fine-tuning a model like LLaMA\ncan substantially improve their safety. Our safety-tuning does not make models\nsignificantly less capable or helpful as measured by standard benchmarks.\nHowever, we do find a behavior of exaggerated safety, where too much\nsafety-tuning makes models refuse to respond to reasonable prompts that\nsuperficially resemble unsafe ones. Our study sheds light on trade-offs in\ntraining LLMs to follow instructions and exhibit safe behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzgun_M/0/1/0/all/0/1\">Mirac Suzgun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attanasio_G/0/1/0/all/0/1\">Giuseppe Attanasio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottger_P/0/1/0/all/0/1\">Paul R&#xf6;ttger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recovering from Privacy-Preserving Masking with Large Language Models. (arXiv:2309.08628v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.08628","description":"<p>Model adaptation is crucial to handle the discrepancy between proxy training\ndata and actual users data received. To effectively perform adaptation, textual\ndata of users is typically stored on servers or their local devices, where\ndownstream natural language processing (NLP) models can be directly trained\nusing such in-domain data. However, this might raise privacy and security\nconcerns due to the extra risks of exposing user information to adversaries.\nReplacing identifying information in textual data with a generic marker has\nbeen recently explored. In this work, we leverage large language models (LLMs)\nto suggest substitutes of masked tokens and have their effectiveness evaluated\non downstream language modeling tasks. Specifically, we propose multiple\npre-trained and fine-tuned LLM-based approaches and perform empirical studies\non various datasets for the comparison of these methods. Experimental results\nshow that models trained on the obfuscation corpora are able to achieve\ncomparable performance with the ones trained on the original data without\nprivacy-preserving token masking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vats_A/0/1/0/all/0/1\">Arpita Vats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_P/0/1/0/all/0/1\">Peng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_D/0/1/0/all/0/1\">Debjyoti Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yingyi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yutong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_Z/0/1/0/all/0/1\">Zeeshan Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services. (arXiv:2309.11325v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.11325","description":"<p>We propose DISC-LawLLM, an intelligent legal system utilizing large language\nmodels (LLMs) to provide a wide range of legal services. We adopt legal\nsyllogism prompting strategies to construct supervised fine-tuning datasets in\nthe Chinese Judicial domain and fine-tune LLMs with legal reasoning capability.\nWe augment LLMs with a retrieval module to enhance models' ability to access\nand utilize external legal knowledge. A comprehensive legal benchmark,\nDISC-Law-Eval, is presented to evaluate intelligent legal systems from both\nobjective and subjective dimensions. Quantitative and qualitative results on\nDISC-Law-Eval demonstrate the effectiveness of our system in serving various\nusers across diverse legal scenarios. The detailed resources are available at\nhttps://github.com/FudanDISC/DISC-LawLLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1\">Shengbin Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chenchen Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Song Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Data Collection and Unsupervised Learning for Code-switched Tunisian Arabic Automatic Speech Recognition. (arXiv:2309.11327v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2309.11327","description":"<p>Crafting an effective Automatic Speech Recognition (ASR) solution for\ndialects demands innovative approaches that not only address the data scarcity\nissue but also navigate the intricacies of linguistic diversity. In this paper,\nwe address the aforementioned ASR challenge, focusing on the Tunisian dialect.\nFirst, textual and audio data is collected and in some cases annotated. Second,\nwe explore self-supervision, semi-supervision and few-shot code-switching\napproaches to push the state-of-the-art on different Tunisian test sets;\ncovering different acoustic, linguistic and prosodic conditions. Finally, and\ngiven the absence of conventional spelling, we produce a human evaluation of\nour transcripts to avoid the noise coming from spelling inadequacies in our\ntesting references. Our models, allowing to transcribe audio samples in a\nlinguistic mix involving Tunisian Arabic, English and French, and all the data\nused during training and testing are released for public use and further\nimprovements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Abdallah_A/0/1/0/all/0/1\">Ahmed Amine Ben Abdallah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kabboudi_A/0/1/0/all/0/1\">Ata Kabboudi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kanoun_A/0/1/0/all/0/1\">Amir Kanoun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zaiem_S/0/1/0/all/0/1\">Salah Zaiem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain-of-Verification Reduces Hallucination in Large Language Models. (arXiv:2309.11495v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.11495","description":"<p>Generation of plausible yet incorrect factual information, termed\nhallucination, is an unsolved issue in large language models. We study the\nability of language models to deliberate on the responses they give in order to\ncorrect their mistakes. We develop the Chain-of-Verification (CoVe) method\nwhereby the model first (i) drafts an initial response; then (ii) plans\nverification questions to fact-check its draft; (iii) answers those questions\nindependently so the answers are not biased by other responses; and (iv)\ngenerates its final verified response. In experiments, we show CoVe decreases\nhallucinations across a variety of tasks, from list-based questions from\nWikidata, closed book MultiSpanQA and longform text generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1\">Shehzaad Dhuliawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komeili_M/0/1/0/all/0/1\">Mojtaba Komeili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1\">Roberta Raileanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Cambridge Law Corpus: A Corpus for Legal AI Research. (arXiv:2309.12269v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.12269","description":"<p>We introduce the Cambridge Law Corpus (CLC), a corpus for legal AI research.\nIt consists of over 250 000 court cases from the UK. Most cases are from the\n21st century, but the corpus includes cases as old as the 16th century. This\npaper presents the first release of the corpus, containing the raw text and\nmeta-data. Together with the corpus, we provide annotations on case outcomes\nfor 638 cases, done by legal experts. Using our annotated data, we have trained\nand evaluated case outcome extraction with GPT-3, GPT-4 and RoBERTa models to\nprovide benchmarks. We include an extensive legal and ethical discussion to\naddress the potentially sensitive nature of this material. As a consequence,\nthe corpus will only be released for research purposes under certain\nrestrictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ostling_A/0/1/0/all/0/1\">Andreas &#xd6;stling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sargeant_H/0/1/0/all/0/1\">Holli Sargeant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Huiyuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bull_L/0/1/0/all/0/1\">Ludwig Bull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terenin_A/0/1/0/all/0/1\">Alexander Terenin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonsson_L/0/1/0/all/0/1\">Leif Jonsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magnusson_M/0/1/0/all/0/1\">M&#xe5;ns Magnusson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steffek_F/0/1/0/all/0/1\">Felix Steffek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Reversal Curse: LLMs trained on \"A is B\" fail to learn \"B is A\". (arXiv:2309.12288v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.12288","description":"<p>We expose a surprising failure of generalization in auto-regressive large\nlanguage models (LLMs). If a model is trained on a sentence of the form \"A is\nB\", it will not automatically generalize to the reverse direction \"B is A\".\nThis is the Reversal Curse. For instance, if a model is trained on \"Olaf Scholz\nwas the ninth Chancellor of Germany\", it will not automatically be able to\nanswer the question, \"Who was the ninth Chancellor of Germany?\". Moreover, the\nlikelihood of the correct answer (\"Olaf Scholz\") will not be higher than for a\nrandom name. Thus, models exhibit a basic failure of logical deduction and do\nnot generalize a prevalent pattern in their training set (i.e. if \"A is B''\noccurs, \"B is A\" is more likely to occur). We provide evidence for the Reversal\nCurse by finetuning GPT-3 and Llama-1 on fictitious statements such as \"Uriah\nHawthorne is the composer of 'Abyssal Melodies'\" and showing that they fail to\ncorrectly answer \"Who composed 'Abyssal Melodies?'\". The Reversal Curse is\nrobust across model sizes and model families and is not alleviated by data\naugmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about\nreal-world celebrities, such as \"Who is Tom Cruise's mother? [A: Mary Lee\nPfeiffer]\" and the reverse \"Who is Mary Lee Pfeiffer's son?\". GPT-4 correctly\nanswers questions like the former 79% of the time, compared to 33% for the\nlatter. This shows a failure of logical deduction that we hypothesize is caused\nby the Reversal Curse. Code is available at\nhttps://github.com/lukasberglund/reversal_curse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berglund_L/0/1/0/all/0/1\">Lukas Berglund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1\">Meg Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaufmann_M/0/1/0/all/0/1\">Max Kaufmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balesni_M/0/1/0/all/0/1\">Mikita Balesni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stickland_A/0/1/0/all/0/1\">Asa Cooper Stickland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korbak_T/0/1/0/all/0/1\">Tomasz Korbak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_O/0/1/0/all/0/1\">Owain Evans</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Creativity Support in the Age of Large Language Models: An Empirical Study Involving Emerging Writers. (arXiv:2309.12570v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2309.12570","description":"<p>The development of large language models (LLMs) capable of following\ninstructions and engaging in conversational interactions sparked increased\ninterest in their utilization across various support tools. We investigate the\nutility of modern LLMs in assisting professional writers via an empirical user\nstudy (n=30). The design of our collaborative writing interface is grounded in\nthe cognitive process model of writing that views writing as a goal-oriented\nthinking process encompassing non-linear cognitive activities: planning,\ntranslating, and reviewing. Participants are asked to submit a post-completion\nsurvey to provide feedback on the potential and pitfalls of LLMs as writing\ncollaborators. Upon analyzing the writer-LLM interactions, we find that while\nwriters seek LLM's help across all three types of cognitive activities, they\nfind LLMs more helpful in translation and reviewing. Our findings from\nanalyzing both the interactions and the survey responses highlight future\nresearch directions in creative writing assistance using LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_V/0/1/0/all/0/1\">Vishakh Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatPRCS: A Personalized Support System for English Reading Comprehension based on ChatGPT. (arXiv:2309.12808v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.12808","description":"<p>As a common approach to learning English, reading comprehension primarily\nentails reading articles and answering related questions. However, the\ncomplexity of designing effective exercises results in students encountering\nstandardized questions, making it challenging to align with individualized\nlearners' reading comprehension ability. By leveraging the advanced\ncapabilities offered by large language models, exemplified by ChatGPT, this\npaper presents a novel personalized support system for reading comprehension,\nreferred to as ChatPRCS, based on the Zone of Proximal Development theory.\nChatPRCS employs methods including reading comprehension proficiency\nprediction, question generation, and automatic evaluation, among others, to\nenhance reading comprehension instruction. First, we develop a new algorithm\nthat can predict learners' reading comprehension abilities using their\nhistorical data as the foundation for generating questions at an appropriate\nlevel of difficulty. Second, a series of new ChatGPT prompt patterns is\nproposed to address two key aspects of reading comprehension objectives:\nquestion generation, and automated evaluation. These patterns further improve\nthe quality of generated questions. Finally, by integrating personalized\nability and reading comprehension prompt patterns, ChatPRCS is systematically\nvalidated through experiments. Empirical results demonstrate that it provides\nlearners with high-quality reading comprehension questions that are broadly\naligned with expert-crafted questions at a statistical level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xizhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yihua Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Changqin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaodi Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-09-25T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}