{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-07-31T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Writer adaptation for offline text recognition: An exploration of neural network-based methods. (arXiv:2307.15071v1 [cs.CV])","link":"http://arxiv.org/abs/2307.15071","description":"<p>Handwriting recognition has seen significant success with the use of deep\nlearning. However, a persistent shortcoming of neural networks is that they are\nnot well-equipped to deal with shifting data distributions. In the field of\nhandwritten text recognition (HTR), this shows itself in poor recognition\naccuracy for writers that are not similar to those seen during training. An\nideal HTR model should be adaptive to new writing styles in order to handle the\nvast amount of possible writing styles. In this paper, we explore how HTR\nmodels can be made writer adaptive by using only a handful of examples from a\nnew writer (e.g., 16 examples) for adaptation. Two HTR architectures are used\nas base models, using a ResNet backbone along with either an LSTM or\nTransformer sequence decoder. Using these base models, two methods are\nconsidered to make them writer adaptive: 1) model-agnostic meta-learning\n(MAML), an algorithm commonly used for tasks such as few-shot classification,\nand 2) writer codes, an idea originating from automatic speech recognition.\nResults show that an HTR-specific version of MAML known as MetaHTR improves\nperformance compared to the baseline with a 1.4 to 2.0 improvement in word\nerror rate (WER). The improvement due to writer adaptation is between 0.2 and\n0.7 WER, where a deeper model seems to lend itself better to adaptation using\nMetaHTR than a shallower model. However, applying MetaHTR to larger HTR models\nor sentence-level HTR may become prohibitive due to its high computational and\nmemory requirements. Lastly, writer codes based on learned features or Hinge\nstatistical features did not lead to improved recognition performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Werff_T/0/1/0/all/0/1\">Tobias van der Werff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhali_M/0/1/0/all/0/1\">Maruf A. Dhali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schomaker_L/0/1/0/all/0/1\">Lambert Schomaker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting the Presence of COVID-19 Vaccination Hesitancy from South African Twitter Data Using Machine Learning. (arXiv:2307.15072v1 [cs.CY])","link":"http://arxiv.org/abs/2307.15072","description":"<p>Very few social media studies have been done on South African user-generated\ncontent during the COVID-19 pandemic and even fewer using hand-labelling over\nautomated methods. Vaccination is a major tool in the fight against the\npandemic, but vaccine hesitancy jeopardizes any public health effort. In this\nstudy, sentiment analysis on South African tweets related to vaccine hesitancy\nwas performed, with the aim of training AI-mediated classification models and\nassessing their reliability in categorizing UGC. A dataset of 30000 tweets from\nSouth Africa were extracted and hand-labelled into one of three sentiment\nclasses: positive, negative, neutral. The machine learning models used were\nLSTM, bi-LSTM, SVM, BERT-base-cased and the RoBERTa-base models, whereby their\nhyperparameters were carefully chosen and tuned using the WandB platform. We\nused two different approaches when we pre-processed our data for comparison:\none was semantics-based, while the other was corpus-based. The pre-processing\nof the tweets in our dataset was performed using both methods, respectively.\nAll models were found to have low F1-scores within a range of 45$\\%$-55$\\%$,\nexcept for BERT and RoBERTa which both achieved significantly better measures\nwith overall F1-scores of 60$\\%$ and 61$\\%$, respectively. Topic modelling\nusing an LDA was performed on the miss-classified tweets of the RoBERTa model\nto gain insight on how to further improve model accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perikli_N/0/1/0/all/0/1\">Nicholas Perikli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Srimoy Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogbuokiri_B/0/1/0/all/0/1\">Blessing Ogbuokiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nia_Z/0/1/0/all/0/1\">Zahra Movahedi Nia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lieberman_B/0/1/0/all/0/1\">Benjamin Lieberman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_N/0/1/0/all/0/1\">Nidhi Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahbi_S/0/1/0/all/0/1\">Salah-Eddine Dahbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevenson_F/0/1/0/all/0/1\">Finn Stevenson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bragazzi_N/0/1/0/all/0/1\">Nicola Bragazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_J/0/1/0/all/0/1\">Jude Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mellado_B/0/1/0/all/0/1\">Bruce Mellado</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cascaded Cross-Modal Transformer for Request and Complaint Detection. (arXiv:2307.15097v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15097","description":"<p>We propose a novel cascaded cross-modal transformer (CCMT) that combines\nspeech and text transcripts to detect customer requests and complaints in phone\nconversations. Our approach leverages a multimodal paradigm by transcribing the\nspeech using automatic speech recognition (ASR) models and translating the\ntranscripts into different languages. Subsequently, we combine\nlanguage-specific BERT-based models with Wav2Vec2.0 audio features in a novel\ncascaded cross-attention transformer model. We apply our system to the Requests\nSub-Challenge of the ACM Multimedia 2023 Computational Paralinguistics\nChallenge, reaching unweighted average recalls (UAR) of 65.41% and 85.87% for\nthe complaint and request classes, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ristea_N/0/1/0/all/0/1\">Nicolae-Catalin Ristea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VISU at WASSA 2023 Shared Task: Detecting Emotions in Reaction to News Stories Leveraging BERT and Stacked Embeddings. (arXiv:2307.15164v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15164","description":"<p>Our system, VISU, participated in the WASSA 2023 Shared Task (3) of Emotion\nClassification from essays written in reaction to news articles. Emotion\ndetection from complex dialogues is challenging and often requires\ncontext/domain understanding. Therefore in this research, we have focused on\ndeveloping deep learning (DL) models using the combination of word embedding\nrepresentations with tailored prepossessing strategies to capture the nuances\nof emotions expressed. Our experiments used static and contextual embeddings\n(individual and stacked) with Bidirectional Long short-term memory (BiLSTM) and\nTransformer based models. We occupied rank tenth in the emotion detection task\nby scoring a Macro F1-Score of 0.2717, validating the efficacy of our\nimplemented approaches for small and imbalanced datasets with mixed categories\nof target emotions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vivek Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sushmita Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_P/0/1/0/all/0/1\">Prayag Tiwari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RCT Rejection Sampling for Causal Estimation Evaluation. (arXiv:2307.15176v1 [cs.AI])","link":"http://arxiv.org/abs/2307.15176","description":"<p>Confounding is a significant obstacle to unbiased estimation of causal\neffects from observational data. For settings with high-dimensional covariates\n-- such as text data, genomics, or the behavioral social sciences --\nresearchers have proposed methods to adjust for confounding by adapting machine\nlearning methods to the goal of causal estimation. However, empirical\nevaluation of these adjustment methods has been challenging and limited. In\nthis work, we build on a promising empirical evaluation strategy that\nsimplifies evaluation design and uses real data: subsampling randomized\ncontrolled trials (RCTs) to create confounded observational datasets while\nusing the average causal effects from the RCTs as ground-truth. We contribute a\nnew sampling algorithm, which we call RCT rejection sampling, and provide\ntheoretical guarantees that causal identification holds in the observational\ndata to allow for valid comparisons to the ground-truth RCT. Using synthetic\ndata, we show our algorithm indeed results in low bias when oracle estimators\nare evaluated on the confounded samples, which is not always the case for a\npreviously proposed algorithm. In addition to this identification result, we\nhighlight several finite data considerations for evaluation designers who plan\nto use RCT rejection sampling on their own datasets. As a proof of concept, we\nimplement an example evaluation pipeline and walk through these finite data\nconsiderations with a novel, real-world RCT -- which we release publicly --\nconsisting of approximately 70k observations and text data as high-dimensional\ncovariates. Together, these contributions build towards a broader agenda of\nimproved empirical evaluation for causal estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keith_K/0/1/0/all/0/1\">Katherine A. Keith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_S/0/1/0/all/0/1\">Sergey Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bragg_J/0/1/0/all/0/1\">Jonathan Bragg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_R/0/1/0/all/0/1\">Rohit Bhattacharya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"f-Divergence Minimization for Sequence-Level Knowledge Distillation. (arXiv:2307.15190v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15190","description":"<p>Knowledge distillation (KD) is the process of transferring knowledge from a\nlarge model to a small one. It has gained increasing attention in the natural\nlanguage processing community, driven by the demands of compressing\never-growing language models. In this work, we propose an f-DISTILL framework,\nwhich formulates sequence-level knowledge distillation as minimizing a\ngeneralized f-divergence function. We propose four distilling variants under\nour framework and show that existing SeqKD and ENGINE approaches are\napproximations of our f-DISTILL methods. We further derive step-wise\ndecomposition for our f-DISTILL, reducing intractable sequence-level divergence\nto word-level losses that can be computed in a tractable manner. Experiments\nacross four datasets show that our methods outperform existing KD approaches,\nand that our symmetric distilling losses can better force the student to learn\nfrom the teacher distribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yuqiao Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zichao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wenyu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lili Mou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization. (arXiv:2307.15199v1 [cs.CV])","link":"http://arxiv.org/abs/2307.15199","description":"<p>In a joint vision-language space, a text feature (e.g., from \"a photo of a\ndog\") could effectively represent its relevant image features (e.g., from dog\nphotos). Inspired by this, we propose PromptStyler which simulates various\ndistribution shifts in the joint space by synthesizing diverse styles via\nprompts without using any images to deal with source-free domain\ngeneralization. Our method learns to generate a variety of style features (from\n\"a S* style of a\") via learnable style word vectors for pseudo-words S*. To\nensure that learned styles do not distort content information, we force\nstyle-content features (from \"a S* style of a [class]\") to be located nearby\ntheir corresponding content features (from \"[class]\") in the joint\nvision-language space. After learning style word vectors, we train a linear\nclassifier using synthesized style-content features. PromptStyler achieves the\nstate of the art on PACS, VLCS, OfficeHome and DomainNet, although it does not\nrequire any images and takes just ~30 minutes for training using a single GPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Junhyeong Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_G/0/1/0/all/0/1\">Gilhyun Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hunmin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1\">Suha Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback. (arXiv:2307.15217v1 [cs.AI])","link":"http://arxiv.org/abs/2307.15217","description":"<p>Reinforcement learning from human feedback (RLHF) is a technique for training\nAI systems to align with human goals. RLHF has emerged as the central method\nused to finetune state-of-the-art large language models (LLMs). Despite this\npopularity, there has been relatively little public work systematizing its\nflaws. In this paper, we (1) survey open problems and fundamental limitations\nof RLHF and related methods; (2) overview techniques to understand, improve,\nand complement RLHF in practice; and (3) propose auditing and disclosure\nstandards to improve societal oversight of RLHF systems. Our work emphasizes\nthe limitations of RLHF and highlights the importance of a multi-faceted\napproach to the development of safer AI systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1\">Stephen Casper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davies_X/0/1/0/all/0/1\">Xander Davies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Claudia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilbert_T/0/1/0/all/0/1\">Thomas Krendl Gilbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheurer_J/0/1/0/all/0/1\">J&#xe9;r&#xe9;my Scheurer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rando_J/0/1/0/all/0/1\">Javier Rando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freedman_R/0/1/0/all/0/1\">Rachel Freedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korbak_T/0/1/0/all/0/1\">Tomasz Korbak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindner_D/0/1/0/all/0/1\">David Lindner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freire_P/0/1/0/all/0/1\">Pedro Freire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tony Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marks_S/0/1/0/all/0/1\">Samuel Marks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segerie_C/0/1/0/all/0/1\">Charbel-Rapha&#xeb;l Segerie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carroll_M/0/1/0/all/0/1\">Micah Carroll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_A/0/1/0/all/0/1\">Andi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christoffersen_P/0/1/0/all/0/1\">Phillip Christoffersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damani_M/0/1/0/all/0/1\">Mehul Damani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slocum_S/0/1/0/all/0/1\">Stewart Slocum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_U/0/1/0/all/0/1\">Usman Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siththaranjan_A/0/1/0/all/0/1\">Anand Siththaranjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeau_M/0/1/0/all/0/1\">Max Nadeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michaud_E/0/1/0/all/0/1\">Eric J. Michaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfau_J/0/1/0/all/0/1\">Jacob Pfau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krasheninnikov_D/0/1/0/all/0/1\">Dmitrii Krasheninnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langosco_L/0/1/0/all/0/1\">Lauro Langosco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hase_P/0/1/0/all/0/1\">Peter Hase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biyik_E/0/1/0/all/0/1\">Erdem B&#x131;y&#x131;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1\">Anca Dragan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krueger_D/0/1/0/all/0/1\">David Krueger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1\">Dorsa Sadigh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1\">Dylan Hadfield-Menell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Lexical Simplification via Paraphrase Generation. (arXiv:2307.15286v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15286","description":"<p>Lexical simplification (LS) methods based on pretrained language models have\nmade remarkable progress, generating potential substitutes for a complex word\nthrough analysis of its contextual surroundings. However, these methods require\nseparate pretrained models for different languages and disregard the\npreservation of sentence meaning. In this paper, we propose a novel\nmultilingual LS method via paraphrase generation, as paraphrases provide\ndiversity in word selection while preserving the sentence's meaning. We regard\nparaphrasing as a zero-shot translation task within multilingual neural machine\ntranslation that supports hundreds of languages. After feeding the input\nsentence into the encoder of paraphrase modeling, we generate the substitutes\nbased on a novel decoding strategy that concentrates solely on the lexical\nvariations of the complex word. Experimental results demonstrate that our\napproach surpasses BERT-based methods and zero-shot GPT3-based method\nsignificantly on English, Spanish, and Portuguese.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiang_J/0/1/0/all/0/1\">Jipeng Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yunhao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_K/0/1/0/all/0/1\">Kaixun Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatHome: Development and Evaluation of a Domain-Specific Language Model for Home Renovation. (arXiv:2307.15290v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15290","description":"<p>This paper presents the development and evaluation of ChatHome, a\ndomain-specific language model (DSLM) designed for the intricate field of home\nrenovation. Considering the proven competencies of large language models (LLMs)\nlike GPT-4 and the escalating fascination with home renovation, this study\nendeavors to reconcile these aspects by generating a dedicated model that can\nyield high-fidelity, precise outputs relevant to the home renovation arena.\nChatHome's novelty rests on its methodology, fusing domain-adaptive pretraining\nand instruction-tuning over an extensive dataset. This dataset includes\nprofessional articles, standard documents, and web content pertinent to home\nrenovation. This dual-pronged strategy is designed to ensure that our model can\nassimilate comprehensive domain knowledge and effectively address user\ninquiries. Via thorough experimentation on diverse datasets, both universal and\ndomain-specific, including the freshly introduced \"EvalHome\" domain dataset, we\nsubstantiate that ChatHome not only amplifies domain-specific functionalities\nbut also preserves its versatility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1\">Cheng Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xianghui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuaijiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1\">Xiaoquan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liangyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1\">Wei Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WC-SBERT: Zero-Shot Text Classification via SBERT with Self-Training for Wikipedia Categories. (arXiv:2307.15293v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15293","description":"<p>Our research focuses on solving the zero-shot text classification problem in\nNLP, with a particular emphasis on innovative self-training strategies. To\nachieve this objective, we propose a novel self-training strategy that uses\nlabels rather than text for training, significantly reducing the model's\ntraining time. Specifically, we use categories from Wikipedia as our training\nset and leverage the SBERT pre-trained model to establish positive correlations\nbetween pairs of categories within the same text, facilitating associative\ntraining. For new test datasets, we have improved the original self-training\napproach, eliminating the need for prior training and testing data from each\ntarget dataset. Instead, we adopt Wikipedia as a unified training dataset to\nbetter approximate the zero-shot scenario. This modification allows for rapid\nfine-tuning and inference across different datasets, greatly reducing the time\nrequired for self-training. Our experimental results demonstrate that this\nmethod can adapt the model to the target dataset within minutes. Compared to\nother BERT-based transformer models, our approach significantly reduces the\namount of training data by training only on labels, not the actual text, and\ngreatly improves training efficiency by utilizing a unified training set.\nAdditionally, our method achieves state-of-the-art results on both the Yahoo\nTopic and AG News datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chi_T/0/1/0/all/0/1\">Te-Yu Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yu-Meng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chia-Wen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiu-Xia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Jyh-Shing Roger Jang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrafficSafetyGPT: Tuning a Pre-trained Large Language Model to a Domain-Specific Expert in Transportation Safety. (arXiv:2307.15311v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15311","description":"<p>Large Language Models (LLMs) have shown remarkable effectiveness in various\ngeneral-domain natural language processing (NLP) tasks. However, their\nperformance in transportation safety domain tasks has been suboptimal,\nprimarily attributed to the requirement for specialized transportation safety\nexpertise in generating accurate responses [1]. To address this challenge, we\nintroduce TrafficSafetyGPT, a novel LLAMA-based model, which has undergone\nsupervised fine-tuning using TrafficSafety-2K dataset which has human labels\nfrom government produced guiding books and ChatGPT-generated instruction-output\npairs. Our proposed TrafficSafetyGPT model and TrafficSafety-2K train dataset\nare accessible at https://github.com/ozheng1993/TrafficSafetyGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_O/0/1/0/all/0/1\">Ou Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdel_Aty_M/0/1/0/all/0/1\">Mohamed Abdel-Aty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongdong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenzhu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shengxuan Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tutorials on Stance Detection using Pre-trained Language Models: Fine-tuning BERT and Prompting Large Language Models. (arXiv:2307.15331v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15331","description":"<p>This paper presents two self-contained tutorials on stance detection in\nTwitter data using BERT fine-tuning and prompting large language models (LLMs).\nThe first tutorial explains BERT architecture and tokenization, guiding users\nthrough training, tuning, and evaluating standard and domain-specific BERT\nmodels with HuggingFace transformers. The second focuses on constructing\nprompts and few-shot examples to elicit stances from ChatGPT and open-source\nFLAN-T5 without fine-tuning. Various prompting strategies are implemented and\nevaluated using confusion matrices and macro F1 scores. The tutorials provide\ncode, visualizations, and insights revealing the strengths of few-shot ChatGPT\nand FLAN-T5 which outperform fine-tuned BERTs. By covering both model\nfine-tuning and prompting-based techniques in an accessible, hands-on manner,\nthese tutorials enable learners to gain applied experience with cutting-edge\nmethods for stance detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yun-Shiuan Chuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BARTPhoBEiT: Pre-trained Sequence-to-Sequence and Image Transformers Models for Vietnamese Visual Question Answering. (arXiv:2307.15335v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15335","description":"<p>Visual Question Answering (VQA) is an intricate and demanding task that\nintegrates natural language processing (NLP) and computer vision (CV),\ncapturing the interest of researchers. The English language, renowned for its\nwealth of resources, has witnessed notable advancements in both datasets and\nmodels designed for VQA. However, there is a lack of models that target\nspecific countries such as Vietnam. To address this limitation, we introduce a\ntransformer-based Vietnamese model named BARTPhoBEiT. This model includes\npre-trained Sequence-to-Sequence and bidirectional encoder representation from\nImage Transformers in Vietnamese and evaluates Vietnamese VQA datasets.\nExperimental results demonstrate that our proposed model outperforms the strong\nbaseline and improves the state-of-the-art in six metrics: Accuracy, Precision,\nRecall, F1-score, WUPS 0.0, and WUPS 0.9.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Khiem Vinh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu Thuy Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding. (arXiv:2307.15337v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15337","description":"<p>This work aims at decreasing the end-to-end generation latency of large\nlanguage models (LLMs). One of the major causes of the high generation latency\nis the sequential decoding approach adopted by almost all state-of-the-art\nLLMs. In this work, motivated by the thinking and writing process of humans, we\npropose \"Skeleton-of-Thought\" (SoT), which guides LLMs to first generate the\nskeleton of the answer, and then conducts parallel API calls or batched\ndecoding to complete the contents of each skeleton point in parallel. Not only\ndoes SoT provide considerable speed-up (up to 2.39x across 11 different LLMs),\nbut it can also potentially improve the answer quality on several question\ncategories in terms of diversity and relevance. SoT is an initial attempt at\ndata-centric optimization for efficiency, and reveal the potential of pushing\nLLMs to think more like a human for answer quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1\">Xuefei Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zinan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zixuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huazhong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teach Me How to Improve My Argumentation Skills: A Survey on Feedback in Argumentation. (arXiv:2307.15341v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15341","description":"<p>The use of argumentation in education has been shown to improve critical\nthinking skills for end-users such as students, and computational models for\nargumentation have been developed to assist in this process. Although these\nmodels are useful for evaluating the quality of an argument, they oftentimes\ncannot explain why a particular argument is considered poor or not, which makes\nit difficult to provide constructive feedback to users to strengthen their\ncritical thinking skills. In this survey, we aim to explore the different\ndimensions of feedback (Richness, Visualization, Interactivity, and\nPersonalization) provided by the current computational models for\nargumentation, and the possibility of enhancing the power of explanations of\nsuch models, ultimately helping learners improve their critical thinking\nskills.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guerraoui_C/0/1/0/all/0/1\">Cam&#xe9;lia Guerraoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reisert_P/0/1/0/all/0/1\">Paul Reisert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inoue_N/0/1/0/all/0/1\">Naoya Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mim_F/0/1/0/all/0/1\">Farjana Sultana Mim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naito_S/0/1/0/all/0/1\">Shoichi Naito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jungmin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robbani_I/0/1/0/all/0/1\">Irfan Robbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Med-HALT: Medical Domain Hallucination Test for Large Language Models. (arXiv:2307.15343v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15343","description":"<p>This research paper focuses on the challenges posed by hallucinations in\nlarge language models (LLMs), particularly in the context of the medical\ndomain. Hallucination, wherein these models generate plausible yet unverified\nor incorrect information, can have serious consequences in healthcare\napplications. We propose a new benchmark and dataset, Med-HALT (Medical Domain\nHallucination Test), designed specifically to evaluate and reduce\nhallucinations. Med-HALT provides a diverse multinational dataset derived from\nmedical examinations across various countries and includes multiple innovative\ntesting modalities. Med-HALT includes two categories of tests reasoning and\nmemory-based hallucination tests, designed to assess LLMs's problem-solving and\ninformation retrieval abilities.\n</p>\n<p>Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2,\nMPT, and Falcon, revealing significant differences in their performance. The\npaper provides detailed insights into the dataset, promoting transparency and\nreproducibility. Through this work, we aim to contribute to the development of\nsafer and more reliable language models in healthcare. Our benchmark can be\nfound at medhalt.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Umapathi_L/0/1/0/all/0/1\">Logesh Kumar Umapathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_A/0/1/0/all/0/1\">Ankit Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankarasubbu_M/0/1/0/all/0/1\">Malaikannan Sankarasubbu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Tourist Assistance using ChatGPT: Comparing Capabilities in Hindi, Telugu, and Kannada. (arXiv:2307.15376v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15376","description":"<p>This research investigates the effectiveness of ChatGPT, an AI language model\nby OpenAI, in translating English into Hindi, Telugu, and Kannada languages,\naimed at assisting tourists in India's linguistically diverse environment. To\nmeasure the translation quality, a test set of 50 questions from diverse fields\nsuch as general knowledge, food, and travel was used. These were assessed by\nfive volunteers for accuracy and fluency, and the scores were subsequently\nconverted into a BLEU score. The BLEU score evaluates the closeness of a\nmachine-generated translation to a human translation, with a higher score\nindicating better translation quality. The Hindi translations outperformed\nothers, showcasing superior accuracy and fluency, whereas Telugu translations\nlagged behind. Human evaluators rated both the accuracy and fluency of\ntranslations, offering a comprehensive perspective on the language model's\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolar_S/0/1/0/all/0/1\">Sanjana Kolar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Rohit Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Fully Unsupervised Framework for Intent Induction in Customer Support Dialogues. (arXiv:2307.15410v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15410","description":"<p>State of the art models in intent induction require annotated datasets.\nHowever, annotating dialogues is time-consuming, laborious and expensive. In\nthis work, we propose a completely unsupervised framework for intent induction\nwithin a dialogue. In addition, we show how pre-processing the dialogue corpora\ncan improve results. Finally, we show how to extract the dialogue flows of\nintentions by investigating the most common sequences. Although we test our\nwork in the MultiWOZ dataset, the fact that this framework requires no prior\nknowledge make it applicable to any possible use case, making it very relevant\nto real world customer support applications across industry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Costa_R/0/1/0/all/0/1\">Rita Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_B/0/1/0/all/0/1\">Bruno Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viana_S/0/1/0/all/0/1\">S&#xe9;rgio Viana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coheur_L/0/1/0/all/0/1\">Luisa Coheur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Learning Behaviour of In-context Learning: A Comparison with Supervised Learning. (arXiv:2307.15411v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15411","description":"<p>Large language models (LLMs) have shown remarkable capacity for in-context\nlearning (ICL), where learning a new task from just a few training examples is\ndone without being explicitly pre-trained. However, despite the success of\nLLMs, there has been little understanding of how ICL learns the knowledge from\nthe given prompts. In this paper, to make progress toward understanding the\nlearning behaviour of ICL, we train the same LLMs with the same demonstration\nexamples via ICL and supervised learning (SL), respectively, and investigate\ntheir performance under label perturbations (i.e., noisy labels and label\nimbalance) on a range of classification tasks. First, via extensive\nexperiments, we find that gold labels have significant impacts on the\ndownstream in-context performance, especially for large language models;\nhowever, imbalanced labels matter little to ICL across all model sizes. Second,\nwhen comparing with SL, we show empirically that ICL is less sensitive to label\nperturbations than SL, and ICL gradually attains comparable performance to SL\nas the model size increases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xindi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1\">Frank Rudzicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mercer_R/0/1/0/all/0/1\">Robert E. Mercer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Social Media Popularity Prediction with Multiple Post Dependencies. (arXiv:2307.15413v1 [cs.MM])","link":"http://arxiv.org/abs/2307.15413","description":"<p>Social Media Popularity Prediction has drawn a lot of attention because of\nits profound impact on many different applications, such as recommendation\nsystems and multimedia advertising. Despite recent efforts to leverage the\ncontent of social media posts to improve prediction accuracy, many existing\nmodels fail to fully exploit the multiple dependencies between posts, which are\nimportant to comprehensively extract content information from posts. To tackle\nthis problem, we propose a novel prediction framework named Dependency-aware\nSequence Network (DSN) that exploits both intra- and inter-post dependencies.\nFor intra-post dependency, DSN adopts a multimodal feature extractor with an\nefficient fine-tuning strategy to obtain task-specific representations from\nimages and textual information of posts. For inter-post dependency, DSN uses a\nhierarchical information propagation method to learn category representations\nthat could better describe the difference between posts. DSN also exploits\nrecurrent networks with a series of gating layers for more flexible local\ntemporal processing abilities and multi-head attention for long-term\ndependencies. The experimental results on the Social Media Popularity Dataset\ndemonstrate the superiority of our method compared to existing state-of-the-art\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaohui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mengyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Ye Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yong Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Critical Review of Large Language Models: Sensitivity, Bias, and the Path Toward Specialized AI. (arXiv:2307.15425v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15425","description":"<p>This paper examines the comparative effectiveness of a specialized compiled\nlanguage model and a general-purpose model like OpenAI's GPT-3.5 in detecting\nSDGs within text data. It presents a critical review of Large Language Models\n(LLMs), addressing challenges related to bias and sensitivity. The necessity of\nspecialized training for precise, unbiased analysis is underlined. A case study\nusing a company descriptions dataset offers insight into the differences\nbetween the GPT-3.5 and the specialized SDG detection model. While GPT-3.5\nboasts broader coverage, it may identify SDGs with limited relevance to the\ncompanies' activities. In contrast, the specialized model zeroes in on highly\npertinent SDGs. The importance of thoughtful model selection is emphasized,\ntaking into account task requirements, cost, complexity, and transparency.\nDespite the versatility of LLMs, the use of specialized models is suggested for\ntasks demanding precision and accuracy. The study concludes by encouraging\nfurther research to find a balance between the capabilities of LLMs and the\nneed for domain-specific expertise and interpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hajikhani_A/0/1/0/all/0/1\">Arash Hajikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cole_C/0/1/0/all/0/1\">Carolyn Cole</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CFN-ESA: A Cross-Modal Fusion Network with Emotion-Shift Awareness for Dialogue Emotion Recognition. (arXiv:2307.15432v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15432","description":"<p>Multimodal Emotion Recognition in Conversation (ERC) has garnered growing\nattention from research communities in various fields. In this paper, we\npropose a cross-modal fusion network with emotion-shift awareness (CFN-ESA) for\nERC. Extant approaches employ each modality equally without distinguishing the\namount of emotional information, rendering it hard to adequately extract\ncomplementary and associative information from multimodal data. To cope with\nthis problem, in CFN-ESA, textual modalities are treated as the primary source\nof emotional information, while visual and acoustic modalities are taken as the\nsecondary sources. Besides, most multimodal ERC models ignore emotion-shift\ninformation and overfocus on contextual information, leading to the failure of\nemotion recognition under emotion-shift scenario. We elaborate an emotion-shift\nmodule to address this challenge. CFN-ESA mainly consists of the unimodal\nencoder (RUME), cross-modal encoder (ACME), and emotion-shift module (LESM).\nRUME is applied to extract conversation-level contextual emotional cues while\npulling together the data distributions between modalities; ACME is utilized to\nperform multimodal interaction centered on textual modality; LESM is used to\nmodel emotion shift and capture related information, thereby guide the learning\nof the main task. Experimental results demonstrate that CFN-ESA can effectively\npromote performance for ERC and remarkably outperform the state-of-the-art\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yingjian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhigang Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Probabilistic Programming to Complexity-based Programming. (arXiv:2307.15453v1 [cs.AI])","link":"http://arxiv.org/abs/2307.15453","description":"<p>The paper presents the main characteristics and a preliminary implementation\nof a novel computational framework named CompLog. Inspired by probabilistic\nprogramming systems like ProbLog, CompLog builds upon the inferential\nmechanisms proposed by Simplicity Theory, relying on the computation of two\nKolmogorov complexities (here implemented as min-path searches via ASP\nprograms) rather than probabilistic inference. The proposed system enables\nusers to compute ex-post and ex-ante measures of unexpectedness of a certain\nsituation, mapping respectively to posterior and prior subjective\nprobabilities. The computation is based on the specification of world and\nmental models by means of causal and descriptive relations between predicates\nweighted by complexity. The paper illustrates a few examples of application:\ngenerating relevant descriptions, and providing alternative approaches to\ndisjunction and to negation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sileno_G/0/1/0/all/0/1\">Giovanni Sileno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dessalles_J/0/1/0/all/0/1\">Jean-Louis Dessalles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trie-NLG: Trie Context Augmentation to Improve Personalized Query Auto-Completion for Short and Unseen Prefixes. (arXiv:2307.15455v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15455","description":"<p>Query auto-completion (QAC) aims at suggesting plausible completions for a\ngiven query prefix. Traditionally, QAC systems have leveraged tries curated\nfrom historical query logs to suggest most popular completions. In this\ncontext, there are two specific scenarios that are difficult to handle for any\nQAC system: short prefixes (which are inherently ambiguous) and unseen\nprefixes. Recently, personalized Natural Language Generation (NLG) models have\nbeen proposed to leverage previous session queries as context for addressing\nthese two challenges. However, such NLG models suffer from two drawbacks: (1)\nsome of the previous session queries could be noisy and irrelevant to the user\nintent for the current prefix, and (2) NLG models cannot directly incorporate\nhistorical query popularity. This motivates us to propose a novel NLG model for\nQAC, Trie-NLG, which jointly leverages popularity signals from trie and\npersonalization signals from previous session queries. We train the Trie-NLG\nmodel by augmenting the prefix with rich context comprising of recent session\nqueries and top trie completions. This simple modeling approach overcomes the\nlimitations of trie-based and NLG-based approaches and leads to\nstate-of-the-art performance. We evaluate the Trie-NLG model using two large\nQAC datasets. On average, our model achieves huge ~57% and ~14% boost in MRR\nover the popular trie-based lookup and the strong BART-based baseline methods,\nrespectively. We make our code publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maurya_K/0/1/0/all/0/1\">Kaushal Kumar Maurya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desarkar_M/0/1/0/all/0/1\">Maunendra Sankar Desarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1\">Puneet Agrawal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modal Concept Learning and Inference for Vision-Language Models. (arXiv:2307.15460v1 [cs.CV])","link":"http://arxiv.org/abs/2307.15460","description":"<p>Large-scale pre-trained Vision-Language Models (VLMs), such as CLIP,\nestablish the correlation between texts and images, achieving remarkable\nsuccess on various downstream tasks with fine-tuning. In existing fine-tuning\nmethods, the class-specific text description is matched against the whole\nimage. We recognize that this whole image matching is not effective since\nimages from the same class often contain a set of different semantic objects,\nand an object further consists of a set of semantic parts or concepts.\nIndividual semantic parts or concepts may appear in image samples from\ndifferent classes. To address this issue, in this paper, we develop a new\nmethod called cross-model concept learning and inference (CCLI). Using the\npowerful text-image correlation capability of CLIP, our method automatically\nlearns a large set of distinctive visual concepts from images using a set of\nsemantic text concepts. Based on these visual concepts, we construct a\ndiscriminative representation of images and learn a concept inference network\nto perform downstream image classification tasks, such as few-shot learning and\ndomain generalization. Extensive experimental results demonstrate that our CCLI\nmethod is able to improve the performance upon the current state-of-the-art\nmethods by large margins, for example, by up to 8.0% improvement on few-shot\nlearning and by up to 1.3% for domain generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yushun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhihai He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding. (arXiv:2307.15484v1 [cs.SD])","link":"http://arxiv.org/abs/2307.15484","description":"<p>Recently, there has been a growing interest in text-to-speech (TTS) methods\nthat can be trained with minimal supervision by combining two types of discrete\nspeech representations and using two sequence-to-sequence tasks to decouple\nTTS. To address the challenges associated with high dimensionality and waveform\ndistortion in discrete representations, we propose Diff-LM-Speech, which models\nsemantic embeddings into mel-spectrogram based on diffusion models and\nintroduces a prompt encoder structure based on variational autoencoders and\nprosody bottlenecks to improve prompt representation capabilities.\nAutoregressive language models often suffer from missing and repeated words,\nwhile non-autoregressive frameworks face expression averaging problems due to\nduration prediction models. To address these issues, we propose\nTetra-Diff-Speech, which designs a duration diffusion model to achieve diverse\nprosodic expressions. While we expect the information content of semantic\ncoding to be between that of text and acoustic coding, existing models extract\nsemantic coding with a lot of redundant information and dimensionality\nexplosion. To verify that semantic coding is not necessary, we propose\nTri-Diff-Speech. Experimental results show that our proposed methods outperform\nbaseline methods. We provide a website with audio samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiang_C/0/1/0/all/0/1\">Chunyu Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_H/0/1/0/all/0/1\">Hao Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">He Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1\">Ruibo Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longbiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_J/0/1/0/all/0/1\">Jianwu Dang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The timing bottleneck: Why timing and overlap are mission-critical for conversational user interfaces, speech recognition and dialogue systems. (arXiv:2307.15493v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15493","description":"<p>Speech recognition systems are a key intermediary in voice-driven\nhuman-computer interaction. Although speech recognition works well for pristine\nmonologic audio, real-life use cases in open-ended interactive settings still\npresent many challenges. We argue that timing is mission-critical for dialogue\nsystems, and evaluate 5 major commercial ASR systems for their conversational\nand multilingual support. We find that word error rates for natural\nconversational data in 6 languages remain abysmal, and that overlap remains a\nkey challenge (study 1). This impacts especially the recognition of\nconversational words (study 2), and in turn has dire consequences for\ndownstream intent recognition (study 3). Our findings help to evaluate the\ncurrent state of conversational ASR, contribute towards multidimensional error\nanalysis and evaluation, and identify phenomena that need most attention on the\nway to build robust interactive speech technologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liesenfeld_A/0/1/0/all/0/1\">Andreas Liesenfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1\">Alianda Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dingemanse_M/0/1/0/all/0/1\">Mark Dingemanse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ETHER: Aligning Emergent Communication for Hindsight Experience Replay. (arXiv:2307.15494v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15494","description":"<p>Natural language instruction following is paramount to enable collaboration\nbetween artificial agents and human beings. Natural language-conditioned\nreinforcement learning (RL) agents have shown how natural languages'\nproperties, such as compositionality, can provide a strong inductive bias to\nlearn complex policies. Previous architectures like HIGhER combine the benefit\nof language-conditioning with Hindsight Experience Replay (HER) to deal with\nsparse rewards environments. Yet, like HER, HIGhER relies on an oracle\npredicate function to provide a feedback signal highlighting which linguistic\ndescription is valid for which state. This reliance on an oracle limits its\napplication. Additionally, HIGhER only leverages the linguistic information\ncontained in successful RL trajectories, thus hurting its final performance and\ndata-efficiency. Without early successful trajectories, HIGhER is no better\nthan DQN upon which it is built. In this paper, we propose the Emergent Textual\nHindsight Experience Replay (ETHER) agent, which builds on HIGhER and addresses\nboth of its limitations by means of (i) a discriminative visual referential\ngame, commonly studied in the subfield of Emergent Communication (EC), used\nhere as an unsupervised auxiliary task and (ii) a semantic grounding scheme to\nalign the emergent language with the natural language of the\ninstruction-following benchmark. We show that the referential game's agents\nmake an artificial language emerge that is aligned with the natural-like\nlanguage used to describe goals in the BabyAI benchmark and that it is\nexpressive enough so as to also describe unsuccessful RL trajectories and thus\nprovide feedback to the RL agent to leverage the linguistic, structured\ninformation contained in all trajectories. Our work shows that EC is a viable\nunsupervised auxiliary task for RL and provides missing pieces to make HER more\nwidely applicable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Denamganai_K/0/1/0/all/0/1\">Kevin Denamgana&#xef;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_D/0/1/0/all/0/1\">Daniel Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vardal_O/0/1/0/all/0/1\">Ozan Vardal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Missaoui_S/0/1/0/all/0/1\">Sondess Missaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1\">James Alfred Walker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Format Consistency for Instruction Tuning. (arXiv:2307.15504v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15504","description":"<p>Instruction tuning has emerged as a promising approach to enhancing large\nlanguage models in following human instructions. It is shown that increasing\nthe diversity and number of instructions in the training data can consistently\nenhance generalization performance, which facilitates a recent endeavor to\ncollect various instructions and integrate existing instruction tuning datasets\ninto larger collections. However, different users have their unique ways of\nexpressing instructions, and there often exist variations across different\ndatasets in the instruction styles and formats, i.e., format inconsistency. In\nthis work, we study how format inconsistency may impact the performance of\ninstruction tuning. We propose a framework called \"Unified Instruction Tuning\"\n(UIT), which calls OpenAI APIs for automatic format transfer among different\ninstruction tuning datasets. We show that UIT successfully improves the\ngeneralization performance on unseen instructions, which highlights the\nimportance of format consistency for instruction tuning. To make the UIT\nframework more practical, we further propose a novel perplexity-based denoising\nmethod to reduce the noise of automatic format transfer. We also train a\nsmaller offline model that achieves comparable format transfer capability than\nOpenAI APIs to reduce costs in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shihao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kunlun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_R/0/1/0/all/0/1\">Runchu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huadong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1\">Xin Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaojiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Road to Quality is Paved with Good Revisions: A Detailed Evaluation Methodology for Revision Policies in Incremental Sequence Labelling. (arXiv:2307.15508v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15508","description":"<p>Incremental dialogue model components produce a sequence of output prefixes\nbased on incoming input. Mistakes can occur due to local ambiguities or to\nwrong hypotheses, making the ability to revise past outputs a desirable\nproperty that can be governed by a policy. In this work, we formalise and\ncharacterise edits and revisions in incremental sequence labelling and propose\nmetrics to evaluate revision policies. We then apply our methodology to profile\nthe incremental behaviour of three Transformer-based encoders in various tasks,\npaving the road for better revision policies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madureira_B/0/1/0/all/0/1\">Brielen Madureira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahardipraja_P/0/1/0/all/0/1\">Patrick Kahardipraja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlangen_D/0/1/0/all/0/1\">David Schlangen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Oracle Computability and Turing Reducibility in the Calculus of Inductive Constructions. (arXiv:2307.15543v1 [cs.LO])","link":"http://arxiv.org/abs/2307.15543","description":"<p>We develop synthetic notions of oracle computability and Turing reducibility\nin the Calculus of Inductive Constructions (CIC), the constructive type theory\nunderlying the Coq proof assistant. As usual in synthetic approaches, we employ\na definition of oracle computations based on meta-level functions rather than\nobject-level models of computation, relying on the fact that in constructive\nsystems such as CIC all definable functions are computable by construction.\nSuch an approach lends itself well to machine-checked proofs, which we carry\nout in Coq.\n</p>\n<p>There is a tension in finding a good synthetic rendering of the higher-order\nnotion of oracle computability. On the one hand, it has to be informative\nenough to prove central results, ensuring that all notions are faithfully\ncaptured. On the other hand, it has to be restricted enough to benefit from\naxioms for synthetic computability, which usually concern first-order objects.\nDrawing inspiration from a definition by Andrej Bauer based on continuous\nfunctions in the effective topos, we use a notion of sequential continuity to\ncharacterise valid oracle computations.\n</p>\n<p>As main technical results, we show that Turing reducibility forms an upper\nsemilattice, transports decidability, and is strictly more expressive than\ntruth-table reducibility, and prove that whenever both a predicate $p$ and its\ncomplement are semi-decidable relative to an oracle $q$, then $p$\nTuring-reduces to $q$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Forster_Y/0/1/0/all/0/1\">Yannick Forster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirst_D/0/1/0/all/0/1\">Dominik Kirst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muck_N/0/1/0/all/0/1\">Niklas M&#xfc;ck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"'What are you referring to?' Evaluating the Ability of Multi-Modal Dialogue Models to Process Clarificational Exchanges. (arXiv:2307.15554v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15554","description":"<p>Referential ambiguities arise in dialogue when a referring expression does\nnot uniquely identify the intended referent for the addressee. Addressees\nusually detect such ambiguities immediately and work with the speaker to repair\nit using meta-communicative, Clarificational Exchanges (CE): a Clarification\nRequest (CR) and a response. Here, we argue that the ability to generate and\nrespond to CRs imposes specific constraints on the architecture and objective\nfunctions of multi-modal, visually grounded dialogue models. We use the SIMMC\n2.0 dataset to evaluate the ability of different state-of-the-art model\narchitectures to process CEs, with a metric that probes the contextual updates\nthat arise from them in the model. We find that language-based models are able\nto encode simple multi-modal semantic information and process some CEs,\nexcelling with those related to the dialogue history, whilst multi-modal models\ncan use additional learning objectives to obtain disentangled object\nrepresentations, which become crucial to handle complex referential ambiguities\nacross modalities overall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiyah_Garcia_J/0/1/0/all/0/1\">Javier Chiyah-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suglia_A/0/1/0/all/0/1\">Alessandro Suglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eshghi_A/0/1/0/all/0/1\">Arash Eshghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hastie_H/0/1/0/all/0/1\">Helen Hastie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"All-for-One and One-For-All: Deep learning-based feature fusion for Synthetic Speech Detection. (arXiv:2307.15555v1 [cs.SD])","link":"http://arxiv.org/abs/2307.15555","description":"<p>Recent advances in deep learning and computer vision have made the synthesis\nand counterfeiting of multimedia content more accessible than ever, leading to\npossible threats and dangers from malicious users. In the audio field, we are\nwitnessing the growth of speech deepfake generation techniques, which solicit\nthe development of synthetic speech detection algorithms to counter possible\nmischievous uses such as frauds or identity thefts. In this paper, we consider\nthree different feature sets proposed in the literature for the synthetic\nspeech detection task and present a model that fuses them, achieving overall\nbetter performances with respect to the state-of-the-art solutions. The system\nwas tested on different scenarios and datasets to prove its robustness to\nanti-forensic attacks and its generalization capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mari_D/0/1/0/all/0/1\">Daniele Mari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salvi_D/0/1/0/all/0/1\">Davide Salvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bestagini_P/0/1/0/all/0/1\">Paolo Bestagini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milani_S/0/1/0/all/0/1\">Simone Milani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When to generate hedges in peer-tutoring interactions. (arXiv:2307.15582v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15582","description":"<p>This paper explores the application of machine learning techniques to predict\nwhere hedging occurs in peer-tutoring interactions. The study uses a\nnaturalistic face-to-face dataset annotated for natural language turns,\nconversational strategies, tutoring strategies, and nonverbal behaviours. These\nelements are processed into a vector representation of the previous turns,\nwhich serves as input to several machine learning models. Results show that\nembedding layers, that capture the semantic information of the previous turns,\nsignificantly improves the model's performance. Additionally, the study\nprovides insights into the importance of various features, such as\ninterpersonal rapport and nonverbal behaviours, in predicting hedges by using\nShapley values for feature explanation. We discover that the eye gaze of both\nthe tutor and the tutee has a significant impact on hedge prediction. We\nfurther validate this observation through a follow-up ablation study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abulimiti_A/0/1/0/all/0/1\">Alafate Abulimiti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1\">Chlo&#xe9; Clavel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassell_J/0/1/0/all/0/1\">Justine Cassell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Distortion-free Watermarks for Language Models. (arXiv:2307.15593v1 [cs.LG])","link":"http://arxiv.org/abs/2307.15593","description":"<p>We propose a methodology for planting watermarks in text from an\nautoregressive language model that are robust to perturbations without changing\nthe distribution over text up to a certain maximum generation budget. We\ngenerate watermarked text by mapping a sequence of random numbers -- which we\ncompute using a randomized watermark key -- to a sample from the language\nmodel. To detect watermarked text, any party who knows the key can align the\ntext to the random number sequence. We instantiate our watermark methodology\nwith two sampling schemes: inverse transform sampling and exponential minimum\nsampling. We apply these watermarks to three language models -- OPT-1.3B,\nLLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power\nand robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B\nand LLaMA-7B models, we find we can reliably detect watermarked text ($p \\leq\n0.01$) from $35$ tokens even after corrupting between $40$-$50$\\% of the tokens\nvia random edits (i.e., substitutions, insertions or deletions). For the\nAlpaca-7B model, we conduct a case study on the feasibility of watermarking\nresponses to typical user instructions. Due to the lower entropy of the\nresponses, detection is more difficult: around $25\\%$ of the responses -- whose\nmedian length is around $100$ tokens -- are detectable with $p \\leq 0.01$, and\nthe watermark is also less robust to certain automated paraphrasing attacks we\nimplement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuditipudi_R/0/1/0/all/0/1\">Rohith Kuditipudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thickstun_J/0/1/0/all/0/1\">John Thickstun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Data Generation in Vision-and-Language Navigation. (arXiv:2307.15644v1 [cs.CV])","link":"http://arxiv.org/abs/2307.15644","description":"<p>Recent research in language-guided visual navigation has demonstrated a\nsignificant demand for the diversity of traversable environments and the\nquantity of supervision for training generalizable agents. To tackle the common\ndata scarcity issue in existing vision-and-language navigation datasets, we\npropose an effective paradigm for generating large-scale data for learning,\nwhich applies 1200+ photo-realistic environments from HM3D and Gibson datasets\nand synthesizes 4.9 million instruction trajectory pairs using fully-accessible\nresources on the web. Importantly, we investigate the influence of each\ncomponent in this paradigm on the agent's performance and study how to\nadequately apply the augmented data to pre-train and fine-tune an agent. Thanks\nto our large-scale dataset, the performance of an existing agent can be pushed\nup (+11% absolute with regard to previous SoTA) to a significantly new best of\n80% single-run success rate on the R2R test split by simple imitation learning.\nThe long-lasting generalization gap between navigating in seen and unseen\nenvironments is also reduced to less than 1% (versus 8% in the previous best\nmethod). Moreover, our paradigm also facilitates different models to achieve\nnew state-of-the-art navigation results on CVDN, REVERIE, and R2R in continuous\nenvironments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jialu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yicong Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1\">Stephen Gould</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty in Natural Language Generation: From Theory to Applications. (arXiv:2307.15703v1 [cs.CL])","link":"http://arxiv.org/abs/2307.15703","description":"<p>Recent advances of powerful Language Models have allowed Natural Language\nGeneration (NLG) to emerge as an important technology that can not only perform\ntraditional tasks like summarisation or translation, but also serve as a\nnatural language interface to a variety of applications. As such, it is crucial\nthat NLG systems are trustworthy and reliable, for example by indicating when\nthey are likely to be wrong; and supporting multiple views, backgrounds and\nwriting styles -- reflecting diverse human sub-populations. In this paper, we\nargue that a principled treatment of uncertainty can assist in creating systems\nand evaluation protocols better aligned with these goals. We first present the\nfundamental theory, frameworks and vocabulary required to represent\nuncertainty. We then characterise the main sources of uncertainty in NLG from a\nlinguistic perspective, and propose a two-dimensional taxonomy that is more\ninformative and faithful than the popular aleatoric/epistemic dichotomy.\nFinally, we move from theory to applications and highlight exciting research\ndirections that exploit uncertainty to power decoding, controllable generation,\nself-assessment, selective answering, active learning and more.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baan_J/0/1/0/all/0/1\">Joris Baan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daheim_N/0/1/0/all/0/1\">Nico Daheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilia_E/0/1/0/all/0/1\">Evgenia Ilia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulmer_D/0/1/0/all/0/1\">Dennis Ulmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haau-Sing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_R/0/1/0/all/0/1\">Raquel Fern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zerva_C/0/1/0/all/0/1\">Chrysoula Zerva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1\">Wilker Aziz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning From How Humans Correct. (arXiv:2102.00225v14 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.00225","description":"<p>In industry NLP application, our manually labeled data has a certain number\nof noisy data. We present a simple method to find the noisy data and re-label\nthem manually, meanwhile we collect the correction information. Then we present\nnovel method to incorporate the human correction information into deep learning\nmodel. Human know how to correct noisy data. So the correction information can\nbe inject into deep learning model. We do the experiment on our own text\nclassification dataset, which is manually labeled, because we re-label the\nnoisy data in our dataset for our industry application. The experiment result\nshows that our method improve the classification accuracy from 91.7% to 92.5%.\nThe 91.7% accuracy is trained on the corrected dataset, which improve the\nbaseline from 83.3% to 91.7%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Lexical Simplification for Turkish. (arXiv:2201.05878v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05878","description":"<p>In this paper, we present the first automatic lexical simplification system\nfor the Turkish language. Recent text simplification efforts rely on manually\ncrafted simplified corpora and comprehensive NLP tools that can analyse the\ntarget text both in word and sentence levels. Turkish is a morphologically rich\nagglutinative language that requires unique considerations such as the proper\nhandling of inflectional cases. Being a low-resource language in terms of\navailable resources and industrial-strength tools, it makes the text\nsimplification task harder to approach. We present a new text simplification\npipeline based on pretrained representation model BERT together with\nmorphological features to generate grammatically correct and semantically\nappropriate word-level simplifications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uluslu_A/0/1/0/all/0/1\">Ahmet Yavuz Uluslu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Meta-learner via Gradient Similarity for Few-shot Text Classification. (arXiv:2209.04702v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.04702","description":"<p>Few-shot text classification aims to classify the text under the few-shot\nscenario. Most of the previous methods adopt optimization-based meta learning\nto obtain task distribution. However, due to the neglect of matching between\nthe few amount of samples and complicated models, as well as the distinction\nbetween useful and useless task features, these methods suffer from the\noverfitting issue. To address this issue, we propose a novel Adaptive\nMeta-learner via Gradient Similarity (AMGS) method to improve the model\ngeneralization ability to a new task. Specifically, the proposed AMGS\nalleviates the overfitting based on two aspects: (i) acquiring the potential\nsemantic representation of samples and improving model generalization through\nthe self-supervised auxiliary task in the inner loop, (ii) leveraging the\nadaptive meta-learner via gradient similarity to add constraints on the\ngradient obtained by base-learner in the outer loop. Moreover, we make a\nsystematic analysis of the influence of regularization on the entire framework.\nExperimental results on several benchmarks demonstrate that the proposed AMGS\nconsistently improves few-shot text classification performance compared with\nthe state-of-the-art optimization-based meta-learning approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1\">Tianyi Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Honghui Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Q/0/1/0/all/0/1\">Qiaoyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Dezhong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Multimodal Prediction of Spontaneous Humour: A Novel Dataset and First Results. (arXiv:2209.14272v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2209.14272","description":"<p>Humour is a substantial element of human affect and cognition. Its automatic\nunderstanding can facilitate a more naturalistic human-device interaction and\nthe humanisation of artificial intelligence. Current methods of humour\ndetection are solely based on staged data making them inadequate for\n'real-world' applications. We address this deficiency by introducing the novel\nPassau-Spontaneous Football Coach Humour (Passau-SFCH) dataset, comprising of\nabout 11 hours of recordings. The Passau-SFCH dataset is annotated for the\npresence of humour and its dimensions (sentiment and direction) as proposed in\nMartin's Humor Style Questionnaire. We conduct a series of experiments,\nemploying pretrained Transformers, convolutional neural networks, and\nexpert-designed features. The performance of each modality (text, audio, video)\nfor spontaneous humour recognition is analysed and their complementarity is\ninvestigated. Our findings suggest that for the automatic analysis of humour\nand its sentiment, facial expressions are most promising, while humour\ndirection can be best modelled via text-based features. The results reveal\nconsiderable differences among various subjects, highlighting the individuality\nof humour usage and style. Further, we observe that a decision-level fusion\nyields the best recognition result. Finally, we make our code publicly\navailable at https://www.github.com/EIHW/passau-sfch. The Passau-SFCH dataset\nis available upon request.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Christ_L/0/1/0/all/0/1\">Lukas Christ</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amiriparian_S/0/1/0/all/0/1\">Shahin Amiriparian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kathan_A/0/1/0/all/0/1\">Alexander Kathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_N/0/1/0/all/0/1\">Niklas M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konig_A/0/1/0/all/0/1\">Andreas K&#xf6;nig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rationale-Guided Few-Shot Classification to Detect Abusive Language. (arXiv:2211.17046v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.17046","description":"<p>Abusive language is a concerning problem in online social media. Past\nresearch on detecting abusive language covers different platforms, languages,\ndemographies, etc. However, models trained using these datasets do not perform\nwell in cross-domain evaluation settings. To overcome this, a common strategy\nis to use a few samples from the target domain to train models to get better\nperformance in that domain (cross-domain few-shot training). However, this\nmight cause the models to overfit the artefacts of those samples. A compelling\nsolution could be to guide the models toward rationales, i.e., spans of text\nthat justify the text's label. This method has been found to improve model\nperformance in the in-domain setting across various NLP tasks. In this paper,\nwe propose RGFS (Rationale-Guided Few-Shot Classification) for abusive language\ndetection. We first build a multitask learning setup to jointly learn\nrationales, targets, and labels, and find a significant improvement of 6% macro\nF1 on the rationale detection task over training solely rationale classifiers.\nWe introduce two rationale-integrated BERT-based architectures (the RGFS\nmodels) and evaluate our systems over five different abusive language datasets,\nfinding that in the few-shot classification setting, RGFS-based models\noutperform baseline models by about 7% in macro F1 scores and perform\ncompetitively to models finetuned on other source domains. Furthermore,\nRGFS-based models outperform LIME/SHAP-based approaches in terms of\nplausibility and are close in performance in terms of faithfulness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_P/0/1/0/all/0/1\">Punyajoy Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_D/0/1/0/all/0/1\">Divyanshu Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kedia_K/0/1/0/all/0/1\">Kushal Kedia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_B/0/1/0/all/0/1\">Binny Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Animesh Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Answering Climate Questionnaires from Unstructured Climate Reports. (arXiv:2301.04253v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.04253","description":"<p>The topic of Climate Change (CC) has received limited attention in NLP\ndespite its urgency. Activists and policymakers need NLP tools to effectively\nprocess the vast and rapidly growing unstructured textual climate reports into\nstructured form. To tackle this challenge we introduce two new large-scale\nclimate questionnaire datasets and use their existing structure to train\nself-supervised models. We conduct experiments to show that these models can\nlearn to generalize to climate disclosures of different organizations types\nthan seen during training. We then use these models to help align texts from\nunstructured climate documents to the semi-structured questionnaires in a human\npilot study. Finally, to support further NLP research in the climate domain we\nintroduce a benchmark of existing climate text classification datasets to\nbetter evaluate and compare existing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Spokoyny_D/0/1/0/all/0/1\">Daniel Spokoyny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laud_T/0/1/0/all/0/1\">Tanmay Laud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corringham_T/0/1/0/all/0/1\">Tom Corringham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.12247","description":"<p>The recent explosion of interest in multimodal applications has resulted in a\nwide selection of datasets and methods for representing and integrating\ninformation from different modalities. Despite these empirical advances, there\nremain fundamental research questions: How can we quantify the interactions\nthat are necessary to solve a multimodal task? Subsequently, what are the most\nsuitable multimodal models to capture these interactions? To answer these\nquestions, we propose an information-theoretic approach to quantify the degree\nof redundancy, uniqueness, and synergy relating input modalities with an output\ntask. We term these three measures as the PID statistics of a multimodal\ndistribution (or PID for short), and introduce two new estimators for these PID\nstatistics that scale to high-dimensional distributions. To validate PID\nestimation, we conduct extensive experiments on both synthetic datasets where\nthe PID is known and on large-scale multimodal benchmarks where PID estimations\nare compared with human annotations. Finally, we demonstrate their usefulness\nin (1) quantifying interactions within multimodal datasets, (2) quantifying\ninteractions captured by multimodal models, (3) principled approaches for model\nselection, and (4) three real-world case studies engaging with domain experts\nin pathology, mood prediction, and robotic perception where our framework helps\nto recommend strong multimodal models for each application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1\">Chun Kai Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_S/0/1/0/all/0/1\">Suzanne Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Richard Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zihao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allen_N/0/1/0/all/0/1\">Nicholas Allen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auerbach_R/0/1/0/all/0/1\">Randy Auerbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_F/0/1/0/all/0/1\">Faisal Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Open-domain Paradox for Chatbots: Common Ground as the Basis for Human-like Dialogue. (arXiv:2303.11708v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.11708","description":"<p>There is a surge in interest in the development of open-domain chatbots,\ndriven by the recent advancements of large language models. The \"openness\" of\nthe dialogue is expected to be maximized by providing minimal information to\nthe users about the common ground they can expect, including the presumed joint\nactivity. However, evidence suggests that the effect is the opposite. Asking\nusers to \"just chat about anything\" results in a very narrow form of dialogue,\nwhich we refer to as the \"open-domain paradox\". In this position paper, we\nexplain this paradox through the theory of common ground as the basis for\nhuman-like communication. Furthermore, we question the assumptions behind\nopen-domain chatbots and identify paths forward for enabling common ground in\nhuman-computer dialogue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Skantze_G/0/1/0/all/0/1\">Gabriel Skantze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dogruoz_A/0/1/0/all/0/1\">A. Seza Do&#x11f;ru&#xf6;z</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping. (arXiv:2304.07810v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2304.07810","description":"<p>In argumentative writing, writers must brainstorm hierarchical writing goals,\nensure the persuasiveness of their arguments, and revise and organize their\nplans through drafting. Recent advances in large language models (LLMs) have\nmade interactive text generation through a chat interface (e.g., ChatGPT)\npossible. However, this approach often neglects implicit writing context and\nuser intent, lacks support for user control and autonomy, and provides limited\nassistance for sensemaking and revising writing plans. To address these\nchallenges, we introduce VISAR, an AI-enabled writing assistant system designed\nto help writers brainstorm and revise hierarchical goals within their writing\ncontext, organize argument structures through synchronized text editing and\nvisual programming, and enhance persuasiveness with argumentation spark\nrecommendations. VISAR allows users to explore, experiment with, and validate\ntheir writing plans using automatic draft prototyping. A controlled lab study\nconfirmed the usability and effectiveness of VISAR in facilitating the\nargumentative writing planning process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jie Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhaliwal_R/0/1/0/all/0/1\">Ranjodh Singh Dhaliwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Toby Jia-Jun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferring Procedural Knowledge across Commonsense Tasks. (arXiv:2304.13867v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.13867","description":"<p>Stories about everyday situations are an essential part of human\ncommunication, motivating the need to develop AI agents that can reliably\nunderstand these stories. Despite the long list of supervised methods for story\ncompletion and procedural understanding, current AI has no mechanisms to\nautomatically track and explain procedures in unseen stories. To bridge this\ngap, we study the ability of AI models to transfer procedural knowledge to\nnovel narrative tasks in a transparent manner. We design LEAP: a comprehensive\nframework that integrates state-of-the-art modeling architectures, training\nregimes, and augmentation strategies based on both natural and synthetic\nstories. To address the lack of densely annotated training data, we devise a\nrobust automatic labeler based on few-shot prompting to enhance the augmented\ndata. Our experiments with in- and out-of-domain tasks reveal insights into the\ninterplay of different architectures, training regimes, and augmentation\nstrategies. LEAP's labeler has a clear positive impact on out-of-domain\ndatasets, while the resulting dense annotation provides native explainability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaixin Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SANTA: Separate Strategies for Inaccurate and Incomplete Annotation Noise in Distantly-Supervised Named Entity Recognition. (arXiv:2305.04076v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.04076","description":"<p>Distantly-Supervised Named Entity Recognition effectively alleviates the\nburden of time-consuming and expensive annotation in the supervised setting.\nBut the context-free matching process and the limited coverage of knowledge\nbases introduce inaccurate and incomplete annotation noise respectively.\nPrevious studies either considered only incomplete annotation noise or\nindiscriminately handle two types of noise with the same strategy. In this\npaper, we argue that the different causes of two types of noise bring up the\nrequirement of different strategies in model architecture. Therefore, we\npropose the SANTA to handle these two types of noise separately with (1)\nMemory-smoothed Focal Loss and Entity-aware KNN to relieve the entity ambiguity\nproblem caused by inaccurate annotation, and (2) Boundary Mixup to alleviate\ndecision boundary shifting problem caused by incomplete annotation and a\nnoise-tolerant loss to improve the robustness. Benefiting from our separate\ntailored strategies, we confirm in the experiment that the two types of noise\nare well mitigated. SANTA also achieves a new state-of-the-art on five public\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1\">Shuzheng Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zefan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1\">Shuang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_G/0/1/0/all/0/1\">Guoqiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jiaxing Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Local Spectro-Temporal Features for Speech Analysis. (arXiv:2305.10270v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10270","description":"<p>We introduce the problem of phone classification in the context of speech\nrecognition, and explore several sets of local spectro-temporal features that\ncan be used for phone classification. In particular, we present some\npreliminary results for phone classification using two sets of features that\nare commonly used for object detection: Haar features and SVM-classified\nHistograms of Gradients (HoG).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guerzhoy_M/0/1/0/all/0/1\">Michael Guerzhoy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overview of Robust and Multilingual Automatic Evaluation Metrics\\\\for Open-Domain Dialogue Systems at DSTC 11 Track 4. (arXiv:2306.12794v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.12794","description":"<p>The advent and fast development of neural networks have revolutionized the\nresearch on dialogue systems and subsequently have triggered various challenges\nregarding their automatic evaluation. Automatic evaluation of open-domain\ndialogue systems as an open challenge has been the center of the attention of\nmany researchers. Despite the consistent efforts to improve automatic metrics'\ncorrelations with human evaluation, there have been very few attempts to assess\ntheir robustness over multiple domains and dimensions. Also, their focus is\nmainly on the English language. All of these challenges prompt the development\nof automatic evaluation metrics that are reliable in various domains,\ndimensions, and languages. This track in the 11th Dialogue System Technology\nChallenge (DSTC11) is part of the ongoing effort to promote robust and\nmultilingual automatic evaluation metrics. This article describes the datasets\nand baselines provided to participants and discusses the submission and result\ndetails of the two proposed subtasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Cantelar_M/0/1/0/all/0/1\">Mario Rodr&#xed;guez-Cantelar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chengguang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1\">Ke Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghazarian_S/0/1/0/all/0/1\">Sarik Ghazarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DHaro_L/0/1/0/all/0/1\">Luis Fernando D&#x27;Haro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudnicky_A/0/1/0/all/0/1\">Alexander Rudnicky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARB: Advanced Reasoning Benchmark for Large Language Models. (arXiv:2307.13692v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.13692","description":"<p>Large Language Models (LLMs) have demonstrated remarkable performance on\nvarious quantitative reasoning and knowledge benchmarks. However, many of these\nbenchmarks are losing utility as LLMs get increasingly high scores, despite not\nyet reaching expert performance in these domains. We introduce ARB, a novel\nbenchmark composed of advanced reasoning problems in multiple fields. ARB\npresents a more challenging test than prior benchmarks, featuring problems in\nmathematics, physics, biology, chemistry, and law. As a subset of ARB, we\nintroduce a challenging set of math and physics problems which require advanced\nsymbolic reasoning and domain knowledge. We evaluate recent models such as\nGPT-4 and Claude on ARB and demonstrate that current models score well below\n50% on more demanding tasks. In order to improve both automatic and assisted\nevaluation capabilities, we introduce a rubric-based evaluation approach,\nallowing GPT-4 to score its own intermediate reasoning steps. Further, we\nconduct a human evaluation of the symbolic subset of ARB, finding promising\nagreement between annotators and GPT-4 rubric evaluation scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sawada_T/0/1/0/all/0/1\">Tomohiro Sawada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paleka_D/0/1/0/all/0/1\">Daniel Paleka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Havrilla_A/0/1/0/all/0/1\">Alexander Havrilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadepalli_P/0/1/0/all/0/1\">Pranav Tadepalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidas_P/0/1/0/all/0/1\">Paula Vidas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kranias_A/0/1/0/all/0/1\">Alexander Kranias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nay_J/0/1/0/all/0/1\">John J. Nay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Kshitij Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komatsuzaki_A/0/1/0/all/0/1\">Aran Komatsuzaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Turkish Native Language Identification. (arXiv:2307.14850v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.14850","description":"<p>In this paper, we present the first application of Native Language\nIdentification (NLI) for the Turkish language. NLI involves predicting the\nwriter's first language by analysing their writing in different languages.\nWhile most NLI research has focused on English, our study extends its scope to\nTurkish. We used the recently constructed Turkish Learner Corpus and employed a\ncombination of three syntactic features (CFG production rules, part-of-speech\nn-grams, and function words) with L2 texts to demonstrate their effectiveness\nin this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uluslu_A/0/1/0/all/0/1\">Ahmet Yavuz Uluslu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_G/0/1/0/all/0/1\">Gerold Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matching Patients to Clinical Trials with Large Language Models. (arXiv:2307.15051v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.15051","description":"<p>Clinical trials are vital in advancing drug development and evidence-based\nmedicine, but their success is often hindered by challenges in patient\nrecruitment. In this work, we investigate the potential of large language\nmodels (LLMs) to assist individual patients and referral physicians in\nidentifying suitable clinical trials from an extensive selection. Specifically,\nwe introduce TrialGPT, a novel architecture employing LLMs to predict\ncriterion-level eligibility with detailed explanations, which are then\naggregated for ranking and excluding candidate clinical trials based on\nfree-text patient notes. We evaluate TrialGPT on three publicly available\ncohorts of 184 patients and 18,238 annotated clinical trials. The experimental\nresults demonstrate several key findings: First, TrialGPT achieves high\ncriterion-level prediction accuracy with faithful explanations. Second, the\naggregated trial-level TrialGPT scores are highly correlated with expert\neligibility annotations. Third, these scores prove effective in ranking\nclinical trials and exclude ineligible candidates. Our error analysis suggests\nthat current LLMs still make some mistakes due to limited medical knowledge and\ndomain-specific context understanding. Nonetheless, we believe the explanatory\ncapabilities of LLMs are highly valuable. Future research is warranted on how\nsuch AI assistants can be integrated into the routine trial matching workflow\nin real-world settings to improve its efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qiao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Floudas_C/0/1/0/all/0/1\">Charalampos S. Floudas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jimeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-07-30T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}