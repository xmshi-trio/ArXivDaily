{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-11-09T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"CELLS: A Parallel Corpus for Biomedical Lay Language Generation. (arXiv:2211.03818v1 [cs.CL])","link":"http://arxiv.org/abs/2211.03818","description":"<p>Recent lay language generation systems have used Transformer models trained\non a parallel corpus to increase health information accessibility. However, the\napplicability of these models is constrained by the limited size and topical\nbreadth of available corpora. We introduce CELLS, the largest (63k pairs) and\nbroadest-ranging (12 journals) parallel corpus for lay language generation. The\nabstract and the corresponding lay language summary are written by domain\nexperts, assuring the quality of our dataset. Furthermore, qualitative\nevaluation of expert-authored plain language summaries has revealed background\nexplanation as a key strategy to increase accessibility. Such explanation is\nchallenging for neural models to generate because it goes beyond simplification\nby adding content absent from the source. We derive two specialized paired\ncorpora from CELLS to address key challenges in lay language generation:\ngenerating background explanations and simplifying the original abstract. We\nadopt retrieval-augmented models as an intuitive fit for the task of background\nexplanation generation, and show improvements in summary quality and simplicity\nwhile maintaining factual correctness. Taken together, this work presents the\nfirst comprehensive study of background explanation for lay language\ngeneration, paving the path for disseminating scientific knowledge to a broader\naudience. CELLS is publicly available at:\nhttps://github.com/LinguisticAnomalies/pls_retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yue Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1\">Wei Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leroy_G/0/1/0/all/0/1\">Gondy Leroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1\">Trevor Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AX-MABSA: A Framework for Extremely Weakly Supervised Multi-label Aspect Based Sentiment Analysis. (arXiv:2211.03837v1 [cs.CL])","link":"http://arxiv.org/abs/2211.03837","description":"<p>Aspect Based Sentiment Analysis is a dominant research area with potential\napplications in social media analytics, business, finance, and health. Prior\nworks in this area are primarily based on supervised methods, with a few\ntechniques using weak supervision limited to predicting a single aspect\ncategory per review sentence. In this paper, we present an extremely weakly\nsupervised multi-label Aspect Category Sentiment Analysis framework which does\nnot use any labelled data. We only rely on a single word per class as an\ninitial indicative information. We further propose an automatic word selection\ntechnique to choose these seed categories and sentiment words. We explore\nunsupervised language model post-training to improve the overall performance,\nand propose a multi-label generator model to generate multiple aspect\ncategory-sentiment pairs per review sentence. Experiments conducted on four\nbenchmark datasets showcase our method to outperform other weakly supervised\nbaselines by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamila_S/0/1/0/all/0/1\">Sabyasachi Kamila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magdy_W/0/1/0/all/0/1\">Walid Magdy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Sourav Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">MingXue Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Looking at the Overlooked: An Analysis on the Word-Overlap Bias in Natural Language Inference. (arXiv:2211.03862v1 [cs.CL])","link":"http://arxiv.org/abs/2211.03862","description":"<p>It has been shown that NLI models are usually biased with respect to the\nword-overlap between premise and hypothesis; they take this feature as a\nprimary cue for predicting the entailment label. In this paper, we focus on an\noverlooked aspect of the overlap bias in NLI models: the reverse word-overlap\nbias. Our experimental results demonstrate that current NLI models are highly\nbiased towards the non-entailment label on instances with low overlap, and the\nexisting debiasing methods, which are reportedly successful on existing\nchallenge datasets, are generally ineffective in addressing this category of\nbias. We investigate the reasons for the emergence of the overlap bias and the\nrole of minority examples in its mitigation. For the former, we find that the\nword-overlap bias does not stem from pre-training, and for the latter, we\nobserve that in contrast to the accepted assumption, eliminating minority\nexamples does not affect the generalizability of debiasing methods with respect\nto the overlap bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajaee_S/0/1/0/all/0/1\">Sara Rajaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaghoobzadeh_Y/0/1/0/all/0/1\">Yadollah Yaghoobzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilehvar_M/0/1/0/all/0/1\">Mohammad Taher Pilehvar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Strictly Breadth-First AMR Parsing. (arXiv:2211.03922v1 [cs.CL])","link":"http://arxiv.org/abs/2211.03922","description":"<p>AMR parsing is the task that maps a sentence to an AMR semantic graph\nautomatically. We focus on the breadth-first strategy of this task, which was\nproposed recently and achieved better performance than other strategies.\nHowever, current models under this strategy only \\emph{encourage} the model to\nproduce the AMR graph in breadth-first order, but \\emph{cannot guarantee} this.\nTo solve this problem, we propose a new architecture that \\emph{guarantees}\nthat the parsing will strictly follow the breadth-first order. In each parsing\nstep, we introduce a \\textbf{focused parent} vertex and use this vertex to\nguide the generation. With the help of this new architecture and some other\nimprovements in the sentence and graph encoder, our model obtains better\nperformance on both the AMR 1.0 and 2.0 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gildea_D/0/1/0/all/0/1\">Daniel Gildea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proactive Detractor Detection Framework Based on Message-Wise Sentiment Analysis Over Customer Support Interactions. (arXiv:2211.03923v1 [cs.CL])","link":"http://arxiv.org/abs/2211.03923","description":"<p>In this work, we propose a framework relying solely on chat-based customer\nsupport (CS) interactions for predicting the recommendation decision of\nindividual users. For our case study, we analyzed a total number of 16.4k users\nand 48.7k customer support conversations within the financial vertical of a\nlarge e-commerce company in Latin America. Consequently, our main contributions\nand objectives are to use Natural Language Processing (NLP) to assess and\npredict the recommendation behavior where, in addition to using static\nsentiment analysis, we exploit the predictive power of each user's sentiment\ndynamics. Our results show that, with respective feature interpretability, it\nis possible to predict the likelihood of a user to recommend a product or\nservice, based solely on the message-wise sentiment evolution of their CS\nconversations in a fully automated way.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gallo_J/0/1/0/all/0/1\">Juan Sebasti&#xe1;n Salcedo Gallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solano_J/0/1/0/all/0/1\">Jes&#xfa;s Solano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_J/0/1/0/all/0/1\">Javier Hern&#xe1;n Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarruk_Valencia_D/0/1/0/all/0/1\">David Zarruk-Valencia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correa_Bahnsen_A/0/1/0/all/0/1\">Alejandro Correa-Bahnsen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparative layer-wise analysis of self-supervised speech models. (arXiv:2211.03929v1 [cs.CL])","link":"http://arxiv.org/abs/2211.03929","description":"<p>Many self-supervised speech models, varying in their pre-training objective,\ninput modality, and pre-training data, have been proposed in the last few\nyears. Despite impressive empirical successes on downstream tasks, we still\nhave a limited understanding of the properties encoded by the models and the\ndifferences across models. In this work, we examine the intermediate\nrepresentations for a variety of recent models. Specifically, we measure\nacoustic, phonetic, and word-level properties encoded in individual layers,\nusing a lightweight analysis tool based on canonical correlation analysis\n(CCA). We find that these properties evolve across layers differently depending\non the model, and the variations relate to the choice of pre-training\nobjective. We further investigate the utility of our analyses for downstream\ntasks by comparing the property trends with performance on speech recognition\nand spoken language understanding tasks. We discover that CCA trends provide\nreliable guidance to choose layers of interest for downstream tasks and that\nsingle-layer performance often matches or improves upon using all layers,\nsuggesting implications for more efficient use of pre-trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pasad_A/0/1/0/all/0/1\">Ankita Pasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bowen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tell Your Story: Task-Oriented Dialogs for Interactive Content Creation. (arXiv:2211.03940v1 [cs.CL])","link":"http://arxiv.org/abs/2211.03940","description":"<p>People capture photos and videos to relive and share memories of personal\nsignificance. Recently, media montages (stories) have become a popular mode of\nsharing these memories due to their intuitive and powerful storytelling\ncapabilities. However, creating such montages usually involves a lot of manual\nsearches, clicks, and selections that are time-consuming and cumbersome,\nadversely affecting user experiences.\n</p>\n<p>To alleviate this, we propose task-oriented dialogs for montage creation as a\nnovel interactive tool to seamlessly search, compile, and edit montages from a\nmedia collection. To the best of our knowledge, our work is the first to\nleverage multi-turn conversations for such a challenging application, extending\nthe previous literature studying simple media retrieval tasks. We collect a new\ndataset C3 (Conversational Content Creation), comprising 10k dialogs\nconditioned on media montages simulated from a large media collection.\n</p>\n<p>We take a simulate-and-paraphrase approach to collect these dialogs to be\nboth cost and time efficient, while drawing from natural language distribution.\nOur analysis and benchmarking of state-of-the-art language models showcase the\nmultimodal challenges present in the dataset. Lastly, we present a real-world\nmobile demo application that shows the feasibility of the proposed work in\nreal-world applications. Our code and data will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kottur_S/0/1/0/all/0/1\">Satwik Kottur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Seungwhan Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markosyan_A/0/1/0/all/0/1\">Aram H. Markosyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_H/0/1/0/all/0/1\">Hardik Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damavandi_B/0/1/0/all/0/1\">Babak Damavandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geramifard_A/0/1/0/all/0/1\">Alborz Geramifard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter and Data Efficient Continual Pre-training for Robustness to Dialectal Variance in Arabic. (arXiv:2211.03966v1 [cs.CL])","link":"http://arxiv.org/abs/2211.03966","description":"<p>The use of multilingual language models for tasks in low and high-resource\nlanguages has been a success story in deep learning. In recent times, Arabic\nhas been receiving widespread attention on account of its dialectal variance.\nWhile prior research studies have tried to adapt these multilingual models for\ndialectal variants of Arabic, it still remains a challenging problem owing to\nthe lack of sufficient monolingual dialectal data and parallel translation data\nof such dialectal variants. It remains an open problem on whether the limited\ndialectical data can be used to improve the models trained in Arabic on its\ndialectal variants. First, we show that multilingual-BERT (mBERT) incrementally\npretrained on Arabic monolingual data takes less training time and yields\ncomparable accuracy when compared to our custom monolingual Arabic model and\nbeat existing models (by an avg metric of +$6.41$). We then explore two\ncontinual pre-training methods -- (1) using small amounts of dialectical data\nfor continual finetuning and (2) parallel Arabic to English data and a\nTranslation Language Modeling loss function. We show that both approaches help\nimprove performance on dialectal classification tasks ($+4.64$ avg. gain) when\nused on monolingual models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Soumajyoti Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kaixiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Sailik Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lausen_L/0/1/0/all/0/1\">Leonard Lausen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_S/0/1/0/all/0/1\">Sheng Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_S/0/1/0/all/0/1\">Saab Mansour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation for Sparse Retrieval by Filling Vocabulary and Word Frequency Gaps. (arXiv:2211.03988v1 [cs.CL])","link":"http://arxiv.org/abs/2211.03988","description":"<p>IR models using a pretrained language model significantly outperform lexical\napproaches like BM25. In particular, SPLADE, which encodes texts to sparse\nvectors, is an effective model for practical use because it shows robustness to\nout-of-domain datasets. However, SPLADE still struggles with exact matching of\nlow-frequency words in training data. In addition, domain shifts in vocabulary\nand word frequencies deteriorate the IR performance of SPLADE. Because\nsupervision data are scarce in the target domain, addressing the domain shifts\nwithout supervision data is necessary. This paper proposes an unsupervised\ndomain adaptation method by filling vocabulary and word-frequency gaps. First,\nwe expand a vocabulary and execute continual pretraining with a masked language\nmodel on a corpus of the target domain. Then, we multiply SPLADE-encoded sparse\nvectors by inverse document frequency weights to consider the importance of\ndocuments with lowfrequency words. We conducted experiments using our method on\ndatasets with a large vocabulary gap from a source domain. We show that our\nmethod outperforms the present stateof-the-art domain adaptation method. In\naddition, our method achieves state-of-the-art results, combined with BM25.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iida_H/0/1/0/all/0/1\">Hiroki Iida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1\">Naoaki Okazaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Unstructured Knowledge Access in Conversational Dialogue with ASR Errors. (arXiv:2211.03990v1 [cs.CL])","link":"http://arxiv.org/abs/2211.03990","description":"<p>Performance of spoken language understanding (SLU) can be degraded with\nautomatic speech recognition (ASR) errors. We propose a novel approach to\nimprove SLU robustness by randomly corrupting clean training text with an ASR\nerror simulator, followed by self-correcting the errors and minimizing the\ntarget classification loss in a joint manner. In the proposed error simulator,\nwe leverage confusion networks generated from an ASR decoder without human\ntranscriptions to generate a variety of error patterns for model training. We\nevaluate our approach on the DSTC10 challenge targeted for knowledge-grounded\ntask-oriented conversational dialogues with ASR errors. Experimental results\nshow the effectiveness of our proposed approach, boosting the knowledge-seeking\nturn detection (KTD) F1 significantly from 0.9433 to 0.9904. Knowledge cluster\nclassification is boosted from 0.7924 to 0.9333 in Recall@1. After knowledge\ndocument re-ranking, our approach shows significant improvement in all\nknowledge selection metrics, from 0.7358 to 0.7806 in Recall@1, from 0.8301 to\n0.9333 in Recall@5, and from 0.7798 to 0.8460 in MRR@5 on the test set. In the\nrecent DSTC10 evaluation, our approach demonstrates significant improvement in\nknowledge selection, boosting Recall@1 from 0.495 to 0.7144 compared to the\nofficial baseline. Our source code is released in GitHub\nhttps://github.com/yctam/dstc10_track2_task2.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tam_Y/0/1/0/all/0/1\">Yik-Cheung Tam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiacheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Jiakai Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zecheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_T/0/1/0/all/0/1\">Tinglong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Shuhan Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COV19IR : COVID-19 Domain Literature Information Retrieval. (arXiv:2211.04013v1 [cs.IR])","link":"http://arxiv.org/abs/2211.04013","description":"<p>Increasing number of COVID-19 research literatures cause new challenges in\neffective literature screening and COVID-19 domain knowledge aware Information\nRetrieval. To tackle the challenges, we demonstrate two tasks along\nwithsolutions, COVID-19 literature retrieval, and question answering. COVID-19\nliterature retrieval task screens matching COVID-19 literature documents for\ntextual user query, and COVID-19 question answering task predicts proper text\nfragments from text corpus as the answer of specific COVID-19 related\nquestions. Based on transformer neural network, we provided solutions to\nimplement the tasks on CORD-19 dataset, we display some examples to show the\neffectiveness of our proposed solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bose_A/0/1/0/all/0/1\">Arusarka Bose</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zili Zhou</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guandong Xu</a> (3) ((1) Indian Institute of Technology Kharagpur, (2) University of Manchester, (3) University of Technology Sydney)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dynamic Graph Interactive Framework with Label-Semantic Injection for Spoken Language Understanding. (arXiv:2211.04023v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04023","description":"<p>Multi-intent detection and slot filling joint models are gaining increasing\ntraction since they are closer to complicated real-world scenarios. However,\nexisting approaches (1) focus on identifying implicit correlations between\nutterances and one-hot encoded labels in both tasks while ignoring explicit\nlabel characteristics; (2) directly incorporate multi-intent information for\neach token, which could lead to incorrect slot prediction due to the\nintroduction of irrelevant intent. In this paper, we propose a framework termed\nDGIF, which first leverages the semantic information of labels to give the\nmodel additional signals and enriched priors. Then, a multi-grain interactive\ngraph is constructed to model correlations between intents and slots.\nSpecifically, we propose a novel approach to construct the interactive graph\nbased on the injection of label semantics, which can automatically update the\ngraph to better alleviate error propagation. Experimental results show that our\nframework significantly outperforms existing approaches, obtaining a relative\nimprovement of 13.7% over the previous best model on the MixATIS dataset in\noverall accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhihong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xuxin Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1\">Tengtao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Knowledge Is Needed? Towards Explainable Memory for kNN-MT Domain Adaptation. (arXiv:2211.04052v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04052","description":"<p>kNN-MT presents a new paradigm for domain adaptation by building an external\ndatastore, which usually saves all target language token occurrences in the\nparallel corpus. As a result, the constructed datastore is usually large and\npossibly redundant. In this paper, we investigate the interpretability issue of\nthis approach: what knowledge does the NMT model need? We propose the notion of\nlocal correctness (LAC) as a new angle, which describes the potential\ntranslation correctness for a single entry and for a given neighborhood.\nEmpirical study shows that our investigation successfully finds the conditions\nwhere the NMT model could easily fail and need related knowledge. Experiments\non six diverse target domains and two language-pairs show that pruning\naccording to local correctness brings a light and more explainable memory for\nkNN-MT domain adaptation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenhao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yunzhe Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ATCO2 corpus: A Large-Scale Dataset for Research on Automatic Speech Recognition and Natural Language Understanding of Air Traffic Control Communications. (arXiv:2211.04054v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04054","description":"<p>Personal assistants, automatic speech recognizers and dialogue understanding\nsystems are becoming more critical in our interconnected digital world. A clear\nexample is air traffic control (ATC) communications. ATC aims at guiding\naircraft and controlling the airspace in a safe and optimal manner. These\nvoice-based dialogues are carried between an air traffic controller (ATCO) and\npilots via very-high frequency radio channels. In order to incorporate these\nnovel technologies into ATC (low-resource domain), large-scale annotated\ndatasets are required to develop the data-driven AI systems. Two examples are\nautomatic speech recognition (ASR) and natural language understanding (NLU). In\nthis paper, we introduce the ATCO2 corpus, a dataset that aims at fostering\nresearch on the challenging ATC field, which has lagged behind due to lack of\nannotated data. The ATCO2 corpus covers 1) data collection and pre-processing,\n2) pseudo-annotations of speech data, and 3) extraction of ATC-related named\nentities. The ATCO2 corpus is split into three subsets. 1) ATCO2-test-set\ncorpus contains 4 hours of ATC speech with manual transcripts and a subset with\ngold annotations for named-entity recognition (callsign, command, value). 2)\nThe ATCO2-PL-set corpus consists of 5281 hours of unlabeled ATC data enriched\nwith automatic transcripts from an in-domain speech recognizer, contextual\ninformation, speaker turn information, signal-to-noise ratio estimate and\nEnglish language detection score per sample. Both available for purchase\nthrough ELDA at <a href=\"http://catalog.elra.info/en-us/repository/browse/ELRA-S0484.\">this http URL</a> 3)\nThe ATCO2-test-set-1h corpus is a one-hour subset from the original test set\ncorpus, that we are offering for free at https://www.atco2.org/data. We expect\nthe ATCO2 corpus will foster research on robust ASR and NLU not only in the\nfield of ATC communications but also in the general research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1\">Juan Zuluaga-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vesely_K/0/1/0/all/0/1\">Karel Vesel&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szoke_I/0/1/0/all/0/1\">Igor Sz&#xf6;ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocour_M/0/1/0/all/0/1\">Martin Kocour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigault_M/0/1/0/all/0/1\">Mickael Rigault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choukri_K/0/1/0/all/0/1\">Khalid Choukri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1\">Amrutha Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarfjoo_S/0/1/0/all/0/1\">Seyyed Saeed Sarfjoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nigmatulina_I/0/1/0/all/0/1\">Iuliia Nigmatulina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cevenini_C/0/1/0/all/0/1\">Claudia Cevenini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolcarek_P/0/1/0/all/0/1\">Pavel Kol&#x10d;&#xe1;rek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tart_A/0/1/0/all/0/1\">Allan Tart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cernocky_J/0/1/0/all/0/1\">Jan &#x10c;ernock&#xfd;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-resolution embedding extractor for speaker diarisation. (arXiv:2211.04060v1 [cs.SD])","link":"http://arxiv.org/abs/2211.04060","description":"<p>Speaker embedding extractors significantly influence the performance of\nclustering-based speaker diarisation systems. Conventionally, only one\nembedding is extracted from each speech segment. However, because of the\nsliding window approach, a segment easily includes two or more speakers owing\nto speaker change points. This study proposes a novel embedding extractor\narchitecture, referred to as a high-resolution embedding extractor (HEE), which\nextracts multiple high-resolution embeddings from each speech segment. Hee\nconsists of a feature-map extractor and an enhancer, where the enhancer with\nthe self-attention mechanism is the key to success. The enhancer of HEE\nreplaces the aggregation process; instead of a global pooling layer, the\nenhancer combines relative information to each frame via attention leveraging\nthe global context. Extracted dense frame-level embeddings can each represent a\nspeaker. Thus, multiple speakers can be represented by different frame-level\nfeatures in each segment. We also propose an artificially generating mixture\ndata training framework to train the proposed HEE. Through experiments on five\nevaluation sets, including four public datasets, the proposed HEE demonstrates\nat least 10% improvement on each evaluation set, except for one dataset, which\nwe analyse that rapid speaker changes less exist.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heo_H/0/1/0/all/0/1\">Hee-Soo Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Youngki Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bong-Jin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">You Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1\">Jee-weon Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COPEN: Probing Conceptual Knowledge in Pre-trained Language Models. (arXiv:2211.04079v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04079","description":"<p>Conceptual knowledge is fundamental to human cognition and knowledge bases.\nHowever, existing knowledge probing works only focus on evaluating factual\nknowledge of pre-trained language models (PLMs) and ignore conceptual\nknowledge. Since conceptual knowledge often appears as implicit commonsense\nbehind texts, designing probes for conceptual knowledge is hard. Inspired by\nknowledge representation schemata, we comprehensively evaluate conceptual\nknowledge of PLMs by designing three tasks to probe whether PLMs organize\nentities by conceptual similarities, learn conceptual properties, and\nconceptualize entities in contexts, respectively. For the tasks, we collect and\nannotate 24k data instances covering 393 concepts, which is COPEN, a COnceptual\nknowledge Probing bENchmark. Extensive experiments on different sizes and types\nof PLMs show that existing PLMs systematically lack conceptual knowledge and\nsuffer from various spurious correlations. We believe this is a critical\nbottleneck for realizing human-like cognition in PLMs. COPEN and our codes are\npublicly released at https://github.com/THU-KEG/COPEN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengding Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConsPrompt: Easily Exploiting Contrastive Samples for Few-shot Prompt Learning. (arXiv:2211.04118v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04118","description":"<p>Prompt learning recently become an effective linguistic tool to motivate the\nPLMs' knowledge on few-shot-setting tasks. However, studies have shown the lack\nof robustness still exists in prompt learning, since suitable initialization of\ncontinuous prompt and expert-first manual prompt are essential in fine-tuning\nprocess. What is more, human also utilize their comparative ability to motivate\ntheir existing knowledge for distinguishing different examples. Motivated by\nthis, we explore how to use contrastive samples to strengthen prompt learning.\nIn detail, we first propose our model ConsPrompt combining with prompt encoding\nnetwork, contrastive sampling module, and contrastive scoring module.\nSubsequently, two sampling strategies, similarity-based and label-based\nstrategies, are introduced to realize differential contrastive learning. The\neffectiveness of proposed ConsPrompt is demonstrated in five different few-shot\nlearning tasks and shown the similarity-based sampling strategy is more\neffective than label-based in combining contrastive learning. Our results also\nexhibits the state-of-the-art performance and robustness in different few-shot\nsettings, which proves that the ConsPrompt could be assumed as a better\nknowledge probe to motivate PLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weng_J/0/1/0/all/0/1\">Jinta Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhihong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conciseness: An Overlooked Language Task. (arXiv:2211.04126v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04126","description":"<p>We report on novel investigations into training models that make sentences\nconcise. We define the task and show that it is different from related tasks\nsuch as summarization and simplification. For evaluation, we release two test\nsets, consisting of 2000 sentences each, that were annotated by two and five\nhuman annotators, respectively. We demonstrate that conciseness is a difficult\ntask for which zero-shot setups with large neural language models often do not\nperform well. Given the limitations of these approaches, we propose a synthetic\ndata generation method based on round-trip translations. Using this data to\neither train Transformers from scratch or fine-tune T5 models yields our\nstrongest baselines that can be further improved by fine-tuning on an\nartificial conciseness dataset that we derived from multi-annotator machine\ntranslation test sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stahlberg_F/0/1/0/all/0/1\">Felix Stahlberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Aashish Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alberti_C/0/1/0/all/0/1\">Chris Alberti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shankar Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning with Tabular Language Models. (arXiv:2211.04128v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04128","description":"<p>Despite recent advancements in tabular language model research, real-world\napplications are still challenging. In industry, there is an abundance of\ntables found in spreadsheets, but acquisition of substantial amounts of labels\nis expensive, since only experts can annotate the often highly technical and\ndomain-specific tables. Active learning could potentially reduce labeling\ncosts, however, so far there are no works related to active learning in\nconjunction with tabular language models. In this paper we investigate\ndifferent acquisition functions in a real-world industrial tabular language\nmodel use case for sub-cell named entity recognition. Our results show that\ncell-level acquisition functions with built-in diversity can significantly\nreduce the labeling effort, while enforced table diversity is detrimental. We\nfurther see open fundamental questions concerning computational efficiency and\nthe perspective of human annotators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ringsquandl_M/0/1/0/all/0/1\">Martin Ringsquandl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koleva_A/0/1/0/all/0/1\">Aneta Koleva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perspectives on neural proof nets. (arXiv:2211.04141v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04141","description":"<p>In this paper I will present a novel way of combining proof net proof search\nwith neural networks. It contrasts with the 'standard' approach which has been\napplied to proof search in type-logical grammars in various different forms. In\nthe standard approach, we first transform words to formulas (supertagging) then\nmatch atomic formulas to obtain a proof. I will introduce an alternative way to\nsplit the task into two: first, we generate the graph structure in a way which\nguarantees it corresponds to a lambda-term, then we obtain the detailed\nstructure using vertex labelling. Vertex labelling is a well-studied task in\ngraph neural networks, and different ways of implementing graph generation\nusing neural networks will be explored.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moot_R/0/1/0/all/0/1\">Richard Moot</a> (TEXTE, LIRMM, CNRS)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query-Specific Knowledge Graphs for Complex Finance Topics. (arXiv:2211.04142v1 [cs.IR])","link":"http://arxiv.org/abs/2211.04142","description":"<p>Across the financial domain, researchers answer complex questions by\nextensively \"searching\" for relevant information to generate long-form reports.\nThis workshop paper discusses automating the construction of query-specific\ndocument and entity knowledge graphs (KGs) for complex research topics. We\nfocus on the CODEC dataset, where domain experts (1) create challenging\nquestions, (2) construct long natural language narratives, and (3) iteratively\nsearch and assess the relevance of documents and entities. For the construction\nof query-specific KGs, we show that state-of-the-art ranking systems have\nheadroom for improvement, with specific failings due to a lack of context or\nexplicit knowledge representation. We demonstrate that entity and document\nrelevance are positively correlated, and that entity-based query feedback\nimproves document ranking effectiveness. Furthermore, we construct\nquery-specific KGs using retrieval and evaluate using CODEC's \"ground-truth\ngraphs\", showing the precision and recall trade-offs. Lastly, we point to\nfuture work, including adaptive KG retrieval algorithms and GNN-based weighting\nmethods, while highlighting key challenges such as high-quality data,\ninformation extraction recall, and the size and sparsity of complex topic\ngraphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mackie_I/0/1/0/all/0/1\">Iain Mackie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalton_J/0/1/0/all/0/1\">Jeffrey Dalton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Third-Party Aligner for Neural Word Alignments. (arXiv:2211.04198v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04198","description":"<p>Word alignment is to find translationally equivalent words between source and\ntarget sentences. Previous work has demonstrated that self-training can achieve\ncompetitive word alignment results. In this paper, we propose to use word\nalignments generated by a third-party word aligner to supervise the neural word\nalignment training. Specifically, source word and target word of each word pair\naligned by the third-party aligner are trained to be close neighbors to each\nother in the contextualized embedding space when fine-tuning a pre-trained\ncross-lingual language model. Experiments on the benchmarks of various language\npairs show that our approach can surprisingly do self-correction over the\nthird-party supervision by finding more accurate word alignments and deleting\nwrong word alignments, leading to better performance than various third-party\nword aligners, including the currently best one. When we integrate all\nsupervisions from various third-party aligners, we achieve state-of-the-art\nword alignment performances, with averagely more than two points lower\nalignment error rates than the best third-party aligner. We released our code\nat https://github.com/sdongchuanqi/Third-Party-Supervised-Aligner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chuanqi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_X/0/1/0/all/0/1\">Xiangyu Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preserving Semantics in Textual Adversarial Attacks. (arXiv:2211.04205v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04205","description":"<p>Adversarial attacks in NLP challenge the way we look at language models. The\ngoal of this kind of adversarial attack is to modify the input text to fool a\nclassifier while maintaining the original meaning of the text. Although most\nexisting adversarial attacks claim to fulfill the constraint of semantics\npreservation, careful scrutiny shows otherwise. We show that the problem lies\nin the text encoders used to determine the similarity of adversarial examples,\nspecifically in the way they are trained. Unsupervised training methods make\nthese encoders more susceptible to problems with antonym recognition. To\novercome this, we introduce a simple, fully supervised sentence embedding\ntechnique called Semantics-Preserving-Encoder (SPE). The results show that our\nsolution minimizes the variation in the meaning of the adversarial examples\ngenerated. It also significantly improves the overall quality of adversarial\nexamples, as confirmed by human evaluators. Furthermore, it can be used as a\ncomponent in any existing attack to speed up its execution while maintaining\nsimilar attack success.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Herel_D/0/1/0/all/0/1\">David Herel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cisneros_H/0/1/0/all/0/1\">Hugo Cisneros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikolov_T/0/1/0/all/0/1\">Tomas Mikolov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Relation Discovery: Towards General and Label-aware Open Relation Extraction. (arXiv:2211.04215v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04215","description":"<p>Open Relation Extraction (OpenRE) aims to discover novel relations from open\ndomains. Previous OpenRE methods mainly suffer from two problems: (1)\nInsufficient capacity to discriminate between known and novel relations. When\nextending conventional test settings to a more general setting where test data\nmight also come from seen classes, existing approaches have a significant\nperformance decline. (2) Secondary labeling must be performed before practical\napplication. Existing methods cannot label human-readable and meaningful types\nfor novel relations, which is urgently required by the downstream tasks. To\naddress these issues, we propose the Active Relation Discovery (ARD) framework,\nwhich utilizes relational outlier detection for discriminating known and novel\nrelations and involves active learning for labeling novel relations. Extensive\nexperiments on three real-world datasets show that ARD significantly\noutperforms previous state-of-the-art methods on both conventional and our\nproposed general OpenRE settings. The source code and datasets will be\navailable for reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hong-Gee Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-conditioned Embedding Diffusion for Text Generation. (arXiv:2211.04236v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04236","description":"<p>Can continuous diffusion models bring the same performance breakthrough on\nnatural language they did for image generation? To circumvent the discrete\nnature of text data, we can simply project tokens in a continuous space of\nembeddings, as is standard in language modeling. We propose Self-conditioned\nEmbedding Diffusion, a continuous diffusion mechanism that operates on token\nembeddings and allows to learn flexible and scalable diffusion models for both\nconditional and unconditional text generation. Through qualitative and\nquantitative evaluation, we show that our text diffusion models generate\nsamples comparable with those produced by standard autoregressive language\nmodels - while being in theory more efficient on accelerator hardware at\ninference time. Our work paves the way for scaling up diffusion models for\ntext, similarly to autoregressive models, and for improving performance with\nrecent refinements to continuous diffusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Strudel_R/0/1/0/all/0/1\">Robin Strudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tallec_C/0/1/0/all/0/1\">Corentin Tallec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altche_F/0/1/0/all/0/1\">Florent Altch&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yilun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganin_Y/0/1/0/all/0/1\">Yaroslav Ganin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensch_A/0/1/0/all/0/1\">Arthur Mensch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grathwohl_W/0/1/0/all/0/1\">Will Grathwohl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savinov_N/0/1/0/all/0/1\">Nikolay Savinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dieleman_S/0/1/0/all/0/1\">Sander Dieleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifre_L/0/1/0/all/0/1\">Laurent Sifre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leblond_R/0/1/0/all/0/1\">R&#xe9;mi Leblond</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DetAIL : A Tool to Automatically Detect and Analyze Drift In Language. (arXiv:2211.04250v1 [cs.LG])","link":"http://arxiv.org/abs/2211.04250","description":"<p>Machine learning and deep learning-based decision making has become part of\ntoday's software. The goal of this work is to ensure that machine learning and\ndeep learning-based systems are as trusted as traditional software. Traditional\nsoftware is made dependable by following rigorous practice like static\nanalysis, testing, debugging, verifying, and repairing throughout the\ndevelopment and maintenance life-cycle. Similarly for machine learning systems,\nwe need to keep these models up to date so that their performance is not\ncompromised. For this, current systems rely on scheduled re-training of these\nmodels as new data kicks in. In this work, we propose to measure the data drift\nthat takes place when new data kicks in so that one can adaptively re-train the\nmodels whenever re-training is actually required irrespective of schedules. In\naddition to that, we generate various explanations at sentence level and\ndataset level to capture why a given payload text has drifted.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madaan_N/0/1/0/all/0/1\">Nishtha Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manjunatha_A/0/1/0/all/0/1\">Adithya Manjunatha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nambiar_H/0/1/0/all/0/1\">Hrithik Nambiar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_A/0/1/0/all/0/1\">Aviral Kumar Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_H/0/1/0/all/0/1\">Harivansh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_D/0/1/0/all/0/1\">Diptikalyan Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bedathur_S/0/1/0/all/0/1\">Srikanta Bedathur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging Fairness and Environmental Sustainability in Natural Language Processing. (arXiv:2211.04256v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04256","description":"<p>Fairness and environmental impact are important research directions for the\nsustainable development of artificial intelligence. However, while each topic\nis an active research area in natural language processing (NLP), there is a\nsurprising lack of research on the interplay between the two fields. This\nlacuna is highly problematic, since there is increasing evidence that an\nexclusive focus on fairness can actually hinder environmental sustainability,\nand vice versa. In this work, we shed light on this crucial intersection in NLP\nby (1) investigating the efficiency of current fairness approaches through\nsurveying example methods for reducing unfair stereotypical bias from the\nliterature, and (2) evaluating a common technique to reduce energy consumption\n(and thus environmental impact) of English NLP models, knowledge distillation\n(KD), for its impact on fairness. In this case study, we evaluate the effect of\nimportant KD factors, including layer and dimensionality reduction, with\nrespect to: (a) performance on the distillation task (natural language\ninference and semantic similarity prediction), and (b) multiple measures and\ndimensions of stereotypical bias (e.g., gender bias measured via the Word\nEmbedding Association Test). Our results lead us to clarify current assumptions\nregarding the effect of KD on unfair bias: contrary to other findings, we show\nthat KD can actually decrease model fairness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hessenthaler_M/0/1/0/all/0/1\">Marius Hessenthaler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1\">Emma Strubell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SocioProbe: What, When, and Where Language Models Learn about Sociodemographics. (arXiv:2211.04281v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04281","description":"<p>Pre-trained language models (PLMs) have outperformed other NLP models on a\nwide range of tasks. Opting for a more thorough understanding of their\ncapabilities and inner workings, researchers have established the extend to\nwhich they capture lower-level knowledge like grammaticality, and mid-level\nsemantic knowledge like factual understanding. However, there is still little\nunderstanding of their knowledge of higher-level aspects of language. In\nparticular, despite the importance of sociodemographic aspects in shaping our\nlanguage, the questions of whether, where, and how PLMs encode these aspects,\ne.g., gender or age, is still unexplored. We address this research gap by\nprobing the sociodemographic knowledge of different single-GPU PLMs on multiple\nEnglish data sets via traditional classifier probing and information-theoretic\nminimum description length probing. Our results show that PLMs do encode these\nsociodemographics, and that this knowledge is sometimes spread across the\nlayers of some of the tested PLMs. We further conduct a multilingual analysis\nand investigate the effect of supplementary training to further explore to what\nextent, where, and with what amount of pre-training data the knowledge is\nencoded. Our overall results indicate that sociodemographic knowledge is still\na major challenge for NLP. PLMs require large amounts of pre-training data to\nacquire the knowledge and models that excel in general language understanding\ndo not seem to own more knowledge about these aspects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel Bowman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BER: Balanced Error Rate For Speaker Diarization. (arXiv:2211.04304v1 [cs.SD])","link":"http://arxiv.org/abs/2211.04304","description":"<p>DER is the primary metric to evaluate diarization performance while facing a\ndilemma: the errors in short utterances or segments tend to be overwhelmed by\nlonger ones. Short segments, e.g., `yes' or `no,' still have semantic\ninformation. Besides, DER overlooks errors in less-talked speakers. Although\nJER balances speaker errors, it still suffers from the same dilemma.\nConsidering all those aspects, duration error, segment error, and\nspeaker-weighted error constituting a complete diarization evaluation, we\npropose a Balanced Error Rate (BER) to evaluate speaker diarization. First, we\npropose a segment-level error rate (SER) via connected sub-graphs and adaptive\nIoU threshold to get accurate segment matching. Second, to evaluate diarization\nin a unified way, we adopt a speaker-specific harmonic mean between duration\nand segment, followed by a speaker-weighted average. Third, we analyze our\nmetric via the modularized system, EEND, and the multi-modal method on real\ndatasets. SER and BER are publicly available at https://github.com/X-LANCE/BER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Kai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning. (arXiv:2211.04325v1 [cs.LG])","link":"http://arxiv.org/abs/2211.04325","description":"<p>We analyze the growth of dataset sizes used in machine learning for natural\nlanguage processing and computer vision, and extrapolate these using two\nmethods; using the historical growth rate and estimating the compute-optimal\ndataset size for future predicted compute budgets. We investigate the growth in\ndata usage by estimating the total stock of unlabeled data available on the\ninternet over the coming decades. Our analysis indicates that the stock of\nhigh-quality language data will be exhausted soon; likely before 2026. By\ncontrast, the stock of low-quality language data and image data will be\nexhausted only much later; between 2030 and 2050 (for low-quality language) and\nbetween 2030 and 2060 (for images). Our work suggests that the current trend of\never-growing ML models that rely on enormous datasets might slow down if data\nefficiency is not drastically improved or new sources of data become available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Villalobos_P/0/1/0/all/0/1\">Pablo Villalobos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sevilla_J/0/1/0/all/0/1\">Jaime Sevilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heim_L/0/1/0/all/0/1\">Lennart Heim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besiroglu_T/0/1/0/all/0/1\">Tamay Besiroglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hobbhahn_M/0/1/0/all/0/1\">Marius Hobbhahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_A/0/1/0/all/0/1\">Anson Ho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt-Based Metric Learning for Few-Shot NER. (arXiv:2211.04337v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04337","description":"<p>Few-shot named entity recognition (NER) targets generalizing to unseen labels\nand/or domains with few labeled examples. Existing metric learning methods\ncompute token-level similarities between query and support sets, but are not\nable to fully incorporate label semantics into modeling. To address this issue,\nwe propose a simple method to largely improve metric learning for NER: 1)\nmultiple prompt schemas are designed to enhance label semantics; 2) we propose\na novel architecture to effectively combine multiple prompt-based\nrepresentations. Empirically, our method achieves new state-of-the-art (SOTA)\nresults under 16 of the 18 considered settings, substantially outperforming the\nprevious SOTA by an average of 8.84% and a maximum of 34.51% in relative gains\nof micro F1. Our code is available at https://github.com/AChen-qaq/ProML.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanru Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yanan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhilin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NaturalAdversaries: Can Naturalistic Adversaries Be as Effective as Artificial Adversaries?. (arXiv:2211.04364v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04364","description":"<p>While a substantial body of prior work has explored adversarial example\ngeneration for natural language understanding tasks, these examples are often\nunrealistic and diverge from the real-world data distributions. In this work,\nwe introduce a two-stage adversarial example generation framework\n(NaturalAdversaries), for designing adversaries that are effective at fooling a\ngiven classifier and demonstrate natural-looking failure cases that could\nplausibly occur during in-the-wild deployment of the models.\n</p>\n<p>At the first stage a token attribution method is used to summarize a given\nclassifier's behaviour as a function of the key tokens in the input. In the\nsecond stage a generative model is conditioned on the key tokens from the first\nstage. NaturalAdversaries is adaptable to both black-box and white-box\nadversarial attacks based on the level of access to the model parameters. Our\nresults indicate these adversaries generalize across domains, and offer\ninsights for future research on improving robustness of neural text\nclassification models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gabriel_S/0/1/0/all/0/1\">Saadia Gabriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1\">Hamid Palangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multimodal Approach for Dementia Detection from Spontaneous Speech with Tensor Fusion Layer. (arXiv:2211.04368v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04368","description":"<p>Alzheimer's disease (AD) is a progressive neurological disorder, meaning that\nthe symptoms develop gradually throughout the years. It is also the main cause\nof dementia, which affects memory, thinking skills, and mental abilities.\nNowadays, researchers have moved their interest towards AD detection from\nspontaneous speech, since it constitutes a time-effective procedure. However,\nexisting state-of-the-art works proposing multimodal approaches do not take\ninto consideration the inter- and intra-modal interactions and propose early\nand late fusion approaches. To tackle these limitations, we propose deep neural\nnetworks, which can be trained in an end-to-end trainable way and capture the\ninter- and intra-modal interactions. Firstly, each audio file is converted to\nan image consisting of three channels, i.e., log-Mel spectrogram, delta, and\ndelta-delta. Next, each transcript is passed through a BERT model followed by a\ngated self-attention layer. Similarly, each image is passed through a Swin\nTransformer followed by an independent gated self-attention layer. Acoustic\nfeatures are extracted also from each audio file. Finally, the representation\nvectors from the different modalities are fed to a tensor fusion layer for\ncapturing the inter-modal interactions. Extensive experiments conducted on the\nADReSS Challenge dataset indicate that our introduced approaches obtain\nvaluable advantages over existing research initiatives reaching Accuracy and\nF1-score up to 86.25% and 85.48% respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ilias_L/0/1/0/all/0/1\">Loukas Ilias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askounis_D/0/1/0/all/0/1\">Dimitris Askounis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Psarras_J/0/1/0/all/0/1\">John Psarras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"nBIIG: A Neural BI Insights Generation System for Table Reporting. (arXiv:2211.04417v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04417","description":"<p>We present nBIIG, a neural Business Intelligence (BI) Insights Generation\nsystem. Given a table, our system applies various analyses to create\ncorresponding RDF representations, and then uses a neural model to generate\nfluent textual insights out of these representations. The generated insights\ncan be used by an analyst, via a human-in-the-loop paradigm, to enhance the\ntask of creating compelling table reports. The underlying generative neural\nmodel is trained over large and carefully distilled data, curated from multiple\nBI domains. Thus, the system can generate faithful and fluent insights over\nopen-domain tables, making it practical and useful.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perlitz_Y/0/1/0/all/0/1\">Yotam Perlitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheinwald_D/0/1/0/all/0/1\">Dafna Sheinwald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmueli_Scheuer_M/0/1/0/all/0/1\">Michal Shmueli-Scheuer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Order Matters when you Increase Masking. (arXiv:2211.04427v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04427","description":"<p>Word order, an essential property of natural languages, is injected in\nTransformer-based neural language models using position encoding. However,\nrecent experiments have shown that explicit position encoding is not always\nuseful, since some models without such feature managed to achieve state-of-the\nart performance on some tasks. To understand better this phenomenon, we examine\nthe effect of removing position encodings on the pre-training objective itself\n(i.e., masked language modelling), to test whether models can reconstruct\nposition information from co-occurrences alone. We do so by controlling the\namount of masked tokens in the input sentence, as a proxy to affect the\nimportance of position information for the task. We find that the necessity of\nposition information increases with the amount of masking, and that masked\nlanguage models without position encodings are not able to reconstruct this\ninformation on the task. These findings point towards a direct relationship\nbetween the amount of masking and the ability of Transformers to capture\norder-sensitive aspects of language using position encoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lasri_K/0/1/0/all/0/1\">Karim Lasri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenci_A/0/1/0/all/0/1\">Alessandro Lenci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poibeau_T/0/1/0/all/0/1\">Thierry Poibeau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Review of coreference resolution in English and Persian. (arXiv:2211.04428v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04428","description":"<p>Coreference resolution (CR) is one of the most challenging areas of natural\nlanguage processing. This task seeks to identify all textual references to the\nsame real-world entity. Research in this field is divided into coreference\nresolution and anaphora resolution. Due to its application in textual\ncomprehension and its utility in other tasks such as information extraction\nsystems, document summarization, and machine translation, this field has\nattracted considerable interest. Consequently, it has a significant effect on\nthe quality of these systems. This article reviews the existing corpora and\nevaluation metrics in this field. Then, an overview of the coreference\nalgorithms, from rule-based methods to the latest deep learning techniques, is\nprovided. Finally, coreference resolution and pronoun resolution systems in\nPersian are investigated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_H/0/1/0/all/0/1\">Hassan Haji Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talebpour_A/0/1/0/all/0/1\">Alireza Talebpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aznaveh_A/0/1/0/all/0/1\">Ahmad Mahmoudi Aznaveh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazdani_S/0/1/0/all/0/1\">Samaneh Yazdani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SLATE: A Sequence Labeling Approach for Task Extraction from Free-form Inked Content. (arXiv:2211.04454v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04454","description":"<p>We present SLATE, a sequence labeling approach for extracting tasks from\nfree-form content such as digitally handwritten (or \"inked\") notes on a virtual\nwhiteboard. Our approach allows us to create a single, low-latency model to\nsimultaneously perform sentence segmentation and classification of these\nsentences into task/non-task sentences. SLATE greatly outperforms a baseline\ntwo-model (sentence segmentation followed by classification model) approach,\nachieving a task F1 score of 84.4\\%, a sentence segmentation (boundary\nsimilarity) score of 88.4% and three times lower latency compared to the\nbaseline. Furthermore, we provide insights into tackling challenges of\nperforming NLP on the inking domain. We release both our code and dataset for\nthis novel task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_A/0/1/0/all/0/1\">Apurva Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serrao_R/0/1/0/all/0/1\">Ryan Serrao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1\">Biyi Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antonius_G/0/1/0/all/0/1\">Gilbert Antonius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jenna Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tra My Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Sheng Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nosakhare_E/0/1/0/all/0/1\">Ehi Nosakhare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaffer_I/0/1/0/all/0/1\">Irene Shaffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Soundararajan Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vivek Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperbolic Centroid Calculations for Text Classification. (arXiv:2211.04462v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04462","description":"<p>A new development in NLP is the construction of hyperbolic word embeddings.\nAs opposed to their Euclidean counterparts, hyperbolic embeddings are\nrepresented not by vectors, but by points in hyperbolic space. This makes the\nmost common basic scheme for constructing document representations, namely the\naveraging of word vectors, meaningless in the hyperbolic setting. We\nreinterpret the vector mean as the centroid of the points represented by the\nvectors, and investigate various hyperbolic centroid schemes and their\neffectiveness at text classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gerek_A/0/1/0/all/0/1\">Ayd&#x131;n Gerek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferahlar_C/0/1/0/all/0/1\">C&#xfc;neyt Ferahlar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sert_B/0/1/0/all/0/1\">Bilge &#x15e;ipal Sert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuney_M/0/1/0/all/0/1\">Mehmet Can Y&#xfc;ney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tasdemir_O/0/1/0/all/0/1\">Onur Ta&#x15f;demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalafat_Z/0/1/0/all/0/1\">Zeynep Billur Kalafat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelkit_M/0/1/0/all/0/1\">Mert Kelkit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganiz_M/0/1/0/all/0/1\">Murat Can Ganiz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UA-GEC: Grammatical Error Correction and Fluency Corpus for the Ukrainian Language. (arXiv:2103.16997v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.16997","description":"<p>We present a corpus professionally annotated for grammatical error correction\n(GEC) and fluency edits in the Ukrainian language. To the best of our\nknowledge, this is the first GEC corpus for the Ukrainian language. We\ncollected texts with errors (20,715 sentences) from a diverse pool of\ncontributors, including both native and non-native speakers. The data cover a\nwide variety of writing domains, from text chats and essays to formal writing.\nProfessional proofreaders corrected and annotated the corpus for errors\nrelating to fluency, grammar, punctuation, and spelling. This corpus can be\nused for developing and evaluating GEC systems in Ukrainian. More generally, it\ncan be used for researching multilingual and low-resource NLP, morphologically\nrich languages, document-level GEC, and fluency correction. The corpus is\npublicly available at https://github.com/grammarly/ua-gec\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Syvokon_O/0/1/0/all/0/1\">Oleksiy Syvokon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nahorna_O/0/1/0/all/0/1\">Olena Nahorna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scientia Potentia Est -- On the Role of Knowledge in Computational Argumentation. (arXiv:2107.00281v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.00281","description":"<p>Despite extensive research efforts in recent years, computational\nargumentation (CA) remains one of the most challenging areas of natural\nlanguage processing. The reason for this is the inherent complexity of the\ncognitive processes behind human argumentation, which integrate a plethora of\ndifferent types of knowledge, ranging from topic-specific facts and common\nsense to rhetorical knowledge. The integration of knowledge from such a wide\nrange in CA requires modeling capabilities far beyond many other natural\nlanguage understanding tasks. Existing research on mining, assessing, reasoning\nover, and generating arguments largely acknowledges that much more knowledge is\nneeded to accurately model argumentation computationally. However, a systematic\noverview of the types of knowledge introduced in existing CA models is missing,\nhindering targeted progress in the field. Adopting the operational definition\nof knowledge as any task-relevant normative information not provided as input,\nthe survey paper at hand fills this gap by (1) proposing a taxonomy of types of\nknowledge required in CA tasks, (2) systematizing the large body of CA work\naccording to the reliance on and exploitation of these knowledge types for the\nfour main research areas in CA, and (3) outlining and discussing directions for\nfuture research efforts in CA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachsmuth_H/0/1/0/all/0/1\">Henning Wachsmuth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Optimal is Greedy Decoding for Extractive Question Answering?. (arXiv:2108.05857v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.05857","description":"<p>Fine-tuned language models use greedy decoding to answer reading\ncomprehension questions with relative success. However, this approach does not\nensure that the answer is a span in the given passage, nor does it guarantee\nthat it is the most probable one. Does greedy decoding actually perform worse\nthan an algorithm that does adhere to these properties? To study the\nperformance and optimality of greedy decoding, we present exact-extract, a\ndecoding algorithm that efficiently finds the most probable answer span in the\ncontext. We compare the performance of T5 with both decoding algorithms on\nzero-shot and few-shot extractive question answering. When no training examples\nare available, exact-extract significantly outperforms greedy decoding.\nHowever, greedy decoding quickly converges towards the performance of\nexact-extract with the introduction of a few training examples, becoming more\nextractive and increasingly likelier to generate the most probable span as the\ntraining set grows. We also show that self-supervised training can bias the\nmodel towards extractive behavior, increasing performance in the zero-shot\nsetting without resorting to annotated examples. Overall, our results suggest\nthat pretrained language models are so good at adapting to extractive question\nanswering, that it is often enough to fine-tune on a small training set for the\ngreedy algorithm to emulate the optimal decoding strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Castel_O/0/1/0/all/0/1\">Or Castel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ram_O/0/1/0/all/0/1\">Ori Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efrat_A/0/1/0/all/0/1\">Avia Efrat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complex Knowledge Base Question Answering: A Survey. (arXiv:2108.06688v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.06688","description":"<p>Knowledge base question answering (KBQA) aims to answer a question over a\nknowledge base (KB). Early studies mainly focused on answering simple questions\nover KBs and achieved great success. However, their performance on complex\nquestions is still far from satisfactory. Therefore, in recent years,\nresearchers propose a large number of novel methods, which looked into the\nchallenges of answering complex questions. In this survey, we review recent\nadvances on KBQA with the focus on solving complex questions, which usually\ncontain multiple subjects, express compound relations, or involve numerical\noperations. In detail, we begin with introducing the complex KBQA task and\nrelevant background. Then, we describe benchmark datasets for complex KBQA task\nand introduce the construction process of these datasets. Next, we present two\nmainstream categories of methods for complex KBQA, namely semantic\nparsing-based (SP-based) methods and information retrieval-based (IR-based)\nmethods. Specifically, we illustrate their procedures with flow designs and\ndiscuss their major differences and similarities. After that, we summarize the\nchallenges that these two categories of methods encounter when answering\ncomplex questions, and explicate advanced solutions and techniques used in\nexisting work. Finally, we conclude and discuss several promising directions\nrelated to complex KBQA for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yunshi Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_G/0/1/0/all/0/1\">Gaole He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jinhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LexGLUE: A Benchmark Dataset for Legal Language Understanding in English. (arXiv:2110.00976v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.00976","description":"<p>Laws and their interpretations, legal arguments and agreements\\ are typically\nexpressed in writing, leading to the production of vast corpora of legal text.\nTheir analysis, which is at the center of legal practice, becomes increasingly\nelaborate as these collections grow in size. Natural language understanding\n(NLU) technologies can be a valuable tool to support legal practitioners in\nthese endeavors. Their usefulness, however, largely depends on whether current\nstate-of-the-art models can generalize across various tasks in the legal\ndomain. To answer this currently open question, we introduce the Legal General\nLanguage Understanding Evaluation (LexGLUE) benchmark, a collection of datasets\nfor evaluating model performance across a diverse set of legal NLU tasks in a\nstandardized way. We also provide an evaluation and analysis of several generic\nand legal-oriented models demonstrating that the latter consistently offer\nperformance improvements across multiple tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jana_A/0/1/0/all/0/1\">Abhik Jana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartung_D/0/1/0/all/0/1\">Dirk Hartung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bommarito_M/0/1/0/all/0/1\">Michael Bommarito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1\">Ion Androutsopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_D/0/1/0/all/0/1\">Daniel Martin Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear algebra with transformers. (arXiv:2112.01898v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.01898","description":"<p>Transformers can learn to perform numerical computations from examples only.\nI study nine problems of linear algebra, from basic matrix operations to\neigenvalue decomposition and inversion, and introduce and discuss four encoding\nschemes to represent real numbers. On all problems, transformers trained on\nsets of random matrices achieve high accuracies (over 90%). The models are\nrobust to noise, and can generalize out of their training distribution. In\nparticular, models trained to predict Laplace-distributed eigenvalues\ngeneralize to different classes of matrices: Wigner matrices or matrices with\npositive eigenvalues. The reverse is not true.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Charton_F/0/1/0/all/0/1\">Fran&#xe7;ois Charton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate detection of sepsis at ED triage using machine learning with clinical natural language processing. (arXiv:2204.07657v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.07657","description":"<p>Sepsis is a life-threatening condition with organ dysfunction and is a\nleading cause of death and critical illness worldwide. Accurate detection of\nsepsis during emergency department triage would allow early initiation of lab\nanalysis, antibiotic administration, and other sepsis treatment protocols. The\npurpose of this study was to determine whether EHR data can be extracted and\nsynthesized with the latest machine learning algorithms (KATE Sepsis) and\nclinical natural language processing to produce accurate sepsis models, and\ncompare KATE Sepsis performance with existing sepsis screening protocols, such\nas SIRS and qSOFA. A machine learning model (KATE Sepsis) was developed using\npatient encounters with triage data from 16 participating hospitals. KATE\nSepsis, SIRS, standard screening (SIRS with source of infection) and qSOFA were\ntested in three settings. Cohort-A was a retrospective analysis on medical\nrecords from a single Site 1. Cohort-B was a prospective analysis of Site 1.\nCohort-C was a retrospective analysis on Site 1 with 15 additional sites.\nAcross all cohorts, KATE Sepsis demonstrates an AUC of 0.94-0.963 with\n73-74.87% TPR and 3.76-7.17% FPR. Standard screening demonstrates an AUC of\n0.682-0.726 with 39.39-51.19% TPR and 2.9-6.02% FPR. The qSOFA protocol\ndemonstrates an AUC of 0.544-0.56, with 10.52-13.18% TPR and 1.22-1.68% FPR.\nFor severe sepsis, across all cohorts, KATE Sepsis demonstrates an AUC of\n0.935-0.972 with 70-82.26% TPR and 4.64-8.62% FPR. For septic shock, across all\ncohorts, KATE Sepsis demonstrates an AUC of 0.96-0.981 with 85.71-89.66% TPR\nand 4.85-8.8% FPR. SIRS, standard screening, and qSOFA demonstrate low AUC and\nTPR for severe sepsis and septic shock detection. KATE Sepsis provided\nsubstantially better sepsis detection performance in triage than commonly used\nscreening protocols.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivanov_O/0/1/0/all/0/1\">Oleksandr Ivanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molander_K/0/1/0/all/0/1\">Karin Molander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunne_R/0/1/0/all/0/1\">Robert Dunne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Stephen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masek_K/0/1/0/all/0/1\">Kevin Masek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_E/0/1/0/all/0/1\">Erica Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brecher_D/0/1/0/all/0/1\">Deena Brecher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lisa Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Travers_D/0/1/0/all/0/1\">Debbie Travers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delaney_D/0/1/0/all/0/1\">Deb Delaney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montgomery_K/0/1/0/all/0/1\">Kyla Montgomery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reilly_C/0/1/0/all/0/1\">Christian Reilly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Control Globally, Understand Locally: A Global-to-Local Hierarchical Graph Network for Emotional Support Conversation. (arXiv:2204.12749v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.12749","description":"<p>Emotional support conversation aims at reducing the emotional distress of the\nhelp-seeker, which is a new and challenging task. It requires the system to\nexplore the cause of help-seeker's emotional distress and understand their\npsychological intention to provide supportive responses. However, existing\nmethods mainly focus on the sequential contextual information, ignoring the\nhierarchical relationships with the global cause and local psychological\nintention behind conversations, thus leads to a weak ability of emotional\nsupport. In this paper, we propose a Global-to-Local Hierarchical Graph Network\nto capture the multi-source information (global cause, local intentions and\ndialog history) and model hierarchical relationships between them, which\nconsists of a multi-source encoder, a hierarchical graph reasoner, and a\nglobal-guide decoder. Furthermore, a novel training objective is designed to\nmonitor semantic information of the global cause. Experimental results on the\nemotional support conversation dataset, ESConv, confirm that the proposed GLHG\nhas achieved the state-of-the-art performance on the automatic and human\nevaluations. The code will be released in here\n\\footnote{\\small{~https://github.com/pengwei-iie/GLHG}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Luxi Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuqiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yajing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunpeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Global and Local Hierarchies for Hierarchical Text Classification. (arXiv:2205.02613v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.02613","description":"<p>Hierarchical text classification aims to leverage label hierarchy in\nmulti-label text classification. Existing methods encode label hierarchy in a\nglobal view, where label hierarchy is treated as the static hierarchical\nstructure containing all labels. Since global hierarchy is static and\nirrelevant to text samples, it makes these methods hard to exploit hierarchical\ninformation. Contrary to global hierarchy, local hierarchy as a structured\nlabels hierarchy corresponding to each text sample. It is dynamic and relevant\nto text samples, which is ignored in previous methods. To exploit global and\nlocal hierarchies,we propose Hierarchy-guided BERT with Global and Local\nhierarchies (HBGL), which utilizes the large-scale parameters and prior\nlanguage knowledge of BERT to model both global and local\nhierarchies.Moreover,HBGL avoids the intentional fusion of semantic and\nhierarchical modules by directly modeling semantic and hierarchical information\nwith BERT.Compared with the state-of-the-art method HGCLR,our method achieves\nsignificant improvement on three benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Ting Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Deqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Leilei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhongzhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qinghong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Curious Case of Control. (arXiv:2205.12113v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12113","description":"<p>Children acquiring English make systematic errors on subject control\nsentences even after they have reached near-adult competence (C. Chomsky,\n1969), possibly due to heuristics based on semantic roles (Maratsos, 1974).\nGiven the advanced fluency of large generative language models, we ask whether\nmodel outputs are consistent with these heuristics, and to what degree\ndifferent models are consistent with each other. We find that models can be\ncategorized by behavior into three separate groups, with broad differences\nbetween the groups. The outputs of models in the largest group are consistent\nwith positional heuristics that succeed on subject control but fail on object\ncontrol. This result is surprising, given that object control is orders of\nmagnitude more frequent in the text data used to train such models. We examine\nto what degree the models are sensitive to prompting with agent-patient\ninformation, finding that raising the salience of agent and patient relations\nresults in significant changes in the outputs of most models. Based on this\nobservation, we leverage an existing dataset of semantic proto-role annotations\n(White, et al. 2020) to explore the connections between control and labeling\nevent participants with properties typically associated with agents and\npatients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stengel_Eskin_E/0/1/0/all/0/1\">Elias Stengel-Eskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When More Data Hurts: A Troubling Quirk in Developing Broad-Coverage Natural Language Understanding Systems. (arXiv:2205.12228v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12228","description":"<p>In natural language understanding (NLU) production systems, users' evolving\nneeds necessitate the addition of new features over time, indexed by new\nsymbols added to the meaning representation space. This requires additional\ntraining data and results in ever-growing datasets. We present the first\nsystematic investigation of this incremental symbol learning scenario. Our\nanalysis reveals a troubling quirk in building broad-coverage NLU systems: as\nthe training dataset grows, performance on the new symbol often decreases if we\ndo not accordingly increase its training data. This suggests that it becomes\nmore difficult to learn new symbols with a larger training dataset. We show\nthat this trend holds for multiple mainstream models on two common NLU tasks:\nintent recognition and semantic parsing. Rejecting class imbalance as the sole\nculprit, we reveal that the trend is closely associated with an effect we call\nsource signal dilution, where strong lexical cues for the new symbol become\ndiluted as the training dataset grows. Selectively dropping training examples\nto prevent dilution often reverses the trend, showing the over-reliance of\nmainstream neural NLU models on simple lexical cues. Code, models, and data are\navailable at https://aka.ms/nlu-incremental-symbol-learning\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stengel_Eskin_E/0/1/0/all/0/1\">Elias Stengel-Eskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Platanios_E/0/1/0/all/0/1\">Emmanouil Antonios Platanios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauls_A/0/1/0/all/0/1\">Adam Pauls</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomson_S/0/1/0/all/0/1\">Sam Thomson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Hao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisner_J/0/1/0/all/0/1\">Jason Eisner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gradient-Based Constrained Sampling from Language Models. (arXiv:2205.12558v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12558","description":"<p>Large pretrained language models generate fluent text but are notoriously\nhard to controllably sample from. In this work, we study constrained sampling\nfrom such language models: generating text that satisfies user-defined\nconstraints, while maintaining fluency and the model's performance in a\ndownstream task. We propose MuCoLa -- a sampling procedure that combines the\nlog-likelihood of the language model with arbitrary (differentiable)\nconstraints in a single energy function, and then generates samples in a\nnon-autoregressive manner. Specifically, it initializes the entire output\nsequence with noise and follows a Markov chain defined by Langevin Dynamics\nusing the gradients of the energy function. We evaluate MuCoLa on text\ngeneration with soft and hard constraints as well as their combinations\nobtaining significant improvements over competitive baselines for toxicity\navoidance, sentiment control, and keyword-guided generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sachin Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paria_B/0/1/0/all/0/1\">Biswajit Paria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RobustLR: Evaluating Robustness to Logical Perturbation in Deductive Reasoning. (arXiv:2205.12598v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12598","description":"<p>Transformers have been shown to be able to perform deductive reasoning on a\nlogical rulebase containing rules and statements written in English natural\nlanguage. While the progress is promising, it is currently unclear if these\nmodels indeed perform logical reasoning by understanding the underlying logical\nsemantics in the language. To this end, we propose RobustLR, a suite of\nevaluation datasets that evaluate the robustness of these models to minimal\nlogical edits in rulebases and some standard logical equivalence conditions. In\nour experiments with RoBERTa and T5, we find that the models trained in prior\nworks do not perform consistently on the different perturbations in RobustLR,\nthus showing that the models are not robust to the proposed logical\nperturbations. Further, we find that the models find it especially hard to\nlearn logical negation and disjunction operators. Overall, using our evaluation\nsets, we demonstrate some shortcomings of the deductive reasoning-based\nlanguage models, which can eventually help towards designing better models for\nlogical reasoning over natural language. All the datasets and code base have\nbeen made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1\">Soumya Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1\">Zeyi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Large-scale Paraphrase Acquisition and Generation. (arXiv:2210.03235v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03235","description":"<p>This paper addresses the quality issues in existing Twitter-based paraphrase\ndatasets, and discusses the necessity of using two separate definitions of\nparaphrase for identification and generation tasks. We present a new\nMulti-Topic Paraphrase in Twitter (MultiPIT) corpus that consists of a total of\n130k sentence pairs with crowdsoursing (MultiPIT_crowd) and expert\n(MultiPIT_expert) annotations using two different paraphrase definitions for\nparaphrase identification, in addition to a multi-reference test set\n(MultiPIT_NMR) and a large automatically constructed training set\n(MultiPIT_Auto) for paraphrase generation. With improved data annotation\nquality and task-specific paraphrase definition, the best pre-trained language\nmodel fine-tuned on our dataset achieves the state-of-the-art performance of\n84.2 F1 for automatic paraphrase identification. Furthermore, our empirical\nresults also demonstrate that the paraphrase generation models trained on\nMultiPIT_Auto generate more diverse and high-quality paraphrases compared to\ntheir counterparts fine-tuned on other corpora such as Quora, MSCOCO, and\nParaNMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">Yao Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAPS: A Novel Few-Shot Relation Extraction Pipeline with Query-Information Guided Attention and Adaptive Prototype Fusion. (arXiv:2210.08242v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08242","description":"<p>Few-shot relation extraction (FSRE) aims at recognizing unseen relations by\nlearning with merely a handful of annotated instances. To generalize to new\nrelations more effectively, this paper proposes a novel pipeline for the FSRE\ntask based on queRy-information guided Attention and adaptive Prototype fuSion,\nnamely RAPS. Specifically, RAPS first derives the relation prototype by the\nquery-information guided attention module, which exploits rich interactive\ninformation between the support instances and the query instances, in order to\nobtain more accurate initial prototype representations. Then RAPS elaborately\ncombines the derived initial prototype with the relation information by the\nadaptive prototype fusion mechanism to get the integrated prototype for both\ntrain and prediction. Experiments on the benchmark dataset FewRel 1.0 show a\nsignificant improvement of our method against state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuzhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cen_M/0/1/0/all/0/1\">Min Cen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongzhou Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Language Model to Train if You Have One Million GPU Hours?. (arXiv:2210.15424v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.15424","description":"<p>The crystallization of modeling methods around the Transformer architecture\nhas been a boon for practitioners. Simple, well-motivated architectural\nvariations can transfer across tasks and scale, increasing the impact of\nmodeling research. However, with the emergence of state-of-the-art 100B+\nparameters models, large language models are increasingly expensive to\naccurately design and train. Notably, it can be difficult to evaluate how\nmodeling decisions may impact emergent capabilities, given that these\ncapabilities arise mainly from sheer scale alone. In the process of building\nBLOOM--the Big Science Large Open-science Open-access Multilingual language\nmodel--our goal is to identify an architecture and training setup that makes\nthe best use of our 1,000,000 A100-GPU-hours budget. Specifically, we perform\nan ablation study at the billion-parameter scale comparing different modeling\npractices and their impact on zero-shot generalization. In addition, we study\nthe impact of various popular pre-training corpora on zero-shot generalization.\nWe also study the performance of a multilingual model and how it compares to\nthe English-only one. Finally, we consider the scaling behaviour of\nTransformers to choose the target model size, shape, and training setup. All\nour models and code are open-sourced at https://huggingface.co/bigscience .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scao_T/0/1/0/all/0/1\">Teven Le Scao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Thomas Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hesslow_D/0/1/0/all/0/1\">Daniel Hesslow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saulnier_L/0/1/0/all/0/1\">Lucile Saulnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekman_S/0/1/0/all/0/1\">Stas Bekman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1\">M Saiful Bari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsahar_H/0/1/0/all/0/1\">Hady Elsahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1\">Niklas Muennighoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phang_J/0/1/0/all/0/1\">Jason Phang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Press_O/0/1/0/all/0/1\">Ofir Press</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanh_V/0/1/0/all/0/1\">Victor Sanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutawika_L/0/1/0/all/0/1\">Lintang Sutawika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tae_J/0/1/0/all/0/1\">Jaesung Tae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yong_Z/0/1/0/all/0/1\">Zheng Xin Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Launay_J/0/1/0/all/0/1\">Julien Launay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent Linguistic Structures in Neural Networks are Fragile. (arXiv:2210.17406v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.17406","description":"<p>Large language models (LLMs) have been reported to have strong performance on\nnatural language processing tasks. However, performance metrics such as\naccuracy do not measure the quality of the model in terms of its ability to\nrobustly represent complex linguistic structure. In this work, we propose a\nframework to evaluate the robustness of linguistic representations using\nprobing tasks. We leverage recent advances in extracting emergent linguistic\nconstructs from LLMs and apply syntax-preserving perturbations to test the\nstability of these constructs in order to better understand the representations\nlearned by LLMs. Empirically, we study the performance of four LLMs across six\ndifferent corpora on the proposed robustness measures. We provide evidence that\ncontext-free representation (e.g., GloVe) are in some cases competitive with\ncontext-dependent representations from modern LLMs (e.g., BERT), yet equally\nbrittle to syntax-preserving manipulations. Emergent syntactic representations\nin neural networks are brittle, thus our work poses the attention on the risk\nof comparing such structures to those that are object of a long lasting debate\nin linguistics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malfa_E/0/1/0/all/0/1\">Emanuele La Malfa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wicker_M/0/1/0/all/0/1\">Matthew Wicker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiatkowska_M/0/1/0/all/0/1\">Marta Kiatkowska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chronic pain patient narratives allow for the estimation of current pain intensity. (arXiv:2210.17473v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.17473","description":"<p>Chronic pain is a multi-dimensional experience, and pain intensity plays an\nimportant part, impacting the patients emotional balance, psychology, and\nbehaviour. Standard self-reporting tools, such as the Visual Analogue Scale for\npain, fail to capture this burden. Moreover, this type of tools is susceptible\nto a degree of subjectivity, dependent on the patients clear understanding of\nhow to use it, social biases, and their ability to translate a complex\nexperience to a scale. To overcome these and other self-reporting challenges,\npain intensity estimation has been previously studied based on facial\nexpressions, electroencephalograms, brain imaging, and autonomic features.\nHowever, to the best of our knowledge, it has never been attempted to base this\nestimation on the patient narratives of the personal experience of chronic\npain, which is what we propose in this work. Indeed, in the clinical assessment\nand management of chronic pain, verbal communication is essential to convey\ninformation to physicians that would otherwise not be easily accessible through\nstandard reporting tools, since language, sociocultural, and psychosocial\nvariables are intertwined. We show that language features from patient\nnarratives indeed convey information relevant for pain intensity estimation,\nand that our computational models can take advantage of that. Specifically, our\nresults show that patients with mild pain focus more on the use of verbs,\nwhilst moderate and severe pain patients focus on adverbs, and nouns and\nadjectives, respectively, and that these differences allow for the distinction\nbetween these three pain classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nunes_D/0/1/0/all/0/1\">Diogo A.P. Nunes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_Gomes_J/0/1/0/all/0/1\">Joana Ferreira-Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaz_C/0/1/0/all/0/1\">Carlos Vaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1\">Daniela Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimenta_S/0/1/0/all/0/1\">Sofia Pimenta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neto_F/0/1/0/all/0/1\">Fani Neto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matos_D/0/1/0/all/0/1\">David Martins de Matos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eliciting Knowledge from Large Pre-Trained Models for Unsupervised Knowledge-Grounded Conversation. (arXiv:2211.01587v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.01587","description":"<p>Recent advances in large-scale pre-training provide large models with the\npotential to learn knowledge from the raw text. It is thus natural to ask\nwhether it is possible to leverage these large models as knowledge bases for\ndownstream tasks. In this work, we answer the aforementioned question in\nunsupervised knowledge-grounded conversation. We explore various methods that\nbest elicit knowledge from large models. Our human study indicates that, though\nhallucinations exist, large models post the unique advantage of being able to\noutput common sense and summarize facts that cannot be directly retrieved from\nthe search engine. To better exploit such generated knowledge in dialogue\ngeneration, we treat the generated knowledge as a noisy knowledge source and\npropose the posterior-based reweighing as well as the noisy training strategy.\nEmpirical results on two benchmarks show advantages over the state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jianqiao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael R. Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-11-08T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}