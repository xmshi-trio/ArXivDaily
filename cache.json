{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-04-20T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Token Imbalance Adaptation for Radiology Report Generation. (arXiv:2304.09185v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09185","description":"<p>Imbalanced token distributions naturally exist in text documents, leading\nneural language models to overfit on frequent tokens. The token imbalance may\ndampen the robustness of radiology report generators, as complex medical terms\nappear less frequently but reflect more medical information. In this study, we\ndemonstrate how current state-of-the-art models fail to generate infrequent\ntokens on two standard benchmark datasets (IU X-RAY and MIMIC-CXR) of radiology\nreport generation. % However, no prior study has proposed methods to adapt\ninfrequent tokens for text generators feeding with medical images. To solve the\nchallenge, we propose the \\textbf{T}oken \\textbf{Im}balance Adapt\\textbf{er}\n(\\textit{TIMER}), aiming to improve generation robustness on infrequent tokens.\nThe model automatically leverages token imbalance by an unlikelihood loss and\ndynamically optimizes generation processes to augment infrequent tokens. We\ncompare our approach with multiple state-of-the-art methods on the two\nbenchmarks. Experiments demonstrate the effectiveness of our approach in\nenhancing model robustness overall and infrequent tokens. Our ablation analysis\nshows that our reinforcement learning method has a major effect in adapting\ntoken imbalance for radiology report generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuexin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_I/0/1/0/all/0/1\">I-Chan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolei Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Neural Lambda Calculus: Neurosymbolic AI meets the foundations of computing and functional programming. (arXiv:2304.09276v1 [cs.LG])","link":"http://arxiv.org/abs/2304.09276","description":"<p>Over the last decades, deep neural networks based-models became the dominant\nparadigm in machine learning. Further, the use of artificial neural networks in\nsymbolic learning has been seen as increasingly relevant recently. To study the\ncapabilities of neural networks in the symbolic AI domain, researchers have\nexplored the ability of deep neural networks to learn mathematical\nconstructions, such as addition and multiplication, logic inference, such as\ntheorem provers, and even the execution of computer programs. The latter is\nknown to be too complex a task for neural networks. Therefore, the results were\nnot always successful, and often required the introduction of biased elements\nin the learning process, in addition to restricting the scope of possible\nprograms to be executed. In this work, we will analyze the ability of neural\nnetworks to learn how to execute programs as a whole. To do so, we propose a\ndifferent approach. Instead of using an imperative programming language, with\ncomplex structures, we use the Lambda Calculus ({\\lambda}-Calculus), a simple,\nbut Turing-Complete mathematical formalism, which serves as the basis for\nmodern functional programming languages and is at the heart of computability\ntheory. We will introduce the use of integrated neural learning and lambda\ncalculi formalization. Finally, we explore execution of a program in\n{\\lambda}-Calculus is based on reductions, we will show that it is enough to\nlearn how to perform these reductions so that we can execute any program.\nKeywords: Machine Learning, Lambda Calculus, Neurosymbolic AI, Neural Networks,\nTransformer Model, Sequence-to-Sequence Models, Computational Models\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Flach_J/0/1/0/all/0/1\">Jo&#xe3;o Flach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamb_L/0/1/0/all/0/1\">Luis C. Lamb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BIM-GPT: a Prompt-Based Virtual Assistant Framework for BIM Information Retrieval. (arXiv:2304.09333v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09333","description":"<p>Efficient information retrieval (IR) from building information models (BIMs)\nposes significant challenges due to the necessity for deep BIM knowledge or\nextensive engineering efforts for automation. We introduce BIM-GPT, a\nprompt-based virtual assistant (VA) framework integrating BIM and generative\npre-trained transformer (GPT) technologies to support NL-based IR. A prompt\nmanager and dynamic template generate prompts for GPT models, enabling\ninterpretation of NL queries, summarization of retrieved information, and\nanswering BIM-related questions. In tests on a BIM IR dataset, our approach\nachieved 83.5% and 99.5% accuracy rates for classifying NL queries with no data\nand 2% data incorporated in prompts, respectively. Additionally, we validated\nthe functionality of BIM-GPT through a VA prototype for a hospital building.\nThis research contributes to the development of effective and versatile VAs for\nBIM IR in the construction industry, significantly enhancing BIM accessibility\nand reducing engineering efforts and training data requirements for processing\nNL queries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Junwen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1\">Martin Fischer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Unintended Consequences of Censoring Digital Technology -- Evidence from Italy's ChatGPT Ban. (arXiv:2304.09339v1 [econ.GN])","link":"http://arxiv.org/abs/2304.09339","description":"<p>We analyse the effects of the ban of ChatGPT, a generative pre-trained\ntransformer chatbot, on individual productivity. We first compile data on the\nhourly coding output of over 8,000 professional GitHub users in Italy and other\nEuropean countries to analyse the impact of the ban on individual productivity.\nCombining the high-frequency data with the sudden announcement of the ban in a\ndifference-in-differences framework, we find that the output of Italian\ndevelopers decreased by around 50% in the first two business days after the ban\nand recovered after that. Applying a synthetic control approach to daily Google\nsearch and Tor usage data shows that the ban led to a significant increase in\nthe use of censorship bypassing tools. Our findings show that users swiftly\nimplement strategies to bypass Internet restrictions but this adaptation\nactivity creates short-term disruptions and hampers productivity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/econ/1/au:+Kreitmeir_D/0/1/0/all/0/1\">David H. Kreitmeir</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Raschky_P/0/1/0/all/0/1\">Paul A. Raschky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM as A Robotic Brain: Unifying Egocentric Memory and Control. (arXiv:2304.09349v1 [cs.AI])","link":"http://arxiv.org/abs/2304.09349","description":"<p>Embodied AI focuses on the study and development of intelligent systems that\npossess a physical or virtual embodiment (i.e. robots) and are able to\ndynamically interact with their environment. Memory and control are the two\nessential parts of an embodied system and usually require separate frameworks\nto model each of them. In this paper, we propose a novel and generalizable\nframework called LLM-Brain: using Large-scale Language Model as a robotic brain\nto unify egocentric memory and control. The LLM-Brain framework integrates\nmultiple multimodal language models for robotic tasks, utilizing a zero-shot\nlearning approach. All components within LLM-Brain communicate using natural\nlanguage in closed-loop multi-round dialogues that encompass perception,\nplanning, control, and memory. The core of the system is an embodied LLM to\nmaintain egocentric memory and control the robot. We demonstrate LLM-Brain by\nexamining two downstream tasks: active exploration and embodied question\nanswering. The active exploration tasks require the robot to extensively\nexplore an unknown environment within a limited number of actions. Meanwhile,\nthe embodied question answering tasks necessitate that the robot answers\nquestions based on observations acquired during prior explorations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mai_J/0/1/0/all/0/1\">Jinjie Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_G/0/1/0/all/0/1\">Guocheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shuffle & Divide: Contrastive Learning for Long Text. (arXiv:2304.09374v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09374","description":"<p>We propose a self-supervised learning method for long text documents based on\ncontrastive learning. A key to our method is Shuffle and Divide (SaD), a simple\ntext augmentation algorithm that sets up a pretext task required for\ncontrastive updates to BERT-based document embedding. SaD splits a document\ninto two sub-documents containing randomly shuffled words in the entire\ndocuments. The sub-documents are considered positive examples, leaving all\nother documents in the corpus as negatives. After SaD, we repeat the\ncontrastive update and clustering phases until convergence. It is naturally a\ntime-consuming, cumbersome task to label text documents, and our method can\nhelp alleviate human efforts, which are most expensive resources in AI. We have\nempirically evaluated our method by performing unsupervised text classification\non the 20 Newsgroups, Reuters-21578, BBC, and BBCSport datasets. In particular,\nour method pushes the current state-of-the-art, SS-SB-MT, on 20 Newsgroups by\n20.94% in accuracy. We also achieve the state-of-the-art performance on\nReuters-21578 and exceptionally-high accuracy performances (over 95%) for\nunsupervised classification on the BBC and BBCSport datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joonseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joe_S/0/1/0/all/0/1\">Seongho Joe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kyoungwon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Bogun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hoyoung Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jaeseon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwon_Y/0/1/0/all/0/1\">Youngjune Gwon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of Leveraging Knowledge Distillation for Compressing Multilingual Neural Machine Translation Models. (arXiv:2304.09388v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09388","description":"<p>Knowledge distillation (KD) is a well-known method for compressing neural\nmodels. However, works focusing on distilling knowledge from large multilingual\nneural machine translation (MNMT) models into smaller ones are practically\nnonexistent, despite the popularity and superiority of MNMT. This paper bridges\nthis gap by presenting an empirical investigation of knowledge distillation for\ncompressing MNMT models. We take Indic to English translation as a case study\nand demonstrate that commonly used language-agnostic and language-aware KD\napproaches yield models that are 4-5x smaller but also suffer from performance\ndrops of up to 3.5 BLEU. To mitigate this, we then experiment with design\nconsiderations such as shallower versus deeper models, heavy parameter sharing,\nmulti-stage training, and adapters. We observe that deeper compact models tend\nto be as good as shallower non-compact ones, and that fine-tuning a distilled\nmodel on a High-Quality subset slightly boosts translation quality. Overall, we\nconclude that compressing MNMT models via KD is challenging, indicating immense\nscope for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gumma_V/0/1/0/all/0/1\">Varun Gumma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1\">Raj Dabre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MixPro: Simple yet Effective Data Augmentation for Prompt-based Learning. (arXiv:2304.09402v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09402","description":"<p>Prompt-based learning reformulates downstream tasks as cloze problems by\ncombining the original input with a template. This technique is particularly\nuseful in few-shot learning, where a model is trained on a limited amount of\ndata. However, the limited templates and text used in few-shot prompt-based\nlearning still leave significant room for performance improvement.\nAdditionally, existing methods using model ensembles can constrain the model\nefficiency. To address these issues, we propose an augmentation method called\nMixPro, which augments both the vanilla input text and the templates through\ntoken-level, sentence-level, and epoch-level Mixup strategies. We conduct\nexperiments on five few-shot datasets, and the results show that MixPro\noutperforms other augmentation baselines, improving model performance by an\naverage of 5.08% compared to before augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_L/0/1/0/all/0/1\">Longxu Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yutai Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yunlong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_H/0/1/0/all/0/1\">Honglin Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Do Things with Deep Learning Code. (arXiv:2304.09406v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09406","description":"<p>The premise of this article is that a basic understanding of the composition\nand functioning of large language models is critically urgent. To that end, we\nextract a representational map of OpenAI's GPT-2 with what we articulate as two\nclasses of deep learning code, that which pertains to the model and that which\nunderwrites applications built around the model. We then verify this map\nthrough case studies of two popular GPT-2 applications: the text adventure\ngame, AI Dungeon, and the language art project, This Word Does Not Exist. Such\nan exercise allows us to test the potential of Critical Code Studies when the\nobject of study is deep learning code and to demonstrate the validity of code\nas an analytical focus for researchers in the subfields of Critical Artificial\nIntelligence and Critical Machine Learning Studies. More broadly, however, our\nwork draws attention to the means by which ordinary users might interact with,\nand even direct, the behavior of deep learning systems, and by extension works\ntoward demystifying some of the auratic mystery of \"AI.\" What is at stake is\nthe possibility of achieving an informed sociotechnical consensus about the\nresponsible applications of large language models, as well as a more expansive\nsense of their creative capabilities-indeed, understanding how and where\nengagement occurs allows all of us to become more active participants in the\ndevelopment of machine learning systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_M/0/1/0/all/0/1\">Minh Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raley_R/0/1/0/all/0/1\">Rita Raley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TieFake: Title-Text Similarity and Emotion-Aware Fake News Detection. (arXiv:2304.09421v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09421","description":"<p>Fake news detection aims to detect fake news widely spreading on social media\nplatforms, which can negatively influence the public and the government. Many\napproaches have been developed to exploit relevant information from news\nimages, text, or videos. However, these methods may suffer from the following\nlimitations: (1) ignore the inherent emotional information of the news, which\ncould be beneficial since it contains the subjective intentions of the authors;\n(2) pay little attention to the relation (similarity) between the title and\ntextual information in news articles, which often use irrelevant title to\nattract reader' attention. To this end, we propose a novel Title-Text\nsimilarity and emotion-aware Fake news detection (TieFake) method by jointly\nmodeling the multi-modal context information and the author sentiment in a\nunified framework. Specifically, we respectively employ BERT and ResNeSt to\nlearn the representations for text and images, and utilize publisher emotion\nextractor to capture the author's subjective emotion in the news content. We\nalso propose a scale-dot product attention mechanism to capture the similarity\nbetween title features and textual features. Experiments are conducted on two\npublicly available multi-modal datasets, and the results demonstrate that our\nproposed method can significantly improve the performance of fake news\ndetection. Our code is available at https://github.com/UESTC-GQJ/TieFake.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Quanjiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhao Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Ling Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhouguo Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes. (arXiv:2304.09433v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09433","description":"<p>A long standing goal of the data management community is to develop general,\nautomated systems that ingest semi-structured documents and output queryable\ntables without human effort or domain specific customization. Given the sheer\nvariety of potential documents, state-of-the art systems make simplifying\nassumptions and use domain specific training. In this work, we ask whether we\ncan maintain generality by using large language models (LLMs). LLMs, which are\npretrained on broad data, can perform diverse downstream tasks simply\nconditioned on natural language task descriptions.\n</p>\n<p>We propose and evaluate EVAPORATE, a simple, prototype system powered by\nLLMs. We identify two fundamentally different strategies for implementing this\nsystem: prompt the LLM to directly extract values from documents or prompt the\nLLM to synthesize code that performs the extraction. Our evaluations show a\ncost-quality tradeoff between these two approaches. Code synthesis is cheap,\nbut far less accurate than directly processing each document with the LLM. To\nimprove quality while maintaining low cost, we propose an extended code\nsynthesis implementation, EVAPORATE-CODE+, which achieves better quality than\ndirect extraction. Our key insight is to generate many candidate functions and\nensemble their extractions using weak supervision. EVAPORATE-CODE+ not only\noutperforms the state-of-the art systems, but does so using a sublinear pass\nover the documents with the LLM. This equates to a 110x reduction in the number\nof tokens the LLM needs to process, averaged across 16 real-world evaluation\nsettings of 10k documents each.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Simran Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Brandon Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eyuboglu_S/0/1/0/all/0/1\">Sabri Eyuboglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_A/0/1/0/all/0/1\">Avanika Narayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hojel_A/0/1/0/all/0/1\">Andrew Hojel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trummer_I/0/1/0/all/0/1\">Immanuel Trummer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EC^2: Emergent Communication for Embodied Control. (arXiv:2304.09448v1 [cs.LG])","link":"http://arxiv.org/abs/2304.09448","description":"<p>Embodied control requires agents to leverage multi-modal pre-training to\nquickly learn how to act in new environments, where video demonstrations\ncontain visual and motion details needed for low-level perception and control,\nand language instructions support generalization with abstract, symbolic\nstructures. While recent approaches apply contrastive learning to force\nalignment between the two modalities, we hypothesize better modeling their\ncomplementary differences can lead to more holistic representations for\ndownstream adaption. To this end, we propose Emergent Communication for\nEmbodied Control (EC^2), a novel scheme to pre-train video-language\nrepresentations for few-shot embodied control. The key idea is to learn an\nunsupervised \"language\" of videos via emergent communication, which bridges the\nsemantics of video details and structures of natural language. We learn\nembodied representations of video trajectories, emergent language, and natural\nlanguage using a language model, which is then used to finetune a lightweight\npolicy network for downstream control. Through extensive experiments in\nMetaworld and Franka Kitchen embodied benchmarks, EC^2 is shown to consistently\noutperform previous contrastive learning methods for both videos and texts as\ntask inputs. Further ablations confirm the importance of the emergent language,\nwhich is beneficial for both video and language learning, and significantly\nsuperior to using pre-trained video captions. We also present a quantitative\nand qualitative analysis of the emergent language and discuss future directions\ntoward better understanding and leveraging emergent communication in embodied\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1\">Yao Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1\">Shunyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Mingyu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion fusion for mental illness detection from social media: A survey. (arXiv:2304.09493v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09493","description":"<p>Mental illnesses are one of the most prevalent public health problems\nworldwide, which negatively influence people's lives and society's health. With\nthe increasing popularity of social media, there has been a growing research\ninterest in the early detection of mental illness by analysing user-generated\nposts on social media. According to the correlation between emotions and mental\nillness, leveraging and fusing emotion information has developed into a\nvaluable research topic. In this article, we provide a comprehensive survey of\napproaches to mental illness detection in social media that incorporate emotion\nfusion. We begin by reviewing different fusion strategies, along with their\nadvantages and disadvantages. Subsequently, we discuss the major challenges\nfaced by researchers working in this area, including issues surrounding the\navailability and quality of datasets, the performance of algorithms and\ninterpretability. We additionally suggest some potential directions for future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1\">Sophia Ananiadou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlling keywords and their positions in text generation. (arXiv:2304.09516v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09516","description":"<p>One of the challenges in text generation is to control generation as intended\nby a user. Previous studies have proposed to specify the keywords that should\nbe included in the generated text. However, this is insufficient to generate\ntext which reflect the user intent. For example, placing the important keyword\nbeginning of the text would helps attract the reader's attention, but existing\nmethods do not enable such flexible control. In this paper, we tackle a novel\ntask of controlling not only keywords but also the position of each keyword in\nthe text generation. To this end, we show that a method using special tokens\ncan control the relative position of keywords. Experimental results on\nsummarization and story generation tasks show that the proposed method can\ncontrol keywords and their positions. We also demonstrate that controlling the\nkeyword positions can generate summary texts that are closer to the user's\nintent than baseline. We release our code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sasazawa_Y/0/1/0/all/0/1\">Yuichi Sasazawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morishita_T/0/1/0/all/0/1\">Terufumi Morishita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozaki_H/0/1/0/all/0/1\">Hiroaki Ozaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imaichi_O/0/1/0/all/0/1\">Osamu Imaichi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogawa_Y/0/1/0/all/0/1\">Yasuhiro Sogawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent. (arXiv:2304.09542v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09542","description":"<p>Large Language Models (LLMs) have demonstrated a remarkable ability to\ngeneralize zero-shot to various language-related tasks. This paper focuses on\nthe study of exploring generative LLMs such as ChatGPT and GPT-4 for relevance\nranking in Information Retrieval (IR). Surprisingly, our experiments reveal\nthat properly instructed ChatGPT and GPT-4 can deliver competitive, even\nsuperior results than supervised methods on popular IR benchmarks. Notably,\nGPT-4 outperforms the fully fine-tuned monoT5-3B on MS MARCO by an average of\n2.7 nDCG on TREC datasets, an average of 2.3 nDCG on eight BEIR datasets, and\nan average of 2.7 nDCG on ten low-resource languages Mr.TyDi. Subsequently, we\ndelve into the potential for distilling the ranking capabilities of ChatGPT\ninto a specialized model. Our small specialized model that trained on 10K\nChatGPT generated data outperforms monoT5 trained on 400K annotated MS MARCO\ndata on BEIR. The code to reproduce our results is available at\nwww.github.com/sunnweiwei/RankGPT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1\">Lingyong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dawei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemEval 2023 Task 6: LegalEval -- Understanding Legal Texts. (arXiv:2304.09548v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09548","description":"<p>In populous countries, pending legal cases have been growing exponentially.\nThere is a need for developing NLP-based techniques for processing and\nautomatically understanding legal documents. To promote research in the area of\nLegal NLP we organized the shared task LegalEval - Understanding Legal Texts at\nSemEval 2023. LegalEval task has three sub-tasks: Task-A (Rhetorical Roles\nLabeling) is about automatically structuring legal documents into semantically\ncoherent units, Task-B (Legal Named Entity Recognition) deals with identifying\nrelevant entities in a legal document and Task-C (Court Judgement Prediction\nwith Explanation) explores the possibility of automatically predicting the\noutcome of a legal case along with providing an explanation for the prediction.\nIn total 26 teams (approx. 100 participants spread across the world) submitted\nsystems paper. In each of the sub-tasks, the proposed systems outperformed the\nbaselines; however, there is a lot of scope for improvement. This paper\ndescribes the tasks, and analyzes techniques proposed by various teams.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Modi_A/0/1/0/all/0/1\">Ashutosh Modi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalamkar_P/0/1/0/all/0/1\">Prathamesh Kalamkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karn_S/0/1/0/all/0/1\">Saurabh Karn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_A/0/1/0/all/0/1\">Aman Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Abhinav Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanikella_S/0/1/0/all/0/1\">Sai Kiran Tanikella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guha_S/0/1/0/all/0/1\">Shouvik Kumar Guha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malhan_S/0/1/0/all/0/1\">Sachin Malhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1\">Vivek Raghavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Robustness of Aspect-based Sentiment Analysis: Rethinking Model, Data, and Training. (arXiv:2304.09563v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09563","description":"<p>Aspect-based sentiment analysis (ABSA) aims at automatically inferring the\nspecific sentiment polarities toward certain aspects of products or services\nbehind the social media texts or reviews, which has been a fundamental\napplication to the real-world society. Since the early 2010s, ABSA has achieved\nextraordinarily high accuracy with various deep neural models. However,\nexisting ABSA models with strong in-house performances may fail to generalize\nto some challenging cases where the contexts are variable, i.e., low robustness\nto real-world environments. In this study, we propose to enhance the ABSA\nrobustness by systematically rethinking the bottlenecks from all possible\nangles, including model, data, and training. First, we strengthen the current\nbest-robust syntax-aware models by further incorporating the rich external\nsyntactic dependencies and the labels with aspect simultaneously with a\nuniversal-syntax graph convolutional network. In the corpus perspective, we\npropose to automatically induce high-quality synthetic training data with\nvarious types, allowing models to learn sufficient inductive bias for better\nrobustness. Last, we based on the rich pseudo data perform adversarial training\nto enhance the resistance to the context perturbation and meanwhile employ\ncontrastive learning to reinforce the representations of instances with\ncontrastive sentiments. Extensive robustness evaluations are conducted. The\nresults demonstrate that our enhanced syntax-aware model achieves better\nrobustness performances than all the state-of-the-art baselines. By\nadditionally incorporating our synthetic corpus, the robust testing results are\npushed with around 10% accuracy, which are then further improved by installing\nthe advanced training strategies. In-depth analyses are presented for revealing\nthe factors influencing the ABSA robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hao Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1\">Donghong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yafeng Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT Equipped with Emotional Dialogue Capabilities?. (arXiv:2304.09582v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09582","description":"<p>This report presents a study on the emotional dialogue capability of ChatGPT,\nan advanced language model developed by OpenAI. The study evaluates the\nperformance of ChatGPT on emotional dialogue understanding and generation\nthrough a series of experiments on several downstream tasks. Our findings\nindicate that while ChatGPT's performance on emotional dialogue understanding\nmay still lag behind that of supervised models, it exhibits promising results\nin generating emotional responses. Furthermore, the study suggests potential\navenues for future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weixiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yanpeng Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CB-Conformer: Contextual biasing Conformer for biased word recognition. (arXiv:2304.09607v1 [cs.SD])","link":"http://arxiv.org/abs/2304.09607","description":"<p>Due to the mismatch between the source and target domains, how to better\nutilize the biased word information to improve the performance of the automatic\nspeech recognition model in the target domain becomes a hot research topic.\nPrevious approaches either decode with a fixed external language model or\nintroduce a sizeable biasing module, which leads to poor adaptability and slow\ninference. In this work, we propose CB-Conformer to improve biased word\nrecognition by introducing the Contextual Biasing Module and the Self-Adaptive\nLanguage Model to vanilla Conformer. The Contextual Biasing Module combines\naudio fragments and contextual information, with only 0.2% model parameters of\nthe original Conformer. The Self-Adaptive Language Model modifies the internal\nweights of biased words based on their recall and precision, resulting in a\ngreater focus on biased words and more successful integration with the\nautomatic speech recognition model than the standard fixed language model. In\naddition, we construct and release an open-source Mandarin biased-word dataset\nbased on WenetSpeech. Experiments indicate that our proposed method brings a\n15.34% character error rate reduction, a 14.13% biased word recall increase,\nand a 6.80% biased word F1-score increase compared with the base Conformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yaoxun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Baiji Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+and_Q/0/1/0/all/0/1\">Qiaochu Huang and</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingchen Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Shiyin Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging Natural Language Processing and Psycholinguistics: computationally grounded semantic similarity and relatedness datasets for Basque and Spanish. (arXiv:2304.09616v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09616","description":"<p>We present a computationally-grounded word similarity dataset based on two\nwell-known Natural Language Processing resources; text corpora and knowledge\nbases. This dataset aims to fulfil a gap in psycholinguistic research by\nproviding a variety of quantifications of semantic similarity in an extensive\nset of noun pairs controlled by variables that play a significant role in\nlexical processing. The dataset creation has consisted in three steps, 1)\ncomputing four key psycholinguistic features for each noun; concreteness,\nfrequency, semantic and phonological neighbourhood density; 2) pairing nouns\nacross these four variables; 3) for each noun pair, assigning three types of\nword similarity measurements, computed out of text, Wordnet and hybrid\nembeddings. The present dataset includes noun pairs' information in Basque and\nEuropean Spanish, but further work intends to extend it to more languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goikoetxea_J/0/1/0/all/0/1\">J. Goikoetxea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arantzeta_M/0/1/0/all/0/1\">M. Arantzeta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_I/0/1/0/all/0/1\">I. San Martin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BRENT: Bidirectional Retrieval Enhanced Norwegian Transformer. (arXiv:2304.09649v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09649","description":"<p>Retrieval-based language models are increasingly employed in\nquestion-answering tasks. These models search in a corpus of documents for\nrelevant information instead of having all factual knowledge stored in its\nparameters, thereby enhancing efficiency, transparency, and adaptability. We\ndevelop the first Norwegian retrieval-based model by adapting the REALM\nframework and evaluating it on various tasks. After training, we also separate\nthe language model, which we call the reader, from the retriever components,\nand show that this can be fine-tuned on a range of downstream tasks. Results\nshow that retrieval augmented language modeling improves the reader's\nperformance on extractive question-answering, suggesting that this type of\ntraining improves language models' general ability to use context and that this\ndoes not happen at the expense of other abilities such as part-of-speech\ntagging, dependency parsing, named entity recognition, and lemmatization. Code,\ntrained models, and data are made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Charpentier_L/0/1/0/all/0/1\">Lucas Georges Gabriel Charpentier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wold_S/0/1/0/all/0/1\">Sondre Wold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samuel_D/0/1/0/all/0/1\">David Samuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ronningstad_E/0/1/0/all/0/1\">Egil R&#xf8;nningstad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MPMQA: Multimodal Question Answering on Product Manuals. (arXiv:2304.09660v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09660","description":"<p>Visual contents, such as illustrations and images, play a big role in product\nmanual understanding. Existing Product Manual Question Answering (PMQA)\ndatasets tend to ignore visual contents and only retain textual parts. In this\nwork, to emphasize the importance of multimodal contents, we propose a\nMultimodal Product Manual Question Answering (MPMQA) task. For each question,\nMPMQA requires the model not only to process multimodal contents but also to\nprovide multimodal answers. To support MPMQA, a large-scale dataset PM209 is\nconstructed with human annotations, which contains 209 product manuals from 27\nwell-known consumer electronic brands. Human annotations include 6 types of\nsemantic regions for manual contents and 22,021 pairs of question and answer.\nEspecially, each answer consists of a textual sentence and related visual\nregions from manuals. Taking into account the length of product manuals and the\nfact that a question is always related to a small number of pages, MPMQA can be\nnaturally split into two subtasks: retrieving most related pages and then\ngenerating multimodal answers. We further propose a unified model that can\nperform these two subtasks all together and achieve comparable performance with\nmultiple task-specific models. The PM209 dataset is available at\nhttps://github.com/AIM3-RUC/MPMQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1\">Anwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shuo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GeneGPT: Teaching Large Language Models to Use NCBI Web APIs. (arXiv:2304.09667v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09667","description":"<p>In this paper, we present GeneGPT, a novel method for teaching large language\nmodels (LLMs) to use the Web Application Programming Interfaces (APIs) of the\nNational Center for Biotechnology Information (NCBI) and answer genomics\nquestions. Specifically, we prompt Codex (code-davinci-002) to solve the\nGeneTuring tests with few-shot URL requests of NCBI API calls as demonstrations\nfor in-context learning. During inference, we stop the decoding once a call\nrequest is detected and make the API call with the generated URL. We then\nappend the raw execution results returned by NCBI APIs to the generated texts\nand continue the generation until the answer is found or another API call is\ndetected. Our preliminary results show that GeneGPT achieves state-of-the-art\nresults on three out of four one-shot tasks and four out of five zero-shot\ntasks in the GeneTuring dataset. Overall, GeneGPT achieves a macro-average\nscore of 0.76, which is much higher than retrieval-augmented LLMs such as the\nNew Bing (0.44), biomedical LLMs such as BioMedLM (0.08) and BioGPT (0.04), as\nwell as other LLMs such as GPT-3 (0.16) and ChatGPT (0.12).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qiao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yifan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive-Hint Prompting Improves Reasoning in Large Language Models. (arXiv:2304.09797v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09797","description":"<p>The performance of Large Language Models (LLMs) in reasoning tasks depends\nheavily on prompt design, with Chain-of-Thought (CoT) and self-consistency\nbeing critical methods that enhance this ability. However, these methods do not\nfully exploit the answers generated by the LLM to guide subsequent responses.\nThis paper proposes a new prompting method, named Progressive-Hint Prompting\n(PHP), that enables automatic multiple interactions between users and LLMs by\nusing previously generated answers as hints to progressively guide toward the\ncorrect answers. PHP is orthogonal to CoT and self-consistency, making it easy\nto combine with state-of-the-art techniques to further improve performance. We\nconducted an extensive and comprehensive evaluation to demonstrate the\neffectiveness of the proposed method. Our experimental results on six\nbenchmarks show that combining CoT and self-consistency with PHP significantly\nimproves accuracy while remaining highly efficient. For instance, with\ntext-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding\ncompared to Complex CoT, and a 46.17% reduction in sample paths with\nself-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances\non SVAMP (91.9%), GSM8K (95.5%) and AQuA (79.9%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chuanyang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Corpora for Germanic Low-Resource Languages and Dialects. (arXiv:2304.09805v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09805","description":"<p>Despite much progress in recent years, the vast majority of work in natural\nlanguage processing (NLP) is on standard languages with many speakers. In this\nwork, we instead focus on low-resource languages and in particular\nnon-standardized low-resource languages. Even within branches of major language\nfamilies, often considered well-researched, little is known about the extent\nand type of available resources and what the major NLP challenges are for these\nlanguage varieties. The first step to address this situation is a systematic\nsurvey of available corpora (most importantly, annotated corpora, which are\nparticularly valuable for NLP research). Focusing on Germanic low-resource\nlanguage varieties, we provide such a survey in this paper. Except for\ngeolocation (origin of speaker or document), we find that manually annotated\nlinguistic resources are sparse and, if they exist, mostly cover morphosyntax.\nDespite this lack of resources, we observe that interest in this area is\nincreasing: there is active development and a growing research community. To\nfacilitate research, we make our overview of over 80 corpora publicly\navailable. We share a companion website of this overview at\nhttps://github.com/mainlp/germanic-lrl-corpora .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blaschke_V/0/1/0/all/0/1\">Verena Blaschke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Two-Stage Framework with Self-Supervised Distillation For Cross-Domain Text Classification. (arXiv:2304.09820v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09820","description":"<p>Cross-domain text classification aims to adapt models to a target domain that\nlacks labeled data. It leverages or reuses rich labeled data from the different\nbut related source domain(s) and unlabeled data from the target domain. To this\nend, previous work focuses on either extracting domain-invariant features or\ntask-agnostic features, ignoring domain-aware features that may be present in\nthe target domain and could be useful for the downstream task. In this paper,\nwe propose a two-stage framework for cross-domain text classification. In the\nfirst stage, we finetune the model with mask language modeling (MLM) and\nlabeled data from the source domain. In the second stage, we further fine-tune\nthe model with self-supervised distillation (SSD) and unlabeled data from the\ntarget domain. We evaluate its performance on a public cross-domain text\nclassification benchmark and the experiment results show that our method\nachieves new state-of-the-art results for both single-source domain adaptations\n(94.17% $\\uparrow$1.03%) and multi-source domain adaptations (95.09%\n$\\uparrow$1.34%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yunlong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Libo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fairness in AI and Its Long-Term Implications on Society. (arXiv:2304.09826v1 [cs.CY])","link":"http://arxiv.org/abs/2304.09826","description":"<p>Successful deployment of artificial intelligence (AI) in various settings has\nled to numerous positive outcomes for individuals and society. However, AI\nsystems have also been shown to harm parts of the population due to biased\npredictions. We take a closer look at AI fairness and analyse how lack of AI\nfairness can lead to deepening of biases over time and act as a social\nstressor. If the issues persist, it could have undesirable long-term\nimplications on society, reinforced by interactions with other risks. We\nexamine current strategies for improving AI fairness, assess their limitations\nin terms of real-world deployment, and explore potential paths forward to\nensure we reap AI's benefits without harming significant parts of the society.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bohdal_O/0/1/0/all/0/1\">Ondrej Bohdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1\">Timothy Hospedales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barez_F/0/1/0/all/0/1\">Fazl Barez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models. (arXiv:2304.09842v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09842","description":"<p>Large language models (LLMs) have achieved remarkable progress in various\nnatural language processing tasks with emergent abilities. However, they face\ninherent limitations, such as an inability to access up-to-date information,\nutilize external tools, or perform precise mathematical reasoning. In this\npaper, we introduce Chameleon, a plug-and-play compositional reasoning\nframework that augments LLMs to help address these challenges. Chameleon\nsynthesizes programs to compose various tools, including LLM models,\noff-the-shelf vision models, web search engines, Python functions, and\nrule-based modules tailored to user interests. Built on top of an LLM as a\nnatural language planner, Chameleon infers the appropriate sequence of tools to\ncompose and execute in order to generate a final response. We showcase the\nadaptability and effectiveness of Chameleon on two tasks: ScienceQA and TabMWP.\nNotably, Chameleon with GPT-4 achieves an 86.54% accuracy on ScienceQA,\nsignificantly improving upon the best published few-shot model by 11.37%; using\nGPT-4 as the underlying LLM, Chameleon achieves a 17.8% increase over the\nstate-of-the-art model, leading to a 98.78% overall accuracy on TabMWP. Further\nstudies suggest that using GPT-4 as a planner exhibits more consistent and\nrational tool selection and is able to infer potential constraints given the\ninstructions, compared to other LLMs like ChatGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1\">Michel Galley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Verifiability in Generative Search Engines. (arXiv:2304.09848v1 [cs.CL])","link":"http://arxiv.org/abs/2304.09848","description":"<p>Generative search engines directly generate responses to user queries, along\nwith in-line citations. A prerequisite trait of a trustworthy generative search\nengine is verifiability, i.e., systems should cite comprehensively (high\ncitation recall; all statements are fully supported by citations) and\naccurately (high citation precision; every cite supports its associated\nstatement). We conduct human evaluation to audit four popular generative search\nengines -- Bing Chat, NeevaAI, perplexity.ai, and YouChat -- across a diverse\nset of queries from a variety of sources (e.g., historical Google user queries,\ndynamically-collected open-ended questions on Reddit, etc.). We find that\nresponses from existing generative search engines are fluent and appear\ninformative, but frequently contain unsupported statements and inaccurate\ncitations: on average, a mere 51.5% of generated sentences are fully supported\nby citations and only 74.5% of citations support their associated sentence. We\nbelieve that these results are concerningly low for systems that may serve as a\nprimary tool for information-seeking users, especially given their facade of\ntrustworthiness. We hope that our results further motivate the development of\ntrustworthy generative search engines and help researchers and users better\nunderstand the shortcomings of existing commercial systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nelson F. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PMC-Patients: A Large-scale Dataset of Patient Summaries and Relations for Benchmarking Retrieval-based Clinical Decision Support Systems. (arXiv:2202.13876v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13876","description":"<p>Objective: Retrieval-based Clinical Decision Support (ReCDS) can aid clinical\nworkflow by providing relevant literature and similar patients for a given\npatient. However, the development of ReCDS systems has been severely obstructed\nby the lack of diverse patient collections and publicly available large-scale\npatient-level annotation datasets. In this paper, we aim to define and\nbenchmark two ReCDS tasks: Patient-to-Article Retrieval (ReCDS-PAR) and\nPatient-to-Patient Retrieval (ReCDS-PPR) using a novel dataset called\nPMC-Patients. Methods: We extract patient summaries from PubMed Central\narticles using simple heuristics and utilize the PubMed citation graph to\ndefine patient-article relevance and patient-patient similarity. We also\nimplement and evaluate several ReCDS systems on the PMC-Patients benchmarks,\nincluding sparse retrievers, dense retrievers, and nearest neighbor retrievers.\nWe conduct several case studies to show the clinical utility of PMC-Patients.\nResults: PMC-Patients contains 167k patient summaries with 3.1M patient-article\nrelevance annotations and 293k patient-patient similarity annotations, which is\nthe largest-scale resource for ReCDS and also one of the largest patient\ncollections. Human evaluation and analysis show that PMC-Patients is a diverse\ndataset with high-quality annotations. The evaluation of various ReCDS systems\nshows that the PMC-Patients benchmark is challenging and calls for further\nresearch. Conclusion: We present PMC-Patients, a large-scale, diverse, and\npublicly available patient summary dataset with the largest-scale patient-level\nrelation annotations. Based on PMC-Patients, we formally define two benchmark\ntasks for ReCDS systems and evaluate various existing retrieval methods.\nPMC-Patients can largely facilitate methodology research on ReCDS systems and\nshows real-world clinical utility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhengyun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qiao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fangyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_T/0/1/0/all/0/1\">Tuorui Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sheng Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive language and vision learning of general fashion concepts. (arXiv:2204.03972v4 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2204.03972","description":"<p>The steady rise of online shopping goes hand in hand with the development of\nincreasingly complex ML and NLP models. While most use cases are cast as\nspecialized supervised learning problems, we argue that practitioners would\ngreatly benefit from more transferable representations of products. In this\nwork, we build on recent developments in contrastive learning to train\nFashionCLIP, a CLIP-like model for the fashion industry. We showcase its\ncapabilities for retrieval, classification and grounding, and release our model\nand code to the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chia_P/0/1/0/all/0/1\">Patrick John Chia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attanasio_G/0/1/0/all/0/1\">Giuseppe Attanasio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terragni_S/0/1/0/all/0/1\">Silvia Terragni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magalhaes_A/0/1/0/all/0/1\">Ana Rita Magalh&#xe3;es</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goncalves_D/0/1/0/all/0/1\">Diogo Goncalves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greco_C/0/1/0/all/0/1\">Ciro Greco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1\">Jacopo Tagliabue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeAttack: Code-Based Adversarial Attacks for Pre-trained Programming Language Models. (arXiv:2206.00052v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.00052","description":"<p>Pre-trained programming language (PL) models (such as CodeT5, CodeBERT,\nGraphCodeBERT, etc.,) have the potential to automate software engineering tasks\ninvolving code understanding and code generation. However, these models operate\nin the natural channel of code, i.e., they are primarily concerned with the\nhuman understanding of the code. They are not robust to changes in the input\nand thus, are potentially susceptible to adversarial attacks in the natural\nchannel. We propose, CodeAttack, a simple yet effective black-box attack model\nthat uses code structure to generate effective, efficient, and imperceptible\nadversarial code samples and demonstrates the vulnerabilities of the\nstate-of-the-art PL models to code-specific adversarial attacks. We evaluate\nthe transferability of CodeAttack on several code-code (translation and repair)\nand code-NL (summarization) tasks across different programming languages.\nCodeAttack outperforms state-of-the-art adversarial NLP attack models to\nachieve the best overall drop in performance while being more efficient,\nimperceptible, consistent, and fluent. The code can be found at\nhttps://github.com/reddy-lab-code-research/CodeAttack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1\">Akshita Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1\">Chandan K. Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discourse-Aware Graph Networks for Textual Logical Reasoning. (arXiv:2207.01450v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.01450","description":"<p>Textual logical reasoning, especially question-answering (QA) tasks with\nlogical reasoning, requires awareness of particular logical structures. The\npassage-level logical relations represent entailment or contradiction between\npropositional units (e.g., a concluding sentence). However, such structures are\nunexplored as current QA systems focus on entity-based relations. In this work,\nwe propose logic structural-constraint modeling to solve the logical reasoning\nQA and introduce discourse-aware graph networks (DAGNs). The networks first\nconstruct logic graphs leveraging in-line discourse connectives and generic\nlogic theories, then learn logic representations by end-to-end evolving the\nlogic relations with an edge-reasoning mechanism and updating the graph\nfeatures. This pipeline is applied to a general encoder, whose fundamental\nfeatures are joined with the high-level logic features for answer prediction.\nExperiments on three textual logical reasoning datasets demonstrate the\nreasonability of the logical structures built in DAGNs and the effectiveness of\nthe learned logic features. Moreover, zero-shot transfer results show the\nfeatures' generality to unseen logical texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yinya Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code Translation with Compiler Representations. (arXiv:2207.03578v4 [cs.PL] UPDATED)","link":"http://arxiv.org/abs/2207.03578","description":"<p>In this paper, we leverage low-level compiler intermediate representations\n(IR) to improve code translation. Traditional transpilers rely on syntactic\ninformation and handcrafted rules, which limits their applicability and\nproduces unnatural-looking code. Applying neural machine translation (NMT)\napproaches to code has successfully broadened the set of programs on which one\ncan get a natural-looking translation. However, they treat the code as\nsequences of text tokens, and still do not differentiate well enough between\nsimilar pieces of code which have different semantics in different languages.\nThe consequence is low quality translation, reducing the practicality of NMT,\nand stressing the need for an approach significantly increasing its accuracy.\nHere we propose to augment code translation with IRs, specifically LLVM IR,\nwith results on the C++, Java, Rust, and Go languages. Our method improves upon\nthe state of the art for unsupervised code translation, increasing the number\nof correct translations by 11% on average, and up to 79% for the Java -&gt; Rust\npair with greedy decoding. With beam search, it increases the number of correct\ntranslations by 5.5% in average. We extend previous test sets for code\ntranslation, by adding hundreds of Go and Rust functions. Additionally, we\ntrain models with high performance on the problem of IR decompilation,\ngenerating programming source code from IR, and study using IRs as intermediary\npivot for translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Szafraniec_M/0/1/0/all/0/1\">Marc Szafraniec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roziere_B/0/1/0/all/0/1\">Baptiste Roziere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leather_H/0/1/0/all/0/1\">Hugh Leather</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charton_F/0/1/0/all/0/1\">Francois Charton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labatut_P/0/1/0/all/0/1\">Patrick Labatut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COLO: A Contrastive Learning based Re-ranking Framework for One-Stage Summarization. (arXiv:2209.14569v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.14569","description":"<p>Traditional training paradigms for extractive and abstractive summarization\nsystems always only use token-level or sentence-level training objectives.\nHowever, the output summary is always evaluated from summary-level which leads\nto the inconsistency in training and evaluation. In this paper, we propose a\nContrastive Learning based re-ranking framework for one-stage summarization\ncalled COLO. By modeling a contrastive objective, we show that the\nsummarization model is able to directly generate summaries according to the\nsummary-level score without additional modules and parameters. Extensive\nexperiments demonstrate that COLO boosts the extractive and abstractive results\nof one-stage systems on CNN/DailyMail benchmark to 44.58 and 46.33 ROUGE-1\nscore while preserving the parameter efficiency and inference efficiency.\nCompared with state-of-the-art multi-stage systems, we save more than 100 GPU\ntraining hours and obtaining 3~8 speed-up ratio during inference while\nmaintaining comparable results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_C/0/1/0/all/0/1\">Chenxin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fighting FIRe with FIRE: Assessing the Validity of Text-to-Video Retrieval Benchmarks. (arXiv:2210.05038v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05038","description":"<p>Searching troves of videos with textual descriptions is a core multimodal\nretrieval task. Owing to the lack of a purpose-built dataset for text-to-video\nretrieval, video captioning datasets have been re-purposed to evaluate models\nby (1) treating captions as positive matches to their respective videos and (2)\nassuming all other videos to be negatives. However, this methodology leads to a\nfundamental flaw during evaluation: since captions are marked as relevant only\nto their original video, many alternate videos also match the caption, which\nintroduces false-negative caption-video pairs. We show that when these false\nnegatives are corrected, a recent state-of-the-art model gains 25\\% recall\npoints -- a difference that threatens the validity of the benchmark itself. To\ndiagnose and mitigate this issue, we annotate and release 683K additional\ncaption-video pairs. Using these, we recompute effectiveness scores for three\nmodels on two standard benchmarks (MSR-VTT and MSVD). We find that (1) the\nrecomputed metrics are up to 25\\% recall points higher for the best models, (2)\nthese benchmarks are nearing saturation for Recall@10, (3) caption length\n(generality) is related to the number of positives, and (4) annotation costs\ncan be mitigated through sampling. We recommend retiring these benchmarks in\ntheir current form, and we make recommendations for future text-to-video\nretrieval benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1\">Pedro Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azab_M/0/1/0/all/0/1\">Mahmoud Azab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silvert_B/0/1/0/all/0/1\">Becka Silvert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_R/0/1/0/all/0/1\">Renato Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labson_L/0/1/0/all/0/1\">Linzy Labson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_H/0/1/0/all/0/1\">Hardik Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Seungwhan Moon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation. (arXiv:2210.07558v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07558","description":"<p>With the ever-growing size of pretrained models (PMs), fine-tuning them has\nbecome more expensive and resource-hungry. As a remedy, low-rank adapters\n(LoRA) keep the main pretrained weights of the model frozen and just introduce\nsome learnable truncated SVD modules (so-called LoRA blocks) to the model.\nWhile LoRA blocks are parameter-efficient, they suffer from two major problems:\nfirst, the size of these blocks is fixed and cannot be modified after training\n(for example, if we need to change the rank of LoRA blocks, then we need to\nre-train them from scratch); second, optimizing their rank requires an\nexhaustive search and effort. In this work, we introduce a dynamic low-rank\nadaptation (DyLoRA) technique to address these two problems together. Our\nDyLoRA method trains LoRA blocks for a range of ranks instead of a single rank\nby sorting the representation learned by the adapter module at different ranks\nduring training. We evaluate our solution on different natural language\nunderstanding (GLUE benchmark) and language generation tasks (E2E, DART and\nWebNLG) using different pretrained models such as RoBERTa and GPT with\ndifferent sizes. Our results show that we can train dynamic search-free models\nwith DyLoRA at least 4 to 7 times (depending to the task) faster than LoRA\nwithout significantly compromising performance. Moreover, our models can\nperform consistently well on a much larger range of ranks compared to LoRA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valipour_M/0/1/0/all/0/1\">Mojtaba Valipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobyzev_I/0/1/0/all/0/1\">Ivan Kobyzev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InferEM: Inferring the Speaker's Intention for Empathetic Dialogue Generation. (arXiv:2212.06373v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.06373","description":"<p>Current approaches to empathetic response generation typically encode the\nentire dialogue history directly and put the output into a decoder to generate\nfriendly feedback. These methods focus on modelling contextual information but\nneglect capturing the direct intention of the speaker. We argue that the last\nutterance in the dialogue empirically conveys the intention of the speaker.\nConsequently, we propose a novel model named InferEM for empathetic response\ngeneration. We separately encode the last utterance and fuse it with the entire\ndialogue through the multi-head attention based intention fusion module to\ncapture the speaker's intention. Besides, we utilize previous utterances to\npredict the last utterance, which simulates human's psychology to guess what\nthe interlocutor may speak in advance. To balance the optimizing rates of the\nutterance prediction and response generation, a multi-task learning strategy is\ndesigned for InferEM. Experimental results demonstrate the plausibility and\nvalidity of InferEM in improving empathetic expression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lv_G/0/1/0/all/0/1\">Guoqing Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhigang Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge. (arXiv:2303.14070v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.14070","description":"<p>Recent large language models (LLMs) in the general domain, such as ChatGPT,\nhave shown remarkable success in following instructions and producing\nhuman-like responses. However, such language models have yet to be adapted for\nthe medical domain, resulting in poor accuracy of responses and an inability to\nprovide sound advice on medical diagnoses, medications, etc. To address this\nproblem, we fine-tuned our ChatDoctor model based on 100k real-world\npatient-physician conversations from an online medical consultation site.\nBesides, we add autonomous knowledge retrieval capabilities to our ChatDoctor,\nfor example, Wikipedia or a disease database as a knowledge brain. By\nfine-tuning the LLMs using these 100k patient-physician conversations, our\nmodel showed significant improvements in understanding patients' needs and\nproviding informed advice. The autonomous ChatDoctor model based on Wikipedia\nand Database Brain can access real-time and authoritative information and\nanswer patient questions based on this information, significantly improving the\naccuracy of the model's responses, which shows extraordinary potential for the\nmedical field with a low tolerance for error. To facilitate the further\ndevelopment of dialogue models in the medical field, we make available all\nsource code, datasets, and model weights available at:\nhttps://github.com/Kent0n-Li/ChatDoctor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zihan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dan_R/0/1/0/all/0/1\">Ruilong Dan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">You Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers. (arXiv:2304.00215v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.00215","description":"<p>Relation prediction on knowledge graphs (KGs) is a key research topic.\nDominant embedding-based methods mainly focus on the transductive setting and\nlack the inductive ability to generalize to new entities for inference.\nExisting methods for inductive reasoning mostly mine the connections between\nentities, i.e., relational paths, without considering the nature of head and\ntail entities contained in the relational context. This paper proposes a novel\nmethod that captures both connections between entities and the intrinsic nature\nof entities, by simultaneously aggregating RElational Paths and cOntext with a\nunified hieRarchical Transformer framework, namely REPORT. REPORT relies solely\non relation semantics and can naturally generalize to the fully-inductive\nsetting, where KGs for training and inference have no common entities. In the\nexperiments, REPORT performs consistently better than all baselines on almost\nall the eight version subsets of two fully-inductive datasets. Moreover. REPORT\nis interpretable by providing each element's contribution to the prediction\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhendong Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT detectors are biased against non-native English writers. (arXiv:2304.02819v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.02819","description":"<p>The rapid adoption of generative language models has brought about\nsubstantial advancements in digital communication, while simultaneously raising\nconcerns regarding the potential misuse of AI-generated content. Although\nnumerous detection methods have been proposed to differentiate between AI and\nhuman-generated content, the fairness and robustness of these detectors remain\nunderexplored. In this study, we evaluate the performance of several\nwidely-used GPT detectors using writing samples from native and non-native\nEnglish writers. Our findings reveal that these detectors consistently\nmisclassify non-native English writing samples as AI-generated, whereas native\nwriting samples are accurately identified. Furthermore, we demonstrate that\nsimple prompting strategies can not only mitigate this bias but also\neffectively bypass GPT detectors, suggesting that GPT detectors may\nunintentionally penalize writers with constrained linguistic expressions. Our\nresults call for a broader conversation about the ethical implications of\ndeploying ChatGPT content detectors and caution against their use in evaluative\nor educational settings, particularly when they may inadvertently penalize or\nexclude non-native English speakers from the global discourse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1\">Weixin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuksekgonul_M/0/1/0/all/0/1\">Mert Yuksekgonul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yining Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_E/0/1/0/all/0/1\">Eric Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models. (arXiv:2304.03738v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2304.03738","description":"<p>As the capabilities of generative language models continue to advance, the\nimplications of biases ingrained within these models have garnered increasing\nattention from researchers, practitioners, and the broader public. This article\ninvestigates the challenges and risks associated with biases in large-scale\nlanguage models like ChatGPT. We discuss the origins of biases, stemming from,\namong others, the nature of training data, model specifications, algorithmic\nconstraints, product design, and policy decisions. We explore the ethical\nconcerns arising from the unintended consequences of biased model outputs. We\nfurther analyze the potential opportunities to mitigate biases, the\ninevitability of some biases, and the implications of deploying these models in\nvarious applications, such as virtual assistants, content generation, and\nchatbots. Finally, we review the current approaches to identify, quantify, and\nmitigate biases in language models, emphasizing the need for a\nmulti-disciplinary, collaborative effort to develop more equitable,\ntransparent, and responsible AI systems. This article aims to stimulate a\nthoughtful dialogue within the artificial intelligence community, encouraging\nresearchers and developers to reflect on the role of biases in generative\nlanguage models and the ongoing pursuit of ethical AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferrara_E/0/1/0/all/0/1\">Emilio Ferrara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PDFVQA: A New Dataset for Real-World VQA on PDF Documents. (arXiv:2304.06447v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2304.06447","description":"<p>Document-based Visual Question Answering examines the document understanding\nof document images in conditions of natural language questions. We proposed a\nnew document-based VQA dataset, PDF-VQA, to comprehensively examine the\ndocument understanding from various aspects, including document element\nrecognition, document layout structural understanding as well as contextual\nunderstanding and key information extraction. Our PDF-VQA dataset extends the\ncurrent scale of document understanding that limits on the single document page\nto the new scale that asks questions over the full document of multiple pages.\nWe also propose a new graph-based VQA model that explicitly integrates the\nspatial and hierarchically structural relationships between different document\nelements to boost the document structural understanding. The performances are\ncompared with several baselines over different question types and\ntasks\\footnote{The full dataset will be released after paper acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yihao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Siwen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyunsuk Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sabi\\'a: Portuguese Large Language Models. (arXiv:2304.07880v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.07880","description":"<p>As the capabilities of language models continue to advance, it is conceivable\nthat \"one-size-fits-all\" model will remain as the main paradigm. For instance,\ngiven the vast number of languages worldwide, many of which are low-resource,\nthe prevalent practice is to pretrain a single model on multiple languages. In\nthis paper, we add to the growing body of evidence that challenges this\npractice, demonstrating that monolingual pretraining on the target language\nsignificantly improves models already extensively trained on diverse corpora.\nMore specifically, we further pretrain GPT-J and LLaMA models on Portuguese\ntexts using 3% or less of their original pretraining budget. Few-shot\nevaluations on Poeta, a suite of 14 Portuguese datasets, reveal that our models\noutperform English-centric and multilingual counterparts by a significant\nmargin. Our best model, Sabi\\'a-65B, performs on par with GPT-3.5-turbo. By\nevaluating on datasets originally conceived in the target language as well as\ntranslated ones, we study the contributions of language-specific pretraining in\nterms of 1) capturing linguistic nuances and structures inherent to the target\nlanguage, and 2) enriching the model's knowledge about a domain or culture. Our\nresults indicate that the majority of the benefits stem from the\ndomain-specific knowledge acquired through monolingual pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pires_R/0/1/0/all/0/1\">Ramon Pires</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abonizio_H/0/1/0/all/0/1\">Hugo Abonizio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almeida_T/0/1/0/all/0/1\">Thales Sales Almeida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speaker Profiling in Multiparty Conversations. (arXiv:2304.08801v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.08801","description":"<p>In conversational settings, individuals exhibit unique behaviors, rendering a\none-size-fits-all approach insufficient for generating responses by dialogue\nagents. Although past studies have aimed to create personalized dialogue agents\nusing speaker persona information, they have relied on the assumption that the\nspeaker's persona is already provided. However, this assumption is not always\nvalid, especially when it comes to chatbots utilized in industries like\nbanking, hotel reservations, and airline bookings. This research paper aims to\nfill this gap by exploring the task of Speaker Profiling in Conversations\n(SPC). The primary objective of SPC is to produce a summary of persona\ncharacteristics for each individual speaker present in a dialogue. To\naccomplish this, we have divided the task into three subtasks: persona\ndiscovery, persona-type identification, and persona-value extraction. Given a\ndialogue, the first subtask aims to identify all utterances that contain\npersona information. Subsequently, the second task evaluates these utterances\nto identify the type of persona information they contain, while the third\nsubtask identifies the specific persona values for each identified type. To\naddress the task of SPC, we have curated a new dataset named SPICE, which comes\nwith specific labels. We have evaluated various baselines on this dataset and\nbenchmarked it with a new neural model, SPOT, which we introduce in this paper.\nFurthermore, we present a comprehensive analysis of SPOT, examining the\nlimitations of individual modules both quantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shivani Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rishabh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting the Role of Similarity and Dissimilarity in Best Counter Argument Retrieval. (arXiv:2304.08807v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.08807","description":"<p>This paper studies the task of best counter-argument retrieval given an input\nargument. Following the definition that the best counter-argument addresses the\nsame aspects as the input argument while having the opposite stance, we aim to\ndevelop an efficient and effective model for scoring counter-arguments based on\nsimilarity and dissimilarity metrics. We first conduct an experimental study on\nthe effectiveness of available scoring methods, including traditional\nLearning-To-Rank (LTR) and recent neural scoring models. We then propose\nBipolar-encoder, a novel BERT-based model to learn an optimal representation\nfor simultaneous similarity and dissimilarity. Experimental results show that\nour proposed method can achieve the accuracy@1 of 49.04\\%, which significantly\noutperforms other baselines by a large margin. When combined with an\nappropriate caching technique, Bipolar-encoder is comparably efficient at\nprediction time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hongguang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shuirong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cam-Tu Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-04-19T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}