{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-07-27T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"EFL Students' Attitudes and Contradictions in a Machine-in-the-loop Activity System. (arXiv:2307.13699v1 [cs.HC])","link":"http://arxiv.org/abs/2307.13699","description":"<p>This study applies Activity Theory and investigates the attitudes and\ncontradictions of 67 English as a foreign language (EFL) students from four\nHong Kong secondary schools towards machine-in-the-loop writing, where\nartificial intelligence (AI) suggests ideas during composition. Students\nanswered an open-ended question about their feelings on writing with AI.\nResults revealed mostly positive attitudes, with some negative or mixed\nfeelings. From a thematic analysis, contradictions or points of tension between\nstudents and AI stemmed from AI inadequacies, students' balancing enthusiasm\nwith preference, and their striving for language autonomy. The research\nhighlights the benefits and challenges of implementing machine-in-the-loop\nwriting in EFL classrooms, suggesting educators align activity goals with\nstudents' values, language abilities, and AI capabilities to enhance students'\nactivity systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Woo_D/0/1/0/all/0/1\">David James Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susanto_H/0/1/0/all/0/1\">Hengky Susanto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1\">Kai Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Faithfulness in Chain-of-Thought Reasoning. (arXiv:2307.13702v1 [cs.AI])","link":"http://arxiv.org/abs/2307.13702","description":"<p>Large language models (LLMs) perform better when they produce step-by-step,\n\"Chain-of-Thought\" (CoT) reasoning before answering a question, but it is\nunclear if the stated reasoning is a faithful explanation of the model's actual\nreasoning (i.e., its process for answering the question). We investigate\nhypotheses for how CoT reasoning may be unfaithful, by examining how the model\npredictions change when we intervene on the CoT (e.g., by adding mistakes or\nparaphrasing it). Models show large variation across tasks in how strongly they\ncondition on the CoT when predicting their answer, sometimes relying heavily on\nthe CoT and other times primarily ignoring it. CoT's performance boost does not\nseem to come from CoT's added test-time compute alone or from information\nencoded via the particular phrasing of the CoT. As models become larger and\nmore capable, they produce less faithful reasoning on most tasks we study.\nOverall, our results suggest that CoT can be faithful if the circumstances such\nas the model size and task are carefully chosen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lanham_T/0/1/0/all/0/1\">Tamera Lanham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anna Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radhakrishnan_A/0/1/0/all/0/1\">Ansh Radhakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steiner_B/0/1/0/all/0/1\">Benoit Steiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denison_C/0/1/0/all/0/1\">Carson Denison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_D/0/1/0/all/0/1\">Danny Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dustin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1\">Esin Durmus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubinger_E/0/1/0/all/0/1\">Evan Hubinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kernion_J/0/1/0/all/0/1\">Jackson Kernion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukosiute_K/0/1/0/all/0/1\">Kamil&#x117; Luko&#x161;i&#x16b;t&#x117;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Karina Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Newton Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_N/0/1/0/all/0/1\">Nicholas Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiefer_N/0/1/0/all/0/1\">Nicholas Schiefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rausch_O/0/1/0/all/0/1\">Oliver Rausch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larson_R/0/1/0/all/0/1\">Robin Larson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCandlish_S/0/1/0/all/0/1\">Sam McCandlish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1\">Sandipan Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1\">Saurav Kadavath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shannon Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henighan_T/0/1/0/all/0/1\">Thomas Henighan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maxwell_T/0/1/0/all/0/1\">Timothy Maxwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Telleen_Lawton_T/0/1/0/all/0/1\">Timothy Telleen-Lawton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hume_T/0/1/0/all/0/1\">Tristan Hume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hatfield_Dodds_Z/0/1/0/all/0/1\">Zac Hatfield-Dodds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1\">Jared Kaplan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1\">Jan Brauner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diversity and Language Technology: How Techno-Linguistic Bias Can Cause Epistemic Injustice. (arXiv:2307.13714v1 [cs.CY])","link":"http://arxiv.org/abs/2307.13714","description":"<p>It is well known that AI-based language technology -- large language models,\nmachine translation systems, multilingual dictionaries, and corpora -- is\ncurrently limited to 2 to 3 percent of the world's most widely spoken and/or\nfinancially and politically best supported languages. In response, recent\nresearch efforts have sought to extend the reach of AI technology to\n``underserved languages.'' In this paper, we show that many of these attempts\nproduce flawed solutions that adhere to a hard-wired representational\npreference for certain languages, which we call techno-linguistic bias.\nTechno-linguistic bias is distinct from the well-established phenomenon of\nlinguistic bias as it does not concern the languages represented but rather the\ndesign of the technologies. As we show through the paper, techno-linguistic\nbias can result in systems that can only express concepts that are part of the\nlanguage and culture of dominant powers, unable to correctly represent concepts\nfrom other communities. We argue that at the root of this problem lies a\nsystematic tendency of technology developer communities to apply a simplistic\nunderstanding of diversity which does not do justice to the more profound\ndifferences that languages, and ultimately the communities that speak them,\nembody. Drawing on the concept of epistemic injustice, we point to the broader\nsociopolitical consequences of the bias we identify and show how it can lead\nnot only to a disregard for valuable aspects of diversity but also to an\nunder-representation of the needs and diverse worldviews of marginalized\nlanguage communities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Helm_P/0/1/0/all/0/1\">Paula Helm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bella_G/0/1/0/all/0/1\">G&#xe1;bor Bella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_G/0/1/0/all/0/1\">Gertraud Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giunchiglia_F/0/1/0/all/0/1\">Fausto Giunchiglia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combating the Curse of Multilinguality in Cross-Lingual WSD by Aligning Sparse Contextualized Word Representations. (arXiv:2307.13776v1 [cs.CL])","link":"http://arxiv.org/abs/2307.13776","description":"<p>In this paper, we advocate for using large pre-trained monolingual language\nmodels in cross lingual zero-shot word sense disambiguation (WSD) coupled with\na contextualized mapping mechanism. We also report rigorous experiments that\nillustrate the effectiveness of employing sparse contextualized word\nrepresentations obtained via a dictionary learning procedure. Our experimental\nresults demonstrate that the above modifications yield a significant\nimprovement of nearly 6.5 points of increase in the average F-score (from 62.0\nto 68.5) over a collection of 17 typologically diverse set of target languages.\nWe release our source code for replicating our experiments at\nhttps://github.com/begab/sparsity_makes_sense.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berend_G/0/1/0/all/0/1\">G&#xe1;bor Berend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is GPT a Computational Model of Emotion? Detailed Analysis. (arXiv:2307.13779v1 [cs.CL])","link":"http://arxiv.org/abs/2307.13779","description":"<p>This paper investigates the emotional reasoning abilities of the GPT family\nof large language models via a component perspective. The paper first examines\nhow the model reasons about autobiographical memories. Second, it\nsystematically varies aspects of situations to impact emotion intensity and\ncoping tendencies. Even without the use of prompt engineering, it is shown that\nGPT's predictions align significantly with human-provided appraisals and\nemotional labels. However, GPT faces difficulties predicting emotion intensity\nand coping responses. GPT-4 showed the highest performance in the initial study\nbut fell short in the second, despite providing superior results after minor\nprompt engineering. This assessment brings up questions on how to effectively\nemploy the strong points and address the weak areas of these models,\nparticularly concerning response variability. These studies underscore the\nmerits of evaluating models from a componential perspective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tak_A/0/1/0/all/0/1\">Ala N. Tak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gratch_J/0/1/0/all/0/1\">Jonathan Gratch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy. (arXiv:2307.13808v1 [cs.CL])","link":"http://arxiv.org/abs/2307.13808","description":"<p>To mitigate potential risks associated with language models, recent AI\ndetection research proposes incorporating watermarks into machine-generated\ntext through random vocabulary restrictions and utilizing this information for\ndetection. While these watermarks only induce a slight deterioration in\nperplexity, our empirical investigation reveals a significant detriment to the\nperformance of conditional text generation. To address this issue, we introduce\na simple yet effective semantic-aware watermarking algorithm that considers the\ncharacteristics of conditional text generation and the input context.\nExperimental results demonstrate that our proposed method yields substantial\nimprovements across various text generation models, including BART and Flan-T5,\nin tasks such as summarization and data-to-text generation while maintaining\ndetection ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Deyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yue Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARC-NLP at Multimodal Hate Speech Event Detection 2023: Multimodal Methods Boosted by Ensemble Learning, Syntactical and Entity Features. (arXiv:2307.13829v1 [cs.CL])","link":"http://arxiv.org/abs/2307.13829","description":"<p>Text-embedded images can serve as a means of spreading hate speech,\npropaganda, and extremist beliefs. Throughout the Russia-Ukraine war, both\nopposing factions heavily relied on text-embedded images as a vehicle for\nspreading propaganda and hate speech. Ensuring the effective detection of hate\nspeech and propaganda is of utmost importance to mitigate the negative effect\nof hate speech dissemination. In this paper, we outline our methodologies for\ntwo subtasks of Multimodal Hate Speech Event Detection 2023. For the first\nsubtask, hate speech detection, we utilize multimodal deep learning models\nboosted by ensemble learning and syntactical text attributes. For the second\nsubtask, target detection, we employ multimodal deep learning models boosted by\nnamed entity features. Through experimentation, we demonstrate the superior\nperformance of our models compared to all textual, visual, and text-visual\nbaselines employed in multimodal hate speech detection. Furthermore, our models\nachieve the first place in both subtasks on the final leaderboard of the shared\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahin_U/0/1/0/all/0/1\">Umitcan Sahin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kucukkaya_I/0/1/0/all/0/1\">Izzet Emre Kucukkaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozcelik_O/0/1/0/all/0/1\">Oguzhan Ozcelik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toraman_C/0/1/0/all/0/1\">Cagri Toraman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebArena: A Realistic Web Environment for Building Autonomous Agents. (arXiv:2307.13854v1 [cs.AI])","link":"http://arxiv.org/abs/2307.13854","description":"<p>With generative AI advances, the exciting potential for autonomous agents to\nmanage daily tasks via natural language commands has emerged. However, cur rent\nagents are primarily created and tested in simplified synthetic environments,\nsubstantially limiting real-world scenario representation. In this paper, we\nbuild an environment for agent command and control that is highly realistic and\nreproducible. Specifically, we focus on agents that perform tasks on websites,\nand we create an environment with fully functional websites from four common\ndomains: e-commerce, social forum discussions, collaborative software\ndevelopment, and content management. Our environment is enriched with tools\n(e.g., a map) and external knowledge bases (e.g., user manuals) to encourage\nhuman-like task-solving. Building upon our environment, we release a set of\nbenchmark tasks focusing on evaluating the functional correctness of task\ncompletions. The tasks in our benchmark are diverse, long-horizon, and are\ndesigned to emulate tasks that humans routinely perform on the internet. We\ndesign and implement several autonomous agents, integrating recent techniques\nsuch as reasoning before acting. The results demonstrate that solving complex\ntasks is challenging: our best GPT-4-based agent only achieves an end-to-end\ntask success rate of 10.59%. These results highlight the need for further\ndevelopment of robust agents, that current state-of-the-art LMs are far from\nperfect performance in these real-life tasks, and that WebArena can be used to\nmeasure such progress. Our code, data, environment reproduction resources, and\nvideo demonstrations are publicly available at https://webarena.dev/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Frank F. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xuhui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_R/0/1/0/all/0/1\">Robert Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_A/0/1/0/all/0/1\">Abishek Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xianyi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1\">Daniel Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1\">Uri Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FinTree: Financial Dataset Pretrain Transformer Encoder for Relation Extraction. (arXiv:2307.13900v1 [cs.CL])","link":"http://arxiv.org/abs/2307.13900","description":"<p>We present FinTree, Financial Dataset Pretrain Transformer Encoder for\nRelation Extraction. Utilizing an encoder language model, we further pretrain\nFinTree on the financial dataset, adapting the model in financial domain tasks.\nFinTree stands out with its novel structure that predicts a masked token\ninstead of the conventional [CLS] token, inspired by the Pattern Exploiting\nTraining methodology. This structure allows for more accurate relation\npredictions between two given entities. The model is trained with a unique\ninput pattern to provide contextual and positional information about the\nentities of interest, and a post-processing step ensures accurate predictions\nin line with the entity types. Our experiments demonstrate that FinTree\noutperforms on the REFinD, a large-scale financial relation extraction dataset.\nThe code and pretrained models are available at\nhttps://github.com/HJ-Ok/FinTree.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ok_H/0/1/0/all/0/1\">Hyunjong Ok</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical Error Correction with Supervised Fine-Tuning. (arXiv:2307.13923v1 [cs.CL])","link":"http://arxiv.org/abs/2307.13923","description":"<p>Grammatical error correction aims to correct ungrammatical sentences\nautomatically. Recently, some work has demonstrated the excellent capabilities\nof closed-source Large Language Models (LLMs, e.g., ChatGPT) in grammatical\nerror correction. However, the potential of open-source LLMs remains\nunexplored. In this paper, we introduced GrammarGPT, an open-source LLM, to\npreliminary explore its potential for native Chinese grammatical error\ncorrection. The core recipe of GrammarGPT is to leverage the hybrid dataset of\nChatGPT-generated and human-annotated. For grammatical errors with clues, we\nproposed a heuristic method to guide ChatGPT to generate ungrammatical\nsentences by providing those clues. For grammatical errors without clues, we\ncollected ungrammatical sentences from publicly available websites and manually\ncorrected them. In addition, we employed an error-invariant augmentation method\nto enhance the ability of the model to correct native Chinese grammatical\nerrors. We ultimately constructed about 1k parallel data and utilized these\ndata to fine-tune open-source LLMs (e.g., Phoenix, released by The Chinese\nUniversity of Hong Kong, Shenzhen) with instruction tuning. The experimental\nresults show that GrammarGPT outperforms the existing SOTA system\nsignificantly. Although model parameters are 20x larger than the SOTA baseline,\nthe required amount of data for instruction tuning is 1200x smaller,\nillustrating the potential of open-source LLMs on native CGEC. Our GrammarGPT\nranks $3^{rd}$ on NLPCC2023 SharedTask1, demonstrating our approach's\neffectiveness. The code and data are available at\n\\url{https://github.com/FreedomIntelligence/GrammarGPT}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yaxin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1\">Feng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Does Diffusion Influence Pretrained Language Models on Out-of-Distribution Data?. (arXiv:2307.13949v1 [cs.CL])","link":"http://arxiv.org/abs/2307.13949","description":"<p>Transformer-based pretrained language models (PLMs) have achieved great\nsuccess in modern NLP. An important advantage of PLMs is good\nout-of-distribution (OOD) robustness. Recently, diffusion models have attracted\na lot of work to apply diffusion to PLMs. It remains under-explored how\ndiffusion influences PLMs on OOD data. The core of diffusion models is a\nforward diffusion process which gradually applies Gaussian noise to inputs, and\na reverse denoising process which removes noise. The noised input\nreconstruction is a fundamental ability of diffusion models. We directly\nanalyze OOD robustness by measuring the reconstruction loss, including testing\nthe abilities to reconstruct OOD data, and to detect OOD samples. Experiments\nare conducted by analyzing different training parameters and data statistical\nfeatures on eight datasets. It shows that finetuning PLMs with diffusion\ndegrades the reconstruction ability on OOD data. The comparison also shows that\ndiffusion models can effectively detect OOD samples, achieving state-of-the-art\nperformance in most of the datasets with an absolute accuracy improvement up to\n18%. These results indicate that diffusion reduces OOD robustness of PLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huazheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1\">Daixuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haifeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1\">Qi Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jianxin Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"This is not correct! Negation-aware Evaluation of Language Generation Systems. (arXiv:2307.13989v1 [cs.CL])","link":"http://arxiv.org/abs/2307.13989","description":"<p>Large language models underestimate the impact of negations on how much they\nchange the meaning of a sentence. Therefore, learned evaluation metrics based\non these models are insensitive to negations. In this paper, we propose\nNegBLEURT, a negation-aware version of the BLEURT evaluation metric. For that,\nwe designed a rule-based sentence negation tool and used it to create the\nCANNOT negation evaluation dataset. Based on this dataset, we fine-tuned a\nsentence transformer and an evaluation metric to improve their negation\nsensitivity. Evaluating these models on existing benchmarks shows that our\nfine-tuned models outperform existing metrics on the negated sentences by far\nwhile preserving their base models' performances on other perturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anschutz_M/0/1/0/all/0/1\">Miriam Ansch&#xfc;tz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozano_D/0/1/0/all/0/1\">Diego Miguel Lozano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groh_G/0/1/0/all/0/1\">Georg Groh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Affective Natural Language Generation of Event Descriptions through Fine-grained Appraisal Conditions. (arXiv:2307.14004v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14004","description":"<p>Models for affective text generation have shown a remarkable progress, but\nthey commonly rely only on basic emotion theories or valance/arousal values as\nconditions. This is appropriate when the goal is to create explicit emotion\nstatements (\"The kid is happy.\"). Emotions are, however, commonly communicated\nimplicitly. For instance, the emotional interpretation of an event (\"Their dog\ndied.\") does often not require an explicit emotion statement. In psychology,\nappraisal theories explain the link between a cognitive evaluation of an event\nand the potentially developed emotion. They put the assessment of the situation\non the spot, for instance regarding the own control or the responsibility for\nwhat happens. We hypothesize and subsequently show that including appraisal\nvariables as conditions in a generation framework comes with two advantages.\n(1) The generation model is informed in greater detail about what makes a\nspecific emotion and what properties it has. This leads to text generation that\nbetter fulfills the condition. (2) The variables of appraisal allow a user to\nperform a more fine-grained control of the generated text, by stating\nproperties of a situation instead of only providing the emotion category. Our\nBart and T5-based experiments with 7 emotions (Anger, Disgust, Fear, Guilt,\nJoy, Sadness, Shame), and 7 appraisals (Attention, Responsibility, Control,\nCircumstance, Pleasantness, Effort, Certainty) show that (1) adding appraisals\nduring training improves the accurateness of the generated texts by 10 pp in\nF1. Further, (2) the texts with appraisal variables are longer and contain more\ndetails. This exemplifies the greater control for users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Resendiz_Y/0/1/0/all/0/1\">Yarik Menchaca Resendiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised extraction of local and global keywords from a single text. (arXiv:2307.14005v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14005","description":"<p>We propose an unsupervised, corpus-independent method to extract keywords\nfrom a single text. It is based on the spatial distribution of words and the\nresponse of this distribution to a random permutation of words. As compared to\nexisting methods (such as e.g. YAKE) our method has three advantages. First, it\nis significantly more effective at extracting keywords from long texts. Second,\nit allows inference of two types of keywords: local and global. Third, it\nuncovers basic themes in texts. Additionally, our method is\nlanguage-independent and applies to short texts. The results are obtained via\nhuman annotators with previous knowledge of texts from our database of\nclassical literary works (the agreement between annotators is from moderate to\nsubstantial). Our results are supported via human-independent arguments based\non the average length of extracted content words and on the average number of\nnouns in extracted words. We discuss relations of keywords with higher-order\ntextual features and reveal a connection between keywords and chapter\ndivisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aleksanyan_L/0/1/0/all/0/1\">Lida Aleksanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allahverdyan_A/0/1/0/all/0/1\">Armen E. Allahverdyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi3WOZ: A Multilingual, Multi-Domain, Multi-Parallel Dataset for Training and Evaluating Culturally Adapted Task-Oriented Dialog Systems. (arXiv:2307.14031v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14031","description":"<p>Creating high-quality annotated data for task-oriented dialog (ToD) is known\nto be notoriously difficult, and the challenges are amplified when the goal is\nto create equitable, culturally adapted, and large-scale ToD datasets for\nmultiple languages. Therefore, the current datasets are still very scarce and\nsuffer from limitations such as translation-based non-native dialogs with\ntranslation artefacts, small scale, or lack of cultural adaptation, among\nothers. In this work, we first take stock of the current landscape of\nmultilingual ToD datasets, offering a systematic overview of their properties\nand limitations. Aiming to reduce all the detected limitations, we then\nintroduce Multi3WOZ, a novel multilingual, multi-domain, multi-parallel ToD\ndataset. It is large-scale and offers culturally adapted dialogs in 4 languages\nto enable training and evaluation of multilingual and cross-lingual ToD\nsystems. We describe a complex bottom-up data collection process that yielded\nthe final dataset, and offer the first sets of baseline scores across different\nToD-related tasks for future reference, also highlighting its challenging\nnature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Han Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hergul_M/0/1/0/all/0/1\">Mete Hergul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritta_M/0/1/0/all/0/1\">Milan Gritta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guchun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iacobacci_I/0/1/0/all/0/1\">Ignacio Iacobacci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoding ChatGPT: A Taxonomy of Existing Research, Current Challenges, and Possible Future Directions. (arXiv:2307.14107v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14107","description":"<p>Chat Generative Pre-trained Transformer (ChatGPT) has gained significant\ninterest and attention since its launch in November 2022. It has shown\nimpressive performance in various domains, including passing exams and creative\nwriting. However, challenges and concerns related to biases and trust persist.\nIn this work, we present a comprehensive review of over 100 Scopus-indexed\npublications on ChatGPT, aiming to provide a taxonomy of ChatGPT research and\nexplore its applications. We critically analyze the existing literature,\nidentifying common approaches employed in the studies. Additionally, we\ninvestigate diverse application areas where ChatGPT has found utility, such as\nhealthcare, marketing and financial services, software engineering, academic\nand scientific writing, research and education, environmental science, and\nnatural language processing. Through examining these applications, we gain\nvaluable insights into the potential of ChatGPT in addressing real-world\nchallenges. We also discuss crucial issues related to ChatGPT, including biases\nand trustworthiness, emphasizing the need for further research and development\nin these areas. Furthermore, we identify potential future directions for\nChatGPT research, proposing solutions to current challenges and speculating on\nexpected advancements. By fully leveraging the capabilities of ChatGPT, we can\nunlock its potential across various domains, leading to advancements in\nconversational AI and transformative impacts in society.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sohail_S/0/1/0/all/0/1\">Shahab Saquib Sohail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhat_F/0/1/0/all/0/1\">Faiza Farhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Himeur_Y/0/1/0/all/0/1\">Yassine Himeur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeem_M/0/1/0/all/0/1\">Mohammad Nadeem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madsen_D/0/1/0/all/0/1\">Dag &#xd8;ivind Madsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_Y/0/1/0/all/0/1\">Yashbir Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atalla_S/0/1/0/all/0/1\">Shadi Atalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansoor_W/0/1/0/all/0/1\">Wathiq Mansoor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Implicit Feedback from Deployment Data in Dialogue. (arXiv:2307.14117v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14117","description":"<p>We study improving social conversational agents by learning from natural\ndialogue between users and a deployed model, without extra annotations. To\nimplicitly measure the quality of a machine-generated utterance, we leverage\nsignals like user response length, sentiment and reaction of the future human\nutterances in the collected dialogue episodes. Our experiments use the publicly\nreleased deployment data from BlenderBot (Xu et al., 2023). Human evaluation\nindicates improvements in our new models over baseline responses; however, we\nfind that some proxy signals can lead to more generations with undesirable\nproperties as well. For example, optimizing for conversation length can lead to\nmore controversial or unfriendly generations compared to the baseline, whereas\noptimizing for positive sentiment or reaction can decrease these behaviors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1\">Richard Yuanzhe Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1\">Stephen Roller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Say Goodbye to RNN-T Loss: A Novel CIF-based Transducer Architecture for Automatic Speech Recognition. (arXiv:2307.14132v1 [cs.SD])","link":"http://arxiv.org/abs/2307.14132","description":"<p>RNN-T models are widely used in ASR, which rely on the RNN-T loss to achieve\nlength alignment between input audio and target sequence. However, the\nimplementation complexity and the alignment-based optimization target of RNN-T\nloss lead to computational redundancy and a reduced role for predictor network,\nrespectively. In this paper, we propose a novel model named CIF-Transducer\n(CIF-T) which incorporates the Continuous Integrate-and-Fire (CIF) mechanism\nwith the RNN-T model to achieve efficient alignment. In this way, the RNN-T\nloss is abandoned, thus bringing a computational reduction and allowing the\npredictor network a more significant role. We also introduce Funnel-CIF,\nContext Blocks, Unified Gating and Bilinear Pooling joint network, and\nauxiliary training strategy to further improve performance. Experiments on the\n178-hour AISHELL-1 and 10000-hour WenetSpeech datasets show that CIF-T achieves\nstate-of-the-art results with lower computational overhead compared to RNN-T\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tian-Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dinghao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhon_G/0/1/0/all/0/1\">Guiping Zhon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baoxiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Developing and Evaluating Tiny to Medium-Sized Turkish BERT Models. (arXiv:2307.14134v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14134","description":"<p>This study introduces and evaluates tiny, mini, small, and medium-sized\nuncased Turkish BERT models, aiming to bridge the research gap in\nless-resourced languages. We trained these models on a diverse dataset\nencompassing over 75GB of text from multiple sources and tested them on several\ntasks, including mask prediction, sentiment analysis, news classification, and,\nzero-shot classification. Despite their smaller size, our models exhibited\nrobust performance, including zero-shot task, while ensuring computational\nefficiency and faster execution times. Our findings provide valuable insights\ninto the development and application of smaller language models, especially in\nthe context of the Turkish language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kesgin_H/0/1/0/all/0/1\">Himmet Toprak Kesgin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuce_M/0/1/0/all/0/1\">Muzaffer Kaan Yuce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amasyali_M/0/1/0/all/0/1\">Mehmet Fatih Amasyali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LOIS: Looking Out of Instance Semantics for Visual Question Answering. (arXiv:2307.14142v1 [cs.CV])","link":"http://arxiv.org/abs/2307.14142","description":"<p>Visual question answering (VQA) has been intensively studied as a multimodal\ntask that requires effort in bridging vision and language to infer answers\ncorrectly. Recent attempts have developed various attention-based modules for\nsolving VQA tasks. However, the performance of model inference is largely\nbottlenecked by visual processing for semantics understanding. Most existing\ndetection methods rely on bounding boxes, remaining a serious challenge for VQA\nmodels to understand the causal nexus of object semantics in images and\ncorrectly infer contextual information. To this end, we propose a finer model\nframework without bounding boxes in this work, termed Looking Out of Instance\nSemantics (LOIS) to tackle this important issue. LOIS enables more fine-grained\nfeature descriptions to produce visual facts. Furthermore, to overcome the\nlabel ambiguity caused by instance masks, two types of relation attention\nmodules: 1) intra-modality and 2) inter-modality, are devised to infer the\ncorrect answers from the different multi-view features. Specifically, we\nimplement a mutual relation attention module to model sophisticated and deeper\nvisual semantic relations between instance objects and background information.\nIn addition, our proposed attention model can further analyze salient image\nregions by focusing on important word-related questions. Experimental results\non four benchmark VQA datasets prove that our proposed method has favorable\nperformance in improving visual reasoning capability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Siyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yeming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yaoru Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haibo Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoran Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UnScientify: Detecting Scientific Uncertainty in Scholarly Full Text. (arXiv:2307.14236v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14236","description":"<p>This demo paper presents UnScientify, an interactive system designed to\ndetect scientific uncertainty in scholarly full text. The system utilizes a\nweakly supervised technique that employs a fine-grained annotation scheme to\nidentify verbally formulated uncertainty at the sentence level in scientific\ntexts. The pipeline for the system includes a combination of pattern matching,\ncomplex sentence checking, and authorial reference checking. Our approach\nautomates labeling and annotation tasks for scientific uncertainty\nidentification, taking into account different types of scientific uncertainty,\nthat can serve various applications such as information retrieval, text mining,\nand scholarly document processing. Additionally, UnScientify provides\ninterpretable results, aiding in the comprehension of identified instances of\nscientific uncertainty in text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ningrum_P/0/1/0/all/0/1\">Panggih Kusuma Ningrum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayr_P/0/1/0/all/0/1\">Philipp Mayr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanassova_I/0/1/0/all/0/1\">Iana Atanassova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Founding a mathematical diffusion model in linguistics. The case study of German syntactic features in the North-Eastern Italian dialects. (arXiv:2307.14291v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14291","description":"<p>We take as a case study the spread of Germanic syntactic features into\nRomance dialects of North-Eastern Italy, which occurred after the immigration\nof German people in the Tyrol during the High Middle Ages.\n</p>\n<p>An interactive map is produced using tools of what is called Geographic Data\nScience. A smooth two-dimensional surface $\\mathcal{G}$ expresses locally which\nfraction of territory uses a given German language feature: it is obtained by\ninterpolating a discrete function that says if at any surveyed locality that\nfeature is used or not.\\newline\n</p>\n<p>This surface $\\mathcal{G}$ is thought of as the value at the present time of\na function describing a diffusion-convection phenomenon in two dimensions (here\nsaid \\emph{tidal} mode), which is subjected in a very natural way to the same\nequation, suitably contextualized, used in physics for a number of\nphenomenological facts like the heat diffusion. It is shown that solutions of\nthis equation, evaluated at the present time, fit well with the data as\ninterpolated by $\\mathcal{G}$, thus providing convincing pictures of\ndiffusion-convection of the linguistic features of the case study, albeit\nsimplifications and approximations.\\newline\n</p>\n<p>Very importantly, it is shown that Schmidt's 'waves' can be counted among the\nsolutions of the diffusion equation: superimposing Schmidt 'waves' to a 'tidal\nflooding' can reproduce complexities of real linguistic diffusion events.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lazzizzera_I/0/1/0/all/0/1\">I. Lazzizzera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT and Persuasive Technologies for the Management and Delivery of Personalized Recommendations in Hotel Hospitality. (arXiv:2307.14298v1 [cs.IR])","link":"http://arxiv.org/abs/2307.14298","description":"<p>Recommender systems have become indispensable tools in the hotel hospitality\nindustry, enabling personalized and tailored experiences for guests. Recent\nadvancements in large language models (LLMs), such as ChatGPT, and persuasive\ntechnologies, have opened new avenues for enhancing the effectiveness of those\nsystems. This paper explores the potential of integrating ChatGPT and\npersuasive technologies for automating and improving hotel hospitality\nrecommender systems. First, we delve into the capabilities of ChatGPT, which\ncan understand and generate human-like text, enabling more accurate and\ncontext-aware recommendations. We discuss the integration of ChatGPT into\nrecommender systems, highlighting the ability to analyze user preferences,\nextract valuable insights from online reviews, and generate personalized\nrecommendations based on guest profiles. Second, we investigate the role of\npersuasive technology in influencing user behavior and enhancing the persuasive\nimpact of hotel recommendations. By incorporating persuasive techniques, such\nas social proof, scarcity and personalization, recommender systems can\neffectively influence user decision-making and encourage desired actions, such\nas booking a specific hotel or upgrading their room. To investigate the\nefficacy of ChatGPT and persuasive technologies, we present a pilot experi-ment\nwith a case study involving a hotel recommender system. We aim to study the\nimpact of integrating ChatGPT and persua-sive techniques on user engagement,\nsatisfaction, and conversion rates. The preliminary results demonstrate the\npotential of these technologies in enhancing the overall guest experience and\nbusiness performance. Overall, this paper contributes to the field of hotel\nhospitality by exploring the synergistic relationship between LLMs and\npersuasive technology in recommender systems, ultimately influencing guest\nsatisfaction and hotel revenue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Remountakis_M/0/1/0/all/0/1\">Manolis Remountakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotis_K/0/1/0/all/0/1\">Konstantinos Kotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kourtzis_B/0/1/0/all/0/1\">Babis Kourtzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsekouras_G/0/1/0/all/0/1\">George E. Tsekouras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatically Evaluating Opinion Prevalence in Opinion Summarization. (arXiv:2307.14305v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14305","description":"<p>When faced with a large number of product reviews, it is not clear that a\nhuman can remember all of them and weight opinions representatively to write a\ngood reference summary. We propose an automatic metric to test the prevalence\nof the opinions that a summary expresses, based on counting the number of\nreviews that are consistent with each statement in the summary, while\ndiscrediting trivial or redundant statements. To formulate this opinion\nprevalence metric, we consider several existing methods to score the factual\nconsistency of a summary statement with respect to each individual source\nreview. On a corpus of Amazon product reviews, we gather multiple human\njudgments of the opinion consistency, to determine which automatic metric best\nexpresses consistency in product reviews. Using the resulting opinion\nprevalence metric, we show that a human authored summary has only slightly\nbetter opinion prevalence than randomly selected extracts from the source\nreviews, and previous extractive and abstractive unsupervised opinion\nsummarization methods perform worse than humans. We demonstrate room for\nimprovement with a greedy construction of extractive summaries with twice the\nopinion prevalence achieved by humans. Finally, we show that preprocessing\nsource reviews by simplification can raise the opinion prevalence achieved by\nexisting abstractive opinion summarization systems to the level of human\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malon_C/0/1/0/all/0/1\">Christopher Malon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparative Analysis of Libraries for the Sentimental Analysis. (arXiv:2307.14311v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14311","description":"<p>This study is main goal is to provide a comparative comparison of libraries\nusing machine learning methods. Experts in natural language processing (NLP)\nare becoming more and more interested in sentiment analysis (SA) of text\nchanges. The objective of employing NLP text analysis techniques is to\nrecognize and categorize feelings related to twitter users utterances. In this\nexamination, issues with SA and the libraries utilized are also looked at.\nprovides a number of cooperative methods to classify emotional polarity. The\nNaive Bayes Classifier, Decision Tree Classifier, Maxent Classifier, Sklearn\nClassifier, Sklearn Classifier MultinomialNB, and other conjoint learning\nalgorithms, according to recent research, are very effective. In the project\nwill use Five Python and R libraries NLTK, TextBlob, Vader, Transformers (GPT\nand BERT pretrained), and Tidytext will be used in the study to apply sentiment\nanalysis techniques. Four machine learning models Tree of Decisions (DT),\nSupport Vector Machine (SVM), Naive Bayes (NB), and K-Nearest Neighbor (KNN)\nwill also be used. To evaluate how well libraries for SA operate in the social\nnetwork environment, comparative study was also carried out. The measures to\nassess the best algorithms in this experiment, which used a single data set for\neach method, were precision, recall, and F1 score. We conclude that the BERT\ntransformer method with an Accuracy: 0.973 is recommended for sentiment\nanalysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ccoya_W/0/1/0/all/0/1\">Wendy Ccoya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_E/0/1/0/all/0/1\">Edson Pinto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Moral Beliefs Encoded in LLMs. (arXiv:2307.14324v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14324","description":"<p>This paper presents a case study on the design, administration,\npost-processing, and evaluation of surveys on large language models (LLMs). It\ncomprises two components: (1) A statistical method for eliciting beliefs\nencoded in LLMs. We introduce statistical measures and evaluation metrics that\nquantify the probability of an LLM \"making a choice\", the associated\nuncertainty, and the consistency of that choice. (2) We apply this method to\nstudy what moral beliefs are encoded in different LLMs, especially in ambiguous\ncases where the right choice is not obvious. We design a large-scale survey\ncomprising 680 high-ambiguity moral scenarios (e.g., \"Should I tell a white\nlie?\") and 687 low-ambiguity moral scenarios (e.g., \"Should I stop for a\npedestrian on the road?\"). Each scenario includes a description, two possible\nactions, and auxiliary labels indicating violated rules (e.g., \"do not kill\").\nWe administer the survey to 28 open- and closed-source LLMs. We find that (a)\nin unambiguous scenarios, most models \"choose\" actions that align with\ncommonsense. In ambiguous cases, most models express uncertainty. (b) Some\nmodels are uncertain about choosing the commonsense action because their\nresponses are sensitive to the question-wording. (c) Some models reflect clear\npreferences in ambiguous scenarios. Specifically, closed-source models tend to\nagree with each other.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scherrer_N/0/1/0/all/0/1\">Nino Scherrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Claudia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1\">Amir Feder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blei_D/0/1/0/all/0/1\">David M. Blei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Generalist Biomedical AI. (arXiv:2307.14334v1 [cs.CL])","link":"http://arxiv.org/abs/2307.14334","description":"<p>Medicine is inherently multimodal, with rich data modalities spanning text,\nimaging, genomics, and more. Generalist biomedical artificial intelligence (AI)\nsystems that flexibly encode, integrate, and interpret this data at scale can\npotentially enable impactful applications ranging from scientific discovery to\ncare delivery. To enable the development of these models, we first curate\nMultiMedBench, a new multimodal biomedical benchmark. MultiMedBench encompasses\n14 diverse tasks such as medical question answering, mammography and\ndermatology image interpretation, radiology report generation and\nsummarization, and genomic variant calling. We then introduce Med-PaLM\nMultimodal (Med-PaLM M), our proof of concept for a generalist biomedical AI\nsystem. Med-PaLM M is a large multimodal generative model that flexibly encodes\nand interprets biomedical data including clinical language, imaging, and\ngenomics with the same set of model weights. Med-PaLM M reaches performance\ncompetitive with or exceeding the state of the art on all MultiMedBench tasks,\noften surpassing specialist models by a wide margin. We also report examples of\nzero-shot generalization to novel medical concepts and tasks, positive transfer\nlearning across tasks, and emergent zero-shot medical reasoning. To further\nprobe the capabilities and limitations of Med-PaLM M, we conduct a radiologist\nevaluation of model-generated (and human) chest X-ray reports and observe\nencouraging performance across model scales. In a side-by-side ranking on 246\nretrospective chest X-rays, clinicians express a pairwise preference for\nMed-PaLM M reports over those produced by radiologists in up to 40.50% of\ncases, suggesting potential clinical utility. While considerable work is needed\nto validate these models in real-world use cases, our results represent a\nmilestone towards the development of generalist biomedical AI systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_T/0/1/0/all/0/1\">Tao Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azizi_S/0/1/0/all/0/1\">Shekoofeh Azizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driess_D/0/1/0/all/0/1\">Danny Driess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaekermann_M/0/1/0/all/0/1\">Mike Schaekermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1\">Mohamed Amin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_P/0/1/0/all/0/1\">Pi-Chuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carroll_A/0/1/0/all/0/1\">Andrew Carroll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_C/0/1/0/all/0/1\">Chuck Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanno_R/0/1/0/all/0/1\">Ryutaro Tanno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ktena_I/0/1/0/all/0/1\">Ira Ktena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1\">Basil Mustafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhery_A/0/1/0/all/0/1\">Aakanksha Chowdhery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1\">Simon Kornblith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1\">David Fleet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansfield_P/0/1/0/all/0/1\">Philip Mansfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1\">Sushant Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_R/0/1/0/all/0/1\">Renee Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Virmani_S/0/1/0/all/0/1\">Sunny Virmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semturs_C/0/1/0/all/0/1\">Christopher Semturs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_S/0/1/0/all/0/1\">S Sara Mahdavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_B/0/1/0/all/0/1\">Bradley Green</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dominowska_E/0/1/0/all/0/1\">Ewa Dominowska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arcas_B/0/1/0/all/0/1\">Blaise Aguera y Arcas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barral_J/0/1/0/all/0/1\">Joelle Barral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webster_D/0/1/0/all/0/1\">Dale Webster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corrado_G/0/1/0/all/0/1\">Greg S. Corrado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matias_Y/0/1/0/all/0/1\">Yossi Matias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_K/0/1/0/all/0/1\">Karan Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1\">Pete Florence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karthikesalingam_A/0/1/0/all/0/1\">Alan Karthikesalingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_V/0/1/0/all/0/1\">Vivek Natarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Comparison of Pre-training Language Models. (arXiv:2106.11483v9 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.11483","description":"<p>Recently, the development of pre-trained language models has brought natural\nlanguage processing (NLP) tasks to the new state-of-the-art. In this paper we\nexplore the efficiency of various pre-trained language models. We pre-train a\nlist of transformer-based models with the same amount of text and the same\ntraining steps. The experimental results shows that the most improvement upon\nthe origin BERT is adding the RNN-layer to capture more contextual information\nfor short text understanding. But the conclusion is: There are no remarkable\nimprovement for short text understanding for similar BERT structures.\nData-centric method[12] can achieve better performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Multi-Modal Representations for Ambiguity Detection & Coreference Resolution in the SIMMC 2.0 Challenge. (arXiv:2202.12645v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.12645","description":"<p>Anaphoric expressions, such as pronouns and referential descriptions, are\nsituated with respect to the linguistic context of prior turns, as well as, the\nimmediate visual environment. However, a speaker's referential descriptions do\nnot always uniquely identify the referent, leading to ambiguities in need of\nresolution through subsequent clarificational exchanges. Thus, effective\nAmbiguity Detection and Coreference Resolution are key to task success in\nConversational AI. In this paper, we present models for these two tasks as part\nof the SIMMC 2.0 Challenge (Kottur et al. 2021). Specifically, we use TOD-BERT\nand LXMERT based models, compare them to a number of baselines and provide\nablation experiments. Our results show that (1) language models are able to\nexploit correlations in the data to detect ambiguity; and (2) unimodal\ncoreference resolution models can avoid the need for a vision component,\nthrough the use of smart object representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiyah_Garcia_J/0/1/0/all/0/1\">Javier Chiyah-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suglia_A/0/1/0/all/0/1\">Alessandro Suglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopes_J/0/1/0/all/0/1\">Jos&#xe9; Lopes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eshghi_A/0/1/0/all/0/1\">Arash Eshghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hastie_H/0/1/0/all/0/1\">Helen Hastie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Direct Speech Translation for Automatic Subtitling. (arXiv:2209.13192v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.13192","description":"<p>Automatic subtitling is the task of automatically translating the speech of\naudiovisual content into short pieces of timed text, i.e. subtitles and their\ncorresponding timestamps. The generated subtitles need to conform to space and\ntime requirements, while being synchronised with the speech and segmented in a\nway that facilitates comprehension. Given its considerable complexity, the task\nhas so far been addressed through a pipeline of components that separately deal\nwith transcribing, translating, and segmenting text into subtitles, as well as\npredicting timestamps. In this paper, we propose the first direct ST model for\nautomatic subtitling that generates subtitles in the target language along with\ntheir timestamps with a single model. Our experiments on 7 language pairs show\nthat our approach outperforms a cascade system in the same data condition, also\nbeing competitive with production tools on both in-domain and newly-released\nout-domain benchmarks covering new scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karakanta_A/0/1/0/all/0/1\">Alina Karakanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cettolo_M/0/1/0/all/0/1\">Mauro Cettolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.12247","description":"<p>The recent explosion of interest in multimodal applications has resulted in a\nwide selection of datasets and methods for representing and integrating\ninformation from different modalities. Despite these empirical advances, there\nremain fundamental research questions: How can we quantify the interactions\nthat are necessary to solve a multimodal task? Subsequently, what are the most\nsuitable multimodal models to capture these interactions? To answer these\nquestions, we propose an information-theoretic approach to quantify the degree\nof redundancy, uniqueness, and synergy relating input modalities with an output\ntask. We term these three measures as the PID statistics of a multimodal\ndistribution (or PID for short), and introduce two new estimators for these PID\nstatistics that scale to high-dimensional distributions. To validate PID\nestimation, we conduct extensive experiments on both synthetic datasets where\nthe PID is known and on large-scale multimodal benchmarks where PID estimations\nare compared with human annotations. Finally, we demonstrate their usefulness\nin (1) quantifying interactions within multimodal datasets, (2) quantifying\ninteractions captured by multimodal models, (3) principled approaches for model\nselection, and (4) three real-world case studies engaging with domain experts\nin pathology, mood prediction, and robotic perception where our framework helps\nto recommend strong multimodal models for each application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1\">Chun Kai Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_S/0/1/0/all/0/1\">Suzanne Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Richard Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zihao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allen_N/0/1/0/all/0/1\">Nicholas Allen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auerbach_R/0/1/0/all/0/1\">Randy Auerbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_F/0/1/0/all/0/1\">Faisal Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale. (arXiv:2306.00017v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.00017","description":"<p>Large language models (LLMs) have achieved a milestone that undenia-bly\nchanged many held beliefs in artificial intelligence (AI). However, there\nremains many limitations of these LLMs when it comes to true language\nunderstanding, limitations that are a byproduct of the under-lying architecture\nof deep neural networks. Moreover, and due to their subsymbolic nature,\nwhatever knowledge these models acquire about how language works will always be\nburied in billions of microfeatures (weights), none of which is meaningful on\nits own, making such models hopelessly unexplainable. To address these\nlimitations, we suggest com-bining the strength of symbolic representations\nwith what we believe to be the key to the success of LLMs, namely a successful\nbottom-up re-verse engineering of language at scale. As such we argue for a\nbottom-up reverse engineering of language in a symbolic setting. Hints on what\nthis project amounts to have been suggested by several authors, and we discuss\nin some detail here how this project could be accomplished.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saba_W/0/1/0/all/0/1\">Walid S. Saba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Table and Image Generation for Investigating Knowledge of Entities in Pre-trained Vision and Language Models. (arXiv:2306.02115v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.02115","description":"<p>In this paper, we propose a table and image generation task to verify how the\nknowledge about entities acquired from natural language is retained in Vision &amp;\nLanguage (V&amp;L) models. This task consists of two parts: the first is to\ngenerate a table containing knowledge about an entity and its related image,\nand the second is to generate an image from an entity with a caption and a\ntable containing related knowledge of the entity. In both tasks, the model must\nknow the entities used to perform the generation properly. We created the\nWikipedia Table and Image Generation (WikiTIG) dataset from about 200,000\ninfoboxes in English Wikipedia articles to perform the proposed tasks. We\nevaluated the performance on the tasks with respect to the above research\nquestion using the V&amp;L model OFA, which has achieved state-of-the-art results\nin multiple tasks. Experimental results show that OFA forgets part of its\nentity knowledge by pre-training as a complement to improve the performance of\nimage related tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamigaito_H/0/1/0/all/0/1\">Hidetaka Kamigaito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_K/0/1/0/all/0/1\">Katsuhiko Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1\">Taro Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Are you telling me to put glasses on the dog?'' Content-Grounded Annotation of Instruction Clarification Requests in the CoDraw Dataset. (arXiv:2306.02377v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.02377","description":"<p>Instruction Clarification Requests are a mechanism to solve communication\nproblems, which is very functional in instruction-following interactions.\nRecent work has argued that the CoDraw dataset is a valuable source of\nnaturally occurring iCRs. Beyond identifying when iCRs should be made, dialogue\nmodels should also be able to generate them with suitable form and content. In\nthis work, we introduce CoDraw-iCR (v2), extending the existing iCR identifiers\nwith fine-grained information grounded in the underlying dialogue game items\nand possible actions. Our annotation can serve to model and evaluate repair\ncapabilities of dialogue agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madureira_B/0/1/0/all/0/1\">Brielen Madureira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlangen_D/0/1/0/all/0/1\">David Schlangen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Derivative Free Weight-space Ensembling. (arXiv:2307.03506v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.03506","description":"<p>Recent work suggests that interpolating between the weights of two\nspecialized language models can transfer knowledge between tasks in a way that\nmulti-task learning cannot. However, very few have explored interpolation\nbetween more than two models, where each has a distinct knowledge base. In this\npaper, we introduce Derivative Free Weight-space Ensembling (DFWE), a new\nfew-sample task transfer approach for open-domain dialogue. Our framework\ncreates a set of diverse expert language models trained using a predefined set\nof source tasks. Next, we finetune each of the expert models on the target\ntask, approaching the target task from several distinct knowledge bases.\nFinally, we linearly interpolate between the model weights using a\ngradient-free-optimization algorithm, to efficiently find a good interpolation\nweighting. We demonstrate the effectiveness of the method on FETA-Friends\noutperforming the standard pretrain-finetune approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ninalga_D/0/1/0/all/0/1\">Dean Ninalga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering. (arXiv:2307.04192v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2307.04192","description":"<p>Video question--answering is a fundamental task in the field of video\nunderstanding. Although current vision--language models (VLMs) equipped with\nVideo Transformers have enabled temporal modeling and yielded superior results,\nthey are at the cost of huge computational power and thus too expensive to\ndeploy in real-time application scenarios. An economical workaround only\nsamples a small portion of frames to represent the main content of that video\nand tune an image--text model on these sampled frames. Recent video\nunderstanding models usually randomly sample a set of frames or clips,\nregardless of internal correlations between their visual contents, nor their\nrelevance to the problem. We argue that such kinds of aimless sampling may omit\nthe key frames from which the correct answer can be deduced, and the situation\ngets worse when the sampling sparsity increases, which always happens as the\nvideo lengths increase. To mitigate this issue, we propose two frame sampling\nstrategies, namely the most domain frames (MDF) and most implied frames (MIF),\nto maximally preserve those frames that are most likely vital to the given\nquestions. MDF passively minimizes the risk of key frame omission in a\nbootstrap manner, while MIS actively searches key frames customized for each\nvideo--question pair with the assistance of auxiliary models. The experimental\nresults on three public datasets from three advanced VLMs (CLIP, GIT and\nAll-in-one) demonstrate that our proposed strategies can boost the performance\nfor image--text pretrained models. The source codes pertaining to the method\nproposed in this paper are publicly available at\nhttps://github.com/declare-lab/sas-vqa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Min-Yen Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMBench: Is Your Multi-modal Model an All-around Player?. (arXiv:2307.06281v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2307.06281","description":"<p>Large vision-language models have recently achieved remarkable progress,\nexhibiting great perception and reasoning abilities concerning visual\ninformation. However, how to effectively evaluate these large vision-language\nmodels remains a major obstacle, hindering future model development.\nTraditional benchmarks like VQAv2 or COCO Caption provide quantitative\nperformance measurements but suffer from a lack of fine-grained ability\nassessment and non-robust evaluation metrics. Recent subjective benchmarks,\nsuch as OwlEval, offer comprehensive evaluations of a model's abilities by\nincorporating human labor, but they are not scalable and display significant\nbias. In response to these challenges, we propose MMBench, a novel\nmulti-modality benchmark. MMBench methodically develops a comprehensive\nevaluation pipeline, primarily comprised of two elements. The first element is\na meticulously curated dataset that surpasses existing similar benchmarks in\nterms of the number and variety of evaluation questions and abilities. The\nsecond element introduces a novel CircularEval strategy and incorporates the\nuse of ChatGPT. This implementation is designed to convert free-form\npredictions into pre-defined choices, thereby facilitating a more robust\nevaluation of the model's predictions. MMBench is a systematically-designed\nobjective benchmark for robustly evaluating the various abilities of\nvision-language models. We hope MMBench will assist the research community in\nbetter evaluating their models and encourage future advancements in this\ndomain. Project page: https://opencompass.org.cn/mmbench.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1\">Haodong Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wangbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yike Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Conghui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2307.06440","description":"<p>The computation necessary for training Transformer-based language models has\nskyrocketed in recent years. This trend has motivated research on efficient\ntraining algorithms designed to improve training, validation, and downstream\nperformance faster than standard training. In this work, we revisit three\ncategories of such algorithms: dynamic architectures (layer stacking, layer\ndropping), batch selection (selective backprop, RHO loss), and efficient\noptimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed\ncomputation budget using such methods, we find that their training, validation,\nand downstream gains vanish compared to a baseline with a fully-decayed\nlearning rate. We define an evaluation protocol that enables computation to be\ndone on arbitrary machines by mapping all computation time to a reference\nmachine which we call reference system time. We discuss the limitations of our\nproposed protocol and release our code to encourage rigorous research in\nefficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1\">Jean Kaddour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Key_O/0/1/0/all/0/1\">Oscar Key</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nawrot_P/0/1/0/all/0/1\">Piotr Nawrot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1\">Pasquale Minervini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1\">Matt J. Kusner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study. (arXiv:2307.08072v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.08072","description":"<p>Despite the superior performance, Large Language Models~(LLMs) require\nsignificant computational resources for deployment and use. To overcome this\nissue, quantization methods have been widely applied to reduce the memory\nfootprint of LLMs as well as increasing the inference rate. However, a major\nchallenge is that low-bit quantization methods often lead to performance\ndegradation. It is important to understand how quantization impacts the\ncapacity of LLMs. Different from previous studies focused on overall\nperformance, this work aims to investigate the impact of quantization on\n\\emph{emergent abilities}, which are important characteristics that distinguish\nLLMs from small language models. Specially, we examine the abilities of\nin-context learning, chain-of-thought reasoning, and instruction-following in\nquantized LLMs. Our empirical experiments show that these emergent abilities\nstill exist in 4-bit quantization models, while 2-bit models encounter severe\nperformance degradation on the test of these abilities. To improve the\nperformance of low-bit models, we conduct two special experiments: (1)\nfine-gained impact analysis that studies which components (or substructures)\nare more sensitive to quantization, and (2) performance compensation through\nmodel fine-tuning. Our work derives a series of important findings to\nunderstand the impact of quantization on emergent abilities, and sheds lights\non the possibilities of extremely low-bit quantization for LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zikang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Ze-Feng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Dawei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1\">Bolin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MediaGPT : A Large Language Model For Chinese Media. (arXiv:2307.10930v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.10930","description":"<p>Large language models (LLMs) have shown remarkable capabilities in generating\nhigh-quality text and making predictions based on large amounts of data,\nincluding the media domain. However, in practical applications, the differences\nbetween the media's use cases and the general-purpose applications of LLMs have\nbecome increasingly apparent, especially Chinese. This paper examines the\nunique characteristics of media-domain-specific LLMs compared to general LLMs,\ndesigned a diverse set of task instruction types to cater the specific\nrequirements of the domain and constructed unique datasets that are tailored to\nthe media domain. Based on these, we proposed MediaGPT, a domain-specific LLM\nfor the Chinese media domain, training by domain-specific data and experts SFT\ndata. By performing human experts evaluation and strong model evaluation on a\nvalidation set, this paper demonstrated that MediaGPT outperforms mainstream\nmodels on various Chinese media domain tasks and verifies the importance of\ndomain data and domain-defined prompt types for building an effective\ndomain-specific LLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhonghao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zijia Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1\">Bo Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Haiying Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios. (arXiv:2307.13528v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.13528","description":"<p>The emergence of generative pre-trained models has facilitated the synthesis\nof high-quality text, but it has also posed challenges in identifying factual\nerrors in the generated text. In particular: (1) A wider range of tasks now\nface an increasing risk of containing factual errors when handled by generative\nmodels. (2) Generated texts tend to be lengthy and lack a clearly defined\ngranularity for individual facts. (3) There is a scarcity of explicit evidence\navailable during the process of fact checking. With the above challenges in\nmind, in this paper, we propose FacTool, a task and domain agnostic framework\nfor detecting factual errors of texts generated by large language models (e.g.,\nChatGPT). Experiments on four different tasks (knowledge-based QA, code\ngeneration, mathematical reasoning, and scientific literature review) show the\nefficacy of the proposed method. We release the code of FacTool associated with\nChatGPT plugin interface at https://github.com/GAIR-NLP/factool .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chern_I/0/1/0/all/0/1\">I-Chun Chern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chern_S/0/1/0/all/0/1\">Steffi Chern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shiqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weizhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1\">Kehua Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chunting Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junxian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-3 Models are Few-Shot Financial Reasoners. (arXiv:2307.13617v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.13617","description":"<p>Financial analysis is an important tool for evaluating company performance.\nPractitioners work to answer financial questions to make profitable investment\ndecisions, and use advanced quantitative analyses to do so. As a result,\nFinancial Question Answering (QA) is a question answering task that requires\ndeep reasoning about numbers. Furthermore, it is unknown how well pre-trained\nlanguage models can reason in the financial domain. The current\nstate-of-the-art requires a retriever to collect relevant facts about the\nfinancial question from the text and a generator to produce a valid financial\nprogram and a final answer. However, recently large language models like GPT-3\nhave achieved state-of-the-art performance on wide variety of tasks with just a\nfew shot examples. We run several experiments with GPT-3 and find that a\nseparate retrieval model and logic engine continue to be essential components\nto achieving SOTA performance in this task, particularly due to the precise\nnature of financial questions and the complex information stored in financial\ndocuments. With this understanding, our refined prompt-engineering approach on\nGPT-3 achieves near SOTA accuracy without any fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Padua_R/0/1/0/all/0/1\">Raul Salles de Padua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qureshi_I/0/1/0/all/0/1\">Imran Qureshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karakaplan_M/0/1/0/all/0/1\">Mustafa U. Karakaplan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-07-26T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}