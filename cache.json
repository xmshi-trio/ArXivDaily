{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-02-15T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"PK-ICR: Persona-Knowledge Interactive Context Retrieval for Grounded Dialogue. (arXiv:2302.06674v1 [cs.CL])","link":"http://arxiv.org/abs/2302.06674","description":"<p>Identifying relevant Persona or Knowledge for conversational systems is a\ncritical component of grounded dialogue response generation. However, each\ngrounding has been studied in isolation with more practical multi-context tasks\nonly recently introduced. We define Persona and Knowledge Dual Context\nIdentification as the task to identify Persona and Knowledge jointly for a\ngiven dialogue, which could be of elevated importance in complex multi-context\nDialogue settings. We develop a novel grounding retrieval method that utilizes\nall contexts of dialogue simultaneously while also requiring limited training\nvia zero-shot inference due to compatibility with neural Q \\&amp; A retrieval\nmodels. We further analyze the hard-negative behavior of combining Persona and\nDialogue via our novel null-positive rank test.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1\">Minsik Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joosung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Symbolic Discovery of Optimization Algorithms. (arXiv:2302.06675v1 [cs.LG])","link":"http://arxiv.org/abs/2302.06675","description":"<p>We present a method to formulate algorithm discovery as program search, and\napply it to discover optimization algorithms for deep neural network training.\nWe leverage efficient search techniques to explore an infinite and sparse\nprogram space. To bridge the large generalization gap between proxy and target\ntasks, we also introduce program selection and simplification strategies. Our\nmethod discovers a simple and effective optimization algorithm, $\\textbf{Lion}$\n($\\textit{Evo$\\textbf{L}$ved S$\\textbf{i}$gn M$\\textbf{o}$me$\\textbf{n}$tum}$).\nIt is more memory-efficient than Adam as it only keeps track of the momentum.\nDifferent from adaptive optimizers, its update has the same magnitude for each\nparameter calculated through the sign operation. We compare Lion with widely\nused optimizers, such as Adam and Adafactor, for training a variety of models\non different tasks. On image classification, Lion boosts the accuracy of ViT by\nup to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On\nvision-language contrastive learning, we achieve 88.3% $\\textit{zero-shot}$ and\n91.1% $\\textit{fine-tuning}$ accuracy on ImageNet, surpassing the previous best\nresults by 2% and 0.1%, respectively. On diffusion models, Lion outperforms\nAdam by achieving a better FID score and reducing the training compute by up to\n2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion\nexhibits a similar or better performance compared to Adam. Our analysis of Lion\nreveals that its performance gain grows with the training batch size. It also\nrequires a smaller learning rate than Adam due to the larger norm of the update\nproduced by the sign function. Additionally, we examine the limitations of Lion\nand identify scenarios where its improvements are small or not statistically\nsignificant. The implementation of Lion is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangning Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Da Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Real_E/0/1/0/all/0/1\">Esteban Real</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaiyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1\">Hieu Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xuanyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luong_T/0/1/0/all/0/1\">Thang Luong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yifeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bag of Tricks for In-Distribution Calibration of Pretrained Transformers. (arXiv:2302.06690v1 [cs.CL])","link":"http://arxiv.org/abs/2302.06690","description":"<p>While pre-trained language models (PLMs) have become a de-facto standard\npromoting the accuracy of text classification tasks, recent studies find that\nPLMs often predict over-confidently. Although various calibration methods have\nbeen proposed, such as ensemble learning and data augmentation, most of the\nmethods have been verified in computer vision benchmarks rather than in\nPLM-based text classification tasks. In this paper, we present an empirical\nstudy on confidence calibration for PLMs, addressing three categories,\nincluding confidence penalty losses, data augmentations, and ensemble methods.\nWe find that the ensemble model overfitted to the training set shows sub-par\ncalibration performance and also observe that PLMs trained with confidence\npenalty loss have a trade-off between calibration and accuracy. Building on\nthese observations, we propose the Calibrated PLM (CALL), a combination of\ncalibration techniques. The CALL complements the drawbacks that may occur when\nutilizing a calibration method individually and boosts both classification and\ncalibration accuracy. Design choices in CALL's training procedures are\nextensively studied, and we provide a detailed analysis of how calibration\ntechniques affect the calibration performance of PLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jaeyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_D/0/1/0/all/0/1\">Dongbin Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Sungchul Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Sungbin Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guiding Pretraining in Reinforcement Learning with Large Language Models. (arXiv:2302.06692v1 [cs.LG])","link":"http://arxiv.org/abs/2302.06692","description":"<p>Reinforcement learning algorithms typically struggle in the absence of a\ndense, well-shaped reward function. Intrinsically motivated exploration methods\naddress this limitation by rewarding agents for visiting novel states or\ntransitions, but these methods offer limited benefits in large environments\nwhere most discovered novelty is irrelevant for downstream tasks. We describe a\nmethod that uses background knowledge from text corpora to shape exploration.\nThis method, called ELLM (Exploring with LLMs) rewards an agent for achieving\ngoals suggested by a language model prompted with a description of the agent's\ncurrent state. By leveraging large-scale language model pretraining, ELLM\nguides agents toward human-meaningful and plausibly useful behaviors without\nrequiring a human in the loop. We evaluate ELLM in the Crafter game environment\nand the Housekeep robotic simulator, showing that ELLM-trained agents have\nbetter coverage of common-sense behaviors during pretraining and usually match\nor improve performance on a range of downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuqing Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watkins_O/0/1/0/all/0/1\">Olivia Watkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colas_C/0/1/0/all/0/1\">C&#xe9;dric Colas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhishek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Planning Abilities of Large Language Models (A Critical Investigation with a Proposed Benchmark). (arXiv:2302.06706v1 [cs.AI])","link":"http://arxiv.org/abs/2302.06706","description":"<p>Intrigued by the claims of emergent reasoning capabilities in LLMs trained on\ngeneral web corpora, in this paper, we set out to investigate their planning\ncapabilities. We aim to evaluate (1) how good LLMs are by themselves in\ngenerating and validating simple plans in commonsense planning tasks (of the\ntype that humans are generally quite good at) and (2) how good LLMs are in\nbeing a source of heuristic guidance for other agents--either AI planners or\nhuman planners--in their planning tasks. To investigate these questions in a\nsystematic rather than anecdotal manner, we start by developing a benchmark\nsuite based on the kinds of domains employed in the International Planning\nCompetition. On this benchmark, we evaluate LLMs in three modes: autonomous,\nheuristic and human-in-the-loop. Our results show that LLM's ability to\nautonomously generate executable plans is quite meager, averaging only about 3%\nsuccess rate. The heuristic and human-in-the-loop modes show slightly more\npromise. In addition to these results, we also make our benchmark and\nevaluation tools available to support investigations by research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valmeekam_K/0/1/0/all/0/1\">Karthik Valmeekam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sreedharan_S/0/1/0/all/0/1\">Sarath Sreedharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marquez_M/0/1/0/all/0/1\">Matthew Marquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olmo_A/0/1/0/all/0/1\">Alberto Olmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kambhampati_S/0/1/0/all/0/1\">Subbarao Kambhampati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning Model Attribution Challenge. (arXiv:2302.06716v1 [cs.LG])","link":"http://arxiv.org/abs/2302.06716","description":"<p>We present the findings of the Machine Learning Model Attribution Challenge\n(\\href{https://mlmac.io}{https://mlmac.io}). Fine-tuned machine learning models\nmay derive from other trained models without obvious attribution\ncharacteristics. In this challenge, participants identify the\npublicly-available base models that underlie a set of anonymous, fine-tuned\nlarge language models (LLMs) using only textual output of the models.\nContestants aim to correctly attribute the most fine-tuned models, with ties\nbroken in the favor of contestants whose solutions use fewer calls to the\nfine-tuned models' API. The most successful approaches were manual, as\nparticipants observed similarities between model outputs and developed\nattribution heuristics based on public documentation of the base models, though\nseveral teams also submitted automated, statistical solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Merkhofe_E/0/1/0/all/0/1\">Elizabeth Merkhofe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhari_D/0/1/0/all/0/1\">Deepesh Chaudhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_H/0/1/0/all/0/1\">Hyrum S. Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manville_K/0/1/0/all/0/1\">Keith Manville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_L/0/1/0/all/0/1\">Lily Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gante_J/0/1/0/all/0/1\">Jo&#xe3;o Gante</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STREET: A Multi-Task Structured Reasoning and Explanation Benchmark. (arXiv:2302.06729v1 [cs.CL])","link":"http://arxiv.org/abs/2302.06729","description":"<p>We introduce STREET, a unified multi-task and multi-domain natural language\nreasoning and explanation benchmark. Unlike most existing question-answering\n(QA) datasets, we expect models to not only answer questions, but also produce\nstep-by-step structured explanations describing how premises in the question\nare used to produce intermediate conclusions that can prove the correctness of\na certain answer. We perform extensive evaluation with popular language models\nsuch as few-shot prompting GPT-3 and fine-tuned T5. We find that these models\nstill lag behind human performance when producing such structured reasoning\nsteps. We believe this work will provide a way for the community to better\ntrain and test systems on multi-step reasoning and explanations in natural\nlanguage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_D/0/1/0/all/0/1\">Danilo Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaofei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Henry Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Rui Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Deguang Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burger_J/0/1/0/all/0/1\">Juliette Burger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramos_A/0/1/0/all/0/1\">Anjelica Ramos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1\">George Karypis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Model Analysis for Ontology Subsumption Inference. (arXiv:2302.06761v1 [cs.CL])","link":"http://arxiv.org/abs/2302.06761","description":"<p>Pre-trained language models (LMs) have made significant advances in various\nNatural Language Processing (NLP) domains, but it is unclear to what extent\nthey can infer formal semantics in ontologies, which are often used to\nrepresent conceptual knowledge and serve as the schema of data graphs. To\ninvestigate an LM's knowledge of ontologies, we propose OntoLAMA, a set of\ninference-based probing tasks and datasets from ontology subsumption axioms\ninvolving both atomic and complex concepts. We conduct extensive experiments on\nontologies of different domains and scales, and our results demonstrate that\nLMs encode relatively less background knowledge of Subsumption Inference (SI)\nthan traditional Natural Language Inference (NLI) but can improve on SI\nsignificantly when a small number of samples are given. We will open-source our\ncode and datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jimenez_Ruiz_E/0/1/0/all/0/1\">Ernesto Jim&#xe9;nez-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horrocks_I/0/1/0/all/0/1\">Ian Horrocks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Stable Entropy Hypothesis and Entropy-Aware Decoding: An Analysis and Algorithm for Robust Natural Language Generation. (arXiv:2302.06784v1 [cs.CL])","link":"http://arxiv.org/abs/2302.06784","description":"<p>State-of-the-art language generation models can degenerate when applied to\nopen-ended generation problems such as text completion, story generation, or\ndialog modeling. This degeneration usually shows up in the form of incoherence,\nlack of vocabulary diversity, and self-repetition or copying from the context.\nIn this paper, we postulate that ``human-like'' generations usually lie in a\nnarrow and nearly flat entropy band, and violation of these entropy bounds\ncorrelates with degenerate behavior. Our experiments show that this stable\nnarrow entropy zone exists across models, tasks, and domains and confirm the\nhypothesis that violations of this zone correlate with degeneration. We then\nuse this insight to propose an entropy-aware decoding algorithm that respects\nthese entropy bounds resulting in less degenerate, more contextual, and\n\"human-like\" language generation in open-ended text generation settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_K/0/1/0/all/0/1\">Kushal Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODonnell_T/0/1/0/all/0/1\">Timothy J. O&#x27;Donnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jackie C.K.Cheung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Role of Semantic Parsing in Understanding Procedural Text. (arXiv:2302.06829v1 [cs.CL])","link":"http://arxiv.org/abs/2302.06829","description":"<p>In this paper, we investigate whether symbolic semantic representations,\nextracted from deep semantic parsers, can help reasoning over the states of\ninvolved entities in a procedural text. We consider a deep semantic\nparser~(TRIPS) and semantic role labeling as two sources of semantic parsing\nknowledge. First, we propose PROPOLIS, a symbolic parsing-based procedural\nreasoning framework. Second, we integrate semantic parsing information into\nstate-of-the-art neural models to conduct procedural reasoning. Our experiments\nindicate that explicitly incorporating such semantic knowledge improves\nprocedural understanding. This paper presents new metrics for evaluating\nprocedural reasoning tasks that clarify the challenges and identify differences\namong neural, symbolic, and integrated models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faghihi_H/0/1/0/all/0/1\">Hossein Rajaby Faghihi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kordjamshidi_P/0/1/0/all/0/1\">Parisa Kordjamshidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_C/0/1/0/all/0/1\">Choh Man Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allen_J/0/1/0/all/0/1\">James Allen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BLIAM: Literature-based Data Synthesis for Synergistic Drug Combination Prediction. (arXiv:2302.06860v1 [cs.CL])","link":"http://arxiv.org/abs/2302.06860","description":"<p>Language models pre-trained on scientific literature corpora have\nsubstantially advanced scientific discovery by offering high-quality feature\nrepresentations for downstream applications. However, these features are often\nnot interpretable, and thus can reveal limited insights to domain experts.\nInstead of obtaining features from language models, we propose BLIAM, a\nliterature-based data synthesis approach to directly generate training data\npoints that are interpretable and model-agnostic to downstream applications.\nThe key idea of BLIAM is to create prompts using existing training data and\nthen use these prompts to synthesize new data points. BLIAM performs these two\nsteps iteratively as new data points will define more informative prompts and\nnew prompts will in turn synthesize more accurate data points. Notably,\nliterature-based data augmentation might introduce data leakage since labels of\ntest data points in downstream applications might have already been mentioned\nin the language model corpus. To prevent such leakage, we introduce GDSC-combo,\na large-scale drug combination discovery dataset that was published after the\nbiomedical language model was trained. We found that BLIAM substantially\noutperforms a non-augmented approach and manual prompting in this rigorous data\nsplit setting. BLIAM can be further used to synthesize data points for novel\ndrugs and cell lines that were not even measured in biomedical experiments. In\naddition to the promising prediction performance, the data points synthesized\nby BLIAM are interpretable and model-agnostic, enabling in silico augmentation\nfor in vitro experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woicik_A/0/1/0/all/0/1\">Addie Woicik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains. (arXiv:2302.06868v1 [cs.CL])","link":"http://arxiv.org/abs/2302.06868","description":"<p>Prompting pre-trained language models leads to promising results across\nnatural language processing tasks but is less effective when applied in\nlow-resource domains, due to the domain gap between the pre-training data and\nthe downstream task. In this work, we bridge this gap with a novel and\nlightweight prompting methodology called SwitchPrompt for the adaptation of\nlanguage models trained on datasets from the general domain to diverse\nlow-resource domains. Using domain-specific keywords with a trainable gated\nprompt, SwitchPrompt offers domain-oriented prompting, that is, effective\nguidance on the target domains for general-domain language models. Our few-shot\nexperiments on three text classification benchmarks demonstrate the efficacy of\nthe general-domain pre-trained language models when used with SwitchPrompt.\nThey often even outperform their domain-specific counterparts trained with\nbaseline state-of-the-art prompting methods by up to 10.7% performance increase\nin accuracy. This result indicates that SwitchPrompt effectively reduces the\nneed for domain-specific language model pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goswami_K/0/1/0/all/0/1\">Koustava Goswami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lange_L/0/1/0/all/0/1\">Lukas Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araki_J/0/1/0/all/0/1\">Jun Araki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning gain differences between ChatGPT and human tutor generated algebra hints. (arXiv:2302.06871v1 [cs.CY])","link":"http://arxiv.org/abs/2302.06871","description":"<p>Large Language Models (LLMs), such as ChatGPT, are quickly advancing AI to\nthe frontiers of practical consumer use and leading industries to re-evaluate\nhow they allocate resources for content production. Authoring of open\neducational resources and hint content within adaptive tutoring systems is\nlabor intensive. Should LLMs like ChatGPT produce educational content on par\nwith human-authored content, the implications would be significant for further\nscaling of computer tutoring system approaches. In this paper, we conduct the\nfirst learning gain evaluation of ChatGPT by comparing the efficacy of its\nhints with hints authored by human tutors with 77 participants across two\nalgebra topic areas, Elementary Algebra and Intermediate Algebra. We find that\n70% of hints produced by ChatGPT passed our manual quality checks and that both\nhuman and ChatGPT conditions produced positive learning gains. However, gains\nwere only statistically significant for human tutor created hints. Learning\ngains from human-created hints were substantially and statistically\nsignificantly higher than ChatGPT hints in both topic areas, though ChatGPT\nparticipants in the Intermediate Algebra experiment were near ceiling and not\neven with the control at pre-test. We discuss the limitations of our study and\nsuggest several future directions for the field. Problem and hint content used\nin the experiment is provided for replicability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pardos_Z/0/1/0/all/0/1\">Zachary A. Pardos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandari_S/0/1/0/all/0/1\">Shreya Bhandari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot learning approaches for classifying low resource domain specific software requirements. (arXiv:2302.06951v1 [cs.CL])","link":"http://arxiv.org/abs/2302.06951","description":"<p>With the advent of strong pre-trained natural language processing models like\nBERT, DeBERTa, MiniLM, T5, the data requirement for industries to fine-tune\nthese models to their niche use cases has drastically reduced (typically to a\nfew hundred annotated samples for achieving a reasonable performance). However,\nthe availability of even a few hundred annotated samples may not always be\nguaranteed in low resource domains like automotive, which often limits the\nusage of such deep learning models in an industrial setting. In this paper we\naim to address the challenge of fine-tuning such pre-trained models with only a\nfew annotated samples, also known as Few-shot learning. Our experiments focus\non evaluating the performance of a diverse set of algorithms and methodologies\nto achieve the task of classifying BOSCH automotive domain textual software\nrequirements into 3 categories, while utilizing only 15 annotated samples per\ncategory for fine-tuning. We find that while SciBERT and DeBERTa based models\ntend to be the most accurate at 15 training samples, their performance\nimprovement scales minimally as the number of annotated samples is increased to\n50 in comparison to Siamese and T5 based models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_A/0/1/0/all/0/1\">Anmol Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timmapathini_H/0/1/0/all/0/1\">Hari Prasad Timmapathini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murali_V/0/1/0/all/0/1\">Vidhya Murali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gohad_A/0/1/0/all/0/1\">Atul Anil Gohad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Multi-source Active Learning for Natural Language Inference. (arXiv:2302.06976v1 [cs.CL])","link":"http://arxiv.org/abs/2302.06976","description":"<p>In recent years, active learning has been successfully applied to an array of\nNLP tasks. However, prior work often assumes that training and test data are\ndrawn from the same distribution. This is problematic, as in real-life settings\ndata may stem from several sources of varying relevance and quality. We show\nthat four popular active learning schemes fail to outperform random selection\nwhen applied to unlabelled pools comprised of multiple data sources on the task\nof natural language inference. We reveal that uncertainty-based strategies\nperform poorly due to the acquisition of collective outliers, i.e.,\nhard-to-learn instances that hamper learning and generalization. When outliers\nare removed, strategies are found to recover and outperform random baselines.\nIn further analysis, we find that collective outliers vary in form between\nsources, and show that hard-to-learn data is not always categorically harmful.\nLastly, we leverage dataset cartography to introduce difficulty-stratified\ntesting and find that different strategies are affected differently by example\nlearnability and difficulty.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Snijders_A/0/1/0/all/0/1\">Ard Snijders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Margatina_K/0/1/0/all/0/1\">Katerina Margatina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models. (arXiv:2302.07027v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07027","description":"<p>Pretrained language models (PLMs) are trained on massive corpora, but often\nneed to specialize to specific domains. A parameter-efficient adaptation method\nsuggests training an adapter for each domain on the task of language modeling.\nThis leads to good in-domain scores but can be impractical for domain- or\nresource-restricted settings. A solution is to use a related-domain adapter for\nthe novel domain at test time. In this paper, we introduce AdapterSoup, an\napproach that performs weight-space averaging of adapters trained on different\ndomains. Our approach is embarrassingly parallel: first, we train a set of\ndomain-specific adapters; then, for each novel domain, we determine which\nadapters should be averaged at test time. We present extensive experiments\nshowing that AdapterSoup consistently improves performance to new domains\nwithout extra training. We also explore weight averaging of adapters trained on\nthe same domain with different hyper-parameters, and show that it preserves the\nperformance of a PLM on new domains while obtaining strong in-domain results.\nWe explore various approaches for choosing which adapters to combine, such as\ntext clustering and semantic similarity. We find that using clustering leads to\nthe most competitive results on novel domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chronopoulou_A/0/1/0/all/0/1\">Alexandra Chronopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1\">Matthew E. Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodge_J/0/1/0/all/0/1\">Jesse Dodge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Summarization Data to Help Text Simplification. (arXiv:2302.07124v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07124","description":"<p>One of the major problems with text simplification is the lack of\nhigh-quality data. The sources of simplification datasets are limited to\nWikipedia and Newsela, restricting further development of this field. In this\npaper, we analyzed the similarity between text summarization and text\nsimplification and exploited summarization data to help simplify. First, we\nproposed an alignment algorithm to extract sentence pairs from summarization\ndatasets. Then, we designed four attributes to characterize the degree of\nsimplification and proposed a method to filter suitable pairs. We named these\npairs Sum4Simp (S4S). Next, we conducted human evaluations to show that S4S is\nhigh-quality and compared it with a real simplification dataset. Finally, we\nconducted experiments to illustrate that the S4S can improve the performance of\nseveral mainstream simplification models, especially in low-resource scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Renliang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhixian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Heritage Digital Twin: a bicycle made for two. The integration of digital methodologies into cultural heritage research. (arXiv:2302.07138v1 [cs.CY])","link":"http://arxiv.org/abs/2302.07138","description":"<p>The paper concerns the definition of a novel ontology for cultural heritage\nbased on the concept of digital twin. The ontology, called Heritage Digital\nTwin ontology, is a compatible extension of the well-known CIDOC CRM ISO\nstandard for cultural heritage documentation and incorporates all the different\ndocumentation systems presently in use for cultural heritage documentation. In\nthe authors' view, it supports documentation interoperability at a higher level\nthan the ones currently in use and enables effective cooperation among\ndifferent users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niccolucci_F/0/1/0/all/0/1\">Franco Niccolucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markhoff_B/0/1/0/all/0/1\">B&#xe9;atrice Markhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theodoridou_M/0/1/0/all/0/1\">Maria Theodoridou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felicetti_A/0/1/0/all/0/1\">Achille Felicetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermon_S/0/1/0/all/0/1\">Sorin Hermon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Complex Event Scenarios via Simple Entity-focused Questions. (arXiv:2302.07139v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07139","description":"<p>Event scenarios are often complex and involve multiple event sequences\nconnected through different entity participants. Exploring such complex\nscenarios requires an ability to branch through different sequences, something\nthat is difficult to achieve with standard event language modeling. To address\nthis, we propose a question-guided generation framework that models events in\ncomplex scenarios as answers to questions about participants. At any step in\nthe generation process, the framework uses the previously generated events as\ncontext, but generates the next event as an answer to one of three questions:\nwhat else a participant did, what else happened to a participant, or what else\nhappened. The participants and the questions themselves can be sampled or be\nprovided as input from a user, allowing for controllable exploration. Our\nempirical evaluation shows that this question-guided generation provides better\ncoverage of participants, diverse events within a domain, comparable\nperplexities for modeling event sequences, and more effective control for\ninteractive schema generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koupaee_M/0/1/0/all/0/1\">Mahnaz Koupaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambers_N/0/1/0/all/0/1\">Nathanael Chambers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Friendly Face: Do Text-to-Image Systems Rely on Stereotypes when the Input is Under-Specified?. (arXiv:2302.07159v1 [cs.CY])","link":"http://arxiv.org/abs/2302.07159","description":"<p>As text-to-image systems continue to grow in popularity with the general\npublic, questions have arisen about bias and diversity in the generated images.\nHere, we investigate properties of images generated in response to prompts\nwhich are visually under-specified, but contain salient social attributes\n(e.g., 'a portrait of a threatening person' versus 'a portrait of a friendly\nperson'). Grounding our work in social cognition theory, we find that in many\ncases, images contain similar demographic biases to those reported in the\nstereotype literature. However, trends are inconsistent across different models\nand further investigation is warranted.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fraser_K/0/1/0/all/0/1\">Kathleen C. Fraser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiritchenko_S/0/1/0/all/0/1\">Svetlana Kiritchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nejadgholi_I/0/1/0/all/0/1\">Isar Nejadgholi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking. (arXiv:2302.07189v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07189","description":"<p>Discovering entity mentions that are out of a Knowledge Base (KB) from texts\nplays a critical role in KB maintenance, but has not yet been fully explored.\nThe current methods are mostly limited to the simple threshold-based approach\nand feature-based classification; the datasets for evaluation are relatively\nrare. In this work, we propose BLINKout, a new BERT-based Entity Linking (EL)\nmethod which can identify mentions that do not have a corresponding KB entity\nby matching them to a special NIL entity. To this end, we integrate novel\ntechniques including NIL representation, NIL classification, and synonym\nenhancement. We also propose Ontology Pruning and Versioning strategies to\nconstruct out-of-KB mentions from normal, in-KB EL datasets. Results on four\ndatasets of clinical notes and publications show that BLINKout outperforms\nexisting methods to detect out-of-KB mentions for medical ontologies UMLS and\nSNOMED CT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horrocks_I/0/1/0/all/0/1\">Ian Horrocks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Psycholinguistic Analysis of BERT's Representations of Compounds. (arXiv:2302.07232v1 [cs.CL])","link":"http://arxiv.org/abs/2302.07232","description":"<p>This work studies the semantic representations learned by BERT for compounds,\nthat is, expressions such as sunlight or bodyguard. We build on recent studies\nthat explore semantic information in Transformers at the word level and test\nwhether BERT aligns with human semantic intuitions when dealing with\nexpressions (e.g., sunlight) whose overall meaning depends -- to a various\nextent -- on the semantics of the constituent words (sun, light). We leverage a\ndataset that includes human judgments on two psycholinguistic measures of\ncompound semantic analysis: lexeme meaning dominance (LMD; quantifying the\nweight of each constituent toward the compound meaning) and semantic\ntransparency (ST; evaluating the extent to which the compound meaning is\nrecoverable from the constituents' semantics). We show that BERT-based measures\nmoderately align with human intuitions, especially when using contextualized\nrepresentations, and that LMD is overall more predictable than ST. Contrary to\nthe results reported for 'standard' words, higher, more contextualized layers\nare the best at representing compound meaning. These findings shed new light on\nthe abilities of BERT in dealing with fine-grained semantic phenomena.\nMoreover, they can provide insights into how speakers represent compounds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Buijtelaar_L/0/1/0/all/0/1\">Lars Buijtelaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pezzelle_S/0/1/0/all/0/1\">Sandro Pezzelle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Captioning with Guidance of Multimodal Latent Topics. (arXiv:1708.09667v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1708.09667","description":"<p>The topic diversity of open-domain videos leads to various vocabularies and\nlinguistic expressions in describing video contents, and therefore, makes the\nvideo captioning task even more challenging. In this paper, we propose an\nunified caption framework, M&amp;M TGM, which mines multimodal topics in\nunsupervised fashion from data and guides the caption decoder with these\ntopics. Compared to pre-defined topics, the mined multimodal topics are more\nsemantically and visually coherent and can reflect the topic distribution of\nvideos better. We formulate the topic-aware caption generation as a multi-task\nlearning problem, in which we add a parallel task, topic prediction, in\naddition to the caption task. For the topic prediction task, we use the mined\ntopics as the teacher to train a student topic prediction model, which learns\nto predict the latent topics from multimodal contents of videos. The topic\nprediction provides intermediate supervision to the learning process. As for\nthe caption task, we propose a novel topic-aware decoder to generate more\naccurate and detailed video descriptions with the guidance from latent topics.\nThe entire learning procedure is end-to-end and it optimizes both tasks\nsimultaneously. The results from extensive experiments conducted on the MSR-VTT\nand Youtube2Text datasets demonstrate the effectiveness of our proposed model.\nM&amp;M TGM not only outperforms prior state-of-the-art methods on multiple\nevaluation metrics and on both benchmark datasets, but also achieves better\ngeneralization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jia Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauptmann_A/0/1/0/all/0/1\">Alexander Hauptmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BLiMP: The Benchmark of Linguistic Minimal Pairs for English. (arXiv:1912.00582v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1912.00582","description":"<p>We introduce The Benchmark of Linguistic Minimal Pairs (shortened to BLiMP),\na challenge set for evaluating what language models (LMs) know about major\ngrammatical phenomena in English. BLiMP consists of 67 sub-datasets, each\ncontaining 1000 minimal pairs isolating specific contrasts in syntax,\nmorphology, or semantics. The data is automatically generated according to\nexpert-crafted grammars, and aggregate human agreement with the labels is\n96.4%. We use it to evaluate n-gram, LSTM, and Transformer (GPT-2 and\nTransformer-XL) LMs. We find that state-of-the-art models identify\nmorphological contrasts reliably, but they struggle with semantic restrictions\non the distribution of quantifiers and negative polarity items and subtle\nsyntactic phenomena such as extraction islands.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Warstadt_A/0/1/0/all/0/1\">Alex Warstadt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parrish_A/0/1/0/all/0/1\">Alicia Parrish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haokun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohananey_A/0/1/0/all/0/1\">Anhad Mohananey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng-Fu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grammar-aware sentence classification on quantum computers. (arXiv:2012.03756v2 [quant-ph] UPDATED)","link":"http://arxiv.org/abs/2012.03756","description":"<p>Natural language processing (NLP) is at the forefront of great advances in\ncontemporary AI, and it is arguably one of the most challenging areas of the\nfield. At the same time, in the area of Quantum Computing (QC), with the steady\ngrowth of quantum hardware and notable improvements towards implementations of\nquantum algorithms, we are approaching an era when quantum computers perform\ntasks that cannot be done on classical computers with a reasonable amount of\nresources. This provides a new range of opportunities for AI, and for NLP\nspecifically. In this work, we work with the Categorical Distributional\nCompositional (DisCoCat) model of natural language meaning, whose underlying\nmathematical underpinnings make it amenable to quantum instantiations. Earlier\nwork on fault-tolerant quantum algorithms has already demonstrated potential\nquantum advantage for NLP, notably employing DisCoCat. In this work, we focus\non the capabilities of noisy intermediate-scale quantum (NISQ) hardware and\nperform the first implementation of an NLP task on a NISQ processor, using the\nDisCoCat framework. Sentences are instantiated as parameterised quantum\ncircuits; word-meanings are embedded in quantum states using parameterised\nquantum-circuits and the sentence's grammatical structure faithfully manifests\nas a pattern of entangling operations which compose the word-circuits into a\nsentence-circuit. The circuits' parameters are trained using a classical\noptimiser in a supervised NLP task of binary classification. Our novel QNLP\nmodel shows concrete promise for scalability as the quality of the quantum\nhardware improves in the near future and solidifies a novel branch of\nexperimental research at the intersection of QC and AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Meichanetzidis_K/0/1/0/all/0/1\">Konstantinos Meichanetzidis</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Toumi_A/0/1/0/all/0/1\">Alexis Toumi</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Felice_G/0/1/0/all/0/1\">Giovanni de Felice</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Coecke_B/0/1/0/all/0/1\">Bob Coecke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridge the Gap Between CV and NLP! An Optimization-based Textual Adversarial Attack Framework. (arXiv:2110.15317v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.15317","description":"<p>Despite recent success on various tasks, deep learning techniques still\nperform poorly on adversarial examples with small perturbations. While\noptimization-based methods for adversarial attacks are well-explored in the\nfield of computer vision, it is impractical to directly apply them in natural\nlanguage processing due to the discrete nature of the text. To address the\nproblem, we propose a unified framework to extend the existing\noptimization-based adversarial attack methods in the vision domain to craft\ntextual adversarial samples. In this framework, continuously optimized\nperturbations are added to the embedding layer and amplified in the forward\npropagation process. Then the final perturbed latent representations are\ndecoded with a masked language model head to obtain potential adversarial\nsamples. In this paper, we instantiate our framework with an attack algorithm\nnamed Textual Projected Gradient Descent (T-PGD). We find our algorithm\neffective even using proxy gradient information. Therefore, we perform the more\nchallenging transfer black-box attack and conduct comprehensive experiments to\nevaluate our attack algorithm with several models on three benchmark datasets.\nExperimental results demonstrate that our method achieves an overall better\nperformance and produces more fluent and grammatical adversarial samples\ncompared to strong baseline methods. All the code and data will be made public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lifan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Explanations and Human Understanding. (arXiv:2202.04092v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2202.04092","description":"<p>Explanations are hypothesized to improve human understanding of machine\nlearning models and achieve a variety of desirable outcomes, ranging from model\ndebugging to enhancing human decision making. However, empirical studies have\nfound mixed and even negative results. An open question, therefore, is under\nwhat conditions explanations can improve human understanding and in what way.\nUsing adapted causal diagrams, we provide a formal characterization of the\ninterplay between machine explanations and human understanding, and show how\nhuman intuitions play a central role in enabling human understanding.\nSpecifically, we identify three core concepts of interest that cover all\nexisting quantitative measures of understanding in the context of human-AI\ndecision making: task decision boundary, model decision boundary, and model\nerror. Our key result is that without assumptions about task-specific\nintuitions, explanations may potentially improve human understanding of model\ndecision boundary, but they cannot improve human understanding of task decision\nboundary or model error. To achieve complementary human-AI performance, we\narticulate possible ways on how explanations need to work with human\nintuitions. For instance, human intuitions about the relevance of features\n(e.g., education is more important than age in predicting a person's income)\ncan be critical in detecting model error. We validate the importance of human\nintuitions in shaping the outcome of machine explanations with empirical\nhuman-subject studies. Overall, our work provides a general framework along\nwith actionable implications for future algorithmic development and empirical\nexperiments of machine explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chacha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Amit Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Language Models Plagiarize?. (arXiv:2203.07618v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07618","description":"<p>Past literature has illustrated that language models (LMs) often memorize\nparts of training instances and reproduce them in natural language generation\n(NLG) processes. However, it is unclear to what extent LMs \"reuse\" a training\ncorpus. For instance, models can generate paraphrased sentences that are\ncontextually similar to training samples. In this work, therefore, we study\nthree types of plagiarism (i.e., verbatim, paraphrase, and idea) among GPT-2\ngenerated texts, in comparison to its training data, and further analyze the\nplagiarism patterns of fine-tuned LMs with domain-specific corpora which are\nextensively used in practice. Our results suggest that (1) three types of\nplagiarism widely exist in LMs beyond memorization, (2) both size and decoding\nmethods of LMs are strongly associated with the degrees of plagiarism they\nexhibit, and (3) fine-tuned LMs' plagiarism patterns vary based on their corpus\nsimilarity and homogeneity. Given that a majority of LMs' training data is\nscraped from the Web without informing content owners, their reiteration of\nwords, phrases, and even core ideas from training sets into generated texts has\nethical implications. Their patterns are likely to exacerbate as both the size\nof LMs and their training data increase, raising concerns about\nindiscriminately pursuing larger models with larger training corpora.\nPlagiarized content can also contain individuals' personal and sensitive\ninformation. These findings overall cast doubt on the practicality of current\nLMs in mission-critical writing tasks and urge more discussions around the\nobserved phenomena. Data and source code are available at\nhttps://github.com/Brit7777/LM-plagiarism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jooyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thai Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinghui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongwon Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Many Data Samples is an Additional Instruction Worth?. (arXiv:2203.09161v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.09161","description":"<p>Recently introduced instruction-paradigm empowers non-expert users to\nleverage NLP resources by defining a new task in natural language.\nInstruction-tuned models have significantly outperformed multitask learning\nmodels (without instruction); however they are far from state-of-the-art\ntask-specific models. Conventional approaches to improve model performance via\ncreating datasets with large number of task instances or architectural changes\nin the model may not be feasible for non-expert users. However, they can write\nalternate instructions to represent an instruction task. Is\nInstruction-augmentation helpful? We augment a subset of tasks in the expanded\nversion of NATURAL INSTRUCTIONS with additional instructions and find that it\nsignificantly improves model performance (up to 35%), especially in the\nlow-data regime. Our results indicate that an additional instruction can be\nequivalent to ~200 data samples on average across tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Puri_R/0/1/0/all/0/1\">Ravsehaj Singh Puri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Mihir Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QASem Parsing: Text-to-text Modeling of QA-based Semantics. (arXiv:2205.11413v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11413","description":"<p>Several recent works have suggested to represent semantic relations with\nquestions and answers, decomposing textual information into separate\ninterrogative natural language statements. In this paper, we consider three\nQA-based semantic tasks - namely, QA-SRL, QANom and QADiscourse, each targeting\na certain type of predication - and propose to regard them as jointly providing\na comprehensive representation of textual information. To promote this goal, we\ninvestigate how to best utilize the power of sequence-to-sequence (seq2seq)\npre-trained language models, within the unique setup of semi-structured\noutputs, consisting of an unordered set of question-answer pairs. We examine\ndifferent input and output linearization strategies, and assess the effect of\nmultitask learning and of simple data augmentation techniques in the setting of\nimbalanced training data. Consequently, we release the first unified QASem\nparsing tool, practical for downstream applications who can benefit from an\nexplicit, QA-based account of information units in a text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klein_A/0/1/0/all/0/1\">Ayal Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirsch_E/0/1/0/all/0/1\">Eran Hirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eliav_R/0/1/0/all/0/1\">Ron Eliav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyatkin_V/0/1/0/all/0/1\">Valentina Pyatkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1\">Avi Caciularu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1\">Ido Dagan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RevUp: Revise and Update Information Bottleneck for Event Representation. (arXiv:2205.12248v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.12248","description":"<p>The existence of external (``side'') semantic knowledge has been shown to\nresult in more expressive computational event models. To enable the use of side\ninformation that may be noisy or missing, we propose a semi-supervised\ninformation bottleneck-based discrete latent variable model. We reparameterize\nthe model's discrete variables with auxiliary continuous latent variables and a\nlight-weight hierarchical structure. Our model is learned to minimize the\nmutual information between the observed data and optional side knowledge that\nis not already captured by the new, auxiliary variables. We theoretically show\nthat our approach generalizes past approaches, and perform an empirical case\nstudy of our approach on event modeling. We corroborate our theoretical results\nwith strong empirical experiments, showing that the proposed method outperforms\nprevious proposed approaches on multiple datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rezaee_M/0/1/0/all/0/1\">Mehdi Rezaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraro_F/0/1/0/all/0/1\">Francis Ferraro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies. (arXiv:2208.10264v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.10264","description":"<p>We introduce a new type of test, called a Turing Experiment (TE), for\nevaluating how well a language model, such as GPT-3, can simulate different\naspects of human behavior. Unlike the Turing Test, which involves simulating a\nsingle arbitrary individual, a TE requires simulating a representative sample\nof participants in human subject research. We give TEs that attempt to\nreplicate well-established findings in prior studies. We design a methodology\nfor simulating TEs and illustrate its use to compare how well different\nlanguage models are able to reproduce classic economic, psycholinguistic, and\nsocial psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram\nShock Experiment, and Wisdom of Crowds. In the first three TEs, the existing\nfindings were replicated using recent models, while the last TE reveals a\n\"hyper-accuracy distortion\" present in some language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aher_G/0/1/0/all/0/1\">Gati Aher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arriaga_R/0/1/0/all/0/1\">Rosa I. Arriaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalai_A/0/1/0/all/0/1\">Adam Tauman Kalai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers with Learnable Activation Functions. (arXiv:2208.14111v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.14111","description":"<p>Activation functions can have a significant impact on reducing the\ntopological complexity of input data and therefore improve the performance of\nthe model. Selecting a suitable activation function is an essential step in\nneural model design. However, the choice of activation function is seldom\ndiscussed or explored in Transformer-based language models. Their activation\nfunctions are chosen beforehand and then remain fixed from pre-training to\nfine-tuning. As a result, the inductive biases they imposed on models cannot be\nadjusted during this long life cycle. Moreover, subsequently developed models\n(e.g., RoBERTa, BART, and GPT-3) often follow up prior work (e.g., BERT) to use\nthe same activation function without justification. In this paper, we\ninvestigate the effectiveness of using Rational Activation Function (RAF), a\nlearnable activation function, in the Transformer architecture. In contrast to\nconventional, predefined activation functions, RAFs can adaptively learn\noptimal activation functions during training according to input data. Our\nexperiments show the RAF-based Transformer (RAFT) achieves a lower validation\nperplexity than a vanilla BERT with the GELU function. We further evaluate RAFT\non downstream tasks in low- and full-data settings. Our results show that RAFT\noutperforms the counterpart model across the majority of tasks and settings.\nFor instance, RAFT outperforms vanilla BERT on the GLUE benchmark by 5.71\npoints on average in low-data scenario (where 100 training examples are\navailable) and by 2.05 points on SQuAD in full-data setting. Analysis of the\nshapes of learned RAFs further unveils that they substantially vary between\ndifferent layers of the pre-trained model and mostly look very different from\nconventional activation functions. RAFT opens a new research direction for\nanalyzing and interpreting pre-trained models according to the learned\nactivation functions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Haishuo Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Ji-Ung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moosavi_N/0/1/0/all/0/1\">Nafise Sadat Moosavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focus-Driven Contrastive Learniang for Medical Question Summarization. (arXiv:2209.00484v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.00484","description":"<p>Automatic medical question summarization can significantly help the system to\nunderstand consumer health questions and retrieve correct answers. The Seq2Seq\nmodel based on maximum likelihood estimation (MLE) has been applied in this\ntask, which faces two general problems: the model can not capture well question\nfocus and and the traditional MLE strategy lacks the ability to understand\nsentence-level semantics. To alleviate these problems, we propose a novel\nquestion focus-driven contrastive learning framework (QFCL). Specially, we\npropose an easy and effective approach to generate hard negative samples based\non the question focus, and exploit contrastive learning at both encoder and\ndecoder to obtain better sentence level representations. On three medical\nbenchmark datasets, our proposed model achieves new state-of-the-art results,\nand obtains a performance gain of 5.33, 12.85 and 3.81 points over the baseline\nBART model on three datasets respectively. Further human judgement and detailed\nanalysis prove that our QFCL model learns better sentence representations with\nthe ability to distinguish different sentence meanings, and generates\nhigh-quality summaries by capturing question focus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1\">Shuai Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fact-Saboteurs: A Taxonomy of Evidence Manipulation Attacks against Fact-Verification Systems. (arXiv:2209.03755v3 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2209.03755","description":"<p>Mis- and disinformation are a substantial global threat to our security and\nsafety. To cope with the scale of online misinformation, researchers have been\nworking on automating fact-checking by retrieving and verifying against\nrelevant evidence. However, despite many advances, a comprehensive evaluation\nof the possible attack vectors against such systems is still lacking.\nParticularly, the automated fact-verification process might be vulnerable to\nthe exact disinformation campaigns it is trying to combat. In this work, we\nassume an adversary that automatically tampers with the online evidence in\norder to disrupt the fact-checking model via camouflaging the relevant evidence\nor planting a misleading one. We first propose an exploratory taxonomy that\nspans these two targets and the different threat model dimensions. Guided by\nthis, we design and propose several potential attack methods. We show that it\nis possible to subtly modify claim-salient snippets in the evidence and\ngenerate diverse and claim-aligned evidence. Thus, we highly degrade the\nfact-checking performance under many different permutations of the taxonomy's\ndimensions. The attacks are also robust against post-hoc modifications of the\nclaim. Our analysis further hints at potential limitations in models' inference\nwhen faced with contradicting evidence. We emphasize that these attacks can\nhave harmful implications on the inspectable and human-in-the-loop usage\nscenarios of such models, and conclude by discussing challenges and directions\nfor future defenses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelnabi_S/0/1/0/all/0/1\">Sahar Abdelnabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1\">Mario Fritz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How people talk about each other: Modeling Generalized Intergroup Bias and Emotion. (arXiv:2209.06687v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.06687","description":"<p>Current studies of bias in NLP rely mainly on identifying (unwanted or\nnegative) bias towards a specific demographic group. While this has led to\nprogress recognizing and mitigating negative bias, and having a clear notion of\nthe targeted group is necessary, it is not always practical. In this work we\nextrapolate to a broader notion of bias, rooted in social science and\npsychology literature. We move towards predicting interpersonal group\nrelationship (IGR) - modeling the relationship between the speaker and the\ntarget in an utterance - using fine-grained interpersonal emotions as an\nanchor. We build and release a dataset of English tweets by US Congress members\nannotated for interpersonal emotion -- the first of its kind, and 'found\nsupervision' for IGR labels; our analyses show that subtle emotional signals\nare indicative of different biases. While humans can perform better than chance\nat identifying IGR given an utterance, we show that neural models perform much\nbetter; furthermore, a shared encoding between IGR and interpersonal perceived\nemotion enabled performance gains in both tasks. Data and code for this paper\nare available at https://github.com/venkatasg/interpersonal-bias\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Govindarajan_V/0/1/0/all/0/1\">Venkata S Govindarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atwell_K/0/1/0/all/0/1\">Katherine Atwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinno_B/0/1/0/all/0/1\">Barea Sinno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaver_D/0/1/0/all/0/1\">David I. Beaver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Tuning with Special Token Adaptation. (arXiv:2210.04382v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04382","description":"<p>Parameter-efficient tuning aims at updating only a small subset of parameters\nwhen adapting a pretrained model to downstream tasks. In this work, we\nintroduce PASTA, in which we only modify the special token representations\n(e.g., [SEP] and [CLS] in BERT) before the self-attention module at each layer\nin Transformer-based models. PASTA achieves comparable performance to full\nfinetuning in natural language understanding tasks including text\nclassification and NER with up to only 0.029% of total parameters trained. Our\nwork not only provides a simple yet effective way of parameter-efficient\ntuning, which has a wide range of practical applications when deploying\nfinetuned models for multiple tasks, but also demonstrates the pivotal role of\nspecial tokens in pretrained language models\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaocong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">James Y. Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Like a bilingual baby: The advantage of visually grounding a bilingual language model. (arXiv:2210.05487v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05487","description":"<p>Unlike most neural language models, humans learn language in a rich,\nmulti-sensory and, often, multi-lingual environment. Current language models\ntypically fail to fully capture the complexities of multilingual language use.\nWe train an LSTM language model on images and captions in English and Spanish\nfrom MS-COCO-ES. We find that the visual grounding improves the model's\nunderstanding of semantic similarity both within and across languages and\nimproves perplexity. However, we find no significant advantage of visual\ngrounding for abstract words. Our results provide additional evidence of the\nadvantages of visually grounded language models and point to the need for more\nnaturalistic language data from multilingual speakers and multilingual datasets\nwith perceptual grounding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khai-Nguyen Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zixin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mali_A/0/1/0/all/0/1\">Ankur Mali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_A/0/1/0/all/0/1\">Alex Kelly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Multimodal Learning for Emergence of Graphical Sensory-Motor Communication. (arXiv:2210.06468v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2210.06468","description":"<p>In this paper, we investigate whether artificial agents can develop a shared\nlanguage in an ecological setting where communication relies on a sensory-motor\nchannel. To this end, we introduce the Graphical Referential Game (GREG) where\na speaker must produce a graphical utterance to name a visual referent object\nwhile a listener has to select the corresponding object among distractor\nreferents, given the delivered message. The utterances are drawing images\nproduced using dynamical motor primitives combined with a sketching library. To\ntackle GREG we present CURVES: a multimodal contrastive deep learning mechanism\nthat represents the energy (alignment) between named referents and utterances\ngenerated through gradient ascent on the learned energy landscape. We\ndemonstrate that CURVES not only succeeds at solving the GREG but also enables\nagents to self-organize a language that generalizes to feature compositions\nnever seen during training. In addition to evaluating the communication\nperformance of our approach, we also explore the structure of the emerging\nlanguage. Specifically, we show that the resulting language forms a coherent\nlexicon shared between agents and that basic compositional rules on the\ngraphical productions could not explain the compositional generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karch_T/0/1/0/all/0/1\">Tristan Karch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lemesle_Y/0/1/0/all/0/1\">Yoann Lemesle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laroche_R/0/1/0/all/0/1\">Romain Laroche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moulin_Frier_C/0/1/0/all/0/1\">Cl&#xe9;ment Moulin-Frier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models. (arXiv:2210.08933v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08933","description":"<p>Recently, diffusion models have emerged as a new paradigm for generative\nmodels. Despite the success in domains using continuous signals such as vision\nand audio, adapting diffusion models to natural language is under-explored due\nto the discrete nature of texts, especially for conditional generation. We\ntackle this challenge by proposing DiffuSeq: a diffusion model designed for\nsequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation\nover a wide range of Seq2Seq tasks, we find DiffuSeq achieving comparable or\neven better performance than six established baselines, including a\nstate-of-the-art model that is based on pre-trained language models. Apart from\nquality, an intriguing property of DiffuSeq is its high diversity during\ngeneration, which is desired in many Seq2Seq tasks. We further include a\ntheoretical analysis revealing the connection between DiffuSeq and\nautoregressive/non-autoregressive models. Bringing together theoretical\nanalysis and empirical evidence, we demonstrate the great potential of\ndiffusion models in complex conditional language generation tasks. Code is\navailable at \\url{https://github.com/Shark-NLP/DiffuSeq}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1\">Shansan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mukai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiangtao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Search Is What You Need For Neural Text Generation. (arXiv:2210.14140v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.14140","description":"<p>Generating text with autoregressive language models (LMs) is of great\nimportance to many natural language processing (NLP) applications. Previous\nsolutions for this task often produce text that contains degenerative\nexpressions or lacks semantic consistency. Recently, Su et al. introduced a new\ndecoding method, contrastive search, based on the isotropic representation\nspace of the language model and obtained new state of the art on various\nbenchmarks. Additionally, Su et al. argued that the representations of\nautoregressive LMs (e.g. GPT-2) are intrinsically anisotropic which is also\nshared by previous studies. Therefore, to ensure the language model follows an\nisotropic distribution, Su et al. proposed a contrastive learning scheme,\nSimCTG, which calibrates the language model's representations through\nadditional training.\n</p>\n<p>In this study, we first answer the question: \"Are autoregressive LMs really\nanisotropic?\". To this end, we extensively evaluate the isotropy of LMs across\n16 major languages. Surprisingly, we find that the anisotropic problem only\nexists in the two specific English GPT-2-small/medium models. On the other\nhand, all other evaluated LMs are naturally isotropic which is in contrast to\nthe conclusion drawn by previous studies. Based on our findings, we further\nassess the contrastive search decoding method using off-the-shelf LMs on four\ngeneration tasks across 16 languages. Our experimental results demonstrate that\ncontrastive search significantly outperforms previous decoding methods without\nany additional training. More notably, on 12 out of the 16 evaluated languages,\ncontrastive search performs comparably with human-level performances as judged\nby human evaluations. Our code and other related resources are publicly\navailable at https://github.com/yxuansu/Contrastive_Search_Is_What_You_Need.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent Linguistic Structures in Neural Networks are Fragile. (arXiv:2210.17406v6 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.17406","description":"<p>Large Language Models (LLMs) have been reported to have strong performance on\nnatural language processing tasks. However, performance metrics such as\naccuracy do not measure the quality of the model in terms of its ability to\nrobustly represent complex linguistic structure. In this paper, focusing on the\nability of language models to represent syntax, we propose a framework to\nassess the consistency and robustness of linguistic representations. To this\nend, we introduce measures of robustness of neural network models that leverage\nrecent advances in extracting linguistic constructs from LLMs via probing\ntasks, i.e., simple tasks used to extract meaningful information about a single\nfacet of a language model, such as syntax reconstruction and root\nidentification. Empirically, we study the performance of four LLMs across six\ndifferent corpora on the proposed robustness measures by analysing their\nperformance and robustness with respect to syntax-preserving perturbations. We\nprovide evidence that context-free representation (e.g., GloVe) are in some\ncases competitive with context-dependent representations from modern LLMs\n(e.g., BERT), yet equally brittle to syntax-preserving perturbations. Our key\nobservation is that emergent syntactic representations in neural networks are\nbrittle. We make the code, trained models and logs available to the community\nas a contribution to the debate about the capabilities of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malfa_E/0/1/0/all/0/1\">Emanuele La Malfa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wicker_M/0/1/0/all/0/1\">Matthew Wicker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiatkowska_M/0/1/0/all/0/1\">Marta Kiatkowska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The 2022 n2c2/UW Shared Task on Extracting Social Determinants of Health. (arXiv:2301.05571v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.05571","description":"<p>Objective: The n2c2/UW SDOH Challenge explores the extraction of social\ndeterminant of health (SDOH) information from clinical notes. The objectives\ninclude the advancement of natural language processing (NLP) information\nextraction techniques for SDOH and clinical information more broadly. This\npaper presents the shared task, data, participating teams, performance results,\nand considerations for future work.\n</p>\n<p>Materials and Methods: The task used the Social History Annotated Corpus\n(SHAC), which consists of clinical text with detailed event-based annotations\nfor SDOH events such as alcohol, drug, tobacco, employment, and living\nsituation. Each SDOH event is characterized through attributes related to\nstatus, extent, and temporality. The task includes three subtasks related to\ninformation extraction (Subtask A), generalizability (Subtask B), and learning\ntransfer (Subtask C). In addressing this task, participants utilized a range of\ntechniques, including rules, knowledge bases, n-grams, word embeddings, and\npretrained language models (LM).\n</p>\n<p>Results: A total of 15 teams participated, and the top teams utilized\npretrained deep learning LM. The top team across all subtasks used a\nsequence-to-sequence approach achieving 0.901 F1 for Subtask A, 0.774 F1\nSubtask B, and 0.889 F1 for Subtask C.\n</p>\n<p>Conclusions: Similar to many NLP tasks and domains, pretrained LM yielded the\nbest performance, including generalizability and learning transfer. An error\nanalysis indicates extraction performance varies by SDOH, with lower\nperformance achieved for conditions, like substance use and homelessness, that\nincrease health risks (risk factors) and higher performance achieved for\nconditions, like substance abstinence and living with family, that reduce\nhealth risks (protective factors).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lybarger_K/0/1/0/all/0/1\">Kevin Lybarger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yetisgen_M/0/1/0/all/0/1\">Meliha Yetisgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uzuner_O/0/1/0/all/0/1\">&#xd6;zlem Uzuner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Flan Collection: Designing Data and Methods for Effective Instruction Tuning. (arXiv:2301.13688v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2301.13688","description":"<p>We study the design decisions of publicly available instruction tuning\nmethods, and break down the development of Flan 2022 (Chung et al., 2022).\nThrough careful ablation studies on the Flan Collection of tasks and methods,\nwe tease apart the effect of design decisions which enable Flan-T5 to\noutperform prior work by 3-17%+ across evaluation settings. We find task\nbalancing and enrichment techniques are overlooked but critical to effective\ninstruction tuning, and in particular, training with mixed prompt settings\n(zero-shot, few-shot, and chain-of-thought) actually yields stronger (2%+)\nperformance in all settings. In further experiments, we show Flan-T5 requires\nless finetuning to converge higher and faster than T5 on single downstream\ntasks, motivating instruction-tuned models as more computationally-efficient\nstarting checkpoints for new tasks. Finally, to accelerate research on\ninstruction tuning, we make the Flan 2022 collection of datasets, templates,\nand methods publicly available at\nhttps://github.com/google-research/FLAN/tree/main/flan/v2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Le Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tu Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1\">Albert Webson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1\">Barret Zoph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Execution-based Code Generation using Deep Reinforcement Learning. (arXiv:2301.13816v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2301.13816","description":"<p>The utilization of programming language (PL) models, pretrained on\nlarge-scale code corpora, as a means of automating software engineering\nprocesses has demonstrated considerable potential in streamlining various code\ngeneration tasks such as code completion, code translation, and program\nsynthesis. However, current approaches mainly rely on supervised fine-tuning\nobjectives borrowed from text generation, neglecting specific sequence-level\nfeatures of code, including but not limited to compilability as well as\nsyntactic and functional correctness. To address this limitation, we propose\nPPOCoder, a new framework for code generation that combines pretrained PL\nmodels with Proximal Policy Optimization (PPO) deep reinforcement learning and\nemploys execution feedback as the external source of knowledge into the model\noptimization. PPOCoder is transferable across different code generation tasks\nand PLs. Extensive experiments on three code generation tasks demonstrate the\neffectiveness of our proposed approach compared to SOTA methods, improving the\nsuccess rate of compilation and functional correctness over different PLs. Our\ncode can be found at https://github.com/reddy-lab-code-research/PPOCoder .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shojaee_P/0/1/0/all/0/1\">Parshin Shojaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Aneesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tipirneni_S/0/1/0/all/0/1\">Sindhu Tipirneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1\">Chandan K. Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Can Be Easily Distracted by Irrelevant Context. (arXiv:2302.00093v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.00093","description":"<p>Large language models have achieved impressive performance on various natural\nlanguage processing tasks. However, so far they have been evaluated primarily\non benchmarks where all information in the input context is relevant for\nsolving the task. In this work, we investigate the distractibility of large\nlanguage models, i.e., how the model problem-solving accuracy can be influenced\nby irrelevant context. In particular, we introduce Grade-School Math with\nIrrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant\ninformation in the problem description. We use this benchmark to measure the\ndistractibility of cutting-edge prompting techniques for large language models,\nand find that the model performance is dramatically decreased when irrelevant\ninformation is included. We also identify several approaches for mitigating\nthis deficiency, such as decoding with self-consistency and adding to the\nprompt an instruction that tells the language model to ignore the irrelevant\ninformation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1\">Freda Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_K/0/1/0/all/0/1\">Kanishka Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scales_N/0/1/0/all/0/1\">Nathan Scales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohan_D/0/1/0/all/0/1\">David Dohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scharli_N/0/1/0/all/0/1\">Nathanael Sch&#xe4;rli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Languages are Rewards: Chain of Hindsight Finetuning using Human Feedback. (arXiv:2302.02676v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.02676","description":"<p>Learning from human preferences is important for language models to be\nhelpful and useful for humans, and to align with human and social values.\nExisting works focus on supervised finetuning of pretrained models, based on\ncurated model generations that are preferred by human labelers. Such works have\nachieved remarkable successes in understanding and following instructions\n(e.g., InstructGPT, ChatGPT, etc). However, to date, a key limitation of\nsupervised finetuning is that it cannot learn from negative ratings; models are\nonly trained on positive-rated data, which makes it data inefficient. Because\ncollecting human feedback data is both time consuming and expensive, it is\nvital for the model to learn from all feedback, akin to the remarkable ability\nof humans to learn from diverse feedback. In this work, we propose a novel\ntechnique called Hindsight Finetuning for making language models learn from\ndiverse human feedback. In fact, our idea is motivated by how humans learn from\nhindsight experience. We condition the model on a sequence of model generations\npaired with hindsight feedback, and finetune the model to predict the most\npreferred output. By doing so, models can learn to identify and correct\nnegative attributes or errors. Applying the method to GPT-J, we observe that it\nsignificantly improves results on summarization and dialogue tasks using the\nsame amount of human feedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sferrazza_C/0/1/0/all/0/1\">Carmelo Sferrazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Categorical Archive of ChatGPT Failures. (arXiv:2302.03494v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.03494","description":"<p>Large language models have been demonstrated to be valuable in different\nfields. ChatGPT, developed by OpenAI, has been trained using massive amounts of\ndata and simulates human conversation by comprehending context and generating\nappropriate responses. It has garnered significant attention due to its ability\nto effectively answer a broad range of human inquiries, with fluent and\ncomprehensive answers surpassing prior public chatbots in both security and\nusefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,\nwhich is the focus of this study. Ten categories of failures, including\nreasoning, factual errors, math, coding, and bias, are presented and discussed.\nThe risks, limitations, and societal implications of ChatGPT are also\nhighlighted. The goal of this study is to assist researchers and developers in\nenhancing future language models and chatbots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1\">Ali Borji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Song of Ice and Fire: Analyzing Textual Autotelic Agents in ScienceWorld. (arXiv:2302.05244v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2302.05244","description":"<p>Building open-ended agents that can autonomously discover a diversity of\nbehaviours is one of the long-standing goals of artificial intelligence. This\nchallenge can be studied in the framework of autotelic RL agents, i.e. agents\nthat learn by selecting and pursuing their own goals, self-organizing a\nlearning curriculum. Recent work identified language has a key dimension of\nautotelic learning, in particular because it enables abstract goal sampling and\nguidance from social peers for hindsight relabelling. Within this perspective,\nwe study the following open scientific questions: What is the impact of\nhindsight feedback from a social peer (e.g. selective vs. exhaustive)? How can\nthe agent learn from very rare language goal examples in its experience replay?\nHow can multiple forms of exploration be combined, and take advantage of easier\ngoals as stepping stones to reach harder ones? To address these questions, we\nuse ScienceWorld, a textual environment with rich abstract and combinatorial\nphysics. We show the importance of selectivity from the social peer's feedback;\nthat experience replay needs to over-sample examples of rare goals; and that\nfollowing self-generated goal sequences where the agent's competence is\nintermediate leads to significant improvements in final performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teodorescu_L/0/1/0/all/0/1\">Laetitia Teodorescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_E/0/1/0/all/0/1\">Eric Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cote_M/0/1/0/all/0/1\">Marc-Alexandre C&#xf4;t&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Brief Report on LawGPT 1.0: A Virtual Legal Assistant Based on GPT-3. (arXiv:2302.05729v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.05729","description":"<p>LawGPT 1.0 is a virtual legal assistant built on the state-of-the-art\nlanguage model GPT-3, fine-tuned for the legal domain. The system is designed\nto provide legal assistance to users in a conversational manner, helping them\nwith tasks such as answering legal questions, generating legal documents, and\nproviding legal advice. In this paper, we provide a brief overview of LawGPT\n1.0, its architecture, and its performance on a set of legal benchmark tasks.\nPlease note that the detailed information about the model is protected by a\nnon-disclosure agreement (NDA) and cannot be disclosed in this report.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha-Thanh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NYCU-TWO at Memotion 3: Good Foundation, Good Teacher, then you have Good Meme Analysis. (arXiv:2302.06078v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.06078","description":"<p>This paper presents a robust solution to the Memotion 3.0 Shared Task. The\ngoal of this task is to classify the emotion and the corresponding intensity\nexpressed by memes, which are usually in the form of images with short captions\non social media. Understanding the multi-modal features of the given memes will\nbe the key to solving the task. In this work, we use CLIP to extract aligned\nimage-text features and propose a novel meme sentiment analysis framework,\nconsisting of a Cooperative Teaching Model (CTM) for Task A and a Cascaded\nEmotion Classifier (CEC) for Tasks B&amp;C. CTM is based on the idea of knowledge\ndistillation, and can better predict the sentiment of a given meme in Task A;\nCEC can leverage the emotion intensity suggestion from the prediction of Task C\nto classify the emotion more precisely in Task B. Experiments show that we\nachieved the 2nd place ranking for both Task A and Task B and the 4th place\nranking for Task C, with weighted F1-scores of 0.342, 0.784, and 0.535\nrespectively. The results show the robustness and effectiveness of our\nframework. Our code is released at github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yu-Chien Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kuang-Da Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_T/0/1/0/all/0/1\">Ting-Yun Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wen-Chih Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Noisy Crowd Labels with Logics. (arXiv:2302.06337v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.06337","description":"<p>This paper explores the integration of symbolic logic knowledge into deep\nneural networks for learning from noisy crowd labels. We introduce Logic-guided\nLearning from Noisy Crowd Labels (Logic-LNCL), an EM-alike iterative logic\nknowledge distillation framework that learns from both noisy labeled data and\nlogic rules of interest. Unlike traditional EM methods, our framework contains\na ``pseudo-E-step'' that distills from the logic rules a new type of learning\ntarget, which is then used in the ``pseudo-M-step'' for training the\nclassifier. Extensive evaluations on two real-world datasets for text sentiment\nclassification and named entity recognition demonstrate that the proposed\nframework improves the state-of-the-art and provides a new solution to learning\nfrom noisy crowd labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhijun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hailong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Haoqian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pengpeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-02-14T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}