{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2024-01-02T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"ChatEd: A Chatbot Leveraging ChatGPT for an Enhanced Learning Experience in Higher Education. (arXiv:2401.00052v1 [cs.CY])","link":"http://arxiv.org/abs/2401.00052","description":"<p>With the rapid evolution of Natural Language Processing (NLP), Large Language\nModels (LLMs) like ChatGPT have emerged as powerful tools capable of\ntransforming various sectors. Their vast knowledge base and dynamic interaction\ncapabilities represent significant potential in improving education by\noperating as a personalized assistant. However, the possibility of generating\nincorrect, biased, or unhelpful answers are a key challenge to resolve when\ndeploying LLMs in an education context. This work introduces an innovative\narchitecture that combines the strengths of ChatGPT with a traditional\ninformation retrieval based chatbot framework to offer enhanced student support\nin higher education. Our empirical evaluations underscore the high promise of\nthis approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kevin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramos_J/0/1/0/all/0/1\">Jason Ramos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawrence_R/0/1/0/all/0/1\">Ramon Lawrence</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Essay Scoring in a Brazilian Scenario. (arXiv:2401.00095v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00095","description":"<p>This paper presents a novel Automatic Essay Scoring (AES) algorithm tailored\nfor the Portuguese-language essays of Brazil's Exame Nacional do Ensino M\\'edio\n(ENEM), addressing the challenges in traditional human grading systems. Our\napproach leverages advanced deep learning techniques to align closely with\nhuman grading criteria, targeting efficiency and scalability in evaluating\nlarge volumes of student essays. This research not only responds to the\nlogistical and financial constraints of manual grading in Brazilian educational\nassessments but also promises to enhance fairness and consistency in scoring,\nmarking a significant step forward in the application of AES in large-scale\nacademic settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matsuoka_F/0/1/0/all/0/1\">Felipe Akio Matsuoka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Knowledge All Large Language Models Needed for Causal Reasoning?. (arXiv:2401.00139v1 [cs.AI])","link":"http://arxiv.org/abs/2401.00139","description":"<p>This paper explores the causal reasoning of large language models (LLMs) to\nenhance their interpretability and reliability in advancing artificial\nintelligence. Despite the proficiency of LLMs in a range of tasks, their\npotential for understanding causality requires further exploration. We propose\na novel causal attribution model that utilizes \"do-operators\" for constructing\ncounterfactual scenarios, allowing us to systematically quantify the influence\nof input numerical data and LLMs' pre-existing knowledge on their causal\nreasoning processes. Our newly developed experimental setup assesses LLMs'\nreliance on contextual information and inherent knowledge across various\ndomains. Our evaluation reveals that LLMs' causal reasoning ability depends on\nthe context and domain-specific knowledge provided, and supports the argument\nthat \"knowledge is, indeed, what LLMs principally require for sound causal\nreasoning\". On the contrary, in the absence of knowledge, LLMs still maintain a\ndegree of causal reasoning using the available numerical data, albeit with\nlimitations in the calculations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hengrui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Rui Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained Language Models for Question Answering over Knowledge Graph. (arXiv:2401.00158v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00158","description":"<p>Question Answering over Knowledge Graph (KGQA) aims to seek answer entities\nfor the natural language question from a large-scale Knowledge Graph~(KG). To\nbetter perform reasoning on KG, recent work typically adopts a pre-trained\nlanguage model~(PLM) to model the question, and a graph neural network~(GNN)\nbased module to perform multi-hop reasoning on the KG. Despite the\neffectiveness, due to the divergence in model architecture, the PLM and GNN are\nnot closely integrated, limiting the knowledge sharing and fine-grained feature\ninteractions. To solve it, we aim to simplify the above two-module approach,\nand develop a more capable PLM that can directly support subgraph reasoning for\nKGQA, namely ReasoningLM. In our approach, we propose a subgraph-aware\nself-attention mechanism to imitate the GNN for performing structured\nreasoning, and also adopt an adaptation tuning strategy to adapt the model\nparameters with 20,000 subgraphs with synthesized questions. After adaptation,\nthe PLM can be parameter-efficient fine-tuned on downstream tasks. Experiments\nshow that ReasoningLM surpasses state-of-the-art models by a large margin, even\nwith fewer updated parameters and less training data. Our codes and data are\npublicly available at~\\url{https://github.com/RUCAIBox/ReasoningLM}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jinhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating the Impact of False Negatives in Dense Retrieval with Contrastive Confidence Regularization. (arXiv:2401.00165v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00165","description":"<p>In open-domain Question Answering (QA), dense retrieval is crucial for\nfinding relevant passages for answer generation. Typically, contrastive\nlearning is used to train a retrieval model that maps passages and queries to\nthe same semantic space. The objective is to make similar ones closer and\ndissimilar ones further apart. However, training such a system is challenging\ndue to the false negative issue, where relevant passages may be missed during\ndata annotation. Hard negative sampling, which is commonly used to improve\ncontrastive learning, can introduce more noise in training. This is because\nhard negatives are those closer to a given query, and thus more likely to be\nfalse negatives. To address this issue, we propose a novel contrastive\nconfidence regularizer for Noise Contrastive Estimation (NCE) loss, a commonly\nused loss for dense retrieval. Our analysis shows that the regularizer helps\ndense retrieval models be more robust against false negatives with a\ntheoretical guarantee. Additionally, we propose a model-agnostic method to\nfilter out noisy negative passages in the dataset, improving any downstream\ndense retrieval models. Through experiments on three datasets, we demonstrate\nthat our method achieves better retrieval performance in comparison to existing\nstate-of-the-art dense retrieval systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yeqin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cam-Tu Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L3Cube-MahaSocialNER: A Social Media based Marathi NER Dataset and BERT models. (arXiv:2401.00170v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00170","description":"<p>This work introduces the L3Cube-MahaSocialNER dataset, the first and largest\nsocial media dataset specifically designed for Named Entity Recognition (NER)\nin the Marathi language. The dataset comprises 18,000 manually labeled\nsentences covering eight entity classes, addressing challenges posed by social\nmedia data, including non-standard language and informal idioms. Deep learning\nmodels, including CNN, LSTM, BiLSTM, and Transformer models, are evaluated on\nthe individual dataset with IOB and non-IOB notations. The results demonstrate\nthe effectiveness of these models in accurately recognizing named entities in\nMarathi informal text. The L3Cube-MahaSocialNER dataset offers user-centric\ninformation extraction and supports real-time applications, providing a\nvaluable resource for public opinion analysis, news, and marketing on social\nmedia platforms. We also show that the zero-shot results of the regular NER\nmodel are poor on the social NER test set thus highlighting the need for more\nsocial NER datasets. The datasets and models are publicly available at\nhttps://github.com/l3cube-pune/MarathiNLP\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhari_H/0/1/0/all/0/1\">Harsh Chaudhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_A/0/1/0/all/0/1\">Anuja Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavekar_D/0/1/0/all/0/1\">Dhanashree Lavekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khairnar_P/0/1/0/all/0/1\">Pranav Khairnar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Principle Interference in Technical and Scientific Translation. (arXiv:2401.00177v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00177","description":"<p>In this article, I will explore the nature of interference in translation,\nespecially in technical and scientific texts, using a descriptivist approach. I\nwill have a brief overview of the historical excursion of interference in\ntechnical and scientific translation. My aim is to explain this phenomenon and\nits causes with all its paradoxes, instead of simply condemning it as an\nexample of supposedly bad translation. Thus, I will focus on its status in the\nbibliography of translation, on the motives for and consequences of\ninterference in specialized translation, as well as on the nature of the\narguments given for and against this phenomenon. Therefore the relationship\nbetween different societies has always been possible with the act of\ntranslation. When civilizations are examined throughout history, it is seen\nthat the dissemination of knowledge among different societies has been achieved\nby translation. These societies have often become aware of the advancements in\ntechnology and science by means of translation. Therefore; translation becomes\nvery significant in technical contact between societies and humans. Since the\ntranslation of technical texts is the preliminary scope of this thesis, it will\nbe beneficial to have a brief look at the history of technical translation in\nthe world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qani_M/0/1/0/all/0/1\">Mohammad Ibrahim Qani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Problem of Alignment. (arXiv:2401.00210v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00210","description":"<p>Large Language Models produce sequences learned as statistical patterns from\nlarge corpora. In order not to reproduce corpus biases, after initial training\nmodels must be aligned with human values, preferencing certain continuations\nover others. Alignment, which can be viewed as the superimposition of normative\nstructure onto a statistical model, reveals a conflicted and complex\ninterrelationship between language and technology. This relationship shapes\ntheories of language, linguistic practice and subjectivity, which are\nespecially relevant to the current sophistication in artificially produced\ntext. We examine this practice of structuration as a two-way interaction\nbetween users and models by analysing how ChatGPT4 redacts perceived\n`anomalous' language in fragments of Joyce's Ulysses and the new linguistic\npractice of prompt engineering. We then situate this alignment problem\nhistorically, revisiting earlier postwar linguistic debates which counterposed\ntwo views of meaning: as discrete structures, and as continuous probability\ndistributions. We discuss the largely occluded work of the Moscow Linguistic\nSchool, which sought to reconcile this opposition. Our attention to the Moscow\nSchool and later related arguments by Searle and Kristeva casts the problem of\nalignment in a new light: as one involving attention to the social\nstructuration of linguistic practice, including structuration of anomalies\nthat, like the Joycean text, exist in defiance of expressive conventions. These\ndebates around the communicative orientation toward language can help explain\nsome of the contemporary behaviours and interdependencies that take place\nbetween users and LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hristova_T/0/1/0/all/0/1\">Tsvetelina Hristova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magee_L/0/1/0/all/0/1\">Liam Magee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldatic_K/0/1/0/all/0/1\">Karen Soldatic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Evaluate Coreference in Literary Texts?. (arXiv:2401.00238v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00238","description":"<p>In this short paper, we examine the main metrics used to evaluate textual\ncoreference and we detail some of their limitations. We show that a unique\nscore cannot represent the full complexity of the problem at stake, and is thus\nuninformative, or even misleading. We propose a new way of evaluating\ncoreference, taking into account the context (in our case, the analysis of\nfictions, esp. novels). More specifically, we propose to distinguish long\ncoreference chains (corresponding to main characters), from short ones\n(corresponding to secondary characters), and singletons (isolated elements).\nThis way, we hope to get more interpretable and thus more informative results\nthrough evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duron_Tejedor_A/0/1/0/all/0/1\">Ana-Isabel Duron-Tejedor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amsili_P/0/1/0/all/0/1\">Pascal Amsili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poibeau_T/0/1/0/all/0/1\">Thierry Poibeau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Large Language Model for Speech Synthesis: An Empirical Study. (arXiv:2401.00246v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00246","description":"<p>Large language models (LLMs) have made significant advancements in natural\nlanguage processing and are concurrently extending the language ability to\nother modalities, such as speech and vision. Nevertheless, most of the previous\nwork focuses on prompting LLMs with perception abilities like auditory\ncomprehension, and the effective approach for augmenting LLMs with speech\nsynthesis capabilities remains ambiguous. In this paper, we conduct a\ncomprehensive empirical exploration of boosting LLMs with the ability to\ngenerate speech, by combining pre-trained LLM LLaMA/OPT and text-to-speech\nsynthesis model VALL-E. We compare three integration methods between LLMs and\nspeech synthesis models, including directly fine-tuned LLMs, superposed layers\nof LLMs and VALL-E, and coupled LLMs and VALL-E using LLMs as a powerful text\nencoder. Experimental results show that, using LoRA method to fine-tune LLMs\ndirectly to boost the speech synthesis capability does not work well, and\nsuperposed LLMs and VALL-E can improve the quality of generated speech both in\nspeaker similarity and word error rate (WER). Among these three methods,\ncoupled methods leveraging LLMs as the text encoder can achieve the best\nperformance, making it outperform original speech synthesis models with a\nconsistently better speaker similarity and a significant (10.9%) WER reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_H/0/1/0/all/0/1\">Hongkun Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shujie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Zero-Shot Generalizability on Mandarin-English Code-Switched ASR and Speech-to-text Translation of Recent Foundation Models with Self-Supervision and Weak Supervision. (arXiv:2401.00273v1 [eess.AS])","link":"http://arxiv.org/abs/2401.00273","description":"<p>This work evaluated several cutting-edge large-scale foundation models based\non self-supervision or weak supervision, including SeamlessM4T, SeamlessM4T v2,\nand Whisper-large-v3, on three code-switched corpora. We found that\nself-supervised models can achieve performances close to the supervised model,\nindicating the effectiveness of multilingual self-supervised pre-training. We\nalso observed that these models still have room for improvement as they kept\nmaking similar mistakes and had unsatisfactory performances on modeling\nintra-sentential code-switching. In addition, the validity of several variants\nof Whisper was explored, and we concluded that they remained effective in a\ncode-switching scenario, and similar techniques for self-supervised models are\nworth studying to boost the performance of code-switched tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_C/0/1/0/all/0/1\">Chih-Kai Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Po Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_K/0/1/0/all/0/1\">Ke-Han Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuan_C/0/1/0/all/0/1\">Chun-Yi Kuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsiao_C/0/1/0/all/0/1\">Chi-Yuan Hsiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation is all you need. Prompting Generative Large Language Models for Annotation Tasks in the Social Sciences. A Primer using Open Models. (arXiv:2401.00284v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00284","description":"<p>This paper explores the use of open generative Large Language Models (LLMs)\nfor annotation tasks in the social sciences. The study highlights the\nchallenges associated with proprietary models, such as limited reproducibility\nand privacy concerns, and advocates for the adoption of open (source) models\nthat can be operated on independent devices. Two examples of annotation tasks,\nsentiment analysis in tweets and identification of leisure activities in\nchildhood aspirational essays are provided. The study evaluates the performance\nof different prompting strategies and models (neural-chat-7b-v3-2,\nStarling-LM-7B-alpha, openchat_3.5, zephyr-7b-alpha and zephyr-7b-beta). The\nresults indicate the need for careful validation and tailored prompt\nengineering. The study highlights the advantages of open models for data\nprivacy and reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weber_M/0/1/0/all/0/1\">Maximilian Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichardt_M/0/1/0/all/0/1\">Merle Reichardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Art of Defending: A Systematic Evaluation and Analysis of LLM Defense Strategies on Safety and Over-Defensiveness. (arXiv:2401.00287v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00287","description":"<p>As Large Language Models (LLMs) play an increasingly pivotal role in natural\nlanguage processing applications, their safety concerns become critical areas\nof NLP research. This paper presents Safety and Over-Defensiveness Evaluation\n(SODE) benchmark: a collection of diverse safe and unsafe prompts with\ncarefully designed evaluation methods that facilitate systematic evaluation,\ncomparison, and analysis over 'safety' and 'over-defensiveness.' With SODE, we\nstudy a variety of LLM defense strategies over multiple state-of-the-art LLMs,\nwhich reveals several interesting and important findings, such as (a) the\nwidely popular 'self-checking' techniques indeed improve the safety against\nunsafe inputs, but this comes at the cost of extreme over-defensiveness on the\nsafe inputs, (b) providing a safety instruction along with in-context exemplars\n(of both safe and unsafe inputs) consistently improves safety and also\nmitigates undue over-defensiveness of the models, (c) providing contextual\nknowledge easily breaks the safety guardrails and makes the models more\nvulnerable to generating unsafe responses. Overall, our work reveals numerous\nsuch critical findings that we believe will pave the way and facilitate further\nresearch in improving the safety of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolin_P/0/1/0/all/0/1\">Pavel Dolin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seth_A/0/1/0/all/0/1\">Agastya Seth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Red Teaming for Large Language Models At Scale: Tackling Hallucinations on Mathematics Tasks. (arXiv:2401.00290v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00290","description":"<p>We consider the problem of red teaming LLMs on elementary calculations and\nalgebraic tasks to evaluate how various prompting techniques affect the quality\nof outputs. We present a framework to procedurally generate numerical questions\nand puzzles, and compare the results with and without the application of\nseveral red teaming techniques. Our findings suggest that even though\nstructured reasoning and providing worked-out examples slow down the\ndeterioration of the quality of answers, the gpt-3.5-turbo and gpt-4 models are\nnot well suited for elementary calculations and reasoning tasks, also when\nbeing red teamed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Buszydlik_A/0/1/0/all/0/1\">Aleksander Buszydlik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobiczek_K/0/1/0/all/0/1\">Karol Dobiczek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okon_M/0/1/0/all/0/1\">Micha&#x142; Teodor Oko&#x144;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skublicki_K/0/1/0/all/0/1\">Konrad Skublicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippmann_P/0/1/0/all/0/1\">Philip Lippmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Argumentation in Waltz's \"Emerging Structure of International Politics''. (arXiv:2401.00366v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00366","description":"<p>We present an annotation scheme for argumentative and domain-specific aspects\nof scholarly articles on the theory of International Relations. At\nargumentation level we identify Claims and Support/Attack relations. At domain\nlevel we model discourse content in terms of Theory and Data-related\nstatements. We annotate Waltz's 1993 text on structural realism and show that\nour scheme can be reliably applied by domain experts enables insights on two\nresearch questions on justifications of claims.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolska_M/0/1/0/all/0/1\">Magdalena Wolska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frohlich_B/0/1/0/all/0/1\">Bernd Fr&#xf6;hlich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girgensohn_K/0/1/0/all/0/1\">Katrin Girgensohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gholiagha_S/0/1/0/all/0/1\">Sassan Gholiagha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiesel_D/0/1/0/all/0/1\">Dora Kiesel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neyer_J/0/1/0/all/0/1\">J&#xfc;rgen Neyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riehmann_P/0/1/0/all/0/1\">Patrick Riehmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sienknecht_M/0/1/0/all/0/1\">Mitja Sienknecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_B/0/1/0/all/0/1\">Benno Stein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Text Embeddings with Large Language Models. (arXiv:2401.00368v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00368","description":"<p>In this paper, we introduce a novel and simple method for obtaining\nhigh-quality text embeddings using only synthetic data and less than 1k\ntraining steps. Unlike existing methods that often depend on multi-stage\nintermediate pre-training with billions of weakly-supervised text pairs,\nfollowed by fine-tuning with a few labeled datasets, our method does not\nrequire building complex training pipelines or relying on manually collected\ndatasets that are often constrained by task diversity and language coverage. We\nleverage proprietary LLMs to generate diverse synthetic data for hundreds of\nthousands of text embedding tasks across nearly 100 languages. We then\nfine-tune open-source decoder-only LLMs on the synthetic data using standard\ncontrastive loss. Experiments demonstrate that our method achieves strong\nperformance on highly competitive text embedding benchmarks without using any\nlabeled data. Furthermore, when fine-tuned with a mixture of synthetic and\nlabeled data, our model sets new state-of-the-art results on the BEIR and MTEB\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1\">Nan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linjun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_R/0/1/0/all/0/1\">Rangan Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Evoked Emotions in Conversations. (arXiv:2401.00383v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00383","description":"<p>Understanding and predicting the emotional trajectory in multi-party\nmulti-turn conversations is of great significance. Such information can be\nused, for example, to generate empathetic response in human-machine interaction\nor to inform models of pre-emptive toxicity detection. In this work, we\nintroduce the novel problem of Predicting Emotions in Conversations (PEC) for\nthe next turn (n+1), given combinations of textual and/or emotion input up to\nturn n. We systematically approach the problem by modeling three dimensions\ninherently connected to evoked emotions in dialogues, including (i) sequence\nmodeling, (ii) self-dependency modeling, and (iii) recency modeling. These\nmodeling dimensions are then incorporated into two deep neural network\narchitectures, a sequence model and a graph convolutional network model. The\nformer is designed to capture the sequence of utterances in a dialogue, while\nthe latter captures the sequence of utterances and the network formation of\nmulti-party dialogues. We perform a comprehensive empirical evaluation of the\nvarious proposed models for addressing the PEC problem. The results indicate\n(i) the importance of the self-dependency and recency model dimensions for the\nprediction task, (ii) the quality of simpler sequence models in short\ndialogues, (iii) the importance of the graph neural models in improving the\npredictions in long dialogues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Altarawneh_E/0/1/0/all/0/1\">Enas Altarawneh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Ameeta Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenkin_M/0/1/0/all/0/1\">Michael Jenkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papagelis_M/0/1/0/all/0/1\">Manos Papagelis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FusionMind -- Improving question and answering with external context fusion. (arXiv:2401.00388v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00388","description":"<p>Answering questions using pre-trained language models (LMs) and knowledge\ngraphs (KGs) presents challenges in identifying relevant knowledge and\nperforming joint reasoning.We compared LMs (fine-tuned for the task) with the\npreviously published QAGNN method for the Question-answering (QA) objective and\nfurther measured the impact of additional factual context on the QAGNN\nperformance. The QAGNN method employs LMs to encode QA context and estimate KG\nnode importance, and effectively update the question choice entity\nrepresentations using Graph Neural Networks (GNNs). We further experimented\nwith enhancing the QA context encoding by incorporating relevant knowledge\nfacts for the question stem. The models are trained on the OpenbookQA dataset,\nwhich contains ~6000 4-way multiple choice questions and is widely used as a\nbenchmark for QA tasks. Through our experimentation, we found that\nincorporating knowledge facts context led to a significant improvement in\nperformance. In contrast, the addition of knowledge graphs to language models\nresulted in only a modest increase. This suggests that the integration of\ncontextual knowledge facts may be more impactful for enhancing question\nanswering performance compared to solely adding knowledge graphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1\">Shreyas Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Manoj Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_P/0/1/0/all/0/1\">Palash Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porwal_S/0/1/0/all/0/1\">Sanchita Porwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models. (arXiv:2401.00396v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00396","description":"<p>Retrieval-augmented generation (RAG) has become a main technique for\nalleviating hallucinations in large language models (LLMs). Despite the\nintegration of RAG, LLMs may still present unsupported or contradictory claims\nto the retrieved contents. In order to develop effective hallucination\nprevention strategies under RAG, it is important to create benchmark datasets\nthat can measure the extent of hallucination. This paper presents RAGTruth, a\ncorpus tailored for analyzing word-level hallucinations in various domains and\ntasks within the standard RAG frameworks for LLM applications. RAGTruth\ncomprises nearly 18,000 naturally generated responses from diverse LLMs using\nRAG. These responses have undergone meticulous manual annotations at both the\nindividual cases and word levels, incorporating evaluations of hallucination\nintensity. We not only benchmark hallucination frequencies across different\nLLMs, but also critically assess the effectiveness of several existing\nhallucination detection methodologies. Furthermore, we show that using a\nhigh-quality dataset such as RAGTruth, it is possible to finetune a relatively\nsmall LLM and achieve a competitive level of performance in hallucination\ndetection when compared to the existing prompt-based approaches using\nstate-of-the-art large language models such as GPT-4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuanhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Juno Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Siliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_K/0/1/0/all/0/1\">Kashun Shum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_C/0/1/0/all/0/1\">Cheng Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">Randy Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Juntong Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SDIF-DA: A Shallow-to-Deep Interaction Framework with Data Augmentation for Multi-modal Intent Detection. (arXiv:2401.00424v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00424","description":"<p>Multi-modal intent detection aims to utilize various modalities to understand\nthe user's intentions, which is essential for the deployment of dialogue\nsystems in real-world scenarios. The two core challenges for multi-modal intent\ndetection are (1) how to effectively align and fuse different features of\nmodalities and (2) the limited labeled multi-modal intent training data. In\nthis work, we introduce a shallow-to-deep interaction framework with data\naugmentation (SDIF-DA) to address the above challenges. Firstly, SDIF-DA\nleverages a shallow-to-deep interaction module to progressively and effectively\nalign and fuse features across text, video, and audio modalities. Secondly, we\npropose a ChatGPT-based data augmentation approach to automatically augment\nsufficient training data. Experimental results demonstrate that SDIF-DA can\neffectively align and fuse multi-modal features by achieving state-of-the-art\nperformance. In addition, extensive analyses show that the introduced data\naugmentation approach can successfully distill knowledge from the large\nlanguage model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shijue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Libo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bingbing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_G/0/1/0/all/0/1\">Geng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"keqing: knowledge-based question answering is a nature chain-of-thought mentor of LLM. (arXiv:2401.00426v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00426","description":"<p>Large language models (LLMs) have exhibited remarkable performance on various\nnatural language processing (NLP) tasks, especially for question answering.\nHowever, in the face of problems beyond the scope of knowledge, these LLMs tend\nto talk nonsense with a straight face, where the potential solution could be\nincorporating an Information Retrieval (IR) module and generating response\nbased on these retrieved knowledge. In this paper, we present a novel framework\nto assist LLMs, such as ChatGPT, to retrieve question-related structured\ninformation on the knowledge graph, and demonstrate that Knowledge-based\nquestion answering (Keqing) could be a nature Chain-of-Thought (CoT) mentor to\nguide the LLM to sequentially find the answer entities of a complex question\nthrough interpretable logical chains. Specifically, the workflow of Keqing will\nexecute decomposing a complex question according to predefined templates,\nretrieving candidate entities on knowledge graph, reasoning answers of\nsub-questions, and finally generating response with reasoning paths, which\ngreatly improves the reliability of LLM's response. The experimental results on\nKBQA datasets show that Keqing can achieve competitive performance and\nillustrate the logic of answering each question.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaojie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yishi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenxi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinrun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Lei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1\">Bo An</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GeoGalactica: A Scientific Large Language Model in Geoscience. (arXiv:2401.00434v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00434","description":"<p>Large language models (LLMs) have achieved huge success for their general\nknowledge and ability to solve a wide spectrum of tasks in natural language\nprocessing (NLP). Due to their impressive abilities, LLMs have shed light on\npotential inter-discipline applications to foster scientific discoveries of a\nspecific domain by using artificial intelligence (AI for science, AI4S). In the\nmeantime, utilizing NLP techniques in geoscience research and practice is wide\nand convoluted, contributing from knowledge extraction and document\nclassification to question answering and knowledge discovery. In this work, we\ntake the initial step to leverage LLM for science, through a rather\nstraightforward approach. We try to specialize an LLM into geoscience, by\nfurther pre-training the model with a vast amount of texts in geoscience, as\nwell as supervised fine-tuning (SFT) the resulting model with our custom\ncollected instruction tuning dataset. These efforts result in a model\nGeoGalactica consisting of 30 billion parameters. To our best knowledge, it is\nthe largest language model for the geoscience domain. More specifically,\nGeoGalactica is from further pre-training of Galactica. We train GeoGalactica\nover a geoscience-related text corpus containing 65 billion tokens curated from\nextensive data sources in the big science project Deep-time Digital Earth\n(DDE), preserving as the largest geoscience-specific text corpus. Then we\nfine-tune the model with 1 million pairs of instruction-tuning data consisting\nof questions that demand professional geoscience knowledge to answer. In this\ntechnical report, we will illustrate in detail all aspects of GeoGalactica,\nincluding data collection, data cleaning, base model selection, pre-training,\nSFT, and evaluation. We open-source our data curation tools and the checkpoints\nof GeoGalactica during the first 3/4 of pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Cheng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Le Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yutong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhongmou He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuanyuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Beiya Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yunchong Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1\">Boyi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1\">Tao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tianyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1\">Luoyi Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junxian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yunqiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinbing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chenghu Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BatchEval: Towards Human-like Text Evaluation. (arXiv:2401.00437v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00437","description":"<p>Significant progress has been made in automatic text evaluation with the\nintroduction of large language models (LLMs) as evaluators. However, current\nsample-wise evaluation paradigm suffers from the following issues: (1)\nSensitive to prompt design; (2) Poor resistance to noise; (3) Inferior ensemble\nperformance with static reference. Inspired by the fact that humans treat both\ncriterion definition and inter sample comparison as references for evaluation,\nwe propose BatchEval, a paradigm that conducts batch-wise evaluation\niteratively to alleviate the above problems. We explore variants under this\nparadigm and confirm the optimal settings are two stage procedure with\nheterogeneous batch composition strategy and decimal scoring format.\nComprehensive experiments across 3 LLMs on 4 text evaluation tasks demonstrate\nthat BatchEval outperforms state-of-the-art methods by 10.5% on Pearson\ncorrelations with only 64% API cost on average. Further analyses have been\nconducted to verify the robustness, generalization, and working mechanism of\nBatchEval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_P/0/1/0/all/0/1\">Peiwen Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shaoxiong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinglin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1\">Boyuan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Heda Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws. (arXiv:2401.00448v1 [cs.LG])","link":"http://arxiv.org/abs/2401.00448","description":"<p>Large language model (LLM) scaling laws are empirical formulas that estimate\nchanges in model quality as a result of increasing parameter count and training\ndata. However, these formulas, including the popular DeepMind Chinchilla\nscaling laws, neglect to include the cost of inference. We modify the\nChinchilla scaling laws to calculate the optimal LLM parameter count and\npre-training data size to train and deploy a model of a given quality and\ninference demand. We conduct our analysis both in terms of a compute budget and\nreal-world costs and find that LLM researchers expecting reasonably large\ninference demand (~1B requests) should train models smaller and longer than\nChinchilla-optimal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sardana_N/0/1/0/all/0/1\">Nikhil Sardana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frankle_J/0/1/0/all/0/1\">Jonathan Frankle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HSC-GPT: A Large Language Model for Human Settlements Construction. (arXiv:2401.00504v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00504","description":"<p>The field of human settlement construction encompasses a range of spatial\ndesigns and management tasks, including urban planning and landscape\narchitecture design. These tasks involve a plethora of instructions and\ndescriptions presented in natural language, which are essential for\nunderstanding design requirements and producing effective design solutions.\nRecent research has sought to integrate natural language processing (NLP) and\ngenerative artificial intelligence (AI) into human settlement construction\ntasks. Due to the efficient processing and analysis capabilities of AI with\ndata, significant successes have been achieved in design within this domain.\nHowever, this task still faces several fundamental challenges. The semantic\ninformation involved includes complex spatial details, diverse data source\nformats, high sensitivity to regional culture, and demanding requirements for\ninnovation and rigor in work scenarios. These factors lead to limitations when\napplying general generative AI in this field, further exacerbated by a lack of\nhigh-quality data for model training. To address these challenges, this paper\nfirst proposes HSC-GPT, a large-scale language model framework specifically\ndesigned for tasks in human settlement construction, considering the unique\ncharacteristics of this domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ran_C/0/1/0/all/0/1\">Chen Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xueqi_Y/0/1/0/all/0/1\">Yao Xueqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuhui_J/0/1/0/all/0/1\">Jiang Xuhui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhengqi_H/0/1/0/all/0/1\">Han Zhengqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jingze_G/0/1/0/all/0/1\">Guo Jingze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xianyue_Z/0/1/0/all/0/1\">Zhang Xianyue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chunyu_L/0/1/0/all/0/1\">Lin Chunyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chumin_L/0/1/0/all/0/1\">Liu Chumin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_Z/0/1/0/all/0/1\">Zhao Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeke_L/0/1/0/all/0/1\">Lian Zeke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jingjing_Z/0/1/0/all/0/1\">Zhang Jingjing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keke_L/0/1/0/all/0/1\">Li Keke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-Task, Multi-Modal Approach for Predicting Categorical and Dimensional Emotions. (arXiv:2401.00536v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00536","description":"<p>Speech emotion recognition (SER) has received a great deal of attention in\nrecent years in the context of spontaneous conversations. While there have been\nnotable results on datasets like the well known corpus of naturalistic dyadic\nconversations, IEMOCAP, for both the case of categorical and dimensional\nemotions, there are few papers which try to predict both paradigms at the same\ntime. Therefore, in this work, we aim to highlight the performance contribution\nof multi-task learning by proposing a multi-task, multi-modal system that\npredicts categorical and dimensional emotions. The results emphasise the\nimportance of cross-regularisation between the two types of emotions. Our\napproach consists of a multi-task, multi-modal architecture that uses parallel\nfeature refinement through self-attention for the feature of each modality. In\norder to fuse the features, our model introduces a set of learnable bridge\ntokens that merge the acoustic and linguistic features with the help of\ncross-attention. Our experiments for categorical emotions on 10-fold validation\nyield results comparable to the current state-of-the-art. In our configuration,\nour multi-task approach provides better results compared to learning each\nparadigm separately. On top of that, our best performing model achieves a high\nresult for valence compared to the previous multi-task experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ispas_A/0/1/0/all/0/1\">Alex-R&#x103;zvan Ispas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deschamps_Berger_T/0/1/0/all/0/1\">Th&#xe9;o Deschamps-Berger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devillers_L/0/1/0/all/0/1\">Laurence Devillers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Networks Against (and For) Self-Training: Classification with Small Labeled and Large Unlabeled Sets. (arXiv:2401.00575v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00575","description":"<p>We propose a semi-supervised text classifier based on self-training using one\npositive and one negative property of neural networks. One of the weaknesses of\nself-training is the semantic drift problem, where noisy pseudo-labels\naccumulate over iterations and consequently the error rate soars. In order to\ntackle this challenge, we reshape the role of pseudo-labels and create a\nhierarchical order of information. In addition, a crucial step in self-training\nis to use the classifier confidence prediction to select the best candidate\npseudo-labels. This step cannot be efficiently done by neural networks, because\nit is known that their output is poorly calibrated. To overcome this challenge,\nwe propose a hybrid metric to replace the plain confidence measurement. Our\nmetric takes into account the prediction uncertainty via a subsampling\ntechnique. We evaluate our model in a set of five standard benchmarks, and show\nthat it significantly outperforms a set of ten diverse baseline models.\nFurthermore, we show that the improvement achieved by our model is additive to\nlanguage model pretraining, which is a widely used technique for using\nunlabeled documents. Our code is available at\nhttps://github.com/p-karisani/RST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karisani_P/0/1/0/all/0/1\">Payam Karisani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Effectiveness of Instruction Tuning in Biomedical Language Processing. (arXiv:2401.00579v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00579","description":"<p>Large Language Models (LLMs), particularly those similar to ChatGPT, have\nsignificantly influenced the field of Natural Language Processing (NLP). While\nthese models excel in general language tasks, their performance in\ndomain-specific downstream tasks such as biomedical and clinical Named Entity\nRecognition (NER), Relation Extraction (RE), and Medical Natural Language\nInference (NLI) is still evolving. In this context, our study investigates the\npotential of instruction tuning for biomedical language processing, applying\nthis technique to two general LLMs of substantial scale. We present a\ncomprehensive, instruction-based model trained on a dataset that consists of\napproximately $200,000$ instruction-focused samples. This dataset represents a\ncarefully curated compilation of existing data, meticulously adapted and\nreformatted to align with the specific requirements of our instruction-based\ntasks. This initiative represents an important step in utilising such models to\nachieve results on par with specialised encoder-only models like BioBERT and\nBioClinicalBERT for various classical biomedical NLP tasks. Our work includes\nan analysis of the dataset's composition and its impact on model performance,\nproviding insights into the intricacies of instruction tuning. By sharing our\ncodes, models, and the distinctively assembled instruction-based dataset, we\nseek to encourage ongoing research and development in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rohanian_O/0/1/0/all/0/1\">Omid Rohanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nouriborji_M/0/1/0/all/0/1\">Mohammadmahdi Nouriborji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1\">David A. Clifton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Analysis of Embedding Layers and Similarity Scores using Siamese Neural Networks. (arXiv:2401.00582v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00582","description":"<p>Large Lanugage Models (LLMs) are gaining increasing popularity in a variety\nof use cases, from language understanding and writing to assistance in\napplication development. One of the most important aspects for optimal\nfuncionality of LLMs is embedding layers. Word embeddings are distributed\nrepresentations of words in a continuous vector space. In the context of LLMs,\nwords or tokens from the input text are transformed into high-dimensional\nvectors using unique algorithms specific to the model. Our research examines\nthe embedding algorithms from leading companies in the industry, such as\nOpenAI, Google's PaLM, and BERT. Using medical data, we have analyzed\nsimilarity scores of each embedding layer, observing differences in performance\namong each algorithm. To enhance each model and provide an additional encoding\nlayer, we also implemented Siamese Neural Networks. After observing changes in\nperformance with the addition of the model, we measured the carbon footage per\nepoch of training. The carbon footprint associated with large language models\n(LLMs) is a significant concern, and should be taken into consideration when\nselecting algorithms for a variety of use cases. Overall, our research compared\nthe accuracy different, leading embedding algorithms and their carbon footage,\nallowing for a holistic review of each embedding algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bingi_Y/0/1/0/all/0/1\">Yash Bingi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yiqiao Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"State of What Art? A Call for Multi-Prompt LLM Evaluation. (arXiv:2401.00595v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00595","description":"<p>Recent advances in large language models (LLMs) have led to the development\nof various evaluation benchmarks. These benchmarks typically rely on a single\ninstruction template for evaluating all LLMs on a specific task. In this paper,\nwe comprehensively analyze the brittleness of results obtained via\nsingle-prompt evaluations across 6.5M instances, involving 20 different LLMs\nand 39 tasks from 3 benchmarks. To improve robustness of the analysis, we\npropose to evaluate LLMs with a set of diverse prompts instead. We discuss\ntailored evaluation metrics for specific use cases (e.g., LLM developers vs.\ndevelopers interested in a specific downstream task), ensuring a more reliable\nand meaningful assessment of LLM capabilities. We then implement these criteria\nand conduct evaluations of multiple models, providing insights into the true\nstrengths and limitations of current LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mizrahi_M/0/1/0/all/0/1\">Moran Mizrahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplan_G/0/1/0/all/0/1\">Guy Kaplan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malkin_D/0/1/0/all/0/1\">Dan Malkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dror_R/0/1/0/all/0/1\">Rotem Dror</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahaf_D/0/1/0/all/0/1\">Dafna Shahaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Personality, Persona, and Profile in Conversational Agents and Chatbots. (arXiv:2401.00609v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00609","description":"<p>We present a review of personality in neural conversational agents (CAs),\nalso called chatbots. First, we define Personality, Persona, and Profile. We\nexplain all personality schemes which have been used in CAs, and list models\nunder the scheme(s) which they use. Second we describe 21 datasets which have\nbeen developed in recent CA personality research. Third, we define the methods\nused to embody personality in a CA, and review recent models using them.\nFourth, we survey some relevant reviews on CAs, personality, and related\ntopics. Finally, we draw conclusions and identify some research challenges for\nthis important emerging field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sutcliffe_R/0/1/0/all/0/1\">Richard Sutcliffe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Anti-microbial Resistance using Large Language Models. (arXiv:2401.00642v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00642","description":"<p>During times of increasing antibiotic resistance and the spread of infectious\ndiseases like COVID-19, it is important to classify genes related to antibiotic\nresistance. As natural language processing has advanced with transformer-based\nlanguage models, many language models that learn characteristics of nucleotide\nsequences have also emerged. These models show good performance in classifying\nvarious features of nucleotide sequences. When classifying nucleotide\nsequences, not only the sequence itself, but also various background knowledge\nis utilized. In this study, we use not only a nucleotide sequence-based\nlanguage model but also a text language model based on PubMed articles to\nreflect more biological background knowledge in the model. We propose a method\nto fine-tune the nucleotide sequence language model and the text language model\nbased on various databases of antibiotic resistance genes. We also propose an\nLLM-based augmentation technique to supplement the data and an ensemble method\nto effectively combine the two models. We also propose a benchmark for\nevaluating the model. Our method achieved better performance than the\nnucleotide sequence language model in the drug resistance class prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_H/0/1/0/all/0/1\">Hyunwoo Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokhansanj_B/0/1/0/all/0/1\">Bahrad Sokhansanj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_J/0/1/0/all/0/1\">James R. Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosen_G/0/1/0/all/0/1\">Gail Rosen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Digger: Detecting Copyright Content Mis-usage in Large Language Model Training. (arXiv:2401.00676v1 [cs.CR])","link":"http://arxiv.org/abs/2401.00676","description":"<p>Pre-training, which utilizes extensive and varied datasets, is a critical\nfactor in the success of Large Language Models (LLMs) across numerous\napplications. However, the detailed makeup of these datasets is often not\ndisclosed, leading to concerns about data security and potential misuse. This\nis particularly relevant when copyrighted material, still under legal\nprotection, is used inappropriately, either intentionally or unintentionally,\ninfringing on the rights of the authors.\n</p>\n<p>In this paper, we introduce a detailed framework designed to detect and\nassess the presence of content from potentially copyrighted books within the\ntraining datasets of LLMs. This framework also provides a confidence estimation\nfor the likelihood of each content sample's inclusion. To validate our\napproach, we conduct a series of simulated experiments, the results of which\naffirm the framework's effectiveness in identifying and addressing instances of\ncontent misuse in LLM training processes. Furthermore, we investigate the\npresence of recognizable quotes from famous literary works within these\ndatasets. The outcomes of our study have significant implications for ensuring\nthe ethical use of copyrighted materials in the development of LLMs,\nhighlighting the need for more transparent and responsible data management\npractices in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haodong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_G/0/1/0/all/0/1\">Gelei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kailong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuekang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guoai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guosheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large language model for Bible sentiment analysis: Sermon on the Mount. (arXiv:2401.00689v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00689","description":"<p>The revolution of natural language processing via large language models has\nmotivated its use in multidisciplinary areas that include social sciences and\nhumanities and more specifically, comparative religion. Sentiment analysis\nprovides a mechanism to study the emotions expressed in text. Recently,\nsentiment analysis has been used to study and compare translations of the\nBhagavad Gita, which is a fundamental and sacred Hindu text. In this study, we\nuse sentiment analysis for studying selected chapters of the Bible. These\nchapters are known as the Sermon on the Mount. We utilize a pre-trained\nlanguage model for sentiment analysis by reviewing five translations of the\nSermon on the Mount, which include the King James version, the New\nInternational Version, the New Revised Standard Version, the Lamsa Version, and\nthe Basic English Version. We provide a chapter-by-chapter and verse-by-verse\ncomparison using sentiment and semantic analysis and review the major\nsentiments expressed. Our results highlight the varying sentiments across the\nchapters and verses. We found that the vocabulary of the respective\ntranslations is significantly different. We detected different levels of\nhumour, optimism, and empathy in the respective chapters that were used by\nJesus to deliver his message.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vora_M/0/1/0/all/0/1\">Mahek Vora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blau_T/0/1/0/all/0/1\">Tom Blau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kachhwal_V/0/1/0/all/0/1\">Vansh Kachhwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solo_A/0/1/0/all/0/1\">Ashu M. G. Solo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rohitash Chandra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Large Language Models on Controllable Generation under Diversified Instructions. (arXiv:2401.00690v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00690","description":"<p>While large language models (LLMs) have exhibited impressive\ninstruction-following capabilities, it is still unclear whether and to what\nextent they can respond to explicit constraints that might be entailed in\nvarious instructions. As a significant aspect of LLM alignment, it is thus\nimportant to formulate such a specialized set of instructions as well as\ninvestigate the resulting behavior of LLMs. To address this vacancy, we propose\na new benchmark CoDI-Eval to systematically and comprehensively evaluate LLMs'\nresponses to instructions with various constraints. We construct a large\ncollection of constraints-attributed instructions as a test suite focused on\nboth generalization and coverage. Specifically, we advocate an instruction\ndiversification process to synthesize diverse forms of constraint expression\nand also deliberate the candidate task taxonomy with even finer-grained\nsub-categories. Finally, we automate the entire evaluation process to\nfacilitate further developments. Different from existing studies on\ncontrollable text generation, CoDI-Eval extends the scope to the prevalent\ninstruction-following paradigm for the first time. We provide extensive\nevaluations of representative LLMs (e.g., ChatGPT, Vicuna) on CoDI-Eval,\nrevealing their limitations in following instructions with specific constraints\nand there is still a significant gap between open-source and commercial\nclosed-source LLMs. We believe this benchmark will facilitate research into\nimproving the controllability of LLMs' responses to instructions. Our data and\ncode are available at https://github.com/Xt-cyh/CoDI-Eval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yihan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Benfeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhendong Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models aren't all that you need. (arXiv:2401.00698v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00698","description":"<p>This paper describes the architecture and systems built towards solving the\nSemEval 2023 Task 2: MultiCoNER II (Multilingual Complex Named Entity\nRecognition) [1]. We evaluate two approaches (a) a traditional Conditional\nRandom Fields model and (b) a Large Language Model (LLM) fine-tuned with a\ncustomized head and compare the two approaches. The novel ideas explored are:\n1) Decaying auxiliary loss (with residual) - where we train the model on an\nauxiliary task of Coarse-Grained NER and include this task as a part of the\nloss function 2) Triplet token blending - where we explore ways of blending the\nembeddings of neighboring tokens in the final NER layer prior to prediction 3)\nTask-optimal heads - where we explore a variety of custom heads and learning\nrates for the final layer of the LLM. We also explore multiple LLMs including\nGPT-3 and experiment with a variety of dropout and other hyperparameter\nsettings before arriving at our final model which achieves micro &amp; macro f1 of\n0.85/0.84 (on dev) and 0.67/0.61 on the test data . We show that while\npre-trained LLMs, by themselves, bring about a large improvement in scores as\ncompared to traditional models, we also demonstrate that tangible improvements\nto the Macro-F1 score can be made by augmenting the LLM with additional\nfeature/loss/model engineering techniques described above.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Holla_K/0/1/0/all/0/1\">Kiran Voderhobli Holla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_C/0/1/0/all/0/1\">Chaithanya Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aryan Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios. (arXiv:2401.00741v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00741","description":"<p>Existing evaluations of tool learning primarily focus on validating the\nalignment of selected tools for large language models (LLMs) with expected\noutcomes. However, these approaches rely on a limited set of scenarios where\nanswers can be pre-determined, diverging from genuine needs. Furthermore, a\nsole emphasis on outcomes disregards the intricate capabilities essential for\nLLMs to effectively utilize tools. To tackle this issue, we propose ToolEyes, a\nfine-grained system tailored for the evaluation of the LLMs' tool learning\ncapabilities in authentic scenarios. The system meticulously examines seven\nreal-world scenarios, analyzing five dimensions crucial to LLMs in tool\nlearning: format alignment, intent comprehension, behavior planning, tool\nselection, and answer organization. Additionally, ToolEyes incorporates a tool\nlibrary boasting approximately 600 tools, serving as an intermediary between\nLLMs and the physical world. Evaluations involving ten LLMs across three\ncategories reveal a preference for specific scenarios and limited cognitive\nabilities in tool learning. Intriguingly, expanding the model size even\nexacerbates the hindrance to tool learning. These findings offer instructive\ninsights aimed at advancing the field of tool learning. The data is available\natt https://github.com/Junjie-Ye/ToolEyes.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Junjie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Songyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Caishuang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yilong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sixian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiaoran Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1\">Shihan Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Translation Testing via Syntactic Tree Pruning. (arXiv:2401.00751v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00751","description":"<p>Machine translation systems have been widely adopted in our daily life,\nmaking life easier and more convenient. Unfortunately, erroneous translations\nmay result in severe consequences, such as financial losses. This requires to\nimprove the accuracy and the reliability of machine translation systems.\nHowever, it is challenging to test machine translation systems because of the\ncomplexity and intractability of the underlying neural models. To tackle these\nchallenges, we propose a novel metamorphic testing approach by syntactic tree\npruning (STP) to validate machine translation systems. Our key insight is that\na pruned sentence should have similar crucial semantics compared with the\noriginal sentence. Specifically, STP (1) proposes a core semantics-preserving\npruning strategy by basic sentence structure and dependency relations on the\nlevel of syntactic tree representation; (2) generates source sentence pairs\nbased on the metamorphic relation; (3) reports suspicious issues whose\ntranslations break the consistency property by a bag-of-words model. We further\nevaluate STP on two state-of-the-art machine translation systems (i.e., Google\nTranslate and Bing Microsoft Translator) with 1,200 source sentences as inputs.\nThe results show that STP can accurately find 5,073 unique erroneous\ntranslations in Google Translate and 5,100 unique erroneous translations in\nBing Microsoft Translator (400% more than state-of-the-art techniques), with\n64.5% and 65.4% precision, respectively. The reported erroneous translations\nvary in types and more than 90% of them cannot be found by state-of-the-art\ntechniques. There are 9,393 erroneous translations unique to STP, which is\n711.9% more than state-of-the-art techniques. Moreover, STP is quite effective\nto detect translation errors for the original sentences with a recall reaching\n74.0%, improving state-of-the-art techniques by 55.1% on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_J/0/1/0/all/0/1\">Juan Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Chunrong Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weisong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Haichuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A & B == B & A: Triggering Logical Reasoning Failures in Large Language Models. (arXiv:2401.00757v1 [cs.SE])","link":"http://arxiv.org/abs/2401.00757","description":"<p>Recent advancements in large language models (LLMs) have propelled Artificial\nIntelligence (AI) to new heights, enabling breakthroughs in various tasks such\nas writing assistance, code generation, and machine translation. A significant\ndistinction of advanced LLMs, such as ChatGPT, is their demonstrated ability to\n\"reason.\" However, evaluating the reasoning ability of LLMs remains a challenge\nas most existing evaluations focus on their accuracy on the downstream tasks\nrather than directly assessing their reasoning processes. Efforts have been\nmade to develop benchmarks and metrics to assess reasoning in LLMs, but they\nsuffer from data leakage or limited scope. In this paper, we introduce\nLogicAsker, an automatic approach that comprehensively evaluates and improves\nthe logical reasoning abilities of LLMs under a set of atomic reasoning skills\nbased on propositional and predicate logic. The results provide insights into\nLLMs' reasoning abilities and reveal the logical rules the LLMs did not learn\nwell. We evaluate LogicAsker on six widely deployed LLMs, including GPT-3,\nChatGPT, GPT-4, Bard, Vicuna, and Guanaco. The results show that test cases\nfrom LogicAsker can find logical reasoning failures in different LLMs with a\nrate of 25\\% - 94\\%. In addition, the test cases of LogicAsker can be further\nused to design demonstration examples for in-context learning, which\neffectively improves the logical reasoning ability of LLMs, e.g., 10\\% for\nGPT-4. As far as we know, our work is the first to create prompts based on\ntesting results to improve LLMs' formal reasoning ability effectively. All the\ncode, data, and results will be released for reproduction and future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yuxuan Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiliu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Youliang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jen-tse Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pinjia He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenxiang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael R. Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Earth is Flat? Unveiling Factual Errors in Large Language Models. (arXiv:2401.00761v1 [cs.SE])","link":"http://arxiv.org/abs/2401.00761","description":"<p>Large Language Models (LLMs) like ChatGPT are foundational in various\napplications due to their extensive knowledge from pre-training and\nfine-tuning. Despite this, they are prone to generating factual and commonsense\nerrors, raising concerns in critical areas like healthcare, journalism, and\neducation to mislead users. Current methods for evaluating LLMs' veracity are\nlimited by test data leakage or the need for extensive human labor, hindering\nefficient and accurate error detection. To tackle this problem, we introduce a\nnovel, automatic testing framework, FactChecker, aimed at uncovering factual\ninaccuracies in LLMs. This framework involves three main steps: First, it\nconstructs a factual knowledge graph by retrieving fact triplets from a\nlarge-scale knowledge database. Then, leveraging the knowledge graph,\nFactChecker employs a rule-based approach to generates three types of questions\n(Yes-No, Multiple-Choice, and WH questions) that involve single-hop and\nmulti-hop relations, along with correct answers. Lastly, it assesses the LLMs'\nresponses for accuracy using tailored matching strategies for each question\ntype. Our extensive tests on six prominent LLMs, including text-davinci-002,\ntext-davinci-003, ChatGPT~(gpt-3.5-turbo, gpt-4), Vicuna, and LLaMA-2, reveal\nthat FactChecker can trigger factual errors in up to 45\\% of questions in these\nmodels. Moreover, we demonstrate that FactChecker's test cases can improve\nLLMs' factual accuracy through in-context learning and fine-tuning (e.g.,\nllama-2-13b-chat's accuracy increase from 35.3\\% to 68.5\\%). We are making all\ncode, data, and results available for future research endeavors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Juluan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Youliang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jen-tse Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenxiang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael R. Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"New Job, New Gender? Measuring the Social Bias in Image Generation Models. (arXiv:2401.00763v1 [cs.SE])","link":"http://arxiv.org/abs/2401.00763","description":"<p>Image generation models can generate or edit images from a given text. Recent\nadvancements in image generation technology, exemplified by DALL-E and\nMidjourney, have been groundbreaking. These advanced models, despite their\nimpressive capabilities, are often trained on massive Internet datasets, making\nthem susceptible to generating content that perpetuates social stereotypes and\nbiases, which can lead to severe consequences. Prior research on assessing bias\nwithin image generation models suffers from several shortcomings, including\nlimited accuracy, reliance on extensive human labor, and lack of comprehensive\nanalysis. In this paper, we propose BiasPainter, a novel metamorphic testing\nframework that can accurately, automatically and comprehensively trigger social\nbias in image generation models. BiasPainter uses a diverse range of seed\nimages of individuals and prompts the image generation models to edit these\nimages using gender, race, and age-neutral queries. These queries span 62\nprofessions, 39 activities, 57 types of objects, and 70 personality traits. The\nframework then compares the edited images to the original seed images, focusing\non any changes related to gender, race, and age. BiasPainter adopts a testing\noracle that these characteristics should not be modified when subjected to\nneutral prompts. Built upon this design, BiasPainter can trigger the social\nbias and evaluate the fairness of image generation models. To evaluate the\neffectiveness of BiasPainter, we use BiasPainter to test five widely-used\ncommercial image generation software and models, such as stable diffusion and\nMidjourney. Experimental results show that 100\\% of the generated test cases\ncan successfully trigger social bias in image generation models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Haonan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jen-tse Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yuxuan Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Youliang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Haoyi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael R. Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Validity Change Prediction. (arXiv:2401.00779v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00779","description":"<p>Temporal validity is an important property of text that is useful for many\ndownstream applications, such as recommender systems, conversational AI, or\nstory understanding. Existing benchmarking tasks often require models to\nidentify the temporal validity duration of a single statement. However, in many\ncases, additional contextual information, such as sentences in a story or posts\non a social media profile, can be collected from the available text stream.\nThis contextual information may greatly alter the duration for which a\nstatement is expected to be valid. We propose Temporal Validity Change\nPrediction, a natural language processing task benchmarking the capability of\nmachine learning models to detect contextual statements that induce such\nchange. We create a dataset consisting of temporal target statements sourced\nfrom Twitter and crowdsource sample context statements. We then benchmark a set\nof transformer-based language models on our dataset. Finally, we experiment\nwith temporal validity duration prediction as an auxiliary task to improve the\nperformance of the state-of-the-art model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wenzel_G/0/1/0/all/0/1\">Georg Wenzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1\">Adam Jatowt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models. (arXiv:2401.00788v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00788","description":"<p>The high cost of full-parameter fine-tuning (FFT) of Large Language Models\n(LLMs) has led to a series of parameter-efficient fine-tuning (PEFT) methods.\nHowever, it remains unclear which methods provide the best cost-performance\ntrade-off at different model scales. We introduce Astraios, a suite of 28\ninstruction-tuned OctoCoder models using 7 tuning methods and 4 model sizes up\nto 16 billion parameters. Through investigations across 5 tasks and 8 different\ndatasets encompassing both code comprehension and code generation tasks, we\nfind that FFT generally leads to the best downstream performance across all\nscales, and PEFT methods differ significantly in their efficacy based on the\nmodel scale. LoRA usually offers the most favorable trade-off between cost and\nperformance. Further investigation into the effects of these methods on both\nmodel robustness and code security reveals that larger models tend to\ndemonstrate reduced robustness and less security. At last, we explore the\nrelationships among updated parameters, cross-entropy loss, and task\nperformance. We find that the tuning effectiveness observed in small models\ngeneralizes well to larger models, and the validation loss in instruction\ntuning can be a reliable indicator of overall downstream performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1\">Terry Yue Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zebaze_A/0/1/0/all/0/1\">Armel Zebaze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suppattarachai_N/0/1/0/all/0/1\">Nitchakarn Suppattarachai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werra_L/0/1/0/all/0/1\">Leandro von Werra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vries_H/0/1/0/all/0/1\">Harm de Vries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1\">Niklas Muennighoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models. (arXiv:2401.00793v1 [cs.LG])","link":"http://arxiv.org/abs/2401.00793","description":"<p>With the growing use of large language models hosted on cloud platforms to\noffer inference services, privacy concerns are escalating, especially\nconcerning sensitive data like investment plans and bank account details.\nSecure Multi-Party Computing (SMPC) emerges as a promising solution to protect\nthe privacy of inference data and model parameters. However, the application of\nSMPC in Privacy-Preserving Inference (PPI) for large language models,\nparticularly those based on the Transformer architecture, often leads to\nconsiderable slowdowns or declines in performance. This is largely due to the\nmultitude of nonlinear operations in the Transformer architecture, which are\nnot well-suited to SMPC and are difficult to circumvent or optimize\neffectively. To address this concern, we introduce an advanced optimization\nframework called SecFormer, designed to strike an optimal balance between\nperformance and efficiency in PPI for Transformer models. By implementing\nknowledge distillation techniques, we successfully eliminate the high-cost\nexponential and maximum operations in PPI without sacrificing model\nperformance. Additionally, we have developed a suite of efficient SMPC\nprotocols that utilize segmented polynomials and Goldschmidt's method to handle\nother complex nonlinear functions within PPI, such as GeLU, LayerNorm, and\nSoftmax. Our extensive experiments reveal that SecFormer outperforms MPCFormer\nin performance, showing improvements of $5.6\\%$ and $24.2\\%$ for\nBERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$, respectively. In terms of\nefficiency, SecFormer is 3.4 and 3.2 times faster than Puma, demonstrating its\neffectiveness and speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jinglong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yehong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_X/0/1/0/all/0/1\">Xin Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PerSHOP -- A Persian dataset for shopping dialogue systems modeling. (arXiv:2401.00811v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00811","description":"<p>Nowadays, dialogue systems are used in many fields of industry and research.\nThere are successful instances of these systems, such as Apple Siri, Google\nAssistant, and IBM Watson. Task-oriented dialogue system is a category of\nthese, that are used in specific tasks. They can perform tasks such as booking\nplane tickets or making restaurant reservations. Shopping is one of the most\npopular areas on these systems. The bot replaces the human salesperson and\ninteracts with the customers by speaking. To train the models behind the scenes\nof these systems, annotated data is needed. In this paper, we developed a\ndataset of dialogues in the Persian language through crowd-sourcing. We\nannotated these dialogues to train a model. This dataset contains nearly 22k\nutterances in 15 different domains and 1061 dialogues. This is the largest\nPersian dataset in this field, which is provided freely so that future\nresearchers can use it. Also, we proposed some baseline models for natural\nlanguage understanding (NLU) tasks. These models perform two tasks for NLU:\nintent classification and entity extraction. The F-1 score metric obtained for\nintent classification is around 91% and for entity extraction is around 93%,\nwhich can be a baseline for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahmoudi_K/0/1/0/all/0/1\">Keyvan Mahmoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faili_H/0/1/0/all/0/1\">Heshaam Faili</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents. (arXiv:2401.00812v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00812","description":"<p>The prominent large language models (LLMs) of today differ from past language\nmodels not only in size, but also in the fact that they are trained on a\ncombination of natural language and formal language (code). As a medium between\nhumans and computers, code translates high-level goals into executable steps,\nfeaturing standard syntax, logical consistency, abstraction, and modularity. In\nthis survey, we present an overview of the various benefits of integrating code\ninto LLMs' training data. Specifically, beyond enhancing LLMs in code\ngeneration, we observe that these unique properties of code help (i) unlock the\nreasoning ability of LLMs, enabling their applications to a range of more\ncomplex natural language tasks; (ii) steer LLMs to produce structured and\nprecise intermediate steps, which can then be connected to external execution\nends through function calls; and (iii) take advantage of code compilation and\nexecution environment, which also provides diverse feedback for model\nimprovement. In addition, we trace how these profound capabilities of LLMs,\nbrought by code, have led to their emergence as intelligent agents (IAs) in\nsituations where the ability to understand instructions, decompose goals, plan\nand execute actions, and refine from feedback are crucial to their success on\ndownstream tasks. Finally, we present several key challenges and future\ndirections of empowering LLMs with code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Ke Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiateng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">John Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chaoqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_Y/0/1/0/all/0/1\">Yi R. Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zixuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiquan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">Chengxiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Computational Framework for Behavioral Assessment of LLM Therapists. (arXiv:2401.00820v1 [cs.CL])","link":"http://arxiv.org/abs/2401.00820","description":"<p>The emergence of ChatGPT and other large language models (LLMs) has greatly\nincreased interest in utilizing LLMs as therapists to support individuals\nstruggling with mental health challenges. However, due to the lack of\nsystematic studies, our understanding of how LLM therapists behave, i.e., ways\nin which they respond to clients, is significantly limited. Understanding their\nbehavior across a wide range of clients and situations is crucial to accurately\nassess their capabilities and limitations in the high-risk setting of mental\nhealth, where undesirable behaviors can lead to severe consequences. In this\npaper, we propose BOLT, a novel computational framework to study the\nconversational behavior of LLMs when employed as therapists. We develop an\nin-context learning method to quantitatively measure the behavior of LLMs based\non 13 different psychotherapy techniques including reflections, questions,\nsolutions, normalizing, and psychoeducation. Subsequently, we compare the\nbehavior of LLM therapists against that of high- and low-quality human therapy,\nand study how their behavior can be modulated to better reflect behaviors\nobserved in high-quality therapy. Our analysis of GPT and Llama-variants\nreveals that these LLMs often resemble behaviors more commonly exhibited in\nlow-quality therapy rather than high-quality therapy, such as offering a higher\ndegree of problem-solving advice when clients share emotions, which is against\ntypical recommendations. At the same time, unlike low-quality therapy, LLMs\nreflect significantly more upon clients' needs and strengths. Our analysis\nframework suggests that despite the ability of LLMs to generate anecdotal\nexamples that appear similar to human therapists, LLM therapists are currently\nnot fully consistent with high-quality care, and thus require additional\nresearch to ensure quality care.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiu_Y/0/1/0/all/0/1\">Yu Ying Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Ashish Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_I/0/1/0/all/0/1\">Inna Wanyin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Althoff_T/0/1/0/all/0/1\">Tim Althoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph-Convolutional Autoencoder Ensembles for the Humanities, Illustrated with a Study of the American Slave Trade. (arXiv:2401.00824v1 [cs.LG])","link":"http://arxiv.org/abs/2401.00824","description":"<p>We introduce a graph-aware autoencoder ensemble framework, with associated\nformalisms and tooling, designed to facilitate deep learning for scholarship in\nthe humanities. By composing sub-architectures to produce a model isomorphic to\na humanistic domain we maintain interpretability while providing function\nsignatures for each sub-architectural choice, allowing both traditional and\ncomputational researchers to collaborate without disrupting established\npractices. We illustrate a practical application of our approach to a\nhistorical study of the American post-Atlantic slave trade, and make several\nspecific technical contributions: a novel hybrid graph-convolutional\nautoencoder mechanism, batching policies for common graph topologies, and\nmasking techniques for particular use-cases. The effectiveness of the framework\nfor broadening participation of diverse domains is demonstrated by a growing\nsuite of two dozen studies, both collaborations with humanists and established\ntasks from machine learning literature, spanning a variety of fields and data\nmodalities. We make performance comparisons of several different architectural\nchoices and conclude with an ambitious list of imminent next steps for this\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lippincott_T/0/1/0/all/0/1\">Tom Lippincott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BSpell: A CNN-Blended BERT Based Bangla Spell Checker. (arXiv:2208.09709v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.09709","description":"<p>Bangla typing is mostly performed using English keyboard and can be highly\nerroneous due to the presence of compound and similarly pronounced letters.\nSpelling correction of a misspelled word requires understanding of word typing\npattern as well as the context of the word usage. A specialized BERT model\nnamed BSpell has been proposed in this paper targeted towards word for word\ncorrection in sentence level. BSpell contains an end-to-end trainable CNN\nsub-model named SemanticNet along with specialized auxiliary loss. This allows\nBSpell to specialize in highly inflected Bangla vocabulary in the presence of\nspelling errors. Furthermore, a hybrid pretraining scheme has been proposed for\nBSpell that combines word level and character level masking. Comparison on two\nBangla and one Hindi spelling correction dataset shows the superiority of our\nproposed approach. BSpell is available as a Bangla spell checking tool via\nGitHub: https://github.com/Hasiburshanto/Bangla-Spell-Checker\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_C/0/1/0/all/0/1\">Chowdhury Rafeed Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">MD. Hasibur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakir_S/0/1/0/all/0/1\">Samiha Zakir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafsan_M/0/1/0/all/0/1\">Mohammad Rafsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mohammed Eunus Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physical Computing: A Category Theoretic Perspective on Physical Computation and System Compositionality. (arXiv:2210.00392v4 [quant-ph] UPDATED)","link":"http://arxiv.org/abs/2210.00392","description":"<p>This paper introduces a category theory-based framework to redefine physical\ncomputing in light of advancements in quantum computing and non-standard\ncomputing systems. By integrating classical definitions within this broader\nperspective, the paper rigorously recontextualizes what constitutes physical\ncomputing devices and processes. It demonstrates how the compositional nature\nand relational structures of physical computing systems can be coherently\nformalized using category theory. This approach not only encapsulates recent\nformalisms in physical computing but also offers a structured method to explore\nthe dynamic interactions within these systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Dehghani_N/0/1/0/all/0/1\">Nima Dehghani</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Caterina_G/0/1/0/all/0/1\">Gianluca Caterina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring AI-Generated Text in Student Writing: How Does AI Help?. (arXiv:2304.02478v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.02478","description":"<p>English as foreign language_EFL_students' use of text generated from\nartificial intelligence_AI_natural language generation_NLG_tools may improve\ntheir writing quality. However, it remains unclear to what extent AI-generated\ntext in these students' writing might lead to higher-quality writing. We\nexplored 23 Hong Kong secondary school students' attempts to write stories\ncomprising their own words and AI-generated text. Human experts scored the\nstories for dimensions of content, language and organization. We analyzed the\nbasic organization and structure and syntactic complexity of the stories'\nAI-generated text and performed multiple linear regression and cluster\nanalyses. The results show the number of human words and the number of\nAI-generated words contribute significantly to scores. Besides, students can be\ngrouped into competent and less competent writers who use more AI-generated\ntext or less AI-generated text compared to their peers. Comparisons of clusters\nreveal some benefit of AI-generated text in improving the quality of both\nhigh-scoring students' and low-scoring students' writing. The findings can\ninform pedagogical strategies to use AI-generated text for EFL students'\nwriting and to address digital divides. This study contributes designs of NLG\ntools and writing activities to implement AI-generated text in schools.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Woo_D/0/1/0/all/0/1\">David James Woo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Susanto_H/0/1/0/all/0/1\">Hengky Susanto</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_C/0/1/0/all/0/1\">Chi Ho Yeung</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1\">Kai Guo</a> (3), (4) <a href=\"http://arxiv.org/find/cs/1/au:+Fung_A/0/1/0/all/0/1\">April Ka Yeng Fung</a> ((1) Precious Blood Secondary School, Hong Kong, (2) Department of Science and Environmental Studies, The Education University of Hong Kong, Hong Kong, (3) Faculty of Education, The University of Hong Kong, Hong Kong, and (4) Hoi Ping Chamber of Commerce Secondary School, Hong Kong)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WikiSQE: A Large-Scale Dataset for Sentence Quality Estimation in Wikipedia. (arXiv:2305.05928v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.05928","description":"<p>Wikipedia can be edited by anyone and thus contains various quality\nsentences. Therefore, Wikipedia includes some poor-quality edits, which are\noften marked up by other editors. While editors' reviews enhance the\ncredibility of Wikipedia, it is hard to check all edited text. Assisting in\nthis process is very important, but a large and comprehensive dataset for\nstudying it does not currently exist. Here, we propose WikiSQE, the first\nlarge-scale dataset for sentence quality estimation in Wikipedia. Each sentence\nis extracted from the entire revision history of English Wikipedia, and the\ntarget quality labels were carefully investigated and selected. WikiSQE has\nabout 3.4 M sentences with 153 quality labels. In the experiment with automatic\nclassification using competitive machine learning models, sentences that had\nproblems with citation, syntax/semantics, or propositions were found to be more\ndifficult to detect. In addition, by performing human annotation, we found that\nthe model we developed performed better than the crowdsourced workers. WikiSQE\nis expected to be a valuable resource for other tasks in NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ando_K/0/1/0/all/0/1\">Kenichiro Ando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekine_S/0/1/0/all/0/1\">Satoshi Sekine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komachi_M/0/1/0/all/0/1\">Mamoru Komachi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Symbol tuning improves in-context learning in language models. (arXiv:2305.08298v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08298","description":"<p>We present symbol tuning - finetuning language models on in-context\ninput-label pairs where natural language labels (e.g., \"positive/negative\nsentiment\") are replaced with arbitrary symbols (e.g., \"foo/bar\"). Symbol\ntuning leverages the intuition that when a model cannot use instructions or\nnatural language labels to figure out a task, it must instead do so by learning\nthe input-label mappings.\n</p>\n<p>We experiment with symbol tuning across Flan-PaLM models up to 540B\nparameters and observe benefits across various settings. First, symbol tuning\nboosts performance on unseen in-context learning tasks and is much more robust\nto underspecified prompts, such as those without instructions or without\nnatural language labels. Second, symbol-tuned models are much stronger at\nalgorithmic reasoning tasks, with up to 18.2% better performance on the List\nFunctions benchmark and up to 15.3% better performance on the Simple Turing\nConcepts benchmark. Finally, symbol-tuned models show large improvements in\nfollowing flipped-labels presented in-context, meaning that they are more\ncapable of using in-context information to override prior semantic knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jerry Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Le Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampinen_A/0/1/0/all/0/1\">Andrew Lampinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangning Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Da Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yifeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Large Language Models Infer Causation from Correlation?. (arXiv:2306.05836v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.05836","description":"<p>Causal inference is one of the hallmarks of human intelligence. While the\nfield of CausalNLP has attracted much interest in the recent years, existing\ncausal inference datasets in NLP primarily rely on discovering causality from\nempirical knowledge (e.g., commonsense knowledge). In this work, we propose the\nfirst benchmark dataset to test the pure causal inference skills of large\nlanguage models (LLMs). Specifically, we formulate a novel task Corr2Cause,\nwhich takes a set of correlational statements and determines the causal\nrelationship between the variables. We curate a large-scale dataset of more\nthan 200K samples, on which we evaluate seventeen existing LLMs. Through our\nexperiments, we identify a key shortcoming of LLMs in terms of their causal\ninference skills, and show that these models achieve almost close to random\nperformance on the task. This shortcoming is somewhat mitigated when we try to\nre-purpose LLMs for this skill via finetuning, but we find that these models\nstill fail to generalize -- they can only perform causal inference in\nin-distribution settings when variable names and textual expressions used in\nthe queries are similar to those in the training set, but fail in\nout-of-distribution settings generated by perturbing these queries. Corr2Cause\nis a challenging task for LLMs, and would be helpful in guiding future research\non improving LLMs' pure reasoning skills and generalizability. Our data is at\nhttps://huggingface.co/datasets/causalnlp/corr2cause. Our code is at\nhttps://github.com/causalNLP/corr2cause.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiarui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1\">Zhiheng Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poff_S/0/1/0/all/0/1\">Spencer Poff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1\">Mona Diab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT Involved in Texts? Measure the Polish Ratio to Detect ChatGPT-Generated Text. (arXiv:2307.11380v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.11380","description":"<p>The remarkable capabilities of large-scale language models, such as ChatGPT,\nin text generation have impressed readers and spurred researchers to devise\ndetectors to mitigate potential risks, including misinformation, phishing, and\nacademic dishonesty. Despite this, most previous studies have been\npredominantly geared towards creating detectors that differentiate between\npurely ChatGPT-generated texts and human-authored texts. This approach,\nhowever, fails to work on discerning texts generated through human-machine\ncollaboration, such as ChatGPT-polished texts. Addressing this gap, we\nintroduce a novel dataset termed HPPT (ChatGPT-polished academic abstracts),\nfacilitating the construction of more robust detectors. It diverges from extant\ncorpora by comprising pairs of human-written and ChatGPT-polished abstracts\ninstead of purely ChatGPT-generated texts. Additionally, we propose the \"Polish\nRatio\" method, an innovative measure of the degree of modification made by\nChatGPT compared to the original human-written text. It provides a mechanism to\nmeasure the degree of ChatGPT influence in the resulting text. Our experimental\nresults show our proposed model has better robustness on the HPPT dataset and\ntwo existing datasets (HC3 and CDB). Furthermore, the \"Polish Ratio\" we\nproposed offers a more comprehensive explanation by quantifying the degree of\nChatGPT involvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lingyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1\">Feng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation. (arXiv:2308.07931v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2308.07931","description":"<p>Self-supervised and language-supervised image models contain rich knowledge\nof the world that is important for generalization. Many robotic tasks, however,\nrequire a detailed understanding of 3D geometry, which is often lacking in 2D\nimage features. This work bridges this 2D-to-3D gap for robotic manipulation by\nleveraging distilled feature fields to combine accurate 3D geometry with rich\nsemantics from 2D foundation models. We present a few-shot learning method for\n6-DOF grasping and placing that harnesses these strong spatial and semantic\npriors to achieve in-the-wild generalization to unseen objects. Using features\ndistilled from a vision-language model, CLIP, we present a way to designate\nnovel objects for manipulation via free-text natural language, and demonstrate\nits ability to generalize to unseen expressions and novel categories of\nobjects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">William Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Ge Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Alan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Jansen Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1\">Leslie Pack Kaelbling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empower Nested Boolean Logic via Self-Supervised Curriculum Learning. (arXiv:2310.05450v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.05450","description":"<p>Beyond the great cognitive powers showcased by language models, it is crucial\nto scrutinize whether their reasoning capabilities stem from strong\ngeneralization or merely exposure to relevant data. As opposed to constructing\nincreasingly complex logic, this paper probes into the boolean logic, the root\ncapability of a logical reasoner. We find that any pre-trained language models\neven including large language models only behave like a random selector in the\nface of multi-nested boolean logic, a task that humans can handle with ease. To\nempower language models with this fundamental capability, this paper proposes a\nnew self-supervised learning method \\textit{Curriculum Logical Reasoning}\n(\\textsc{Clr}), where we augment the training data with nested boolean logic\nchain step-by-step, and program the training from simpler logical patterns\ngradually to harder ones. This new training paradigm allows language models to\neffectively generalize to much harder and longer-hop logic, which can hardly be\nlearned through naive training. Furthermore, we show that boolean logic is a\ngreat foundation for improving the subsequent general logical tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hongqiu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linfeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Experimenting AI Technologies for Disinformation Combat: the IDMO Project. (arXiv:2310.11097v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.11097","description":"<p>The Italian Digital Media Observatory (IDMO) project, part of a European\ninitiative, focuses on countering disinformation and fake news. This report\noutlines contributions from Rai-CRITS to the project, including: (i) the\ncreation of novel datasets for testing technologies (ii) development of an\nautomatic model for categorizing Pagella Politica verdicts to facilitate\nbroader analysis (iii) creation of an automatic model for recognizing textual\nentailment with exceptional accuracy on the FEVER dataset (iv) assessment using\nGPT-4 to detecting content treatment style (v) a game to raise awareness about\nfake news at national events.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Canale_L/0/1/0/all/0/1\">Lorenzo Canale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messina_A/0/1/0/all/0/1\">Alberto Messina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Online Stock Forecasting Utilizing Integrated Quantitative and Qualitative Analysis. (arXiv:2311.15218v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.15218","description":"<p>The application of Machine learning to finance has become a familiar\napproach, even more so in stock market forecasting. The stock market is highly\nvolatile, and huge amounts of data are generated every minute globally. The\nextraction of effective intelligence from this data is of critical importance.\nHowever, a collaboration of numerical stock data with qualitative text data can\nbe a challenging task. In this work, we accomplish this by providing an\nunprecedented, publicly available dataset with technical and fundamental data\nand sentiment that we gathered from news archives, TV news captions, radio\ntranscripts, tweets, daily financial newspapers, etc. The text data entries\nused for sentiment extraction total more than 1.4 Million. The dataset consists\nof daily entries from January 2018 to December 2022 for eight companies\nrepresenting diverse industrial sectors and the Dow Jones Industrial Average\n(DJIA) as a whole. Holistic Fundamental and Technical data is provided training\nready for Model learning and deployment. Most importantly, the data generated\ncould be used for incremental online learning with real-time data points\nretrieved daily since no stagnant data was utilized. All the data was retired\nfrom APIs or self-designed robust information retrieval technologies. These\nadaptable technologies facilitate data extraction for any stock. Moreover, the\nutilization of Spearman's rank correlation over real-time data, linking stock\nreturns with sentiment analysis has produced noteworthy results for the DJIA,\nachieving accuracy levels surpassing 60\\%. The dataset is made available at\nhttps://github.com/batking24/Huge-Stock-Dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bathini_S/0/1/0/all/0/1\">Sai Akash Bathini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cihan_D/0/1/0/all/0/1\">Dagli Cihan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conceptualizing Suicidal Behavior: Utilizing Explanations of Predicted Outcomes to Analyze Longitudinal Social Media Data. (arXiv:2312.08299v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.08299","description":"<p>The COVID-19 pandemic has escalated mental health crises worldwide, with\nsocial isolation and economic instability contributing to a rise in suicidal\nbehavior. Suicide can result from social factors such as shame, abuse,\nabandonment, and mental health conditions like depression, Post-Traumatic\nStress Disorder (PTSD), Attention-Deficit/Hyperactivity Disorder (ADHD),\nanxiety disorders, and bipolar disorders. As these conditions develop, signs of\nsuicidal ideation may manifest in social media interactions. Analyzing social\nmedia data using artificial intelligence (AI) techniques can help identify\npatterns of suicidal behavior, providing invaluable insights for suicide\nprevention agencies, professionals, and broader community awareness\ninitiatives. Machine learning algorithms for this purpose require large volumes\nof accurately labeled data. Previous research has not fully explored the\npotential of incorporating explanations in analyzing and labeling longitudinal\nsocial media data. In this study, we employed a model explanation method, Layer\nIntegrated Gradients, on top of a fine-tuned state-of-the-art language model,\nto assign each token from Reddit users' posts an attribution score for\npredicting suicidal ideation. By extracting and analyzing attributions of\ntokens from the data, we propose a methodology for preliminary screening of\nsocial media posts for suicidal ideation without using large language models\nduring inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Van Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nur_N/0/1/0/all/0/1\">Nasheen Nur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stern_W/0/1/0/all/0/1\">William Stern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mercer_T/0/1/0/all/0/1\">Thomas Mercer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_C/0/1/0/all/0/1\">Chiradeep Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_S/0/1/0/all/0/1\">Siddhartha Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tumbiolo_V/0/1/0/all/0/1\">Victor Tumbiolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_S/0/1/0/all/0/1\">Seng Jhing Goh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forbidden Facts: An Investigation of Competing Objectives in Llama-2. (arXiv:2312.08793v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2312.08793","description":"<p>LLMs often face competing pressures (for example helpfulness vs.\nharmlessness). To understand how models resolve such conflicts, we study\nLlama-2-chat models on the forbidden fact task. Specifically, we instruct\nLlama-2 to truthfully complete a factual recall statement while forbidding it\nfrom saying the correct answer. This often makes the model give incorrect\nanswers. We decompose Llama-2 into 1000+ components, and rank each one with\nrespect to how useful it is for forbidding the correct answer. We find that in\naggregate, around 35 components are enough to reliably implement the full\nsuppression behavior. However, these components are fairly heterogeneous and\nmany operate using faulty heuristics. We discover that one of these heuristics\ncan be exploited via a manually designed adversarial attack which we call The\nCalifornia Attack. Our results highlight some roadblocks standing in the way of\nbeing able to successfully interpret advanced ML systems. Project website\navailable at https://forbiddenfacts.github.io .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tony T. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Miles Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hariharan_K/0/1/0/all/0/1\">Kaivalya Hariharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shavit_N/0/1/0/all/0/1\">Nir Shavit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Mindset: An MBTI Exploration of Large Language Models. (arXiv:2312.12999v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.12999","description":"<p>We present a novel approach for integrating Myers-Briggs Type Indicator\n(MBTI) personality traits into large language models (LLMs), addressing the\nchallenges of personality consistency in personalized AI. Our method, \"Machine\nMindset,\" involves a two-phase fine-tuning and Direct Preference Optimization\n(DPO) to embed MBTI traits into LLMs. This approach ensures that models\ninternalize these traits, offering a stable and consistent personality profile.\nWe demonstrate the effectiveness of our models across various domains, showing\nalignment between model performance and their respective MBTI traits. The paper\nhighlights significant contributions in the development of personality datasets\nand a new training methodology for personality integration in LLMs, enhancing\nthe potential for personalized AI applications. We also open-sourced our model\nand part of the data at \\url{https://github.com/PKU-YuanGroup/Machine-Mindset}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jiaxi Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_L/0/1/0/all/0/1\">Liuzhenghao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jing Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rongsheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jing Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">YongHong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Li Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time is Encoded in the Weights of Finetuned Language Models. (arXiv:2312.13401v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.13401","description":"<p>We present time vectors, a simple tool to customize language models to new\ntime periods. Time vectors are created by finetuning a language model on data\nfrom a single time (e.g., a year or month), and then subtracting the weights of\nthe original pretrained model. This vector specifies a direction in weight\nspace that, as our experiments show, improves performance on text from that\ntime period. Time vectors specialized to adjacent time periods appear to be\npositioned closer together in a manifold. Using this structure, we interpolate\nbetween time vectors to induce new models that perform better on intervening\nand future time periods, without any additional training. We demonstrate the\nconsistency of our findings across different tasks, domains, model sizes, and\ntime scales. Our results suggest that time is encoded in the weight space of\nfinetuned models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nylund_K/0/1/0/all/0/1\">Kai Nylund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gururangan_S/0/1/0/all/0/1\">Suchin Gururangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-aware Decoding Reduces Hallucination in Query-focused Summarization. (arXiv:2312.14335v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.14335","description":"<p>Query-focused summarization (QFS) aims to provide a summary of a single\ndocument/multi documents that can satisfy the information needs of a given\nquery. It is useful for various real-world applications, such as abstractive\nsnippet generation or more recent retrieval augmented generation (RAG). A\nprototypical QFS pipeline consists of a retriever (sparse or dense retrieval)\nand a generator (usually a large language model). However, applying large\nlanguage models (LLM) potentially leads to hallucinations, especially when the\nevidence contradicts the prior belief of LLMs. There has been growing interest\nin developing new decoding methods to improve generation quality and reduce\nhallucination. In this work, we conduct a large-scale reproducibility study on\none recently proposed decoding method -- Context-aware Decoding (CAD). In\naddition to replicating CAD's experiments on news summarization datasets, we\ninclude experiments on QFS datasets, and conduct more rigorous analysis on\ncomputational complexity and hyperparameter sensitivity. Experiments with eight\ndifferent language models show that performance-wise, CAD improves QFS quality\nby (1) reducing factuality errors/hallucinations while (2) mostly retaining the\nmatch of lexical patterns, measured by ROUGE scores, while also at a cost of\nincreased inference-time FLOPs and reduced decoding speed. The code\nimplementation based on Huggingface Library is made available\nhttps://github.com/zhichaoxu-shufe/context-aware-decoding-qfs\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhichao Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aurora:Activating Chinese chat capability for Mixtral-8x7B sparse Mixture-of-Experts through Instruction-Tuning. (arXiv:2312.14557v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.14557","description":"<p>Existing research has demonstrated that refining large language models (LLMs)\nthrough the utilization of machine-generated instruction-following data\nempowers these models to exhibit impressive zero-shot capabilities for novel\ntasks, without requiring human-authored instructions. In this paper, we\nsystematically investigate, preprocess, and integrate three Chinese\ninstruction-following datasets with the aim of enhancing the Chinese\nconversational capabilities of Mixtral-8x7B sparse Mixture-of-Experts model.\nThrough instruction fine-tuning on this carefully processed dataset, we\nsuccessfully construct the Mixtral-8x7B sparse Mixture-of-Experts model named\n\"Aurora.\" To assess the performance of Aurora, we utilize three widely\nrecognized benchmark tests: C-Eval, MMLU, and CMMLU. Empirical studies validate\nthe effectiveness of instruction fine-tuning applied to Mixtral-8x7B sparse\nMixture-of-Experts model. This work is pioneering in the execution of\ninstruction fine-tuning on a sparse expert-mixed model, marking a significant\nbreakthrough in enhancing the capabilities of this model architecture. Our\ncode, data and model are publicly available at\n</p>\n<p>https://github.com/WangRongsheng/Aurora\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rongsheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Ruizhe Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yaofei Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_K/0/1/0/all/0/1\">Kunyan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Han Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jiaxi Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_P/0/1/0/all/0/1\">Patrick Cheong-Iao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yapeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tao Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Model for Causal Decision Making. (arXiv:2312.17122v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.17122","description":"<p>Large Language Models (LLMs) have shown their success in language\nunderstanding and reasoning on general topics. However, their capability to\ninference based on user-specified structured data and knowledge in corpus-rare\nconcepts like causal decision-making is still limited. In this work, we explore\nthe possibility of fine-tuning an open-sourced LLM into LLM4Causal, which can\nidentify the causal task, execute a corresponding function, and interpret its\nnumerical results based on users' queries and the provided dataset. Meanwhile,\nwe propose a data generation process for more controllable GPT prompting and\npresent two instruction-tuning datasets: (1) Causal-Retrieval-Bench for causal\nproblem identification and input parameter extraction for causal function\ncalling and (2) Causal-Interpret-Bench for in-context causal interpretation.\nWith three case studies, we showed that LLM4Causal can deliver end-to-end\nsolutions for causal problems and provide easy-to-understand answers. Numerical\nstudies also reveal that it has a remarkable ability to identify the correct\ncausal task given a query.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haitao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_L/0/1/0/all/0/1\">Lin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuhe Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Rui Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Normalization of Lithuanian Text Using Regular Expressions. (arXiv:2312.17660v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2312.17660","description":"<p>Text Normalization is an integral part of any text-to-speech synthesis\nsystem. In a natural language text, there are elements such as numbers, dates,\nabbreviations, etc. that belong to other semiotic classes. They are called\nnon-standard words (NSW) and need to be expanded into ordinary words. For this\npurpose, it is necessary to identify the semiotic class of each NSW. The\ntaxonomy of semiotic classes adapted to the Lithuanian language is presented in\nthe work. Sets of rules are created for detecting and expanding NSWs based on\nregular expressions. Experiments with three completely different data sets were\nperformed and the accuracy was assessed. Causes of errors are explained and\nrecommendations are given for the development of text normalization rules.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kasparaitis_P/0/1/0/all/0/1\">Pijus Kasparaitis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable Identification of Hate Speech towards Islam using Graph Neural Networks. (arXiv:2311.04916v2 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2311.04916","description":"<p>Islamophobic language is a prevalent challenge on online social interaction\nplatforms. Identifying and eliminating such hatred is a crucial step towards a\nfuture of harmony and peace. This study presents a novel paradigm for\nidentifying and explaining hate speech towards Islam using graph neural\nnetworks. Utilizing the intrinsic ability of graph neural networks to find,\nextract, and use relationships across disparate data points, our model\nconsistently achieves outstanding performance while offering explanations for\nthe underlying correlations and causation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wasi_A/0/1/0/all/0/1\">Azmine Toushik Wasi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2024-01-01T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}