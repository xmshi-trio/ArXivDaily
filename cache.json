{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-11-10T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Detecting Relevant Information in High-Volume Chat Logs: Keyphrase Extraction for Grooming and Drug Dealing Forensic Analysis. (arXiv:2311.04905v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04905","description":"<p>The growing use of digital communication platforms has given rise to various\ncriminal activities, such as grooming and drug dealing, which pose significant\nchallenges to law enforcement and forensic experts. This paper presents a\nsupervised keyphrase extraction approach to detect relevant information in\nhigh-volume chat logs involving grooming and drug dealing for forensic\nanalysis. The proposed method, JointKPE++, builds upon the JointKPE keyphrase\nextractor by employing improvements to handle longer texts effectively. We\nevaluate JointKPE++ using BERT-based pre-trained models on grooming and drug\ndealing datasets, including BERT, RoBERTa, SpanBERT, and BERTimbau. The results\nshow significant improvements over traditional approaches and demonstrate the\npotential for JointKPE++ to aid forensic experts in efficiently detecting\nkeyphrases related to criminal activities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alves_J/0/1/0/all/0/1\">Jeovane Hon&#xf3;rio Alves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedroso_H/0/1/0/all/0/1\">Hor&#xe1;cio A. C. G. Pedroso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venetikides_R/0/1/0/all/0/1\">Rafael Honorio Venetikides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koster_J/0/1/0/all/0/1\">Joel E. M. K&#xf6;ster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grochocki_L/0/1/0/all/0/1\">Luiz Rodrigo Grochocki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_C/0/1/0/all/0/1\">Cinthia O. A. Freitas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barddal_J/0/1/0/all/0/1\">Jean Paul Barddal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FlaCGEC: A Chinese Grammatical Error Correction Dataset with Fine-grained Linguistic Annotation. (arXiv:2311.04906v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04906","description":"<p>Chinese Grammatical Error Correction (CGEC) has been attracting growing\nattention from researchers recently. In spite of the fact that multiple CGEC\ndatasets have been developed to support the research, these datasets lack the\nability to provide a deep linguistic topology of grammar errors, which is\ncritical for interpreting and diagnosing CGEC approaches. To address this\nlimitation, we introduce FlaCGEC, which is a new CGEC dataset featured with\nfine-grained linguistic annotation. Specifically, we collect raw corpus from\nthe linguistic schema defined by Chinese language experts, conduct edits on\nsentences via rules, and refine generated samples manually, which results in\n10k sentences with 78 instantiated grammar points and 3 types of edits. We\nevaluate various cutting-edge CGEC methods on the proposed FlaCGEC dataset and\ntheir unremarkable results indicate that this dataset is challenging in\ncovering a large range of grammatical errors. In addition, we also treat\nFlaCGEC as a diagnostic dataset for testing generalization skills and conduct a\nthorough evaluation of existing CGEC models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1\">Hanyue Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yike Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qingyuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiani Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yunshi Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xuesong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In nomine patris... Elements for a semantics of medieval paternity. (arXiv:2311.04907v1 [cs.DL])","link":"http://arxiv.org/abs/2311.04907","description":"<p>This article examines medieval concepts of paternity and father-son\nrelationships through the digital analysis of medieval textual corpora.\nAlthough historians have access to enormous digital collections in 2023, they\nhave rarely fully exploited these resources. The author proposes a historical\nsemantic approach to this theme, using modeling tools and text mining in\ngeneral, to analyze the evolution of terms related to paternity. The study\nproposes three conclusions: 1. a semantic break occurred in the semantic field\nof paternity at the turn of Antiquity and the Early Middle Ages. The meaning of\npater and its derivatives changed radically over the course of the 4th-6th\ncenturies, particularly as a result of the influence of the dogma of the\nChristian Trinity. Medieval fatherhood was multidimensional, encompassing both\nbiological and spiritual aspects, in other words, complex relationships between\nmultiple carnal and spiritual (i.e. divine) fathers. 2. The role of spiritual\nkinship is crucial to understanding medieval fatherhood, as the work of Anita\nGuerreau-Jalabert and J{\\'e}r{\\^o}me Baschet has already shown. Initially\nattributed to God, this ''ideal paternity'' (paternitas) gradually extended to\nmembers of the Church (popes, bishops, abbots), underlining at the same time\nthe growing importance of spiritual kinship over biological kinship over the\ncenturies studied. 3. To reveal these structures, invisible to the naked eye,\nan interdisciplinary approach is rigorously required. Complementary\ninvestigations into the lemmas mater, filia, frater and other family terms are\nrequired. The use of digital tools and historical semantic analysis opens up\nnew perspectives for researchers in history, anthropology, linguistics and data\nmining, enabling them to explore the representation systems of ancient\nsocieties in depth and with nuance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perreaux_N/0/1/0/all/0/1\">Nicolas Perreaux</a> (LAMOP)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ontology-Driven Processing of Transdisciplinary Domain Knowledge. (arXiv:2311.04910v1 [cs.DL])","link":"http://arxiv.org/abs/2311.04910","description":"<p>The monograph discusses certain aspects of modern real-world problems facing\nhumanity, which are much more challenging than scientific ones. Modern science\nis unable to solve them in a fundamental way. Vernadsky's noosphere thesis, in\nfact, appeals to the scientific worldview that needs to be built in a way that\novercomes the interdisciplinary barriers and increases the effectiveness of\ninterdisciplinary interaction and modern science overall. We are talking about\nthe general transdisciplinary knowledge. In world practice, there is still no\nsystematic methodology and a specific form of generally accepted valid\nscientific theory that would provide transdisciplinary knowledge. Non-linear\ninterdisciplinary interaction is the standard of evolution of modern science.\nAt the same time, a new transdisciplinary theory (domain of scientific\nresearch) is being de facto created and the process is repeated many times:\nfrom an individual or group of disciplines, through interdisciplinary\ninteraction, in a direction that brings us closer to creating a holistic\ngeneral scientific worldview.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Palagin_O/0/1/0/all/0/1\">Oleksandr Palagin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrenko_M/0/1/0/all/0/1\">Mykola Petrenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryvyi_S/0/1/0/all/0/1\">Sergii Kryvyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyko_M/0/1/0/all/0/1\">Mykola Boyko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malakhov_K/0/1/0/all/0/1\">Kyrylo Malakhov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Text to Structure: Using Large Language Models to Support the Development of Legal Expert Systems. (arXiv:2311.04911v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04911","description":"<p>Encoding legislative text in a formal representation is an important\nprerequisite to different tasks in the field of AI &amp; Law. For example,\nrule-based expert systems focused on legislation can support laypeople in\nunderstanding how legislation applies to them and provide them with helpful\ncontext and information. However, the process of analyzing legislation and\nother sources to encode it in the desired formal representation can be\ntime-consuming and represents a bottleneck in the development of such systems.\nHere, we investigate to what degree large language models (LLMs), such as\nGPT-4, are able to automatically extract structured representations from\nlegislation. We use LLMs to create pathways from legislation, according to the\nJusticeBot methodology for legal decision support systems, evaluate the\npathways and compare them to manually created pathways. The results are\npromising, with 60% of generated pathways being rated as equivalent or better\nthan manually created ones in a blind comparison. The approach suggests a\npromising path to leverage the capabilities of LLMs to ease the costly\ndevelopment of systems based on symbolic approaches that are transparent and\nexplainable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Janatian_S/0/1/0/all/0/1\">Samyar Janatian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Westermann_H/0/1/0/all/0/1\">Hannes Westermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jinzhe Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savelka_J/0/1/0/all/0/1\">Jaromir Savelka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benyekhlef_K/0/1/0/all/0/1\">Karim Benyekhlef</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Improved Transformer-based Model for Detecting Phishing, Spam, and Ham: A Large Language Model Approach. (arXiv:2311.04913v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04913","description":"<p>Phishing and spam detection is long standing challenge that has been the\nsubject of much academic research. Large Language Models (LLM) have vast\npotential to transform society and provide new and innovative approaches to\nsolve well-established challenges. Phishing and spam have caused financial\nhardships and lost time and resources to email users all over the world and\nfrequently serve as an entry point for ransomware threat actors. While\ndetection approaches exist, especially heuristic-based approaches, LLMs offer\nthe potential to venture into a new unexplored area for understanding and\nsolving this challenge. LLMs have rapidly altered the landscape from business,\nconsumers, and throughout academia and demonstrate transformational potential\nfor the potential of society. Based on this, applying these new and innovative\napproaches to email detection is a rational next step in academic research. In\nthis work, we present IPSDM, our model based on fine-tuning the BERT family of\nmodels to specifically detect phishing and spam email. We demonstrate our\nfine-tuned version, IPSDM, is able to better classify emails in both unbalanced\nand balanced datasets. This work serves as an important first step towards\nemploying LLMs to improve the security of our information systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jamal_S/0/1/0/all/0/1\">Suhaima Jamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wimmer_H/0/1/0/all/0/1\">Hayden Wimmer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain of Empathy: Enhancing Empathetic Response of Large Language Models Based on Psychotherapy Models. (arXiv:2311.04915v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04915","description":"<p>We present a novel method, the Chain of Empathy (CoE) prompting, that\nutilizes insights from psychotherapy to induce Large Language Models (LLMs) to\nreason about human emotional states. This method is inspired by various\npsychotherapy approaches including Cognitive Behavioral Therapy (CBT),\nDialectical Behavior Therapy (DBT), Person Centered Therapy (PCT), and Reality\nTherapy (RT), each leading to different patterns of interpreting clients'\nmental states. LLMs without reasoning generated predominantly exploratory\nresponses. However, when LLMs used CoE reasoning, we found a more comprehensive\nrange of empathetic responses aligned with the different reasoning patterns of\neach psychotherapy model. The CBT based CoE resulted in the most balanced\ngeneration of empathetic responses. The findings underscore the importance of\nunderstanding the emotional context and how it affects human and AI\ncommunication. Our research contributes to understanding how psychotherapeutic\nmodels can be incorporated into LLMs, facilitating the development of\ncontext-specific, safer, and empathetic AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yoon Kyung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">Inju Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1\">Minjung Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1\">Seoyeon Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahn_S/0/1/0/all/0/1\">Sowon Hahn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable Identification of Hate Speech towards Islam using Graph Neural Networks. (arXiv:2311.04916v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04916","description":"<p>Islamophobic language is a prevalent challenge on online social interaction\nplatforms. Identifying and eliminating such hatred is a crucial step towards a\nfuture of harmony and peace. This study presents a novel paradigm for\nidentifying and explaining hate speech towards Islam using graph neural\nnetworks. Utilizing the intrinsic ability of graph neural networks to find,\nextract, and use relationships across disparate data points, our model\nconsistently achieves outstanding performance while offering explanations for\nthe underlying correlations and causation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wasi_A/0/1/0/all/0/1\">Azmine Toushik Wasi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting Fake News Detection to the Era of Large Language Models. (arXiv:2311.04917v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04917","description":"<p>In the age of large language models (LLMs) and the widespread adoption of\nAI-driven content creation, the landscape of information dissemination has\nwitnessed a paradigm shift. With the proliferation of both human-written and\nmachine-generated real and fake news, robustly and effectively discerning the\nveracity of news articles has become an intricate challenge. While substantial\nresearch has been dedicated to fake news detection, this either assumes that\nall news articles are human-written or abruptly assumes that all\nmachine-generated news are fake. Thus, a significant gap exists in\nunderstanding the interplay between machine-(paraphrased) real news,\nmachine-generated fake news, human-written fake news, and human-written real\nnews. In this paper, we study this gap by conducting a comprehensive evaluation\nof fake news detectors trained in various scenarios. Our primary objectives\nrevolve around the following pivotal question: How to adapt fake news detectors\nto the era of LLMs? Our experiments reveal an interesting pattern that\ndetectors trained exclusively on human-written articles can indeed perform well\nat detecting machine-generated fake news, but not vice versa. Moreover, due to\nthe bias of detectors against machine-generated texts \\cite{su2023fake}, they\nshould be trained on datasets with a lower machine-generated news ratio than\nthe test set. Building on our findings, we provide a practical strategy for the\ndevelopment of robust fake news detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinyan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardie_C/0/1/0/all/0/1\">Claire Cardie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Resource Named Entity Recognition: Can One-vs-All AUC Maximization Help?. (arXiv:2311.04918v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04918","description":"<p>Named entity recognition (NER), a task that identifies and categorizes named\nentities such as persons or organizations from text, is traditionally framed as\na multi-class classification problem. However, this approach often overlooks\nthe issues of imbalanced label distributions, particularly in low-resource\nsettings, which is common in certain NER contexts, like biomedical NER\n(bioNER). To address these issues, we propose an innovative reformulation of\nthe multi-class problem as a one-vs-all (OVA) learning problem and introduce a\nloss function based on the area under the receiver operating characteristic\ncurve (AUC). To enhance the efficiency of our OVA-based approach, we propose\ntwo training strategies: one groups labels with similar linguistic\ncharacteristics, and another employs meta-learning. The superiority of our\napproach is confirmed by its performance, which surpasses traditional NER\nlearning in varying NER settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngoc Dang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1\">Wray Buntine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beare_R/0/1/0/all/0/1\">Richard Beare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Impact of Preference Agreement in Reinforcement Learning from Human Feedback: A Case Study in Summarization. (arXiv:2311.04919v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04919","description":"<p>Reinforcement Learning from Human Feedback (RLHF) can be used to capture\ncomplex and nuanced properties of text generation quality. As a result, the\ntask of text summarization has been identified as a good candidate for this\nprocess. In this paper, we explore how preference agreement impacts the\nefficacy of RLHF for summarization. We show that sampling human preferences to\ninclude a range of annotator agreement results in (1) higher accuracy reward\nmodels and (2) alters the characteristics of quality captured. We additionally\nshow improvements in downstream generation when using a reward model trained\nwith a range of preference agreements. Our contributions have implications for\nthe design of synthetic datasets as well as the importance of considering\nquality differentials in comparison-based data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gooding_S/0/1/0/all/0/1\">Sian Gooding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansoor_H/0/1/0/all/0/1\">Hassan Mansoor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Successor Features for Efficient Multisubject Controlled Text Generation. (arXiv:2311.04921v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04921","description":"<p>While large language models (LLMs) have achieved impressive performance in\ngenerating fluent and realistic text, controlling the generated text so that it\nexhibits properties such as safety, factuality, and non-toxicity remains\nchallenging. % such as DExperts, GeDi, and rectification Existing\ndecoding-based methods are static in terms of the dimension of control; if the\ntarget subject is changed, they require new training. Moreover, it can quickly\nbecome prohibitive to concurrently control multiple subjects. In this work, we\nintroduce SF-GEN, which is grounded in two primary concepts: successor features\n(SFs) to decouple the LLM's dynamics from task-specific rewards, and language\nmodel rectification to proportionally adjust the probability of selecting a\ntoken based on the likelihood that the finished text becomes undesired. SF-GEN\nseamlessly integrates the two to enable dynamic steering of text generation\nwith no need to alter the LLM's parameters. Thanks to the decoupling effect\ninduced by successor features, our method proves to be memory-wise and\ncomputationally efficient for training as well as decoding, especially when\ndealing with multiple target subjects. To the best of our knowledge, our\nresearch represents the first application of successor features in text\ngeneration. In addition to its computational efficiency, the resultant language\nproduced by our method is comparable to the SOTA (and outperforms baselines) in\nboth control measures as well as language quality, which we demonstrate through\na series of experiments in various controllable text generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Meng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fatemi_M/0/1/0/all/0/1\">Mehdi Fatemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jackie Chi Kit Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shabanian_S/0/1/0/all/0/1\">Samira Shabanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are cascade dialogue state tracking models speaking out of turn in spoken dialogues?. (arXiv:2311.04922v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04922","description":"<p>In Task-Oriented Dialogue (TOD) systems, correctly updating the system's\nunderstanding of the user's needs is key to a smooth interaction. Traditionally\nTOD systems are composed of several modules that interact with one another.\nWhile each of these components is the focus of active research communities,\ntheir behavior in interaction can be overlooked. This paper proposes a\ncomprehensive analysis of the errors of state of the art systems in complex\nsettings such as Dialogue State Tracking which highly depends on the dialogue\ncontext. Based on spoken MultiWoz, we identify that errors on non-categorical\nslots' values are essential to address in order to bridge the gap between\nspoken and chat-based dialogue systems. We explore potential solutions to\nimprove transcriptions and help dialogue state tracking generative models\ncorrect such errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Druart_L/0/1/0/all/0/1\">Lucas Druart</a> (LIA), <a href=\"http://arxiv.org/find/cs/1/au:+Jacqmin_L/0/1/0/all/0/1\">L&#xe9;o Jacqmin</a> (LIS), <a href=\"http://arxiv.org/find/cs/1/au:+Favre_B/0/1/0/all/0/1\">Beno&#xee;t Favre</a> (LIS), <a href=\"http://arxiv.org/find/cs/1/au:+Rojas_Barahona_L/0/1/0/all/0/1\">Lina Maria Rojas-Barahona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vielzeuf_V/0/1/0/all/0/1\">Valentin Vielzeuf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is one brick enough to break the wall of spoken dialogue state tracking?. (arXiv:2311.04923v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04923","description":"<p>In Task-Oriented Dialogue (TOD) systems, correctly updating the system's\nunderstanding of the user's needs (a.k.a dialogue state tracking) is key to a\nsmooth interaction. Traditionally, TOD systems perform this update in three\nsteps: transcription of the user's utterance, semantic extraction of the key\nconcepts, and contextualization with the previously identified concepts. Such\ncascade approaches suffer from cascading errors and separate optimization.\nEnd-to-End approaches have been proved helpful up to the semantic extraction\nstep. This paper goes one step further paving the path towards completely\nneural spoken dialogue state tracking by comparing three approaches: (1) a\nstate of the art cascade approach, (2) a locally E2E approach with rule-based\ncontextualization and (3) a completely neural approach. Our study highlights\nthat although they all outperform the recent DSTC11 best model, especially with\na filtering post-processing step, (1) remains the most accurate approach.\nIndeed, both (2) and (3) have trouble propagating context as dialogues unfold\nshowing that context propagation in completely neural approaches is an open\nchallenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Druart_L/0/1/0/all/0/1\">Lucas Druart</a> (LIA), <a href=\"http://arxiv.org/find/cs/1/au:+Vielzeuf_V/0/1/0/all/0/1\">Valentin Vielzeuf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteve_Y/0/1/0/all/0/1\">Yannick Est&#xe8;ve</a> (LIA)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tuning-less Object Naming with a Foundation Model. (arXiv:2311.04924v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04924","description":"<p>We implement a real-time object naming system that enables learning a set of\nnamed entities never seen. Our approach employs an existing foundation model\nthat we consider ready to see anything before starting. It turns seen images\ninto relatively small feature vectors that we associate with index to a\ngradually built vocabulary without any training of fine-tuning of the model.\nOur contribution is using the association mechanism known from transformers as\nattention. It has features that support generalization from irrelevant\ninformation for distinguishing the entities and potentially enable associating\nwith much more than indices to vocabulary. As a result, the system can work in\na one-shot manner and correctly name objects named in different contents. We\nalso outline implementation details of the system modules integrated by a\nblackboard architecture. Finally, we investigate the system's quality, mainly\nhow many objects it can handle in this way.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lucny_A/0/1/0/all/0/1\">Andrej Lucny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrovic_P/0/1/0/all/0/1\">Pavel Petrovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Deep-Learning NLP for Automating the Extraction of Oncology Efficacy Endpoints from Scientific Literature. (arXiv:2311.04925v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04925","description":"<p>Benchmarking drug efficacy is a critical step in clinical trial design and\nplanning. The challenge is that much of the data on efficacy endpoints is\nstored in scientific papers in free text form, so extraction of such data is\ncurrently a largely manual task. Our objective is to automate this task as much\nas possible. In this study we have developed and optimised a framework to\nextract efficacy endpoints from text in scientific papers, using a machine\nlearning approach. Our machine learning model predicts 25 classes associated\nwith efficacy endpoints and leads to high F1 scores (harmonic mean of precision\nand recall) of 96.4% on the test set, and 93.9% and 93.7% on two case studies.\nThese methods were evaluated against - and showed strong agreement with -\nsubject matter experts and show significant promise in the future of automating\nthe extraction of clinical endpoints from free text. Clinical information\nextraction from text data is currently a laborious manual task which scales\npoorly and is prone to human error. Demonstrating the ability to extract\nefficacy endpoints automatically shows great promise for accelerating clinical\ntrial design moving forwards.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gendrin_Brokmann_A/0/1/0/all/0/1\">Aline Gendrin-Brokmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_E/0/1/0/all/0/1\">Eden Harrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noveras_J/0/1/0/all/0/1\">Julianne Noveras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Souliotis_L/0/1/0/all/0/1\">Leonidas Souliotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vince_H/0/1/0/all/0/1\">Harris Vince</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smit_I/0/1/0/all/0/1\">Ines Smit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_F/0/1/0/all/0/1\">Francisco Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milward_D/0/1/0/all/0/1\">David Milward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimitrievska_S/0/1/0/all/0/1\">Sashka Dimitrievska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metcalfe_P/0/1/0/all/0/1\">Paul Metcalfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Louvet_E/0/1/0/all/0/1\">Emilie Louvet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"More Robots are Coming: Large Multimodal Models (ChatGPT) can Solve Visually Diverse Images of Parsons Problems. (arXiv:2311.04926v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04926","description":"<p>The advent of large language models is reshaping computing education. Recent\nresearch has demonstrated that these models can produce better explanations\nthan students, answer multiple-choice questions at or above the class average,\nand generate code that can pass automated tests in introductory courses. These\ncapabilities have prompted instructors to rapidly adapt their courses and\nassessment methods to accommodate changes in learning objectives and the\npotential for academic integrity violations. While some scholars have advocated\nfor the integration of visual problems as a safeguard against the capabilities\nof language models, new multimodal language models now have vision and language\ncapabilities that may allow them to analyze and solve visual problems. In this\npaper, we evaluate the performance of two large multimodal models on visual\nassignments, with a specific focus on Parsons problems presented across diverse\nvisual representations. Our results show that GPT-4V solved 96.7\\% of these\nvisual problems, struggling minimally with a single Parsons problem.\nConversely, Bard performed poorly by only solving 69.2\\% of problems,\nstruggling with common issues like hallucinations and refusals. These findings\nsuggest that merely transitioning to visual programming problems might not be a\npanacea to issues of academic integrity in the generative AI era.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_I/0/1/0/all/0/1\">Irene Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Man_O/0/1/0/all/0/1\">Owen Man</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mettille_S/0/1/0/all/0/1\">Sophie Mettille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_S/0/1/0/all/0/1\">Sebastian Gutierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelikas_K/0/1/0/all/0/1\">Kenneth Angelikas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacNeil_S/0/1/0/all/0/1\">Stephen MacNeil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextualizing the Limits of Model & Evaluation Dataset Curation on Semantic Similarity Classification Tasks. (arXiv:2311.04927v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04927","description":"<p>This paper demonstrates how the limitations of pre-trained models and open\nevaluation datasets factor into assessing the performance of binary semantic\nsimilarity classification tasks. As (1) end-user-facing documentation around\nthe curation of these datasets and pre-trained model training regimes is often\nnot easily accessible and (2) given the lower friction and higher demand to\nquickly deploy such systems in real-world contexts, our study reinforces prior\nwork showing performance disparities across datasets, embedding techniques and\ndistance metrics, while highlighting the importance of understanding how data\nis collected, curated and analyzed in semantic similarity classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Theron_D/0/1/0/all/0/1\">Daniel Theron</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Large Language Models for Collective Decision-Making. (arXiv:2311.04928v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04928","description":"<p>In various work contexts, such as meeting scheduling, collaborating, and\nproject planning, collective decision-making is essential but often challenging\ndue to diverse individual preferences, varying work focuses, and power dynamics\namong members. To address this, we propose a system leveraging Large Language\nModels (LLMs) to facilitate group decision-making by managing conversations and\nbalancing preferences among individuals. Our system extracts individual\npreferences and suggests options that satisfy a significant portion of the\nmembers. We apply this system to corporate meeting scheduling. We create\nsynthetic employee profiles and simulate conversations at scale, leveraging\nLLMs to evaluate the system. Our results indicate efficient coordination with\nreduced interactions between members and the LLM-based system. The system also\neffectively refines proposed options over time, ensuring their quality and\nequity. Finally, we conduct a survey study involving human participants to\nassess our system's ability to aggregate preferences and reasoning. Our\nfindings show that the system exhibits strong performance in both dimensions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papachristou_M/0/1/0/all/0/1\">Marios Papachristou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Longqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Chin-Chia Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Interdisciplinary Outlook on Large Language Models for Scientific Research. (arXiv:2311.04929v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04929","description":"<p>In this paper, we describe the capabilities and constraints of Large Language\nModels (LLMs) within disparate academic disciplines, aiming to delineate their\nstrengths and limitations with precision. We examine how LLMs augment\nscientific inquiry, offering concrete examples such as accelerating literature\nreview by summarizing vast numbers of publications, enhancing code development\nthrough automated syntax correction, and refining the scientific writing\nprocess. Simultaneously, we articulate the challenges LLMs face, including\ntheir reliance on extensive and sometimes biased datasets, and the potential\nethical dilemmas stemming from their use. Our critical discussion extends to\nthe varying impacts of LLMs across fields, from the natural sciences, where\nthey help model complex biological sequences, to the social sciences, where\nthey can parse large-scale qualitative data. We conclude by offering a nuanced\nperspective on how LLMs can be both a boon and a boundary to scientific\nprogress.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boyko_J/0/1/0/all/0/1\">James Boyko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_J/0/1/0/all/0/1\">Joseph Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_N/0/1/0/all/0/1\">Nathan Fox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veiga_M/0/1/0/all/0/1\">Maria Han Veiga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jennifer I-Hsiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modenesi_B/0/1/0/all/0/1\">Bernardo Modenesi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rauch_A/0/1/0/all/0/1\">Andreas H. Rauch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reid_K/0/1/0/all/0/1\">Kenneth N. Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tribedi_S/0/1/0/all/0/1\">Soumi Tribedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Visheratina_A/0/1/0/all/0/1\">Anastasia Visheratina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large language models implicitly learn to straighten neural sentence trajectories to construct a predictive representation of natural language. (arXiv:2311.04930v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04930","description":"<p>Predicting upcoming events is critical to our ability to interact with our\nenvironment. Transformer models, trained on next-word prediction, appear to\nconstruct representations of linguistic input that can support diverse\ndownstream tasks. But how does a predictive objective shape such\nrepresentations? Inspired by recent work in vision (Henaff et al., 2019), we\ntest a hypothesis about predictive representations of autoregressive\ntransformers. In particular, we test whether the neural trajectory of a\nsentence becomes progressively straighter as it passes through the network\nlayers. The key insight is that straighter trajectories should facilitate\nprediction via linear extrapolation. We quantify straightness using a\n1-dimensional curvature metric, and present four findings in support of the\ntrajectory straightening hypothesis: i) In trained models, the curvature\ndecreases from the early to the deeper layers of the network. ii) Models that\nperform better on the next-word prediction objective exhibit greater decreases\nin curvature, suggesting that this improved ability to straighten sentence\ntrajectories may be the driver of better language modeling performance. iii)\nGiven the same linguistic context, the sequences that are generated by the\nmodel have lower curvature than the actual continuations observed in a language\ncorpus, suggesting that the model favors straighter trajectories for making\npredictions. iv) A consistent relationship holds between the average curvature\nand the average surprisal of sentences in the deep model layers, such that\nsentences with straighter trajectories also have lower surprisal. Importantly,\nuntrained models do not exhibit these behaviors. In tandem, these results\nsupport the trajectory straightening hypothesis and provide a possible\nmechanism for how the geometry of the internal representations of\nautoregressive models supports next word prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_E/0/1/0/all/0/1\">Eghbal A. Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedorenko_E/0/1/0/all/0/1\">Evelina Fedorenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT4All: An Ecosystem of Open Source Compressed Language Models. (arXiv:2311.04931v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04931","description":"<p>Large language models (LLMs) have recently achieved human-level performance\non a range of professional and academic benchmarks. The accessibility of these\nmodels has lagged behind their performance. State-of-the-art LLMs require\ncostly infrastructure; are only accessible via rate-limited, geo-locked, and\ncensored web interfaces; and lack publicly available code and technical\nreports. In this paper, we tell the story of GPT4All, a popular open source\nrepository that aims to democratize access to LLMs. We outline the technical\ndetails of the original GPT4All model family, as well as the evolution of the\nGPT4All project from a single model into a fully fledged open source ecosystem.\nIt is our hope that this paper acts as both a technical overview of the\noriginal GPT4All models as well as a case study on the subsequent growth of the\nGPT4All open source ecosystem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anand_Y/0/1/0/all/0/1\">Yuvanesh Anand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nussbaum_Z/0/1/0/all/0/1\">Zach Nussbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Treat_A/0/1/0/all/0/1\">Adam Treat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_A/0/1/0/all/0/1\">Aaron Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1\">Richard Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_B/0/1/0/all/0/1\">Ben Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Community_G/0/1/0/all/0/1\">GPT4All Community</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duderstadt_B/0/1/0/all/0/1\">Brandon Duderstadt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulyar_A/0/1/0/all/0/1\">Andriy Mulyar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Large Language Models in Ophthalmology. (arXiv:2311.04933v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04933","description":"<p>Purpose: The performance of three different large language models (LLMS)\n(GPT-3.5, GPT-4, and PaLM2) in answering ophthalmology professional questions\nwas evaluated and compared with that of three different professional\npopulations (medical undergraduates, medical masters, and attending\nphysicians). Methods: A 100-item ophthalmology single-choice test was\nadministered to three different LLMs (GPT-3.5, GPT-4, and PaLM2) and three\ndifferent professional levels (medical undergraduates, medical masters, and\nattending physicians), respectively. The performance of LLM was comprehensively\nevaluated and compared with the human group in terms of average score,\nstability, and confidence. Results: Each LLM outperformed undergraduates in\ngeneral, with GPT-3.5 and PaLM2 being slightly below the master's level, while\nGPT-4 showed a level comparable to that of attending physicians. In addition,\nGPT-4 showed significantly higher answer stability and confidence than GPT-3.5\nand PaLM2. Conclusion: Our study shows that LLM represented by GPT-4 performs\nbetter in the field of ophthalmology. With further improvements, LLM will bring\nunexpected benefits in medical education and clinical decision making in the\nnear future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Holmes_J/0/1/0/all/0/1\">Jason Holmes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Shuyuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shi-Nan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Huan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Jie Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yi Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt Cache: Modular Attention Reuse for Low-Latency Inference. (arXiv:2311.04934v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04934","description":"<p>We present Prompt Cache, an approach for accelerating inference for large\nlanguage models (LLM) by reusing attention states across different LLM prompts.\nMany input prompts have overlapping text segments, such as system messages,\nprompt templates, and documents provided for context. Our key insight is that\nby precomputing and storing the attention states of these frequently occurring\ntext segments on the inference server, we can efficiently reuse them when these\nsegments appear in user prompts. Prompt Cache employs a schema to explicitly\ndefine such reusable text segments, called prompt modules. The schema ensures\npositional accuracy during attention state reuse and provides users with an\ninterface to access cached states in their prompt. Using a prototype\nimplementation, we evaluate Prompt Cache across several LLMs. We show that\nPrompt Cache significantly reduce latency in time-to-first-token, especially\nfor longer prompts such as document-based question answering and\nrecommendations. The improvements range from 8x for GPU-based inference to 60x\nfor CPU-based inference, all while maintaining output accuracy and without the\nneed for model parameter modifications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gim_I/0/1/0/all/0/1\">In Gim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guojun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seung-seob Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarda_N/0/1/0/all/0/1\">Nikhil Sarda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1\">Anurag Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Lin Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A comparative analysis between Conformer-Transducer, Whisper, and wav2vec2 for improving the child speech recognition. (arXiv:2311.04936v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04936","description":"<p>Automatic Speech Recognition (ASR) systems have progressed significantly in\ntheir performance on adult speech data; however, transcribing child speech\nremains challenging due to the acoustic differences in the characteristics of\nchild and adult voices. This work aims to explore the potential of adapting\nstate-of-the-art Conformer-transducer models to child speech to improve child\nspeech recognition performance. Furthermore, the results are compared with\nthose of self-supervised wav2vec2 models and semi-supervised multi-domain\nWhisper models that were previously finetuned on the same data. We demonstrate\nthat finetuning Conformer-transducer models on child speech yields significant\nimprovements in ASR performance on child speech, compared to the non-finetuned\nmodels. We also show Whisper and wav2vec2 adaptation on different child speech\ndatasets. Our detailed comparative analysis shows that wav2vec2 provides the\nmost consistent performance improvements among the three methods studied.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barcovschi_A/0/1/0/all/0/1\">Andrei Barcovschi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Rishabh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corcoran_P/0/1/0/all/0/1\">Peter Corcoran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LooGLE: Can Long-Context Language Models Understand Long Contexts?. (arXiv:2311.04939v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04939","description":"<p>Large language models (LLMs), despite their impressive performance in various\nlanguage tasks, are typically limited to processing texts within context-window\nsize. This limitation has spurred significant research efforts to enhance LLMs'\nlong-context understanding with high-quality long-sequence benchmarks. However,\nprior datasets in this regard suffer from shortcomings, such as short context\nlength compared to the context window of modern LLMs; outdated documents that\nhave data leakage problems; and an emphasis on short dependency tasks rather\nthan long dependency tasks. In this paper, we present LooGLE, a Long Context\nGeneric Language Evaluation benchmark for LLMs' long context understanding.\nLooGLE features relatively new documents post-2022, with over 24,000 tokens per\ndocument and 6,000 newly generated questions spanning diverse domains. Human\nannotators meticulously crafted more than 1,100 high-quality question-answer\npairs to meet the long dependency requirements. These pairs underwent thorough\ncross-validation, yielding the most precise assessment of LLMs' long dependency\ncapabilities. The evaluation of eight state-of-the-art LLMs on LooGLE revealed\nkey findings: (i) commercial models outperformed open-sourced models; (ii) LLMs\nexcelled in short dependency tasks like short question-answering and cloze\ntasks but struggled with more intricate long dependency tasks; (iii) in-context\nlearning and chaining thoughts offered only marginal improvements; (iv)\nretrieval-based techniques demonstrated substantial benefits for short\nquestion-answering, while strategies for extending context window length had\nlimited impact on long context understanding. As such, LooGLE not only provides\na systematic and comprehensive evaluation schema on long-context LLMs, but also\nsheds light on future development of enhanced models towards \"true long-context\nunderstanding\".\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengmeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zilong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Muhan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explained anomaly detection in text reviews: Can subjective scenarios be correctly evaluated?. (arXiv:2311.04948v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04948","description":"<p>This paper presents a pipeline to detect and explain anomalous reviews in\nonline platforms. The pipeline is made up of three modules and allows the\ndetection of reviews that do not generate value for users due to either\nworthless or malicious composition. The classifications are accompanied by a\nnormality score and an explanation that justifies the decision made. The\npipeline's ability to solve the anomaly detection task was evaluated using\ndifferent datasets created from a large Amazon database. Additionally, a study\ncomparing three explainability techniques involving 241 participants was\nconducted to assess the explainability module. The study aimed to measure the\nimpact of explanations on the respondents' ability to reproduce the\nclassification model and their perceived usefulness. This work can be useful to\nautomate tasks in review online platforms, such as those for electronic\ncommerce, and offers inspiration for addressing similar problems in the field\nof anomaly detection in textual data. We also consider it interesting to have\ncarried out a human evaluation of the capacity of different explainability\ntechniques in a real and infrequent scenario such as the detection of anomalous\nreviews, as well as to reflect on whether it is possible to explain tasks as\nhumanly subjective as this one.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Novoa_Paradela_D/0/1/0/all/0/1\">David Novoa-Paradela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fontenla_Romero_O/0/1/0/all/0/1\">Oscar Fontenla-Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guijarro_Berdinas_B/0/1/0/all/0/1\">Bertha Guijarro-Berdi&#xf1;as</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt Sketching for Large Language Models. (arXiv:2311.04954v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04954","description":"<p>Many recent prompting strategies for large language models (LLMs) query the\nmodel multiple times sequentially -- first to produce intermediate results and\nthen the final answer. However, using these methods, both decoder and model are\nunaware of potential follow-up prompts, leading to disconnected and undesirably\nwordy intermediate responses. In this work, we address this issue by proposing\nprompt sketching, a new prompting paradigm in which an LLM does not only\nrespond by completing a prompt, but by predicting values for multiple variables\nin a template. This way, sketching grants users more control over the\ngeneration process, e.g., by providing a reasoning framework via intermediate\ninstructions, leading to better overall results. The key idea enabling\nsketching with existing, autoregressive models is to adapt the decoding\nprocedure to also score follow-up instructions during text generation, thus\noptimizing overall template likelihood in inference. Our experiments show that\nin a zero-shot setting, prompt sketching outperforms existing, sequential\nprompting schemes such as direct asking or chain-of-thought on 7 out of 8 LLM\nbenchmarking tasks, including state tracking, arithmetic reasoning, and general\nquestion answering. To facilitate future use, we release a number of generic,\nyet effective sketches applicable to many tasks, and an open source library\ncalled dclib, powering our sketch-aware decoders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beurer_Kellner_L/0/1/0/all/0/1\">Luca Beurer-Kellner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1\">Mark Niklas M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1\">Marc Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1\">Martin Vechev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the steerability of large language models toward data-driven personas. (arXiv:2311.04978v1 [cs.CL])","link":"http://arxiv.org/abs/2311.04978","description":"<p>The recent surge in Large Language Model (LLM) related applications has led\nto a concurrent escalation in expectations for LLMs to accommodate a myriad of\npersonas and encompass a broad spectrum of perspectives. An important first\nstep towards addressing this demand is to align language models with specific\npersonas, be it groups of users or individuals. Towards this goal, we first\npresent a new conceptualization of a persona. Moving beyond the traditional\nreliance on demographics like age, gender, or political party affiliation, we\nintroduce a data-driven persona definition methodology built on\ncollaborative-filtering. In this methodology, users are embedded into a\ncontinuous vector space based on their opinions and clustered into cohorts that\nmanifest coherent views across specific inquiries. This methodology allows for\na more nuanced understanding of different latent social groups present in the\noverall population (as opposed to simply using demographic groups) and enhances\nthe applicability of model steerability. Finally, we present an efficient\nmethod to steer LLMs towards a particular persona. We learn a soft-prompting\nmodel to map the continuous representation of users into sequences of virtual\ntokens which, when prepended to the LLM input, enables the LLM to produce\nresponses aligned with a given user. Our results show that our steerability\nalgorithm is superior in performance compared to a collection of baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehrabi_N/0/1/0/all/0/1\">Ninareh Mehrabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peris_C/0/1/0/all/0/1\">Charith Peris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Palash Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpreting Pretrained Language Models via Concept Bottlenecks. (arXiv:2311.05014v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05014","description":"<p>Pretrained language models (PLMs) have made significant strides in various\nnatural language processing tasks. However, the lack of interpretability due to\ntheir ``black-box'' nature poses challenges for responsible implementation.\nAlthough previous studies have attempted to improve interpretability by using,\ne.g., attention weights in self-attention layers, these weights often lack\nclarity, readability, and intuitiveness. In this research, we propose a novel\napproach to interpreting PLMs by employing high-level, meaningful concepts that\nare easily understandable for humans. For example, we learn the concept of\n``Food'' and investigate how it influences the prediction of a model's\nsentiment towards a restaurant review. We introduce C$^3$M, which combines\nhuman-annotated and machine-generated concepts to extract hidden neurons\ndesigned to encapsulate semantically meaningful and task-specific concepts.\nThrough empirical evaluations on real-world datasets, we manifest that our\napproach offers valuable insights to interpret PLM behavior, helps diagnose\nmodel failures, and enhances model robustness amidst noisy concept labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bo_Y/0/1/0/all/0/1\">Yuan Bo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jundong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"First Tragedy, then Parse: History Repeats Itself in the New Era of Large Language Models. (arXiv:2311.05020v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05020","description":"<p>Many NLP researchers are experiencing an existential crisis triggered by the\nastonishing success of ChatGPT and other systems based on large language models\n(LLMs). After such a disruptive change to our understanding of the field, what\nis left to do? Taking a historical lens, we look for guidance from the first\nera of LLMs, which began in 2005 with large $n$-gram models for machine\ntranslation. We identify durable lessons from the first era, and more\nimportantly, we identify evergreen problems where NLP researchers can continue\nto make meaningful contributions in areas where LLMs are ascendant. Among these\nlessons, we discuss the primacy of hardware advancement in shaping the\navailability and importance of scale, as well as the urgent challenge of\nquality evaluation, both automated and human. We argue that disparities in\nscale are transient and that researchers can work to reduce them; that data,\nrather than hardware, is still a bottleneck for many meaningful applications;\nthat meaningful evaluation informed by actual use is still an open problem; and\nthat there is still room for speculative approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1\">Naomi Saphra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleisig_E/0/1/0/all/0/1\">Eve Fleisig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1\">Adam Lopez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Translation of Attention Patterns in VQA Models to Natural Language. (arXiv:2311.05043v1 [cs.CV])","link":"http://arxiv.org/abs/2311.05043","description":"<p>Converting a model's internals to text can yield human-understandable\ninsights about the model. Inspired by the recent success of training-free\napproaches for image captioning, we propose ZS-A2T, a zero-shot framework that\ntranslates the transformer attention of a given model into natural language\nwithout requiring any training. We consider this in the context of Visual\nQuestion Answering (VQA). ZS-A2T builds on a pre-trained large language model\n(LLM), which receives a task prompt, question, and predicted answer, as inputs.\nThe LLM is guided to select tokens which describe the regions in the input\nimage that the VQA model attended to. Crucially, we determine this similarity\nby exploiting the text-image matching capabilities of the underlying VQA model.\nOur framework does not require any training and allows the drop-in replacement\nof different guiding sources (e.g. attribution instead of attention maps), or\nlanguage models. We evaluate this novel task on textual explanation datasets\nfor VQA, giving state-of-the-art performances for the zero-shot setting on\nGQA-REX and VQA-X. Our code is available at:\nhttps://github.com/ExplainableML/ZS-A2T.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salewski_L/0/1/0/all/0/1\">Leonard Salewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koepke_A/0/1/0/all/0/1\">A. Sophia Koepke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lensch_H/0/1/0/all/0/1\">Hendrik P. A. Lensch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepLearningBrasil@LT-EDI-2023: Exploring Deep Learning Techniques for Detecting Depression in Social Media Text. (arXiv:2311.05047v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05047","description":"<p>In this paper, we delineate the strategy employed by our team,\nDeepLearningBrasil, which secured us the first place in the shared task\nDepSign-LT-EDI@RANLP-2023, achieving a 47.0% Macro F1-Score and a notable 2.4%\nadvantage. The task was to classify social media texts into three distinct\nlevels of depression - \"not depressed,\" \"moderately depressed,\" and \"severely\ndepressed.\" Leveraging the power of the RoBERTa and DeBERTa models, we further\npre-trained them on a collected Reddit dataset, specifically curated from\nmental health-related Reddit's communities (Subreddits), leading to an enhanced\nunderstanding of nuanced mental health discourse. To address lengthy textual\ndata, we used truncation techniques that retained the essence of the content by\nfocusing on its beginnings and endings. Our model was robust against unbalanced\ndata by incorporating sample weights into the loss. Cross-validation and\nensemble techniques were then employed to combine our k-fold trained models,\ndelivering an optimal solution. The accompanying code is made available for\ntransparency and further development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_E/0/1/0/all/0/1\">Eduardo Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomes_J/0/1/0/all/0/1\">Juliana Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_A/0/1/0/all/0/1\">Adalberto Barbosa J&#xfa;nior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borges_C/0/1/0/all/0/1\">Cardeque Borges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">N&#xe1;dia da Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Brasil at ABSAPT 2022: Portuguese Transformer Ensemble Approaches. (arXiv:2311.05051v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05051","description":"<p>Aspect-based Sentiment Analysis (ABSA) is a task whose objective is to\nclassify the individual sentiment polarity of all entities, called aspects, in\na sentence. The task is composed of two subtasks: Aspect Term Extraction (ATE),\nidentify all aspect terms in a sentence; and Sentiment Orientation Extraction\n(SOE), given a sentence and its aspect terms, the task is to determine the\nsentiment polarity of each aspect term (positive, negative or neutral). This\narticle presents we present our participation in Aspect-Based Sentiment\nAnalysis in Portuguese (ABSAPT) 2022 at IberLEF 2022. We submitted the best\nperforming systems, achieving new state-of-the-art results on both subtasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomes_J/0/1/0/all/0/1\">Juliana Resplande Santanna Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_E/0/1/0/all/0/1\">Eduardo Augusto Santos Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_A/0/1/0/all/0/1\">Adalberto Ferreira Barbosa Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_R/0/1/0/all/0/1\">Ruan Chaves Rodrigues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_D/0/1/0/all/0/1\">Diogo Fernandes Costa Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maia_D/0/1/0/all/0/1\">Dyonnatan Ferreira Maia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">N&#xe1;dia F&#xe9;lix Felipe da Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filho_A/0/1/0/all/0/1\">Arlindo Rodrigues Galv&#xe3;o Filho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_A/0/1/0/all/0/1\">Anderson da Silva Soares</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Framework to Assess (Dis)agreement Among Diverse Rater Groups. (arXiv:2311.05074v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05074","description":"<p>Recent advancements in conversational AI have created an urgent need for\nsafety guardrails that prevent users from being exposed to offensive and\ndangerous content. Much of this work relies on human ratings and feedback, but\ndoes not account for the fact that perceptions of offense and safety are\ninherently subjective and that there may be systematic disagreements between\nraters that align with their socio-demographic identities. Instead, current\nmachine learning approaches largely ignore rater subjectivity and use gold\nstandards that obscure disagreements (e.g., through majority voting). In order\nto better understand the socio-cultural leanings of such tasks, we propose a\ncomprehensive disagreement analysis framework to measure systematic diversity\nin perspectives among different rater subgroups. We then demonstrate its\nutility by applying this framework to a dataset of human-chatbot conversations\nrated by a demographically diverse pool of raters. Our analysis reveals\nspecific rater groups that have more diverse perspectives than the rest, and\ninforms demographic axes that are crucial to consider for safety annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1\">Vinodkumar Prabhakaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Homan_C/0/1/0/all/0/1\">Christopher Homan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aroyo_L/0/1/0/all/0/1\">Lora Aroyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parrish_A/0/1/0/all/0/1\">Alicia Parrish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_A/0/1/0/all/0/1\">Alex Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_M/0/1/0/all/0/1\">Mark D&#xed;az</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Ding Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mental Health Diagnosis in the Digital Age: Harnessing Sentiment Analysis on Social Media Platforms upon Ultra-Sparse Feature Content. (arXiv:2311.05075v1 [cs.LG])","link":"http://arxiv.org/abs/2311.05075","description":"<p>Amid growing global mental health concerns, particularly among vulnerable\ngroups, natural language processing offers a tremendous potential for early\ndetection and intervention of people's mental disorders via analyzing their\npostings and discussions on social media platforms. However, ultra-sparse\ntraining data, often due to vast vocabularies and low-frequency words, hinders\nthe analysis accuracy. Multi-labeling and Co-occurrences of symptoms may also\nblur the boundaries in distinguishing similar/co-related disorders. To address\nthese issues, we propose a novel semantic feature preprocessing technique with\na three-folded structure: 1) mitigating the feature sparsity with a weak\nclassifier, 2) adaptive feature dimension with modulus loops, and 3)\ndeep-mining and extending features among the contexts. With enhanced semantic\nfeatures, we train a machine learning model to predict and classify mental\ndisorders. We utilize the Reddit Mental Health Dataset 2022 to examine\nconditions such as Anxiety, Borderline Personality Disorder (BPD), and\nBipolar-Disorder (BD) and present solutions to the data sparsity challenge,\nhighlighted by 99.81% non-zero elements. After applying our preprocessing\ntechnique, the feature sparsity decreases to 85.4%. Overall, our methods, when\ncompared to seven benchmark models, demonstrate significant performance\nimprovements: 8.0% in accuracy, 0.069 in precision, 0.093 in recall, 0.102 in\nF1 score, and 0.059 in AUC. This research provides foundational insights for\nmental health prediction and monitoring, providing innovative solutions to\nnavigate challenges associated with ultra-sparse data feature and intricate\nmulti-label classification in the domain of mental health analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1\">Haijian Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Ming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_S/0/1/0/all/0/1\">Shengjie Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks. (arXiv:2311.05085v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05085","description":"<p>Large language models (LLMs) are proficient at generating fluent text with\nminimal task-specific supervision. Yet, their ability to provide well-grounded\nrationalizations for knowledge-intensive tasks remains under-explored. Such\ntasks, like commonsense multiple-choice questions, require rationales based on\nworld knowledge to support predictions and refute alternate options. We\nconsider the task of generating knowledge-guided rationalization in natural\nlanguage by using expert-written examples in a few-shot manner. Surprisingly,\ncrowd-workers preferred knowledge-grounded rationales over crowdsourced\nrationalizations, citing their factuality, sufficiency, and comprehensive\nrefutations. Although LLMs-generated rationales were preferable, further\nimprovements in conciseness and novelty are required. In another study, we show\nhow rationalization of incorrect model predictions erodes humans' trust in\nLLM-generated rationales. Motivated by these observations, we create a\ntwo-stage pipeline to review task predictions and eliminate potential incorrect\ndecisions before rationalization, enabling trustworthy rationale generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1\">Aditi Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_S/0/1/0/all/0/1\">Sajjadur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hannah Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_K/0/1/0/all/0/1\">Kushan Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hruschka_E/0/1/0/all/0/1\">Estevam Hruschka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Legal-HNet: Mixing Legal Long-Context Tokens with Hartley Transform. (arXiv:2311.05089v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05089","description":"<p>Since its introduction, the transformers architecture has seen great adoption\nin NLP applications, but it also has limitations. Although the self-attention\nmechanism allows for generating very rich representations of the input text,\nits effectiveness may be limited in specialized domains such as legal, where,\nfor example, language models often have to process very long texts. In this\npaper, we explore alternatives to replace the attention-based layers with\nsimpler token-mixing mechanisms: Hartley and Fourier transforms. Using these\nnon-parametric techniques, we train models with long input documents from\nscratch in the legal domain setting. We also introduce a new hybrid Seq2Seq\narchitecture, a no-attention-based encoder connected with an attention-based\ndecoder, which performs quite well on existing summarization tasks with much\nless compute and memory requirements. We believe that similar, if not better\nperformance, as in the case of long correlations of abstractive text\nsummarization tasks, can be achieved by adopting these simpler infrastructures.\nThis not only makes training models from scratch accessible to more people, but\nalso contributes to the reduction of the carbon footprint during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giofre_D/0/1/0/all/0/1\">Daniele Giofr&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghantasala_S/0/1/0/all/0/1\">Sneha Ghantasala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Large Language Models in Medicine: Progress, Application, and Challenge. (arXiv:2311.05112v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05112","description":"<p>Large language models (LLMs), such as ChatGPT, have achieved substantial\nattention due to their impressive human language understanding and generation\ncapabilities. Therefore, the application of LLMs in medicine to assist\nphysicians and patient care emerges as a promising research direction in both\nartificial intelligence and clinical medicine. To this end, this survey\nprovides a comprehensive overview of the current progress, applications, and\nchallenges faced by LLMs in medicine. Specifically, we aim to address the\nfollowing questions: 1) What are LLMs and how can medical LLMs be built? 2)\nWhat are the downstream performances of medical LLMs? 3) How can medical LLMs\nbe utilized in real-world clinical practice? 4) What challenges arise from the\nuse of medical LLMs? 5) How can we better construct and utilize medical LLMs?\nAs a result, this survey aims to provide insights into the opportunities and\nchallenges of LLMs in medicine and serve as a valuable resource for\nconstructing practical and effective medical LLMs. A regularly updated list of\npractical guide resources of medical LLMs can be found at\nhttps://github.com/AI-in-Health/MedLLMsPracticalGuide.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hongjian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1\">Boyang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1\">Xinyu Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiru Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sam S. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Peilin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yining Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1\">Chengfeng Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset. (arXiv:2311.05113v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05113","description":"<p>Mathematical understanding and reasoning are crucial tasks for assessing the\ncapabilities of artificial intelligence (AI). However, existing benchmarks\neither require just a few steps of reasoning, or only contain a small amount of\ndata in one specific topic, making it hard to analyse AI's behaviour with\nreference to different problems within a specific topic in detail. In this\nwork, we propose Conic10K, a challenging math problem dataset on conic sections\nin Chinese senior high school education. Our dataset contains various problems\nwith different reasoning depths, while only the knowledge from conic sections\nis required. Since the dataset only involves a narrow range of knowledge, it is\neasy to separately analyse the knowledge a model possesses and the reasoning\nability it has. For each problem, we provide a high-quality formal\nrepresentation, the reasoning steps, and the final solution. Experiments show\nthat existing large language models, including GPT-4, exhibit weak performance\non complex reasoning. We hope that our findings could inspire more advanced\ntechniques for precise natural language understanding and reasoning. Our\ndataset and codes are available at https://github.com/whyNLP/Conic10K.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haoyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_W/0/1/0/all/0/1\">Wenyang Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yezeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weiqi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Translation Quality Estimation Exploiting Synthetic Data and Pre-trained Multilingual Encoder. (arXiv:2311.05117v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05117","description":"<p>Translation quality estimation (TQE) is the task of predicting translation\nquality without reference translations. Due to the enormous cost of creating\ntraining data for TQE, only a few translation directions can benefit from\nsupervised training. To address this issue, unsupervised TQE methods have been\nstudied. In this paper, we extensively investigate the usefulness of synthetic\nTQE data and pre-trained multilingual encoders in unsupervised sentence-level\nTQE, both of which have been proven effective in the supervised training\nscenarios. Our experiment on WMT20 and WMT21 datasets revealed that this\napproach can outperform other unsupervised TQE methods on high- and\nlow-resource translation directions in predicting post-editing effort and human\nevaluation score, and some zero-resource translation directions in predicting\npost-editing effort.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuroda_Y/0/1/0/all/0/1\">Yuto Kuroda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujita_A/0/1/0/all/0/1\">Atsushi Fujita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kajiwara_T/0/1/0/all/0/1\">Tomoyuki Kajiwara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ninomiya_T/0/1/0/all/0/1\">Takashi Ninomiya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quranic Conversations: Developing a Semantic Search tool for the Quran using Arabic NLP Techniques. (arXiv:2311.05120v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05120","description":"<p>The Holy Book of Quran is believed to be the literal word of God (Allah) as\nrevealed to the Prophet Muhammad (PBUH) over a period of approximately 23\nyears. It is the book where God provides guidance on how to live a righteous\nand just life, emphasizing principles like honesty, compassion, charity and\njustice, as well as providing rules for personal conduct, family matters,\nbusiness ethics and much more. However, due to constraints related to the\nlanguage and the Quran organization, it is challenging for Muslims to get all\nrelevant ayahs (verses) pertaining to a matter or inquiry of interest. Hence,\nwe developed a Quran semantic search tool which finds the verses pertaining to\nthe user inquiry or prompt. To achieve this, we trained several models on a\nlarge dataset of over 30 tafsirs, where typically each tafsir corresponds to\none verse in the Quran and, using cosine similarity, obtained the tafsir tensor\nwhich is most similar to the prompt tensor of interest, which was then used to\nindex for the corresponding ayah in the Quran. Using the SNxLM model, we were\nable to achieve a cosine similarity score as high as 0.97 which corresponds to\nthe abdu tafsir for a verse relating to financial matters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shohoud_Y/0/1/0/all/0/1\">Yasser Shohoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoman_M/0/1/0/all/0/1\">Maged Shoman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelazim_S/0/1/0/all/0/1\">Sarah Abdelazim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-supervised Deep Cognate Detection Framework for Low-Resourced Languages Using Morphological Knowledge of Closely-Related Languages. (arXiv:2311.05155v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05155","description":"<p>Exploiting cognates for transfer learning in under-resourced languages is an\nexciting opportunity for language understanding tasks, including unsupervised\nmachine translation, named entity recognition and information retrieval.\nPrevious approaches mainly focused on supervised cognate detection tasks based\non orthographic, phonetic or state-of-the-art contextual language models, which\nunder-perform for most under-resourced languages. This paper proposes a novel\nlanguage-agnostic weakly-supervised deep cognate detection framework for\nunder-resourced languages using morphological knowledge from closely related\nlanguages. We train an encoder to gain morphological knowledge of a language\nand transfer the knowledge to perform unsupervised and weakly-supervised\ncognate detection tasks with and without the pivot language for the\nclosely-related languages. While unsupervised, it overcomes the need for\nhand-crafted annotation of cognates. We performed experiments on different\npublished cognate detection datasets across language families and observed not\nonly significant improvement over the state-of-the-art but also our method\noutperformed the state-of-the-art supervised and unsupervised methods. Our\nmodel can be extended to a wide range of languages from any language family as\nit overcomes the requirement of the annotation of the cognate pairs for\ntraining. The code and dataset building scripts can be found at\nhttps://github.com/koustavagoswami/Weakly_supervised-Cognate_Detection\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goswami_K/0/1/0/all/0/1\">Koustava Goswami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rani_P/0/1/0/all/0/1\">Priya Rani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fransen_T/0/1/0/all/0/1\">Theodorus Fransen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCrae_J/0/1/0/all/0/1\">John P. McCrae</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization. (arXiv:2311.05161v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05161","description":"<p>Large Language Models (LLMs) are proficient in natural language processing\ntasks, but their deployment is often restricted by extensive parameter sizes\nand computational demands. This paper focuses on post-training quantization\n(PTQ) in LLMs, specifically 4-bit weight and 8-bit activation (W4A8)\nquantization, to enhance computational efficiency -- a topic less explored\ncompared to weight-only quantization. We present two innovative techniques:\nactivation-quantization-aware scaling (AQAS) and sequence-length-aware\ncalibration (SLAC) to enhance PTQ by considering the combined effects on\nweights and activations and aligning calibration sequence lengths to target\ntasks. Moreover, we introduce dINT, a hybrid data format combining integer and\ndenormal representations, to address the underflow issue in W4A8 quantization,\nwhere small values are rounded to zero. Through rigorous evaluations of LLMs,\nincluding OPT and LLaMA, we demonstrate that our techniques significantly boost\ntask accuracies to levels comparable with full-precision models. By developing\narithmetic units compatible with dINT, we further confirm that our methods\nyield a 2$\\times$ hardware efficiency improvement compared to 8-bit integer MAC\nunit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jangwhan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1\">Seungcheol Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Seok Joong Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_W/0/1/0/all/0/1\">Wonyong Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jungwook Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models and Prompt Engineering for Biomedical Query Focused Multi-Document Summarisation. (arXiv:2311.05169v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05169","description":"<p>This paper reports on the use of prompt engineering and GPT-3.5 for\nbiomedical query-focused multi-document summarisation. Using GPT-3.5 and\nappropriate prompts, our system achieves top ROUGE-F1 results in the task of\nobtaining short-paragraph-sized answers to biomedical questions in the 2023\nBioASQ Challenge (BioASQ 11b). This paper confirms what has been observed in\nother domains: 1) Prompts that incorporated few-shot samples generally improved\non their counterpart zero-shot variants; 2) The largest improvement was\nachieved by retrieval augmented generation. The fact that these prompts allow\nour top runs to rank within the top two runs of BioASQ 11b demonstrate the\npower of using adequate prompts for Large Language Models in general, and\nGPT-3.5 in particular, for query-focused summarisation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Molla_D/0/1/0/all/0/1\">Diego Moll&#xe1;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PRODIGy: a PROfile-based DIalogue Generation dataset. (arXiv:2311.05195v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05195","description":"<p>Providing dialogue agents with a profile representation can improve their\nconsistency and coherence, leading to better conversations. However, current\nprofile-based dialogue datasets for training such agents contain either\nexplicit profile representations that are simple and dialogue-specific, or\nimplicit representations that are difficult to collect. In this work, we\npropose a unified framework in which we bring together both standard and more\nsophisticated profile representations by creating a new resource where each\ndialogue is aligned with all possible speaker representations such as\ncommunication style, biographies, and personality. This framework allows to\ntest several baselines built using generative language models with several\nprofile configurations. The automatic evaluation shows that profile-based\nmodels have better generalisation capabilities than models trained on dialogues\nonly, both in-domain and cross-domain settings. These results are consistent\nfor fine-tuned models and instruction-based LLMs. Additionally, human\nevaluation demonstrates a clear preference for generations consistent with both\nprofile and context. Finally, to account for possible privacy concerns, all\nexperiments are done under two configurations: inter-character and\nintra-character. In the former, the LM stores the information about the\ncharacter in its internal representation, while in the latter, the LM does not\nretain any personal information but uses it only at inference time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Occhipinti_D/0/1/0/all/0/1\">Daniela Occhipinti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tekiroglu_S/0/1/0/all/0/1\">Serra Sinem Tekiroglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerini_M/0/1/0/all/0/1\">Marco Guerini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. (arXiv:2311.05232v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05232","description":"<p>The emergence of large language models (LLMs) has marked a significant\nbreakthrough in natural language processing (NLP), leading to remarkable\nadvancements in text understanding and generation. Nevertheless, alongside\nthese strides, LLMs exhibit a critical tendency to produce hallucinations,\nresulting in content that is inconsistent with real-world facts or user inputs.\nThis phenomenon poses substantial challenges to their practical deployment and\nraises concerns over the reliability of LLMs in real-world scenarios, which\nattracts increasing attention to detect and mitigate these hallucinations. In\nthis survey, we aim to provide a thorough and in-depth overview of recent\nadvances in the field of LLM hallucinations. We begin with an innovative\ntaxonomy of LLM hallucinations, then delve into the factors contributing to\nhallucinations. Subsequently, we present a comprehensive overview of\nhallucination detection methods and benchmarks. Additionally, representative\napproaches designed to mitigate hallucinations are introduced accordingly.\nFinally, we analyze the challenges that highlight the current limitations and\nformulate open questions, aiming to delineate pathways for future research on\nhallucinations in LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Weijiang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Weitao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Weihong Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhangyin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haotian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qianglong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Weihua Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiaocheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model-Based Minimum Bayes Risk Decoding. (arXiv:2311.05263v1 [cs.AI])","link":"http://arxiv.org/abs/2311.05263","description":"<p>Minimum Bayes Risk (MBR) decoding has been shown to be a powerful alternative\nto beam search decoding in a variety of text generation tasks. MBR decoding\nselects a hypothesis from a pool of hypotheses that has the least expected risk\nunder a probability model according to a given utility function. Since it is\nimpractical to compute the expected risk exactly over all possible hypotheses,\ntwo approximations are commonly used in MBR. First, it integrates over a\nsampled set of hypotheses rather than over all possible hypotheses. Second, it\nestimates the probability of each hypothesis using a Monte Carlo estimator.\nWhile the first approximation is necessary to make it computationally feasible,\nthe second is not essential since we typically have access to the model\nprobability at inference time. We propose Model-Based MBR (MBMBR), a variant of\nMBR that uses the model probability itself as the estimate of the probability\ndistribution instead of the Monte Carlo estimate. We show analytically and\nempirically that the model-based estimate is more promising than the Monte\nCarlo estimate in text generation tasks. Our experiments show that MBMBR\noutperforms MBR in several text generation tasks, both with encoder-decoder\nmodels and with large language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jinnai_Y/0/1/0/all/0/1\">Yuu Jinnai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morimura_T/0/1/0/all/0/1\">Tetsuro Morimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honda_U/0/1/0/all/0/1\">Ukyo Honda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ariu_K/0/1/0/all/0/1\">Kaito Ariu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abe_K/0/1/0/all/0/1\">Kenshi Abe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Waste a Single Annotation: Improving Single-Label Classifiers Through Soft Labels. (arXiv:2311.05265v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05265","description":"<p>In this paper, we address the limitations of the common data annotation and\ntraining methods for objective single-label classification tasks. Typically,\nwhen annotating such tasks annotators are only asked to provide a single label\nfor each sample and annotator disagreement is discarded when a final hard label\nis decided through majority voting. We challenge this traditional approach,\nacknowledging that determining the appropriate label can be difficult due to\nthe ambiguity and lack of context in the data samples. Rather than discarding\nthe information from such ambiguous annotations, our soft label method makes\nuse of them for training. Our findings indicate that additional annotator\ninformation, such as confidence, secondary label and disagreement, can be used\nto effectively generate soft labels. Training classifiers with these soft\nlabels then leads to improved performance and calibration on the hard label\ntest set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Ben Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yue Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1\">Yida Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bontcheva_K/0/1/0/all/0/1\">Kalina Bontcheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingyi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modelling prospective memory and resilient situated communications via Wizard of Oz. (arXiv:2311.05268v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05268","description":"<p>This abstract presents a scenario for human-robot action in a home setting\ninvolving an older adult and a robot. The scenario is designed to explore the\nenvisioned modelling of memory for communication with a socially assistive\nrobots (SAR). The scenario will enable the gathering of data on failures of\nspeech technology and human-robot communication involving shared memory that\nmay occur during daily activities such as a music-listening activity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broz_F/0/1/0/all/0/1\">Frank Broz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neerincx_M/0/1/0/all/0/1\">Mark Neerincx</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Inference from Text: Unveiling Interactions between Variables. (arXiv:2311.05286v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05286","description":"<p>Adjusting for latent covariates is crucial for estimating causal effects from\nobservational textual data. Most existing methods only account for confounding\ncovariates that affect both treatment and outcome, potentially leading to\nbiased causal effects. This bias arises from insufficient consideration of\nnon-confounding covariates, which are relevant only to either the treatment or\nthe outcome. In this work, we aim to mitigate the bias by unveiling\ninteractions between different variables to disentangle the non-confounding\ncovariates when estimating causal effects from text. The disentangling process\nensures covariates only contribute to their respective objectives, enabling\nindependence between variables. Additionally, we impose a constraint to balance\nrepresentations from the treatment group and control group to alleviate\nselection bias. We conduct experiments on two different treatment factors under\nvarious scenarios, and the proposed model significantly outperforms recent\nstrong baselines. Furthermore, our thorough analysis on earnings call\ntranscripts demonstrates that our model can effectively disentangle the\nvariables, and further investigations into real-world scenarios provide\nguidance for investors to make informed decisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuxiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeeLM: Dependency-enhanced Large Language Model for Sentence Embeddings. (arXiv:2311.05296v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05296","description":"<p>Recent studies have proposed using large language models (LLMs) for sentence\nembeddings. However, most existing LLMs are built with an autoregressive\narchitecture that primarily captures forward dependencies while neglecting\nbackward dependencies. Previous work has highlighted the importance of backward\ndependencies in improving sentence embeddings. To address this issue, in this\npaper, we first present quantitative evidence demonstrating the limited\nlearning of backward dependencies in LLMs. Then, we propose a novel approach\ncalled Dependency-Enhanced Large Language Model (DeeLM) to improve sentence\nembeddings. Specifically, we found a turning point in LLMs, where surpassing\nspecific LLM layers leads to a significant performance drop in the semantic\ntextual similarity (STS) task. STS is a crucial task for evaluating sentence\nembeddings. We then extract the layers after the turning point to make them\nbidirectional, allowing for the learning of backward dependencies. Extensive\nexperiments demonstrate that DeeLM outperforms baselines and achieves\nstate-of-the-art performance across various STS tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do personality tests generalize to Large Language Models?. (arXiv:2311.05297v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05297","description":"<p>With large language models (LLMs) appearing to behave increasingly human-like\nin text-based interactions, it has become popular to attempt to evaluate\nvarious properties of these models using tests originally designed for humans.\nWhile re-using existing tests is a resource-efficient way to evaluate LLMs,\ncareful adjustments are usually required to ensure that test results are even\nvalid across human sub-populations. Thus, it is not clear to what extent\ndifferent tests' validity generalizes to LLMs. In this work, we provide\nevidence that LLMs' responses to personality tests systematically deviate from\ntypical human responses, implying that these results cannot be interpreted in\nthe same way as human test results. Concretely, reverse-coded items (e.g. \"I am\nintroverted\" vs \"I am extraverted\") are often both answered affirmatively by\nLLMs. In addition, variation across different prompts designed to \"steer\" LLMs\nto simulate particular personality types does not follow the clear separation\ninto five independent personality factors from human samples. In light of these\nresults, we believe it is important to pay more attention to tests' validity\nfor LLMs before drawing strong conclusions about potentially ill-defined\nconcepts like LLMs' \"personality\".\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dorner_F/0/1/0/all/0/1\">Florian E. Dorner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhr_T/0/1/0/all/0/1\">Tom S&#xfc;hr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samadi_S/0/1/0/all/0/1\">Samira Samadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelava_A/0/1/0/all/0/1\">Augustin Kelava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving. (arXiv:2311.05332v1 [cs.CV])","link":"http://arxiv.org/abs/2311.05332","description":"<p>The pursuit of autonomous driving technology hinges on the sophisticated\nintegration of perception, decision-making, and control systems. Traditional\napproaches, both data-driven and rule-based, have been hindered by their\ninability to grasp the nuance of complex driving environments and the\nintentions of other road users. This has been a significant bottleneck,\nparticularly in the development of common sense reasoning and nuanced scene\nunderstanding necessary for safe and reliable autonomous driving. The advent of\nVisual Language Models (VLM) represents a novel frontier in realizing fully\nautonomous vehicle driving. This report provides an exhaustive evaluation of\nthe latest state-of-the-art VLM, \\modelnamefull, and its application in\nautonomous driving scenarios. We explore the model's abilities to understand\nand reason about driving scenes, make decisions, and ultimately act in the\ncapacity of a driver. Our comprehensive tests span from basic scene recognition\nto complex causal reasoning and real-time decision-making under varying\nconditions. Our findings reveal that \\modelname demonstrates superior\nperformance in scene understanding and causal reasoning compared to existing\nautonomous systems. It showcases the potential to handle out-of-distribution\nscenarios, recognize intentions, and make informed decisions in real driving\ncontexts. However, challenges remain, particularly in direction discernment,\ntraffic light recognition, vision grounding, and spatial reasoning tasks. These\nlimitations underscore the need for further research and development. Project\nis now available on GitHub for interested parties to access and utilize:\n\\url{https://github.com/PJLab-ADG/GPT4V-AD-Exploration}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Licheng Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuemeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1\">Daocheng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1\">Pinlong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Linran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_D/0/1/0/all/0/1\">Dengke Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shaoyan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yeqi Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xinyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_M/0/1/0/all/0/1\">Min Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shuanglu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Botian Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"There's no Data Like Better Data: Using QE Metrics for MT Data Filtering. (arXiv:2311.05350v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05350","description":"<p>Quality Estimation (QE), the evaluation of machine translation output without\nthe need of explicit references, has seen big improvements in the last years\nwith the use of neural metrics. In this paper we analyze the viability of using\nQE metrics for filtering out bad quality sentence pairs in the training data of\nneural machine translation systems~(NMT). While most corpus filtering methods\nare focused on detecting noisy examples in collections of texts, usually huge\namounts of web crawled data, QE models are trained to discriminate more\nfine-grained quality differences. We show that by selecting the highest quality\nsentence pairs in the training data, we can improve translation quality while\nreducing the training size by half. We also provide a detailed analysis of the\nfiltering results, which highlights the differences between both approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peter_J/0/1/0/all/0/1\">Jan-Thorsten Peter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vilar_D/0/1/0/all/0/1\">David Vilar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deutsch_D/0/1/0/all/0/1\">Daniel Deutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finkelstein_M/0/1/0/all/0/1\">Mara Finkelstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juraska_J/0/1/0/all/0/1\">Juraj Juraska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1\">Markus Freitag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TencentLLMEval: A Hierarchical Evaluation of Real-World Capabilities for Human-Aligned LLMs. (arXiv:2311.05374v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05374","description":"<p>Large language models (LLMs) have shown impressive capabilities across\nvarious natural language tasks. However, evaluating their alignment with human\npreferences remains a challenge. To this end, we propose a comprehensive human\nevaluation framework to assess LLMs' proficiency in following instructions on\ndiverse real-world tasks. We construct a hierarchical task tree encompassing 7\nmajor areas covering over 200 categories and over 800 tasks, which covers\ndiverse capabilities such as question answering, reasoning, multiturn dialogue,\nand text generation, to evaluate LLMs in a comprehensive and in-depth manner.\nWe also design detailed evaluation standards and processes to facilitate\nconsistent, unbiased judgments from human evaluators. A test set of over 3,000\ninstances is released, spanning different difficulty levels and knowledge\ndomains. Our work provides a standardized methodology to evaluate human\nalignment in LLMs for both English and Chinese. We also analyze the feasibility\nof automating parts of evaluation with a strong LLM (GPT-4). Our framework\nsupports a thorough assessment of LLMs as they are integrated into real-world\napplications. We have made publicly available the task tree, TencentLLMEval\ndataset, and evaluation methodology which have been demonstrated as effective\nin assessing the performance of Tencent Hunyuan LLMs. By doing so, we aim to\nfacilitate the benchmarking of advances in the development of safe and\nhuman-aligned LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Shuyi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wenlin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaobo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Donlin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lifeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xinhua Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_P/0/1/0/all/0/1\">Pengzhi Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yujie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhichao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyou Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jing Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuhong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memorisation Cartography: Mapping out the Memorisation-Generalisation Continuum in Neural Machine Translation. (arXiv:2311.05379v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05379","description":"<p>When training a neural network, it will quickly memorise some source-target\nmappings from your dataset but never learn some others. Yet, memorisation is\nnot easily expressed as a binary feature that is good or bad: individual\ndatapoints lie on a memorisation-generalisation continuum. What determines a\ndatapoint's position on that spectrum, and how does that spectrum influence\nneural models' performance? We address these two questions for neural machine\ntranslation (NMT) models. We use the counterfactual memorisation metric to (1)\nbuild a resource that places 5M NMT datapoints on a memorisation-generalisation\nmap, (2) illustrate how the datapoints' surface-level characteristics and a\nmodels' per-datum training signals are predictive of memorisation in NMT, (3)\nand describe the influence that subsets of that map have on NMT systems'\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dankers_V/0/1/0/all/0/1\">Verna Dankers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1\">Ivan Titov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1\">Dieuwke Hupkes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mirror: A Universal Framework for Various Information Extraction Tasks. (arXiv:2311.05419v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05419","description":"<p>Sharing knowledge between information extraction tasks has always been a\nchallenge due to the diverse data formats and task variations. Meanwhile, this\ndivergence leads to information waste and increases difficulties in building\ncomplex applications in real scenarios. Recent studies often formulate IE tasks\nas a triplet extraction problem. However, such a paradigm does not support\nmulti-span and n-ary extraction, leading to weak versatility. To this end, we\nreorganize IE problems into unified multi-slot tuples and propose a universal\nframework for various IE tasks, namely Mirror. Specifically, we recast existing\nIE tasks as a multi-span cyclic graph extraction problem and devise a\nnon-autoregressive graph decoding algorithm to extract all spans in a single\nstep. It is worth noting that this graph structure is incredibly versatile, and\nit supports not only complex IE tasks, but also machine reading comprehension\nand classification tasks. We manually construct a corpus containing 57 datasets\nfor model pretraining, and conduct experiments on 30 datasets across 8\ndownstream tasks. The experimental results demonstrate that our model has\ndecent compatibility and outperforms or reaches competitive performance with\nSOTA systems under few-shot and zero-shot settings. The code, model weights,\nand pretraining corpus are available at https://github.com/Spico197/Mirror .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Junfei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zijian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mengsong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guoliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoye Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhefeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huai_B/0/1/0/all/0/1\">Baoxing Huai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents. (arXiv:2311.05437v1 [cs.CV])","link":"http://arxiv.org/abs/2311.05437","description":"<p>LLaVA-Plus is a general-purpose multimodal assistant that expands the\ncapabilities of large multimodal models. It maintains a skill repository of\npre-trained vision and vision-language models and can activate relevant tools\nbased on users' inputs to fulfill real-world tasks. LLaVA-Plus is trained on\nmultimodal instruction-following data to acquire the ability to use tools,\ncovering visual understanding, generation, external knowledge retrieval, and\ncompositions. Empirical results show that LLaVA-Plus outperforms LLaVA in\nexisting capabilities and exhibits new ones. It is distinct in that the image\nquery is directly grounded and actively engaged throughout the entire human-AI\ninteraction sessions, significantly improving tool use performance and enabling\nnew scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haotian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Feng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1\">Tianhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1\">Xueyan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cognitively Inspired Components for Social Conversational Agents. (arXiv:2311.05450v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05450","description":"<p>Current conversational agents (CA) have seen improvement in conversational\nquality in recent years due to the influence of large language models (LLMs)\nlike GPT3. However, two key categories of problem remain. Firstly there are the\nunique technical problems resulting from the approach taken in creating the CA,\nsuch as scope with retrieval agents and the often nonsensical answers of former\ngenerative agents. Secondly, humans perceive CAs as social actors, and as a\nresult expect the CA to adhere to social convention. Failure on the part of the\nCA in this respect can lead to a poor interaction and even the perception of\nthreat by the user. As such, this paper presents a survey highlighting a\npotential solution to both categories of problem through the introduction of\ncognitively inspired additions to the CA. Through computational facsimiles of\nsemantic and episodic memory, emotion, working memory, and the ability to\nlearn, it is possible to address both the technical and social problems\nencountered by CAs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clay_A/0/1/0/all/0/1\">Alex Clay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_E/0/1/0/all/0/1\">Eduardo Alonso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondragon_E/0/1/0/all/0/1\">Esther Mondrag&#xf3;n</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"All Should Be Equal in the Eyes of Language Models: Counterfactually Aware Fair Text Generation. (arXiv:2311.05451v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05451","description":"<p>Fairness in Language Models (LMs) remains a longstanding challenge, given the\ninherent biases in training data that can be perpetuated by models and affect\nthe downstream tasks. Recent methods employ expensive retraining or attempt\ndebiasing during inference by constraining model outputs to contrast from a\nreference set of biased templates or exemplars. Regardless, they dont address\nthe primary goal of fairness to maintain equitability across different\ndemographic groups. In this work, we posit that inferencing LMs to generate\nunbiased output for one demographic under a context ensues from being aware of\noutputs for other demographics under the same context. To this end, we propose\nCounterfactually Aware Fair InferencE (CAFIE), a framework that dynamically\ncompares the model understanding of diverse demographics to generate more\nequitable sentences. We conduct an extensive empirical evaluation using base\nLMs of varying sizes and across three diverse datasets and found that CAFIE\noutperforms strong baselines. CAFIE produces fairer text and strikes the best\nbalance between fairness and language modeling capability\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1\">Pragyan Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Java_A/0/1/0/all/0/1\">Abhinav Java</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jandial_S/0/1/0/all/0/1\">Surgan Jandial</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahid_S/0/1/0/all/0/1\">Simra Shahid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furniturewala_S/0/1/0/all/0/1\">Shaz Furniturewala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1\">Balaji Krishnamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1\">Sumit Bhatia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Representation Distillation via Information Bottleneck Principle. (arXiv:2311.05472v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05472","description":"<p>Pre-trained language models (PLMs) have recently shown great success in text\nrepresentation field. However, the high computational cost and high-dimensional\nrepresentation of PLMs pose significant challenges for practical applications.\nTo make models more accessible, an effective method is to distill large models\ninto smaller representation models. In order to relieve the issue of\nperformance degradation after distillation, we propose a novel Knowledge\nDistillation method called IBKD. This approach is motivated by the Information\nBottleneck principle and aims to maximize the mutual information between the\nfinal representation of the teacher and student model, while simultaneously\nreducing the mutual information between the student model's representation and\nthe input data. This enables the student model to preserve important learned\ninformation while avoiding unnecessary information, thus reducing the risk of\nover-fitting. Empirical studies on two main downstream applications of text\nrepresentation (Semantic Textual Similarity and Dense Retrieval tasks)\ndemonstrate the effectiveness of our proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanzhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_D/0/1/0/all/0/1\">Dingkun Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zehan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards End-to-End Spoken Grammatical Error Correction. (arXiv:2311.05550v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05550","description":"<p>Grammatical feedback is crucial for L2 learners, teachers, and testers.\nSpoken grammatical error correction (GEC) aims to supply feedback to L2\nlearners on their use of grammar when speaking. This process usually relies on\na cascaded pipeline comprising an ASR system, disfluency removal, and GEC, with\nthe associated concern of propagating errors between these individual modules.\nIn this paper, we introduce an alternative \"end-to-end\" approach to spoken GEC,\nexploiting a speech recognition foundation model, Whisper. This foundation\nmodel can be used to replace the whole framework or part of it, e.g., ASR and\ndisfluency removal. These end-to-end approaches are compared to more standard\ncascaded approaches on the data obtained from a free-speaking spoken language\nassessment test, Linguaskill. Results demonstrate that end-to-end spoken GEC is\npossible within this architecture, but the lack of available data limits\ncurrent performance compared to a system using large quantities of text-based\nGEC data. Conversely, end-to-end disfluency detection and removal, which is\neasier for the attention-based Whisper to learn, does outperform cascaded\napproaches. Additionally, the paper discusses the challenges of providing\nfeedback to candidates when using end-to-end systems for spoken GEC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banno_S/0/1/0/all/0/1\">Stefano Bann&#xf2;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_M/0/1/0/all/0/1\">Mengjie Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knill_K/0/1/0/all/0/1\">Kate M. Knill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark J.F. Gales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Iron(ic) Melting Pot: Reviewing Human Evaluation in Humour, Irony and Sarcasm Generation. (arXiv:2311.05552v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05552","description":"<p>Human evaluation is often considered to be the gold standard method of\nevaluating a Natural Language Generation system. However, whilst its importance\nis accepted by the community at large, the quality of its execution is often\nbrought into question. In this position paper, we argue that the generation of\nmore esoteric forms of language - humour, irony and sarcasm - constitutes a\nsubdomain where the characteristics of selected evaluator panels are of utmost\nimportance, and every effort should be made to report demographic\ncharacteristics wherever possible, in the interest of transparency and\nreplicability. We support these claims with an overview of each language form\nand an analysis of examples in terms of how their interpretation is affected by\ndifferent participant variables. We additionally perform a critical survey of\nrecent works in NLG to assess how well evaluation procedures are reported in\nthis subdomain, and note a severe lack of open reporting of evaluator\ndemographic information, and a significant reliance on crowdsourcing platforms\nfor recruitment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loakman_T/0/1/0/all/0/1\">Tyler Loakman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maladry_A/0/1/0/all/0/1\">Aaron Maladry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Removing RLHF Protections in GPT-4 via Fine-Tuning. (arXiv:2311.05553v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05553","description":"<p>As large language models (LLMs) have increased in their capabilities, so does\ntheir potential for dual use. To reduce harmful outputs, produces and vendors\nof LLMs have used reinforcement learning with human feedback (RLHF). In tandem,\nLLM vendors have been increasingly enabling fine-tuning of their most powerful\nmodels. However, concurrent work has shown that fine-tuning can remove RLHF\nprotections. We may expect that the most powerful models currently available\n(GPT-4) are less susceptible to fine-tuning attacks.\n</p>\n<p>In this work, we show the contrary: fine-tuning allows attackers to remove\nRLHF protections with as few as 340 examples and a 95% success rate. These\ntraining examples can be automatically generated with weaker models. We further\nshow that removing RLHF protections does not decrease usefulness on\nnon-censored outputs, providing evidence that our fine-tuning strategy does not\ndecrease usefulness despite using weaker models to generate training data. Our\nresults show the need for further research on protections on LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Q/0/1/0/all/0/1\">Qiusi Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1\">Richard Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bindu_R/0/1/0/all/0/1\">Rohan Bindu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Daniel Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations. (arXiv:2311.05584v1 [cs.LG])","link":"http://arxiv.org/abs/2311.05584","description":"<p>Large language models (LLMs) have emerged as powerful and general solutions\nto many natural language tasks. However, many of the most important\napplications of language generation are interactive, where an agent has to talk\nto a person to reach a desired outcome. For example, a teacher might try to\nunderstand their student's current comprehension level to tailor their\ninstruction accordingly, and a travel agent might ask questions of their\ncustomer to understand their preferences in order to recommend activities they\nmight enjoy. LLMs trained with supervised fine-tuning or \"single-step\" RL, as\nwith standard RLHF, might struggle which tasks that require such goal-directed\nbehavior, since they are not trained to optimize for overall conversational\noutcomes after multiple turns of interaction. In this work, we explore a new\nmethod for adapting LLMs with RL for such goal-directed dialogue. Our key\ninsight is that, though LLMs might not effectively solve goal-directed dialogue\ntasks out of the box, they can provide useful data for solving such tasks by\nsimulating suboptimal but human-like behaviors. Given a textual description of\na goal-directed dialogue task, we leverage LLMs to sample diverse synthetic\nrollouts of hypothetical in-domain human-human interactions. Our algorithm then\nutilizes this dataset with offline reinforcement learning to train an\ninteractive conversational agent that can optimize goal-directed objectives\nover multiple turns. In effect, the LLM produces examples of possible\ninteractions, and RL then processes these examples to learn to perform more\noptimal interactions. Empirically, we show that our proposed approach achieves\nstate-of-the-art performance in various goal-directed dialogue tasks that\ninclude teaching and preference elicitation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Joey Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1\">Anca Dragan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accuracy of a Vision-Language Model on Challenging Medical Cases. (arXiv:2311.05591v1 [cs.CV])","link":"http://arxiv.org/abs/2311.05591","description":"<p>Background: General-purpose large language models that utilize both text and\nimages have not been evaluated on a diverse array of challenging medical cases.\n</p>\n<p>Methods: Using 934 cases from the NEJM Image Challenge published between 2005\nand 2023, we evaluated the accuracy of the recently released Generative\nPre-trained Transformer 4 with Vision model (GPT-4V) compared to human\nrespondents overall and stratified by question difficulty, image type, and skin\ntone. We further conducted a physician evaluation of GPT-4V on 69 NEJM\nclinicopathological conferences (CPCs). Analyses were conducted for models\nutilizing text alone, images alone, and both text and images.\n</p>\n<p>Results: GPT-4V achieved an overall accuracy of 61% (95% CI, 58 to 64%)\ncompared to 49% (95% CI, 49 to 50%) for humans. GPT-4V outperformed humans at\nall levels of difficulty and disagreement, skin tones, and image types; the\nexception was radiographic images, where performance was equivalent between\nGPT-4V and human respondents. Longer, more informative captions were associated\nwith improved performance for GPT-4V but similar performance for human\nrespondents. GPT-4V included the correct diagnosis in its differential for 80%\n(95% CI, 68 to 88%) of CPCs when using text alone, compared to 58% (95% CI, 45\nto 70%) of CPCs when using both images and text.\n</p>\n<p>Conclusions: GPT-4V outperformed human respondents on challenging medical\ncases and was able to synthesize information from both images and text, but\nperformance deteriorated when images were added to highly informative text.\nOverall, our results suggest that multimodal AI models may be useful in medical\ndiagnostic reasoning but that their accuracy may depend heavily on context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Buckley_T/0/1/0/all/0/1\">Thomas Buckley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diao_J/0/1/0/all/0/1\">James A. Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodman_A/0/1/0/all/0/1\">Adam Rodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manrai_A/0/1/0/all/0/1\">Arjun K. Manrai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FAMuS: Frames Across Multiple Sources. (arXiv:2311.05601v1 [cs.CL])","link":"http://arxiv.org/abs/2311.05601","description":"<p>Understanding event descriptions is a central aspect of language processing,\nbut current approaches focus overwhelmingly on single sentences or documents.\nAggregating information about an event \\emph{across documents} can offer a much\nricher understanding. To this end, we present FAMuS, a new corpus of Wikipedia\npassages that \\emph{report} on some event, paired with underlying,\ngenre-diverse (non-Wikipedia) \\emph{source} articles for the same event. Events\nand (cross-sentence) arguments in both report and source are annotated against\nFrameNet, providing broad coverage of different event types. We present results\non two key event understanding tasks enabled by FAMuS: \\emph{source validation}\n-- determining whether a document is a valid source for a target report event\n-- and \\emph{cross-document argument extraction} -- full-document argument\nextraction for a target event from both its report and the correct source\narticle. We release both FAMuS and our models to support further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vashishtha_S/0/1/0/all/0/1\">Siddharth Vashishtha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1\">Alexander Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gantt_W/0/1/0/all/0/1\">William Gantt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1\">Aaron Steven White</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts. (arXiv:2311.05608v1 [cs.CR])","link":"http://arxiv.org/abs/2311.05608","description":"<p>Large vision-language models (VLMs) like GPT-4V represent an unprecedented\nrevolution in the field of artificial intelligence (AI). Compared to\nsingle-modal large language models (LLMs), VLMs possess more versatile\ncapabilities by incorporating additional modalities (e.g., images). Meanwhile,\nthere's a rising enthusiasm in the AI community to develop open-source VLMs,\nsuch as LLaVA and MiniGPT4, which, however, have not undergone rigorous safety\nassessment. In this paper, to demonstrate that more modalities lead to\nunforeseen AI safety issues, we propose FigStep, a novel jailbreaking framework\nagainst VLMs. FigStep feeds harmful instructions into VLMs through the image\nchannel and then uses benign text prompts to induce VLMs to output contents\nthat violate common AI safety policies. Our experimental results show that\nFigStep can achieve an average attack success rate of 94.8% across 2 families\nof popular open-source VLMs, LLaVA and MiniGPT4 (a total of 5 VLMs). Moreover,\nwe demonstrate that the methodology of FigStep can even jailbreak GPT-4V, which\nalready leverages several system-level mechanisms to filter harmful queries.\nAbove all, our experimental results reveal that VLMs are vulnerable to\njailbreaking attacks, which highlights the necessity of novel safety alignments\nbetween visual and textual modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yichen Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ran_D/0/1/0/all/0/1\">Delong Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Conglei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_T/0/1/0/all/0/1\">Tianshuo Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Anyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1\">Sisi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning From How Humans Correct. (arXiv:2102.00225v15 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.00225","description":"<p>In industry NLP application, our manually labeled data has a certain number\nof noisy data. We present a simple method to find the noisy data and re-label\nthem manually, meanwhile we collect the correction information. Then we present\nnovel method to incorporate the human correction information into deep learning\nmodel. Human know how to correct noisy data. So the correction information can\nbe inject into deep learning model. We do the experiment on our own text\nclassification dataset, which is manually labeled, because we re-label the\nnoisy data in our dataset for our industry application. The experiment result\nshows that our method improve the classification accuracy from 91.7% to 92.5%\nin test dataset. The 91.7% accuracy is trained on the corrected dataset, which\nimprove the baseline from 83.3% to 91.7% in test dataset. The accuracy under\nhuman evaluation achieves more than 97%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying Gender Bias Towards Politicians in Cross-Lingual Language Models. (arXiv:2104.07505v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07505","description":"<p>Recent research has demonstrated that large pre-trained language models\nreflect societal biases expressed in natural language. The present paper\nintroduces a simple method for probing language models to conduct a\nmultilingual study of gender bias towards politicians. We quantify the usage of\nadjectives and verbs generated by language models surrounding the names of\npoliticians as a function of their gender. To this end, we curate a dataset of\n250k politicians worldwide, including their names and gender. Our study is\nconducted in seven languages across six different language modeling\narchitectures. The results demonstrate that pre-trained language models' stance\ntowards politicians varies strongly across analyzed languages. We find that\nwhile some words such as dead, and designated are associated with both male and\nfemale politicians, a few specific words such as beautiful and divorced are\npredominantly associated with female politicians. Finally, and contrary to\nprevious findings, our study suggests that larger language models do not tend\nto be significantly more gender-biased than smaller ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stanczak_K/0/1/0/all/0/1\">Karolina Sta&#x144;czak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_S/0/1/0/all/0/1\">Sagnik Ray Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Attention-Based Model for Predicting Contextual Informativeness and Curriculum Learning Applications. (arXiv:2204.09885v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.09885","description":"<p>Both humans and machines learn the meaning of unknown words through\ncontextual information in a sentence, but not all contexts are equally helpful\nfor learning. We introduce an effective method for capturing the level of\ncontextual informativeness with respect to a given target word. Our study makes\nthree main contributions. First, we develop models for estimating contextual\ninformativeness, focusing on the instructional aspect of sentences. Our\nattention-based approach using pre-trained embeddings demonstrates\nstate-of-the-art performance on our single-context dataset and an existing\nmulti-sentence context dataset. Second, we show how our model identifies key\ncontextual elements in a sentence that are likely to contribute most to a\nreader's understanding of the target word. Third, we examine how our contextual\ninformativeness model, originally developed for vocabulary learning\napplications for students, can be used for developing better training curricula\nfor word embedding models in batch learning and few-shot machine learning\nsettings. We believe our results open new possibilities for applications that\nsupport language learning for both human and machine learners.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1\">Sungjin Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frishkoff_G/0/1/0/all/0/1\">Gwen Frishkoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_Thompson_K/0/1/0/all/0/1\">Kevyn Collins-Thompson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpreting Embedding Spaces by Conceptualization. (arXiv:2209.00445v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.00445","description":"<p>One of the main methods for computational interpretation of a text is mapping\nit into a vector in some embedding space. Such vectors can then be used for a\nvariety of textual processing tasks. Recently, most embedding spaces are a\nproduct of training large language models (LLMs). One major drawback of this\ntype of representation is their incomprehensibility to humans. Understanding\nthe embedding space is crucial for several important needs, including the need\nto debug the embedding method and compare it to alternatives, and the need to\ndetect biases hidden in the model. In this paper, we present a novel method of\nunderstanding embeddings by transforming a latent embedding space into a\ncomprehensible conceptual space. We present an algorithm for deriving a\nconceptual space with dynamic on-demand granularity. We devise a new evaluation\nmethod, using either human rater or LLM-based raters, to show that the\nconceptualized vectors indeed represent the semantics of the original latent\nones. We show the use of our method for various tasks, including comparing the\nsemantics of alternative models and tracing the layers of the LLM. The code is\navailable online\nhttps://github.com/adiSimhi/Interpreting-Embedding-Spaces-by-Conceptualization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Simhi_A/0/1/0/all/0/1\">Adi Simhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markovitch_S/0/1/0/all/0/1\">Shaul Markovitch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vicarious Offense and Noise Audit of Offensive Speech Classifiers: Unifying Human and Machine Disagreement on What is Offensive. (arXiv:2301.12534v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.12534","description":"<p>Offensive speech detection is a key component of content moderation. However,\nwhat is offensive can be highly subjective. This paper investigates how machine\nand human moderators disagree on what is offensive when it comes to real-world\nsocial web political discourse. We show that (1) there is extensive\ndisagreement among the moderators (humans and machines); and (2) human and\nlarge-language-model classifiers are unable to predict how other human raters\nwill respond, based on their political leanings. For (1), we conduct a noise\naudit at an unprecedented scale that combines both machine and human responses.\nFor (2), we introduce a first-of-its-kind dataset of vicarious offense. Our\nnoise audit reveals that moderation outcomes vary wildly across different\nmachine moderators. Our experiments with human moderators suggest that\npolitical leanings combined with sensitive issues affect both first-person and\nvicarious offense. The dataset is available through\nhttps://github.com/Homan-Lab/voiced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weerasooriya_T/0/1/0/all/0/1\">Tharindu Cyril Weerasooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Sujan Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Homan_C/0/1/0/all/0/1\">Christopher M. Homan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+KhudaBukhsh_A/0/1/0/all/0/1\">Ashiqur R. KhudaBukhsh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"idT5: Indonesian Version of Multilingual T5 Transformer. (arXiv:2302.00856v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.00856","description":"<p>Indonesian language is spoken by almost 200 million people and is the 10th\nmost spoken language in the world, but it is under-represented in NLP (Natural\nLanguage Processing) research. A sparsity of language resources has hampered\nprevious work on Indonesian. The Transformer is a new architecture rapidly\nbecoming dominant for NLP, surpassing alternatives like convolutional and\nrecurrent neural networks. T5 (Text-to-Text Transfer Transformer) is a\nTransformer model that converts all text-based language problems to\ntext-to-text format for English. The multilingual variant is mT5 (multilingual\nT5) which has shown promising results on many NLP tasks across languages.\nHowever, the size of this multilingual model is a drawback for its application\nin real production applications, which sometimes require only one language. In\nthis study, the mT5 model was adapted for only one language, Indonesian,\nresulting in a pre-trained T5 model that was specific only for Indonesian with\na smaller size. For performance comparison, we fine-tuned this model and the\nmT5 model to the Sentiment Analysis (SA), Question Generation (QG), and\nQuestion Answering (QA) tasks with the exact mechanism and dataset. Fine-tuned\nmodel based on our model achieved 77.18% accuracy on SA, 8% higher than the\nmT5-based model, and obtained nearly the same score as the mT5-based model on\nQG and QA. The results confirm that it is possible to produce a smaller\npre-trained model that maintains comparable yields while reducing the model\nsize by up to 58%. In addition, the resulting model requires less memory, loads\nfaster, and inference times faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fuadi_M/0/1/0/all/0/1\">Mukhlish Fuadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wibawa_A/0/1/0/all/0/1\">Adhi Dharma Wibawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumpeno_S/0/1/0/all/0/1\">Surya Sumpeno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity. (arXiv:2302.04023v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.04023","description":"<p>This paper proposes a framework for quantitatively evaluating interactive\nLLMs such as ChatGPT using publicly available data sets. We carry out an\nextensive technical evaluation of ChatGPT using 23 data sets covering 8\ndifferent common NLP application tasks. We evaluate the multitask, multilingual\nand multi-modal aspects of ChatGPT based on these data sets and a newly\ndesigned multimodal dataset. We find that ChatGPT outperforms LLMs with\nzero-shot learning on most tasks and even outperforms fine-tuned models on some\ntasks. We find that it is better at understanding non-Latin script languages\nthan generating them. It is able to generate multimodal content from textual\nprompts, via an intermediate code generation step. Moreover, we find that\nChatGPT is 63.41% accurate on average in 10 different reasoning categories\nunder logical reasoning, non-textual reasoning, and commonsense reasoning,\nhence making it an unreliable reasoner. It is, for example, better at deductive\nthan inductive reasoning. ChatGPT suffers from hallucination problems like\nother LLMs and it generates more extrinsic hallucinations from its parametric\nmemory as it does not have access to an external knowledge base. Finally, the\ninteractive feature of ChatGPT enables human collaboration with the underlying\nLLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++\non machine translation, in a multi-turn \"prompt engineering\" fashion. We also\nrelease codebase for evaluation set extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bang_Y/0/1/0/all/0/1\">Yejin Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1\">Bryan Wilie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Ziwei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_W/0/1/0/all/0/1\">Willy Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_Q/0/1/0/all/0/1\">Quyet V. Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MLRegTest: A Benchmark for the Machine Learning of Regular Languages. (arXiv:2304.07687v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2304.07687","description":"<p>Evaluating machine learning (ML) systems on their ability to learn known\nclassifiers allows fine-grained examination of the patterns they can learn,\nwhich builds confidence when they are applied to the learning of unknown\nclassifiers. This article presents a new benchmark for ML systems on sequence\nclassification called MLRegTest, which contains training, development, and test\nsets from 1,800 regular languages. Different kinds of formal languages\nrepresent different kinds of long-distance dependencies, and correctly\nidentifying long-distance dependencies in sequences is a known challenge for ML\nsystems to generalize successfully. MLRegTest organizes its languages according\nto their logical complexity (monadic second order, first order, propositional,\nor monomial expressions) and the kind of logical literals (string, tier-string,\nsubsequence, or combinations thereof). The logical complexity and choice of\nliteral provides a systematic way to understand different kinds of\nlong-distance dependencies in regular languages, and therefore to understand\nthe capacities of different ML systems to learn such long-distance\ndependencies. Finally, the performance of different neural networks (simple\nRNN, LSTM, GRU, transformer) on MLRegTest is examined. The main conclusion is\nthat their performance depends significantly on the kind of test set, the class\nof language, and the neural network architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poel_S/0/1/0/all/0/1\">Sam van der Poel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambert_D/0/1/0/all/0/1\">Dakotah Lambert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kostyszyn_K/0/1/0/all/0/1\">Kalina Kostyszyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tiantian Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_R/0/1/0/all/0/1\">Rahul Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andersen_D/0/1/0/all/0/1\">Derek Andersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_J/0/1/0/all/0/1\">Joanne Chau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peterson_E/0/1/0/all/0/1\">Emily Peterson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clair_C/0/1/0/all/0/1\">Cody St. Clair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fodor_P/0/1/0/all/0/1\">Paul Fodor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shibata_C/0/1/0/all/0/1\">Chihiro Shibata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinz_J/0/1/0/all/0/1\">Jeffrey Heinz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sabi\\'a: Portuguese Large Language Models. (arXiv:2304.07880v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.07880","description":"<p>As the capabilities of language models continue to advance, it is conceivable\nthat \"one-size-fits-all\" model will remain as the main paradigm. For instance,\ngiven the vast number of languages worldwide, many of which are low-resource,\nthe prevalent practice is to pretrain a single model on multiple languages. In\nthis paper, we add to the growing body of evidence that challenges this\npractice, demonstrating that monolingual pretraining on the target language\nsignificantly improves models already extensively trained on diverse corpora.\nMore specifically, we further pretrain GPT-J and LLaMA models on Portuguese\ntexts using 3% or less of their original pretraining budget. Few-shot\nevaluations on Poeta, a suite of 14 Portuguese datasets, reveal that our models\noutperform English-centric and multilingual counterparts by a significant\nmargin. Our best model, Sabi\\'a-65B, performs on par with GPT-3.5-turbo. By\nevaluating on datasets originally conceived in the target language as well as\ntranslated ones, we study the contributions of language-specific pretraining in\nterms of 1) capturing linguistic nuances and structures inherent to the target\nlanguage, and 2) enriching the model's knowledge about a domain or culture. Our\nresults indicate that the majority of the benefits stem from the\ndomain-specific knowledge acquired through monolingual pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pires_R/0/1/0/all/0/1\">Ramon Pires</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abonizio_H/0/1/0/all/0/1\">Hugo Abonizio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almeida_T/0/1/0/all/0/1\">Thales Sales Almeida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Human Feedback to Scale Educational Datasets: Combining Crowdworkers and Comparative Judgement. (arXiv:2305.12894v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.12894","description":"<p>Machine Learning models have many potentially beneficial applications in\neducation settings, but a key barrier to their development is securing enough\ndata to train these models. Labelling educational data has traditionally relied\non highly skilled raters using complex, multi-class rubrics, making the process\nexpensive and difficult to scale. An alternative, more scalable approach could\nbe to use non-expert crowdworkers to evaluate student work, however,\nmaintaining sufficiently high levels of accuracy and inter-rater reliability\nwhen using non-expert workers is challenging. This paper reports on two\nexperiments investigating using non-expert crowdworkers and comparative\njudgement to evaluate complex student data. Crowdworkers were hired to evaluate\nstudent responses to open-ended reading comprehension questions. Crowdworkers\nwere randomly assigned to one of two conditions: the control, where they were\nasked to decide whether answers were correct or incorrect (i.e., a categorical\njudgement), or the treatment, where they were shown the same question and\nanswers, but were instead asked to decide which of two candidate answers was\nmore correct (i.e., a comparative/preference-based judgement). We found that\nusing comparative judgement substantially improved inter-rater reliability on\nboth tasks. These results are in-line with well-established literature on the\nbenefits of comparative judgement in the field of educational assessment, as\nwell as with recent trends in artificial intelligence research, where\ncomparative judgement is becoming the preferred method for providing human\nfeedback on model outputs when working with non-expert crowdworkers. However,\nto our knowledge, these results are novel and important in demonstrating the\nbeneficial effects of using the combination of comparative judgement and\ncrowdworkers to evaluate educational data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Henkel_O/0/1/0/all/0/1\">Owen Henkel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hills_L/0/1/0/all/0/1\">Libby Hills</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Let's Think Frame by Frame with VIP: A Video Infilling and Prediction Dataset for Evaluating Video Chain-of-Thought. (arXiv:2305.13903v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13903","description":"<p>Despite exciting recent results showing vision-language systems' capacity to\nreason about images using natural language, their capacity for video reasoning\nremains under-explored. We motivate framing video reasoning as the sequential\nunderstanding of a small number of keyframes, thereby leveraging the power and\nrobustness of vision-language while alleviating the computational complexities\nof processing videos. To evaluate this novel application, we introduce VIP, an\ninference-time challenge dataset designed to explore models' reasoning\ncapabilities through video chain-of-thought. Inspired by visually descriptive\nscene plays, we propose two formats for keyframe description: unstructured\ndense captions and structured scene descriptions that identify the focus,\naction, mood, objects, and setting (FAMOuS) of the keyframe. To evaluate video\nreasoning, we propose two tasks: Video Infilling and Video Prediction, which\ntest abilities to generate multiple intermediate keyframes and predict future\nkeyframes, respectively. We benchmark GPT-4, GPT-3, and VICUNA on VIP,\ndemonstrate the performance gap in these complex video reasoning tasks, and\nencourage future work to prioritize language models for efficient and\ngeneralized video reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Himakunthala_V/0/1/0/all/0/1\">Vaishnavi Himakunthala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_A/0/1/0/all/0/1\">Andy Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rose_D/0/1/0/all/0/1\">Daniel Rose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ryan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_A/0/1/0/all/0/1\">Alex Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonar_C/0/1/0/all/0/1\">Chinmay Sonar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advancements in Arabic Grammatical Error Detection and Correction: An Empirical Investigation. (arXiv:2305.14734v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14734","description":"<p>Grammatical error correction (GEC) is a well-explored problem in English with\nmany existing models and datasets. However, research on GEC in morphologically\nrich languages has been limited due to challenges such as data scarcity and\nlanguage complexity. In this paper, we present the first results on Arabic GEC\nusing two newly developed Transformer-based pretrained sequence-to-sequence\nmodels. We also define the task of multi-class Arabic grammatical error\ndetection (GED) and present the first results on multi-class Arabic GED. We\nshow that using GED information as an auxiliary input in GEC models improves\nGEC performance across three datasets spanning different genres. Moreover, we\nalso investigate the use of contextual morphological preprocessing in aiding\nGEC systems. Our models achieve SOTA results on two Arabic GEC shared task\ndatasets and establish a strong benchmark on a recently created dataset. We\nmake our code, data, and pretrained models publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alhafni_B/0/1/0/all/0/1\">Bashar Alhafni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inoue_G/0/1/0/all/0/1\">Go Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khairallah_C/0/1/0/all/0/1\">Christian Khairallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset. (arXiv:2306.11167v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.11167","description":"<p>The quest for human imitative AI has been an enduring topic in AI research\nsince its inception. The technical evolution and emerging capabilities of the\nlatest cohort of large language models (LLMs) have reinvigorated the subject\nbeyond academia to the cultural zeitgeist. While recent NLP evaluation\nbenchmark tasks test some aspects of human-imitative behaviour (e.g.,\nBIG-bench's 'human-like behavior' tasks), few, if not none, examine creative\nproblem solving abilities. Creative problem solving in humans is a well-studied\ntopic in cognitive neuroscience with standardized tests that predominantly use\nthe ability to associate (heterogeneous) connections among clue words as a\nmetric for creativity. Exposure to misleading stimuli - distractors dubbed red\nherrings - impede human performance in such tasks via the fixation effect and\nEinstellung paradigm. In cognitive neuroscience studies, such fixations are\nexperimentally induced by pre-exposing participants to orthographically similar\nincorrect words to subsequent word-fragments or clues. The popular British quiz\nshow Only Connect's Connecting Wall segment essentially mimics Mednick's Remote\nAssociates Test (RAT) formulation with built-in, deliberate red herrings, which\nmakes it an ideal proxy dataset to explore and study fixation effect and\nEinstellung paradigm from cognitive neuroscience in LLMs. In this paper we\npresent the novel Only Connect Wall (OCW) dataset and report results from our\nevaluation of selected pre-trained language models and LLMs on creative problem\nsolving tasks like grouping clue words by heterogeneous connections, and\nidentifying correct open knowledge domain connections in respective groups. We\nsynthetically generate two additional datasets: OCW-Randomized, OCW-WordNet to\nfurther analyze our red-herrings hypothesis in language models. The code and\nlink to the dataset are available at https://github.com/TaatiTeam/OCW.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naeini_S/0/1/0/all/0/1\">Saeid Naeini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saqur_R/0/1/0/all/0/1\">Raeid Saqur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeidi_M/0/1/0/all/0/1\">Mozhgan Saeidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giorgi_J/0/1/0/all/0/1\">John Giorgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taati_B/0/1/0/all/0/1\">Babak Taati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing. (arXiv:2306.12929v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2306.12929","description":"<p>Transformer models have been widely adopted in various domains over the last\nyears, and especially large language models have advanced the field of AI\nsignificantly. Due to their size, the capability of these networks has\nincreased tremendously, but this has come at the cost of a significant increase\nin necessary compute. Quantization is one of the most effective ways to reduce\nthe computational time and memory consumption of neural networks. Many studies\nhave shown, however, that modern transformer models tend to learn strong\noutliers in their activations, making them difficult to quantize. To retain\nacceptable performance, the existence of these outliers requires activations to\nbe in higher bitwidth or the use of different numeric formats, extra\nfine-tuning, or other workarounds. We show that strong outliers are related to\nvery specific behavior of attention heads that try to learn a \"no-op\" or just a\npartial update of the residual. To achieve the exact zeros needed in the\nattention matrix for a no-update, the input to the softmax is pushed to be\nlarger and larger during training, causing outliers in other parts of the\nnetwork. Based on these observations, we propose two simple (independent)\nmodifications to the attention mechanism - clipped softmax and gated attention.\nWe empirically show that models pre-trained using our methods learn\nsignificantly smaller outliers while maintaining and sometimes even improving\nthe floating-point task performance. This enables us to quantize transformers\nto full INT8 quantization of the activations without any additional effort. We\ndemonstrate the effectiveness of our methods on both language models (BERT,\nOPT) and vision transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bondarenko_Y/0/1/0/all/0/1\">Yelysei Bondarenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagel_M/0/1/0/all/0/1\">Markus Nagel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blankevoort_T/0/1/0/all/0/1\">Tijmen Blankevoort</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research. (arXiv:2306.16900v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.16900","description":"<p>Many recent improvements in NLP stem from the development and use of large\npre-trained language models (PLMs) with billions of parameters. Large model\nsizes makes computational cost one of the main limiting factors for training\nand evaluating such models; and has raised severe concerns about the\nsustainability, reproducibility, and inclusiveness for researching PLMs. These\nconcerns are often based on personal experiences and observations. However,\nthere had not been any large-scale surveys that investigate them. In this work,\nwe provide a first attempt to quantify these concerns regarding three topics,\nnamely, environmental impact, equity, and impact on peer reviewing. By\nconducting a survey with 312 participants from the NLP community, we capture\nexisting (dis)parities between different and within groups with respect to\nseniority, academia, and industry; and their impact on the peer reviewing\nprocess. For each topic, we provide an analysis and devise recommendations to\nmitigate found disparities, some of which already successfully implemented.\nFinally, we discuss additional concerns raised by many participants in\nfree-text responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Ji-Ung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puerto_H/0/1/0/all/0/1\">Haritz Puerto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aken_B/0/1/0/all/0/1\">Betty van Aken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arase_Y/0/1/0/all/0/1\">Yuki Arase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forde_J/0/1/0/all/0/1\">Jessica Zosa Forde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derczynski_L/0/1/0/all/0/1\">Leon Derczynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruckle_A/0/1/0/all/0/1\">Andreas R&#xfc;ckl&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1\">Emma Strubell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodge_J/0/1/0/all/0/1\">Jesse Dodge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Strahler Number of Natural Language Sentences in Comparison with Random Trees. (arXiv:2307.02697v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.02697","description":"<p>The Strahler number was originally proposed to characterize the complexity of\nriver bifurcation and has found various applications. This article proposes\ncomputation of the Strahler number's upper and lower limits for natural\nlanguage sentence tree structures. Through empirical measurements across\ngrammatically annotated data, the Strahler number of natural language sentences\nis shown to be almost 3 or 4, similarly to the case of river bifurcation as\nreported by Strahler (1957). From the theory behind the number, we show that it\nis one kind of lower limit on the amount of memory required to process\nsentences. We consider the Strahler number to provide reasoning that explains\nreports showing that the number of required memory areas to process sentences\nis 3 to 4 for parsing (Schuler et al., 2010), and reports indicating a\npsychological \"magical number\" of 3 to 5 (Cowan, 2001). An analytical and\nempirical analysis shows that the Strahler number is not constant but grows\nlogarithmically; therefore, the Strahler number of sentences derives from the\nrange of sentence lengths. Furthermore, the Strahler number is not different\nfor random trees, which could suggest that its origin is not specific to\nnatural language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_Ishii_K/0/1/0/all/0/1\">Kumiko Tanaka-Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_A/0/1/0/all/0/1\">Akira Tanaka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chameleon: a heterogeneous and disaggregated accelerator system for retrieval-augmented language models. (arXiv:2310.09949v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2310.09949","description":"<p>A Retrieval-Augmented Language Model (RALM) augments a generative language\nmodel by retrieving context-specific knowledge from an external database. This\nstrategy facilitates impressive text generation quality even with smaller\nmodels, thus reducing orders of magnitude of computational demands. However,\nRALMs introduce unique system design challenges due to (a) the diverse workload\ncharacteristics between LM inference and retrieval and (b) the various system\nrequirements and bottlenecks for different RALM configurations such as model\nsizes, database sizes, and retrieval frequencies. We propose Chameleon, a\nheterogeneous accelerator system that integrates both LM and retrieval\naccelerators in a disaggregated architecture. The heterogeneity ensures\nefficient acceleration of both LM inference and retrieval, while the\naccelerator disaggregation enables the system to independently scale both types\nof accelerators to fulfill diverse RALM requirements. Our Chameleon prototype\nimplements retrieval accelerators on FPGAs and assigns LM inference to GPUs,\nwith a CPU server orchestrating these accelerators over the network. Compared\nto CPU-based and CPU-GPU vector search systems, Chameleon achieves up to 23.72x\nspeedup and 26.2x energy efficiency. Evaluated on various RALMs, Chameleon\nexhibits up to 2.16x reduction in latency and 3.18x speedup in throughput\ncompared to the hybrid CPU-GPU architecture. These promising results pave the\nway for bringing accelerator heterogeneity and disaggregation into future RALM\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeller_M/0/1/0/all/0/1\">Marco Zeller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waleffe_R/0/1/0/all/0/1\">Roger Waleffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1\">Torsten Hoefler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_G/0/1/0/all/0/1\">Gustavo Alonso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models. (arXiv:2310.10378v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.10378","description":"<p>Multilingual large-scale Pretrained Language Models (PLMs) have been shown to\nstore considerable amounts of factual knowledge, but large variations are\nobserved across languages. With the ultimate goal of ensuring that users with\ndifferent language backgrounds obtain consistent feedback from the same model,\nwe study the cross-lingual consistency (CLC) of factual knowledge in various\nmultilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC)\nmetric to evaluate knowledge consistency across languages independently from\naccuracy. Using this metric, we conduct an in-depth analysis of the determining\nfactors for CLC, both at model level and at language-pair level. Among other\nresults, we find that increasing model size leads to higher factual probing\naccuracy in most languages, but does not improve cross-lingual consistency.\nFinally, we conduct a case study on CLC when new factual associations are\ninserted in the PLMs via model editing. Results on a small sample of facts\ninserted in English reveal a clear pattern whereby the new piece of knowledge\ntransfers only to languages with which English has a high RankC score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jirui Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_R/0/1/0/all/0/1\">Raquel Fern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisazza_A/0/1/0/all/0/1\">Arianna Bisazza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging Information-Theoretic and Geometric Compression in Language Models. (arXiv:2310.13620v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.13620","description":"<p>For a language model (LM) to faithfully model human language, it must\ncompress vast, potentially infinite information into relatively few dimensions.\nWe propose analyzing compression in (pre-trained) LMs from two points of view:\ngeometric and information-theoretic. We demonstrate that the two views are\nhighly correlated, such that the intrinsic geometric dimension of linguistic\ndata predicts their coding length under the LM. We then show that, in turn,\nhigh compression of a linguistic dataset predicts rapid adaptation to that\ndataset, confirming that being able to compress linguistic information is an\nimportant part of successful LM performance. As a practical byproduct of our\nanalysis, we evaluate a battery of intrinsic dimension estimators for the first\ntime on linguistic data, showing that only some encapsulate the relationship\nbetween information-theoretic compression, geometric compression, and\nease-of-adaptation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_E/0/1/0/all/0/1\">Emily Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kervadec_C/0/1/0/all/0/1\">Corentin Kervadec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baroni_M/0/1/0/all/0/1\">Marco Baroni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT a game changer for geocoding -- a benchmark for geocoding address parsing techniques. (arXiv:2310.14360v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.14360","description":"<p>The remarkable success of GPT models across various tasks, including toponymy\nrecognition motivates us to assess the performance of the GPT-3 model in the\ngeocoding address parsing task. To ensure that the evaluation more accurately\nmirrors performance in real-world scenarios with diverse user input qualities\nand resolve the pressing need for a 'gold standard' evaluation dataset for\ngeocoding systems, we introduce a benchmark dataset of low-quality address\ndescriptions synthesized based on human input patterns mining from actual input\nlogs of a geocoding system in production. This dataset has 21 different input\nerrors and variations; contains over 239,000 address records that are uniquely\nselected from streets across all U.S. 50 states and D.C.; and consists of three\nsubsets to be used as training, validation, and testing sets. Building on this,\nwe train and gauge the performance of the GPT-3 model in extracting address\ncomponents, contrasting its performance with transformer-based and LSTM-based\nmodels. The evaluation results indicate that Bidirectional LSTM-CRF model has\nachieved the best performance over these transformer-based models and GPT-3\nmodel. Transformer-based models demonstrate very comparable results compared to\nthe Bidirectional LSTM-CRF model. The GPT-3 model, though trailing in\nperformance, showcases potential in the address parsing task with few-shot\nexamples, exhibiting room for improvement with additional fine-tuning. We open\nsource the code and data of this presented benchmark so that researchers can\nutilize it for future model development or extend it to evaluate similar tasks,\nsuch as document geocoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhengcong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Diya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_D/0/1/0/all/0/1\">Daniel W. Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensemble of Task-Specific Language Models for Brain Encoding. (arXiv:2310.15720v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.15720","description":"<p>Language models have been shown to be rich enough to encode fMRI activations\nof certain Regions of Interest in our Brains. Previous works have explored\ntransfer learning from representations learned for popular natural language\nprocessing tasks for predicting brain responses. In our work, we improve the\nperformance of such encoders by creating an ensemble model out of 10 popular\nLanguage Models (2 syntactic and 8 semantic). We beat the current baselines by\n10% on average across all ROIs through our ensembling methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arun_A/0/1/0/all/0/1\">Arvindh Arun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+John_J/0/1/0/all/0/1\">Jerrin John</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumaran_S/0/1/0/all/0/1\">Sanjai Kumaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FaMeSumm: Investigating and Improving Faithfulness of Medical Summarization. (arXiv:2311.02271v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.02271","description":"<p>Summaries of medical text shall be faithful by being consistent and factual\nwith source inputs, which is an important but understudied topic for safety and\nefficiency in healthcare. In this paper, we investigate and improve\nfaithfulness in summarization on a broad range of medical summarization tasks.\nOur investigation reveals that current summarization models often produce\nunfaithful outputs for medical input text. We then introduce FaMeSumm, a\nframework to improve faithfulness by fine-tuning pre-trained language models\nbased on medical knowledge. FaMeSumm performs contrastive learning on designed\nsets of faithful and unfaithful summaries, and it incorporates medical terms\nand their contexts to encourage faithful generation of medical terms. We\nconduct comprehensive experiments on three datasets in two languages: health\nquestion and radiology report summarization datasets in English, and a\npatient-doctor dialogue dataset in Chinese. Results demonstrate that FaMeSumm\nis flexible and effective by delivering consistent improvements over mainstream\nlanguage models such as BART, T5, mT5, and PEGASUS, yielding state-of-the-art\nperformances on metrics for faithfulness and general quality. Human evaluation\nby doctors also shows that FaMeSumm generates more faithful outputs. Our code\nis available at https://github.com/psunlpgroup/FaMeSumm .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Nan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yusen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_P/0/1/0/all/0/1\">Prasenjit Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Citance-Contextualized Summarization of Scientific Papers. (arXiv:2311.02408v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.02408","description":"<p>Current approaches to automatic summarization of scientific papers generate\ninformative summaries in the form of abstracts. However, abstracts are not\nintended to show the relationship between a paper and the references cited in\nit. We propose a new contextualized summarization approach that can generate an\ninformative summary conditioned on a given sentence containing the citation of\na reference (a so-called ``citance''). This summary outlines the content of the\ncited paper relevant to the citation location. Thus, our approach extracts and\nmodels the citances of a paper, retrieves relevant passages from cited papers,\nand generates abstractive summaries tailored to each citance. We evaluate our\napproach using $\\textbf{Webis-Context-SciSumm-2023}$, a new dataset containing\n540K~computer science papers and 4.6M~citances therein.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Syed_S/0/1/0/all/0/1\">Shahbaz Syed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakimi_A/0/1/0/all/0/1\">Ahmad Dawar Hakimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Khatib_K/0/1/0/all/0/1\">Khalid Al-Khatib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Principles from Clinical Research for NLP Model Generalization. (arXiv:2311.03663v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.03663","description":"<p>The NLP community typically relies on performance of a model on a held-out\ntest set to assess generalization. Performance drops observed in datasets\noutside of official test sets are generally attributed to\n\"out-of-distribution'' effects. Here, we explore the foundations of\ngeneralizability and study the various factors that affect it, articulating\ngeneralizability lessons from clinical studies. In clinical research\ngeneralizability depends on (a) internal validity of experiments to ensure\ncontrolled measurement of cause and effect, and (b) external validity or\ntransportability of the results to the wider population. We present the need to\nensure internal validity when building machine learning models in natural\nlanguage processing, especially where results may be impacted by spurious\ncorrelations in the data. We demonstrate how spurious factors, such as the\ndistance between entities in relation extraction tasks, can affect model\ninternal validity and in turn adversely impact generalization. We also offer\nguidance on how to analyze generalization failures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elangovan_A/0/1/0/all/0/1\">Aparna Elangovan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiayuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verspoor_K/0/1/0/all/0/1\">Karin Verspoor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Mathematical Autoformalization. (arXiv:2311.03755v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.03755","description":"<p>Autoformalization is the task of translating natural language materials into\nmachine-verifiable formalisations. Progress in autoformalization research is\nhindered by the lack of a sizeable dataset consisting of informal-formal pairs\nexpressing the same essence. Existing methods tend to circumvent this challenge\nby manually curating small corpora or using few-shot learning with large\nlanguage models. But these methods suffer from data scarcity and formal\nlanguage acquisition difficulty. In this work, we create $\\texttt{MMA}$, a\nlarge, flexible, multilingual, and multi-domain dataset of informal-formal\npairs, by using a language model to translate in the reverse direction, that\nis, from formal mathematical statements into corresponding informal ones.\nExperiments show that language models fine-tuned on $\\texttt{MMA}$ produce\n$16-18\\%$ of statements acceptable with minimal corrections on the\n$\\texttt{miniF2F}$ and $\\texttt{ProofNet}$ benchmarks, up from $0\\%$ with the\nbase model. We demonstrate that fine-tuning on multilingual formal data results\nin more capable autoformalization models even when deployed on monolingual\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1\">Albert Q. Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamnik_M/0/1/0/all/0/1\">Mateja Jamnik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aspects of human memory and Large Language Models. (arXiv:2311.03839v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.03839","description":"<p>Large Language Models (LLMs) are huge artificial neural networks which\nprimarily serve to generate text, but also provide a very sophisticated\nprobabilistic model of language use. Since generating a semantically consistent\ntext requires a form of effective memory, we investigate the memory properties\nof LLMs and find surprising similarities with key characteristics of human\nmemory. We argue that the human-like memory properties of the Large Language\nModel do not follow automatically from the LLM architecture but are rather\nlearned from the statistics of the training textual data. These results\nstrongly suggest that the biological features of human memory leave an imprint\non the way that we structure our textual narratives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Janik_R/0/1/0/all/0/1\">Romuald A. Janik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Massive Editing for Large Language Models via Meta Learning. (arXiv:2311.04661v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.04661","description":"<p>While large language models (LLMs) have enabled learning knowledge from the\npre-training corpora, the acquired knowledge may be fundamentally incorrect or\noutdated over time, which necessitates rectifying the knowledge of the language\nmodel (LM) after the training. A promising approach involves employing a\nhyper-network to generate parameter shift, whereas existing hyper-networks\nsuffer from inferior scalability in synchronous editing operation amount. To\nmitigate the problem, we propose the MAssive Language Model Editing Network\n(MALMEN), which formulates the parameter shift aggregation as the least square\nproblem, subsequently updating the LM parameters using the normal equation. To\naccommodate editing multiple facts simultaneously with limited memory budgets,\nwe separate the computation on the hyper-network and LM, enabling arbitrary\nbatch size on both neural networks. Our method is evaluated by editing up to\nthousands of facts on LMs with different architectures, i.e., BERT-base, GPT-2,\nT5-XL (2.8B), and GPT-J (6B), across various knowledge-intensive NLP tasks,\ni.e., closed book fact-checking and question answering. Remarkably, MALMEN is\ncapable of editing hundreds of times more facts than strong baselines with the\nidentical hyper-network architecture and outperforms editor specifically\ndesigned for GPT. Our code is available at\nhttps://github.com/ChenmienTan/malmen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenmien Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Ge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LongQLoRA: Efficient and Effective Method to Extend Context Length of Large Language Models. (arXiv:2311.04879v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.04879","description":"<p>We present LongQLoRA, an efficient and effective method to extend context\nlength of large language models with less training resources. LongQLoRA\ncombines the advantages of Position Interpolation, QLoRA and Shift Short\nAttention of LongLoRA. With a single 32GB V100 GPU, LongQLoRA can extend the\ncontext length of LLaMA2 7B and 13B from 4096 to 8192 and even to 12k within\n1000 finetuning steps. LongQLoRA achieves competitive perplexity performance on\nPG19 and Proof-pile datasets, our model outperforms LongLoRA and is very close\nto MPT-7B-8K within the evaluation context length of 8192. We collect and build\n39k long instruction data to extend context length of Vicuna-13B from 4096 to\n8192 and achieve good performance both in long and short context generation\ntask. We also do some ablation experiments to study the effect of LoRA rank,\nfinetuning steps and attention patterns in inference.The model weights,\ntraining data and code are avaliable at\nhttps://github.com/yangjianxin1/LongQLoRA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianxin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-11-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}