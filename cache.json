{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-05-25T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Finding the Pillars of Strength for Multi-Head Attention. (arXiv:2305.14380v1 [cs.LG])","link":"http://arxiv.org/abs/2305.14380","description":"<p>Recent studies have revealed some issues of Multi-Head Attention (MHA), e.g.,\nredundancy and over-parameterization. Specifically, the heads of MHA were\noriginally designed to attend to information from different representation\nsubspaces, whereas prior studies found that some attention heads likely learn\nsimilar features and can be pruned without harming performance. Inspired by the\nminimum-redundancy feature selection, we assume that focusing on the most\nrepresentative and distinctive features with minimum resources can mitigate the\nabove issues and lead to more effective and efficient MHAs. In particular, we\npropose Grouped Head Attention, trained with a self-supervised group constraint\nthat group attention heads, where each group focuses on an essential but\ndistinctive feature subset. We additionally propose a Voting-to-Stay procedure\nto remove redundant heads, thus achieving a transformer with lighter weights.\nMoreover, our method achieves significant performance gains on three\nwell-established tasks while considerably compressing parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jinjie Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_R/0/1/0/all/0/1\">Rui Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zonglin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_H/0/1/0/all/0/1\">Han Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation. (arXiv:2305.14386v1 [cs.LG])","link":"http://arxiv.org/abs/2305.14386","description":"<p>In this paper, we present a novel approach for distilling math word problem\nsolving capabilities from large language models (LLMs) into smaller, more\nefficient student models. Our approach is designed to consider the student\nmodel's weaknesses and foster a tailored learning experience by generating\ntargeted exercises aligned with educational science principles, such as\nknowledge tracing and personalized learning. Concretely, we let GPT-3 be a math\ntutor and run two steps iteratively: 1) assessing the student model's current\nlearning status on a GPT-generated exercise book, and 2) improving the student\nmodel by training it with tailored exercise samples generated by GPT-3.\nExperimental results reveal that our approach outperforms LLMs (e.g., GPT-3 and\nPaLM) in accuracy across three distinct benchmarks while employing\nsignificantly fewer parameters. Furthermore, we provide a comprehensive\nanalysis of the various components within our methodology to substantiate their\nefficacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhenwen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurohit_T/0/1/0/all/0/1\">Tanmay Rajpurohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaylan_A/0/1/0/all/0/1\">Ashwin Kaylan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v1 [cs.LG])","link":"http://arxiv.org/abs/2305.14387","description":"<p>Large language models (LLMs) such as ChatGPT have seen widespread adoption\ndue to their ability to follow user instructions well. Developing these LLMs\ninvolves a complex yet poorly understood workflow requiring training with human\nfeedback. Replicating and understanding this instruction-following process\nfaces three major challenges: the high cost of data collection, the lack of\ntrustworthy evaluation, and the absence of reference method implementations. We\naddress these challenges with AlpacaFarm, a simulator that enables research and\ndevelopment for learning from feedback at a low cost. First, we design LLM\nprompts to simulate human feedback that are 45x cheaper than crowdworkers and\ndisplay high agreement with humans. Second, we propose an automatic evaluation\nand validate it against human instructions obtained on real-world interactions.\nThird, we contribute reference implementations for several methods (PPO,\nbest-of-n, expert iteration, and more) that learn from pairwise feedback.\nFinally, as an end-to-end validation of AlpacaFarm, we train and evaluate\neleven models on 10k pairs of real human feedback and show that rankings of\nmodels trained in AlpacaFarm match rankings of models trained on human data. As\na demonstration of the research possible in AlpacaFarm, we find that methods\nthat use a reward model can substantially improve over supervised fine-tuning\nand that our reference PPO implementation leads to a +10% improvement in\nwin-rate against Davinci003. We release all components of AlpacaFarm at\nhttps://github.com/tatsu-lab/alpaca_farm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dubois_Y/0/1/0/all/0/1\">Yann Dubois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuechen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taori_R/0/1/0/all/0/1\">Rohan Taori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulrajani_I/0/1/0/all/0/1\">Ishaan Gulrajani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ba_J/0/1/0/all/0/1\">Jimmy Ba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guestrin_C/0/1/0/all/0/1\">Carlos Guestrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori B. Hashimoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Manipulation via Multi-Hop Instructions -- A New Dataset and Weakly-Supervised Neuro-Symbolic Approach. (arXiv:2305.14410v1 [cs.CV])","link":"http://arxiv.org/abs/2305.14410","description":"<p>We are interested in image manipulation via natural language text -- a task\nthat is useful for multiple AI applications but requires complex reasoning over\nmulti-modal spaces. We extend recently proposed Neuro Symbolic Concept Learning\n(NSCL), which has been quite effective for the task of Visual Question\nAnswering (VQA), for the task of image manipulation. Our system referred to as\nNeuroSIM can perform complex multi-hop reasoning over multi-object scenes and\nonly requires weak supervision in the form of annotated data for VQA. NeuroSIM\nparses an instruction into a symbolic program, based on a Domain Specific\nLanguage (DSL) comprising of object attributes and manipulation operations,\nthat guides its execution. We create a new dataset for the task, and extensive\nexperiments demonstrate that NeuroSIM is highly competitive with or beats SOTA\nbaselines that make use of supervised data for manipulation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_H/0/1/0/all/0/1\">Harman Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_P/0/1/0/all/0/1\">Poorva Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Mohit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_K/0/1/0/all/0/1\">Kevin Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondal_A/0/1/0/all/0/1\">Arnab Kumar Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_D/0/1/0/all/0/1\">Dinesh Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_P/0/1/0/all/0/1\">Parag Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_D/0/1/0/all/0/1\">Dinesh Garg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-Expanded ASTE: Rethinking Generalization in Aspect Sentiment Triplet Extraction. (arXiv:2305.14434v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14434","description":"<p>Aspect Sentiment Triplet Extraction (ASTE) is a subtask of Aspect-Based\nSentiment Analysis (ABSA) that considers each opinion term, their expressed\nsentiment, and the corresponding aspect targets. However, existing methods are\nlimited to the in-domain setting with two domains. Hence, we propose a\ndomain-expanded benchmark to address the in-domain, out-of-domain and\ncross-domain settings. We support the new benchmark by annotating more than\n4000 data samples for two new domains based on hotel and cosmetics reviews. Our\nanalysis of five existing methods shows that while there is a significant gap\nbetween in-domain and out-of-domain performance, generative methods have a\nstrong potential for domain generalization. Our datasets, code implementation\nand models are available at https://github.com/DAMO-NLP-SG/domain-expanded-aste .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chia_Y/0/1/0/all/0/1\">Yew Ken Chia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guizhen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aljunied_S/0/1/0/all/0/1\">Sharifah Mahani Aljunied</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Contrast Consistency of Open-Domain Question Answering Systems on Minimally Edited Questions. (arXiv:2305.14441v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14441","description":"<p>Contrast consistency, the ability of a model to make consistently correct\npredictions in the presence of perturbations, is an essential aspect in NLP.\nWhile studied in tasks such as sentiment analysis and reading comprehension, it\nremains unexplored in open-domain question answering (OpenQA) due to the\ndifficulty of collecting perturbed questions that satisfy factuality\nrequirements. In this work, we collect minimally edited questions as\nchallenging contrast sets to evaluate OpenQA models. Our collection approach\ncombines both human annotation and large language model generation. We find\nthat the widely used dense passage retriever (DPR) performs poorly on our\ncontrast sets, despite fitting the training set well and performing\ncompetitively on standard test sets. To address this issue, we introduce a\nsimple and effective query-side contrastive loss with the aid of data\naugmentation to improve DPR training. Our experiments on the contrast sets\ndemonstrate that DPR's contrast consistency is improved without sacrificing its\naccuracy on the standard test sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_Z/0/1/0/all/0/1\">Zheng Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_M/0/1/0/all/0/1\">Mingxuan Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors. (arXiv:2305.14450v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14450","description":"<p>ChatGPT has stimulated the research boom in the field of large language\nmodels. In this paper, we assess the capabilities of ChatGPT from four\nperspectives including Performance, Evaluation Criteria, Robustness and Error\nTypes. Specifically, we first evaluate ChatGPT's performance on 17 datasets\nwith 14 IE sub-tasks under the zero-shot, few-shot and chain-of-thought\nscenarios, and find a huge performance gap between ChatGPT and SOTA results.\nNext, we rethink this gap and propose a soft-matching strategy for evaluation\nto more accurately reflect ChatGPT's performance. Then, we analyze the\nrobustness of ChatGPT on 14 IE sub-tasks, and find that: 1) ChatGPT rarely\noutputs invalid responses; 2) Irrelevant context and long-tail target types\ngreatly affect ChatGPT's performance; 3) ChatGPT cannot understand well the\nsubject-object relationships in RE task. Finally, we analyze the errors of\nChatGPT, and find that \"unannotated spans\" is the most dominant error type.\nThis raises concerns about the quality of annotated data, and indicates the\npossibility of annotating data with ChatGPT. The data and code are released at\nGithub site.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1\">Ridong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_T/0/1/0/all/0/1\">Tao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chaohao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Benyou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Robustness of Finetuned Transformer-based NLP Models. (arXiv:2305.14453v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14453","description":"<p>Transformer-based pretrained models like BERT, GPT-2 and T5 have been\nfinetuned for a large number of natural language processing (NLP) tasks, and\nhave been shown to be very effective. However, while finetuning, what changes\nacross layers in these models with respect to pretrained checkpoints is\nunder-studied. Further, how robust are these models to perturbations in input\ntext? Does the robustness vary depending on the NLP task for which the models\nhave been finetuned? While there exists some work on studying robustness of\nBERT finetuned for a few NLP tasks, there is no rigorous study which compares\nthis robustness across encoder only, decoder only and encoder-decoder models.\n</p>\n<p>In this paper, we study the robustness of three language models (BERT, GPT-2\nand T5) with eight different text perturbations on the General Language\nUnderstanding Evaluation (GLUE) benchmark. Also, we use two metrics (CKA and\nSTIR) to quantify changes between pretrained and finetuned language model\nrepresentations across layers. GPT-2 representations are more robust than BERT\nand T5 across multiple types of input perturbation. Although models exhibit\ngood robustness broadly, dropping nouns, verbs or changing characters are the\nmost impactful. Overall, this study provides valuable insights into\nperturbation-specific weaknesses of popular Transformer-based models which\nshould be kept in mind when passing inputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neerudu_P/0/1/0/all/0/1\">Pavan Kalyan Reddy Neerudu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oota_S/0/1/0/all/0/1\">Subba Reddy Oota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marreddy_M/0/1/0/all/0/1\">Mounika Marreddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kagita_V/0/1/0/all/0/1\">Venkateswara Rao Kagita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Having Beer after Prayer? Measuring Cultural Bias in Large Language Models. (arXiv:2305.14456v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14456","description":"<p>Are language models culturally biased? It is important that language models\nconform to the cultural aspects of the communities they serve. However, we show\nin this paper that language models suffer from a significant bias towards\nWestern culture when handling and generating text in Arabic, often preferring,\nand producing Western-fitting content as opposed to the relevant Arab content.\nWe quantify this bias through a likelihood scoring-based metric using naturally\noccurring contexts that we collect from online social media. Our experiments\nreveal that both Arabic monolingual and multilingual models exhibit bias\ntowards Western culture in eight different cultural aspects: person names,\nfood, clothing, location, literature, beverage, religion, and sports. Models\nalso tend to exhibit more bias when prompted with Arabic sentences that are\nmore linguistically aligned with English. These findings raise concerns about\nthe cultural relevance of current language models. Our analyses show that\nproviding culture-indicating tokens or culturally-relevant demonstrations to\nthe model can help in debiasing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naous_T/0/1/0/all/0/1\">Tarek Naous</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryan_M/0/1/0/all/0/1\">Michael J. Ryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-training Language Models for Comparative Reasoning. (arXiv:2305.14457v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14457","description":"<p>In this paper, we propose a novel framework to pre-train language models for\nenhancing their abilities of comparative reasoning over texts. While recent\nresearch has developed models for NLP tasks that require comparative reasoning,\nthey suffer from costly manual data labeling and limited generalizability to\ndifferent tasks. Our approach involves a scalable method for collecting data\nfor text-based entity comparison, which leverages both structured and\nunstructured data, and the design of three novel pre-training tasks. Evaluation\non a range of downstream tasks including comparative question answering,\nquestion generation, and summarization shows that our pre-training framework\nsignificantly improves the comparative reasoning abilities of language models,\nespecially under low-resource conditions. This work also releases the first\nintegrated benchmark for comparative reasoning over texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mengxia Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA. (arXiv:2305.14458v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14458","description":"<p>Large language models (e.g., GPT-3.5) are uniquely capable of producing\nhighly rated text simplification, yet current human evaluation methods fail to\nprovide a clear understanding of systems' specific strengths and weaknesses. To\naddress this limitation, we introduce SALSA, an edit-based human annotation\nframework that enables holistic and fine-grained text simplification\nevaluation. We develop twenty one linguistically grounded edit types, covering\nthe full spectrum of success and failure across dimensions of conceptual,\nsyntactic and lexical simplicity. Using SALSA, we collect 12K edit annotations\non 700 simplifications, revealing discrepancies in the distribution of\ntransformation approaches performed by fine-tuned models, few-shot LLMs and\nhumans, and finding GPT-3.5 performs more quality edits than humans, but still\nexhibits frequent errors. Using our fine-grained annotations, we develop\nLENS-SALSA, a reference-free automatic simplification metric, trained to\npredict sentence- and word-level quality simultaneously. Additionally, we\nintroduce word-level quality estimation for simplification and report promising\nbaseline results. Our training material, annotation toolkit, and data are\nreleased at <a href=\"http://salsa-eval.com.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heineman_D/0/1/0/all/0/1\">David Heineman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">Yao Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maddela_M/0/1/0/all/0/1\">Mounica Maddela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Generation through Summarization Duality and Explicit Outline Control. (arXiv:2305.14459v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14459","description":"<p>Automatically open-ended long text generation poses significant challenges\ndue to semantic incoherence and plot implausibility. Previous works usually\nalleviate this problem through outlines in the form of short phrases or\nabstractive signals by designing unsupervised tasks, which tend to be unstable\nand weakly interpretable.\n</p>\n<p>Assuming that a summary serves as a mature outline, we introduce a two-stage,\nsummary-enhanced outline supervised generation framework. This framework\nleverages the dual characteristics of the summarization task to improve outline\nprediction, resulting in more explicit and plausible outlines. Furthermore, we\nidentify an underutilization issue in outline-based generation with both\nstandard pretrained language models (e.g., GPT-2, BART) and large language\nmodels (e.g., Vicuna, ChatGPT). To address this, we propose a novel explicit\noutline control method for more effective utilization of generated outlines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1\">Weixiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaram_H/0/1/0/all/0/1\">Hari Sundaram</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Massively Multi-domain Multilingual Readability Assessment. (arXiv:2305.14463v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14463","description":"<p>We present ReadMe++, a massively multi-domain multilingual dataset for\nautomatic readability assessment. Prior work on readability assessment has been\nmostly restricted to the English language and one or two text domains.\nAdditionally, the readability levels of sentences used in many previous\ndatasets are assumed on the document-level other than sentence-level, which\nraises doubt about the quality of previous evaluations. We address those gaps\nin the literature by providing an annotated dataset of 6,330 sentences in\nArabic, English, and Hindi collected from 64 different domains of text. Unlike\nprevious datasets, ReadMe++ offers more domain and language diversity and is\nmanually annotated at a sentence level using the Common European Framework of\nReference for Languages (CEFR) and through a Rank-and-Rate annotation framework\nthat reduces subjectivity in annotation. Our experiments demonstrate that\nmodels fine-tuned using ReadMe++ achieve strong cross-lingual transfer\ncapabilities and generalization to unseen domains. ReadMe++ will be made\npublicly available to the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naous_T/0/1/0/all/0/1\">Tarek Naous</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryan_M/0/1/0/all/0/1\">Michael J. Ryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_M/0/1/0/all/0/1\">Mohit Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Run Like a Girl! Sports-Related Gender Bias in Language and Vision. (arXiv:2305.14468v1 [cs.CV])","link":"http://arxiv.org/abs/2305.14468","description":"<p>Gender bias in Language and Vision datasets and models has the potential to\nperpetuate harmful stereotypes and discrimination. We analyze gender bias in\ntwo Language and Vision datasets. Consistent with prior work, we find that both\ndatasets underrepresent women, which promotes their invisibilization. Moreover,\nwe hypothesize and find that a bias affects human naming choices for people\nplaying sports: speakers produce names indicating the sport (e.g. 'tennis\nplayer' or 'surfer') more often when it is a man or a boy participating in the\nsport than when it is a woman or a girl, with an average of 46% vs. 35% of\nsports-related names for each gender. A computational model trained on these\nnaming data reproduces the bias. We argue that both the data and the model\nresult in representational harm against women.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harrison_S/0/1/0/all/0/1\">Sophia Harrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gualdoni_E/0/1/0/all/0/1\">Eleonora Gualdoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boleda_G/0/1/0/all/0/1\">Gemma Boleda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CGCE: A Chinese Generative Chat Evaluation Benchmark for General and Financial Domains. (arXiv:2305.14471v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14471","description":"<p>Generative chat models, such as ChatGPT and GPT-4, have revolutionized\nnatural language generation (NLG) by incorporating instructions and human\nfeedback to achieve significant performance improvements. However, the lack of\nstandardized evaluation benchmarks for chat models, particularly for Chinese\nand domain-specific models, hinders their assessment and progress. To address\nthis gap, we introduce the Chinese Generative Chat Evaluation (CGCE) benchmark,\nfocusing on general and financial domains. The CGCE benchmark encompasses\ndiverse tasks, including 200 questions in the general domain and 150 specific\nprofessional questions in the financial domain. Manual scoring evaluates\nfactors such as accuracy, coherence, expression clarity, and completeness. The\nCGCE benchmark provides researchers with a standardized framework to assess and\ncompare Chinese generative chat models, fostering advancements in NLG research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuanyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingbing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qing Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BAND: Biomedical Alert News Dataset. (arXiv:2305.14480v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14480","description":"<p>Infectious disease outbreaks continue to pose a significant threat to human\nhealth and well-being. To improve disease surveillance and understanding of\ndisease spread, several surveillance systems have been developed to monitor\ndaily news alerts and social media. However, existing systems lack thorough\nepidemiological analysis in relation to corresponding alerts or news, largely\ndue to the scarcity of well-annotated reports data. To address this gap, we\nintroduce the Biomedical Alert News Dataset (BAND), which includes 1,508\nsamples from existing reported news articles, open emails, and alerts, as well\nas 30 epidemiology-related questions. These questions necessitate the model's\nexpert reasoning abilities, thereby offering valuable insights into the\noutbreak of the disease. The BAND dataset brings new challenges to the NLP\nworld, requiring better disguise capability of the content and the ability to\ninfer important information. We provide several benchmark tasks, including\nNamed Entity Recognition (NER), Question Answering (QA), and Event Extraction\n(EE), to show how existing models are capable of handling these tasks in the\nepidemiology domain. To the best of our knowledge, the BAND corpus is the\nlargest corpus of well-annotated biomedical outbreak alert news with\nelaborately designed questions, making it a valuable resource for\nepidemiologists and NLP researchers alike.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zihao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meiru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zaiqiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yannan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhmatovskaia_A/0/1/0/all/0/1\">Anya Okhmatovskaia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buckeridge_D/0/1/0/all/0/1\">David Buckeridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FOCUS: Effective Embedding Initialization for Specializing Pretrained Multilingual Models on a Single Language. (arXiv:2305.14481v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14481","description":"<p>Using model weights pretrained on a high-resource language as a warm start\ncan reduce the need for data and compute to obtain high-quality language models\nin low-resource languages. To accommodate the new language, the pretrained\nvocabulary and embeddings need to be adapted. Previous work on embedding\ninitialization for such adapted vocabularies has mostly focused on monolingual\nsource models. In this paper, we investigate the multilingual source model\nsetting and propose FOCUS - Fast Overlapping Token Combinations Using\nSparsemax, a novel embedding initialization method that outperforms previous\nwork when adapting XLM-R. FOCUS represents newly added tokens as combinations\nof tokens in the overlap of the pretrained and new vocabularies. The\noverlapping tokens are selected based on semantic similarity in an auxiliary\ntoken embedding space. Our implementation of FOCUS is publicly available on\nGitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dobler_K/0/1/0/all/0/1\">Konstantin Dobler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1\">Gerard de Melo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is a Prestigious Job the same as a Prestigious Country? A Case Study on Multilingual Sentence Embeddings and European Countries. (arXiv:2305.14482v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14482","description":"<p>We study how multilingual sentence representations capture European countries\nand how this differs across European languages. We prompt the models with\ntemplated sentences that we machine-translate into 12 European languages and\nanalyze the most prominent dimensions in the embeddings. Our analysis reveals\nthat the most prominent country feature in the embedding is its economic\nstrength in terms of GPD. When prompted specifically for job prestige, the\nembedding space clearly distinguishes high and low-prestige jobs. The\noccupational dimension is uncorrelated with the most dominant country\ndimensions for three out of four studied models. One model: Distilled\nMultilingual Universal Sentence Encoder, however, exhibited a connection\nbetween occupational prestige and country of origin, which is a potential\nsource of nationality-based discrimination. Our findings are consistent across\nlanguages and, to some extent, with the exception mentioned above, across\nstudied representation models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Libovicky_J/0/1/0/all/0/1\">Jind&#x159;ich Libovick&#xfd;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Model Self-improvement by Reinforcement Learning Contemplation. (arXiv:2305.14483v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14483","description":"<p>Large Language Models (LLMs) have exhibited remarkable performance across\nvarious natural language processing (NLP) tasks. However, fine-tuning these\nmodels often necessitates substantial supervision, which can be expensive and\ntime-consuming to obtain. This paper introduces a novel unsupervised method\ncalled LanguageModel Self-Improvement by Reinforcement Learning Contemplation\n(SIRLC) that improves LLMs without reliance on external labels. Our approach is\ngrounded in the observation that it is simpler for language models to assess\ntext quality than to generate text. Building on this insight, SIRLC assigns\nLLMs dual roles as both student and teacher. As a student, the LLM generates\nanswers to unlabeled questions, while as a teacher, it evaluates the generated\ntext and assigns scores accordingly. The model parameters are updated using\nreinforcement learning to maximize the evaluation score. We demonstrate that\nSIRLC can be applied to various NLP tasks, such as reasoning problems, text\ngeneration, and machine translation. Our experiments show that SIRLC\neffectively improves LLM performance without external supervision, resulting in\na 5.6% increase in answering accuracy for reasoning tasks and a rise in\nBERTScore from 0.82 to 0.86 for translation tasks. Furthermore, SIRLC can be\napplied to models of different sizes, showcasing its broad applicability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jing-Cheng Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kaiyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiong-Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiacheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zongzhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yang Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Large Language Models Robust Zero-shot Coreference Resolvers?. (arXiv:2305.14489v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14489","description":"<p>Recent progress in domain adaptation for coreference resolution relies on\ncontinued training using annotated data from target domains. At the same time,\npre-trained large language models (LMs) have exhibited strong zero- and\nfew-shot learning abilities across a wide range of NLP tasks including pronoun\nresolution. While this demonstrates evidence of coreference ability, previous\nwork has mostly studied this ability using simple sentence-level datasets such\nas the Winograd Schema Challenge. In this work, we assess the feasibility of\nzero-shot learning for coreference resolution by evaluating instruction-tuned\nlanguage models on more difficult, linguistically-complex coreference\nbenchmarks (e.g., CoNLL-2012). We demonstrate that zero-shot prompting\noutperforms current unsupervised coreference systems. Further investigations\nreveal the robust zero-shot generalization ability of instruction-tuned LMs\nacross a wide range of domains, languages, and time periods, as well as a\nstrong reliance on high-quality mention detection systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Nghia T. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sociocultural Norm Similarities and Differences via Situational Alignment and Explainable Textual Entailment. (arXiv:2305.14492v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14492","description":"<p>Designing systems that can reason across cultures requires that they are\ngrounded in the norms of the contexts in which they operate. However, current\nresearch on developing computational models of social norms has primarily\nfocused on American society. Here, we propose a novel approach to discover and\ncompare descriptive social norms across Chinese and American cultures. We\ndemonstrate our approach by leveraging discussions on a Chinese Q&amp;A\nplatform-Zhihu-and the existing SocialChemistry dataset as proxies for\ncontrasting cultural axes, align social situations cross-culturally, and\nextract social norms from texts using in-context learning. Embedding\nChain-of-Thought prompting in a human-AI collaborative framework, we build a\nhigh-quality dataset of 3,069 social norms aligned with social situations\nacross Chinese and American cultures alongside corresponding free-text\nexplanations. To test the ability of models to reason about social norms across\ncultures, we introduce the task of explainable social norm entailment, showing\nthat existing models under 3B parameters have significant room for improvement\nin both automatic and human evaluation. Further analysis of cross-cultural norm\ndifferences based on our dataset shows empirical alignment with the social\norientations framework, revealing several situational and descriptive nuances\nin norms across these cultures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+CH_Wang_S/0/1/0/all/0/1\">Sky CH-Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saakyan_A/0/1/0/all/0/1\">Arkadiy Saakyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_O/0/1/0/all/0/1\">Oliver Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt position really matters in few-shot and zero-shot NLU tasks. (arXiv:2305.14493v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14493","description":"<p>Prompt-based models have made remarkable advancements in the fields of\nzero-shot and few-shot learning, attracting a lot of attention from\nresearchers. Developing an effective prompt template plays a critical role.\nHowever, prior studies have mainly focused on prompt vocabulary selection or\nembedding initialization with the reserved prompt position fixed. In this\nempirical study, we conduct the most comprehensive analysis to date of prompt\nposition option for natural language understanding tasks. Our findings quantify\nthe substantial impact prompt position has on model performance. We observe\nthat the prompt position used in prior studies is often sub-optimal for both\nzero-shot and few-shot settings. These findings suggest prompt position\noptimisation as an interesting research direction alongside the existing focus\non prompt engineering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Junyu Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Middleton_S/0/1/0/all/0/1\">Stuart E. Middleton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niranjan_M/0/1/0/all/0/1\">Mahesan Niranjan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement. (arXiv:2305.14497v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14497","description":"<p>Prompting methods such as Chain-of-Thought (CoT) have shed new light on\nenhancing the reasoning capabilities of large language models, and researchers\nhave extensively explored the generation process of rationales and answers.\nHowever, they have overlooked the potential challenges posed by the poor\nquality of reasoning problems, which may influence the reasoning performance\nsignificantly. In this work, we propose Self-Polish (SP), a novel method that\nfacilitates the model's problem-solving process by prompting them to\nprogressively refine the given problems to be more comprehensible and solvable.\nSpecifically, the method teaches models to eliminate irrelevant information,\nrearrange the logic structure and organize local conditions into new ones\nparallelly. SP is orthogonal to all other prompting methods, making it\nconvenient to integrate with state-of-the-art techniques for further\nimprovement. We conduct thorough experiments on five benchmarks to illustrate\nthe effectiveness of the proposed method. For example, with Text-davinci-003,\nour method boosts the performance of standard few-shot prompting by $8.0\\%$ on\nGSM8K and $17.8\\%$ on MultiArith; it also improves the performance of CoT by\n$6.0\\%$ on GSM8K and $6.0\\%$ on MathQA, respectively. Furthermore, our method\nalso showcases impressive performance on robustness evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1\">Zhiheng Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Senjie Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Rui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Songyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders. (arXiv:2305.14499v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14499","description":"<p>Neural document rerankers are extremely effective in terms of accuracy.\nHowever, the best models require dedicated hardware for serving, which is\ncostly and often not feasible. To avoid this serving-time requirement, we\npresent a method of capturing up to 86% of the gains of a Transformer\ncross-attention model with a lexicalized scoring function that only requires\n10-6% of the Transformer's FLOPs per document and can be served using commodity\nCPUs. When combined with a BM25 retriever, this approach matches the quality of\na state-of-the art dual encoder retriever, that still requires an accelerator\nfor query encoding. We introduce NAIL (Non-Autoregressive Indexing with\nLanguage models) as a model architecture that is compatible with recent\nencoder-decoder and decoder-only large language models, such as T5, GPT-3 and\nPaLM. This model architecture can leverage existing pre-trained checkpoints and\ncan be fine-tuned for efficiently constructing document representations that do\nnot require neural processing of queries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soares_L/0/1/0/all/0/1\">Livio Baldini Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillick_D/0/1/0/all/0/1\">Daniel Gillick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cole_J/0/1/0/all/0/1\">Jeremy R. Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwiatkowski_T/0/1/0/all/0/1\">Tom Kwiatkowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning. (arXiv:2305.14502v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14502","description":"<p>Many recent developments in large language models focus on prompting them to\nperform specific tasks. One effective prompting method is in-context learning,\nwhere the model performs a (possibly new) generation/prediction task given one\n(or more) examples. Past work has shown that the choice of examples can make a\nlarge impact on task performance. However, finding good examples is not\nstraightforward since the definition of a representative group of examples can\nvary greatly depending on the task. While there are many existing methods for\nselecting in-context examples, they generally score examples independently,\nignoring the dependency between them and the order in which they are provided\nto the large language model. In this work, we propose Retrieval for In-Context\nLearning (RetICL), a learnable method for modeling and optimally selecting\nexamples sequentially for in-context learning. We frame the problem of\nsequential example selection as a Markov decision process, design an example\nretriever model using an LSTM, and train it using proximal policy optimization\n(PPO). We validate RetICL on math problem solving datasets and show that it\noutperforms both heuristic and learnable baselines, and achieves\nstate-of-the-art accuracy on the TabMWP dataset. We also use case studies to\nshow that RetICL implicitly learns representations of math problem solving\nstrategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scarlatos_A/0/1/0/all/0/1\">Alexander Scarlatos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_A/0/1/0/all/0/1\">Andrew Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deduction under Perturbed Evidence: Probing Student Simulation Capabilities of Large Language Models. (arXiv:2305.14507v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14507","description":"<p>We explore whether Large Language Models (LLMs) are capable of logical\nreasoning with distorted facts, which we call Deduction under Perturbed\nEvidence (DUPE). DUPE presents a unique challenge to LLMs since they typically\nrely on their parameters, which encode mostly accurate information, to reason\nand make inferences. However, in DUPE, LLMs must reason over manipulated or\nfalsified evidence present in their prompts, which can result in false\nconclusions that are valid only under the manipulated evidence. Our goal with\nDUPE is to determine whether LLMs can arrive at these false conclusions and\nidentify whether the dominant factor influencing the deduction process is the\nencoded data in the parameters or the manipulated evidence in the prompts. To\nevaluate the DUPE capabilities of LLMs, we create a DUPEd version of the\nStrategyQA dataset, where facts are manipulated to reverse the answer to the\nquestion. Our findings show that even the most advanced GPT models struggle to\nreason on manipulated facts - showcasing poor DUPE skills - with accuracy\ndropping by 45% compared to the original dataset. We also investigate prompt\nsettings inspired from student simulation models, which mitigate the accuracy\ndrop to some extent. Our findings have practical implications for understanding\nthe performance of LLMs in real-world applications such as student simulation\nmodels that involve reasoning over inaccurate information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sonkar_S/0/1/0/all/0/1\">Shashank Sonkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard G. Baraniuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eliminating Spurious Correlations from Pre-trained Models via Data Mixing. (arXiv:2305.14521v1 [cs.LG])","link":"http://arxiv.org/abs/2305.14521","description":"<p>Machine learning models pre-trained on large datasets have achieved\nremarkable convergence and robustness properties. However, these models often\nexploit spurious correlations between certain attributes and labels, which are\nprevalent in the majority of examples within specific categories but are not\npredictive of these categories in general. The learned spurious correlations\nmay persist even after fine-tuning on new data, which degrades models'\nperformance on examples that do not exhibit the spurious correlation. In this\nwork, we propose a simple and highly effective method to eliminate spurious\ncorrelations from pre-trained models. The key idea of our method is to leverage\na small set of examples with spurious attributes, and balance the spurious\nattributes across all classes via data mixing. We theoretically confirm the\neffectiveness of our method, and empirically demonstrate its state-of-the-art\nperformance on various vision and NLP tasks, including eliminating spurious\ncorrelations from pre-trained ResNet50 on Waterbirds and CelebA, adversarially\npre-trained ResNet50 on ImageNet, and BERT pre-trained on CivilComments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Yihao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Payani_A/0/1/0/all/0/1\">Ali Payani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirzasoleiman_B/0/1/0/all/0/1\">Baharan Mirzasoleiman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Choose How to Choose Your Chatbot: A Massively Multi-System MultiReference Data Set for Dialog Metric Evaluation. (arXiv:2305.14533v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14533","description":"<p>We release MMSMR, a Massively Multi-System MultiReference dataset to enable\nfuture work on metrics and evaluation for dialog. Automatic metrics for\ndialogue evaluation should be robust proxies for human judgments; however, the\nverification of robustness is currently far from satisfactory. To quantify the\nrobustness correlation and understand what is necessary in a test set, we\ncreate and release an 8-reference dialog dataset by extending single-reference\nevaluation sets and introduce this new language learning conversation dataset.\nWe then train 1750 systems and evaluate them on our novel test set and the\nDailyDialog dataset. We release the novel test set, and model hyper parameters,\ninference outputs, and metric scores for each system on a variety of datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khayrallah_H/0/1/0/all/0/1\">Huda Khayrallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_Z/0/1/0/all/0/1\">Zuhaib Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_E/0/1/0/all/0/1\">Edward Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Propaganda Techniques in Code-Switched Social Media Text. (arXiv:2305.14534v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14534","description":"<p>Propaganda is a form of communication intended to influence the opinions and\nthe mindset of the public to promote a particular agenda. With the rise of\nsocial media, propaganda has spread rapidly, leading to the need for automatic\npropaganda detection systems. Most work on propaganda detection has focused on\nhigh-resource languages, such as English, and little effort has been made to\ndetect propaganda for low-resource languages. Yet, it is common to find a mix\nof multiple languages in social media communication, a phenomenon known as\ncode-switching. Code-switching combines different languages within the same\ntext, which poses a challenge for automatic systems. With this in mind, here we\npropose the novel task of detecting propaganda techniques in code-switched\ntext. To support this task, we create a corpus of 1,030 texts code-switching\nbetween English and Roman Urdu, annotated with 20 propaganda techniques, which\nwe make publicly available. We perform a number of experiments contrasting\ndifferent experimental setups, and we find that it is important to model the\nmultilinguality directly (rather than using translation) as well as to use the\nright fine-tuning strategy. The code and the dataset are publicly available at\nhttps://github.com/mbzuai-nlp/propaganda-codeswitched-text\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salman_M/0/1/0/all/0/1\">Muhammad Umar Salman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanif_A/0/1/0/all/0/1\">Asif Hanif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shehata_S/0/1/0/all/0/1\">Shady Shehata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems. (arXiv:2305.14536v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14536","description":"<p>Although automatic dialogue tutors hold great potential in making education\npersonalized and more accessible, research on such systems has been hampered by\na lack of sufficiently large and high-quality datasets. However, collecting\nsuch datasets remains challenging, as recording tutoring sessions raises\nprivacy concerns and crowdsourcing leads to insufficient data quality. To\naddress this problem, we propose a framework to semi-synthetically generate\nsuch dialogues by pairing real teachers with a large language model (LLM)\nscaffolded to represent common student errors. In this paper, we describe our\nongoing efforts to use this framework to collect MathDial, a dataset of\ncurrently ca. 1.5k tutoring dialogues grounded in multi-step math word\nproblems. We show that our dataset exhibits rich pedagogical properties,\nfocusing on guiding students using sense-making questions to let them explore\nproblems. Moreover, we outline that MathDial and its grounding annotations can\nbe used to finetune language models to be more effective tutors (and not just\nsolvers) and highlight remaining challenges that need to be addressed by the\nresearch community. We will release our dataset publicly to foster research in\nthis socially important area of NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Macina_J/0/1/0/all/0/1\">Jakub Macina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daheim_N/0/1/0/all/0/1\">Nico Daheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Sankalan Pal Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_T/0/1/0/all/0/1\">Tanmay Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapur_M/0/1/0/all/0/1\">Manu Kapur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cascaded Beam Search: Plug-and-Play Terminology-Forcing For Neural Machine Translation. (arXiv:2305.14538v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14538","description":"<p>This paper presents a plug-and-play approach for translation with terminology\nconstraints. Terminology constraints are an important aspect of many modern\ntranslation pipelines. In both specialized domains and newly emerging domains\n(such as the COVID-19 pandemic), accurate translation of technical terms is\ncrucial. Recent approaches often train models to copy terminologies from the\ninput into the output sentence by feeding the target terminology along with the\ninput. But this requires expensive training whenever the underlying language\nmodel is changed or the system should specialize to a new domain. We propose\nCascade Beam Search, a plug-and-play terminology-forcing approach that requires\nno training. Cascade Beam Search has two parts: 1) logit manipulation to\nincrease the probability of target terminologies and 2) a cascading beam setup\nbased on grid beam search, where beams are grouped by the number of\nterminologies they contain. We evaluate the performance of our approach by\ncompeting against the top submissions of the WMT21 terminology translation\ntask. Our plug-and-play approach performs on par with the winning submissions\nwithout using a domain-specific language model and with no additional training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Odermatt_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Odermatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egressy_B/0/1/0/all/0/1\">B&#xe9;ni Egressy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1\">Roger Wattenhofer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond. (arXiv:2305.14540v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14540","description":"<p>With the recent appearance of LLMs in practical settings, having methods that\ncan effectively detect factual inconsistencies is crucial to reduce the\npropagation of misinformation and improve trust in model outputs. When testing\non existing factual consistency benchmarks, we find that a few large language\nmodels (LLMs) perform competitively on classification benchmarks for factual\ninconsistency detection compared to traditional non-LLM methods. However, a\ncloser analysis reveals that most LLMs fail on more complex formulations of the\ntask and exposes issues with existing evaluation benchmarks, affecting\nevaluation precision. To address this, we propose a new protocol for\ninconsistency detection benchmark creation and implement it in a 10-domain\nbenchmark called SummEdits. This new benchmark is 20 times more cost-effective\nper sample than previous benchmarks and highly reproducible, as we estimate\ninter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with\nperformance close to random chance. The best-performing model, GPT-4, is still\n8\\% below estimated human performance, highlighting the gaps in LLMs' ability\nto reason about facts and detect inconsistencies when they occur.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1\">Philippe Laban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryscinski_W/0/1/0/all/0/1\">Wojciech Kry&#x15b;ci&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1\">Divyansh Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1\">Alexander R. Fabbri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Transferability of Whisper-based Representations for \"In-the-Wild\" Cross-Task Downstream Speech Applications. (arXiv:2305.14546v1 [eess.AS])","link":"http://arxiv.org/abs/2305.14546","description":"<p>Large self-supervised pre-trained speech models have achieved remarkable\nsuccess across various speech-processing tasks. The self-supervised training of\nthese models leads to universal speech representations that can be used for\ndifferent downstream tasks, ranging from automatic speech recognition (ASR) to\nspeaker identification. Recently, Whisper, a transformer-based model was\nproposed and trained on large amount of weakly supervised data for ASR; it\noutperformed several state-of-the-art self-supervised models. Given the\nsuperiority of Whisper for ASR, in this paper we explore the transferability of\nthe representation for four other speech tasks in SUPERB benchmark. Moreover,\nwe explore the robustness of Whisper representation for ``in the wild'' tasks\nwhere speech is corrupted by environment noise and room reverberation.\nExperimental results show Whisper achieves promising results across tasks and\nenvironmental conditions, thus showing potential for cross-task real-world\ndeployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chemudupati_V/0/1/0/all/0/1\">Vamsikrishna Chemudupati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tahaei_M/0/1/0/all/0/1\">Marzieh Tahaei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guimaraes_H/0/1/0/all/0/1\">Heitor Guimaraes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pimentel_A/0/1/0/all/0/1\">Arthur Pimentel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Avila_A/0/1/0/all/0/1\">Anderson Avila</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Falk_T/0/1/0/all/0/1\">Tiago Falk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Automatic Fine-grained Inconsistency Detection in Text Summarization. (arXiv:2305.14548v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14548","description":"<p>Existing factual consistency evaluation approaches for text summarization\nprovide binary predictions and limited insights into the weakness of\nsummarization systems. Therefore, we propose the task of fine-grained\ninconsistency detection, the goal of which is to predict the fine-grained types\nof factual errors in a summary. Motivated by how humans inspect factual\ninconsistency in summaries, we propose an interpretable fine-grained\ninconsistency detection model, FineGrainFact, which explicitly represents the\nfacts in the documents and summaries with semantic frames extracted by semantic\nrole labeling, and highlights the related semantic frames to predict\ninconsistency. The highlighted semantic frames help verify predicted error\ntypes and correct inconsistent summaries. Experiment results demonstrate that\nour model outperforms strong baselines and provides evidence to support or\nrefute the summary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1\">Hou Pong Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1\">Qi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sources of Hallucination by Large Language Models on Inference Tasks. (arXiv:2305.14552v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14552","description":"<p>Large Language Models (LLMs) are claimed to be capable of Natural Language\nInference (NLI), necessary for applied tasks like question answering and\nsummarization, yet this capability is under-explored. We present a series of\nbehavioral studies on several LLM families (LLaMA, GPT-3.5, and PaLM) which\nprobe their behavior using controlled experiments. We establish two factors\nwhich predict much of their performance, and propose that these are major\nsources of hallucination in generative LLM. First, the most influential factor\nis memorization of the training data. We show that models falsely label NLI\ntest samples as entailing when the hypothesis is attested in the training text,\nregardless of the premise. We further show that named entity IDs are used as\n\"indices\" to access the memorized data. Second, we show that LLMs exploit a\nfurther corpus-based heuristic using the relative frequencies of words. We show\nthat LLMs score significantly worse on NLI test samples which do not conform to\nthese factors than those which do; we also discuss a tension between the two\nfactors, and a performance trade-off.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McKenna_N/0/1/0/all/0/1\">Nick McKenna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Liang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1\">Mohammad Javad Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Mark Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steedman_M/0/1/0/all/0/1\">Mark Steedman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"All Roads Lead to Rome? Exploring the Invariance of Transformers' Representations. (arXiv:2305.14555v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14555","description":"<p>Transformer models bring propelling advances in various NLP tasks, thus\ninducing lots of interpretability research on the learned representations of\nthe models. However, we raise a fundamental question regarding the reliability\nof the representations. Specifically, we investigate whether transformers learn\nessentially isomorphic representation spaces, or those that are sensitive to\nthe random seeds in their pretraining process. In this work, we formulate the\nBijection Hypothesis, which suggests the use of bijective methods to align\ndifferent models' representation spaces. We propose a model based on invertible\nneural networks, BERT-INN, to learn the bijection more effectively than other\nexisting bijective methods such as the canonical correlation analysis (CCA). We\nshow the advantage of BERT-INN both theoretically and through extensive\nexperiments, and apply it to align the reproduced BERT embeddings to draw\ninsights that are meaningful to the interpretability research. Our code is at\nhttps://github.com/twinkle0331/BERT-similarity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yuxin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qipeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unraveling ChatGPT: A Critical Analysis of AI-Generated Goal-Oriented Dialogues and Annotations. (arXiv:2305.14556v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14556","description":"<p>Large pre-trained language models have exhibited unprecedented capabilities\nin producing high-quality text via prompting techniques. This fact introduces\nnew possibilities for data collection and annotation, particularly in\nsituations where such data is scarce, complex to gather, expensive, or even\nsensitive. In this paper, we explore the potential of these models to generate\nand annotate goal-oriented dialogues, and conduct an in-depth analysis to\nevaluate their quality. Our experiments employ ChatGPT, and encompass three\ncategories of goal-oriented dialogues (task-oriented, collaborative, and\nexplanatory), two generation modes (interactive and one-shot), and two\nlanguages (English and Italian). Based on extensive human-based evaluations, we\ndemonstrate that the quality of generated dialogues and annotations is on par\nwith those generated by humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Labruna_T/0/1/0/all/0/1\">Tiziano Labruna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brenna_S/0/1/0/all/0/1\">Sofia Brenna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaninello_A/0/1/0/all/0/1\">Andrea Zaninello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magnini_B/0/1/0/all/0/1\">Bernardo Magnini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents. (arXiv:2305.14564v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14564","description":"<p>Strategies such as chain-of-thought prompting improve the performance of\nlarge language models (LLMs) on complex reasoning tasks by decomposing input\nexamples into intermediate steps. However, it remains unclear how to apply such\nmethods to reason over long input documents, in which both the decomposition\nand the output of each intermediate step are non-trivial to obtain. In this\nwork, we propose PEARL, a prompting framework to improve reasoning over long\ndocuments, which consists of three stages: action mining, plan formulation, and\nplan execution. More specifically, given a question about a long document,\nPEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE,\nFIND_EVENT, FIND_RELATION) and then executes them over the document to obtain\nthe answer. Each stage of PEARL is implemented via zero-shot or few-shot\nprompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate\nPEARL on a challenging subset of the QuALITY dataset, which contains questions\nthat require complex reasoning over long narrative texts. PEARL outperforms\nzero-shot and chain-of-thought prompting on this dataset, and ablation\nexperiments show that each stage of PEARL is critical to its performance.\nOverall, PEARL is a first step towards leveraging LLMs to reason over long\ndocuments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Simeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Unified Question Answering: Tuning Models or Prompts?. (arXiv:2305.14569v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14569","description":"<p>Question-answering (QA) tasks often investigate specific question types,\nknowledge domains, or reasoning skills, leading to specialized models catering\nto specific categories of QA tasks. While recent research has explored the idea\nof unified QA models, such models are usually explored for high-resource\nscenarios and require re-training to extend their capabilities. To overcome\nthese drawbacks, the paper explores the potential of two paradigms of tuning,\nmodel, and prompts, for unified QA under a low-resource setting. The paper\nprovides an exhaustive analysis of their applicability using 16 QA datasets,\nrevealing that prompt tuning can perform as well as model tuning in a few-shot\nsetting with a good initialization. The study also shows that parameter-sharing\nresults in superior few-shot performance, simple knowledge transfer techniques\nfor prompt initialization can be effective, and prompt tuning achieves a\nsignificant performance boost from pre-training in a low-resource regime. The\nresearch offers insights into the advantages and limitations of prompt tuning\nfor unified QA in a few-shot setting, contributing to the development of\neffective and efficient systems in low-resource scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bansal_S/0/1/0/all/0/1\">Srijan Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1\">Semih Yavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_M/0/1/0/all/0/1\">Meghana Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding. (arXiv:2305.14571v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14571","description":"<p>Current state-of-the-art models for natural language understanding require a\npreprocessing step to convert raw text into discrete tokens. This process known\nas tokenization relies on a pre-built vocabulary of words or sub-word\nmorphemes. This fixed vocabulary limits the model's robustness to spelling\nerrors and its capacity to adapt to new domains. In this work, we introduce a\nnovel open-vocabulary language model that adopts a hierarchical two-level\napproach: one at the word level and another at the sequence level. Concretely,\nwe design an intra-word module that uses a shallow Transformer architecture to\nlearn word representations from their characters, and a deep inter-word\nTransformer module that contextualizes each word representation by attending to\nthe entire word sequence. Our model thus directly operates on character\nsequences with explicit awareness of word boundaries, but without biased\nsub-word or word-level vocabulary. Experiments on various downstream tasks show\nthat our method outperforms strong baselines. We also demonstrate that our\nhierarchical model is robust to textual corruption and domain shift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Li Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luisier_F/0/1/0/all/0/1\">Florian Luisier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1\">Kayhan Batmanghelich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florencio_D/0/1/0/all/0/1\">Dinei Florencio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cha Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting and Mitigating Indirect Stereotypes in Word Embeddings. (arXiv:2305.14574v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14574","description":"<p>Societal biases in the usage of words, including harmful stereotypes, are\nfrequently learned by common word embedding methods. These biases manifest not\nonly between a word and an explicit marker of its stereotype, but also between\nwords that share related stereotypes. This latter phenomenon, sometimes called\n\"indirect bias,'' has resisted prior attempts at debiasing. In this paper, we\npropose a novel method called Biased Indirect Relationship Modification (BIRM)\nto mitigate indirect bias in distributional word embeddings by modifying biased\nrelationships between words before embeddings are learned. This is done by\nconsidering how the co-occurrence probability of a given pair of words changes\nin the presence of words marking an attribute of bias, and using this to\naverage out the effect of a bias attribute. To evaluate this method, we perform\na series of common tests and demonstrate that measures of bias in the word\nembeddings are reduced in exchange for minor reduction in the semantic quality\nof the embeddings. In addition, we conduct novel tests for measuring indirect\nstereotypes by extending the Word Embedding Association Test (WEAT) with new\ntest sets for indirect binary gender stereotypes. With these tests, we\ndemonstrate the presence of more subtle stereotypes not addressed by previous\nwork. The proposed method is able to reduce the presence of some of these new\nstereotypes, serving as a crucial next step towards non-stereotyped word\nembeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+George_E/0/1/0/all/0/1\">Erin George</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chew_J/0/1/0/all/0/1\">Joyce Chew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Needell_D/0/1/0/all/0/1\">Deanna Needell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings. (arXiv:2305.14576v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14576","description":"<p>Pre-trained language models (PLMs) have ignited a surge in demand for\neffective fine-tuning techniques, particularly in low-resource domains and\nlanguages. Active learning (AL), a set of algorithms designed to decrease\nlabeling costs by minimizing label complexity, has shown promise in confronting\nthe labeling bottleneck. Concurrently, adapter modules, designed for\nparameter-efficient fine-tuning (PEFT), have showcased notable potential in\nlow-resource settings. However, the interplay between AL and adapter-based PEFT\nremains unexplored. In our study, we empirically investigate PEFT behavior with\nAL in low-resource settings for text classification tasks. Our findings affirm\nthe superiority of PEFT over full-fine tuning (FFT) in low-resource settings\nand demonstrate that this advantage persists in AL setups. Finally, we delve\ninto the properties of PEFT and FFT through the lens of forgetting dynamics and\ninstance-level representations, linking them to AL instance selection behavior\nand the stability of PEFT. Our research underscores the synergistic potential\nof AL, PEFT, and TAPT in low-resource settings, paving the way for advancements\nin efficient and effective fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jukic_J/0/1/0/all/0/1\">Josip Juki&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snajder_J/0/1/0/all/0/1\">Jan &#x160;najder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Difference-Masking: Choosing What to Mask in Continued Pretraining. (arXiv:2305.14577v1 [cs.LG])","link":"http://arxiv.org/abs/2305.14577","description":"<p>Self-supervised learning (SSL) and the objective of masking-and-predicting in\nparticular have led to promising SSL performance on a variety of downstream\ntasks. However, while most approaches randomly mask tokens, there is strong\nintuition from the field of education that deciding what to mask can\nsubstantially improve learning outcomes. We introduce Difference-Masking, an\napproach that automatically chooses what to mask during continued pretraining\nby considering what makes an unlabelled target domain different from the\npretraining domain. Empirically, we find that Difference-Masking outperforms\nbaselines on continued pretraining settings across four diverse language and\nmultimodal video tasks. The cross-task applicability of Difference-Masking\nsupports the effectiveness of our framework for SSL pretraining in language,\nvision, and other domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilf_A/0/1/0/all/0/1\">Alex Wilf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akter_S/0/1/0/all/0/1\">Syeda Nahida Akter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_L/0/1/0/all/0/1\">Leena Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_S/0/1/0/all/0/1\">Sheryl Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mengrou Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyberg_E/0/1/0/all/0/1\">Eric Nyberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Connecting the Dots: What Graph-Based Text Representations Work Best for Text Classification using Graph Neural Networks?. (arXiv:2305.14578v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14578","description":"<p>Given the success of Graph Neural Networks (GNNs) for structure-aware machine\nlearning, numerous studies have explored their application to text\nclassification, as an alternative to traditional feature representation models.\nHowever, most studies considered just a specific domain and validated on data\nwith particular characteristics. This work presents an extensive empirical\ninvestigation of graph-based text representation methods proposed for text\nclassification, identifying practical implications and open challenges in the\nfield. We compare several GNN architectures as well as BERT across five\ndatasets, encompassing short and also long documents. The results show that: i)\ngraph performance is highly related to the textual input features and domain,\nii) despite its outstanding performance, BERT has difficulties converging when\ndealing with short texts, iii) graph methods are particularly beneficial for\nlonger documents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bugueno_M/0/1/0/all/0/1\">Margarita Bugue&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1\">Gerard de Melo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating OpenAI's Whisper ASR for Punctuation Prediction and Topic Modeling of life histories of the Museum of the Person. (arXiv:2305.14580v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14580","description":"<p>Automatic speech recognition (ASR) systems play a key role in applications\ninvolving human-machine interactions. Despite their importance, ASR models for\nthe Portuguese language proposed in the last decade have limitations in\nrelation to the correct identification of punctuation marks in automatic\ntranscriptions, which hinder the use of transcriptions by other systems,\nmodels, and even by humans. However, recently Whisper ASR was proposed by\nOpenAI, a general-purpose speech recognition model that has generated great\nexpectations in dealing with such limitations. This chapter presents the first\nstudy on the performance of Whisper for punctuation prediction in the\nPortuguese language. We present an experimental evaluation considering both\ntheoretical aspects involving pausing points (comma) and complete ideas\n(exclamation, question, and fullstop), as well as practical aspects involving\ntranscript-based topic modeling - an application dependent on punctuation marks\nfor promising performance. We analyzed experimental results from videos of\nMuseum of the Person, a virtual museum that aims to tell and preserve people's\nlife histories, thus discussing the pros and cons of Whisper in a real-world\nscenario. Although our experiments indicate that Whisper achieves\nstate-of-the-art results, we conclude that some punctuation marks require\nimprovements, such as exclamation, semicolon and colon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gris_L/0/1/0/all/0/1\">Lucas Rafael Stefanel Gris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcacini_R/0/1/0/all/0/1\">Ricardo Marcacini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_A/0/1/0/all/0/1\">Arnaldo Candido Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casanova_E/0/1/0/all/0/1\">Edresson Casanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_A/0/1/0/all/0/1\">Anderson Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aluisio_S/0/1/0/all/0/1\">Sandra Maria Alu&#xed;sio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making the Implicit Explicit: Implicit Content as a First Class Citizen in NLP. (arXiv:2305.14583v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14583","description":"<p>Language is multifaceted. A given utterance can be re-expressed in equivalent\nforms, and its implicit and explicit content support various logical and\npragmatic inferences. When processing an utterance, we consider these different\naspects, as mediated by our interpretive goals -- understanding that \"it's dark\nin here\" may be a veiled direction to turn on a light. Nonetheless, NLP methods\ntypically operate over the surface form alone, eliding this nuance.\n</p>\n<p>In this work, we represent language with language, and direct an LLM to\ndecompose utterances into logical and plausible inferences. The reduced\ncomplexity of the decompositions makes them easier to embed, opening up novel\napplications. Variations on our technique lead to state-of-the-art improvements\non sentence embedding benchmarks, a substantive application in computational\npolitical science, and to a novel construct-discovery process, which we\nvalidate with human annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoyle_A/0/1/0/all/0/1\">Alexander Hoyle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_R/0/1/0/all/0/1\">Rupak Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_P/0/1/0/all/0/1\">Pranav Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Resnik_P/0/1/0/all/0/1\">Philip Resnik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextualized Topic Coherence Metrics. (arXiv:2305.14587v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14587","description":"<p>The recent explosion in work on neural topic modeling has been criticized for\noptimizing automated topic evaluation metrics at the expense of actual\nmeaningful topic identification. But human annotation remains expensive and\ntime-consuming. We propose LLM-based methods inspired by standard human topic\nevaluations, in a family of metrics called Contextualized Topic Coherence\n(CTC). We evaluate both a fully automated version as well as a semi-automated\nCTC that allows human-centered evaluation of coherence while maintaining the\nefficiency of automated methods. We evaluate CTC relative to five other metrics\non six topic models and find that it outperforms automated topic coherence\nmethods, works well on short documents, and is not susceptible to meaningless\nbut high-scoring topics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_H/0/1/0/all/0/1\">Hamed Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoover_J/0/1/0/all/0/1\">Jacob Louis Hoover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mimno_D/0/1/0/all/0/1\">David Mimno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naacke_H/0/1/0/all/0/1\">Hubert Naacke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constantin_C/0/1/0/all/0/1\">Camelia Constantin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amann_B/0/1/0/all/0/1\">Bernd Amann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating end-to-end entity linking on domain-specific knowledge bases: Learning about ancient technologies from museum collections. (arXiv:2305.14588v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14588","description":"<p>To study social, economic, and historical questions, researchers in the\nsocial sciences and humanities have started to use increasingly large\nunstructured textual datasets. While recent advances in NLP provide many tools\nto efficiently process such data, most existing approaches rely on generic\nsolutions whose performance and suitability for domain-specific tasks is not\nwell understood. This work presents an attempt to bridge this domain gap by\nexploring the use of modern Entity Linking approaches for the enrichment of\nmuseum collection data. We collect a dataset comprising of more than 1700 texts\nannotated with 7,510 mention-entity pairs, evaluate some off-the-shelf\nsolutions in detail using this dataset and finally fine-tune a recent\nend-to-end EL model on this data. We show that our fine-tuned model\nsignificantly outperforms other approaches currently available in this domain\nand present a proof-of-concept use case of this model. We release our dataset\nand our best model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cadavid_Sanchez_S/0/1/0/all/0/1\">Sebastian Cadavid-Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kacem_K/0/1/0/all/0/1\">Khalil Kacem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frade_R/0/1/0/all/0/1\">Rafael Aparecido Martins Frade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boehm_J/0/1/0/all/0/1\">Johannes Boehm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaney_T/0/1/0/all/0/1\">Thomas Chaney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lashkari_D/0/1/0/all/0/1\">Danial Lashkari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simig_D/0/1/0/all/0/1\">Daniel Simig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RE$^2$: Region-Aware Relation Extraction from Visually Rich Documents. (arXiv:2305.14590v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14590","description":"<p>Current research in form understanding predominantly relies on large\npre-trained language models, necessitating extensive data for pre-training.\nHowever, the importance of layout structure (i.e., the spatial relationship\nbetween the entity blocks in the visually rich document) to relation extraction\nhas been overlooked. In this paper, we propose REgion-Aware Relation Extraction\n(RE$^2$) that leverages region-level spatial structure among the entity blocks\nto improve their relation prediction. We design an edge-aware graph attention\nnetwork to learn the interaction between entities while considering their\nspatial relationship defined by their region-level representations. We also\nintroduce a constraint objective to regularize the model towards consistency\nwith the inherent constraints of the relation extraction task. Extensive\nexperiments across various datasets, languages and domains demonstrate the\nsuperiority of our proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramu_P/0/1/0/all/0/1\">Pritika Ramu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sijia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouatadid_L/0/1/0/all/0/1\">Lalla Mouatadid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rimchala_J/0/1/0/all/0/1\">Joy Rimchala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALGO: Synthesizing Algorithmic Programs with Generated Oracle Verifiers. (arXiv:2305.14591v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14591","description":"<p>Large language models (LLMs) excel at implementing code from functionality\ndescriptions, but struggle with algorithmic problems that require not only\nimplementation but also identification of the suitable algorithm. Moreover,\nLLM-generated programs lack guaranteed correctness and require human\nverification. To address these challenges, we propose ALGO, a framework that\nsynthesizes Algorithmic programs with LLM-Generated Oracles to guide the\ncreation and verify their correctness. ALGO first generates a probably correct\nbut possibly slow reference oracle by prompting an LLM to exhaustively\nenumerate all the combinations of relevant variables. This oracle is then\nutilized to guide an arbitrary search strategy in exploring the algorithm space\nand to verify the algorithms synthesized. Our study shows that the\nLLM-generated oracles are correct for 88% of the cases. With the oracles as\nverifiers, ALGO can be integrated with any existing code generation model in a\nmodel-agnostic manner to enhance its performance. Experiments show that when\nequipped with ALGO, we achieve an 8x better one-submission pass rate over the\nCodex model and a 2.6x better one-submission pass rate over CodeT, the current\nstate-of-the-art model on CodeContests. We can also get 1.3x better pass rate\nover the ChatGPT Code Interpreter on unseen problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kexun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Danqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Jingtao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instruction Tuning with Lexicons for Zero-Shot Style Classification. (arXiv:2305.14592v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14592","description":"<p>Style is used to convey authors' intentions and attitudes. Despite the\nsuccess of large pre-trained language models on style classification, prior\nwork relies on fine-tuning with labeled examples. Prompting large language\nmodels to classify style without fine-tuning is challenging because language\nstyles can be difficult to define. In this study, we investigate the\neffectiveness of style lexicons as a means for instructing language models how\nto identify new styles that are unseen during training. Our experiments show\nthat lexicon-based instructions improve transfer zero-shot performance\nsignificantly. We will release our code and data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1\">Ruohao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attentiveness to Answer Choices Doesn't Always Entail High QA Accuracy. (arXiv:2305.14596v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14596","description":"<p>When large language models (LMs) are applied in zero- or few-shot settings to\ndiscriminative tasks such as multiple-choice questions, their attentiveness\n(i.e., probability mass) is spread across many vocabulary tokens that are not\nvalid choices. Such a spread across multiple surface forms with identical\nmeaning is thought to cause an underestimation of a model's true performance,\nreferred to as the \"surface form competition\" (SFC) hypothesis. This has\nmotivated the introduction of various probability normalization methods.\nHowever, many core questions remain unanswered. How do we measure SFC or\nattentiveness? Are there direct ways of increasing attentiveness on valid\nchoices? Does increasing attentiveness always improve task accuracy? We propose\na mathematical formalism for studying this phenomenon, provide a metric for\nquantifying attentiveness, and identify a simple method for increasing it --\nnamely, in-context learning with even just one example containing answer\nchoices. The formalism allows us to quantify SFC and bound its impact. Our\nexperiments on three diverse datasets and six LMs reveal several surprising\nfindings. For example, encouraging models to generate a valid answer choice\ncan, in fact, be detrimental to task performance for some LMs, and prior\nprobability normalization methods are less effective (sometimes even\ndetrimental) to instruction-tuned LMs. We conclude with practical insights for\neffectively using prompted LMs for multiple-choice tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wiegreffe_S/0/1/0/all/0/1\">Sarah Wiegreffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finlayson_M/0/1/0/all/0/1\">Matthew Finlayson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tafjord_O/0/1/0/all/0/1\">Oyvind Tafjord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Voices of Her: Analyzing Gender Differences in the AI Publication World. (arXiv:2305.14597v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14597","description":"<p>While several previous studies have analyzed gender bias in research, we are\nstill missing a comprehensive analysis of gender differences in the AI\ncommunity, covering diverse topics and different development trends. Using the\nAI Scholar dataset of 78K researchers in the field of AI, we identify several\ngender differences: (1) Although female researchers tend to have fewer overall\ncitations than males, this citation difference does not hold for all\nacademic-age groups; (2) There exist large gender homophily in co-authorship on\nAI papers; (3) Female first-authored papers show distinct linguistic styles,\nsuch as longer text, more positive emotion words, and more catchy titles than\nmale first-authored papers. Our analysis provides a window into the current\ndemographic trends in our AI community, and encourages more gender equality and\ndiversity in the future. Our code and data are at\nhttps://github.com/causalNLP/ai-scholar-gender.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yiwen Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiarui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1\">Zhiheng Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoelkopf_B/0/1/0/all/0/1\">Bernhard Schoelkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging Continuous and Discrete Spaces: Interpretable Sentence Representation Learning via Compositional Operations. (arXiv:2305.14599v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14599","description":"<p>Traditional sentence embedding models encode sentences into vector\nrepresentations to capture useful properties such as the semantic similarity\nbetween sentences. However, in addition to similarity, sentence semantics can\nalso be interpreted via compositional operations such as sentence fusion or\ndifference. It is unclear whether the compositional semantics of sentences can\nbe directly reflected as compositional operations in the embedding space. To\nmore effectively bridge the continuous embedding and discrete text spaces, we\nexplore the plausibility of incorporating various compositional properties into\nthe sentence embedding space that allows us to interpret embedding\ntransformations as compositional sentence operations. We propose InterSent, an\nend-to-end framework for learning interpretable sentence embeddings that\nsupports compositional sentence operations in the embedding space. Our method\noptimizes operator networks and a bottleneck encoder-decoder model to produce\nmeaningful and interpretable sentence embeddings. Experimental results\ndemonstrate that our method significantly improves the interpretability of\nsentence embeddings on four textual generation tasks over existing approaches\nwhile maintaining strong performance on traditional semantic similarity tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">James Y. Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wenlin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaiqiang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Semantic Role Labeling from Compatible Label Sequences. (arXiv:2305.14600v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14600","description":"<p>This paper addresses the question of how to efficiently learn from disjoint,\ncompatible label sequences. We argue that the compatible structures between\ndisjoint label sets help model learning and inference. We verify this\nhypothesis on the task of semantic role labeling (SRL), specifically, tagging a\nsentence with two role sequences: VerbNet arguments and PropBank arguments.\nPrior work has shown that cross-task interaction improves performance. However,\nthe two tasks are still separately decoded, running the risk of generating\nstructurally inconsistent label sequences (as per lexicons like SEMLINK). To\neliminate this issue, we first propose a simple and effective setup that\njointly handles VerbNet and PropBank labels as one sequence. With this setup,\nwe show that enforcing SEMLINK constraints during decoding constantly improves\nthe overall F1. With special input constructions, our joint model infers\nVerbNet arguments from PropBank arguments with over 99% accuracy. We also\npropose a constrained marginal model that uses SEMLINK information during\ntraining to further benefit from the large amounts of PropBank-only data. Our\nmodels achieve state-of-the-art F1's on VerbNet and PropBank argument labeling\non the CoNLL05 dataset with strong out-of-domain generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazeminejad_G/0/1/0/all/0/1\">Ghazaleh Kazeminejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_S/0/1/0/all/0/1\">Susan W. Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palmer_M/0/1/0/all/0/1\">Martha Palmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenPI2.0: An Improved Dataset for Entity Tracking in Texts. (arXiv:2305.14603v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14603","description":"<p>Representing texts as information about entities has long been deemed\neffective in event reasoning. We propose OpenPI2.0, an improved dataset for\ntracking entity states in procedural texts. OpenPI2.0 features not only\ncanonicalized entities that facilitate evaluation, but also salience\nannotations including both manual labels and automatic predictions. Regarding\nentity salience, we provide a survey on annotation subjectivity, modeling\nfeasibility, and downstream applications in tasks such as question answering\nand classical planning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hainiu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kommula_A/0/1/0/all/0/1\">Abhinav Kommula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_N/0/1/0/all/0/1\">Niket Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language Models. (arXiv:2305.14610v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14610","description":"<p>We introduce the notion of geopolitical bias -- a tendency to report\ndifferent geopolitical knowledge depending on the linguistic context. As a case\nstudy, we consider territorial disputes between countries. For example, for the\nwidely contested Spratly Islands, would an LM be more likely to say they belong\nto China if asked in Chinese, vs. to the Philippines if asked in Tagalog? To\nevaluate if such biases exist, we first collect a dataset of territorial\ndisputes from Wikipedia, then associate each territory with a set of\nmultilingual, multiple-choice questions. This dataset, termed BorderLines,\nconsists of 250 territories with questions in 45 languages. We pose these\nquestion sets to language models, and analyze geopolitical bias in their\nresponses through several proposed quantitative metrics. The metrics compare\nbetween responses in different question languages as well as to the actual\ngeopolitical situation. The phenomenon of geopolitical bias is a uniquely\ncross-lingual evaluation, contrasting with prior work's monolingual (mostly\nEnglish) focus on bias evaluation. Its existence shows that the knowledge of\nLMs, unlike multilingual humans, is inconsistent across languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bryan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selectively Answering Ambiguous Questions. (arXiv:2305.14613v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14613","description":"<p>Trustworthy language models should abstain from answering questions when they\ndo not know the answer. However, the answer to a question can be unknown for a\nvariety of reasons. Prior research has focused on the case in which the\nquestion is clear and the answer is unambiguous but possibly unknown. However,\nthe answer to a question can also be unclear due to uncertainty of the\nquestioner's intent or context. We investigate question answering from this\nperspective, focusing on answering a subset of questions with a high degree of\naccuracy, from a set of questions in which many are inherently ambiguous. In\nthis setting, we find that the most reliable approach to calibration involves\nquantifying repetition within a set of sampled model outputs, rather than the\nmodel's likelihood or self-verification as used in prior work. % We find this\nto be the case across different types of uncertainty, varying model scales and\nboth with or without instruction tuning. Our results suggest that\nsampling-based confidence scores help calibrate answers to relatively\nunambiguous questions, with more dramatic improvements on ambiguous questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cole_J/0/1/0/all/0/1\">Jeremy R. Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Michael J.Q. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillick_D/0/1/0/all/0/1\">Daniel Gillick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenschlos_J/0/1/0/all/0/1\">Julian Martin Eisenschlos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1\">Bhuwan Dhingra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1\">Jacob Eisenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Grounding Issues in Image Caption. (arXiv:2305.14616v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14616","description":"<p>This paper explores the grounding issue concerning multimodal semantic\nrepresentation from a computational cognitive-linguistic view. Five perceptual\nproperties of groundedness are annotated and analyzed: Affordance, Perceptual\nsalience, Object number, Gaze cueing, and Ecological Niche Association (ENA).\nWe annotated selected images from the Flickr30k dataset with exploratory\nanalyses and statistical modeling of their captions. Our findings suggest that\na comprehensive understanding of an object or event requires cognitive\nattention, semantic distinctions in linguistic expression, and multimodal\nconstruction. During this construction process, viewers integrate situated\nmeaning and affordance into multimodal semantics, which is consolidated into\nimage captions used in the image-text dataset incorporating visual and textual\nelements. Our findings suggest that situated meaning and affordance grounding\nare critical for grounded natural language understanding systems to generate\nappropriate responses and show the potential to advance the understanding of\nhuman construal in diverse situations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Er Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_H/0/1/0/all/0/1\">Hsin-Yu Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Po-Ya Angela Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_Y/0/1/0/all/0/1\">Yu-Hsiang Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_S/0/1/0/all/0/1\">Shu-Kai Hsieh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COMET-M: Reasoning about Multiple Events in Complex Sentences. (arXiv:2305.14617v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14617","description":"<p>Understanding the speaker's intended meaning often involves drawing\ncommonsense inferences to reason about what is not stated explicitly. In\nmulti-event sentences, it requires understanding the relationships between\nevents based on contextual knowledge. We propose COMET-M (Multi-Event), an\nevent-centric commonsense model capable of generating commonsense inferences\nfor a target event within a complex sentence. COMET-M builds upon COMET\n(Bosselut et al., 2019), which excels at generating event-centric inferences\nfor simple sentences, but struggles with the complexity of multi-event\nsentences prevalent in natural text. To overcome this limitation, we curate a\nmulti-event inference dataset of 35K human-written inferences. We trained\nCOMET-M on the human-written inferences and also created baselines using\nautomatically labeled examples. Experimental results demonstrate the\nsignificant performance improvement of COMET-M over COMET in generating\nmulti-event inferences. Moreover, COMET-M successfully produces distinct\ninferences for each target event, taking the complete context into\nconsideration. COMET-M holds promise for downstream tasks involving natural\ntext such as coreference resolution, dialogue, and story understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1\">Sahithya Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_R/0/1/0/all/0/1\">Raymond Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1\">Vered Shwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations. (arXiv:2305.14618v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14618","description":"<p>Abductive reasoning aims to find plausible explanations for an event. This\nstyle of reasoning is critical for commonsense tasks where there are often\nmultiple plausible explanations. Existing approaches for abductive reasoning in\nnatural language processing (NLP) often rely on manually generated annotations\nfor supervision; however, such annotations can be subjective and biased.\nInstead of using direct supervision, this work proposes an approach for\nabductive commonsense reasoning that exploits the fact that only a subset of\nexplanations is correct for a given context. The method uses posterior\nregularization to enforce a mutual exclusion constraint, encouraging the model\nto learn the distinction between fluent explanations and plausible ones. We\nevaluate our approach on a diverse set of abductive reasoning datasets;\nexperimental results show that our approach outperforms or is comparable to\ndirectly applying pretrained language models in a zero-shot manner and other\nknowledge-augmented zero-shot methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenting Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_J/0/1/0/all/0/1\">Justin T. Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardie_C/0/1/0/all/0/1\">Claire Cardie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EXnet: Efficient In-context Learning for Data-less Text classification. (arXiv:2305.14622v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14622","description":"<p>Large pre-trained language models (PLMs) have made significant progress in\nencoding world knowledge and spawned a new set of learning paradigms including\nzero-shot, few-shot, and in-context learning. Many language tasks can be\nmodeled as a set of prompts (for example, is this text about geography?) and\nlanguage models can provide binary answers, i.e., Yes or No. There is evidence\nto suggest that the next-word prediction used by many PLMs does not align well\nwith zero-shot paradigms. Therefore, PLMs are fine-tuned as a\nquestion-answering system. In-context learning extends zero-shot learning by\nincorporating prompts and examples, resulting in increased task accuracy. Our\npaper presents EXnet, a model specifically designed to perform in-context\nlearning without any limitations on the number of examples. We argue that\nin-context learning is an effective method to increase task accuracy, and\nproviding examples facilitates cross-task generalization, especially when it\ncomes to text classification tasks. With extensive experiments, we show that\neven our smallest model (15M parameters) generalizes to several unseen\nclassification tasks and domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shome_D/0/1/0/all/0/1\">Debaditya Shome</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_K/0/1/0/all/0/1\">Kuldeep Yadav</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models. (arXiv:2305.14623v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14623","description":"<p>Fact-checking is an essential task in NLP that is commonly utilized for\nvalidating the factual accuracy of claims. Prior work has mainly focused on\nfine-tuning pre-trained languages models on specific datasets, which can be\ncomputationally intensive and time-consuming. With the rapid development of\nlarge language models (LLMs), such as ChatGPT and GPT-3, researchers are now\nexploring their in-context learning capabilities for a wide range of tasks. In\nthis paper, we aim to assess the capacity of LLMs for fact-checking by\nintroducing Self-Checker, a framework comprising a set of plug-and-play modules\nthat facilitate fact-checking by purely prompting LLMs in an almost zero-shot\nsetting. This framework provides a fast and efficient way to construct\nfact-checking systems in low-resource environments. Empirical results\ndemonstrate the potential of Self-Checker in utilizing LLMs for fact-checking.\nHowever, there is still significant room for improvement compared to SOTA\nfine-tuned models, which suggests that LLM adoption could be a promising\napproach for future fact-checking research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Miaoran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KNN-LM Does Not Improve Open-ended Text Generation. (arXiv:2305.14625v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14625","description":"<p>In this paper, we study the generation quality of interpolation-based\nretrieval-augmented language models (LMs). These methods, best exemplified by\nthe KNN-LM, interpolate the LM's predicted distribution of the next word with a\ndistribution formed from the most relevant retrievals for a given prefix. While\nthe KNN-LM and related methods yield impressive decreases in perplexity, we\ndiscover that they do not exhibit corresponding improvements in open-ended\ngeneration quality, as measured by both automatic evaluation metrics (e.g.,\nMAUVE) and human evaluations. Digging deeper, we find that interpolating with a\nretrieval distribution actually increases perplexity compared to a baseline\nTransformer LM for the majority of tokens in the WikiText-103 test set, even\nthough the overall perplexity is lower due to a smaller number of tokens for\nwhich perplexity dramatically decreases after interpolation. However, when\ndecoding a long sequence at inference time, significant improvements on this\nsmaller subset of tokens are washed out by slightly worse predictions on most\ntokens. Furthermore, we discover that the entropy of the retrieval distribution\nincreases faster than that of the base LM as the generated sequence becomes\nlonger, which indicates that retrieval is less reliable when using\nmodel-generated text as queries (i.e., is subject to exposure bias). We hope\nthat our analysis spurs future work on improved decoding algorithms and\ninterpolation strategies for retrieval-augmented language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shufan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yixiao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozdov_A/0/1/0/all/0/1\">Andrew Drozdov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garimella_A/0/1/0/all/0/1\">Aparna Garimella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manjunatha_V/0/1/0/all/0/1\">Varun Manjunatha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enabling Large Language Models to Generate Text with Citations. (arXiv:2305.14627v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14627","description":"<p>Large language models (LLMs) have emerged as a widely-used tool for\ninformation seeking, but their generated outputs are prone to hallucination. In\nthis work, we aim to enable LLMs to generate text with citations, improving\ntheir factual correctness and verifiability. Existing work mainly relies on\ncommercial search engines and human evaluation, making it challenging to\nreproduce and compare with different modeling approaches. We propose ALCE, the\nfirst benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a\ndiverse set of questions and retrieval corpora and requires building end-to-end\nsystems to retrieve supporting evidence and generate answers with citations. We\nbuild automatic metrics along three dimensions -- fluency, correctness, and\ncitation quality -- and demonstrate their strong correlation with human\njudgements. Our experiments with state-of-the-art LLMs and novel prompting\nstrategies show that current systems have considerable room for improvements --\nfor example, on the ELI5 dataset, even the best model has 49% of its\ngenerations lacking complete citation support. Our extensive analyses further\nhighlight promising future directions, including developing better retrievers,\nadvancing long-context LLMs, and improving the ability to synthesize\ninformation from multiple sources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_H/0/1/0/all/0/1\">Howard Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiatong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixture of Prompt Experts for Generalizable and Interpretable Question Answering. (arXiv:2305.14628v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14628","description":"<p>One of the ultimate quests of question answering (QA) is to deploy a system\nthat can answer any type of question from the users, and refrain from answering\nwhen it does not know the answer. While recent advancements in scaling large\nlanguage models (LLMs) brought significant improvements on various QA datasets,\nit remains difficult for a single model to generalize across question types\nthat require distinct reasoning abilities. In this paper, we first provide\nempirical evidence that state-of-the-art LLMs such as Codex suffer from poor\ngeneralizability on question types beyond those seen in the prompt. To address\nthis, we propose a Mixture-of-Prompt-Experts (MOPE) system that ensembles\nmultiple specialized LLMs. We first implement each specialized model based on\nthe same backbone model (Codex) but with prompts optimized for different\nreasoning categories including factual, multihop, mathematical, and commonsense\nreasoning. By strategically selecting the best specialized model for each given\nquestion, our MOPE system significantly outperforms any single specialized\nmodel on a collection of 12 QA datasets from four reasoning types. Moreover,\nthe attribution and agreement among specialized expert models offer greater\ninterpretability, allowing for better selective question answering. Our human\nstudy further confirms that presenting the expert predictions and answer\nselection process helps annotators more accurately decide when to trust the\nsystem's output. We release all code and data to facilitate future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1\">Chenglei Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weijia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1\">Jordan Boyd-Graber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Testing Causal Models of Word Meaning in GPT-3 and -4. (arXiv:2305.14630v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14630","description":"<p>Large Language Models (LLMs) have driven extraordinary improvements in NLP.\nHowever, it is unclear how such models represent lexical concepts-i.e., the\nmeanings of the words they use. This paper evaluates the lexical\nrepresentations of GPT-3 and GPT-4 through the lens of HIPE theory, a theory of\nconcept representations which focuses on representations of words describing\nartifacts (such as \"mop\", \"pencil\", and \"whistle\"). The theory posits a causal\ngraph that relates the meanings of such words to the form, use, and history of\nthe objects to which they refer. We test LLMs using the same stimuli originally\nused by Chaigneau et al. (2004) to evaluate the theory in humans, and consider\na variety of prompt designs. Our experiments concern judgements about causal\noutcomes, object function, and object naming. We find no evidence that GPT-3\nencodes the causal structure hypothesized by HIPE, but do find evidence that\nGPT-4 encodes such structure. The results contribute to a growing body of\nresearch characterizing the representational capacity of large language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Musker_S/0/1/0/all/0/1\">Sam Musker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation. (arXiv:2305.14635v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14635","description":"<p>End-to-end speech translation (ST) is the task of translating speech signals\nin the source language into text in the target language. As a cross-modal task,\nend-to-end ST is difficult to train with limited data. Existing methods often\ntry to transfer knowledge from machine translation (MT), but their performances\nare restricted by the modality gap between speech and text. In this paper, we\npropose Cross-modal Mixup via Optimal Transport CMOT to overcome the modality\ngap. We find the alignment between speech and text sequences via optimal\ntransport and then mix up the sequences from different modalities at a token\nlevel using the alignment. Experiments on the MuST-C ST benchmark demonstrate\nthat CMOT achieves an average BLEU of 30.0 in 8 translation directions,\noutperforming previous methods. Further analysis shows CMOT can adaptively find\nthe alignment between modalities, which helps alleviate the modality gap\nbetween speech and text. Code is publicly available at\nhttps://github.com/ictnlp/CMOT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qingkai Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iteratively Improving Biomedical Entity Linking and Event Extraction via Hard Expectation-Maximization. (arXiv:2305.14645v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14645","description":"<p>Biomedical entity linking and event extraction are two crucial tasks to\nsupport text understanding and retrieval in the biomedical domain. These two\ntasks intrinsically benefit each other: entity linking disambiguates the\nbiomedical concepts by referring to external knowledge bases and the domain\nknowledge further provides additional clues to understand and extract the\nbiological processes, while event extraction identifies a key trigger and\nentities involved to describe each biological process which also captures the\nstructural context to better disambiguate the biomedical entities. However,\nprevious research typically solves these two tasks separately or in a pipeline,\nleading to error propagation. What's more, it's even more challenging to solve\nthese two tasks together as there is no existing dataset that contains\nannotations for both tasks. To solve these challenges, we propose joint\nbiomedical entity linking and event extraction by regarding the event\nstructures and entity references in knowledge bases as latent variables and\nupdating the two task-specific models in a hard Expectation-Maximization (EM)\nfashion: (1) predicting the missing variables for each partially annotated\ndataset based on the current two task-specific models, and (2) updating the\nparameters of each model on the corresponding pseudo completed dataset.\nExperimental results on two benchmark datasets: Genia 2011 for event extraction\nand BC4GO for entity linking, show that our joint framework significantly\nimproves the model for each individual task and outperforms the strong\nbaselines for both tasks. We will make the code and model checkpoints publicly\navailable once the paper is accepted.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaochu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minqian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-review Generation with Checklist-guided Iterative Introspection. (arXiv:2305.14647v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14647","description":"<p>Opinions in the scientific domain can be divergent, leading to controversy or\nconsensus among reviewers. However, current opinion summarization datasets\nmostly focus on product review domains, which do not account for this\nvariability under the assumption that the input opinions are non-controversial.\nTo address this gap, we propose the task of scientific opinion summarization,\nwhere research paper reviews are synthesized into meta-reviews. To facilitate\nthis task, we introduce a new ORSUM dataset covering 10,989 paper meta-reviews\nand 40,903 paper reviews from 39 conferences. Furthermore, we propose the\nChecklist-guided Iterative Introspection (CGI$^2$) approach, which breaks down\nthe task into several stages and iteratively refines the summary under the\nguidance of questions from a checklist. We conclude that (1) human-written\nsummaries are not always reliable since many do not follow the guideline, and\n(2) the combination of task decomposition and iterative self-refinement shows\npromising discussion involvement ability and can be applied to other complex\ntext generation using black-box LLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1\">Qi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidhu_M/0/1/0/all/0/1\">Mankeerat Sidhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1\">Hou Pong Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisit and Outstrip Entity Alignment: A Perspective of Generative Models. (arXiv:2305.14651v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14651","description":"<p>Recent embedding-based methods have achieved great successes on exploiting\nentity alignment from knowledge graph (KG) embeddings of multiple modals. In\nthis paper, we study embedding-based entity alignment (EEA) from a perspective\nof generative models. We show that EEA is a special problem where the main\nobjective is analogous to that in a typical generative model, based on which we\ntheoretically prove the effectiveness of the recently developed generative\nadversarial network (GAN)-based EEA methods. We then reveal that their\nincomplete objective limits the capacity on both entity alignment and entity\nsynthesis (i.e., generating new entities). We mitigate this problem by\nintroducing a generative EEA (abbr., GEEA) framework with the proposed mutual\nvariational autoencoder (M-VAE) as the generative model. M-VAE can convert an\nentity from one KG to another and generate new entities from random noise\nvectors. We demonstrate the power of GEEA with theoretical analysis and\nempirical experiments on both entity alignment and entity synthesis tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Lingbing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion. (arXiv:2305.14652v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14652","description":"<p>Video multimodal fusion aims to integrate multimodal signals in videos, such\nas visual, audio and text, to make a complementary prediction with multiple\nmodalities contents. However, unlike other image-text multimodal tasks, video\nhas longer multimodal sequences with more redundancy and noise in both visual\nand audio modalities. Prior denoising methods like forget gate are coarse in\nthe granularity of noise filtering. They often suppress the redundant and noisy\ninformation at the risk of losing critical information. Therefore, we propose a\ndenoising bottleneck fusion (DBF) model for fine-grained video multimodal\nfusion. On the one hand, we employ a bottleneck mechanism to filter out noise\nand redundancy with a restrained receptive field. On the other hand, we use a\nmutual information maximization module to regulate the filter-out module to\npreserve key information within different modalities. Our DBF model achieves\nsignificant improvement over current state-of-the-art baselines on multiple\nbenchmarks covering multimodal sentiment analysis and multimodal summarization\ntasks. It proves that our model can effectively capture salient features from\nnoisy and redundant video, audio, and text inputs. The code for this paper is\npublicly available at https://github.com/WSXRHFG/DBF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shaoxaing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Damai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Ziwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Binghuai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluate What You Can't Evaluate: Unassessable Generated Responses Quality. (arXiv:2305.14658v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14658","description":"<p>LLMs (large language models) such as ChatGPT have shown remarkable language\nunderstanding and generation capabilities. Although reference-free evaluators\nbased on LLMs show better human alignment than traditional reference-based\nevaluators, there are many challenges in using reference-free evaluators based\non LLMs. Reference-free evaluators are more suitable for open-ended examples\nwith different semantics responses. But not all examples are open-ended. For\nclosed-ended examples with unique correct semantic response, reference-free\nevaluators will still consider it high quality when giving a response that is\ninconsistent with the facts and the semantic of reference. In order to\ncomprehensively evaluate the reliability of evaluators based on LLMs, we\nconstruct two adversarial meta-evaluation dialogue generation datasets\nKdConv-ADV and DSTC7-ADV based on KdConv and DSTC7-AVSD, respectively. Compared\nto previous meta-evaluation benchmarks, KdConv-ADV and DSTC7-ADV are much more\nchallenging since they requires evaluators to be able to reasonably evaluate\nclosed-ended examples with the help of external knowledge or even its own\nknowledge. Empirical results show that the ability of LLMs to identify\nunreasonable responses is insufficient. There are risks in using eference-free\nevaluators based on LLMs to evaluate the quality of dialogue responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongkang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InteractiveIE: Towards Assessing the Strength of Human-AI Collaboration in Improving the Performance of Information Extraction. (arXiv:2305.14659v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14659","description":"<p>Learning template based information extraction from documents is a crucial\nyet difficult task. Prior template-based IE approaches assume foreknowledge of\nthe domain templates; however, real-world IE do not have pre-defined schemas\nand it is a figure-out-as you go phenomena. To quickly bootstrap templates in a\nreal-world setting, we need to induce template slots from documents with zero\nor minimal supervision. Since the purpose of question answering intersect with\nthe goal of information extraction, we use automatic question generation to\ninduce template slots from the documents and investigate how a tiny amount of a\nproxy human-supervision on-the-fly (termed as InteractiveIE) can further boost\nthe performance. Extensive experiments on biomedical and legal documents, where\nobtaining training data is expensive, reveal encouraging trends of performance\nimprovement using InteractiveIE over AI-only baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mondal_I/0/1/0/all/0/1\">Ishani Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Michelle Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+N_A/0/1/0/all/0/1\">Anandhavelu N</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garimella_A/0/1/0/all/0/1\">Aparna Garimella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraro_F/0/1/0/all/0/1\">Francis Ferraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blair_Stanek_A/0/1/0/all/0/1\">Andrew Blair-Stanek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1\">Jordan Boyd-Graber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complex Mathematical Symbol Definition Structures: A Dataset and Model for Coordination Resolution in Definition Extraction. (arXiv:2305.14660v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14660","description":"<p>Mathematical symbol definition extraction is important for improving\nscholarly reading interfaces and scholarly information extraction (IE).\nHowever, the task poses several challenges: math symbols are difficult to\nprocess as they are not composed of natural language morphemes; and scholarly\npapers often contain sentences that require resolving complex coordinate\nstructures. We present SymDef, an English language dataset of 5,927 sentences\nfrom full-text scientific papers where each sentence is annotated with all\nmathematical symbols linked with their corresponding definitions. This dataset\nfocuses specifically on complex coordination structures such as \"respectively\"\nconstructions, which often contain overlapping definition spans. We also\nintroduce a new definition extraction method that masks mathematical symbols,\ncreates a copy of each sentence for each symbol, specifies a target symbol, and\npredicts its corresponding definition spans using slot filling. Our experiments\nshow that our definition extraction model significantly outperforms RoBERTa and\nother strong IE baseline systems by 10.9 points with a macro F1 score of 84.82.\nWith our dataset and model, we can detect complex definitions in scholarly\ndocuments to make scientific writing more readable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martin_Boyle_A/0/1/0/all/0/1\">Anna Martin-Boyle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Head_A/0/1/0/all/0/1\">Andrew Head</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidhu_R/0/1/0/all/0/1\">Risham Sidhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hearst_M/0/1/0/all/0/1\">Marti A. Hearst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Are What You Annotate: Towards Better Models through Annotator Representations. (arXiv:2305.14663v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14663","description":"<p>Annotator disagreement is ubiquitous in natural language processing (NLP)\ntasks. There are multiple reasons for such disagreements, including the\nsubjectivity of the task, difficult cases, unclear guidelines, and so on.\nRather than simply aggregating labels to obtain data annotations, we instead\npropose to explicitly account for the annotator idiosyncrasies and leverage\nthem in the modeling process. We create representations for the annotators\n(annotator embeddings) and their annotations (annotation embeddings) with\nlearnable matrices associated with each. Our approach significantly improves\nmodel performance on various NLP benchmarks by adding fewer than 1% model\nparameters. By capturing the unique tendencies and subjectivity of individual\nannotators, our embeddings help democratize AI and ensure that AI models are\ninclusive of diverse viewpoints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_N/0/1/0/all/0/1\">Naihao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Siyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinliang Frederick Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Winston Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffusion Models in NLP: A Survey. (arXiv:2305.14671v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14671","description":"<p>This survey paper provides a comprehensive review of the use of diffusion\nmodels in natural language processing (NLP). Diffusion models are a class of\nmathematical models that aim to capture the diffusion of information or signals\nacross a network or manifold. In NLP, diffusion models have been used in a\nvariety of applications, such as natural language generation, sentiment\nanalysis, topic modeling, and machine translation. This paper discusses the\ndifferent formulations of diffusion models used in NLP, their strengths and\nlimitations, and their applications. We also perform a thorough comparison\nbetween diffusion models and alternative generative models, specifically\nhighlighting the autoregressive (AR) models, while also examining how diverse\narchitectures incorporate the Transformer in conjunction with diffusion models.\nCompared to AR models, diffusion models have significant advantages for\nparallel generation, text interpolation, token-level controls such as syntactic\nstructures and semantic contents, and robustness. Exploring further\npermutations of integrating Transformers into diffusion models would be a\nvaluable pursuit. Also, the development of multimodal diffusion models and\nlarge-scale diffusion language models with notable capabilities for few-shot\nlearning would be important directions for the future advance of diffusion\nmodels in NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1\">Hao Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Z/0/1/0/all/0/1\">Zae Myung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying Character Similarity with Vision Transformers. (arXiv:2305.14672v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14672","description":"<p>Record linkage is a bedrock of quantitative social science, as analyses often\nrequire linking data from multiple, noisy sources. Off-the-shelf string\nmatching methods are widely used, as they are straightforward and cheap to\nimplement and scale. Not all character substitutions are equally probable, and\nfor some settings there are widely used handcrafted lists denoting which string\nsubstitutions are more likely, that improve the accuracy of string matching.\nHowever, such lists do not exist for many settings, skewing research with\nlinked datasets towards a few high-resource contexts that are not\nrepresentative of the diversity of human societies. This study develops an\nextensible way to measure character substitution costs for OCR'ed documents, by\nemploying large-scale self-supervised training of vision transformers (ViT)\nwith augmented digital fonts. For each language written with the CJK script, we\ncontrastively learn a metric space where different augmentations of the same\ncharacter are represented nearby. In this space, homoglyphic characters - those\nwith similar appearance such as ``O'' and ``0'' - have similar vector\nrepresentations. Using the cosine distance between characters' representations\nas the substitution cost in an edit distance matching algorithm significantly\nimproves record linkage compared to other widely used string matching methods,\nas OCR errors tend to be homoglyphic in nature. Homoglyphs can plausibly\ncapture character visual similarity across any script, including low-resource\nsettings. We illustrate this by creating homoglyph sets for 3,000 year old\nancient Chinese characters, which are highly pictorial. Fascinatingly, a ViT is\nable to capture relationships in how different abstract concepts were\nconceptualized by ancient societies, that have been noted in the archaeological\nliterature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinmei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Abhishek Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jheng_S/0/1/0/all/0/1\">Shao-Yu Jheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dell_M/0/1/0/all/0/1\">Melissa Dell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GRILL: Grounded Vision-language Pre-training via Aligning Text and Image Regions. (arXiv:2305.14676v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14676","description":"<p>Generalization to unseen tasks is an important ability for few-shot learners\nto achieve better zero-/few-shot performance on diverse tasks. However, such\ngeneralization to vision-language tasks including grounding and generation\ntasks has been under-explored; existing few-shot VL models struggle to handle\ntasks that involve object grounding and multiple images such as visual\ncommonsense reasoning or NLVR2. In this paper, we introduce GRILL, GRounded\nvIsion Language aLigning, a novel VL model that can be generalized to diverse\ntasks including visual question answering, captioning, and grounding tasks with\nno or very few training instances. Specifically, GRILL learns object grounding\nand localization by exploiting object-text alignments, which enables it to\ntransfer to grounding tasks in a zero-/few-shot fashion. We evaluate our model\non various zero-/few-shot VL tasks and show that it consistently surpasses the\nstate-of-the-art few-shot methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Woojeong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_D/0/1/0/all/0/1\">Damien Jose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent inabilities? Inverse scaling over the course of pretraining. (arXiv:2305.14681v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14681","description":"<p>Does inverse scaling only occur as a function of model parameter size, or can\nit also occur over the course of training? We carry out an exploratory study\ninvestigating whether, over the course of training on the language modeling\ntask, the performance of language models at specific tasks can decrease while\ngeneral performance remains high. We find that for two tasks from the Inverse\nScaling Challenge - quote-repetition and redefine-math - this is indeed the\ncase. Specifically, we find that for Pythia (Biderman et al., 2023) models with\na higher number of parameters, performance decreases over the course of\ntraining at these two tasks, despite these models showing standard (positive)\nscaling overall. This highlights the importance of testing model performance at\nall relevant benchmarks any time they are trained on additional data, even if\ntheir overall performance improves.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michaelov_J/0/1/0/all/0/1\">James A. Michaelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1\">Benjamin K. Bergen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TACR: A Table-alignment-based Cell-selection and Reasoning Model for Hybrid Question-Answering. (arXiv:2305.14682v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14682","description":"<p>Hybrid Question-Answering (HQA), which targets reasoning over tables and\npassages linked from table cells, has witnessed significant research in recent\nyears. A common challenge in HQA and other passage-table QA datasets is that it\nis generally unrealistic to iterate over all table rows, columns, and linked\npassages to retrieve evidence. Such a challenge made it difficult for previous\nstudies to show their reasoning ability in retrieving answers. To bridge this\ngap, we propose a novel Table-alignment-based Cell-selection and Reasoning\nmodel (TACR) for hybrid text and table QA, evaluated on the HybridQA and\nWikiTableQuestions datasets. In evidence retrieval, we design a\ntable-question-alignment enhanced cell-selection method to retrieve\nfine-grained evidence. In answer reasoning, we incorporate a QA module that\ntreats the row containing selected cells as context. Experimental results over\nthe HybridQA and WikiTableQuestions (WTQ) datasets show that TACR achieves\nstate-of-the-art results on cell selection and outperforms fine-grained\nevidence retrieval baselines on HybridQA, while achieving competitive\nperformance on WTQ. We also conducted a detailed analysis to demonstrate that\nbeing able to align questions to tables in the cell-selection stage can result\nin important gains from experiments of over 90\\% table row and column selection\naccuracy, meanwhile also improving output explainability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yicheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_B/0/1/0/all/0/1\">B&#xf6;rje F. Karlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okumura_M/0/1/0/all/0/1\">Manabu Okumura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ExpertPrompting: Instructing Large Language Models to be Distinguished Experts. (arXiv:2305.14688v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14688","description":"<p>The answering quality of an aligned large language model (LLM) can be\ndrastically improved if treated with proper crafting of prompts. In this paper,\nwe propose ExpertPrompting to elicit the potential of LLMs to answer as\ndistinguished experts. We first utilize In-Context Learning to automatically\nsynthesize detailed and customized descriptions of the expert identity for each\nspecific instruction, and then ask LLMs to provide answer conditioned on such\nagent background. Based on this augmented prompting strategy, we produce a new\nset of instruction-following data using GPT-3.5, and train a competitive\nopen-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation\nto show that 1) the expert data is of significantly higher quality than vanilla\nanswers, and 2) ExpertLLaMA outperforms existing open-source opponents and\nachieves 96\\% of the original ChatGPT's capability. All data and the\nExpertLLaMA model will be made publicly available at\n\\url{https://github.com/OFA-Sys/ExpertLLaMA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Benfeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">An Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhendong Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Have Large Language Models Developed a Personality?: Applicability of Self-Assessment Tests in Measuring Personality in LLMs. (arXiv:2305.14693v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14693","description":"<p>Have Large Language Models (LLMs) developed a personality? The short answer\nis a resounding \"We Don't Know!\". In this paper, we show that we do not yet\nhave the right tools to measure personality in language models. Personality is\nan important characteristic that influences behavior. As LLMs emulate\nhuman-like intelligence and performance in various tasks, a natural question to\nask is whether these models have developed a personality. Previous works have\nevaluated machine personality through self-assessment personality tests, which\nare a set of multiple-choice questions created to evaluate personality in\nhumans. A fundamental assumption here is that human personality tests can\naccurately measure personality in machines. In this paper, we investigate the\nemergence of personality in five LLMs of different sizes ranging from 1.5B to\n30B. We propose the Option-Order Symmetry property as a necessary condition for\nthe reliability of these self-assessment tests. Under this condition, the\nanswer to self-assessment questions is invariant to the order in which the\noptions are presented. We find that many LLMs personality test responses do not\npreserve option-order symmetry. We take a deeper look at LLMs test responses\nwhere option-order symmetry is preserved to find that in these cases, LLMs do\nnot take into account the situational statement being tested and produce the\nexact same answer irrespective of the situation being tested. We also identify\nthe existence of inherent biases in these LLMs which is the root cause of the\naforementioned phenomenon and makes self-assessment tests unreliable. These\nobservations indicate that self-assessment tests are not the correct tools to\nmeasure personality in LLMs. Through this paper, we hope to draw attention to\nthe shortcomings of current literature in measuring personality in LLMs and\ncall for developing tools for machine personality measurement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiaoyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akshat Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohebbizadeh_K/0/1/0/all/0/1\">Kiyan Mohebbizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shujie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Anant Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Causal View of Entity Bias in (Large) Language Models. (arXiv:2305.14695v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14695","description":"<p>Entity bias widely affects pretrained (large) language models, causing them\nto excessively rely on (biased) parametric knowledge to make unfaithful\npredictions. Although causality-inspired methods have shown great potential to\nmitigate entity bias, it is hard to precisely estimate the parameters of\nunderlying causal models in practice. The rise of black-box LLMs also makes the\nsituation even worse, because of their inaccessible parameters and uncalibrated\nlogits. To address these problems, we propose a specific structured causal\nmodel (SCM) whose parameters are comparatively easier to estimate. Building\nupon this SCM, we propose causal intervention techniques to mitigate entity\nbias for both white-box and black-box settings. The proposed causal\nintervention perturbs the original entity with neighboring entities. This\nintervention reduces specific biasing information pertaining to the original\nentity while still preserving sufficient common predictive information from\nsimilar entities. When evaluated on the relation extraction task, our\ntraining-time intervention significantly improves the F1 score of RoBERTa by\n5.7 points on EntRED, in which spurious shortcuts between entities and labels\nare removed. Meanwhile, our in-context intervention effectively reduces the\nknowledge conflicts between parametric knowledge and contextual knowledge in\nGPT-3.5 and improves the F1 score by 9.14 points on a challenging test set\nderived from Re-TACRED.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_W/0/1/0/all/0/1\">Wenjie Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SELFOOD: Self-Supervised Out-Of-Distribution Detection via Learning to Rank. (arXiv:2305.14696v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14696","description":"<p>Deep neural classifiers trained with cross-entropy loss (CE loss) often\nsuffer from poor calibration, necessitating the task of out-of-distribution\n(OOD) detection. Traditional supervised OOD detection methods require expensive\nmanual annotation of in-distribution and OOD samples. To address the annotation\nbottleneck, we introduce SELFOOD, a self-supervised OOD detection method that\nrequires only in-distribution samples as supervision. We cast OOD detection as\nan inter-document intra-label (IDIL) ranking problem and train the classifier\nwith our pairwise ranking loss, referred to as IDIL loss. Specifically, given a\nset of in-distribution documents and their labels, for each label, we train the\nclassifier to rank the softmax scores of documents belonging to that label to\nbe higher than the scores of documents that belong to other labels. Unlike CE\nloss, our IDIL loss function reaches zero when the desired confidence ranking\nis achieved and gradients are backpropagated to decrease probabilities\nassociated with incorrect labels rather than continuously increasing the\nprobability of the correct label. Extensive experiments with several\nclassifiers on multiple classification datasets demonstrate the effectiveness\nof our method in both coarse- and fine-grained settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mekala_D/0/1/0/all/0/1\">Dheeraj Mekala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samavedhi_A/0/1/0/all/0/1\">Adithya Samavedhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chengyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling rapid language learning by distilling Bayesian priors into artificial neural networks. (arXiv:2305.14701v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14701","description":"<p>Humans can learn languages from remarkably little experience. Developing\ncomputational models that explain this ability has been a major challenge in\ncognitive science. Bayesian models that build in strong inductive biases -\nfactors that guide generalization - have been successful at explaining how\nhumans might generalize from few examples in controlled settings but are\nusually too restrictive to be tractably applied to more naturalistic data. By\ncontrast, neural networks have flexible representations that allow them to\nlearn well from naturalistic data but require many more examples than humans\nreceive. We show that learning from limited naturalistic data is possible with\nan approach that combines the strong inductive biases of a Bayesian model with\nthe flexible representations of a neural network. This approach works by\ndistilling a Bayesian model's biases into a neural network. Like a Bayesian\nmodel, the resulting system can learn formal linguistic patterns from a small\nnumber of examples. Like a neural network, it can also learn aspects of English\nsyntax from a corpus of natural language - and it outperforms a standard neural\nnetwork at acquiring the linguistic phenomena of recursion and priming.\nBridging the divide between Bayesian models and neural networks makes it\npossible to handle a broader range of learning scenarios than either approach\ncan handle on its own.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McCoy_R/0/1/0/all/0/1\">R. Thomas McCoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Influential Factors in Human Preference Judgments via GPT-4. (arXiv:2305.14702v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14702","description":"<p>Pairwise human judgments are pivotal in guiding large language models (LLMs)\nto generate outputs that align with human preferences. They are also often used\nin summarization evaluation, complementing existing automatic metrics. Despite\ntheir significance, however, there has been limited research probing these\npairwise human judgments. The collective impact and respective weights of\nfactors such as informativeness, coherence, fluency, and factual consistency\nremain elusive. The impact of hidden factors on the final judgment is also\nunclear. In this paper, we conduct an in-depth examination of a dataset of\npairwise human judgments released by OpenAI. Utilizing the Bradley-Terry-Luce\nmodel, we identify key factors that could potentially influence human\njudgments. Our research uncovers the inherent preferences embedded in human\njudgments and suggests strategies to boost sample efficiency. Finally, we\nprovide insights on the construction of balanced datasets for human judgment\nevaluations, a crucial step in shaping the behaviors of future LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yebowen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaiqiang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sangwoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foroosh_H/0/1/0/all/0/1\">Hassan Foroosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse Mixture of Experts. (arXiv:2305.14705v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14705","description":"<p>The explosive growth of language models and their applications have led to an\nincreased demand for efficient and scalable methods. In this paper, we\nintroduce Flan-MoE, a set of Instruction-Finetuned Sparse Mixture-of-Expert\n(MoE) models. We show that naively finetuning MoE models on a task-specific\ndataset (in other words, no instruction-finetuning) often yield worse\nperformance compared to dense models of the same computational complexity.\nHowever, our Flan-MoE outperforms dense models under multiple experiment\nsettings: instruction-finetuning only and instruction-finetuning followed by\ntask-specific finetuning. This shows that instruction-finetuning is an\nessential stage for MoE models. Specifically, our largest model, Flan-MoE-32B,\nsurpasses the performance of Flan-PaLM-62B on four benchmarks, while utilizing\nonly one-third of the FLOPs. The success of Flan-MoE encourages rethinking the\ndesign of large-scale, high-performance language models, under the setting of\ntask-agnostic learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Le Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yanqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1\">Nan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1\">Barret Zoph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedus_W/0/1/0/all/0/1\">William Fedus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tu Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuexin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wuyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1\">Albert Webson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_V/0/1/0/all/0/1\">Vincent Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The student becomes the master: Matching GPT3 on Scientific Factual Error Correction. (arXiv:2305.14707v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14707","description":"<p>Due to the prohibitively high cost of creating error correction datasets,\nmost Factual Claim Correction methods rely on a powerful verification model to\nguide the correction process. This leads to a significant drop in performance\nin domains like Scientific Claim Correction, where good verification models do\nnot always exist. In this work, we introduce a claim correction system that\nmakes no domain assumptions and does not require a verifier but is able to\noutperform existing methods by an order of magnitude -- achieving 94%\ncorrection accuracy on the SciFact dataset, and 62.5% on the SciFact-Open\ndataset, compared to the next best methods 0.5% and 1.50% respectively. Our\nmethod leverages the power of prompting with LLMs during training to create a\nrichly annotated dataset that can be used for fully supervised training and\nregularization. We additionally use a claim-aware decoding procedure to improve\nthe quality of corrected claims. Our method is competitive with the very LLM\nthat was used to generate the annotated dataset -- with GPT3.5 achieving 89.5%\nand 60% correction accuracy on SciFact and SciFact-Open, despite using 1250\ntimes as many parameters as our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ashok_D/0/1/0/all/0/1\">Dhananjay Ashok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1\">Atharva Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1\">Hai Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poczos_B/0/1/0/all/0/1\">Barnab&#xe1;s P&#xf3;czos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models. (arXiv:2305.14710v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14710","description":"<p>Instruction-tuned models are trained on crowdsourcing datasets with task\ninstructions to achieve superior performance. However, in this work we raise\nsecurity concerns about this training paradigm. Our studies demonstrate that an\nattacker can inject backdoors by issuing very few malicious instructions among\nthousands of gathered data and control model behavior through data poisoning,\nwithout even the need of modifying data instances or labels themselves. Through\nsuch instruction attacks, the attacker can achieve over 90% attack success rate\nacross four commonly used NLP datasets, and cause persistent backdoors that are\neasily transferred to 15 diverse datasets zero-shot. In this way, the attacker\ncan directly apply poisoned instructions designed for one dataset on many other\ndatasets. Moreover, the poisoned model cannot be cured by continual learning.\nLastly, instruction attacks show resistance to existing inference-time defense.\nThese findings highlight the need for more robust defenses against data\npoisoning attacks in instructiontuning models and underscore the importance of\nensuring data quality in instruction crowdsourcing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiashu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingyu Derek Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gender Biases in Automatic Evaluation Metrics: A Case Study on Image Captioning. (arXiv:2305.14711v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14711","description":"<p>Pretrained model-based evaluation metrics have demonstrated strong\nperformance with high correlations with human judgments in various natural\nlanguage generation tasks such as image captioning. Despite the impressive\nresults, their impact on fairness is under-explored -- it is widely\nacknowledged that pretrained models can encode societal biases, and utilizing\nthem for evaluation purposes may inadvertently manifest and potentially amplify\nbiases. In this paper, we conduct a systematic study in gender biases of\nmodel-based evaluation metrics with a focus on image captioning tasks.\nSpecifically, we first identify and quantify gender biases in different\nevaluation metrics regarding profession, activity, and object concepts. Then,\nwe demonstrate the negative consequences of using these biased metrics, such as\nfavoring biased generation models in deployment and propagating the biases to\ngeneration models through reinforcement learning. We also present a simple but\neffective alternative to reduce gender biases by combining n-gram\nmatching-based and pretrained model-based evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Haoyi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zi-Yi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianlu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GlobalBench: A Benchmark for Global Progress in Natural Language Processing. (arXiv:2305.14716v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14716","description":"<p>Despite the major advances in NLP, significant disparities in NLP system\nperformance across languages still exist. Arguably, these are due to uneven\nresource allocation and sub-optimal incentives to work on less resourced\nlanguages. To track and further incentivize the global development of equitable\nlanguage technology, we introduce GlobalBench. Prior multilingual benchmarks\nare static and have focused on a limited number of tasks and languages. In\ncontrast, GlobalBench is an ever-expanding collection that aims to dynamically\ntrack progress on all NLP datasets in all languages. Rather than solely\nmeasuring accuracy, GlobalBench also tracks the estimated per-speaker utility\nand equity of technology across all languages, providing a multi-faceted view\nof how language technology is serving people of the world. Furthermore,\nGlobalBench is designed to identify the most under-served languages, and\nrewards research efforts directed towards those languages. At present, the most\nunder-served languages are the ones with a relatively high population, but\nnonetheless overlooked by composite multilingual benchmarks (like Punjabi,\nPortuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190\nlanguages, and has 1,128 system submissions spanning 62 languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yueqi Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Catherine Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanuja_S/0/1/0/all/0/1\">Simran Khanuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faisal_F/0/1/0/all/0/1\">Fahim Faisal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostapenko_A/0/1/0/all/0/1\">Alissa Ostapenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Correlations Between Contexts and Definitions with Multiple Definition Modeling. (arXiv:2305.14717v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14717","description":"<p>Definition modeling is an important task in advanced natural language\napplications such as understanding and conversation. Since its introduction, it\nfocus on generating one definition for a target word or phrase in a given\ncontext, which we refer to as Single Definition Modeling (SDM). However, this\napproach does not adequately model the correlations and patterns among\ndifferent contexts and definitions of words. In addition, the creation of a\ntraining dataset for SDM requires significant human expertise and effort. In\nthis paper, we carefully design a new task called Multiple Definition Modeling\n(MDM) that pool together all contexts and definition of target words. We\ndemonstrate the ease of creating a model as well as multiple training sets\nautomatically. % In the experiments, we demonstrate and analyze the benefits of\nMDM, including improving SDM's performance by using MDM as the pretraining task\nand its comparable performance in the zero-shot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xin Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Language Models with Advantage-based Offline Policy Gradients. (arXiv:2305.14718v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14718","description":"<p>Improving language model generations according to some user-defined quality\nor style constraints is challenging. Typical approaches include learning on\nadditional human-written data, filtering ``low-quality'' data using heuristics\nand/or using reinforcement learning with human feedback (RLHF). However,\nfiltering can remove valuable training signals, whereas data collection and\nRLHF constantly require additional human-written or LM exploration data which\ncan be costly to obtain. A natural question to ask is ``Can we leverage RL to\noptimize LM utility on existing crowd-sourced and internet data?''\n</p>\n<p>To this end, we present Left-over Lunch RL (LoL-RL), a simple training\nalgorithm that uses offline policy gradients for learning language generation\ntasks as a 1-step RL game. LoL-RL can finetune LMs to optimize arbitrary\nclassifier-based or human-defined utility functions on any sequence-to-sequence\ndata. Experiments with five different language generation tasks using models of\nvarying sizes and multiple rewards show that models trained with LoL-RL can\nconsistently outperform the best supervised learning models. We also release\nour experimental code. https://github.com/abaheti95/LoL-RL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baheti_A/0/1/0/all/0/1\">Ashutosh Baheti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark Riedl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CuRIAM: Corpus re Interpretation and Metalanguage in U.S. Supreme Court Opinions. (arXiv:2305.14719v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14719","description":"<p>Most judicial decisions involve the interpretation of legal texts; as such,\njudicial opinion requires the use of language as a medium to comment on or draw\nattention to other language. Language used this way is called metalanguage. We\ndevelop an annotation schema for categorizing types of legal metalanguage and\napply our schema to a set of U.S. Supreme Court opinions, yielding a corpus\ntotaling 59k tokens. We remark on several patterns observed in the kinds of\nmetalanguage used by the justices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kranzlein_M/0/1/0/all/0/1\">Michael Kranzlein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nathan Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tobia_K/0/1/0/all/0/1\">Kevin Tobia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors. (arXiv:2305.14724v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14724","description":"<p>Visual metaphors are powerful rhetorical devices used to persuade or\ncommunicate creative ideas through images. Similar to linguistic metaphors,\nthey convey meaning implicitly through symbolism and juxtaposition of the\nsymbols. We propose a new task of generating visual metaphors from linguistic\nmetaphors. This is a challenging task for diffusion-based text-to-image models,\nsuch as DALL$\\cdot$E 2, since it requires the ability to model implicit meaning\nand compositionality. We propose to solve the task through the collaboration\nbetween Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3\n(davinci-002) with Chain-of-Thought prompting generates text that represents a\nvisual elaboration of the linguistic metaphor containing the implicit meaning\nand relevant objects, which is then used as input to the diffusion-based\ntext-to-image models.Using a human-AI collaboration framework, where humans\ninteract both with the LLM and the top-performing diffusion model, we create a\nhigh-quality dataset containing 6,476 visual metaphors for 1,540 linguistic\nmetaphors and their associated visual elaborations. Evaluation by professional\nillustrators shows the promise of LLM-Diffusion Model collaboration for this\ntask.To evaluate the utility of our Human-AI collaboration framework and the\nquality of our dataset, we perform both an intrinsic human-based evaluation and\nan extrinsic evaluation using visual entailment as a downstream task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saakyan_A/0/1/0/all/0/1\">Arkadiy Saakyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winn_O/0/1/0/all/0/1\">Olivia Winn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panagopoulou_A/0/1/0/all/0/1\">Artemis Panagopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apidianaki_M/0/1/0/all/0/1\">Marianna Apidianaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes. (arXiv:2305.14725v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14725","description":"<p>We propose attribute-aware multimodal entity linking, where the input is a\nmention described with a text and image, and the goal is to predict the\ncorresponding target entity from a multimodal knowledge base (KB) where each\nentity is also described with a text description, a visual image and a set of\nattributes and values. To support this research, we construct AMELI, a\nlarge-scale dataset consisting of 18,472 reviews and 35,598 products. To\nestablish baseline performance on AMELI, we experiment with the current\nstate-of-the-art multimodal entity linking approaches and our enhanced\nattribute-aware model and demonstrate the importance of incorporating the\nattribute information into the entity linking process. To be best of our\nknowledge, we are the first to build benchmark dataset and solutions for the\nattribute-aware multimodal entity linking task. Datasets and codes will be made\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Barry Menglong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sijia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minqian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Licheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-Context Demonstration Selection with Cross Entropy Difference. (arXiv:2305.14726v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14726","description":"<p>Large language models (LLMs) can use in-context demonstrations to improve\nperformance on zero-shot tasks. However, selecting the best in-context examples\nis challenging because model performance can vary widely depending on the\nselected examples. We present a cross-entropy difference (CED) method for\nselecting in-context demonstrations. Our method is based on the observation\nthat the effectiveness of in-context demonstrations negatively correlates with\nthe perplexity of the test example by a language model that was finetuned on\nthat demonstration. We utilize parameter efficient finetuning to train small\nmodels on training data that are used for computing the cross-entropy\ndifference between a test example and every candidate in-context demonstration.\nThis metric is used to rank and select in-context demonstrations independently\nfor each test input. We evaluate our method on a mix-domain dataset that\ncombines 8 benchmarks, representing 4 text generation tasks, showing that CED\nfor in-context demonstration selection can improve performance for a variety of\nLLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iter_D/0/1/0/all/0/1\">Dan Iter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1\">Reid Pryzant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruochen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations. (arXiv:2305.14728v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14728","description":"<p>Although deep language representations have become the dominant form of\nlanguage featurization in recent years, in many settings it is important to\nunderstand a model's decision-making process. This necessitates not only an\ninterpretable model but also interpretable features. In particular, language\nmust be featurized in a way that is interpretable while still characterizing\nthe original text well. We present SenteCon, a method for introducing human\ninterpretability in deep language representations. Given a passage of text,\nSenteCon encodes the text as a layer of interpretable categories in which each\ndimension corresponds to the relevance of a specific category. Our empirical\nevaluations indicate that encoding language with SenteCon provides high-level\ninterpretability at little to no cost to predictive performance on downstream\ntasks. Moreover, we find that SenteCon outperforms existing interpretable\nlanguage representations with respect to both its downstream performance and\nits agreement with human characterizations of the text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_V/0/1/0/all/0/1\">Victoria Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advancements in Arabic Grammatical Error Detection and Correction: An Empirical Investigation. (arXiv:2305.14734v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14734","description":"<p>Grammatical error correction (GEC) is a well-explored problem in English with\nmany existing models and datasets. However, research on GEC in morphologically\nrich languages has been limited due to challenges such as data scarcity and\nlanguage complexity. In this paper, we present the first results on Arabic GEC\nby using two newly developed Transformer-based pretrained sequence-to-sequence\nmodels. We address the task of multi-class Arabic grammatical error detection\n(GED) and present the first results on multi-class Arabic GED. We show that\nusing GED information as auxiliary input in GEC models improves GEC performance\nacross three datasets spanning different genres. Moreover, we also investigate\nthe use of contextual morphological preprocessing in aiding GEC systems. Our\nmodels achieve state-of-the-art results on two Arabic GEC shared tasks datasets\nand establish a strong benchmark on a newly created dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alhafni_B/0/1/0/all/0/1\">Bashar Alhafni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inoue_G/0/1/0/all/0/1\">Go Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khairallah_C/0/1/0/all/0/1\">Christian Khairallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection. (arXiv:2305.14735v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14735","description":"<p>A standard method for measuring the impacts of AI on marginalized communities\nis to determine performance discrepancies between specified demographic groups.\nThese approaches aim to address harms toward vulnerable groups, but they\nobscure harm patterns faced by intersectional subgroups or shared across\ndemographic groups. We instead operationalize \"the margins\" as data points that\nare statistical outliers due to having demographic attributes distant from the\n\"norm\" and measure harms toward these outliers. We propose a Group-Based\nPerformance Disparity Index (GPDI) that measures the extent to which a\nsubdivision of a dataset into subgroups identifies those facing increased\nharms. We apply our approach to detecting disparities in toxicity detection and\nfind that text targeting outliers is 28% to 86% more toxic for all types of\ntoxicity examined. We also discover that model performance is consistently\nworse for demographic outliers, with disparities in error between outliers and\nnon-outliers ranging from 28% to 71% across toxicity types. Our outlier-based\nanalysis has comparable or higher GPDI than traditional subgroup-based\nanalyses, suggesting that outlier analysis enhances identification of subgroups\nfacing greater harms. Finally, we find that minoritized racial and religious\ngroups are most associated with outliers, which suggests that outlier analysis\nis particularly beneficial for identifying harms against those groups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raman_V/0/1/0/all/0/1\">Vyoma Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleisig_E/0/1/0/all/0/1\">Eve Fleisig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trusting Your Evidence: Hallucinate Less with Context-aware Decoding. (arXiv:2305.14739v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14739","description":"<p>Language models (LMs) often struggle to pay enough attention to the input\ncontext, and generate texts that are unfaithful or contain hallucinations. To\nmitigate this issue, we present context-aware decoding (CAD), which follows a\ncontrastive output distribution that amplifies the difference between the\noutput probabilities when a model is used with and without context. Our\nexperiments show that CAD, without additional training, significantly improves\nthe faithfulness of different LM families, including OPT, GPT, LLaMA and\nFLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality\nmetrics). Furthermore, CAD is particularly effective in overriding a model's\nprior knowledge when it contradicts the provided context, leading to\nsubstantial improvements in tasks where resolving the knowledge conflict is\nessential.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weijia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaochuang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_S/0/1/0/all/0/1\">Scott Wen-tau Yih</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ECHo: Event Causality Inference via Human-centric Reasoning. (arXiv:2305.14740v1 [cs.AI])","link":"http://arxiv.org/abs/2305.14740","description":"<p>We introduce ECHo, a diagnostic dataset of event causality inference grounded\nin visual-and-linguistic social scenarios. ECHo employs real-world\nhuman-centric deductive information collected from crime drama, bridging the\ngap in multimodal reasoning towards higher social intelligence through the\nelicitation of intermediate Theory-of-Mind (ToM). We propose a unified\nframework aligned with the Chain-of-Thought (CoT) paradigm to assess the\nreasoning capability of current AI systems. This ToM-enhanced CoT pipeline can\naccommodate and integrate various large foundation models in zero-shot\nvisual-and-linguistic understanding. With this framework, we scrutinize the\nadvanced large language and multimodal models via three complementary\nhuman-centric ECHo tasks. Further analysis demonstrates ECHo as a challenging\ndataset to expose imperfections and inconsistencies in reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanzhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Min-Yen Kan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation. (arXiv:2305.14750v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14750","description":"<p>When answering complex questions, large language models (LLMs) may produce\nanswers that do not satisfy all criteria of the question. While existing\nself-evaluation techniques aim to detect if such answers are correct, these\ntechniques are unable to determine which criteria of the question are satisfied\nby the generated answers. To address this issue, we propose answer-based claim\ndecomposition (ABCD), a prompting strategy that decomposes questions into a\nseries of true/false claims that can be used to verify which criteria of the\ninput question an answer satisfies. Using the decomposed ABCD claims, we\nperform fine-grained self-evaluation. Through preliminary experiments on three\ndatasets, including a newly-collected challenge dataset ObscureQA, we find that\nGPT-3.5 has some ability to determine to what extent its answer satisfies the\ncriteria of the input question, and can give insights into the errors and\nknowledge gaps of the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balepur_N/0/1/0/all/0/1\">Nishant Balepur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moorjani_S/0/1/0/all/0/1\">Samraj Moorjani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaram_H/0/1/0/all/0/1\">Hari Sundaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialogVCS: Robust Natural Language Understanding in Dialogue System Upgrade. (arXiv:2305.14751v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14751","description":"<p>In the constant updates of the product dialogue systems, we need to retrain\nthe natural language understanding (NLU) model as new data from the real users\nwould be merged into the existent data accumulated in the last updates. Within\nthe newly added data, new intents would emerge and might have semantic\nentanglement with the existing intents, e.g. new intents that are semantically\ntoo specific or generic are actually subset or superset of some existing\nintents in the semantic space, thus impairing the robustness of the NLU model.\nAs the first attempt to solve this problem, we setup a new benchmark consisting\nof 4 Dialogue Version Control dataSets (DialogVCS). We formulate the intent\ndetection with imperfect data in the system update as a multi-label\nclassification task with positive but unlabeled intents, which asks the models\nto recognize all the proper intents, including the ones with semantic\nentanglement, in the inference. We also propose comprehensive baseline models\nand conduct in-depth analyses for the benchmark, showing that the semantically\nentangled intents can be effectively recognized with an automatic workflow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zefan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Haoran Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiaqi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Gang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Binghuai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Take This Out of Context! On the Need for Contextual Models and Evaluations for Stylistic Rewriting. (arXiv:2305.14755v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14755","description":"<p>Most existing stylistic text rewriting methods operate on a sentence level,\nbut ignoring the broader context of the text can lead to generic, ambiguous,\nand incoherent rewrites. In this paper, we propose the integration of preceding\ntextual context into both the rewriting and evaluation stages of stylistic text\nrewriting, focusing on formality, toxicity, and sentiment transfer tasks. We\nconduct a comparative evaluation of rewriting through few-shot prompting of\nGPT-3.5 and GPT NeoX, comparing non-contextual rewrites to contextual rewrites.\nOur experiments show that humans often prefer contextual rewrites over\nnon-contextual ones, but automatic metrics (e.g., BLEU, sBERT) do not. To\nbridge this gap, we propose context-infused versions of common automatic\nmetrics, and show that these better reflect human preferences. Overall, our\npaper highlights the importance of integrating preceding textual context into\nboth the rewriting and evaluation stages of stylistic text rewriting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yerukola_A/0/1/0/all/0/1\">Akhila Yerukola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xuhui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human-Centered Metrics for Dialog System Evaluation. (arXiv:2305.14757v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14757","description":"<p>We present metrics for evaluating dialog systems through a\npsychologically-grounded \"human\" lens: conversational agents express a\ndiversity of both states (short-term factors like emotions) and traits\n(longer-term factors like personality) just as people do. These interpretable\nmetrics consist of five measures from established psychology constructs that\ncan be applied both across dialogs and on turns within dialogs: emotional\nentropy, linguistic style and emotion matching, as well as agreeableness and\nempathy. We compare these human metrics against 6 state-of-the-art automatic\nmetrics (e.g. BARTScore and BLEURT) on 7 standard dialog system data sets. We\nalso introduce a novel data set, the Three Bot Dialog Evaluation Corpus, which\nconsists of annotated conversations from ChatGPT, GPT-3, and BlenderBot. We\ndemonstrate the proposed human metrics offer novel information, are\nuncorrelated with automatic metrics, and lead to increased accuracy beyond\nexisting automatic metrics for predicting crowd-sourced dialog judgements. The\ninterpretability and unique signal of our proposed human-centered framework\nmake it a valuable tool for evaluating and improving dialog systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giorgi_S/0/1/0/all/0/1\">Salvatore Giorgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Havaldar_S/0/1/0/all/0/1\">Shreya Havaldar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1\">Farhan Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_Z/0/1/0/all/0/1\">Zuhaib Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaidya_S/0/1/0/all/0/1\">Shalaka Vaidya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_G/0/1/0/all/0/1\">Gary Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1\">Lyle H. Ungar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_H/0/1/0/all/0/1\">H. Andrew Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Joao Sedoc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bi-Drop: Generalizable Fine-tuning for Pre-trained Language Models via Adaptive Subnetwork Optimization. (arXiv:2305.14760v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14760","description":"<p>Pretrained language models have achieved remarkable success in a variety of\nnatural language understanding tasks. Nevertheless, finetuning large pretrained\nmodels on downstream tasks is susceptible to overfitting if the training set is\nlimited, which will lead to diminished performance. In this work, we propose a\ndynamic fine-tuning strategy for pretrained language models called Bi-Drop. It\nutilizes the gradient information of various sub-models generated by dropout to\nupdate the model parameters selectively. Experiments on the GLUE benchmark show\nthat Bi-Drop outperforms previous fine-tuning methods by a considerable margin,\nand exhibits consistent superiority over vanilla fine-tuning across various\npretrained models. Furthermore, empirical results indicate that Bi-Drop yields\nsubstantial improvements in the multiple task or domain transfer, data\nimbalance, and low-resource scenarios, demonstrating superb generalization\nability and robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1\">Shoujie Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1\">Heming Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Damai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Binghuai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning. (arXiv:2305.14761v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14761","description":"<p>Charts are very popular for analyzing data, visualizing key insights and\nanswering complex reasoning questions about data. To facilitate chart-based\ndata analysis using natural language, several downstream tasks have been\nintroduced recently such as chart question answering and chart summarization.\nHowever, most of the methods that solve these tasks use pretraining on language\nor vision-language tasks that do not attempt to explicitly model the structure\nof the charts (e.g., how data is visually encoded and how chart elements are\nrelated to each other). To address this, we first build a large corpus of\ncharts covering a wide variety of topics and visual styles. We then present\nUniChart, a pretrained model for chart comprehension and reasoning. UniChart\nencodes the relevant text, data, and visual elements of charts and then uses a\nchart-grounded text decoder to generate the expected output in natural\nlanguage. We propose several chart-specific pretraining tasks that include: (i)\nlow-level tasks to extract the visual elements (e.g., bars, lines) and data\nfrom charts, and (ii) high-level tasks to acquire chart understanding and\nreasoning skills. We find that pretraining the model on a large corpus with\nchart-specific low- and high-level tasks followed by finetuning on three\ndown-streaming tasks results in state-of-the-art performance on three\ndownstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Masry_A/0/1/0/all/0/1\">Ahmed Masry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavehzadeh_P/0/1/0/all/0/1\">Parsa Kavehzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_X/0/1/0/all/0/1\">Xuan Long Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoque_E/0/1/0/all/0/1\">Enamul Hoque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models. (arXiv:2305.14763v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14763","description":"<p>The escalating debate on AI's capabilities warrants developing reliable\nmetrics to assess machine \"intelligence\". Recently, many anecdotal examples\nwere used to suggest that newer large language models (LLMs) like ChatGPT and\nGPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached\nconflicting conclusions regarding those abilities. We investigate the extent of\nLLMs' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs\nexhibit certain N-ToM abilities, this behavior is far from being robust. We\nfurther examine the factors impacting performance on N-ToM tasks and discover\nthat LLMs struggle with adversarial examples, indicating reliance on shallow\nheuristics rather than robust ToM abilities. We caution against drawing\nconclusions from anecdotal examples, limited benchmark testing, and using\nhuman-designed psychological tests to evaluate models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shapira_N/0/1/0/all/0/1\">Natalie Shapira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_M/0/1/0/all/0/1\">Mosh Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alavi_S/0/1/0/all/0/1\">Seyed Hossein Alavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xuhui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1\">Vered Shwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BeamSearchQA: Large Language Models are Strong Zero-Shot QA Solver. (arXiv:2305.14766v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14766","description":"<p>Open-domain question answering is a crucial task that often requires\naccessing external information. Existing methods typically adopt a single-turn\nretrieve-then-read approach, where relevant documents are first retrieved, and\nquestions are then answered based on the retrieved information. However, there\nare cases where answering a question requires implicit knowledge that is not\ndirectly retrievable from the question itself. In this work, we propose a novel\nquestion-answering pipeline called eamSearchQA. Our approach leverages large\nlanguage models(LLMs) to iteratively generate new questions about the original\nquestion, enabling an iterative reasoning process. By iteratively refining and\nexpanding the scope of the question, our method aims to capture and utilize\nhidden knowledge that may not be directly obtainable through retrieval. We\nevaluate our approach on the widely-used open-domain NQ and WebQ datasets. The\nexperimental results demonstrate that BeamSearchQA significantly outperforms\nother zero-shot baselines, indicating its effectiveness in tackling the\nchallenges of open-domain question answering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Natural Language Explanations to Rescale Human Judgments. (arXiv:2305.14770v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14770","description":"<p>The rise of large language models (LLMs) has brought a critical need for\nhigh-quality human-labeled data, particularly for processes like human feedback\nand evaluation. A common practice is to label data via consensus annotation\nover the judgments of multiple crowdworkers. However, different annotators may\nhave different interpretations of labeling schemes unless given extensive\ntraining, and for subjective NLP tasks, even trained expert annotators can\ndiverge heavily. We show that these nuances can be captured by high quality\nnatural language explanations, and propose a method to rescale ordinal\nannotation in the presence of disagreement using LLMs. Specifically, we feed\nLikert ratings and corresponding natural language explanations into an LLM and\nprompt it to produce a numeric score. This score should reflect the underlying\nassessment of the example by the annotator. The presence of explanations allows\nthe LLM to homogenize ratings across annotators in spite of scale usage\ndifferences. We explore our technique in the context of a document-grounded\nquestion answering task on which large language models achieve near-human\nperformance. Among questions where annotators identify incompleteness in the\nanswers, our rescaling improves correlation between nearly all annotator pairs,\nimproving pairwise correlation on these examples by an average of 0.2 Kendall's\ntau.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wadhwa_M/0/1/0/all/0/1\">Manya Wadhwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jifan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SSD-2: Scaling and Inference-time Fusion of Diffusion Language Models. (arXiv:2305.14771v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14771","description":"<p>Diffusion-based language models (LMs) have been shown to be competent\ngenerative models that are easy to control at inference and are a promising\nalternative to autoregressive LMs. While autoregressive LMs have benefited\nimmensely from scaling and instruction-based learning, existing studies on\ndiffusion LMs have been conducted on a relatively smaller scale. Starting with\na recently proposed diffusion model SSD-LM, in this work we explore methods to\nscale it from 0.4B to 13B parameters, proposing several techniques to improve\nits training and inference efficiency. We call the new model SSD-2. We further\nshow that this model can be easily finetuned to follow instructions. Finally,\nleveraging diffusion models' capability at inference-time control, we show that\nSSD-2 facilitates novel ensembles with 100x smaller models that can be\ncustomized and deployed by individual users. We find that compared to\nautoregressive models, the collaboration between diffusion models is more\neffective, leading to higher-quality and more relevant model responses due to\ntheir ability to incorporate bi-directional contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaochuang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sachin Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghazvininejad_M/0/1/0/all/0/1\">Marjan Ghazvininejad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Controllable QA-based Framework for Decontextualization. (arXiv:2305.14772v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14772","description":"<p>Many real-world applications require surfacing extracted snippets to users,\nwhether motivated by assistive tools for literature surveys or document\ncross-referencing, or needs to mitigate and recover from model generated\ninaccuracies., Yet, these passages can be difficult to consume when divorced\nfrom their original document context. In this work, we explore the limits of\nLLMs to perform decontextualization of document snippets in user-facing\nscenarios, focusing on two real-world settings - question answering and\ncitation context previews for scientific documents. We propose a\nquestion-answering framework for decontextualization that allows for better\nhandling of user information needs and preferences when determining the scope\nof rewriting. We present results showing state-of-the-art LLMs under our\nframework remain competitive with end-to-end approaches. We also explore\nincorporating user preferences into the system, finding our framework allows\nfor controllability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Newman_B/0/1/0/all/0/1\">Benjamin Newman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldaini_L/0/1/0/all/0/1\">Luca Soldaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fok_R/0/1/0/all/0/1\">Raymond Fok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring the Knowledge Acquisition-Utilization Gap in Pretrained Language Models. (arXiv:2305.14775v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14775","description":"<p>While pre-trained language models (PLMs) have shown evidence of acquiring\nvast amounts of knowledge, it remains unclear how much of this parametric\nknowledge is actually usable in performing downstream tasks. We propose a\nsystematic framework to measure parametric knowledge utilization in PLMs. Our\nframework first extracts knowledge from a PLM's parameters and subsequently\nconstructs a downstream task around this extracted knowledge. Performance on\nthis task thus depends exclusively on utilizing the model's possessed\nknowledge, avoiding confounding factors like insufficient signal. As an\ninstantiation, we study factual knowledge of PLMs and measure utilization\nacross 125M to 13B parameter PLMs. We observe that: (1) PLMs exhibit two gaps -\nin acquired vs. utilized knowledge, (2) they show limited robustness in\nutilizing knowledge under distribution shifts, and (3) larger models close the\nacquired knowledge gap but the utilized knowledge gap remains. Overall, our\nstudy provides insights into PLMs' capabilities beyond their acquired\nknowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kazemnejad_A/0/1/0/all/0/1\">Amirhossein Kazemnejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathi_P/0/1/0/all/0/1\">Prasanna Parthasarathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Conditional Alt-Text Generation for Twitter Images. (arXiv:2305.14779v1 [cs.CV])","link":"http://arxiv.org/abs/2305.14779","description":"<p>In this work we present an approach for generating alternative text (or\nalt-text) descriptions for images shared on social media, specifically Twitter.\nThis task is more than just a special case of image captioning, as alt-text is\nboth more literally descriptive and context-specific. Also critically, images\nposted to Twitter are often accompanied by user-written text that despite not\nnecessarily describing the image may provide useful context that if properly\nleveraged can be informative -- e.g. the tweet may name an uncommon object in\nthe image that the model has not previously seen. We address this with a CLIP\nprefix model that extracts an embedding of the image and passes it to a mapping\nnetwork that outputs a short sequence in word embedding space, or a ``prefix'',\nto which we also concatenate the text from the tweet itself. This lets the\nmodel condition on both visual and textual information from the post. The\ncombined multimodal prefix is then fed as a prompt to a pretrained language\nmodel which autoregressively completes the sequence to generate the alt-text.\nWhile prior work has used similar methods for captioning, ours is the first to\nour knowledge that incorporates textual information from the associated social\nmedia post into the prefix as well, and we further demonstrate through\nablations that utility of these two information sources stacks. We put forward\na new dataset scraped from Twitter and evaluate on it across a variety of\nautomated metrics as well as human evaluation, and show that our approach of\nconditioning on both tweet text and visual information significantly\noutperforms prior work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivatsan_N/0/1/0/all/0/1\">Nikita Srivatsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samaniego_S/0/1/0/all/0/1\">Sofia Samaniego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florez_O/0/1/0/all/0/1\">Omar Florez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangled Phonetic Representation for Chinese Spelling Correction. (arXiv:2305.14783v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14783","description":"<p>Chinese Spelling Correction (CSC) aims to detect and correct erroneous\ncharacters in Chinese texts. Although efforts have been made to introduce\nphonetic information (Hanyu Pinyin) in this task, they typically merge phonetic\nrepresentations with character representations, which tends to weaken the\nrepresentation effect of normal texts. In this work, we propose to disentangle\nthe two types of features to allow for direct interaction between textual and\nphonetic information. To learn useful phonetic representations, we introduce a\npinyin-to-character objective to ask the model to predict the correct\ncharacters based solely on phonetic information, where a separation mask is\nimposed to disable attention from phonetic input to text. To avoid overfitting\nthe phonetics, we further design a self-distillation module to ensure that\nsemantic information plays a major role in the prediction. Extensive\nexperiments on three CSC benchmarks demonstrate the superiority of our method\nin using phonetic information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zihong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1\">Xiaojun Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qifan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anthropomorphization of AI: Opportunities and Risks. (arXiv:2305.14784v1 [cs.AI])","link":"http://arxiv.org/abs/2305.14784","description":"<p>Anthropomorphization is the tendency to attribute human-like traits to\nnon-human entities. It is prevalent in many social contexts -- children\nanthropomorphize toys, adults do so with brands, and it is a literary device.\nIt is also a versatile tool in science, with behavioral psychology and\nevolutionary biology meticulously documenting its consequences. With widespread\nadoption of AI systems, and the push from stakeholders to make it human-like\nthrough alignment techniques, human voice, and pictorial avatars, the tendency\nfor users to anthropomorphize it increases significantly. We take a dyadic\napproach to understanding this phenomenon with large language models (LLMs) by\nstudying (1) the objective legal implications, as analyzed through the lens of\nthe recent blueprint of AI bill of rights and the (2) subtle psychological\naspects customization and anthropomorphization. We find that anthropomorphized\nLLMs customized for different user bases violate multiple provisions in the\nlegislative blueprint. In addition, we point out that anthropomorphization of\nLLMs affects the influence they can have on their users, thus having the\npotential to fundamentally change the nature of human-AI interaction, with\npotential for manipulation and negative influence. With LLMs being\nhyper-personalized for vulnerable groups like children and patients among\nothers, our work is a timely and important contribution. We propose a\nconservative strategy for the cautious use of anthropomorphization to improve\ntrustworthiness of AI systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1\">Ameet Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurohit_T/0/1/0/all/0/1\">Tanmay Rajpurohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1\">Ashwin Kalyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT and Simple Linguistic Inferences: Blind Spots and Blinds. (arXiv:2305.14785v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14785","description":"<p>This paper sheds light on the limitations of ChatGPT's understanding\ncapabilities, focusing on simple inference tasks that are typically easy for\nhumans but appear to be challenging for the model. Specifically, we target (i)\ngrammatically-specified entailments, (ii) premises with evidential adverbs of\nuncertainty, and (iii) monotonicity entailments. We present expert-designed\nevaluation sets for these inference types and conduct experiments in a\nzero-shot setup. Our results show that the model struggles with these types of\ninferences, exhibiting moderate to low accuracy. Moreover, while ChatGPT\ndemonstrates knowledge of the underlying linguistic concepts when prompted\ndirectly, it often fails to incorporate this knowledge to make correct\ninferences. Even more strikingly, further experiments show that embedding the\npremise under presupposition triggers or non-factive verbs causes the model to\npredict entailment more frequently {regardless} of the correct semantic label.\nOverall these results suggest that, despite GPT's celebrated language\nunderstanding capacity, ChatGPT has blindspots with respect to certain types of\nentailment, and that certain entailment-cancelling features act as ``blinds''\novershadowing the semantics of the embedded premise. Our analyses emphasize the\nneed for further research into the linguistic comprehension and reasoning\ncapabilities of LLMs, in order to improve their reliability, and establish\ntheir trustworthiness for real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basmov_V/0/1/0/all/0/1\">Victoria Basmov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting Language Models to Compress Contexts. (arXiv:2305.14788v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14788","description":"<p>Transformer-based language models (LMs) are powerful and widely-applicable\ntools, but their usefulness is constrained by a finite context window and the\nexpensive computational cost of processing long text documents. We propose to\nadapt pre-trained LMs into AutoCompressors. These models are capable of\ncompressing long contexts into compact summary vectors, which are then\naccessible to the model as soft prompts. Summary vectors are trained with an\nunsupervised objective, whereby long documents are processed in segments and\nsummary vectors from all previous segments are used in language modeling. We\nfine-tune OPT models on sequences of up to 30,720 tokens and show that\nAutoCompressors can utilize long contexts to improve perplexity. We evaluate\nAutoCompressors on in-context learning by compressing task demonstrations. We\nfind that summary vectors are good substitutes for plain-text demonstrations,\nincreasing accuracy while reducing inference cost. Finally, we explore the\nbenefits of pre-computing summary vectors for large corpora by applying summary\nvectors to retrieval-augmented language modeling. Overall, AutoCompressors\nemerge as a simple and inexpensive solution for extending the context window of\nLMs while speeding up inference over long contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chevalier_A/0/1/0/all/0/1\">Alexis Chevalier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wettig_A/0/1/0/all/0/1\">Alexander Wettig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajith_A/0/1/0/all/0/1\">Anirudh Ajith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advancing Topic Segmentation and Outline Generation in Chinese Texts: The Paragraph-level Topic Representation, Corpus, and Benchmark. (arXiv:2305.14790v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14790","description":"<p>Topic segmentation and outline generation strive to divide a document into\ncoherent topic sections and generate corresponding subheadings. Such a process\nunveils the discourse topic structure of a document that benefits quickly\ngrasping and understanding the overall context of the document from a higher\nlevel. However, research and applications in this field have been restrained\ndue to the lack of proper paragraph-level topic representations and\nlarge-scale, high-quality corpora in Chinese compared to the success achieved\nin English. Addressing these issues, we introduce a hierarchical\nparagraph-level topic structure representation with title, subheading, and\nparagraph that comprehensively models the document discourse topic structure.\nIn addition, we ensure a more holistic representation of topic distribution\nwithin the document by using sentences instead of keywords to represent\nsub-topics. Following this representation, we construct the largest Chinese\nParagraph-level Topic Structure corpus (CPTS), four times larger than the\npreviously largest one. We also employ a two-stage man-machine collaborative\nannotation method to ensure the high quality of the corpus both in form and\nsemantics. Finally, we validate the computability of CPTS on two fundamental\ntasks (topic segmentation and outline generation) by several strong baselines,\nand its efficacy has been preliminarily confirmed on the downstream task:\ndiscourse parsing. The representation, corpus, and benchmark we established\nwill provide a solid foundation for future studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1\">Feng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaomin Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qiaoming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models as Counterfactual Generator: Strengths and Weaknesses. (arXiv:2305.14791v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14791","description":"<p>Large language models (LLMs) have demonstrated remarkable performance in a\nrange of natural language understanding and generation tasks. Yet, their\nability to generate counterfactuals, which can be used for areas like data\naugmentation, remains under-explored. This study aims to investigate the\ncounterfactual generation capabilities of LLMs and analysis factors that\ninfluence this ability. First, we evaluate how effective are LLMs in\ncounterfactual generation through data augmentation experiments for small\nlanguage models (SLMs) across four tasks: sentiment analysis, natural language\ninference, named entity recognition, and relation extraction. While LLMs show\npromising enhancements in various settings, they struggle in complex tasks due\nto their self-limitations and the lack of logical guidance to produce\ncounterfactuals that align with commonsense. Second, our analysis reveals the\npivotal role of providing accurate task definitions and detailed step-by-step\ninstructions to LLMs in generating counterfactuals. Interestingly, we also find\nthat LLMs can generate reasonable counterfactuals even with unreasonable\ndemonstrations, which illustrates that demonstrations are primarily to regulate\nthe output format.This study provides the first comprehensive insight into\ncounterfactual generation abilities of LLMs, and offers a novel perspective on\nutilizing LLMs for data augmentation to enhance SLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mayi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_X/0/1/0/all/0/1\">Xin Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_T/0/1/0/all/0/1\">Tieyun Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faithful Low-Resource Data-to-Text Generation through Cycle Training. (arXiv:2305.14793v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14793","description":"<p>Methods to generate text from structured data have advanced significantly in\nrecent years, primarily due to fine-tuning of pre-trained language models on\nlarge datasets. However, such models can fail to produce output faithful to the\ninput data, particularly on out-of-domain data. Sufficient annotated data is\noften not available for specific domains, leading us to seek an unsupervised\napproach to improve the faithfulness of output text. Since the problem is\nfundamentally one of consistency between the representations of the structured\ndata and text, we evaluate the effectiveness of cycle training in this work.\nCycle training uses two models which are inverses of each other: one that\ngenerates text from structured data, and one which generates the structured\ndata from natural language text. We show that cycle training, when initialized\nwith a small amount of supervised data (100 samples in our case), achieves\nnearly the same performance as fully supervised approaches for the data-to-text\ngeneration task on the WebNLG, E2E, WTQ, and WSQL datasets. We perform\nextensive empirical analysis with automated evaluation metrics and a newly\ndesigned human evaluation schema to reveal different cycle training strategies'\neffectiveness of reducing various types of generation errors. Our code is\npublicly available at https://github.com/Edillower/CycleNLG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhuoer Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1\">Marcus Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedula_N/0/1/0/all/0/1\">Nikhita Vedula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filice_S/0/1/0/all/0/1\">Simone Filice</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malmasi_S/0/1/0/all/0/1\">Shervin Malmasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rokhlenko_O/0/1/0/all/0/1\">Oleg Rokhlenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak Supervision for Text Classification. (arXiv:2305.14794v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14794","description":"<p>Recent advances in weakly supervised text classification mostly focus on\ndesigning sophisticated methods to turn high-level human heuristics into\nquality pseudo-labels. In this paper, we revisit the seed matching-based\nmethod, which is arguably the simplest way to generate pseudo-labels, and show\nthat its power was greatly underestimated. We show that the limited performance\nof seed matching is largely due to the label bias injected by the simple\nseed-match rule, which prevents the classifier from learning reliable\nconfidence for selecting high-quality pseudo-labels. Interestingly, simply\ndeleting the seed words present in the matched input texts can mitigate the\nlabel bias and help learn better confidence. Subsequently, the performance\nachieved by seed matching can be improved significantly, making it on par with\nor even better than the state-of-the-art. Furthermore, to handle the case when\nthe seed words are not made known, we propose to simply delete the word tokens\nin the input text randomly with a high deletion ratio. Remarkably, seed\nmatching equipped with this random deletion method can often achieve even\nbetter performance than that with seed deletion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chengyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions. (arXiv:2305.14795v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14795","description":"<p>The information stored in large language models (LLMs) falls out of date\nquickly, and retraining from scratch is often not an option. This has recently\ngiven rise to a range of techniques for injecting new facts through updating\nmodel weights. Current evaluation paradigms are extremely limited, mainly\nvalidating the recall of edited facts, but changing one fact should cause\nrippling changes to the model's related beliefs. If we edit the UK Prime\nMinister to now be Rishi Sunak, then we should get a different answer to Who is\nmarried to the British Prime Minister? In this work, we present a benchmark\nMQuAKE (Multi-hop Question Answering for Knowledge Editing) comprising\nmulti-hop questions that assess whether edited models correctly answer\nquestions where the answer should change as an entailed consequence of edited\nfacts. While we find that current knowledge-editing approaches can recall\nedited facts accurately, they fail catastrophically on the constructed\nmulti-hop questions. We thus propose a simple memory-based approach, MeLLo,\nwhich stores all edited facts externally while prompting the language model\niteratively to generate answers that are consistent with the edited facts.\nWhile MQuAKE remains challenging, we show that MeLLo scales well with LLMs (up\nto 175B) and outperforms previous model editors by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zexuan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhengxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating Large Language Model Capabilities without Labeled Test Data. (arXiv:2305.14802v1 [cs.CL])","link":"http://arxiv.org/abs/2305.14802","description":"<p>Large Language Models (LLMs) have exhibited an impressive ability to perform\nin-context learning (ICL) from only a few examples, but the success of ICL\nvaries widely from task to task. Thus, it is important to quickly determine\nwhether ICL is applicable to a new task, but directly evaluating ICL accuracy\ncan be expensive in situations where test data is expensive to annotate -- the\nexact situations where ICL is most appealing. In this paper, we propose the\ntask of ICL accuracy estimation, in which we predict the accuracy of an LLM\nwhen doing in-context learning on a new task given only unlabeled data for that\ntask. To perform ICL accuracy estimation, we propose a method that trains a\nmeta-model using LLM confidence scores as features. We compare our method to\nseveral strong accuracy estimation baselines on a new benchmark that covers 4\nLLMs and 3 task collections. On average, the meta-model improves over all\nbaselines and achieves the same estimation performance as directly evaluating\non 40 labeled test examples per task, across the total 12 settings. We\nencourage future work to improve on our methods and evaluate on our ICL\naccuracy estimation benchmark to deepen our understanding of when ICL works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Harvey Yiyun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinyuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_A/0/1/0/all/0/1\">Albert Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSEE: Dually Sparsity-embedded Efficient Tuning of Pre-trained Language Models. (arXiv:2111.00160v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.00160","description":"<p>Gigantic pre-trained models have become central to natural language\nprocessing (NLP), serving as the starting point for fine-tuning towards a range\nof downstream tasks. However, two pain points persist for this paradigm: (a) as\nthe pre-trained models grow bigger (e.g., 175B parameters for GPT-3), even the\nfine-tuning process can be time-consuming and computationally expensive; (b)\nthe fine-tuned model has the same size as its starting point by default, which\nis neither sensible due to its more specialized functionality, nor practical\nsince many fine-tuned models will be deployed in resource-constrained\nenvironments. To address these pain points, we propose a framework for\nresource- and parameter-efficient fine-tuning by leveraging the sparsity prior\nin both weight updates and the final model weights. Our proposed framework,\ndubbed Dually Sparsity-Embedded Efficient Tuning (DSEE), aims to achieve two\nkey objectives: (i) parameter efficient fine-tuning - by enforcing\nsparsity-aware low-rank updates on top of the pre-trained weights; and (ii)\nresource-efficient inference - by encouraging a sparse weight structure towards\nthe final fine-tuned model. We leverage sparsity in these two directions by\nexploiting both unstructured and structured sparse patterns in pre-trained\nlanguage models via a unified approach. Extensive experiments and in-depth\ninvestigations, with diverse network backbones (i.e., BERT, RoBERTa, and GPT-2)\non dozens of datasets, consistently demonstrate impressive\nparameter-/inference-efficiency, while maintaining competitive downstream\nperformance. For instance, DSEE saves about 25% inference FLOPs while achieving\ncomparable performance, with 0.5% trainable parameters on BERT. Codes are\navailable in https://github.com/VITA-Group/DSEE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models. (arXiv:2201.05337v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05337","description":"<p>Controllable Text Generation (CTG) is emerging area in the field of natural\nlanguage generation (NLG). It is regarded as crucial for the development of\nadvanced text generation technologies that are more natural and better meet the\nspecific constraints in practical applications. In recent years, methods using\nlarge-scale pre-trained language models (PLMs), in particular the widely used\ntransformer-based PLMs, have become a new paradigm of NLG, allowing generation\nof more diverse and fluent text. However, due to the lower level of\ninterpretability of deep neural networks, the controllability of these methods\nneed to be guaranteed. To this end, controllable text generation using\ntransformer-based PLMs has become a rapidly growing yet challenging new\nresearch hotspot. A diverse range of approaches have emerged in the recent 3-4\nyears, targeting different CTG tasks which may require different types of\ncontrolled constraints. In this paper, we present a systematic critical review\non the common tasks, main approaches and evaluation methods in this area.\nFinally, we discuss the challenges that the field is facing, and put forward\nvarious promising future directions. To the best of our knowledge, this is the\nfirst survey paper to summarize CTG techniques from the perspective of PLMs. We\nhope it can help researchers in related fields to quickly track the academic\nfrontier, providing them with a landscape of the area and a roadmap for future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Haolin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaoyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawei Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Different Affordances on Facebook and SMS Text Messaging Do Not Impede Generalization of Language-Based Predictive Models. (arXiv:2202.01802v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.01802","description":"<p>Adaptive mobile device-based health interventions often use machine learning\nmodels trained on non-mobile device data, such as social media text, due to the\ndifficulty and high expense of collecting large text message (SMS) data.\nTherefore, understanding the differences and generalization of models between\nthese platforms is crucial for proper deployment. We examined the\npsycho-linguistic differences between Facebook and text messages, and their\nimpact on out-of-domain model performance, using a sample of 120 users who\nshared both. We found that users use Facebook for sharing experiences (e.g.,\nleisure) and SMS for task-oriented and conversational purposes (e.g., plan\nconfirmations), reflecting the differences in the affordances. To examine the\ndownstream effects of these differences, we used pre-trained Facebook-based\nlanguage models to estimate age, gender, depression, life satisfaction, and\nstress on both Facebook and SMS. We found no significant differences in\ncorrelations between the estimates and self-reports across 6 of 8 models. These\nresults suggest using pre-trained Facebook language models to achieve better\naccuracy with just-in-time interventions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tingting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giorgi_S/0/1/0/all/0/1\">Salvatore Giorgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1\">Xiangyu Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guntuku_S/0/1/0/all/0/1\">Sharath Chandra Guntuku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellew_D/0/1/0/all/0/1\">Douglas Bellew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curtis_B/0/1/0/all/0/1\">Brenda Curtis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1\">Lyle Ungar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Cross-lingual Prompting with Dual Prompt Augmentation. (arXiv:2202.07255v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.07255","description":"<p>Prompting shows promising results in few-shot scenarios. However, its\nstrength for multilingual/cross-lingual problems has not been fully exploited.\nZhao and Sch\\\"utze (2021) made initial explorations in this direction by\npresenting that cross-lingual prompting outperforms cross-lingual finetuning.\nIn this paper, we conduct an empirical exploration on the effect of each\ncomponent in cross-lingual prompting and derive language-agnostic Universal\nPrompting, which helps alleviate the discrepancies between source-language\ntraining and target-language inference. Based on this, we propose DPA, a dual\nprompt augmentation framework, aiming at relieving the data scarcity issue in\nfew-shot cross-lingual prompting. Notably, for XNLI, our method achieves 46.54%\nwith only 16 English training examples per class, significantly better than\n34.99% of finetuning. Our code is available at\nhttps://github.com/DAMO-NLP-SG/DPA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Meng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yue Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GENEVA: Benchmarking Generalizability for Event Argument Extraction with Hundreds of Event Types and Argument Roles. (arXiv:2205.12505v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12505","description":"<p>Recent works in Event Argument Extraction (EAE) have focused on improving\nmodel generalizability to cater to new events and domains. However, standard\nbenchmarking datasets like ACE and ERE cover less than 40 event types and 25\nentity-centric argument roles. Limited diversity and coverage hinder these\ndatasets from adequately evaluating the generalizability of EAE models. In this\npaper, we first contribute by creating a large and diverse EAE ontology. This\nontology is created by transforming FrameNet, a comprehensive semantic role\nlabeling (SRL) dataset for EAE, by exploiting the similarity between these two\ntasks. Then, exhaustive human expert annotations are collected to build the\nontology, concluding with 115 events and 220 argument roles, with a significant\nportion of roles not being entities. We utilize this ontology to further\nintroduce GENEVA, a diverse generalizability benchmarking dataset comprising\nfour test suites, aimed at evaluating models' ability to handle limited data\nand unseen event type generalization. We benchmark six EAE models from various\nfamilies. The results show that owing to non-entity argument roles, even the\nbest-performing model can only achieve 39% F1 score, indicating how GENEVA\nprovides new challenges for generalization in EAE. Overall, our large and\ndiverse EAE ontology can aid in creating more comprehensive future resources,\nwhile GENEVA is a challenging benchmarking dataset encouraging further research\nfor improving generalizability in EAE. The code and data can be found at\nhttps://github.com/PlusLabNLP/GENEVA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parekh_T/0/1/0/all/0/1\">Tanmay Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1\">I-Hung Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making Large Language Models Better Reasoners with Step-Aware Verifier. (arXiv:2206.02336v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.02336","description":"<p>Few-shot learning is a challenging task that requires language models to\ngeneralize from limited examples. Large language models like GPT-3 and PaLM\nhave made impressive progress in this area, but they still face difficulties in\nreasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve\ntheir reasoning skills, previous work has proposed to guide the language model\nwith prompts that elicit a series of reasoning steps before giving the final\nanswer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in\nproblem-solving rate. In this paper, we present DIVERSE (Diverse Verifier on\nReasoning Step), a novel approach that further enhances the reasoning\ncapability of language models. DIVERSE has three main components: first, it\ngenerates diverse prompts to explore different reasoning paths for the same\nquestion; second, it uses a verifier to filter out incorrect answers based on a\nweighted voting scheme; and third, it verifies each reasoning step individually\ninstead of the whole chain. We evaluate DIVERSE on the latest language model\ncode-davinci-002 and show that it achieves new state-of-the-art results on six\nof eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zeqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shizhuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating and Inducing Personality in Pre-trained Language Models. (arXiv:2206.07550v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.07550","description":"<p>Originating as a philosophical quest, the study of personality concerns how\nindividuals differ in thinking, feeling, and behaving. Towards building social\nmachines that work with humans on a daily basis, we are motivated to ask: Do\nexisting Large Language Models (LLMs) possess personalities akin to their human\ncounterparts? If so, how can we evaluate them? Further, given this evaluation\nframework, how can we induce a particular personality in a controllable\nfashion? To answer these three questions, we propose the Machine Personality\nInventory (MPI) dataset for evaluating the machine personality; MPI follows\nstandardized personality tests, built upon the Big Five Personality Factors\n(Big Five) theory and personality assessment inventories. By systematically\nevaluating LLMs with MPI, we provide the first piece of evidence showing the\nexistence of personality in LLMs. We further devise a Personality Prompting\n(P^2) method to induce LLMs with a specific personality in a controllable\nmanner, capable of producing diverse behaviors. We hope this work sheds light\non future studies by adopting personality as the essential guide for various\ndownstream tasks, building human-like and in situ dialogue agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1\">Guangyuan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Manjie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wenjuan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yixin Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"News Summarization and Evaluation in the Era of GPT-3. (arXiv:2209.12356v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.12356","description":"<p>The recent success of prompting large language models like GPT-3 has led to a\nparadigm shift in NLP research. In this paper, we study its impact on text\nsummarization, focusing on the classic benchmark domain of news summarization.\nFirst, we investigate how GPT-3 compares against fine-tuned models trained on\nlarge summarization datasets. We show that not only do humans overwhelmingly\nprefer GPT-3 summaries, prompted using only a task description, but these also\ndo not suffer from common dataset-specific issues such as poor factuality.\nNext, we study what this means for evaluation, particularly the role of gold\nstandard test sets. Our experiments show that both reference-based and\nreference-free automatic metrics cannot reliably evaluate GPT-3 summaries.\nFinally, we evaluate models on a setting beyond generic summarization,\nspecifically keyword-based summarization, and show how dominant fine-tuning\napproaches compare to prompting.\n</p>\n<p>To support further research, we release: (a) a corpus of 10K generated\nsummaries from fine-tuned and prompt-based models across 4 standard\nsummarization benchmarks, (b) 1K human preference judgments comparing different\nsystems for generic- and keyword-based summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_T/0/1/0/all/0/1\">Tanya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Creation of Named Entity Recognition Datasets by Querying Phrase Representations. (arXiv:2210.07586v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07586","description":"<p>Most weakly supervised named entity recognition (NER) models rely on\ndomain-specific dictionaries provided by experts. This approach is infeasible\nin many domains where dictionaries do not exist. While a phrase retrieval model\nwas used to construct pseudo-dictionaries with entities retrieved from\nWikipedia automatically in a recent study, these dictionaries often have\nlimited coverage because the retriever is likely to retrieve popular entities\nrather than rare ones. In this study, we present a novel framework, HighGEN,\nthat generates NER datasets with high-coverage pseudo-dictionaries.\nSpecifically, we create entity-rich dictionaries with a novel search method,\ncalled phrase embedding search, which encourages the retriever to search a\nspace densely populated with various entities. In addition, we use a new\nverification process based on the embedding distance between candidate entity\nmentions and entity types to reduce the false-positive noise in weak labels\ngenerated by high-coverage dictionaries. We demonstrate that HighGEN\noutperforms the previous best model by an average F1 score of 4.7 across five\nNER benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunjae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jaehyo Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards standardizing Korean Grammatical Error Correction: Datasets and Annotation. (arXiv:2210.14389v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.14389","description":"<p>Research on Korean grammatical error correction (GEC) is limited, compared to\nother major languages such as English. We attribute this problematic\ncircumstance to the lack of a carefully designed evaluation benchmark for\nKorean GEC. In this work, we collect three datasets from different sources\n(Kor-Lang8, Kor-Native, and Kor-Learner) that covers a wide range of Korean\ngrammatical errors. Considering the nature of Korean grammar, We then define 14\nerror types for Korean and provide KAGAS (Korean Automatic Grammatical error\nAnnotation System), which can automatically annotate error types from parallel\ncorpora. We use KAGAS on our datasets to make an evaluation benchmark for\nKorean, and present baseline models trained from our datasets. We show that the\nmodel trained with our datasets significantly outperforms the currently used\nstatistical Korean GEC system (Hanspell) on a wider range of error types,\ndemonstrating the diversity and usefulness of the datasets. The implementations\nand datasets are open-sourced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Soyoung Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungjoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyuwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Junhee Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kihyo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyutae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Multilingual Gender-Ambiguous Text-to-Speech Voices. (arXiv:2211.00375v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2211.00375","description":"<p>The gender of any voice user interface is a key element of its perceived\nidentity. Recently, there has been increasing interest in interfaces where the\ngender is ambiguous rather than clearly identifying as female or male. This\nwork addresses the task of generating novel gender-ambiguous TTS voices in a\nmulti-speaker, multilingual setting. This is accomplished by efficiently\nsampling from a latent speaker embedding space using a proposed gender-aware\nmethod. Extensive objective and subjective evaluations clearly indicate that\nthis method is able to efficiently generate a range of novel, diverse voices\nthat are consistent and perceived as more gender-ambiguous than a baseline\nvoice across all the languages examined. Interestingly, the gender perception\nis found to be robust across two demographic factors of the listeners: native\nlanguage and gender. To our knowledge, this is the first systematic and\nvalidated approach that can reliably generate a variety of gender-ambiguous\nvoices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Markopoulos_K/0/1/0/all/0/1\">Konstantinos Markopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maniati_G/0/1/0/all/0/1\">Georgia Maniati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vamvoukakis_G/0/1/0/all/0/1\">Georgios Vamvoukakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellinas_N/0/1/0/all/0/1\">Nikolaos Ellinas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vardaxoglou_G/0/1/0/all/0/1\">Georgios Vardaxoglou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakoulidis_P/0/1/0/all/0/1\">Panos Kakoulidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Junkwang Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jho_G/0/1/0/all/0/1\">Gunu Jho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_I/0/1/0/all/0/1\">Inchul Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalamandaris_A/0/1/0/all/0/1\">Aimilios Chalamandaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsiakoulis_P/0/1/0/all/0/1\">Pirros Tsiakoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raptis_S/0/1/0/all/0/1\">Spyros Raptis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inverse scaling can become U-shaped. (arXiv:2211.02011v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.02011","description":"<p>Scaling up language models has been empirically shown to improve performance\non a wide range of downstream tasks. However, if we were to observe worse\nperformance as a function of scale (\"inverse scaling\") on certain tasks, this\nwould indicate that scaling can also encourage behaviors that are misaligned\nwith human preferences. The Inverse Scaling Prize (McKenzie et al. 2022)\nidentified eleven such inverse scaling tasks, evaluated on models of up to 280B\nparameters and up to 500 zettaFLOPs of training compute. This paper takes a\ncloser look at these inverse scaling tasks. We evaluate models of up to 540B\nparameters, trained on five times more compute than those evaluated in the\nInverse Scaling Prize. With this increased range of model sizes and training\ncompute, only four out of the eleven tasks remain inverse scaling. Six out of\nthe eleven tasks exhibit \"U-shaped scaling\", where performance decreases up to\na certain size, and then increases again up to the largest model evaluated (the\none remaining task displays positive scaling). In addition, we find that 1-shot\nexamples and chain-of-thought can help mitigate undesirable scaling patterns\neven further. U-shaped scaling suggests that the inverse scaling trend observed\nin McKenzie et al. (2022) may not continue to hold for larger models, which we\nattribute to the presence of distractor tasks that only sufficiently large\nmodels can avoid.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Najoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Style Classification of Rabbinic Literature for Detection of Lost Midrash Tanhuma Material. (arXiv:2211.09710v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.09710","description":"<p>Midrash collections are complex rabbinic works that consist of text in\nmultiple languages, which evolved through long processes of unstable oral and\nwritten transmission. Determining the origin of a given passage in such a\ncompilation is not always straightforward and is often a matter of dispute\namong scholars, yet it is essential for scholars' understanding of the passage\nand its relationship to other texts in the rabbinic corpus.\n</p>\n<p>To help solve this problem, we propose a system for classification of\nrabbinic literature based on its style, leveraging recently released pretrained\nTransformer models for Hebrew. Additionally, we demonstrate how our method can\nbe applied to uncover lost material from Midrash Tanhuma.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tannor_S/0/1/0/all/0/1\">Shlomo Tannor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dershowitz_N/0/1/0/all/0/1\">Nachum Dershowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavee_M/0/1/0/all/0/1\">Moshe Lavee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Transformers with Dynamic Token Pooling. (arXiv:2211.09761v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.09761","description":"<p>Transformers achieve unrivalled performance in modelling language, but remain\ninefficient in terms of memory and time complexity. A possible remedy is to\nreduce the sequence length in the intermediate layers by pooling fixed-length\nsegments of tokens. Nevertheless, natural units of meaning, such as words or\nphrases, display varying sizes. To address this mismatch, we equip language\nmodels with a dynamic-pooling mechanism, which predicts segment boundaries in\nan autoregressive fashion. We compare several methods to infer boundaries,\nincluding end-to-end learning through stochastic re-parameterisation,\nsupervised learning (based on segmentations from subword tokenizers or spikes\nin conditional entropy), as well as linguistically motivated boundaries. We\nperform character-level evaluation on texts from multiple datasets and\nmorphologically diverse languages. The results demonstrate that dynamic\npooling, which jointly segments and models language, is both faster and more\naccurate than vanilla Transformers and fixed-length pooling within the same\ncomputational budget.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nawrot_P/0/1/0/all/0/1\">Piotr Nawrot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chorowski_J/0/1/0/all/0/1\">Jan Chorowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lancucki_A/0/1/0/all/0/1\">Adrian &#x141;a&#x144;cucki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo M. Ponti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conceptor-Aided Debiasing of Large Language Models. (arXiv:2211.11087v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.11087","description":"<p>Pre-trained large language models (LLMs) reflect the inherent social biases\nof their training corpus. Many methods have been proposed to mitigate this\nissue, but they often fail to debias or they sacrifice model accuracy. We use\nconceptors--a soft projection method--to identify and remove the bias subspace\nin LLMs such as BERT and GPT. We propose two methods of applying conceptors (1)\nbias subspace projection by post-processing; and (2) a new architecture,\nconceptor-intervened BERT (CI-BERT), which explicitly incorporates the\nconceptor projection into all layers during training. We find that conceptor\npost-processing achieves state-of-the-art (SoTA) debiasing results while\nmaintaining or improving LLMs' performance on the GLUE benchmark. Also, it is\nrobust in various scenarios and can mitigate intersectional bias efficiently by\nits logical operation on the existing bias subspaces. Although CI-BERT's\ntraining takes all layers' bias into account and can beat its post-processing\ncounterpart in bias mitigation, CI-BERT reduces the language model accuracy. We\nalso show the importance of carefully constructing the bias subspace. The best\nresults are obtained by removing outliers from the list of biased words,\ncombining them (via the conceptor AND operation), and computing their\nembeddings using the sentences from a cleaner corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1\">Lyle Ungar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text. (arXiv:2211.11300v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.11300","description":"<p>Self-supervised representation learning has proved to be a valuable component\nfor out-of-distribution (OoD) detection with only the texts of in-distribution\n(ID) examples. These approaches either train a language model from scratch or\nfine-tune a pre-trained language model using ID examples, and then take the\nperplexity output by the language model as OoD scores. In this paper, we\nanalyze the complementary characteristics of both OoD detection methods and\npropose a multi-level knowledge distillation approach that integrates their\nstrengths while mitigating their limitations. Specifically, we use a fine-tuned\nmodel as the teacher to teach a randomly initialized student model on the ID\nexamples. Besides the prediction layer distillation, we present a\nsimilarity-based intermediate layer distillation method to thoroughly explore\nthe representation space of the teacher model. In this way, the learned student\ncan better represent the ID data manifold while gaining a stronger ability to\nmap OoD examples outside the ID data manifold with the regularization inherited\nfrom pre-training. Besides, the student model sees only ID examples during\nparameter learning, further promoting more distinguishable features for OoD\ndetection. We conduct extensive experiments over multiple benchmark datasets,\ni.e., CLINC150, SST, ROSTD, 20 NewsGroups, and AG News; showing that the\nproposed method yields new state-of-the-art performance. We also explore its\napplication as an AIGC detector to distinguish between answers generated by\nChatGPT and human experts. It is observed that our model exceeds human\nevaluators in the pair-expert task on the Human ChatGPT Comparison Corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qianhui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Huiqiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Haonan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_B/0/1/0/all/0/1\">B&#xf6;rje F. Karlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chin-Yew Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. (arXiv:2211.13308v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.13308","description":"<p>Learned representations of scientific documents can serve as valuable input\nfeatures for downstream tasks, without the need for further fine-tuning.\nHowever, existing benchmarks for evaluating these representations fail to\ncapture the diversity of relevant tasks. In response, we introduce SciRepEval,\nthe first comprehensive benchmark for training and evaluating scientific\ndocument representations. It includes 25 challenging and realistic tasks, 11 of\nwhich are new, across four formats: classification, regression, ranking and\nsearch. We then use the benchmark to study and improve the generalization\nability of scientific document representation models. We show how\nstate-of-the-art models struggle to generalize across task formats, and that\nsimple multi-task training fails to improve them. However, a new approach that\nlearns multiple embeddings per document, each tailored to a different format,\ncan improve performance. We experiment with task-format-specific control codes\nand adapters in a multi-task setting and find that they outperform the existing\nsingle-embedding state-of-the-art by up to 1.5 points absolute.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amanpreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DArcy_M/0/1/0/all/0/1\">Mike D&#x27;Arcy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_S/0/1/0/all/0/1\">Sergey Feldman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards human-compatible autonomous car: A study of non-verbal Turing test in automated driving with affective transition modelling. (arXiv:2212.02908v6 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2212.02908","description":"<p>Autonomous cars are indispensable when humans go further down the hands-free\nroute. Although existing literature highlights that the acceptance of the\nautonomous car will increase if it drives in a human-like manner, sparse\nresearch offers the naturalistic experience from a passenger's seat perspective\nto examine the humanness of current autonomous cars. The present study tested\nwhether the AI driver could create a human-like ride experience for passengers\nbased on 69 participants' feedback in a real-road scenario. We designed a ride\nexperience-based version of the non-verbal Turing test for automated driving.\nParticipants rode in autonomous cars (driven by either human or AI drivers) as\na passenger and judged whether the driver was human or AI. The AI driver failed\nto pass our test because passengers detected the AI driver above chance. In\ncontrast, when the human driver drove the car, the passengers' judgement was\naround chance. We further investigated how human passengers ascribe humanness\nin our test. Based on Lewin's field theory, we advanced a computational model\ncombining signal detection theory with pre-trained language models to predict\npassengers' humanness rating behaviour. We employed affective transition\nbetween pre-study baseline emotions and corresponding post-stage emotions as\nthe signal strength of our model. Results showed that the passengers'\nascription of humanness would increase with the greater affective transition.\nOur study suggested an important role of affective transition in passengers'\nascription of humanness, which might become a future direction for autonomous\ndriving.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaoning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qiaoli Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhengming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Anqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haiyan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Miner Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ku_Y/0/1/0/all/0/1\">Yixuan Ku</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages. (arXiv:2212.05409v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.05409","description":"<p>Building Natural Language Understanding (NLU) capabilities for Indic\nlanguages, which have a collective speaker base of more than one billion\nspeakers is absolutely crucial. In this work, we aim to improve the NLU\ncapabilities of Indic languages by making contributions along 3 important axes\n(i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing on\nIndic languages. Specifically, we curate the largest monolingual corpora,\nIndicCorp, with 20.9B tokens covering 24 languages from 4 language families - a\n2.3x increase over prior work, while supporting 12 additional languages. Next,\nwe create a human-supervised benchmark, IndicXTREME, consisting of nine diverse\nNLU tasks covering 20 languages. Across languages and tasks, IndicXTREME\ncontains a total of 105 evaluation sets, of which 52 are new contributions to\nthe literature. To the best of our knowledge, this is the first effort towards\ncreating a standard benchmark for Indic languages that aims to test the\nmultilingual zero-shot capabilities of pretrained language models. Finally, we\ntrain IndicBERT v2, a state-of-the-art model supporting all the languages.\nAveraged across languages and tasks, the model achieves an absolute improvement\nof 2 points over a strong baseline. The data and models are available at\nhttps://github.com/AI4Bharat/IndicBERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doddapaneni_S/0/1/0/all/0/1\">Sumanth Doddapaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aralikatte_R/0/1/0/all/0/1\">Rahul Aralikatte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_G/0/1/0/all/0/1\">Gowtham Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_S/0/1/0/all/0/1\">Shreya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A fine-grained comparison of pragmatic language understanding in humans and language models. (arXiv:2212.06801v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.06801","description":"<p>Pragmatics and non-literal language understanding are essential to human\ncommunication, and present a long-standing challenge for artificial language\nmodels. We perform a fine-grained comparison of language models and humans on\nseven pragmatic phenomena, using zero-shot prompting on an expert-curated set\nof English materials. We ask whether models (1) select pragmatic\ninterpretations of speaker utterances, (2) make similar error patterns as\nhumans, and (3) use similar linguistic cues as humans to solve the tasks. We\nfind that the largest models achieve high accuracy and match human error\npatterns: within incorrect responses, models favor literal interpretations over\nheuristic-based distractors. We also find preliminary evidence that models and\nhumans are sensitive to similar linguistic cues. Our results suggest that\npragmatic behaviors can emerge in models without explicitly constructed\nrepresentations of mental states. However, models tend to struggle with\nphenomena relying on social expectation violations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jennifer Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Floyd_S/0/1/0/all/0/1\">Sammy Floyd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jouravlev_O/0/1/0/all/0/1\">Olessia Jouravlev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedorenko_E/0/1/0/all/0/1\">Evelina Fedorenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibson_E/0/1/0/all/0/1\">Edward Gibson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Text Generation via Probability Density Estimation in the Latent Space. (arXiv:2212.08307v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08307","description":"<p>Previous work on controllable text generation has explored the idea of\ncontrol from the latent space, such as optimizing a representation with\nattribute-related classifiers or sampling a representation from relevant\ndiscrete samples. However, they are not effective enough in modeling both the\nlatent space and the control, leaving controlled text with low quality and\ndiversity. In this work, we propose a novel control framework using probability\ndensity estimation in the latent space. Our method utilizes an invertible\ntransformation function, the Normalizing Flow, that maps the complex\ndistributions in the latent space to simple Gaussian distributions in the prior\nspace. Thus, we can perform sophisticated and flexible control in the prior\nspace and feed the control effects back into the latent space owing to the\none-one-mapping property of invertible transformations. Experiments on\nsingle-attribute controls and multi-attribute control reveal that our method\noutperforms several strong baselines on attribute relevance and text quality\nand achieves the SOTA. Further analysis of control strength adjustment\ndemonstrates the flexibility of our control strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuxuan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiaocheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Sicheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lingyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Heng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Weihong Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoder Tuning: Efficient Language Understanding as Decoding. (arXiv:2212.08408v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08408","description":"<p>With the evergrowing sizes of pre-trained models (PTMs), it has been an\nemerging practice to only provide the inference APIs for users, namely\nmodel-as-a-service (MaaS) setting. To adapt PTMs with model parameters frozen,\nmost current approaches focus on the input side, seeking for powerful prompts\nto stimulate models for correct answers. However, we argue that input-side\nadaptation could be arduous due to the lack of gradient signals and they\nusually require thousands of API queries, resulting in high computation and\ntime costs. In light of this, we present Decoder Tuning (DecT), which in\ncontrast optimizes task-specific decoder networks on the output side.\nSpecifically, DecT first extracts prompt-stimulated output scores for initial\npredictions. On top of that, we train an additional decoder network on the\noutput representations to incorporate posterior data knowledge. By\ngradient-based optimization, DecT can be trained within several seconds and\nrequires only one PTM query per sample. Empirically, we conduct extensive\nnatural language understanding experiments and show that DecT significantly\noutperforms state-of-the-art algorithms with a $200\\times$ speed-up.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1\">Ganqu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wentao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Longtao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models are Better Reasoners with Self-Verification. (arXiv:2212.09561v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2212.09561","description":"<p>Recently, with the chain of thought (CoT) prompting, large language models\n(LLMs), e.g., GPT-3, have shown strong reasoning ability in several natural\nlanguage processing tasks such as arithmetic, commonsense, and logical\nreasoning. However, LLMs with CoT require multi-step prompting and multi-token\nprediction, which is highly sensitive to individual mistakes and vulnerable to\nerror accumulation. The above issues make the LLMs need the ability to verify\nthe answers. In fact, after inferring conclusions in some thinking decision\ntasks, people often check them by re-verifying steps to avoid some mistakes. In\nthis paper, we propose and prove that LLMs also have similar self-verification\nabilities. We take the conclusion obtained by CoT as one of the conditions for\nsolving the original problem. By taking turns masking the original conditions\nand predicting their results, we calculate an explainable answer verification\nscore based on whether the re-predicted conditions are correct. Experimental\nresults demonstrate that the proposed method can improve the reasoning\nperformance on various arithmetic, commonsense, and logical reasoning datasets.\nOur code is publicly available at:\nhttps://github.com/WENGSYX/Self-Verification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Minjun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shizhu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering. (arXiv:2212.09662v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09662","description":"<p>Visual language data such as plots, charts, and infographics are ubiquitous\nin the human world. However, state-of-the-art vision-language models do not\nperform well on these data. We propose MatCha (Math reasoning and Chart\nderendering pretraining) to enhance visual language models' capabilities in\njointly modeling charts/plots and language data. Specifically, we propose\nseveral pretraining tasks that cover plot deconstruction and numerical\nreasoning which are the key capabilities in visual language modeling.\n</p>\n<p>We perform the MatCha pretraining starting from Pix2Struct, a recently\nproposed image-to-text visual language model. On standard benchmarks such as\nPlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods by as\nmuch as nearly 20%. We also examine how well MatCha pretraining transfers to\ndomains such as screenshots, textbook diagrams, and document figures and\nobserve overall improvement, verifying the usefulness of MatCha pretraining on\nbroader visual language tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccinno_F/0/1/0/all/0/1\">Francesco Piccinno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krichene_S/0/1/0/all/0/1\">Syrine Krichene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_C/0/1/0/all/0/1\">Chenxi Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kenton Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1\">Mandar Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altun_Y/0/1/0/all/0/1\">Yasemin Altun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenschlos_J/0/1/0/all/0/1\">Julian Martin Eisenschlos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context. (arXiv:2212.10007v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10007","description":"<p>While pre-trained language models (LM) for code have achieved great success\nin code completion, they generate code conditioned only on the contents within\nthe file, i.e., in-file context, but ignore the rich semantics in other files\nwithin the same project, i.e., cross-file context, a critical source of\ninformation that is especially useful in modern modular software development.\nSuch overlooking constrains code language models' capacity in code completion,\nleading to unexpected behaviors such as generating hallucinated class member\nfunctions or function calls with unexpected arguments. In this work, we develop\na cross-file context finder tool, CCFINDER, that effectively locates and\nretrieves the most relevant cross-file context. We propose CoCoMIC, a framework\nthat incorporates cross-file context to learn the in-file and cross-file\ncontext jointly on top of pretrained code LMs. CoCoMIC successfully improves\nthe existing code LM with a 33.94% relative increase in exact match and a\n28.69% relative increase in identifier matching for code completion when the\ncross-file context is provided.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yangruibo Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanathan_M/0/1/0/all/0/1\">Murali Krishna Ramanathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nallapati_R/0/1/0/all/0/1\">Ramesh Nallapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_P/0/1/0/all/0/1\">Parminder Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis. (arXiv:2212.10356v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10356","description":"<p>Length extrapolation permits training a transformer language model on short\nsequences that preserves perplexities when tested on substantially longer\nsequences. A relative positional embedding design, ALiBi, has had the widest\nusage to date. We dissect ALiBi via the lens of receptive field analysis\nempowered by a novel cumulative normalized gradient tool. The concept of\nreceptive field further allows us to modify the vanilla Sinusoidal positional\nembedding to create ~\\textbf{Sandwich}, the first parameter-free relative\npositional embedding design that truly length information uses longer than the\ntraining sequence. Sandwich shares with KERPLE and T5 the same logarithmic\ndecaying temporal bias pattern with learnable relative positional embeddings;\nthese elucidate future extrapolatable positional embedding design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chi_T/0/1/0/all/0/1\">Ta-Chung Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1\">Ting-Han Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudnicky_A/0/1/0/all/0/1\">Alexander I. Rudnicky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramadge_P/0/1/0/all/0/1\">Peter J. Ramadge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary. (arXiv:2212.10380v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10380","description":"<p>Dual encoders are now the dominant architecture for dense retrieval. Yet, we\nhave little understanding of how they represent text, and why this leads to\ngood performance. In this work, we shed light on this question via\ndistributions over the vocabulary. We propose to interpret the vector\nrepresentations produced by dual encoders by projecting them into the model's\nvocabulary space. We show that the resulting projections contain rich semantic\ninformation, and draw connection between them and sparse retrieval. We find\nthat this view can offer an explanation for some of the failure cases of dense\nretrievers. For example, we observe that the inability of models to handle tail\nentities is correlated with a tendency of the token distributions to forget\nsome of the tokens of those entities. We leverage this insight and propose a\nsimple way to enrich query and passage representations with lexical information\nat inference time, and show that this significantly improves performance\ncompared to the original model in zero-shot settings, and specifically on the\nBEIR benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ram_O/0/1/0/all/0/1\">Ori Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bezalel_L/0/1/0/all/0/1\">Liat Bezalel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zicher_A/0/1/0/all/0/1\">Adi Zicher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1\">Amir Globerson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization. (arXiv:2212.10465v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10465","description":"<p>We present SODA: the first publicly available, million-scale high-quality\nsocial dialogue dataset. In contrast to most existing crowdsourced, small-scale\ndialogue corpora, we distill 1.5M socially-grounded dialogues from a large\nlanguage model (InstructGPT; Ouyang et al., 2022). Dialogues are distilled by\ncontextualizing social commonsense knowledge from a knowledge graph (Atomic10x;\nWest et al., 2022). Human evaluation shows that dialogues in SODA are more\nconsistent, specific, and (surprisingly) natural than those in prior\nhuman-authored datasets.\n</p>\n<p>Using SODA, we train COSMO: a generalizable conversation model that is\nsignificantly more natural and consistent on unseen datasets than\nbest-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna).\nExperiments reveal COSMO is sometimes even preferred to the original\nhuman-written gold responses. Additionally, our results shed light on the\ndistinction between knowledge-enriched conversations and natural social\nchitchats. We make our data, models, and code public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gunhee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Current Task-oriented Dialogue Models Automate Real-world Scenarios in the Wild?. (arXiv:2212.10504v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10504","description":"<p>Task-oriented dialogue (TOD) systems are mainly based on the\nslot-filling-based TOD (SF-TOD) framework, in which dialogues are broken down\ninto smaller, controllable units (i.e., slots) to fulfill a specific task. A\nseries of approaches based on this framework achieved remarkable success on\nvarious TOD benchmarks. However, we argue that the current TOD benchmarks are\nlimited to surrogate real-world scenarios and that the current TOD models are\nstill a long way to cover the scenarios. In this position paper, we first\nidentify current status and limitations of SF-TOD systems. After that, we\nexplore the WebTOD framework, the alternative direction for building a scalable\nTOD system when a web/mobile interface is available. In WebTOD, the dialogue\nsystem learns how to understand the web/mobile interface that the human agent\ninteracts with, powered by a large-scale language model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungdong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_D/0/1/0/all/0/1\">Donghyeon Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ham_D/0/1/0/all/0/1\">Donghoon Ham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Youngki Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Shin Ah Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">Hyunhoon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_W/0/1/0/all/0/1\">Wangkyo Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_D/0/1/0/all/0/1\">Donghyun Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_H/0/1/0/all/0/1\">Hyungsuk Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_W/0/1/0/all/0/1\">Woomyoung Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DePlot: One-shot visual language reasoning by plot-to-table translation. (arXiv:2212.10505v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10505","description":"<p>Visual language such as charts and plots is ubiquitous in the human world.\nComprehending plots and charts requires strong reasoning skills. Prior\nstate-of-the-art (SOTA) models require at least tens of thousands of training\nexamples and their reasoning capabilities are still much limited, especially on\ncomplex human-written queries. This paper presents the first one-shot solution\nto visual language reasoning. We decompose the challenge of visual language\nreasoning into two steps: (1) plot-to-text translation, and (2) reasoning over\nthe translated text. The key in this method is a modality conversion module,\nnamed as DePlot, which translates the image of a plot or chart to a linearized\ntable. The output of DePlot can then be directly used to prompt a pretrained\nlarge language model (LLM), exploiting the few-shot reasoning capabilities of\nLLMs. To obtain DePlot, we standardize the plot-to-table task by establishing\nunified task formats and metrics, and train DePlot end-to-end on this task.\nDePlot can then be used off-the-shelf together with LLMs in a plug-and-play\nfashion. Compared with a SOTA model finetuned on more than &gt;28k data points,\nDePlot+LLM with just one-shot prompting achieves a 24.0% improvement over\nfinetuned SOTA on human-written queries from the task of chart QA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenschlos_J/0/1/0/all/0/1\">Julian Martin Eisenschlos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccinno_F/0/1/0/all/0/1\">Francesco Piccinno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krichene_S/0/1/0/all/0/1\">Syrine Krichene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_C/0/1/0/all/0/1\">Chenxi Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kenton Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1\">Mandar Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altun_Y/0/1/0/all/0/1\">Yasemin Altun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards multi-document summarization in the open-domain. (arXiv:2212.10526v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10526","description":"<p>Multi-document summarization (MDS) traditionally assumes a set of\ntopic-related documents are provided. However, this document set is often an\nartifact of the dataset curation process; in practice, it is not necessarily\navailable and would need to be retrieved given an information need, i.e. a\nquestion or topic statement. We study this more challenging \"open-domain\"\nsetting by formalizing the task and bootstrapping it using existing datasets,\nretrievers and summarizers. Via extensive experimentation, we determine that:\n(1) state-of-the-art summarizers suffer large reductions in performance when\napplied to the open-domain, even when retrieval performance is high, (2)\nadditional training in the open-domain setting can reduce this sensitivity to\nimperfect retrieval, and (3) summarizers are insensitive to the retrieval of\nduplicate documents and the order of retrieved documents, but highly sensitive\nto other errors, like the retrieval of irrelevant documents. Based on our\nresults, we provide practical guidelines to enable future work on open-domain\nMDS, e.g. how to choose the number of retrieved documents to summarize. Our\nresults suggest that new methods for retrieval and summarization, as well as\nannotated resources for training and evaluation, will be necessary for further\nprogress in the open-domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giorgi_J/0/1/0/all/0/1\">John Giorgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldaini_L/0/1/0/all/0/1\">Luca Soldaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bader_G/0/1/0/all/0/1\">Gary Bader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lucy Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MGeo: Multi-Modal Geographic Pre-Training Method. (arXiv:2301.04283v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.04283","description":"<p>As a core task in location-based services (LBS) (e.g., navigation maps),\nquery and point of interest (POI) matching connects users' intent with\nreal-world geographic information. Recently, pre-trained models (PTMs) have\nmade advancements in many natural language processing (NLP) tasks. Generic\ntext-based PTMs do not have enough geographic knowledge for query-POI matching.\nTo overcome this limitation, related literature attempts to employ\ndomain-adaptive pre-training based on geo-related corpus. However, a query\ngenerally contains mentions of multiple geographic objects, such as nearby\nroads and regions of interest (ROIs). The geographic context (GC), i.e., these\ndiverse geographic objects and their relationships, is therefore pivotal to\nretrieving the most relevant POI. Single-modal PTMs can barely make use of the\nimportant GC and therefore have limited performance. In this work, we propose a\nnovel query-POI matching method Multi-modal Geographic language model (MGeo),\nwhich comprises a geographic encoder and a multi-modal interaction module. MGeo\nrepresents GC as a new modality and is able to fully extract multi-modal\ncorrelations for accurate query-POI matching. Besides, there is no publicly\navailable benchmark for this topic. In order to facilitate further research, we\nbuild a new open-source large-scale benchmark Geographic TExtual Similarity\n(GeoTES). The POIs come from an open-source geographic information system\n(GIS). The queries are manually generated by annotators to prevent privacy\nissues. Compared with several strong baselines, the extensive experiment\nresults and detailed ablation analyses on GeoTES demonstrate that our proposed\nmulti-modal pre-training method can significantly improve the query-POI\nmatching capability of generic PTMs, even when the queries' GC is not provided.\nOur code and dataset are publicly available at\nhttps://github.com/PhantomGrapes/MGeo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1\">Ruixue Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boli Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yao Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REPLUG: Retrieval-Augmented Black-Box Language Models. (arXiv:2301.12652v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.12652","description":"<p>We introduce REPLUG, a retrieval-augmented language modeling framework that\ntreats the language model (LM) as a black box and augments it with a tuneable\nretrieval model. Unlike prior retrieval-augmented LMs that train language\nmodels with special cross attention mechanisms to encode the retrieved text,\nREPLUG simply prepends retrieved documents to the input for the frozen\nblack-box LM. This simple design can be easily applied to any existing\nretrieval and language models. Furthermore, we show that the LM can be used to\nsupervise the retrieval model, which can then find documents that help the LM\nmake better predictions. Our experiments demonstrate that REPLUG with the tuned\nretriever significantly improves the performance of GPT-3 (175B) on language\nmodeling by 6.3%, as well as the performance of Codex on five-shot MMLU by\n5.1%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weijia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_R/0/1/0/all/0/1\">Rich James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring The Impact Of Programming Language Distribution. (arXiv:2302.01973v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.01973","description":"<p>Current benchmarks for evaluating neural code models focus on only a small\nsubset of programming languages, excluding many popular languages such as Go or\nRust. To ameliorate this issue, we present the BabelCode framework for\nexecution-based evaluation of any benchmark in any language. BabelCode enables\nnew investigations into the qualitative performance of models' memory, runtime,\nand individual test case results. Additionally, we present a new code\ntranslation dataset called Translating Python Programming Puzzles (TP3) from\nthe Python Programming Puzzles (Schuster et al. 2021) benchmark that involves\ntranslating expert-level python functions to any language. With both BabelCode\nand the TP3 benchmark, we investigate if balancing the distributions of 14\nlanguages in a training dataset improves a large language model's performance\non low-resource languages. Training a model on a balanced corpus results in, on\naverage, 12.34% higher $pass@k$ across all tasks and languages compared to the\nbaseline. We find that this strategy achieves 66.48% better $pass@k$ on\nlow-resource languages at the cost of only a 12.94% decrease to high-resource\nlanguages. In our three translation tasks, this strategy yields, on average,\n30.77% better low-resource $pass@k$ while having 19.58% worse high-resource\n$pass@k$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orlanski_G/0/1/0/all/0/1\">Gabriel Orlanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1\">Kefan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_J/0/1/0/all/0/1\">Jeffrey Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howland_J/0/1/0/all/0/1\">Joshua Howland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malmaud_J/0/1/0/all/0/1\">Jonathan Malmaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1\">Jacob Austin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rishabh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catasta_M/0/1/0/all/0/1\">Michele Catasta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explanation Selection Using Unlabeled Data for Chain-of-Thought Prompting. (arXiv:2302.04813v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.04813","description":"<p>Recent work has addressed textual reasoning tasks by prompting large language\nmodels with explanations via the chain-of-thought paradigm. However, subtly\ndifferent explanations can yield widely varying downstream task accuracy, so\nexplanations that have not been \"tuned\" for a task, such as off-the-shelf\nexplanations written by non-experts, may lead to mediocre performance. This\npaper tackles the problem of how to optimize explanation-infused prompts in a\nblack-box fashion. We first generate sets of candidate explanations for each\nexample in the prompt using a leave-one-out scheme. We then use a two-stage\nframework where we first evaluate explanations for each in-context example in\nisolation according to two proxy metrics, log likelihood and accuracy on new\nexamples. Finally, we search over sets of explanations to find a set that\nyields high performance against a silver-labeled development set. Across four\ntextual reasoning tasks spanning question answering, mathematical reasoning,\nand natural language inference, results show that our proxy metrics correlate\nwith ground truth accuracy and our overall method can effectively improve\nprompts over crowdworker annotations and naive search strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STOA-VLP: Spatial-Temporal Modeling of Object and Action for Video-Language Pre-training. (arXiv:2302.09736v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2302.09736","description":"<p>Although large-scale video-language pre-training models, which usually build\na global alignment between the video and the text, have achieved remarkable\nprogress on various downstream tasks, the idea of adopting fine-grained\ninformation during the pre-training stage is not well explored. In this work,\nwe propose STOA-VLP, a pre-training framework that jointly models object and\naction information across spatial and temporal dimensions. More specifically,\nthe model regards object trajectories across frames and multiple action\nfeatures from the video as fine-grained features. Besides, We design two\nauxiliary tasks to better incorporate both kinds of information into the\npre-training process of the video-language model. The first is the dynamic\nobject-text alignment task, which builds a better connection between object\ntrajectories and the relevant noun tokens. The second is the spatial-temporal\naction set prediction, which guides the model to generate consistent action\nfeatures by predicting actions found in the text. Extensive experiments on\nthree downstream tasks (video captioning, text-video retrieval, and video\nquestion answering) demonstrate the effectiveness of our proposed STOA-VLP\n(e.g. 3.7 Rouge-L improvements on MSR-VTT video captioning benchmark, 2.9%\naccuracy improvements on MSVD video question answering benchmark, compared to\nprevious approaches).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Weihong Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Mao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Duyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Heng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiaocheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weighted Sampling for Masked Language Modeling. (arXiv:2302.14225v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.14225","description":"<p>Masked Language Modeling (MLM) is widely used to pretrain language models.\nThe standard random masking strategy in MLM causes the pre-trained language\nmodels (PLMs) to be biased toward high-frequency tokens. Representation\nlearning of rare tokens is poor and PLMs have limited performance on downstream\ntasks. To alleviate this frequency bias issue, we propose two simple and\neffective Weighted Sampling strategies for masking tokens based on the token\nfrequency and training loss. We apply these two strategies to BERT and obtain\nWeighted-Sampled BERT (WSBERT). Experiments on the Semantic Textual Similarity\nbenchmark (STS) show that WSBERT significantly improves sentence embeddings\nover BERT. Combining WSBERT with calibration methods and prompt learning\nfurther improves sentence embeddings. We also investigate fine-tuning WSBERT on\nthe GLUE benchmark and show that Weighted Sampling also improves the transfer\nlearning capability of the backbone PLM. We further analyze and provide\ninsights into how WSBERT improves token embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_K/0/1/0/all/0/1\">Kongzhang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Technical report: Graph Neural Networks go Grammatical. (arXiv:2303.01590v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2303.01590","description":"<p>This paper proposes a framework to formally link a fragment of an algebraic\nlanguage to a Graph Neural Network (GNN). It relies on Context Free Grammars\n(CFG) to organise algebraic operations into generative rules that can be\ntranslated into a GNN layer model. Since the rules and variables of a CFG\ndirectly derived from a language contain redundancies, a grammar reduction\nscheme is presented making tractable the translation into a GNN layer. Applying\nthis strategy, a grammar compliant with the third-order Weisfeiler-Lehman\n(3-WL) test is defined from MATLANG. From this 3-WL CFG, we derive a provably\n3-WL GNN model called G$^2$N$^2$. Moreover, this grammatical approach allows us\nto provide algebraic formulas to count the cycles of length up to six and\nchordal cycles at the edge level, which enlightens the counting power of 3-WL.\nSeveral experiments illustrate that G$^2$N$^2$ efficiently outperforms other\n3-WL GNNs on many downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Piquenot_J/0/1/0/all/0/1\">Jason Piquenot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moscatelli_A/0/1/0/all/0/1\">Aldo Moscatelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berar_M/0/1/0/all/0/1\">Maxime B&#xe9;rar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heroux_P/0/1/0/all/0/1\">Pierre H&#xe9;roux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+raveaux_R/0/1/0/all/0/1\">Romain raveaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramel_J/0/1/0/all/0/1\">Jean-Yves Ramel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adam_S/0/1/0/all/0/1\">S&#xe9;bastien Adam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translate your gibberish: black-box adversarial attack on machine translation systems. (arXiv:2303.10974v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.10974","description":"<p>Neural networks are deployed widely in natural language processing tasks on\nthe industrial scale, and perhaps the most often they are used as compounds of\nautomatic machine translation systems. In this work, we present a simple\napproach to fool state-of-the-art machine translation tools in the task of\ntranslation from Russian to English and vice versa. Using a novel black-box\ngradient-free tensor-based optimizer, we show that many online translation\ntools, such as Google, DeepL, and Yandex, may both produce wrong or offensive\ntranslations for nonsensical adversarial input queries and refuse to translate\nseemingly benign input phrases. This vulnerability may interfere with\nunderstanding a new language and simply worsen the user's experience while\nusing machine translation systems, and, hence, additional improvements of these\ntools are required to establish better translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chertkov_A/0/1/0/all/0/1\">Andrei Chertkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsymboi_O/0/1/0/all/0/1\">Olga Tsymboi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pautov_M/0/1/0/all/0/1\">Mikhail Pautov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1\">Ivan Oseledets</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InterviewBot: Real-Time End-to-End Dialogue System to Interview Students for College Admission. (arXiv:2303.15049v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.15049","description":"<p>We present the InterviewBot that dynamically integrates conversation history\nand customized topics into a coherent embedding space to conduct 10 mins\nhybrid-domain (open and closed) conversations with foreign students applying to\nU.S. colleges for assessing their academic and cultural readiness. To build a\nneural-based end-to-end dialogue model, 7,361 audio recordings of\nhuman-to-human interviews are automatically transcribed, where 440 are manually\ncorrected for finetuning and evaluation. To overcome the input/output size\nlimit of a transformer-based encoder-decoder model, two new methods are\nproposed, context attention and topic storing, allowing the model to make\nrelevant and consistent interactions. Our final model is tested both\nstatistically by comparing its responses to the interview data and dynamically\nby inviting professional interviewers and various students to interact with it\nin real-time, finding it highly satisfactory in fluency and context awareness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keyes_N/0/1/0/all/0/1\">Nathan Keyes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crawford_T/0/1/0/all/0/1\">Terry Crawford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment. (arXiv:2303.16634v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.16634","description":"<p>The quality of texts generated by natural language generation (NLG) systems\nis hard to measure automatically. Conventional reference-based metrics, such as\nBLEU and ROUGE, have been shown to have relatively low correlation with human\njudgments, especially for tasks that require creativity and diversity. Recent\nstudies suggest using large language models (LLMs) as reference-free metrics\nfor NLG evaluation, which have the benefit of being applicable to new tasks\nthat lack human references. However, these LLM-based evaluators still have\nlower human correspondence than medium-size neural evaluators. In this work, we\npresent G-Eval, a framework of using large language models with\nchain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of\nNLG outputs. We experiment with two generation tasks, text summarization and\ndialogue generation. We show that G-Eval with GPT-4 as the backbone model\nachieves a Spearman correlation of 0.514 with human on summarization task,\noutperforming all previous methods by a large margin. We also propose\npreliminary analysis on the behavior of LLM-based evaluators, and highlight the\npotential issue of LLM-based evaluators having a bias towards the LLM-generated\ntexts. The code is at https://github.com/nlpyang/geval\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iter_D/0/1/0/all/0/1\">Dan Iter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruochen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data. (arXiv:2304.01196v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01196","description":"<p>Chat models, such as ChatGPT, have shown impressive capabilities and have\nbeen rapidly adopted across numerous domains. However, these models are only\naccessible through a restricted API, creating barriers for new research and\nprogress in the field. We propose a pipeline that can automatically generate a\nhigh-quality multi-turn chat corpus by leveraging ChatGPT to engage in a\nconversation with itself. Subsequently, we employ parameter-efficient tuning to\nenhance LLaMA, an open-source large language model. The resulting model, named\nBaize, demonstrates good performance in multi-turn dialogues with guardrails\nthat minimize potential risks. Furthermore, we propose a new technique called\nSelf-Distill with Feedback, to further improve the performance of the Baize\nmodels with feedback from ChatGPT. The Baize models and data are released for\nresearch purposes only at https://github.com/project-baize/baize-chatbot. An\nonline demo is also available at\nhttps://huggingface.co/spaces/project-baize/chat-with-baize.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Daya Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models. (arXiv:2304.09842v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.09842","description":"<p>Large language models (LLMs) have achieved remarkable progress in solving\nvarious natural language processing tasks due to emergent reasoning abilities.\nHowever, LLMs have inherent limitations as they are incapable of accessing\nup-to-date information (stored on the Web or in task-specific knowledge bases),\nusing external tools, and performing precise mathematical and logical\nreasoning. In this paper, we present Chameleon, an AI system that mitigates\nthese limitations by augmenting LLMs with plug-and-play modules for\ncompositional reasoning. Chameleon synthesizes programs by composing various\ntools (e.g., LLMs, off-the-shelf vision models, web search engines, Python\nfunctions, and heuristic-based modules) for accomplishing complex reasoning\ntasks. At the heart of Chameleon is an LLM-based planner that assembles a\nsequence of tools to execute to generate the final response. We showcase the\neffectiveness of Chameleon on two multi-modal knowledge-intensive reasoning\ntasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54%\noverall accuracy on ScienceQA, improving the best published few-shot result by\n11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%,\nlifting the state of the art to 98.78%. Our analysis also shows that the\nGPT-4-powered planner exhibits more consistent and rational tool selection via\ninferring potential constraints from instructions, compared to a\nChatGPT-powered planner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1\">Michel Galley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why Does ChatGPT Fall Short in Providing Truthful Answers?. (arXiv:2304.10513v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.10513","description":"<p>Recent advancements in Large Language Models, such as ChatGPT, have\ndemonstrated significant potential to impact various aspects of human life.\nHowever, ChatGPT still faces challenges in aspects like truthfulness, e.g.\nproviding accurate and reliable outputs. Therefore, in this paper, we seek to\nunderstand why ChatGPT falls short in providing truthful answers. For this\npurpose, we first analyze the failures of ChatGPT in complex open-domain\nquestion answering and identifies the abilities under the failures.\nSpecifically, we categorize ChatGPT's failures into four types: comprehension,\nfactualness, specificity, and inference. We further pinpoint three critical\nabilities associated with QA failures: knowledge memorization, knowledge\nrecall, and knowledge reasoning. Additionally, we conduct experiments centered\non these abilities and propose potential approaches to enhance truthfulness.\nThe results indicate that furnishing the model with fine-grained external\nknowledge, hints for knowledge recall, and guidance for reasoning can empower\nthe model to answer questions more truthfully.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions. (arXiv:2304.14402v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.14402","description":"<p>Large language models (LLMs) with instruction fine-tuning demonstrate\nsuperior generative capabilities. However, these models are resource-intensive.\nTo alleviate this issue, we explore distilling knowledge from instruction-tuned\nLLMs into much smaller ones. To this end, we carefully develop a large set of\n2.58M instructions based on both existing and newly-generated instructions. In\naddition to being sizable, we design our instructions to cover a broad set of\ntopics to ensure diversity. Extensive analysis of our instruction dataset\nconfirms its diversity, and we generate responses for these instructions using\ngpt-3.5-turbo. Leveraging these instructions, we fine-tune a diverse herd of\nmodels, collectively referred to as LaMini-LM, which includes models from both\nthe encoder-decoder and decoder-only families, with varying sizes. We evaluate\nthe performance of our models using automatic metrics on 15 different natural\nlanguage processing (NLP) benchmarks, as well as through human assessment. The\nresults demonstrate that our proposed LaMini-LM models are comparable to\ncompetitive baselines, while being nearly 10 times smaller in size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waheed_A/0/1/0/all/0/1\">Abdul Waheed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment. (arXiv:2305.04476v4 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2305.04476","description":"<p>The speech-to-singing (STS) voice conversion task aims to generate singing\nsamples corresponding to speech recordings while facing a major challenge: the\nalignment between the target (singing) pitch contour and the source (speech)\ncontent is difficult to learn in a text-free situation. This paper proposes\nAlignSTS, an STS model based on explicit cross-modal alignment, which views\nspeech variance such as pitch and content as different modalities. Inspired by\nthe mechanism of how humans will sing the lyrics to the melody, AlignSTS: 1)\nadopts a novel rhythm adaptor to predict the target rhythm representation to\nbridge the modality gap between content and pitch, where the rhythm\nrepresentation is computed in a simple yet effective way and is quantized into\na discrete space; and 2) uses the predicted rhythm representation to re-align\nthe content based on cross-attention and conducts a cross-modal fusion for\nre-synthesize. Extensive experiments show that AlignSTS achieves superior\nperformance in terms of both objective and subjective metrics. Audio samples\nare available at https://alignsts.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_R/0/1/0/all/0/1\">Ruiqi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_R/0/1/0/all/0/1\">Rongjie Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lichao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jinglin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain-of-Dictionary Prompting Elicits Translation in Large Language Models. (arXiv:2305.06575v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.06575","description":"<p>Large language models (LLMs) have shown surprisingly good performance in\nmultilingual neural machine translation (MNMT) even when trained without\nparallel data. Yet, despite the fact that the amount of training data is\ngigantic, they still struggle with translating rare words, particularly for\nlow-resource languages. Even worse, it is usually unrealistic to retrieve\nrelevant demonstrations for in-context learning with low-resource languages on\nLLMs, which restricts the practical use of LLMs for translation -- how should\nwe mitigate this problem? To this end, we present a novel method, CoD, which\naugments LLMs with prior knowledge with the chains of multilingual dictionaries\nfor a subset of input words to elicit translation abilities for LLMs. Extensive\nexperiments indicate that augmenting ChatGPT with CoD elicits large gains by up\nto 13x chrF++ points for MNMT (3.08 to 42.63 for English to Serbian written in\nCyrillic script) on FLORES-200 full devtest set. We further demonstrate the\nimportance of chaining the multilingual dictionaries, as well as the\nsuperiority of CoD to few-shot demonstration for low-resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hongyuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haoyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haoran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When the Majority is Wrong: Modeling Annotator Disagreement for Subjective Tasks. (arXiv:2305.06626v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.06626","description":"<p>Though majority vote among annotators is typically used for ground truth\nlabels in natural language processing, annotator disagreement in tasks such as\nhate speech detection may reflect differences in opinion across groups, not\nnoise. Thus, a crucial problem in hate speech detection is determining whether\na statement is offensive to the demographic group that it targets, when that\ngroup may constitute a small fraction of the annotator pool. We construct a\nmodel that predicts individual annotator ratings on potentially offensive text\nand combines this information with the predicted target group of the text to\nmodel the opinions of target group members. We show gains across a range of\nmetrics, including raising performance over the baseline by 22% at predicting\nindividual annotators' ratings and by 33% at predicting variance among\nannotators, which provides a metric for model uncertainty downstream. We find\nthat annotator ratings can be predicted using their demographic information and\nopinions on online content, without the need to track identifying annotator IDs\nthat link each annotator to their ratings. We also find that use of\nnon-invasive survey questions on annotators' online experiences helps to\nmaximize privacy and minimize unnecessary collection of demographic information\nwhen predicting annotators' opinions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fleisig_E/0/1/0/all/0/1\">Eve Fleisig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abebe_R/0/1/0/all/0/1\">Rediet Abebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better speech synthesis through scaling. (arXiv:2305.07243v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2305.07243","description":"<p>In recent years, the field of image generation has been revolutionized by the\napplication of autoregressive transformers and DDPMs. These approaches model\nthe process of image generation as a step-wise probabilistic processes and\nleverage large amounts of compute and data to learn the image distribution.\nThis methodology of improving performance need not be confined to images. This\npaper describes a way to apply advances in the image generative domain to\nspeech synthesis. The result is TorToise -- an expressive, multi-voice\ntext-to-speech system.\n</p>\n<p>All model code and trained weights have been open-sourced at\nhttps://github.com/neonbjb/tortoise-tts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Betker_J/0/1/0/all/0/1\">James Betker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining. (arXiv:2305.10429v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10429","description":"<p>The mixture proportions of pretraining data domains (e.g., Wikipedia, books,\nweb text) greatly affect language model (LM) performance. In this paper, we\npropose Domain Reweighting with Minimax Optimization (DoReMi), which first\ntrains a small proxy model using group distributionally robust optimization\n(Group DRO) over domains to produce domain weights (mixture proportions)\nwithout knowledge of downstream tasks. We then resample a dataset with these\ndomain weights and train a larger, full-sized model. In our experiments, we use\nDoReMi on a 280M-parameter proxy model to find domain weights for training an\n8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves\nperplexity across all domains, even when it downweights a domain. DoReMi\nimproves average few-shot downstream accuracy by 6.5% points over a baseline\nmodel trained using The Pile's default domain weights and reaches the baseline\naccuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has\nno knowledge of downstream tasks, even matches the performance of using domain\nweights tuned on downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sang Michael Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1\">Hieu Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xuanyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1\">Nan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hanxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yifeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Adams Wei Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Web Can Be Your Oyster for Improving Large Language Models. (arXiv:2305.10998v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10998","description":"<p>Large language models (LLMs) encode a large amount of world knowledge.\nHowever, as such knowledge is frozen at the time of model training, the models\nbecome static and limited by the training data at that time. In order to\nfurther improve the capacity of LLMs for knowledge-intensive tasks, we consider\naugmenting LLMs with the large-scale web using search engine. Unlike previous\naugmentation sources (e.g., Wikipedia data dump), the web provides broader,\nmore comprehensive and constantly updated information. In this paper, we\npresent a web-augmented LLM UNIWEB, which is trained over 16\nknowledge-intensive tasks in a unified text-to-text format. Instead of simply\nusing the retrieved contents from web, our approach has made two major\nimprovements. Firstly, we propose an adaptive search engine assisted learning\nmethod that can self-evaluate the confidence level of LLM's predictions, and\nadaptively determine when to refer to the web for more data, which can avoid\nuseless or noisy augmentation from web. Secondly, we design a pretraining task,\ni.e., continual knowledge learning, based on salient spans prediction, to\nreduce the discrepancy between the encoded and retrieved knowledge. Experiments\non a wide range of knowledge-intensive tasks show that our model significantly\noutperforms previous retrieval-augmented methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evidence of Meaning in Language Models Trained on Programs. (arXiv:2305.11169v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.11169","description":"<p>We present evidence that language models can learn meaning despite being\ntrained only to perform next token prediction on text, specifically a corpus of\nprograms. Each program is preceded by a specification in the form of (textual)\ninput-output examples. Working with programs enables us to precisely define\nconcepts relevant to meaning in language (e.g., correctness and semantics),\nmaking program synthesis well-suited as an intermediate testbed for\ncharacterizing the presence (or absence) of meaning in language models.\n</p>\n<p>We first train a Transformer model on the corpus of programs, then probe the\ntrained model's hidden states as it completes a program given a specification.\nDespite providing no inductive bias toward learning the semantics of the\nlanguage, we find that a linear probe is able to extract abstractions of both\ncurrent and future program states from the model states. Moreover, there is a\nstrong, statistically significant correlation between the accuracy of the probe\nand the model's ability to generate a program that implements the\nspecification. To evaluate whether the semantics are represented in the model\nstates rather than learned by the probe, we design a novel experimental\nprocedure that intervenes on the semantics of the language while preserving the\nlexicon and syntax. We also demonstrate that the model learns to generate\ncorrect programs that are, on average, shorter than those in the training set,\nwhich is evidence that language model outputs may differ from the training\ndistribution in semantically meaningful ways. In summary, this paper does not\npropose any new techniques for training language models, but develops an\nexperimental framework for and provides insights into the acquisition and\nrepresentation of (formal) meaning in language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Charles Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rinard_M/0/1/0/all/0/1\">Martin Rinard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards the Automatic Generation of Conversational Interfaces to Facilitate the Exploration of Tabular Data. (arXiv:2305.11326v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.11326","description":"<p>Tabular data is the most common format to publish and exchange structured\ndata online. A clear example is the growing number of open data portals\npublished by all types of public administrations. However, exploitation of\nthese data sources is currently limited to technical people able to\nprogrammatically manipulate and digest such data. As an alternative, we propose\nthe use of chatbots to offer a conversational interface to facilitate the\nexploration of tabular data sources. With our approach, any regular citizen can\nbenefit and leverage them. Moreover, our chatbots are not manually created:\ninstead, they are automatically generated from the data source itself thanks to\nthe instantiation of a configurable collection of conversation patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_M/0/1/0/all/0/1\">Marcos Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabot_J/0/1/0/all/0/1\">Jordi Cabot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clariso_R/0/1/0/all/0/1\">Robert Claris&#xf3;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.11490","description":"<p>Building on the recent remarkable development of large language models\n(LLMs), active attempts are being made to extend the utility of LLMs to\nmultimodal tasks. There have been previous efforts to link language and visual\ninformation, and attempts to add visual capabilities to LLMs are ongoing as\nwell. However, existing attempts use LLMs only as image decoders and no attempt\nhas been made to generate images in the same line as the natural language. By\nadopting a VQ-GAN framework in which latent representations of images are\ntreated as a kind of text tokens, we present a novel method to fine-tune a\npre-trained LLM to read and generate images like text without any structural\nchanges, extra training objectives, or the need for training an ad-hoc network\nwhile still preserving the of the instruction-following capability of the LLM.\nWe apply this framework to chest X-ray (CXR) image and report generation tasks\nas it is a domain in which translation of complex information between visual\nand language domains is important. The code is available at\nhttps://github.com/hyn2028/llm-cxr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Suhyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1\">Won Jun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages. (arXiv:2305.11938v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.11938","description":"<p>Data scarcity is a crucial issue for the development of highly multilingual\nNLP systems. Yet for many under-represented languages (ULs) -- languages for\nwhich NLP re-search is particularly far behind in meeting user needs -- it is\nfeasible to annotate small amounts of data. Motivated by this, we propose\nXTREME-UP, a benchmark defined by: its focus on the scarce-data scenario rather\nthan zero-shot; its focus on user-centric tasks -- tasks with broad adoption by\nspeakers of high-resource languages; and its focus on under-represented\nlanguages where this scarce-data scenario tends to be most realistic. XTREME-UP\nevaluates the capabilities of language models across 88 under-represented\nlanguages over 9 key user-centric technologies including ASR, OCR, MT, and\ninformation access tasks that are of general utility. We create new datasets\nfor OCR, autocomplete, semantic parsing, and transliteration, and build on and\nrefine existing datasets for other tasks. XTREME-UP provides methodology for\nevaluating many modeling scenarios including text-only, multi-modal (vision,\naudio, and text),supervised parameter tuning, and in-context learning. We\nevaluate commonly used models on the benchmark. We release all code and scripts\nto train and evaluate models\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jonathan H. Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutkin_A/0/1/0/all/0/1\">Alexander Gutkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1\">Mihir Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Min Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicosia_M/0/1/0/all/0/1\">Massimo Nicosia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijhwani_S/0/1/0/all/0/1\">Shruti Rijhwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riley_P/0/1/0/all/0/1\">Parker Riley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarr_J/0/1/0/all/0/1\">Jean-Michel A. Sarr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieting_J/0/1/0/all/0/1\">John Wieting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1\">Nitish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katanova_A/0/1/0/all/0/1\">Anna Katanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirov_C/0/1/0/all/0/1\">Christo Kirov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dickinson_D/0/1/0/all/0/1\">Dana L. Dickinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roark_B/0/1/0/all/0/1\">Brian Roark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samanta_B/0/1/0/all/0/1\">Bidisha Samanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Connie Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David I. Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Axelrod_V/0/1/0/all/0/1\">Vera Axelrod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caswell_I/0/1/0/all/0/1\">Isaac Caswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherry_C/0/1/0/all/0/1\">Colin Cherry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrette_D/0/1/0/all/0/1\">Dan Garrette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingle_R/0/1/0/all/0/1\">Reeve Ingle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Melvin Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panteleev_D/0/1/0/all/0/1\">Dmitry Panteleev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TheoremQA: A Theorem-driven Question Answering dataset. (arXiv:2305.12524v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.12524","description":"<p>The recent LLMs like GPT-4 and PaLM-2 have made tremendous progress in\nsolving fundamental math problems like GSM8K by achieving over 90% accuracy.\nHowever, their capabilities to solve more challenging math problems which\nrequire domain-specific knowledge (i.e. theorem) have yet to be investigated.\nIn this paper, we introduce TheoremQA, the first theorem-driven\nquestion-answering dataset designed to evaluate AI models' capabilities to\napply theorems to solve challenging science problems. TheoremQA is curated by\ndomain experts containing 800 high-quality questions covering 350 theorems\n(e.g. Taylor's theorem, Lagrange's theorem, Huffman coding, Quantum Theorem,\nElasticity Theorem, etc) from Math, Physics, EE&amp;CS, and Finance. We evaluate a\nwide spectrum of 16 large language and code models with different prompting\nstrategies like Chain-of-Thoughts and Program-of-Thoughts. We found that\nGPT-4's capabilities to solve these problems are unparalleled, achieving an\naccuracy of 51% with Program-of-Thoughts Prompting. All the existing\nopen-sourced models are below 15%, barely surpassing the random-guess baseline.\nGiven the diversity and broad coverage of TheoremQA, we believe it can be used\nas a better benchmark to evaluate LLMs' capabilities to solve challenging\nscience problems. The data and code are released in\nhttps://github.com/wenhuchen/TheoremQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Ming Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ku_M/0/1/0/all/0/1\">Max Ku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yixin Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xueguang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jianyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1\">Tony Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering. (arXiv:2305.12820v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.12820","description":"<p>Recent advances in tabular question answering (QA) with large language models\nare constrained in their coverage and only answer questions over a single\ntable. However, real-world queries are complex in nature, often over multiple\ntables in a relational database or web page. Single table questions do not\ninvolve common table operations such as set operations, Cartesian products\n(joins), or nested queries. Furthermore, multi-table operations often result in\na tabular output, which necessitates table generation capabilities of tabular\nQA models. To fill this gap, we propose a new task of answering questions over\nmultiple tables. Our model, MultiTabQA, not only answers questions over\nmultiple tables, but also generalizes to generate tabular answers. To enable\neffective training, we build a pre-training dataset comprising of 132,645 SQL\nqueries and tabular answers. Further, we evaluate the generated tables by\nintroducing table-specific metrics of varying strictness assessing various\nlevels of granularity of the table structure. MultiTabQA outperforms\nstate-of-the-art single table QA models adapted to a multi-table QA setting by\nfinetuning on three datasets: Spider, Atis and GeoQuery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pal_V/0/1/0/all/0/1\">Vaishali Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1\">Andrew Yates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanoulas_E/0/1/0/all/0/1\">Evangelos Kanoulas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Large Language Models emulate an inductive Thematic Analysis of semi-structured interviews? An exploration and provocation on the limits of the approach and the model. (arXiv:2305.13014v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13014","description":"<p>Large Language Models (LLMs) have emerged as powerful generative Artificial\nIntelligence solutions which can be applied to several fields and areas of\nwork. This paper presents results and reflection of an experiment done to use\nthe model GPT 3.5-Turbo to emulate some aspects of an inductive Thematic\nAnalysis. Previous research on this subject has largely worked on conducting\ndeductive analysis. Thematic Analysis is a qualitative method for analysis\ncommonly used in social sciences and it is based on interpretations made by the\nhuman analyst(s) and the identification of explicit and latent meanings in\nqualitative data. Attempting an analysis based on human interpretation with an\nLLM clearly is a provocation but also a way to learn something about how these\nsystems can or cannot be used in qualitative research. The paper presents the\nmotivations for attempting this emulation, it reflects on how the six steps to\na Thematic Analysis proposed by Braun and Clarke can at least partially be\nreproduced with the LLM and it also reflects on what are the outputs produced\nby the model. The paper used two existing datasets of open access\nsemi-structured interviews, previously analysed with Thematic Analysis by other\nresearchers. It used the previously produced analysis (and the related themes)\nto compare with the results produced by the LLM. The results show that the\nmodel can infer at least partially some of the main Themes. The objective of\nthe paper is not to replace human analysts in qualitative analysis but to learn\nif some elements of LLM data manipulation can to an extent be of support for\nqualitative research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paoli_S/0/1/0/all/0/1\">Stefano De Paoli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AVeriTeC: A Dataset for Real-world Claim Verification with Evidence from the Web. (arXiv:2305.13117v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13117","description":"<p>Existing datasets for automated fact-checking have substantial limitations,\nsuch as relying on artificial claims, lacking annotations for evidence and\nintermediate reasoning, or including evidence published after the claim. In\nthis paper we introduce AVeriTeC, a new dataset of 4,568 real-world claims\ncovering fact-checks by 50 different organizations. Each claim is annotated\nwith question-answer pairs supported by evidence available online, as well as\ntextual justifications explaining how the evidence combines to produce a\nverdict. Through a multi-round annotation process, we avoid common pitfalls\nincluding context dependence, evidence insufficiency, and temporal leakage, and\nreach a substantial inter-annotator agreement of $\\kappa=0.619$ on verdicts. We\ndevelop a baseline as well as an evaluation scheme for verifying claims through\nseveral question-answering steps against the open web.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1\">Michael Schlichtkrull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhijiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Clashes. (arXiv:2305.13300v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13300","description":"<p>By providing external information to large language models (LLMs), tool\naugmentation (including retrieval augmentation) has emerged as a promising\nsolution for addressing the limitations of LLMs' static parametric memory.\nHowever, how receptive are LLMs to such external evidence, especially when the\nevidence conflicts with their parametric memory? We present the first\ncomprehensive and controlled investigation into the behavior of LLMs when\nencountering knowledge conflicts. We propose a systematic framework to elicit\nhigh-quality parametric memory from LLMs and construct the corresponding\ncounter-memory, which enables us to conduct a series of controlled experiments.\nOur investigation reveals seemingly contradicting behaviors of LLMs. On the one\nhand, different from prior wisdom, we find that LLMs can be highly receptive to\nexternal evidence even when that conflicts with their parametric memory, given\nthat the external evidence is coherent and convincing. On the other hand, LLMs\nalso demonstrate a strong confirmation bias when the external evidence contains\nsome information that is consistent with their parametric memory, despite being\npresented with conflicting evidence at the same time. These results pose\nimportant implications that are worth careful consideration for the further\ndevelopment and deployment of tool- and retrieval-augmented LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiangjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_R/0/1/0/all/0/1\">Renze Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference. (arXiv:2305.13484v2 [cs.DC] UPDATED)","link":"http://arxiv.org/abs/2305.13484","description":"<p>In the rapidly evolving field of deep learning, the performance of model\ninference has become a pivotal aspect as models become more complex and are\ndeployed in diverse applications. Among these, autoregressive models stand out\ndue to their state-of-the-art performance in numerous generative tasks. These\nmodels, by design, harness a temporal dependency structure, where the current\ntoken's probability distribution is conditioned on preceding tokens. This\ninherently sequential characteristic, however, adheres to the Markov Chain\nassumption and lacks temporal parallelism, which poses unique challenges.\nParticularly in industrial contexts where inference requests, following a\nPoisson time distribution, necessitate diverse response lengths, this absence\nof parallelism is more profound. Existing solutions, such as dynamic batching\nand concurrent model instances, nevertheless, come with severe overheads and a\nlack of flexibility, these coarse-grained methods fall short of achieving\noptimal latency and throughput. To address these shortcomings, we propose\nFlavor -- a temporal fusion framework for efficient inference in autoregressive\nmodels, eliminating the need for heuristic settings and applies to a wide range\nof inference scenarios. By providing more fine-grained parallelism on the\ntemporality of requests and employing an efficient memory shuffle algorithm,\nFlover achieves up to 11x faster inference on GPT models compared to the\ncutting-edge solutions provided by NVIDIA Triton FasterTransformer. Crucially,\nby leveraging the advanced tensor parallel technique, Flover proves efficacious\nacross diverse computational landscapes, from single-GPU setups to multi-node\nscenarios, thereby offering robust performance optimization that transcends\nhardware boundaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jinghan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alnaasan_N/0/1/0/all/0/1\">Nawras Alnaasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafi_A/0/1/0/all/0/1\">Aamir Shafi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramoni_H/0/1/0/all/0/1\">Hari Subramoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K%2E_D/0/1/0/all/0/1\">Dhabaleswar K.</a> (DK) <a href=\"http://arxiv.org/find/cs/1/au:+Panda/0/1/0/all/0/1\">Panda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Automated Fact-Checking: A Survey. (arXiv:2305.13507v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13507","description":"<p>Misinformation, i.e. factually incorrect information, is often conveyed in\nmultiple modalities, e.g. an image accompanied by a caption. It is perceived as\nmore credible by humans, and spreads faster and wider than its text-only\ncounterparts. While an increasing body of research investigates automated\nfact-checking (AFC), previous surveys mostly focus on textual misinformation.\nIn this survey, we conceptualise a framework for AFC including subtasks unique\nto multimodal misinformation. Furthermore, we discuss related terminological\ndeveloped in different communities in the context of our framework. We focus on\nfour modalities prevalent in real-world fact-checking: text, image, audio, and\nvideo. We survey benchmarks and models, and discuss limitations and promising\ndirections for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mubashara_A/0/1/0/all/0/1\">Akhtar Mubashara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michael_S/0/1/0/all/0/1\">Schlichtkrull Michael</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhijiang_G/0/1/0/all/0/1\">Guo Zhijiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oana_C/0/1/0/all/0/1\">Cocarascu Oana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elena_S/0/1/0/all/0/1\">Simperl Elena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_V/0/1/0/all/0/1\">Vlachos Andreas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better Low-Resource Entity Recognition Through Translation and Annotation Fusion. (arXiv:2305.13582v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13582","description":"<p>Pre-trained multilingual language models have enabled significant\nadvancements in cross-lingual transfer. However, these models often exhibit a\nperformance disparity when transferring from high-resource languages to\nlow-resource languages, especially for languages that are underrepresented or\nnot in the pre-training data. Motivated by the superior performance of these\nmodels on high-resource languages compared to low-resource languages, we\nintroduce a Translation-and-fusion framework, which translates low-resource\nlanguage text into a high-resource language for annotation using fully\nsupervised models before fusing the annotations back into the low-resource\nlanguage. Based on this framework, we present TransFusion, a model trained to\nfuse predictions from a high-resource language to make robust predictions on\nlow-resource languages. We evaluate our methods on two low-resource named\nentity recognition (NER) datasets, MasakhaNER2.0 and LORELEI NER, covering 25\nlanguages, and show consistent improvement up to +16 F$_1$ over English\nfine-tuning systems, achieving state-of-the-art performance compared to\nTranslate-train systems. Our analysis depicts the unique advantages of the\nTransFusion method which is robust to translation errors and source language\nprediction errors, and complimentary to adapted multilingual language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_V/0/1/0/all/0/1\">Vedaant Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IdEALS: Idiomatic Expressions for Advancement of Language Skills. (arXiv:2305.13637v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13637","description":"<p>Although significant progress has been made in developing methods for\nGrammatical Error Correction (GEC), addressing word choice improvements has\nbeen notably lacking and enhancing sentence expressivity by replacing phrases\nwith advanced expressions is an understudied aspect. In this paper, we focus on\nthis area and present our investigation into the task of incorporating the\nusage of idiomatic expressions in student writing. To facilitate our study, we\ncurate extensive training sets and expert-annotated testing sets using\nreal-world data and evaluate various approaches and compare their performance\nagainst human experts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ri_N/0/1/0/all/0/1\">Narutatsu Ri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bill Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davidson_S/0/1/0/all/0/1\">Sam Davidson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models. (arXiv:2305.13718v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13718","description":"<p>Existing efforts to improve logical reasoning ability of language models have\npredominantly relied on supervised fine-tuning, hindering generalization to new\ndomains and/or tasks. The development of Large Langauge Models (LLMs) has\ndemonstrated the capacity of compressing abundant knowledge into a single\nproxy, enabling them to tackle multiple tasks effectively. Our preliminary\nexperiments, nevertheless, show that LLMs do not show capability on logical\nreasoning. The performance of LLMs on logical reasoning benchmarks is far\nbehind the existing state-of-the-art baselines. In this paper, we make the\nfirst attempt to investigate the feasibility of incorporating logical knowledge\nthrough self-supervised post-training, and activating it via in-context\nlearning, which we termed as LogicLLM. Specifically, we devise an\nauto-regressive objective variant of MERIt and integrate it with two LLM\nseries, i.e., FLAN-T5 and LLaMA, with parameter size ranging from 3 billion to\n13 billion. The results on two challenging logical reasoning benchmarks\ndemonstrate the effectiveness of LogicLLM. Besides, we conduct extensive\nablation studies to analyze the key factors in designing logic-oriented proxy\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_F/0/1/0/all/0/1\">Fangkai Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1\">Zhiyang Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1\">Bosheng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13971","description":"<p>LLMs have shown impressive few-shot performance across many tasks. However,\nthey still struggle when it comes to reliably generating complex output\nstructures, such as those required for information extraction. This limitation\nstems from the fact that LLMs, without fine-tuning, tend to generate free text\nrather than structures precisely following a specific grammar. In this work, we\npropose to enrich the decoding with formal grammar constraints. More\nconcretely, given Context-Free Grammar(CFG), our framework ensures that the\ntoken generated in each decoding step would lead to a valid continuation\ncompliant with the grammar production rules. This process guarantees the\ngeneration of valid sequences. Importantly, our framework can be readily\ncombined with any CFG or decoding algorithm. We demonstrate that the outputs of\nmany NLP tasks can be represented as formal languages, making them suitable for\ndirect use in our framework. We conducted experiments with two challenging\ntasks involving large alphabets in their grammar (Wikidata entities and\nrelations): information extraction and entity disambiguation. Our results with\nLLaMA models indicate that grammar-constrained decoding substantially\noutperforms unconstrained decoding and even competes with task-specific\nfine-tuned models. These findings suggest that integrating grammar-based\nconstraints during decoding holds great promise in making LLMs reliably produce\nstructured outputs, especially in setting where training data is scarce and\nfine-tuning is expensive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Saibo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Josifosky_M/0/1/0/all/0/1\">Martin Josifosky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1\">Maxime Peyrard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One-stop Training of Multiple Capacity Models. (arXiv:2305.14066v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14066","description":"<p>Training models with varying capacities can be advantageous for deploying\nthem in different scenarios. While high-capacity models offer better\nperformance, low-capacity models require fewer computing resources for training\nand inference. In this work, we propose a novel one-stop training framework to\njointly train high-capacity and low-capactiy models. This framework consists of\ntwo composite model architectures and a joint training algorithm called\nTwo-Stage Joint-Training (TSJT). Unlike knowledge distillation, where multiple\ncapacity models are trained from scratch separately, our approach integrates\nsupervisions from different capacity models simultaneously, leading to faster\nand more efficient convergence. Extensive experiments on the multilingual\nmachine translation benchmark WMT10 show that our method outperforms\nlow-capacity baseline models and achieves comparable or better performance on\nhigh-capacity models. Notably, the analysis demonstrates that our method\nsignificantly influences the initial training process, leading to more\nefficient convergence and superior solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haoyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Rui Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Acceptability Judgements. (arXiv:2305.14091v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14091","description":"<p>Years have passed since the NLP community has last focused on linguistic\nacceptability. In this work, we revisit this topic in the context of large\nlanguage models. We introduce CoLAC - Corpus of Linguistic Acceptability in\nChinese, the first large-scale non-English acceptability dataset that is\nverified by native speakers and comes with two sets of labels. Our experiments\nshow that even the largest InstructGPT model performs only at chance level on\nCoLAC, while ChatGPT's performance (48.30 MCC) is also way below supervised\nmodels (59.03 MCC) and human (65.11 MCC). Through cross-lingual transfer\nexperiments and fine-grained linguistic analysis, we demonstrate for the first\ntime that knowledge of linguistic acceptability can be transferred across\ntypologically distinct languages, as well as be traced back to pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hai Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziyin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Weifang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1\">Jackie Yan-Ki Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Aini Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yina Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiahui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Old is GPT?: The HumBEL Framework for Evaluating Language Models using Human Demographic Data. (arXiv:2305.14195v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14195","description":"<p>While large pre-trained language models (LMs) find greater use across NLP,\nexisting evaluation protocols do not consider how LM language use aligns with\nparticular human demographic groups, which can be an important consideration in\nconversational AI applications. To remedy this gap, we consider how LM language\nskills can be measured and compared to human sub-populations. We suggest\nclinical techniques from Speech Language Pathology, which has well-established\nnorms for acquisition of language skills, organized by (human) age. We conduct\nevaluation with a domain expert (i.e., a clinically licensed speech language\npathologist), and also propose automated techniques to substitute clinical\nevaluation at scale. We find LM capability varies widely depending on task with\nGPT-3.5 mimicking the ability of a typical 6-9 year old at tasks requiring\ninference about word meanings and simultaneously outperforming a typical 21\nyear old at memorization. GPT-3.5 (InstructGPT) also has trouble with social\nlanguage use, exhibiting less than 50\\% of the tested pragmatic skills. It\nshows errors in understanding particular word parts-of-speech and associative\nword relations, among other lexical features. Ultimately, findings reiterate\nthe importance of considering demographic alignment and conversational goals\nwhen using these models as public-facing tools. Our framework will be publicly\navailable via code, data, and a python package.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sicilia_A/0/1/0/all/0/1\">Anthony Sicilia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gates_J/0/1/0/all/0/1\">Jennifer C. Gates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models. (arXiv:2305.14323v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14323","description":"<p>Although large language models (LLMs) have achieved excellent performance in\na variety of evaluation benchmarks, they still struggle in complex reasoning\ntasks which require specific knowledge and multi-hop reasoning. To improve the\nreasoning abilities, we propose \\textbf{ChatCoT}, a tool-augmented\nchain-of-thought reasoning framework for chat-based LLMs. In ChatCoT, we model\nthe chain-of-thought~(CoT) reasoning as multi-turn conversations, to utilize\ntools in a more natural way through chatting. At each turn, LLMs can either\ninteract with tools or perform the reasoning. Our approach can effectively\nleverage the multi-turn conversation ability of chat-based LLMs, and integrate\nthe thought chain following and tools manipulation in a unified way. Specially,\nwe initialize the early turns of the conversation by the tools, tasks and\nreasoning format, and propose an iterative \\emph{tool-augmented reasoning} step\nto perform step-by-step tool-augmented reasoning. The experiment results on two\ncomplex reasoning datasets (MATH and HotpotQA) have shown the effectiveness of\nChatCoT on complex reasoning tasks, achieving a 6.8\\% relative improvement over\nthe state-of-the-art baseline. Our code and data are available at:\n\\url{https://github.com/RUCAIBOX/ChatCoT}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhipeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Beichen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zheng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anchor Prediction: Automatic Refinement of Internet Links. (arXiv:2305.14337v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14337","description":"<p>Internet links enable users to deepen their understanding of a topic by\nproviding convenient access to related information. However, the majority of\nlinks are unanchored -- they link to a target webpage as a whole, and readers\nmay expend considerable effort localizing the specific parts of the target\nwebpage that enrich their understanding of the link's source context. To help\nreaders effectively find information in linked webpages, we introduce the task\nof anchor prediction, where the goal is to identify the specific part of the\nlinked target webpage that is most related to the source linking context. We\nrelease the AuthorAnchors dataset, a collection of 34K naturally-occurring\nanchored links, which reflect relevance judgments by the authors of the source\narticle. To model reader relevance judgments, we annotate and release\nReaderAnchors, an evaluation set of anchors that readers find useful. Our\nanalysis shows that effective anchor prediction often requires jointly\nreasoning over lengthy source and target webpages to determine their implicit\nrelations and identify parts of the target webpage that are related but not\nredundant. We benchmark a performant T5-based ranking approach to establish\nbaseline performance on the task, finding ample room for improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nelson F. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kenton Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toutanova_K/0/1/0/all/0/1\">Kristina Toutanova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models Meet World Models: Embodied Experiences Enhance Language Models. (arXiv:2305.10626v2 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2305.10626","description":"<p>While large language models (LMs) have shown remarkable capabilities across\nnumerous tasks, they often struggle with simple reasoning and planning in\nphysical environments, such as understanding object permanence or planning\nhousehold activities. The limitation arises from the fact that LMs are trained\nonly on written text and miss essential embodied knowledge and skills. In this\npaper, we propose a new paradigm of enhancing LMs by finetuning them with world\nmodels, to gain diverse embodied knowledge while retaining their general\nlanguage capabilities. Our approach deploys an embodied agent in a world model,\nparticularly a simulator of the physical world (VirtualHome), and acquires a\ndiverse set of embodied experiences through both goal-oriented planning and\nrandom exploration. These experiences are then used to finetune LMs to teach\ndiverse abilities of reasoning and acting in the physical world, e.g., planning\nand completing goals, object permanence and tracking, etc. Moreover, it is\ndesirable to preserve the generality of LMs during finetuning, which\nfacilitates generalizing the embodied knowledge across tasks rather than being\ntied to specific simulations. We thus further introduce the classical elastic\nweight consolidation (EWC) for selective weight updates, combined with low-rank\nadapters (LoRA) for training efficiency. Extensive experiments show our\napproach substantially improves base LMs on 18 downstream tasks by 64.28% on\naverage. In particular, the small LMs (1.3B and 6B) enhanced by our approach\nmatch or even outperform much larger LMs (e.g., ChatGPT).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1\">Jiannan Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_T/0/1/0/all/0/1\">Tianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1\">Tianmin Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Learning of Medical Concepts Embedding using BEHRT. (arXiv:2305.13052v1 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2305.13052","description":"<p>Electronic Health Records (EHR) data contains medical records such as\ndiagnoses, medications, procedures, and treatments of patients. This data is\noften considered sensitive medical information. Therefore, the EHR data from\nthe medical centers often cannot be shared, making it difficult to create\nprediction models using multi-center EHR data, which is essential for such\nmodels' robustness and generalizability. Federated Learning (FL) is an\nalgorithmic approach that allows learning a shared model using data in multiple\nlocations without the need to store all data in a central place. An example of\na prediction model's task is to predict future diseases. More specifically, the\nmodel needs to predict patient's next visit diagnoses, based on current and\nprevious clinical data. Such a prediction model can support care providers in\nmaking clinical decisions and even provide preventive treatment. We propose a\nfederated learning approach for learning medical concepts embedding. This\npre-trained model can be used for fine-tuning for specific downstream tasks.\nOur approach is based on an embedding model like BEHRT, a deep neural sequence\ntransduction model for EHR. We train using federated learning, both the Masked\nLanguage Modeling (MLM) and the next visit downstream model. We demonstrate our\napproach on the MIMIC-IV dataset. We compare the performance of a model trained\nwith FL against a model trained on centralized data. We find that our federated\nlearning approach reaches very close to the performance of a centralized model,\nand it outperforms local models in terms of average precision. We also show\nthat pre-trained MLM improves the model's average precision performance in the\nnext visit prediction task, compared to an MLM model without pre-training. Our\ncode is available at https://github.com/nadavlab/FederatedBEHRT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shoham_O/0/1/0/all/0/1\">Ofir Ben Shoham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rappoport_N/0/1/0/all/0/1\">Nadav Rappoport</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-05-24T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}