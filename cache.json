{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-05-11T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"$2 * n$ is better than $n^2$: Decomposing Event Coreference Resolution into Two Tractable Problems. (arXiv:2305.05672v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05672","description":"<p>Event Coreference Resolution (ECR) is the task of linking mentions of the\nsame event either within or across documents. Most mention pairs are not\ncoreferent, yet many that are coreferent can be identified through simple\ntechniques such as lemma matching of the event triggers or the sentences in\nwhich they appear. Existing methods for training coreference systems sample\nfrom a largely skewed distribution, making it difficult for the algorithm to\nlearn coreference beyond surface matching. Additionally, these methods are\nintractable because of the quadratic operations needed. To address these\nchallenges, we break the problem of ECR into two parts: a) a heuristic to\nefficiently filter out a large number of non-coreferent pairs, and b) a\ntraining approach on a balanced set of coreferent and non-coreferent mention\npairs. By following this approach, we show that we get comparable results to\nthe state of the art on two popular ECR datasets while significantly reducing\ncompute requirements. We also analyze the mention pairs that are \"hard\" to\naccurately classify as coreferent or non-coreferent. Code at\nhttps://github.com/ahmeshaf/lemma_ce_coref\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1\">Shafiuddin Rehan Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nath_A/0/1/0/all/0/1\">Abhijnan Nath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1\">James H. Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnaswamy_N/0/1/0/all/0/1\">Nikhil Krishnaswamy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors. (arXiv:2305.05711v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05711","description":"<p>Large language models (LLMs) pre-trained on massive corpora have demonstrated\nimpressive few-shot learning ability on many NLP tasks. A common practice is to\nrecast the task into a text-to-text format such that generative LLMs of natural\nlanguage (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is\nnontrivial to perform information extraction (IE) tasks with NL-LLMs since the\noutput of the IE task is usually structured and therefore is hard to be\nconverted into plain text. In this paper, we propose to recast the structured\noutput in the form of code instead of natural language and utilize generative\nLLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular,\nnamed entity recognition and relation extraction. In contrast to NL-LLMs, we\nshow that Code-LLMs can be well-aligned with these IE tasks by designing\ncode-style prompts and formulating these IE tasks as code generation tasks.\nExperiment results on seven benchmarks show that our method consistently\noutperforms fine-tuning moderate-size pre-trained models specially designed for\nIE tasks (e.g., UIE) and prompting NL-LLMs under few-shot settings. We further\nconduct a series of in-depth analyses to demonstrate the merits of leveraging\nCode-LLMs for IE tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1\">Qiong Tang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hang Yan</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuanbin Wu</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a> (2) ((1) Academy for Engineering &amp; Technology, Fudan University, (2) School of Computer Science, Fudan University, (3) School of Computer Science and Technology, East China Normal University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilevel Sentence Embeddings for Personality Prediction. (arXiv:2305.05748v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05748","description":"<p>Representing text into a multidimensional space can be done with sentence\nembedding models such as Sentence-BERT (SBERT). However, training these models\nwhen the data has a complex multilevel structure requires individually trained\nclass-specific models, which increases time and computing costs. We propose a\ntwo step approach which enables us to map sentences according to their\nhierarchical memberships and polarity. At first we teach the upper level\nsentence space through an AdaCos loss function and then finetune with a novel\nloss function mainly based on the cosine similarity of intra-level pairs. We\napply this method to three different datasets: two weakly supervised Big Five\npersonality dataset obtained from English and Japanese Twitter data and the\nbenchmark MNLI dataset. We show that our single model approach performs better\nthan multiple class-specific classification models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tirotta_P/0/1/0/all/0/1\">Paolo Tirotta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuasa_A/0/1/0/all/0/1\">Akira Yuasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morita_M/0/1/0/all/0/1\">Masashi Morita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When and What to Ask Through World States and Text Instructions: IGLU NLP Challenge Solution. (arXiv:2305.05754v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05754","description":"<p>In collaborative tasks, effective communication is crucial for achieving\njoint goals. One such task is collaborative building where builders must\ncommunicate with each other to construct desired structures in a simulated\nenvironment such as Minecraft. We aim to develop an intelligent builder agent\nto build structures based on user input through dialogue. However, in\ncollaborative building, builders may encounter situations that are difficult to\ninterpret based on the available information and instructions, leading to\nambiguity. In the NeurIPS 2022 Competition NLP Task, we address two key\nresearch questions, with the goal of filling this gap: when should the agent\nask for clarification, and what clarification questions should it ask? We move\ntowards this target with two sub-tasks, a classification task and a ranking\ntask. For the classification task, the goal is to determine whether the agent\nshould ask for clarification based on the current world state and dialogue\nhistory. For the ranking task, the goal is to rank the relevant clarification\nquestions from a pool of candidates. In this report, we briefly introduce our\nmethods for the classification and ranking task. For the classification task,\nour model achieves an F1 score of 0.757, which placed the 3rd on the\nleaderboard. For the ranking task, our model achieves about 0.38 for Mean\nReciprocal Rank by extending the traditional ranking model. Lastly, we discuss\nvarious neural approaches for the ranking task and future direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhengxiang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramos_J/0/1/0/all/0/1\">Jerome Ramos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">To Eun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1\">Hossein A. Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1\">Aldo Lipani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ranking & Reweighting Improves Group Distributional Robustness. (arXiv:2305.05759v1 [cs.LG])","link":"http://arxiv.org/abs/2305.05759","description":"<p>Recent work has shown that standard training via empirical risk minimization\n(ERM) can produce models that achieve high accuracy on average but low accuracy\non underrepresented groups due to the prevalence of spurious features. A\npredominant approach to tackle this group robustness problem minimizes the\nworst group error (akin to a minimax strategy) on the training data, hoping it\nwill generalize well on the testing data. However, this is often suboptimal,\nespecially when the out-of-distribution (OOD) test data contains previously\nunseen groups. Inspired by ideas from the information retrieval and\nlearning-to-rank literature, this paper first proposes to use Discounted\nCumulative Gain (DCG) as a metric of model quality for facilitating better\nhyperparameter tuning and model selection. Being a ranking-based metric, DCG\nweights multiple poorly-performing groups (instead of considering just the\ngroup with the worst performance). As a natural next step, we build on our\nresults to propose a ranking-based training method called Discounted Rank\nUpweighting (DRU), which differentially reweights a ranked list of\npoorly-performing groups in the training data to learn models that exhibit\nstrong OOD performance on the test data. Results on several synthetic and\nreal-world datasets highlight the superior generalization ability of our\ngroup-ranking-based (akin to soft-minimax) approach in selecting and learning\nmodels that are robust to group distributional shifts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yachuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_Q/0/1/0/all/0/1\">Qiaozhu Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhillon_P/0/1/0/all/0/1\">Paramveer Dhillon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-dependent communication under environmental constraints. (arXiv:2305.05821v1 [cs.AI])","link":"http://arxiv.org/abs/2305.05821","description":"<p>There is significant evidence that real-world communication cannot be reduced\nto sending signals with context-independent meaning. In this work, based on a\nvariant of the classical Lewis (1969) signaling model, we explore the\nconditions for the emergence of context-dependent communication in a situated\nscenario. In particular, we demonstrate that pressure to minimise the\nvocabulary size is sufficient for such emergence. At the same time, we study\nthe environmental conditions and cognitive capabilities that enable contextual\ndisambiguation of symbol meanings. We show that environmental constraints on\nthe receiver's referent choice can be unilaterally exploited by the sender,\nwithout disambiguation capabilities on the receiver's end. Consistent with\ncommon assumptions, the sender's awareness of the context appears to be\nrequired for contextual communication. We suggest that context-dependent\ncommunication is a situated multilayered phenomenon, crucially influenced by\nenvironment properties such as distribution of contexts. The model developed in\nthis work is a demonstration of how signals may be ambiguous out of context,\nbut still allow for near-perfect communication accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glowka_K/0/1/0/all/0/1\">Krzysztof G&#x142;&#xf3;wka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubek_J/0/1/0/all/0/1\">Julian Zubek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raczaszek_Leonardi_J/0/1/0/all/0/1\">Joanna R&#x105;czaszek-Leonardi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Dense Retrieval Training with Web Anchors. (arXiv:2305.05834v1 [cs.IR])","link":"http://arxiv.org/abs/2305.05834","description":"<p>In this work, we present an unsupervised retrieval method with contrastive\nlearning on web anchors. The anchor text describes the content that is\nreferenced from the linked page. This shows similarities to search queries that\naim to retrieve pertinent information from relevant documents. Based on their\ncommonalities, we train an unsupervised dense retriever, Anchor-DR, with a\ncontrastive learning task that matches the anchor text and the linked document.\nTo filter out uninformative anchors (such as ``homepage'' or other functional\nanchors), we present a novel filtering technique to only select anchors that\ncontain similar types of information as search queries. Experiments show that\nAnchor-DR outperforms state-of-the-art methods on unsupervised dense retrieval\nby a large margin (e.g., by 5.3% NDCG@10 on MSMARCO). The gain of our method is\nespecially significant for search and question answering tasks. Our analysis\nfurther reveals that the pattern of anchor-document pairs is similar to that of\nsearch query-document pairs. Code available at\nhttps://github.com/Veronicium/AnchorDR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yiqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"V\\=arta: A Large-Scale Headline-Generation Dataset for Indic Languages. (arXiv:2305.05858v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05858","description":"<p>We present V\\=arta, a large-scale multilingual dataset for headline\ngeneration in Indic languages. This dataset includes 41.8 million news articles\nin 14 different Indic languages (and English), which come from a variety of\nhigh-quality sources. To the best of our knowledge, this is the largest\ncollection of curated articles for Indic languages currently available. We use\nthe data collected in a series of experiments to answer important questions\nrelated to Indic NLP and multilinguality research in general. We show that the\ndataset is challenging even for state-of-the-art abstractive models and that\nthey perform only slightly better than extractive baselines. Owing to its size,\nwe also show that the dataset can be used to pretrain strong language models\nthat outperform competitive baselines in both NLU and NLG benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aralikatte_R/0/1/0/all/0/1\">Rahul Aralikatte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Ziling Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doddapaneni_S/0/1/0/all/0/1\">Sumanth Doddapaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jackie Chi Kit Cheung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks. (arXiv:2305.05862v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05862","description":"<p>The most recent large language models such as ChatGPT and GPT-4 have garnered\nsignificant attention, as they are capable of generating high-quality responses\nto human input. Despite the extensive testing of ChatGPT and GPT-4 on generic\ntext corpora, showcasing their impressive capabilities, a study focusing on\nfinancial corpora has not been conducted. In this study, we aim to bridge this\ngap by examining the potential of ChatGPT and GPT-4 as a solver for typical\nfinancial text analytic problems in the zero-shot or few-shot setting.\nSpecifically, we assess their capabilities on four representative tasks over\nfive distinct financial textual datasets. The preliminary study shows that\nChatGPT and GPT-4 struggle on tasks such as financial named entity recognition\n(NER) and sentiment analysis, where domain-specific knowledge is required,\nwhile they excel in numerical reasoning tasks. We report both the strengths and\nlimitations of the current versions of ChatGPT and GPT-4, comparing them to the\nstate-of-the-art finetuned models as well as pretrained domain-specific\ngenerative models. Our experiments provide qualitative studies, through which\nwe hope to help understand the capability of the existing models and facilitate\nfurther improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhiqiang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaomo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Sameena Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Address Matching Based On Hierarchical Information. (arXiv:2305.05874v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05874","description":"<p>There is evidence that address matching plays a crucial role in many areas\nsuch as express delivery, online shopping and so on. Address has a hierarchical\nstructure, in contrast to unstructured texts, which can contribute valuable\ninformation for address matching. Based on this idea, this paper proposes a\nnovel method to leverage the hierarchical information in deep learning method\nthat not only improves the ability of existing methods to handle irregular\naddress, but also can pay closer attention to the special part of address.\nExperimental findings demonstrate that the proposed method improves the current\napproach by 3.2% points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengxian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jintao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Ting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shasha Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decker: Double Check with Heterogeneous Knowledge for Commonsense Fact Verification. (arXiv:2305.05921v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05921","description":"<p>Commonsense fact verification, as a challenging branch of commonsense\nquestion-answering (QA), aims to verify through facts whether a given\ncommonsense claim is correct or not. Answering commonsense questions\nnecessitates a combination of knowledge from various levels. However, existing\nstudies primarily rest on grasping either unstructured evidence or potential\nreasoning paths from structured knowledge bases, yet failing to exploit the\nbenefits of heterogeneous knowledge simultaneously. In light of this, we\npropose Decker, a commonsense fact verification model that is capable of\nbridging heterogeneous knowledge by uncovering latent relationships between\nstructured and unstructured knowledge. Experimental results on two commonsense\nfact verification benchmark datasets, CSQA2.0 and CREAK demonstrate the\neffectiveness of our Decker and further analysis verifies its capability to\nseize more precious information through reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_A/0/1/0/all/0/1\">Anni Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WikiSQE: A Large-Scale Dataset for Sentence Quality Estimation in Wikipedia. (arXiv:2305.05928v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05928","description":"<p>Wikipedia can be edited by anyone and thus contains various quality\nsentences. Therefore, Wikipedia includes some poor-quality edits, which are\noften marked up by other editors. While editors' reviews enhance the\ncredibility of Wikipedia, it is hard to check all edited text. Assisting in\nthis process is very important, but a large and comprehensive dataset for\nstudying it does not currently exist. Here, we propose WikiSQE, the first\nlarge-scale dataset for sentence quality estimation in Wikipedia. Each sentence\nis extracted from the entire revision history of Wikipedia, and the target\nquality labels were carefully investigated and selected. WikiSQE has about 3.4\nM sentences with 153 quality labels. In the experiment with automatic\nclassification using competitive machine learning models, sentences that had\nproblems with citation, syntax/semantics, or propositions were found to be more\ndifficult to detect. In addition, we conducted automated essay scoring\nexperiments to evaluate the generalizability of the dataset. We show that the\nmodels trained on WikiSQE perform better than the vanilla model, indicating its\npotential usefulness in other domains. WikiSQE is expected to be a valuable\nresource for other tasks in NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ando_K/0/1/0/all/0/1\">Kenichiro Ando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekine_S/0/1/0/all/0/1\">Satoshi Sekine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komachi_M/0/1/0/all/0/1\">Mamoru Komachi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-hop Commonsense Knowledge Injection Framework for Zero-Shot Commonsense Question Answering. (arXiv:2305.05936v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05936","description":"<p>Commonsense question answering (QA) research requires machines to answer\nquestions based on commonsense knowledge. However, this research requires\nexpensive labor costs to annotate data as the basis of research, and models\nthat rely on fine-tuning paradigms only apply to specific tasks, rather than\nlearn a general commonsense reasoning ability. As a more robust method,\nzero-shot commonsense question answering shows a good prospect. The current\nzero-shot framework tries to convert triples in commonsense knowledge graphs\n(KGs) into QA-form samples as the pre-trained data source to incorporate\ncommonsense knowledge into the model. However, this method ignores the\nmulti-hop relationship in the KG, which is also an important central problem in\ncommonsense reasoning. In this paper, we propose a novel multi-hop commonsense\nknowledge injection framework. Specifically, it explores multi-hop reasoning\nparadigm in KGs that conform to linguistic logic, and we further propose two\nmulti-hop QA generation methods based on KGs. Then, we utilize contrastive\nlearning to pre-train the model with the synthetic QA dataset to inject\nmulti-hop commonsense knowledge. Extensive experiments on five commonsense\nquestion answering benchmarks demonstrate that our framework achieves\nstate-of-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_X/0/1/0/all/0/1\">Xin Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1\">Biwei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qingqing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zheng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiuxin Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment. (arXiv:2305.05940v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05940","description":"<p>In-context learning (ICL) unfolds as large language models become capable of\ninferring test labels conditioned on a few labeled samples without any gradient\nupdate. ICL-enabled large language models provide a promising step forward\ntoward bypassing recurrent annotation costs in a low-resource setting. Yet,\nonly a handful of past studies have explored ICL in a cross-lingual setting, in\nwhich the need for transferring label-knowledge from a high-resource language\nto a low-resource one is immensely crucial. To bridge the gap, we provide the\nfirst in-depth analysis of ICL for cross-lingual text classification. We find\nthat the prevalent mode of selecting random input-label pairs to construct the\nprompt-context is severely limited in the case of cross-lingual ICL, primarily\ndue to the lack of alignment in the input as well as the output spaces. To\nmitigate this, we propose a novel prompt construction strategy -- Cross-lingual\nIn-context Source-Target Alignment (X-InSTA). With an injected coherence in the\nsemantics of the input examples and a task-based alignment across the source\nand target languages, X-InSTA is able to outperform random prompt selection by\na large margin across three different tasks using 44 different cross-lingual\npairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tanwar_E/0/1/0/all/0/1\">Eshaan Tanwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borthakur_M/0/1/0/all/0/1\">Manish Borthakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Subhabrata Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapter-TST: A Parameter Efficient Method for Multiple-Attribute Text Style Transfer. (arXiv:2305.05945v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05945","description":"<p>Adapting a large language model for multiple-attribute text style transfer\nvia fine-tuning can be challenging due to the significant amount of\ncomputational resources and labeled data required for the specific task. In\nthis paper, we address this challenge by introducing AdapterTST, a framework\nthat freezes the pre-trained model's original parameters and enables the\ndevelopment of a multiple-attribute text style transfer model. Using BART as\nthe backbone model, Adapter-TST utilizes different neural adapters to capture\ndifferent attribute information, like a plug-in connected to BART. Our method\nallows control over multiple attributes, like sentiment, tense, voice, etc.,\nand configures the adapters' architecture to generate multiple outputs\nrespected to attributes or compositional editing on the same sentence. We\nevaluate the proposed model on both traditional sentiment transfer and\nmultiple-attribute transfer tasks. The experiment results demonstrate that\nAdapter-TST outperforms all the state-of-the-art baselines with significantly\nlesser computational resources. We have also empirically shown that each\nadapter is able to capture specific stylistic attributes effectively and can be\nconfigured to perform compositional editing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiqiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Path Transformer is Better: A Case Study on Neural Machine Translation. (arXiv:2305.05948v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05948","description":"<p>For years the model performance in machine learning obeyed a power-law\nrelationship with the model size. For the consideration of parameter\nefficiency, recent studies focus on increasing model depth rather than width to\nachieve better performance. In this paper, we study how model width affects the\nTransformer model through a parameter-efficient multi-path structure. To better\nfuse features extracted from different paths, we add three additional\noperations to each sublayer: a normalization at the end of each path, a cheap\noperation to produce more features, and a learnable weighted mechanism to fuse\nall features flexibly. Extensive experiments on 12 WMT machine translation\ntasks show that, with the same number of parameters, the shallower multi-path\nmodel can achieve similar or even better performance than the deeper model. It\nreveals that we should pay more attention to the multi-path structure, and\nthere should be a balance between the model depth and width to train a better\nlarge-scale Transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Ye Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuhan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_A/0/1/0/all/0/1\">Anxiang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Multimodal Misinformation Detection with Logic Reasoning. (arXiv:2305.05964v1 [cs.MM])","link":"http://arxiv.org/abs/2305.05964","description":"<p>Multimodal misinformation on online social platforms is becoming a critical\nconcern due to increasing credibility and easier dissemination brought by\nmultimedia content, compared to traditional text-only information. While\nexisting multimodal detection approaches have achieved high performance, the\nlack of interpretability hinders these systems' reliability and practical\ndeployment. Inspired by NeuralSymbolic AI which combines the learning ability\nof neural networks with the explainability of symbolic learning, we propose a\nnovel logic-based neural model for multimodal misinformation detection which\nintegrates interpretable logic clauses to express the reasoning process of the\ntarget task. To make learning effective, we parameterize symbolic logical\nelements using neural representations, which facilitate the automatic\ngeneration and evaluation of meaningful logic clauses. Additionally, to make\nour framework generalizable across diverse misinformation sources, we introduce\nfive meta-predicates that can be instantiated with different correlations.\nResults on three public datasets (Twitter, Weibo, and Sarcasm) demonstrate the\nfeasibility and versatility of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenya Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoliang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Forgetting in Pre-Trained Representations Through Continual Learning. (arXiv:2305.05968v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05968","description":"<p>Representation forgetting refers to the drift of contextualized\nrepresentations during continual training. Intuitively, the representation\nforgetting can influence the general knowledge stored in pre-trained language\nmodels (LMs), but the concrete effect is still unclear. In this paper, we study\nthe effect of representation forgetting on the generality of pre-trained\nlanguage models, i.e. the potential capability for tackling future downstream\ntasks. Specifically, we design three metrics, including overall generality\ndestruction (GD), syntactic knowledge forgetting (SynF), and semantic knowledge\nforgetting (SemF), to measure the evolution of general knowledge in continual\nlearning. With extensive experiments, we find that the generality is destructed\nin various pre-trained LMs, and syntactic and semantic knowledge is forgotten\nthrough continual learning. Based on our experiments and analysis, we further\nget two insights into alleviating general knowledge forgetting: 1) training on\ngeneral linguistic tasks at first can mitigate general knowledge forgetting; 2)\nthe hybrid continual learning method can mitigate the generality destruction\nand maintain more general knowledge compared with those only considering\nrehearsal or regularization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xuefeng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models. (arXiv:2305.05973v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05973","description":"<p>We propose a novel approach for developing privacy-preserving large-scale\nrecommender systems using differentially private (DP) large language models\n(LLMs) which overcomes certain challenges and limitations in DP training these\ncomplex systems. Our method is particularly well suited for the emerging area\nof LLM-based recommender systems, but can be readily employed for any\nrecommender systems that process representations of natural language inputs.\nOur approach involves using DP training methods to fine-tune a publicly\npre-trained LLM on a query generation task. The resulting model can generate\nprivate synthetic queries representative of the original queries which can be\nfreely shared for any downstream non-private recommendation training procedures\nwithout incurring any additional privacy cost. We evaluate our method on its\nability to securely train effective deep retrieval models, and we observe\nsignificant improvements in their retrieval quality without compromising\nquery-level privacy guarantees compared to methods where the retrieval models\nare directly DP trained.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carranza_A/0/1/0/all/0/1\">Aldo Gael Carranza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farahani_R/0/1/0/all/0/1\">Rezsa Farahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponomareva_N/0/1/0/all/0/1\">Natalia Ponomareva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurakin_A/0/1/0/all/0/1\">Alex Kurakin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagielski_M/0/1/0/all/0/1\">Matthew Jagielski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasr_M/0/1/0/all/0/1\">Milad Nasr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge. (arXiv:2305.05976v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05976","description":"<p>Large language models (LLMs) have been widely studied for their ability to\nstore and utilize positive knowledge. However, negative knowledge, such as\n\"lions don't live in the ocean\", is also ubiquitous in the world but rarely\nmentioned explicitly in the text. What do LLMs know about negative knowledge?\nThis work examines the ability of LLMs to negative commonsense knowledge. We\ndesign a constrained keywords-to-sentence generation task (CG) and a Boolean\nquestion-answering task (QA) to probe LLMs. Our experiments reveal that LLMs\nfrequently fail to generate valid sentences grounded in negative commonsense\nknowledge, yet they can correctly answer polar yes-or-no questions. We term\nthis phenomenon the belief conflict of LLMs. Our further analysis shows that\nstatistical shortcuts and negation reporting bias from language modeling\npre-training cause this conflict.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiangjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Wei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Ziquan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Sijie Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating medically-accurate summaries of patient-provider dialogue: A multi-stage approach using large language models. (arXiv:2305.05982v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05982","description":"<p>A medical provider's summary of a patient visit serves several critical\npurposes, including clinical decision-making, facilitating hand-offs between\nproviders, and as a reference for the patient. An effective summary is required\nto be coherent and accurately capture all the medically relevant information in\nthe dialogue, despite the complexity of patient-generated language. Even minor\ninaccuracies in visit summaries (for example, summarizing \"patient does not\nhave a fever\" when a fever is present) can be detrimental to the outcome of\ncare for the patient.\n</p>\n<p>This paper tackles the problem of medical conversation summarization by\ndiscretizing the task into several smaller dialogue-understanding tasks that\nare sequentially built upon. First, we identify medical entities and their\naffirmations within the conversation to serve as building blocks. We study\ndynamically constructing few-shot prompts for tasks by conditioning on relevant\npatient information and use GPT-3 as the backbone for our experiments. We also\ndevelop GPT-derived summarization metrics to measure performance against\nreference summaries quantitatively. Both our human evaluation study and metrics\nfor medical correctness show that summaries generated using this approach are\nclinically accurate and outperform the baseline approach of summarizing the\ndialog in a zero-shot, single-prompt setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nair_V/0/1/0/all/0/1\">Varun Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schumacher_E/0/1/0/all/0/1\">Elliot Schumacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannan_A/0/1/0/all/0/1\">Anitha Kannan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base. (arXiv:2305.05994v1 [cs.CL])","link":"http://arxiv.org/abs/2305.05994","description":"<p>Analogical reasoning is a fundamental cognitive ability of humans. However,\ncurrent language models (LMs) still struggle to achieve human-like performance\nin analogical reasoning tasks due to a lack of resources for model training. In\nthis work, we address this gap by proposing ANALOGYKB, a million-scale analogy\nknowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB\nidentifies two types of analogies from the KGs: 1) analogies of the same\nrelations, which can be directly extracted from the KGs, and 2) analogies of\nanalogous relations, which are identified with a selection and filtering\npipeline enabled by large LMs (InstructGPT), followed by minor human efforts\nfor data quality control. Evaluations on a series of datasets of two analogical\nreasoning tasks (analogy recognition and generation) demonstrate that ANALOGYKB\nsuccessfully enables LMs to achieve much better results than previous\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Siyu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiangjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changzhi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiaqing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Deqing Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iLab at SemEval-2023 Task 11 Le-Wi-Di: Modelling Disagreement or Modelling Perspectives?. (arXiv:2305.06074v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06074","description":"<p>There are two competing approaches for modelling annotator disagreement:\ndistributional soft-labelling approaches (which aim to capture the level of\ndisagreement) or modelling perspectives of individual annotators or groups\nthereof. We adapt a multi-task architecture -- which has previously shown\nsuccess in modelling perspectives -- to evaluate its performance on the SEMEVAL\nTask 11. We do so by combining both approaches, i.e. predicting individual\nannotator perspectives as an interim step towards predicting annotator\ndisagreement. Despite its previous success, we found that a multi-task approach\nperformed poorly on datasets which contained distinct annotator opinions,\nsuggesting that this approach may not always be suitable when modelling\nperspectives. Furthermore, our results explain that while strongly\nperspectivist approaches might not achieve state-of-the-art performance\naccording to evaluation metrics used by distributional approaches, our approach\nallows for a more nuanced understanding of individual perspectives present in\nthe data. We argue that perspectivist approaches are preferable because they\nenable decision makers to amplify minority views, and that it is important to\nre-evaluate metrics to reflect this goal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vitsakis_N/0/1/0/all/0/1\">Nikolas Vitsakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_A/0/1/0/all/0/1\">Amit Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinkar_T/0/1/0/all/0/1\">Tanvi Dinkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abercrombie_G/0/1/0/all/0/1\">Gavin Abercrombie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konstas_I/0/1/0/all/0/1\">Ioannis Konstas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1\">Verena Rieser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Glimpse in ChatGPT Capabilities and its impact for AI research. (arXiv:2305.06087v1 [cs.AI])","link":"http://arxiv.org/abs/2305.06087","description":"<p>Large language models (LLMs) have recently become a popular topic in the\nfield of Artificial Intelligence (AI) research, with companies such as Google,\nAmazon, Facebook, Amazon, Tesla, and Apple (GAFA) investing heavily in their\ndevelopment. These models are trained on massive amounts of data and can be\nused for a wide range of tasks, including language translation, text\ngeneration, and question answering. However, the computational resources\nrequired to train and run these models are substantial, and the cost of\nhardware and electricity can be prohibitive for research labs that do not have\nthe funding and resources of the GAFA. In this paper, we will examine the\nimpact of LLMs on AI research. The pace at which such models are generated as\nwell as the range of domains covered is an indication of the trend which not\nonly the public but also the scientific community is currently experiencing. We\ngive some examples on how to use such models in research by focusing on\nGPT3.5/ChatGPT3.4 and ChatGPT4 at the current state and show that such a range\nof capabilities in a single system is a strong sign of approaching general\nintelligence. Innovations integrating such models will also expand along the\nmaturation of such AI systems and exhibit unforeseeable applications that will\nhave important impacts on several aspects of our societies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joublin_F/0/1/0/all/0/1\">Frank Joublin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceravola_A/0/1/0/all/0/1\">Antonello Ceravola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deigmoeller_J/0/1/0/all/0/1\">Joerg Deigmoeller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gienger_M/0/1/0/all/0/1\">Michael Gienger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franzius_M/0/1/0/all/0/1\">Mathias Franzius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eggert_J/0/1/0/all/0/1\">Julian Eggert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PAI at SemEval-2023 Task 2: A Universal System for Named Entity Recognition with External Entity Information. (arXiv:2305.06099v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06099","description":"<p>The MultiCoNER II task aims to detect complex, ambiguous, and fine-grained\nnamed entities in low-context situations and noisy scenarios like the presence\nof spelling mistakes and typos for multiple languages. The task poses\nsignificant challenges due to the scarcity of contextual information, the high\ngranularity of the entities(up to 33 classes), and the interference of noisy\ndata. To address these issues, our team {\\bf PAI} proposes a universal Named\nEntity Recognition (NER) system that integrates external entity information to\nimprove performance. Specifically, our system retrieves entities with\nproperties from the knowledge base (i.e. Wikipedia) for a given text, then\nconcatenates entity information with the input sentence and feeds it into\nTransformer-based models. Finally, our system wins 2 first places, 4 second\nplaces, and 1 third place out of 13 tracks. The code is publicly available at\n\\url{https://github.com/diqiuzhuanzhuan/semeval-2023}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Long Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kai Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_T/0/1/0/all/0/1\">Tianbo Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hailong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Weiguo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CQSumDP: A ChatGPT-Annotated Resource for Query-Focused Abstractive Summarization Based on Debatepedia. (arXiv:2305.06147v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06147","description":"<p>Debatepedia is a publicly available dataset consisting of arguments and\ncounter-arguments on controversial topics that has been widely used for the\nsingle-document query-focused abstractive summarization task in recent years.\nHowever, it has been recently found that this dataset is limited by noise and\neven most queries in this dataset do not have any relevance to the respective\ndocument. In this paper, we present a methodology for cleaning the Debatepedia\ndataset by leveraging the generative power of large language models to make it\nsuitable for query-focused abstractive summarization. More specifically, we\nharness the language generation capabilities of ChatGPT to regenerate its\nqueries. We evaluate the effectiveness of the proposed ChatGPT annotated\nversion of the Debatepedia dataset using several benchmark summarization models\nand demonstrate that the newly annotated version of Debatepedia outperforms the\noriginal dataset in terms of both query relevance as well as summary generation\nquality. We will make this annotated and cleaned version of the dataset\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laskar_M/0/1/0/all/0/1\">Md Tahmid Rahman Laskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mizanur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jahan_I/0/1/0/all/0/1\">Israt Jahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoque_E/0/1/0/all/0/1\">Enamul Hoque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jimmy Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A semi-automatic method for document classification in the shipping industry. (arXiv:2305.06148v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06148","description":"<p>In the shipping industry, document classification plays a crucial role in\nensuring that the necessary documents are properly identified and processed for\ncustoms clearance. OCR technology is being used to automate the process of\ndocument classification, which involves identifying important documents such as\nCommercial Invoices, Packing Lists, Export/Import Customs Declarations, Bills\nof Lading, Sea Waybills, Certificates, Air or Rail Waybills, Arrival Notices,\nCertificate of Origin, Importer Security Filings, and Letters of Credit. By\nusing OCR technology, the shipping industry can improve accuracy and efficiency\nin document classification and streamline the customs clearance process. The\naim of this study is to build a robust document classification system based on\nkeyword frequencies. The research is carried out by analyzing Contract-Breach\nlaw documents available with IN-D. The documents were collected by scraping the\nSingapore Government Judiciary website. The database developed has 250\nContract-Breach documents. These documents are splitted to generate 200\ntraining documents and 50 test documents. A semi-automatic approach is used to\nselect keyword vectors for document classification. The accuracy of the\nreported model is 92.00 %.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arvind_N/0/1/0/all/0/1\">Narayanan Arvind</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure-CLIP: Enhance Multi-modal Language Representations with Structure Knowledge. (arXiv:2305.06152v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06152","description":"<p>Large-scale vision-language pre-training has shown promising advances on\nvarious downstream tasks and achieved significant performance in multi-modal\nunderstanding and generation tasks. However, existing methods often perform\npoorly on image-text matching tasks that require a detailed semantics\nunderstanding of the text. Although there have been some works on this problem,\nthey do not sufficiently exploit the structural knowledge present in sentences\nto enhance multi-modal language representations, which leads to poor\nperformance. In this paper, we present an end-to-end framework Structure-CLIP,\nwhich integrates latent detailed semantics from the text to enhance\nfine-grained semantic representations. Specifically, (1) we use scene graphs in\norder to pay more attention to the detailed semantic learning in the text and\nfully explore structured knowledge between fine-grained semantics, and (2) we\nutilize the knowledge-enhanced framework with the help of the scene graph to\nmake full use of representations of structured knowledge. To verify the\neffectiveness of our proposed method, we pre-trained our models with the\naforementioned approach and conduct experiments on different downstream tasks.\nNumerical results show that Structure-CLIP can often achieve state-of-the-art\nperformance on both VG-Attribution and VG-Relation datasets. Extensive\nexperiments show its components are effective and its predictions are\ninterpretable, which proves that our proposed method can enhance detailed\nsemantic representation well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiji Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongsheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weijie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tangjie Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alleviating Over-smoothing for Unsupervised Sentence Representation. (arXiv:2305.06154v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06154","description":"<p>Currently, learning better unsupervised sentence representations is the\npursuit of many natural language processing communities. Lots of approaches\nbased on pre-trained language models (PLMs) and contrastive learning have\nachieved promising results on this task. Experimentally, we observe that the\nover-smoothing problem reduces the capacity of these powerful PLMs, leading to\nsub-optimal sentence representations. In this paper, we present a Simple method\nnamed Self-Contrastive Learning (SSCL) to alleviate this issue, which samples\nnegatives from PLMs intermediate layers, improving the quality of the sentence\nrepresentation. Our proposed method is quite simple and can be easily extended\nto various state-of-the-art models for performance boosting, which can be seen\nas a plug-and-play contrastive framework for learning unsupervised sentence\nrepresentation. Extensive results prove that SSCL brings the superior\nperformance improvements of different strong baselines (e.g., BERT and SimCSE)\non Semantic Textual Similarity and Transfer datasets. Our codes are available\nat https://github.com/nuochenpku/SSCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_L/0/1/0/all/0/1\">Linjun Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Ming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1\">Bowen Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jianhui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Synthetic Targets for Machine Translation. (arXiv:2305.06155v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06155","description":"<p>In this work, we provide a recipe for training machine translation models in\na limited resource setting by leveraging synthetic target data generated using\na large pre-trained model. We show that consistently across different\nbenchmarks in bilingual, multilingual, and speech translation setups, training\nmodels on synthetic targets outperforms training on the actual ground-truth\ndata. This performance gap grows bigger with increasing limits on the amount of\navailable resources in the form of the size of the dataset and the number of\nparameters in the model. We also provide preliminary analysis into whether this\nboost in performance is linked to ease of optimization or more deterministic\nnature of the predictions, and whether this paradigm leads to better\nout-of-distribution performance across different testing domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1\">Sarthak Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hrinchuk_O/0/1/0/all/0/1\">Oleksii Hrinchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuchaiev_O/0/1/0/all/0/1\">Oleksii Kuchaiev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation. (arXiv:2305.06156v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06156","description":"<p>We present The Vault, an open-source, large-scale code-text dataset designed\nto enhance the training of code-focused large language models (LLMs). Existing\nopen-source datasets for training code-based LLMs often face challenges in\nterms of size, quality (due to noisy signals), and format (only containing code\nfunction and text explanation pairings). The Vault overcomes these limitations\nby providing 40 million code-text pairs across 10 popular programming\nlanguages, thorough cleaning for 10+ prevalent issues, and various levels of\ncode-text pairings, including class, function, and line levels. Researchers and\npractitioners can utilize The Vault for training diverse code-focused LLMs or\nincorporate the provided data cleaning methods and scripts to improve their\ndatasets. By employing The Vault as the training dataset for code-centric LLMs,\nwe anticipate significant advancements in code understanding and generation\ntasks, fostering progress in both artificial intelligence research and software\ndevelopment practices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manh_D/0/1/0/all/0/1\">Dung Nguyen Manh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hai_N/0/1/0/all/0/1\">Nam Le Hai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dau_A/0/1/0/all/0/1\">Anh T. V. Dau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nghiem_K/0/1/0/all/0/1\">Khanh Nghiem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_N/0/1/0/all/0/1\">Nghi D. Q. Bui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implications of Multi-Word Expressions on English to Bharti Braille Machine Translation. (arXiv:2305.06157v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06157","description":"<p>In this paper, we have shown the improvement of English to Bharti Braille\nmachine translation system. We have shown how we can improve a baseline NMT\nmodel by adding some linguistic knowledge to it. This was done for five\nlanguage pairs where English sentences were translated into five Indian\nlanguages and then subsequently to corresponding Bharti Braille. This has been\ndemonstrated by adding a sub-module for translating multi-word expressions. The\napproach shows promising results as across language pairs, we could see\nimprovement in the quality of NMT outputs. The least improvement was observed\nin English-Nepali language pair with 22.08% and the most improvement was\nobserved in the English-Hindi language pair with 23.30%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Nisheeth Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katyayan_P/0/1/0/all/0/1\">Pragya Katyayan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review of Vision-Language Models and their Performance on the Hateful Memes Challenge. (arXiv:2305.06159v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06159","description":"<p>Moderation of social media content is currently a highly manual task, yet\nthere is too much content posted daily to do so effectively. With the advent of\na number of multimodal models, there is the potential to reduce the amount of\nmanual labor for this task. In this work, we aim to explore different models\nand determine what is most effective for the Hateful Memes Challenge, a\nchallenge by Meta designed to further machine learning research in content\nmoderation. Specifically, we explore the differences between early fusion and\nlate fusion models in classifying multimodal memes containing text and images.\nWe first implement a baseline using unimodal models for text and images\nseparately using BERT and ResNet-152, respectively. The outputs from these\nunimodal models were then concatenated together to create a late fusion model.\nIn terms of early fusion models, we implement ConcatBERT, VisualBERT, ViLT,\nCLIP, and BridgeTower. It was found that late fusion performed significantly\nworse than early fusion models, with the best performing model being CLIP which\nachieved an AUROC of 70.06. The code for this work is available at\nhttps://github.com/bzhao18/CS-7643-Project.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bryan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Andrew Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watson_B/0/1/0/all/0/1\">Blake Watson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kearney_G/0/1/0/all/0/1\">Gillian Kearney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dale_I/0/1/0/all/0/1\">Isaac Dale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StarCoder: may the source be with you!. (arXiv:2305.06161v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06161","description":"<p>The BigCode community, an open-scientific collaboration working on the\nresponsible development of Large Language Models for Code (Code LLMs),\nintroduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context\nlength, infilling capabilities and fast large-batch inference enabled by\nmulti-query attention. StarCoderBase is trained on 1 trillion tokens sourced\nfrom The Stack, a large collection of permissively licensed GitHub repositories\nwith inspection tools and an opt-out process. We fine-tuned StarCoderBase on\n35B Python tokens, resulting in the creation of StarCoder. We perform the most\ncomprehensive evaluation of Code LLMs to date and show that StarCoderBase\noutperforms every open Code LLM that supports multiple programming languages\nand matches or outperforms the OpenAI code-cushman-001 model. Furthermore,\nStarCoder outperforms every model that is fine-tuned on Python, can be prompted\nto achieve 40\\% pass@1 on HumanEval, and still retains its performance on other\nprogramming languages. We take several important steps towards a safe\nopen-access model release, including an improved PII redaction pipeline and a\nnovel attribution tracing tool, and make the StarCoder models publicly\navailable under a more commercially viable version of the Open Responsible AI\nModel license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Raymond Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allal_L/0/1/0/all/0/1\">Loubna Ben Allal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zi_Y/0/1/0/all/0/1\">Yangtian Zi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1\">Niklas Muennighoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocetkov_D/0/1/0/all/0/1\">Denis Kocetkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_C/0/1/0/all/0/1\">Chenghao Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marone_M/0/1/0/all/0/1\">Marc Marone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akiki_C/0/1/0/all/0/1\">Christopher Akiki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chim_J/0/1/0/all/0/1\">Jenny Chim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheltonozhskii_E/0/1/0/all/0/1\">Evgenii Zheltonozhskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1\">Terry Yue Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Thomas Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehaene_O/0/1/0/all/0/1\">Olivier Dehaene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davaadorj_M/0/1/0/all/0/1\">Mishig Davaadorj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamy_Poirier_J/0/1/0/all/0/1\">Joel Lamy-Poirier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monteiro_J/0/1/0/all/0/1\">Jo&#xe3;o Monteiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shliazhko_O/0/1/0/all/0/1\">Oleh Shliazhko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gontier_N/0/1/0/all/0/1\">Nicolas Gontier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meade_N/0/1/0/all/0/1\">Nicholas Meade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zebaze_A/0/1/0/all/0/1\">Armel Zebaze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yee_M/0/1/0/all/0/1\">Ming-Ho Yee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umapathi_L/0/1/0/all/0/1\">Logesh Kumar Umapathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipkin_B/0/1/0/all/0/1\">Benjamin Lipkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oblokulov_M/0/1/0/all/0/1\">Muhtasham Oblokulov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiruo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murthy_R/0/1/0/all/0/1\">Rudra Murthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stillerman_J/0/1/0/all/0/1\">Jason Stillerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1\">Siva Sankalp Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abulkhanov_D/0/1/0/all/0/1\">Dmitry Abulkhanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zocca_M/0/1/0/all/0/1\">Marco Zocca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_M/0/1/0/all/0/1\">Manan Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fahmy_N/0/1/0/all/0/1\">Nour Fahmy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_U/0/1/0/all/0/1\">Urvashi Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Swayam Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luccioni_S/0/1/0/all/0/1\">Sasha Luccioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_P/0/1/0/all/0/1\">Paulo Villegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunakov_M/0/1/0/all/0/1\">Maxim Kunakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhdanov_F/0/1/0/all/0/1\">Fedor Zhdanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_M/0/1/0/all/0/1\">Manuel Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tony Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timor_N/0/1/0/all/0/1\">Nadav Timor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jennifer Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlesinger_C/0/1/0/all/0/1\">Claire Schlesinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoelkopf_H/0/1/0/all/0/1\">Hailey Schoelkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebert_J/0/1/0/all/0/1\">Jan Ebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dao_T/0/1/0/all/0/1\">Tri Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_M/0/1/0/all/0/1\">Mayank Mishra</a>, et al. (15 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable multimodal sentiment analysis based on textual modality descriptions by using large-scale language models. (arXiv:2305.06162v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06162","description":"<p>Multimodal sentiment analysis is an important area for understanding the\nuser's internal states. Deep learning methods were effective, but the problem\nof poor interpretability has gradually gained attention. Previous works have\nattempted to use attention weights or vector distributions to provide\ninterpretability. However, their explanations were not intuitive and can be\ninfluenced by different trained models. This study proposed a novel approach to\nprovide interpretability by converting nonverbal modalities into text\ndescriptions and by using large-scale language models for sentiment\npredictions. This provides an intuitive approach to directly interpret what\nmodels depend on with respect to making decisions from input texts, thus\nsignificantly improving interpretability. Specifically, we convert descriptions\nbased on two feature patterns for the audio modality and discrete action units\nfor the facial modality. Experimental results on two sentiment analysis tasks\ndemonstrated that the proposed approach maintained, or even improved\neffectiveness for sentiment analysis compared to baselines using conventional\nfeatures, with the highest improvement of 2.49% on the F1 score. The results\nalso showed that multimodal descriptions have similar characteristics on fusing\nmodalities as those of conventional fusion methods. The results demonstrated\nthat the proposed approach is interpretable and effective for multimodal\nsentiment analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sixia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okada_S/0/1/0/all/0/1\">Shogo Okada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Algebra Error Classification with Large Language Models. (arXiv:2305.06163v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06163","description":"<p>Automated feedback as students answer open-ended math questions has\nsignificant potential in improving learning outcomes at large scale. A key part\nof automated feedback systems is an error classification component, which\nidentifies student errors and enables appropriate, predefined feedback to be\ndeployed. Most existing approaches to error classification use a rule-based\nmethod, which has limited capacity to generalize. Existing data-driven methods\navoid these limitations but specifically require mathematical expressions in\nstudent responses to be parsed into syntax trees. This requirement is itself a\nlimitation, since student responses are not always syntactically valid and\ncannot be converted into trees. In this work, we introduce a flexible method\nfor error classification using pre-trained large language models. We\ndemonstrate that our method can outperform existing methods in algebra error\nclassification, and is able to classify a larger set of student responses.\nAdditionally, we analyze common classification errors made by our method and\ndiscuss limitations of automated error classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McNichols_H/0/1/0/all/0/1\">Hunter McNichols</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengxue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_A/0/1/0/all/0/1\">Andrew Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversational Semantic Parsing using Dynamic Context Graphs. (arXiv:2305.06164v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06164","description":"<p>In this paper we consider the task of conversational semantic parsing over\ngeneral purpose knowledge graphs (KGs) with millions of entities, and thousands\nof relation-types. We are interested in developing models capable of\ninteractively mapping user utterances into executable logical forms (e.g.,\nSPARQL) in the context of the conversational history. Our key idea is to\nrepresent information about an utterance and its context via a subgraph which\nis created dynamically, i.e., the number of nodes varies per utterance.\nMoreover, rather than treating the subgraph as a sequence we exploit its\nunderlying structure, and thus encode it using a graph neural network which\nfurther allows us to represent a large number of (unseen) nodes. Experimental\nresults show that modeling context dynamically is superior to static\napproaches, delivering performance improvements across the board (i.e., for\nsimple and complex questions). Our results further confirm that modeling the\nstructure of context is better at processing discourse information, (i.e., at\nhandling ellipsis and resolving coreference) and longer interactions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Parag Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT as a Text Simplification Tool to Remove Bias. (arXiv:2305.06166v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06166","description":"<p>The presence of specific linguistic signals particular to a certain sub-group\nof people can be picked up by language models during training. This may lead to\ndiscrimination if the model has learnt to pick up on a certain group's\nlanguage. If the model begins to associate specific language with a distinct\ngroup, any decisions made based upon this language would hold a strong\ncorrelation to a decision based on their protected characteristic. We explore a\npossible technique for bias mitigation in the form of simplification of text.\nThe driving force of this idea is that simplifying text should standardise\nlanguage to one way of speaking while keeping the same meaning. The experiment\nshows promising results as the classifier accuracy for predicting the sensitive\nattribute drops by up to 17% for the simplified data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barker_C/0/1/0/all/0/1\">Charmaine Barker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazakov_D/0/1/0/all/0/1\">Dimitar Kazakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QICHWABASE: A Quechua Language and Knowledge Base for Quechua Communities. (arXiv:2305.06173v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06173","description":"<p>Over the last decade, the Web has increasingly become a space of language and\nknowledge representation. However, it is only true for well-spread languages\nand well-established communities, while minority communities and their\nresources received less attention. In this paper, we propose QICHWABASE to\nsupport the harmonization process of the Quechua language and knowledge, and\nits community. For doing it, we adopt methods and tools that could become a\ngame changer in favour of Quechua communities around the world. We conclude\nthat the methodology and tools adopted on building QICHWABASE, which is a\nWikibase instance, could enhance the presence of minorities on the Web.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huaman_E/0/1/0/all/0/1\">Elwin Huaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindemann_D/0/1/0/all/0/1\">David Lindemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caruso_V/0/1/0/all/0/1\">Valeria Caruso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huaman_J/0/1/0/all/0/1\">Jorge Luis Huaman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of Climate Campaigns on Social Media using Bayesian Model Averaging. (arXiv:2305.06174v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06174","description":"<p>Climate change is the defining issue of our time, and we are at a defining\nmoment. Various interest groups, social movement organizations, and individuals\nengage in collective action on this issue on social media. In addition, issue\nadvocacy campaigns on social media often arise in response to ongoing societal\nconcerns, especially those faced by energy industries. Our goal in this paper\nis to analyze how those industries, their advocacy group, and climate advocacy\ngroup use social media to influence the narrative on climate change. In this\nwork, we propose a minimally supervised model soup [56] approach combined with\nmessaging themes to identify the stances of climate ads on Facebook. Finally,\nwe release our stance dataset, model, and set of themes related to climate\ncampaigns for future work on opinion mining and the automatic detection of\nclimate change stances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tunazzina Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuning Language Models with Generative Adversarial Feedback. (arXiv:2305.06176v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06176","description":"<p>Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to\nsignificantly enhance the performance of large language models (LLMs) by\naligning their outputs with desired human values. However, RLHF is constrained\nby the expertise and productivity limitations of human evaluators. In this\nstudy, we investigate an alternative approach: Reinforcement Learning with\nGenerative Adversarial Feedback (RLGAF) to RLHF. Our preliminary findings\nindicate that RLGAF can help align LLMs outputs while not suffering from the\ninherent restrictions of RLHF, suggesting promising avenues for further\nresearch on automating AI alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhang Ze Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaw_L/0/1/0/all/0/1\">Lau Jia Jaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wong Qin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_Z/0/1/0/all/0/1\">Zhang Hui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy-Preserving Prompt Tuning for Large Language Model Services. (arXiv:2305.06212v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06212","description":"<p>Prompt tuning provides an efficient way for users to customize Large Language\nModels (LLMs) with their private data in the emerging LLM service scenario.\nHowever, the sensitive nature of private data brings the need for privacy\npreservation in LLM service customization. Based on prompt tuning, we propose\nPrivacy-Preserving Prompt Tuning (RAPT), a framework that provides privacy\nguarantees for LLM services. \\textsc{rapt} adopts a local privacy setting,\nallowing users to privatize their data locally with local differential privacy.\nAs prompt tuning performs poorly when directly trained on privatized data, we\nintroduce a novel privatized token reconstruction task that is trained jointly\nwith the downstream task, allowing LLMs to learn better task-dependent\nrepresentations. Despite the simplicity of our framework, experiments show that\nRAPT achieves competitive performance across tasks while providing privacy\nguarantees against adversaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yansong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhixing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task End-to-End Training Improves Conversational Recommendation. (arXiv:2305.06218v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06218","description":"<p>In this paper, we analyze the performance of a multitask end-to-end\ntransformer model on the task of conversational recommendations, which aim to\nprovide recommendations based on a user's explicit preferences expressed in\ndialogue. While previous works in this area adopt complex multi-component\napproaches where the dialogue management and entity recommendation tasks are\nhandled by separate components, we show that a unified transformer model, based\non the T5 text-to-text transformer model, can perform competitively in both\nrecommending relevant items and generating conversation dialogue. We fine-tune\nour model on the ReDIAL conversational movie recommendation dataset, and create\nadditional training tasks derived from MovieLens (such as the prediction of\nmovie attributes and related movies based on an input movie), in a multitask\nlearning setting. Using a series of probe studies, we demonstrate that the\nlearned knowledge in the additional tasks is transferred to the conversational\nsetting, where each task leads to a 9%-52% increase in its related probe score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ram_N/0/1/0/all/0/1\">Naveen Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuzmin_D/0/1/0/all/0/1\">Dima Kuzmin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chio_E/0/1/0/all/0/1\">Ellie Ka In Chio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alzantot_M/0/1/0/all/0/1\">Moustafa Farid Alzantot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Ontanon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jash_A/0/1/0/all/0/1\">Ambarish Jash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Judith Yue Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ComputeGPT: A computational chat model for numerical problems. (arXiv:2305.06223v1 [cs.PL])","link":"http://arxiv.org/abs/2305.06223","description":"<p>Language models are not accurate in numerical problems. Their architecture\ndoes not allow for anything less than a probabilistic next word. This paper\nintroduces ComputeGPT: an approach of creating a chat model able to answer\ncomputational problems through running on-demand code. ComputeGPT converts each\nquestion to relevant code, runs the code, and returns the computed answer as\npart of the chat. We combine this approach with a local browser-based Python\ninterpretation and fine-tuned prompts in order to achieve state-of-the-art\nefficiency on numerical problems and provide a suitable front-end and safe\nenvironment for the code to be executed in.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lewis_R/0/1/0/all/0/1\">Ryan Hardesty Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Junfeng Jiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Robust Self-attention Features for Speech Emotion Recognition with Label-adaptive Mixup. (arXiv:2305.06273v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06273","description":"<p>Speech Emotion Recognition (SER) is to recognize human emotions in a natural\nverbal interaction scenario with machines, which is considered as a challenging\nproblem due to the ambiguous human emotions. Despite the recent progress in\nSER, state-of-the-art models struggle to achieve a satisfactory performance. We\npropose a self-attention based method with combined use of label-adaptive mixup\nand center loss. By adapting label probabilities in mixup and fitting center\nloss to the mixup training scheme, our proposed method achieves a superior\nperformance to the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_L/0/1/0/all/0/1\">Lei Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lichao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Dazhi Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Aware Document Simplification. (arXiv:2305.06274v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06274","description":"<p>To date, most work on text simplification has focused on sentence-level\ninputs. Early attempts at document simplification merely applied these\napproaches iteratively over the sentences of a document. However, this fails to\ncoherently preserve the discourse structure, leading to suboptimal output\nquality. Recently, strategies from controllable simplification have been\nleveraged to achieve state-of-the-art results on document simplification by\nfirst generating a document-level plan (a sequence of sentence-level\nsimplification operations) and using this plan to guide sentence-level\nsimplification downstream. However, this is still limited in that the\nsimplification model has no direct access to the local inter-sentence document\ncontext, likely having a negative impact on surface realisation. We explore\nvarious systems that use document context within the simplification process\nitself, either by iterating over larger text units or by extending the system\narchitecture to attend over a high-level representation of document context. In\ndoing so, we achieve state-of-the-art performance on the document\nsimplification task, even when not relying on plan-guidance. Further, we\ninvestigate the performance and efficiency tradeoffs of system variants and\nmake suggestions of when each should be preferred.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cripwell_L/0/1/0/all/0/1\">Liam Cripwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Legrand_J/0/1/0/all/0/1\">Jo&#xeb;l Legrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardent_C/0/1/0/all/0/1\">Claire Gardent</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CADGE: Context-Aware Dialogue Generation Enhanced with Graph-Structured Knowledge Aggregation. (arXiv:2305.06294v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06294","description":"<p>Commonsense knowledge is crucial to many natural language processing tasks.\nExisting works usually incorporate graph knowledge with conventional graph\nneural networks (GNNs), leading to the text and graph knowledge encoding\nprocesses being separated in a serial pipeline. We argue that these separate\nrepresentation learning stages may be suboptimal for neural networks to learn\nthe overall context contained in both types of input knowledge. In this paper,\nwe propose a novel context-aware graph-attention model (Context-aware GAT),\nwhich can effectively incorporate global features of relevant knowledge graphs\nbased on a context-enhanced knowledge aggregation process. Specifically, our\nframework leverages a novel representation learning approach to process\nheterogeneous features - combining flattened graph knowledge with text. To the\nbest of our knowledge, this is the first attempt at hierarchically applying\ngraph knowledge aggregation on a connected subgraph in addition to contextual\ninformation to support commonsense dialogue generation. This framework shows\nsuperior performance compared to conventional GNN-based language frameworks.\nBoth automatic and human evaluation demonstrates that our proposed model has\nsignificant performance uplifts over state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhanga_H/0/1/0/all/0/1\">Hongbo Zhanga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loakmana_T/0/1/0/all/0/1\">Tyler Loakmana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lina_C/0/1/0/all/0/1\">Chenghua Lina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goetze_S/0/1/0/all/0/1\">Stefan Goetze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success). (arXiv:2305.06299v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06299","description":"<p>Large language models, particularly GPT-3, are able to produce high quality\nsummaries of general domain news articles in few- and zero-shot settings.\nHowever, it is unclear if such models are similarly capable in more\nspecialized, high-stakes domains such as biomedicine. In this paper, we enlist\ndomain experts (individuals with medical training) to evaluate summaries of\nbiomedical articles generated by GPT-3, given zero supervision. We consider\nboth single- and multi-document settings. In the former, GPT-3 is tasked with\ngenerating regular and plain-language summaries of articles describing\nrandomized controlled trials; in the latter, we assess the degree to which\nGPT-3 is able to \\emph{synthesize} evidence reported across a collection of\narticles. We design an annotation scheme for evaluating model outputs, with an\nemphasis on assessing the factual accuracy of generated summaries. We find that\nwhile GPT-3 is able to summarize and simplify single biomedical articles\nfaithfully, it struggles to provide accurate aggregations of findings over\nmultiple documents. We release all data and annotations used in this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaib_C/0/1/0/all/0/1\">Chantal Shaib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Millicent L. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_S/0/1/0/all/0/1\">Sebastian Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marshall_I/0/1/0/all/0/1\">Iain J. Marshall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Embedding APIs for Information Retrieval. (arXiv:2305.06300v1 [cs.IR])","link":"http://arxiv.org/abs/2305.06300","description":"<p>The ever-increasing size of language models curtails their widespread access\nto the community, thereby galvanizing many companies and startups into offering\naccess to large language models through APIs. One particular API, suitable for\ndense retrieval, is the semantic embedding API that builds vector\nrepresentations of a given text. With a growing number of APIs at our disposal,\nin this paper, our goal is to analyze semantic embedding APIs in realistic\nretrieval scenarios in order to assist practitioners and researchers in finding\nsuitable services according to their needs. Specifically, we wish to\ninvestigate the capabilities of existing APIs on domain generalization and\nmultilingual retrieval. For this purpose, we evaluate the embedding APIs on two\nstandard benchmarks, BEIR, and MIRACL. We find that re-ranking BM25 results\nusing the APIs is a budget-friendly approach and is most effective on English,\nin contrast to the standard practice, i.e., employing them as first-stage\nretrievers. For non-English retrieval, re-ranking still improves the results,\nbut a hybrid model with BM25 works best albeit at a higher cost. We hope our\nwork lays the groundwork for thoroughly evaluating APIs that are critical in\nsearch and more broadly, in information retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamalloo_E/0/1/0/all/0/1\">Ehsan Kamalloo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogundepo_O/0/1/0/all/0/1\">Odunayo Ogundepo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nandan Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfonso_Hermelo_D/0/1/0/all/0/1\">David Alfonso-Hermelo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Evaluation of Attribution by Large Language Models. (arXiv:2305.06311v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06311","description":"<p>A recent focus of large language model (LLM) development, as exemplified by\ngenerative search engines, is to incorporate external references to generate\nand support their claims. However, evaluating the attribution, i.e., verifying\nwhether the generated statement is indeed fully supported by the cited\nreference, remains an open problem. Although human evaluation is common\npractice, it is costly and time-consuming. In this paper, we investigate the\nautomatic evaluation of attribution by LLMs. We begin by providing a definition\nof attribution and then explore two approaches for automatic evaluation:\nprompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed\nfrom related tasks, such as question answering, fact-checking, natural language\ninference, and summarization. To facilitate the evaluation, we manually curate\na set of test examples covering 12 domains from a generative search engine, New\nBing. Our results on the curated test set and simulated test examples from\nexisting benchmark questions highlight both promising signals as well as\nremaining challenges for the automatic evaluation of attribution. We hope our\ntestbed, modeling methodology, and insights will help lay the foundation for\nfuture studies on this important problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boshi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ziru Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Korean Named Entity Recognition Based on Language-Specific Features. (arXiv:2305.06330v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06330","description":"<p>In the paper, we propose a novel way of improving named entity recognition in\nthe Korean language using its language-specific features. While the field of\nnamed entity recognition has been studied extensively in recent years, the\nmechanism of efficiently recognizing named entities in Korean has hardly been\nexplored. This is because the Korean language has distinct linguistic\nproperties that prevent models from achieving their best performances.\nTherefore, an annotation scheme for {Korean corpora} by adopting the CoNLL-U\nformat, which decomposes Korean words into morphemes and reduces the ambiguity\nof named entities in the original segmentation that may contain functional\nmorphemes such as postpositions and particles, is proposed herein. We\ninvestigate how the named entity tags are best represented in this\nmorpheme-based scheme and implement an algorithm to convert word-based {and\nsyllable-based Korean corpora} with named entities into the proposed\nmorpheme-based format. Analyses of the results of {statistical and neural}\nmodels reveal that the proposed morpheme-based format is feasible, and the\n{varied} performances of the models under the influence of various additional\nlanguage-specific features are demonstrated. Extrinsic conditions were also\nconsidered to observe the variance of the performances of the proposed models,\ngiven different types of data, including the original segmentation and\ndifferent types of tagging formats.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yige Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1\">KyungTae Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jungyeul Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"K-UniMorph: Korean Universal Morphology and its Feature Schema. (arXiv:2305.06335v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06335","description":"<p>We present in this work a new Universal Morphology dataset for Korean.\nPreviously, the Korean language has been underrepresented in the field of\nmorphological paradigms amongst hundreds of diverse world languages. Hence, we\npropose this Universal Morphological paradigms for the Korean language that\npreserve its distinct characteristics. For our K-UniMorph dataset, we outline\neach grammatical criterion in detail for the verbal endings, clarify how to\nextract inflected forms, and demonstrate how we generate the morphological\nschemata. This dataset adopts morphological feature schema from Sylak-Glassman\net al. (2015) and Sylak-Glassman (2016) for the Korean language as we extract\ninflected verb forms from the Sejong morphologically analyzed corpus that is\none of the largest annotated corpora for Korean. During the data creation, our\nmethodology also includes investigating the correctness of the conversion from\nthe Sejong corpus. Furthermore, we carry out the inflection task using three\ndifferent Korean word forms: letters, syllables and morphemes. Finally, we\ndiscuss and describe future perspectives on Korean morphological paradigms and\nthe dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jo_E/0/1/0/all/0/1\">Eunkyul Leah Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyuwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xihan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1\">KyungTae Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jungyeul Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chulwoo Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RECKONING: Reasoning through Dynamic Knowledge Encoding. (arXiv:2305.06349v1 [cs.CL])","link":"http://arxiv.org/abs/2305.06349","description":"<p>Recent studies on transformer-based language models show that they can answer\nquestions by reasoning over knowledge provided as part of the context (i.e.,\nin-context reasoning). However, since the available knowledge is often not\nfiltered for a particular question, in-context reasoning can be sensitive to\ndistractor facts, additional content that is irrelevant to a question but that\nmay be relevant for a different question (i.e., not necessarily random noise).\nIn these situations, the model fails to distinguish the knowledge that is\nnecessary to answer the question, leading to spurious reasoning and degraded\nperformance. This reasoning failure contrasts with the model's apparent ability\nto distinguish its contextual knowledge from all the knowledge it has memorized\nduring pre-training. Following this observation, we propose teaching the model\nto reason more robustly by folding the provided contextual knowledge into the\nmodel's parameters before presenting it with a question. Our method, RECKONING,\nis a bi-level learning algorithm that teaches language models to reason by\nupdating their parametric knowledge through back-propagation, allowing them to\nthen answer questions using the updated parameters. During training, the inner\nloop rapidly adapts a copy of the model weights to encode contextual knowledge\ninto its parameters. In the outer loop, the model learns to uses the updated\nweights to reproduce and answer reasoning questions about the memorized\nknowledge. Our experiments on two multi-hop reasoning datasets show that\nRECKONING's performance improves over the in-context reasoning baseline (by up\nto 4.5%). We also find that compared to in-context reasoning, RECKONING\ngeneralizes better to longer reasoning chains unseen during training, is more\nrobust to distractors in the context, and is more computationally efficient\nwhen multiple questions are asked about the same knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1\">Gail Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_E/0/1/0/all/0/1\">Eric Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VideoChat: Chat-Centric Video Understanding. (arXiv:2305.06355v1 [cs.CV])","link":"http://arxiv.org/abs/2305.06355","description":"<p>In this study, we initiate an exploration into video understanding by\nintroducing VideoChat, an end-to-end chat-centric video understanding system.\nIt integrates video foundation models and large language models via a learnable\nneural interface, excelling in spatiotemporal reasoning, event localization,\nand causal relationship inference. To instructively tune this system, we\npropose a video-centric instruction dataset, composed of thousands of videos\nmatched with detailed descriptions and conversations. This dataset emphasizes\nspatiotemporal reasoning and causal relationships, providing a valuable asset\nfor training chat-centric video understanding systems. Preliminary qualitative\nexperiments reveal our system's potential across a broad spectrum of video\napplications and set the standard for future research. Access our code and data\nat https://github.com/OpenGVLab/Ask-Anything\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">KunChang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yinan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yizhuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yali Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Marshall-Olkin Power-Law Distributions in Length-Frequency of Entities. (arXiv:1811.03325v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1811.03325","description":"<p>Entities involve important concepts with concrete meanings and play important\nroles in numerous linguistic tasks. Entities have different forms in different\ntasks and researchers treat those forms as different concepts. In this paper,\nwe are curious to know whether there are some common characteristics connecting\nthose different forms of entities. Specifically, we investigate the underlying\ndistributions of entities from different types and different languages, trying\nto figure out some common properties behind those diverse entities. We find\nfrom twelve datasets about different types of entities and eighteen datasets\nabout different languages of entities that although these entities are\ndramatically diverse from each in many aspects, their length-frequencies can be\nwell characterized by Marshall-Olkin power-law (MOPL) distributions, and these\ndistributions possess defined means and finite variances. Our experiments show\nthat while not all the entities are drawn from the same underlying population,\nthose entities under same types tend to be drawn from the same distribution.\nOur experiments also show that Marshall-Olkin power-law models characterize the\nlength-frequencies of entities much better than pure power-law models and\nlog-normal models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1\">Xiaoshi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajapakse_J/0/1/0/all/0/1\">Jagath C. Rajapakse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Transformer Decoder with Compressed Sub-layers. (arXiv:2101.00542v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00542","description":"<p>The large attention-based encoder-decoder network (Transformer) has become\nprevailing recently due to its effectiveness. But the high computation\ncomplexity of its decoder raises the inefficiency issue. By examining the\nmathematic formulation of the decoder, we show that under some mild conditions,\nthe architecture could be simplified by compressing its sub-layers, the basic\nbuilding block of Transformer, and achieves a higher parallelism. We thereby\npropose Compressed Attention Network, whose decoder layer consists of only one\nsub-layer instead of three. Extensive experiments on 14 WMT machine translation\ntasks show that our model is 1.42x faster with performance on par with a strong\nbaseline. This strong baseline is already 2x faster than the widely used\nstandard baseline without loss in performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Ye Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAP-Gen: Guided Automatic Python Code Generation. (arXiv:2201.08810v2 [cs.PL] UPDATED)","link":"http://arxiv.org/abs/2201.08810","description":"<p>Automatic code generation from natural language descriptions can be highly\nbeneficial during the process of software development. In this work, we propose\nGAP-Gen, a Guided Automatic Python Code Generation method based on Python\nsyntactic constraints and semantic constraints. We first introduce Python\nsyntactic constraints in the form of Syntax-Flow, which is a simplified version\nof Abstract Syntax Tree (AST) reducing the size and high complexity of Abstract\nSyntax Tree but maintaining crucial syntactic information of Python code. In\naddition to Syntax-Flow, we introduce Variable-Flow which abstracts variable\nand function names consistently through out the code. In our work, rather than\npretraining, we focus on modifying the finetuning process which reduces\ncomputational requirements but retains high generation performance on automatic\nPython code generation task. GAP-Gen fine-tunes the transformer based language\nmodels T5 and CodeT5 using the Code-to-Docstring datasets CodeSearchNet,\nCodeSearchNet AdvTest and Code-Docstring Corpus from EdinburghNLP. Our\nexperiments show that GAP-Gen achieves better results on automatic Python code\ngeneration task than previous works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yurun Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junlin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harris_I/0/1/0/all/0/1\">Ian G. Harris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation. (arXiv:2202.13047v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13047","description":"<p>Crowdsourced dialogue corpora are usually limited in scale and topic coverage\ndue to the expensive cost of data curation. This would hinder the\ngeneralization of downstream dialogue models to open-domain topics. In this\nwork, we leverage large language models for dialogue augmentation in the task\nof emotional support conversation (ESC). By treating dialogue augmentation as a\ndialogue completion task, we prompt a fine-tuned language model to complete\nfull dialogues from available dialogue posts of various topics, which are then\npostprocessed based on heuristics. Applying this approach, we construct AugESC,\nan augmented dataset for the ESC task, which largely extends the scale and\ntopic coverage of the crowdsourced ESConv corpus. Through comprehensive human\nevaluation, we demonstrate that our approach is superior to strong baselines of\ndialogue augmentation and that AugESC has comparable dialogue quality to the\ncrowdsourced corpus. We also conduct human interactive evaluation and prove\nthat post-training on AugESC improves downstream dialogue models'\ngeneralization ability to open-domain topics. These results suggest the utility\nof AugESC and highlight the potential of large language models in improving\ndata-scarce dialogue generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chujie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabour_S/0/1/0/all/0/1\">Sahand Sabour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jiaxin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation Projection Invariance Mitigates Representation Collapse. (arXiv:2205.11603v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11603","description":"<p>Fine-tuning contextualized representations learned by pre-trained language\nmodels remains a prevalent practice in NLP. However, fine-tuning can lead to\nrepresentation degradation (also known as representation collapse), which may\nresult in instability, sub-optimal performance, and weak generalization.\n</p>\n<p>In this paper, we propose Representation Projection Invariance (REPINA), a\nnovel regularization method to maintain the information content of\nrepresentation and reduce representation collapse during fine-tuning by\ndiscouraging undesirable changes in the representations. We study the empirical\nbehavior of the proposed regularization in comparison to 5 comparable baselines\nacross 13 language understanding tasks (GLUE benchmark and six additional\ndatasets). When evaluating in-domain performance, REPINA consistently\noutperforms other baselines on most tasks (10 out of 13). We also demonstrate\nits effectiveness in few-shot settings and robustness to label perturbation. As\na by-product, we extend previous studies of representation collapse and propose\nseveral metrics to quantify it. Our empirical findings show that our approach\nis significantly more effective at mitigating representation collapse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Razdaibiedina_A/0/1/0/all/0/1\">Anastasia Razdaibiedina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khetan_A/0/1/0/all/0/1\">Ashish Khetan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karnin_Z/0/1/0/all/0/1\">Zohar Karnin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_V/0/1/0/all/0/1\">Vishaal Kapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madan_V/0/1/0/all/0/1\">Vivek Madan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QAMPARI: An Open-domain Question Answering Benchmark for Questions with Many Answers from Multiple Paragraphs. (arXiv:2205.12665v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12665","description":"<p>Existing benchmarks for open-domain question answering (ODQA) typically focus\non questions whose answers can be extracted from a single paragraph. By\ncontrast, many natural questions, such as \"What players were drafted by the\nBrooklyn Nets?\" have a list of answers. Answering such questions requires\nretrieving and reading from many passages, in a large corpus. We introduce\nQAMPARI, an ODQA benchmark, where question answers are lists of entities,\nspread across many paragraphs. We created QAMPARI by (a) generating questions\nwith multiple answers from Wikipedia's knowledge graph and tables, (b)\nautomatically pairing answers with supporting evidence in Wikipedia paragraphs,\nand (c) manually paraphrasing questions and validating each answer. We train\nODQA models from the retrieve-and-read family and find that QAMPARI is\nchallenging in terms of both passage retrieval and answer generation, reaching\nan F1 score of 32.8 at best. Our results highlight the need for developing ODQA\nmodels that handle a broad range of question types, including single and\nmulti-answer questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amouyal_S/0/1/0/all/0/1\">Samuel Joseph Amouyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfson_T/0/1/0/all/0/1\">Tomer Wolfson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubin_O/0/1/0/all/0/1\">Ohad Rubin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoran_O/0/1/0/all/0/1\">Ori Yoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1\">Jonathan Herzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Paragraph-Level Vision-Language Semantic Alignment for Multi-Modal Summarization. (arXiv:2208.11303v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.11303","description":"<p>Most current multi-modal summarization methods follow a cascaded manner,\nwhere an off-the-shelf object detector is first used to extract visual\nfeatures, then these features are fused with language representations to\ngenerate the summary with an encoder-decoder model. The cascaded way cannot\ncapture the semantic alignments between images and paragraphs, which are\ncrucial to a precise summary. In this paper, we propose ViL-Sum to jointly\nmodel paragraph-level \\textbf{Vi}sion-\\textbf{L}anguage Semantic Alignment and\nMulti-Modal \\textbf{Sum}marization. The core of ViL-Sum is a joint multi-modal\nencoder with two well-designed tasks, image reordering and image selection. The\njoint multi-modal encoder captures the interactions between modalities, where\nthe reordering task guides the model to learn paragraph-level semantic\nalignment and the selection task guides the model to selected summary-related\nimages in the final summary. Experimental results show that our proposed\nViL-Sum significantly outperforms current state-of-the-art methods. In further\nanalysis, we find that two well-designed tasks and joint multi-modal encoder\ncan effectively guide the model to learn reasonable paragraphs-images and\nsummary-images relations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Chenhao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xinnian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuangzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"C2KD: Cross-Lingual Cross-Modal Knowledge Distillation for Multilingual Text-Video Retrieval. (arXiv:2210.03625v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03625","description":"<p>Multilingual text-video retrieval methods have improved significantly in\nrecent years, but the performance for other languages lags behind English. We\npropose a Cross-Lingual Cross-Modal Knowledge Distillation method to improve\nmultilingual text-video retrieval. Inspired by the fact that English text-video\nretrieval outperforms other languages, we train a student model using input\ntext in different languages to match the cross-modal predictions from teacher\nmodels using input text in English. We propose a cross entropy based objective\nwhich forces the distribution over the student's text-video similarity scores\nto be similar to those of the teacher models. We introduce a new multilingual\nvideo dataset, Multi-YouCook2, by translating the English captions in the\nYouCook2 video dataset to 8 other languages. Our method improves multilingual\ntext-video retrieval performance on Multi-YouCook2 and several other datasets\nsuch as Multi-MSRVTT and VATEX. We also conducted an analysis on the\neffectiveness of different multilingual text models as teachers. The code,\nmodels, and dataset are available at https://github.com/roudimit/c2kd.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rouditchenko_A/0/1/0/all/0/1\">Andrew Rouditchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yung-Sung Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shvetsova_N/0/1/0/all/0/1\">Nina Shvetsova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Samuel Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1\">Leonid Karlinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1\">Hilde Kuehne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Cultural Commonsense Knowledge at Scale. (arXiv:2210.07763v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07763","description":"<p>Structured knowledge is important for many AI applications. Commonsense\nknowledge, which is crucial for robust human-centric AI, is covered by a small\nnumber of structured knowledge projects. However, they lack knowledge about\nhuman traits and behaviors conditioned on socio-cultural contexts, which is\ncrucial for situative AI. This paper presents CANDLE, an end-to-end methodology\nfor extracting high-quality cultural commonsense knowledge (CCSK) at scale.\nCANDLE extracts CCSK assertions from a huge web corpus and organizes them into\ncoherent clusters, for 3 domains of subjects (geography, religion, occupation)\nand several cultural facets (food, drinks, clothing, traditions, rituals,\nbehaviors). CANDLE includes judicious techniques for classification-based\nfiltering and scoring of interestingness. Experimental evaluations show the\nsuperiority of the CANDLE CCSK collection over prior works, and an extrinsic\nuse case demonstrates the benefits of CCSK for the GPT-3 language model. Code\nand data can be accessed at https://candle.mpi-inf.mpg.de/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan-Phong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1\">Simon Razniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varde_A/0/1/0/all/0/1\">Aparna Varde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QuaLA-MiniLM: a Quantized Length Adaptive MiniLM. (arXiv:2210.17114v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.17114","description":"<p>Limited computational budgets often prevent transformers from being used in\nproduction and from having their high accuracy utilized. A knowledge\ndistillation approach addresses the computational efficiency by self-distilling\nBERT into a smaller transformer representation having fewer layers and smaller\ninternal embedding. However, the performance of these models drops as we reduce\nthe number of layers, notably in advanced NLP tasks such as span question\nanswering. In addition, a separate model must be trained for each inference\nscenario with its distinct computational budget. Dynamic-TinyBERT tackles both\nlimitations by partially implementing the Length Adaptive Transformer (LAT)\ntechnique onto TinyBERT, achieving x3 speedup over BERT-base with minimal\naccuracy loss. In this work, we expand the Dynamic-TinyBERT approach to\ngenerate a much more highly efficient model. We use MiniLM distillation jointly\nwith the LAT method, and we further enhance the efficiency by applying low-bit\nquantization. Our quantized length-adaptive MiniLM model (QuaLA-MiniLM) is\ntrained only once, dynamically fits any inference scenario, and achieves an\naccuracy-efficiency trade-off superior to any other efficient approaches per\nany computational budget on the SQuAD1.1 dataset (up to x8.8 speedup with &lt;1%\naccuracy loss). The code to reproduce this work is publicly available on\nGithub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guskin_S/0/1/0/all/0/1\">Shira Guskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasserblat_M/0/1/0/all/0/1\">Moshe Wasserblat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Haihao Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMAuC -- The Scientific Multi-Authorship Corpus. (arXiv:2211.02477v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.02477","description":"<p>The rapidly growing volume of scientific publications offers an interesting\nchallenge for research on methods for analyzing the authorship of documents\nwith one or more authors. However, most existing datasets lack scientific\ndocuments or the necessary metadata for constructing new experiments and test\ncases. We introduce SMAuC, a comprehensive, metadata-rich corpus tailored to\nscientific authorship analysis. Comprising over 3 million publications across\nvarious disciplines from over 5 million authors, SMAuC is the largest openly\naccessible corpus for this purpose. It encompasses scientific texts from\nhumanities and natural sciences, accompanied by extensive, curated metadata,\nincluding unambiguous author IDs. SMAuC aims to significantly advance the\ndomain of authorship analysis in scientific texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bevendorff_J/0/1/0/all/0/1\">Janek Bevendorff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sauer_P/0/1/0/all/0/1\">Philipp Sauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gienapp_L/0/1/0/all/0/1\">Lukas Gienapp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kircheis_W/0/1/0/all/0/1\">Wolfgang Kircheis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korner_E/0/1/0/all/0/1\">Erik K&#xf6;rner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_B/0/1/0/all/0/1\">Benno Stein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What's happening in your neighborhood? A Weakly Supervised Approach to Detect Local News. (arXiv:2301.08146v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2301.08146","description":"<p>Local news articles are a subset of news that impact users in a geographical\narea, such as a city, county, or state. Detecting local news (Step 1) and\nsubsequently deciding its geographical location as well as radius of impact\n(Step 2) are two important steps towards accurate local news recommendation.\nNaive rule-based methods, such as detecting city names from the news title,\ntend to give erroneous results due to lack of understanding of the news\ncontent. Empowered by the latest development in natural language processing, we\ndevelop an integrated pipeline that enables automatic local news detection and\ncontent-based local news recommendations. In this paper, we focus on Step 1 of\nthe pipeline, which highlights: (1) a weakly supervised framework incorporated\nwith domain knowledge and auto data processing, and (2) scalability to\nmulti-lingual settings. Compared with Stanford CoreNLP NER model, our pipeline\nhas higher precision and recall evaluated on a real-world and human-labeled\ndataset. This pipeline has potential to more precise local news to users, helps\nlocal businesses get more exposure, and gives people more information about\ntheir neighborhood safety.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1\">Deven Santosh Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shiying He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddiqi_G/0/1/0/all/0/1\">Gosuddin Kamaruddin Siddiqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_R/0/1/0/all/0/1\">Radhika Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distinguishability Calibration to In-Context Learning. (arXiv:2302.06198v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.06198","description":"<p>Recent years have witnessed increasing interests in prompt-based learning in\nwhich models can be trained on only a few annotated instances, making them\nsuitable in low-resource settings. When using prompt-based learning for text\nclassification, the goal is to use a pre-trained language model (PLM) to\npredict a missing token in a pre-defined template given an input text, which\ncan be mapped to a class label. However, PLMs built on the transformer\narchitecture tend to generate similar output embeddings, making it difficult to\ndiscriminate between different class labels. The problem is further exacerbated\nwhen dealing with classification tasks involving many fine-grained class\nlabels. In this work, we alleviate this information diffusion issue, i.e.,\ndifferent tokens share a large proportion of similar information after going\nthrough stacked multiple self-attention layers in a transformer, by proposing a\ncalibration method built on feature transformations through rotation and\nscaling to map a PLM-encoded embedding into a new metric space to guarantee the\ndistinguishability of the resulting embeddings. Furthermore, we take the\nadvantage of hyperbolic embeddings to capture the hierarchical relations among\nfine-grained class-associated token embedding by a coarse-to-fine metric\nlearning strategy to enhance the distinguishability of the learned output\nembeddings. Extensive experiments on the three datasets under various settings\ndemonstrate the effectiveness of our approach. Our code can be found at\nhttps://github.com/donttal/TARA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hanqi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1\">Li Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1\">Lin Gui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rediscovery of CNN's Versatility for Text-based Encoding of Raw Electronic Health Records. (arXiv:2303.08290v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2303.08290","description":"<p>Making the most use of abundant information in electronic health records\n(EHR) is rapidly becoming an important topic in the medical domain. Recent work\npresented a promising framework that embeds entire features in raw EHR data\nregardless of its form and medical code standards. The framework, however, only\nfocuses on encoding EHR with minimal preprocessing and fails to consider how to\nlearn efficient EHR representation in terms of computation and memory usage. In\nthis paper, we search for a versatile encoder not only reducing the large data\ninto a manageable size but also well preserving the core information of\npatients to perform diverse clinical tasks. We found that hierarchically\nstructured Convolutional Neural Network (CNN) often outperforms the\nstate-of-the-art model on diverse tasks such as reconstruction, prediction, and\ngeneration, even with fewer parameters and less training time. Moreover, it\nturns out that making use of the inherent hierarchy of EHR data can boost the\nperformance of any kind of backbone models and clinical tasks performed.\nThrough extensive experiments, we present concrete evidence to generalize our\nresearch findings into real-world practice. We give a clear guideline on\nbuilding the encoder based on the research findings captured while exploring\nnumerous settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_E/0/1/0/all/0/1\">Eunbyeol Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Min Jae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hur_K/0/1/0/all/0/1\">Kyunghoon Hur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiyoun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jinsung Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better Language Models of Code through Self-Improvement. (arXiv:2304.01228v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01228","description":"<p>Pre-trained language models for code (PLMCs) have gained attention in recent\nresearch. These models are pre-trained on large-scale datasets using\nmulti-modal objectives. However, fine-tuning them requires extensive\nsupervision and is limited by the size of the dataset provided. We aim to\nimprove this issue by proposing a simple data augmentation framework. Our\nframework utilizes knowledge gained during the pre-training and fine-tuning\nstage to generate pseudo data, which is then used as training data for the next\nstep. We incorporate this framework into the state-of-the-art language models,\nsuch as CodeT5, CodeBERT, and UnixCoder. The results show that our framework\nsignificantly improves PLMCs' performance in code-related sequence generation\ntasks, such as code summarization and code generation in the CodeXGLUE\nbenchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+To_H/0/1/0/all/0/1\">Hung Quoc To</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_N/0/1/0/all/0/1\">Nghi D. Q. Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tien N. Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nominal Topology for Data Languages. (arXiv:2304.13337v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.13337","description":"<p>We propose a novel topological perspective on data languages recognizable by\norbit-finite nominal monoids. For this purpose, we introduce pro-orbit-finite\nnominal topological spaces. Assuming globally bounded support sizes, they\ncoincide with nominal Stone spaces and are shown to be dually equivalent to a\nsubcategory of nominal boolean algebras. Recognizable data languages are\ncharacterized as topologically clopen sets of pro-orbit-finite words. In\naddition, we explore the expressive power of pro-orbit-finite equations by\nestablishing a nominal version of Reiterman's pseudovariety theorem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Birkmann_F/0/1/0/all/0/1\">Fabian Birkmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milius_S/0/1/0/all/0/1\">Stefan Milius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urbat_H/0/1/0/all/0/1\">Henning Urbat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Turning Flowchart into Dialog: Plan-based Data Augmentation for Low-Resource Flowchart-grounded Troubleshooting Dialogs. (arXiv:2305.01323v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01323","description":"<p>Flowchart-grounded troubleshooting dialogue (FTD) systems, which follow the\ninstructions of a flowchart to diagnose users' problems in specific domains\n(eg., vehicle, laptop), have been gaining research interest in recent years.\nHowever, collecting sufficient dialogues that are naturally grounded on\nflowcharts is costly, thus FTD systems are impeded by scarce training data. To\nmitigate the data sparsity issue, we propose a plan-based data augmentation\n(PlanDA) approach that generates diverse synthetic dialog data at scale by\ntransforming concise flowchart into dialogues. Specifically, its generative\nmodel employs a variational-base framework with a hierarchical planning\nstrategy that includes global and local latent planning variables. Experiments\non the FloDial dataset show that synthetic dialogue produced by PlanDA improves\nthe performance of downstream tasks, including flowchart path retrieval and\nresponse generation, in particular on the Out-of-Flowchart settings. In\naddition, further analysis demonstrate the quality of synthetic data generated\nby PlanDA in paths that are covered by current sample dialogues and paths that\nare not covered.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1\">Haolan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maruf_S/0/1/0/all/0/1\">Sameen Maruf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lizhen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zukerman_I/0/1/0/all/0/1\">Ingrid Zukerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causality-aware Concept Extraction based on Knowledge-guided Prompting. (arXiv:2305.01876v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01876","description":"<p>Concepts benefit natural language understanding but are far from complete in\nexisting knowledge graphs (KGs). Recently, pre-trained language models (PLMs)\nhave been widely used in text-based concept extraction (CE). However, PLMs tend\nto mine the co-occurrence associations from massive corpus as pre-trained\nknowledge rather than the real causal effect between tokens. As a result, the\npre-trained knowledge confounds PLMs to extract biased concepts based on\nspurious co-occurrence correlations, inevitably resulting in low precision. In\nthis paper, through the lens of a Structural Causal Model (SCM), we propose\nequipping the PLM-based extractor with a knowledge-guided prompt as an\nintervention to alleviate concept bias. The prompt adopts the topic of the\ngiven entity from the existing knowledge in KGs to mitigate the spurious\nco-occurrence correlations between entities and biased concepts. Our extensive\nexperiments on representative multilingual KG datasets justify that our\nproposed prompt can effectively alleviate concept bias and improve the\nperformance of PLM-based CE models.The code has been released on\nhttps://github.com/siyuyuan/KPCE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Siyu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Deqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinxi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Shuyu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiaqing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Rui Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curating corpora with classifiers: A case study of clean energy sentiment online. (arXiv:2305.03092v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03092","description":"<p>Well curated, large-scale corpora of social media posts containing broad\npublic opinion offer an alternative data source to complement traditional\nsurveys. While surveys are effective at collecting representative samples and\nare capable of achieving high accuracy, they can be both expensive to run and\nlag public opinion by days or weeks. Both of these drawbacks could be overcome\nwith a real-time, high volume data stream and fast analysis pipeline. A central\nchallenge in orchestrating such a data pipeline is devising an effective method\nfor rapidly selecting the best corpus of relevant documents for analysis.\nQuerying with keywords alone often includes irrelevant documents that are not\neasily disambiguated with bag-of-words natural language processing methods.\nHere, we explore methods of corpus curation to filter irrelevant tweets using\npre-trained transformer-based models, fine-tuned for our binary classification\ntask on hand-labeled tweets. We are able to achieve F1 scores of up to 0.95.\nThe low cost and high performance of fine-tuning such a model suggests that our\napproach could be of broad benefit as a pre-processing step for social media\ndatasets with uncertain corpus boundaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arnold_M/0/1/0/all/0/1\">Michael V. Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodds_P/0/1/0/all/0/1\">Peter Sheridan Dodds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danforth_C/0/1/0/all/0/1\">Christopher M. Danforth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages. (arXiv:2305.04160v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.04160","description":"<p>Large language models (LLMs) have demonstrated remarkable language abilities.\nGPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities\nbeyond previous visual language models. We attribute this to the use of more\nadvanced LLMs compared with previous multimodal models. Unfortunately, the\nmodel architecture and training strategies of GPT-4 are unknown. To endow LLMs\nwith multimodal capabilities, we propose X-LLM, which converts Multi-modalities\n(images, speech, videos) into foreign languages using X2L interfaces and inputs\nthem into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple\nfrozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X''\ndenotes multi-modalities such as image, speech, and videos, and ``L'' denotes\nlanguages. X-LLM's training consists of three stages: (1) Converting Multimodal\nInformation: The first stage trains each X2L interface to align with its\nrespective single-modal encoder separately to convert multimodal information\ninto languages. (2) Aligning X2L representations with the LLM: single-modal\nencoders are aligned with the LLM through X2L interfaces independently. (3)\nIntegrating multiple modalities: all single-modal encoders are aligned with the\nLLM through X2L interfaces to integrate multimodal capabilities into the LLM.\nOur experiments show that X-LLM demonstrates impressive multimodel chat\nabilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen\nimages/instructions, and yields a 84.5\\% relative score compared with GPT-4 on\na synthetic multimodal instruction-following dataset. And we also conduct\nquantitative tests on using LLM for ASR and multimodal ASR, hoping to promote\nthe era of LLM-based speech recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Minglun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haozhi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAT: A Contextualized Conceptualization and Instantiation Framework for Commonsense Reasoning. (arXiv:2305.04808v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.04808","description":"<p>Commonsense reasoning, aiming at endowing machines with a human-like ability\nto make situational presumptions, is extremely challenging to generalize. For\nsomeone who barely knows about \"meditation,\" while is knowledgeable about\n\"singing,\" he can still infer that \"meditation makes people relaxed\" from the\nexisting knowledge that \"singing makes people relaxed\" by first conceptualizing\n\"singing\" as a \"relaxing event\" and then instantiating that event to\n\"meditation.\" This process, known as conceptual induction and deduction, is\nfundamental to commonsense reasoning while lacking both labeled data and\nmethodologies to enhance commonsense modeling. To fill such a research gap, we\npropose CAT (Contextualized ConceptuAlization and InsTantiation), a\nsemi-supervised learning framework that integrates event conceptualization and\ninstantiation to conceptualize commonsense knowledge bases at scale. Extensive\nexperiments show that our framework achieves state-of-the-art performances on\ntwo conceptualization tasks, and the acquired abstract commonsense knowledge\ncan significantly improve commonsense inference modeling. Our code, data, and\nfine-tuned models are publicly available at\nhttps://github.com/HKUST-KnowComp/CAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1\">Tianqing Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Baixuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bo_C/0/1/0/all/0/1\">Chun Yi Louis Bo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Web Content Filtering through knowledge distillation of Large Language Models. (arXiv:2305.05027v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.05027","description":"<p>We introduce a state-of-the-art approach for URL categorization that\nleverages the power of Large Language Models (LLMs) to address the primary\nobjectives of web content filtering: safeguarding organizations from legal and\nethical risks, limiting access to high-risk or suspicious websites, and\nfostering a secure and professional work environment. Our method utilizes LLMs\nto generate accurate classifications and then employs established knowledge\ndistillation techniques to create smaller, more specialized student models\ntailored for web content filtering. Distillation results in a student model\nwith a 9% accuracy rate improvement in classifying websites, sourced from\ncustomer telemetry data collected by a large security vendor, into 30 distinct\ncontent categories based on their URLs, surpassing the current state-of-the-art\napproach. Our student model matches the performance of the teacher LLM with 175\ntimes less parameters, allowing the model to be used for in-line scanning of\nlarge volumes of URLs, and requires 3 orders of magnitude less manually labeled\ntraining data than the current state-of-the-art approach. Depending on the\nspecific use case, the output generated by our approach can either be directly\nreturned or employed as a pre-filter for more resource-intensive operations\ninvolving website images or HTML.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Voros_T/0/1/0/all/0/1\">Tam&#xe1;s V&#xf6;r&#xf6;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergeron_S/0/1/0/all/0/1\">Sean Paul Bergeron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berlin_K/0/1/0/all/0/1\">Konstantin Berlin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E2TIMT: Efficient and Effective Modal Adapter for Text Image Machine Translation. (arXiv:2305.05166v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.05166","description":"<p>Text image machine translation (TIMT) aims to translate texts embedded in\nimages from one source language to another target language. Existing methods,\nboth two-stage cascade and one-stage end-to-end architectures, suffer from\ndifferent issues. The cascade models can benefit from the large-scale optical\ncharacter recognition (OCR) and MT datasets but the two-stage architecture is\nredundant. The end-to-end models are efficient but suffer from training data\ndeficiency. To this end, in our paper, we propose an end-to-end TIMT model\nfully making use of the knowledge from existing OCR and MT datasets to pursue\nboth an effective and efficient framework. More specifically, we build a novel\nmodal adapter effectively bridging the OCR encoder and MT decoder. End-to-end\nTIMT loss and cross-modal contrastive loss are utilized jointly to align the\nfeature distribution of the OCR and MT tasks. Extensive experiments show that\nthe proposed method outperforms the existing two-stage cascade models and\none-stage end-to-end models with a lighter and faster architecture.\nFurthermore, the ablation studies verify the generalization of our method,\nwhere the proposed modal adapter is effective to bridge various OCR and MT\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Cong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yaping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_M/0/1/0/all/0/1\">Mei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_C/0/1/0/all/0/1\">Chengqing Zong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Teacher Knowledge Distillation For Text Image Machine Translation. (arXiv:2305.05226v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.05226","description":"<p>Text image machine translation (TIMT) has been widely used in various\nreal-world applications, which translates source language texts in images into\nanother target language sentence. Existing methods on TIMT are mainly divided\ninto two categories: the recognition-then-translation pipeline model and the\nend-to-end model. However, how to transfer knowledge from the pipeline model\ninto the end-to-end model remains an unsolved problem. In this paper, we\npropose a novel Multi-Teacher Knowledge Distillation (MTKD) method to\neffectively distillate knowledge into the end-to-end TIMT model from the\npipeline model. Specifically, three teachers are utilized to improve the\nperformance of the end-to-end TIMT model. The image encoder in the end-to-end\nTIMT model is optimized with the knowledge distillation guidance from the\nrecognition teacher encoder, while the sequential encoder and decoder are\nimproved by transferring knowledge from the translation sequential and decoder\nteacher models. Furthermore, both token and sentence-level knowledge\ndistillations are incorporated to better boost the translation performance.\nExtensive experimental results show that our proposed MTKD effectively improves\nthe text image translation performance and outperforms existing end-to-end and\npipeline models with fewer parameters and less decoding time, illustrating that\nMTKD can take advantage of both pipeline and end-to-end models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Cong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yaping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_M/0/1/0/all/0/1\">Mei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_C/0/1/0/all/0/1\">Chengqing Zong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Script Knowledge from Large Language Models for Constrained Language Planning. (arXiv:2305.05252v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.05252","description":"<p>In everyday life, humans often plan their actions by following step-by-step\ninstructions in the form of goal-oriented scripts. Previous work has exploited\nlanguage models (LMs) to plan for abstract goals of stereotypical activities\n(e.g., \"make a cake\"), but leaves more specific goals with multi-facet\nconstraints understudied (e.g., \"make a cake for diabetics\"). In this paper, we\ndefine the task of constrained language planning for the first time. We propose\nan overgenerate-then-filter approach to improve large language models (LLMs) on\nthis task, and use it to distill a novel constrained language planning dataset,\nCoScript, which consists of 55,000 scripts. Empirical results demonstrate that\nour method significantly improves the constrained language planning ability of\nLLMs, especially on constraint faithfulness. Furthermore, CoScript is\ndemonstrated to be quite effective in endowing smaller LMs with constrained\nlanguage planning ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Siyu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiangjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Ziquan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1\">Xuyang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Soham Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jankowski_C/0/1/0/all/0/1\">Charles Robert Jankowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Deqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Need Holistically Thought in Medical Conversational QA. (arXiv:2305.05410v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.05410","description":"<p>The medical conversational question answering (CQA) system aims at providing\na series of professional medical services to improve the efficiency of medical\ncare. Despite the success of large language models (LLMs) in complex reasoning\ntasks in various fields, such as mathematics, logic, and commonsense QA, they\nstill need to improve with the increased complexity and specialization of the\nmedical field. This is because medical CQA tasks require not only strong\nmedical reasoning, but also the ability to think broadly and deeply. In this\npaper, to address these challenges in medical CQA tasks that need to be\nconsidered and understood in many aspects, we propose the Holistically Thought\n(HoT) method, which is designed to guide the LLMs to perform the diffused and\nfocused thinking for generating high-quality medical responses. The proposed\nHoT method has been evaluated through automated and manual assessments in three\ndifferent medical CQA datasets containing the English and Chinese languages.\nThe extensive experimental results show that our method can produce more\ncorrectness, professional, and considerate answers than several\nstate-of-the-art (SOTA) methods, manifesting its effectiveness. Our code in\nhttps://github.com/WENGSYX/HoT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Minjun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shizhu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation Learning for Person or Entity-centric Knowledge Graphs: An Application in Healthcare. (arXiv:2305.05640v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2305.05640","description":"<p>Knowledge graphs (KGs) are a popular way to organise information based on\nontologies or schemas and have been used across a variety of scenarios from\nsearch to recommendation. Despite advances in KGs, representing knowledge\nremains a non-trivial task across industries and it is especially challenging\nin the biomedical and healthcare domains due to complex interdependent\nrelations between entities, heterogeneity, lack of standardization, and\nsparseness of data. KGs are used to discover diagnoses or prioritize genes\nrelevant to disease, but they often rely on schemas that are not centred around\na node or entity of interest, such as a person. Entity-centric KGs are\nrelatively unexplored but hold promise in representing important facets\nconnected to a central node and unlocking downstream tasks beyond graph\ntraversal and reasoning, such as generating graph embeddings and training graph\nneural networks for a wide range of predictive tasks. This paper presents an\nend-to-end representation learning framework to extract entity-centric KGs from\nstructured and unstructured data. We introduce a star-shaped ontology to\nrepresent the multiple facets of a person and use it to guide KG creation.\nCompact representations of the graphs are created leveraging graph neural\nnetworks and experiments are conducted using different levels of heterogeneity\nor explicitness. A readmission prediction task is used to evaluate the results\nof the proposed framework, showing a stable system, robust to missing data,\nthat outperforms a range of baseline machine learning classifiers. We highlight\nthat this approach has several potential applications across domains and is\nopen-sourced. Lastly, we discuss lessons learned, challenges, and next steps\nfor the adoption of the framework in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Theodoropoulos_C/0/1/0/all/0/1\">Christos Theodoropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulligan_N/0/1/0/all/0/1\">Natasha Mulligan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stappenbeck_T/0/1/0/all/0/1\">Thaddeus Stappenbeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bettencourt_Silva_J/0/1/0/all/0/1\">Joao Bettencourt-Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-05-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}