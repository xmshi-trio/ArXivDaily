{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-06-14T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Medical Data Augmentation via ChatGPT: A Case Study on Medication Identification and Medication Event Classification. (arXiv:2306.07297v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07297","description":"<p>The identification of key factors such as medications, diseases, and\nrelationships within electronic health records and clinical notes has a wide\nrange of applications in the clinical field. In the N2C2 2022 competitions,\nvarious tasks were presented to promote the identification of key factors in\nelectronic health records (EHRs) using the Contextualized Medication Event\nDataset (CMED). Pretrained large language models (LLMs) demonstrated\nexceptional performance in these tasks. This study aims to explore the\nutilization of LLMs, specifically ChatGPT, for data augmentation to overcome\nthe limited availability of annotated data for identifying the key factors in\nEHRs. Additionally, different pre-trained BERT models, initially trained on\nextensive datasets like Wikipedia and MIMIC, were employed to develop models\nfor identifying these key variables in EHRs through fine-tuning on augmented\ndatasets. The experimental results of two EHR analysis tasks, namely medication\nidentification and medication event classification, indicate that data\naugmentation based on ChatGPT proves beneficial in improving performance for\nboth medication identification and medication event classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarker_S/0/1/0/all/0/1\">Shouvon Sarker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1\">Lijun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xishuang Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Impact of Experiencing Misrecognition by Teachable Agents on Learning and Rapport. (arXiv:2306.07302v1 [cs.HC])","link":"http://arxiv.org/abs/2306.07302","description":"<p>While speech-enabled teachable agents have some advantages over typing-based\nones, they are vulnerable to errors stemming from misrecognition by automatic\nspeech recognition (ASR). These errors may propagate, resulting in unexpected\nchanges in the flow of conversation. We analyzed how such changes are linked\nwith learning gains and learners' rapport with the agents. Our results show\nthey are not related to learning gains or rapport, regardless of the types of\nresponses the agents should have returned given the correct input from learners\nwithout ASR errors. We also discuss the implications for optimal error-recovery\npolicies for teachable agents that can be drawn from these findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1\">Yuya Asano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1\">Diane Litman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mingzhi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobczowski_N/0/1/0/all/0/1\">Nikki Lobczowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nokes_Malach_T/0/1/0/all/0/1\">Timothy Nokes-Malach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1\">Adriana Kovashka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_E/0/1/0/all/0/1\">Erin Walker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks. (arXiv:2306.07303v1 [cs.LG])","link":"http://arxiv.org/abs/2306.07303","description":"<p>Transformer is a deep neural network that employs a self-attention mechanism\nto comprehend the contextual relationships within sequential data. Unlike\nconventional neural networks or updated versions of Recurrent Neural Networks\n(RNNs) such as Long Short-Term Memory (LSTM), transformer models excel in\nhandling long dependencies between input sequence elements and enable parallel\nprocessing. As a result, transformer-based models have attracted substantial\ninterest among researchers in the field of artificial intelligence. This can be\nattributed to their immense potential and remarkable achievements, not only in\nNatural Language Processing (NLP) tasks but also in a wide range of domains,\nincluding computer vision, audio and speech processing, healthcare, and the\nInternet of Things (IoT). Although several survey papers have been published\nhighlighting the transformer's contributions in specific fields, architectural\ndifferences, or performance evaluations, there is still a significant absence\nof a comprehensive survey paper encompassing its major applications across\nvarious domains. Therefore, we undertook the task of filling this gap by\nconducting an extensive survey of proposed transformer models from 2017 to\n2022. Our survey encompasses the identification of the top five application\ndomains for transformer-based models, namely: NLP, Computer Vision,\nMulti-Modality, Audio and Speech Processing, and Signal Processing. We analyze\nthe impact of highly influential transformer-based models in these domains and\nsubsequently classify them based on their respective tasks using a proposed\ntaxonomy. Our aim is to shed light on the existing potential and future\npossibilities of transformers for enthusiastic researchers, thus contributing\nto the broader understanding of this groundbreaking technology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1\">Saidul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elmekki_H/0/1/0/all/0/1\">Hanae Elmekki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsebai_A/0/1/0/all/0/1\">Ahmed Elsebai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bentahar_J/0/1/0/all/0/1\">Jamal Bentahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drawel_N/0/1/0/all/0/1\">Najat Drawel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rjoub_G/0/1/0/all/0/1\">Gaith Rjoub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedrycz_W/0/1/0/all/0/1\">Witold Pedrycz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EriBERTa: A Bilingual Pre-Trained Language Model for Clinical Natural Language Processing. (arXiv:2306.07373v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07373","description":"<p>The utilization of clinical reports for various secondary purposes, including\nhealth research and treatment monitoring, is crucial for enhancing patient\ncare. Natural Language Processing (NLP) tools have emerged as valuable assets\nfor extracting and processing relevant information from these reports. However,\nthe availability of specialized language models for the clinical domain in\nSpanish has been limited.\n</p>\n<p>In this paper, we introduce EriBERTa, a bilingual domain-specific language\nmodel pre-trained on extensive medical and clinical corpora. We demonstrate\nthat EriBERTa outperforms previous Spanish language models in the clinical\ndomain, showcasing its superior capabilities in understanding medical texts and\nextracting meaningful information. Moreover, EriBERTa exhibits promising\ntransfer learning abilities, allowing for knowledge transfer from one language\nto another. This aspect is particularly beneficial given the scarcity of\nSpanish clinical data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iglesia_I/0/1/0/all/0/1\">Iker de la Iglesia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atutxa_A/0/1/0/all/0/1\">Aitziber Atutxa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gojenola_K/0/1/0/all/0/1\">Koldo Gojenola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrena_A/0/1/0/all/0/1\">Ander Barrena</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lost in Translation: Large Language Models in Non-English Content Analysis. (arXiv:2306.07377v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07377","description":"<p>In recent years, large language models (e.g., Open AI's GPT-4, Meta's LLaMa,\nGoogle's PaLM) have become the dominant approach for building AI systems to\nanalyze and generate language online. However, the automated systems that\nincreasingly mediate our interactions online -- such as chatbots, content\nmoderation systems, and search engines -- are primarily designed for and work\nfar more effectively in English than in the world's other 7,000 languages.\nRecently, researchers and technology companies have attempted to extend the\ncapabilities of large language models into languages other than English by\nbuilding what are called multilingual language models.\n</p>\n<p>In this paper, we explain how these multilingual language models work and\nexplore their capabilities and limits. Part I provides a simple technical\nexplanation of how large language models work, why there is a gap in available\ndata between English and other languages, and how multilingual language models\nattempt to bridge that gap. Part II accounts for the challenges of doing\ncontent analysis with large language models in general and multilingual\nlanguage models in particular. Part III offers recommendations for companies,\nresearchers, and policymakers to keep in mind when considering researching,\ndeveloping and deploying large and multilingual language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nicholas_G/0/1/0/all/0/1\">Gabriel Nicholas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_A/0/1/0/all/0/1\">Aliya Bhatia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Quantifier Comprehension in Large Language Models. (arXiv:2306.07384v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07384","description":"<p>With their increasing size, Large language models (LLMs) are becoming\nincreasingly good at language understanding tasks. But even with high\nperformance on specific downstream task, LLMs fail at simple linguistic tests\nfor negation or quantifier understanding. Previous work on testing capability\nof LLMs on understanding quantifiers suggest that as the size of the models\nincrease, they get better at understanding most-type quantifiers but get\nincreasingly worse at understanding few-type quantifiers, thus presenting a\ncase of an inverse-scaling law. In this paper, we question the claims of\ninverse scaling of few-type quantifier understanding in LLMs and show that it\nis a result of inappropriate testing methodology. We also present alternate\nmethods to measure quantifier comprehension in LLMs and show that as the size\nof the models increase, these behaviours are different from what is shown in\nprevious research. LLMs are consistently able to understand the difference\nbetween the meaning of few-type and most-type quantifiers, but when a\nquantifier is added to phrase, LLMs do not always take into account the meaning\nof the quantifier. We in fact see an inverse scaling law for most-type\nquantifiers, which is contrary to human psycho-linguistic experiments and\nprevious work, where the model's understanding of most-type quantifier gets\nworse as the model size increases. We do this evaluation on models ranging from\n125M-175B parameters, which suggests that LLMs do not do as well as expected\nwith quantifiers and statistical co-occurrence of words still takes precedence\nover word meaning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akshat Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implementing BERT and fine-tuned RobertA to detect AI generated news by ChatGPT. (arXiv:2306.07401v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07401","description":"<p>The abundance of information on social media has increased the necessity of\naccurate real-time rumour detection. Manual techniques of identifying and\nverifying fake news generated by AI tools are impracticable and time-consuming\ngiven the enormous volume of information generated every day. This has sparked\nan increase in interest in creating automated systems to find fake news on the\nInternet. The studies in this research demonstrate that the BERT and RobertA\nmodels with fine-tuning had the best success in detecting AI generated news.\nWith a score of 98%, tweaked RobertA in particular showed excellent precision.\nIn conclusion, this study has shown that neural networks can be used to\nidentify bogus news AI generation news created by ChatGPT. The RobertA and BERT\nmodels' excellent performance indicates that these models can play a critical\nrole in the fight against misinformation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zecong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jiaxi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Chen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chenhao Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The economic trade-offs of large language models: A case study. (arXiv:2306.07402v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07402","description":"<p>Contacting customer service via chat is a common practice. Because employing\ncustomer service agents is expensive, many companies are turning to NLP that\nassists human agents by auto-generating responses that can be used directly or\nwith modifications. Large Language Models (LLMs) are a natural fit for this use\ncase; however, their efficacy must be balanced with the cost of training and\nserving them. This paper assesses the practical cost and impact of LLMs for the\nenterprise as a function of the usefulness of the responses that they generate.\nWe present a cost framework for evaluating an NLP model's utility for this use\ncase and apply it to a single brand as a case study in the context of an\nexisting agent assistance product. We compare three strategies for specializing\nan LLM - prompt engineering, fine-tuning, and knowledge distillation - using\nfeedback from the brand's customer service agents. We find that the usability\nof a model's responses can make up for a large difference in inference cost for\nour case study brand, and we extrapolate our findings to the broader enterprise\nspace.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Howell_K/0/1/0/all/0/1\">Kristen Howell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christian_G/0/1/0/all/0/1\">Gwen Christian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fomitchov_P/0/1/0/all/0/1\">Pavel Fomitchov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kehat_G/0/1/0/all/0/1\">Gitit Kehat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marzulla_J/0/1/0/all/0/1\">Julianne Marzulla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rolston_L/0/1/0/all/0/1\">Leanne Rolston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tredup_J/0/1/0/all/0/1\">Jadin Tredup</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmerman_I/0/1/0/all/0/1\">Ilana Zimmerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Selfridge_E/0/1/0/all/0/1\">Ethan Selfridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradley_J/0/1/0/all/0/1\">Joseph Bradley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Topic Extraction in Recommender Systems with Entropy Regularization. (arXiv:2306.07403v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07403","description":"<p>In recent years, many recommender systems have utilized textual data for\ntopic extraction to enhance interpretability. However, our findings reveal a\nnoticeable deficiency in the coherence of keywords within topics, resulting in\nlow explainability of the model. This paper introduces a novel approach called\nentropy regularization to address the issue, leading to more interpretable\ntopics extracted from recommender systems, while ensuring that the performance\nof the primary task stays competitively strong. The effectiveness of the\nstrategy is validated through experiments on a variation of the probabilistic\nmatrix factorization model that utilizes textual data to extract item\nembeddings. The experiment results show a significant improvement in topic\ncoherence, which is quantified by cosine similarity on word embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xuefei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dairui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Ruihai Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Textual Augmentation Techniques Applied to Low Resource Machine Translation: Case of Swahili. (arXiv:2306.07414v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07414","description":"<p>In this work we investigate the impact of applying textual data augmentation\ntasks to low resource machine translation. There has been recent interest in\ninvestigating approaches for training systems for languages with limited\nresources and one popular approach is the use of data augmentation techniques.\nData augmentation aims to increase the quantity of data that is available to\ntrain the system. In machine translation, majority of the language pairs around\nthe world are considered low resource because they have little parallel data\navailable and the quality of neural machine translation (NMT) systems depend a\nlot on the availability of sizable parallel corpora. We study and apply three\nsimple data augmentation techniques popularly used in text classification\ntasks; synonym replacement, random insertion and contextual data augmentation\nand compare their performance with baseline neural machine translation for\nEnglish-Swahili (En-Sw) datasets. We also present results in BLEU, ChrF and\nMeteor scores. Overall, the contextual data augmentation technique shows some\nimprovements both in the $EN \\rightarrow SW$ and $SW \\rightarrow EN$\ndirections. We see that there is potential to use these methods in neural\nmachine translation when more extensive experiments are done with diverse\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gitau_C/0/1/0/all/0/1\">Catherine Gitau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1\">VUkosi Marivate</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gender-Inclusive Grammatical Error Correction through Augmentation. (arXiv:2306.07415v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07415","description":"<p>In this paper we show that GEC systems display gender bias related to the use\nof masculine and feminine terms and the gender-neutral singular \"they\". We\ndevelop parallel datasets of texts with masculine and feminine terms and\nsingular \"they\" and use them to quantify gender bias in three competitive GEC\nsystems. We contribute a novel data augmentation technique for singular \"they\"\nleveraging linguistic insights about its distribution relative to plural\n\"they\". We demonstrate that both this data augmentation technique and a\nrefinement of a similar augmentation technique for masculine and feminine terms\ncan generate training data that reduces bias in GEC systems, especially with\nrespect to singular \"they\" while maintaining the same level of quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lund_G/0/1/0/all/0/1\">Gunnar Lund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omelianchuk_K/0/1/0/all/0/1\">Kostiantyn Omelianchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samokhin_I/0/1/0/all/0/1\">Igor Samokhin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Izindaba-Tindzaba: Machine learning news categorisation for Long and Short Text for isiZulu and Siswati. (arXiv:2306.07426v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07426","description":"<p>Local/Native South African languages are classified as low-resource\nlanguages. As such, it is essential to build the resources for these languages\nso that they can benefit from advances in the field of natural language\nprocessing. In this work, the focus was to create annotated news datasets for\nthe isiZulu and Siswati native languages based on news topic classification\ntasks and present the findings from these baseline classification models. Due\nto the shortage of data for these native South African languages, the datasets\nthat were created were augmented and oversampled to increase data size and\novercome class classification imbalance. In total, four different\nclassification models were used namely Logistic regression, Naive bayes,\nXGBoost and LSTM. These models were trained on three different word embeddings\nnamely Bag-Of-Words, TFIDF and Word2vec. The results of this study showed that\nXGBoost, Logistic Regression and LSTM, trained from Word2vec performed better\nthan the other combinations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madodonga_A/0/1/0/all/0/1\">Andani Madodonga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1\">Vukosi Marivate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adendorff_M/0/1/0/all/0/1\">Matthew Adendorff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resources for Brewing BEIR: Reproducible Reference Models and an Official Leaderboard. (arXiv:2306.07471v1 [cs.IR])","link":"http://arxiv.org/abs/2306.07471","description":"<p>BEIR is a benchmark dataset for zero-shot evaluation of information retrieval\nmodels across 18 different domain/task combinations. In recent years, we have\nwitnessed the growing popularity of a representation learning approach to\nbuilding retrieval models, typically using pretrained transformers in a\nsupervised setting. This naturally begs the question: How effective are these\nmodels when presented with queries and documents that differ from the training\ndata? Examples include searching in different domains (e.g., medical or legal\ntext) and with different types of queries (e.g., keywords vs. well-formed\nquestions). While BEIR was designed to answer these questions, our work\naddresses two shortcomings that prevent the benchmark from achieving its full\npotential: First, the sophistication of modern neural methods and the\ncomplexity of current software infrastructure create barriers to entry for\nnewcomers. To this end, we provide reproducible reference implementations that\ncover the two main classes of approaches: learned dense and sparse models.\nSecond, there does not exist a single authoritative nexus for reporting the\neffectiveness of different models on BEIR, which has led to difficulty in\ncomparing different methods. To remedy this, we present an official\nself-service BEIR leaderboard that provides fair and consistent comparisons of\nretrieval models. By addressing both shortcomings, our work facilitates future\nexplorations in a range of interesting research questions that BEIR enables.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamalloo_E/0/1/0/all/0/1\">Ehsan Kamalloo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nandan Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassance_C/0/1/0/all/0/1\">Carlos Lassance</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xueguang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jheng-Hong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Prompted Estimator: A Novel Approach to Explainable Machine Translation Assessment. (arXiv:2306.07486v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07486","description":"<p>Cross-lingual Machine Translation (MT) quality estimation plays a crucial\nrole in evaluating translation performance. GEMBA, the first MT quality\nassessment metric based on Large Language Models (LLMs), employs one-step\nprompting to achieve state-of-the-art (SOTA) in system-level MT quality\nestimation; however, it lacks segment-level analysis. In contrast,\nChain-of-Thought (CoT) prompting outperforms one-step prompting by offering\nimproved reasoning and explainability. In this paper, we introduce\nKnowledge-Prompted Estimator (KPE), a CoT prompting method that combines three\none-step prompting techniques, including perplexity, token-level similarity,\nand sentence-level similarity. This method attains enhanced performance for\nsegment-level estimation compared with previous deep learning models and\none-step prompting approaches. Furthermore, supplementary experiments on\nword-level visualized alignment demonstrate that our KPE method significantly\nimproves token alignment compared with earlier models and provides better\ninterpretability for MT quality estimation. Code will be released upon\npublication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Shimin Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Minghan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Daimeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yanfei Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Opinion-based Question Answering Systems Through Label Error Detection and Overwrite. (arXiv:2306.07499v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07499","description":"<p>Label error is a ubiquitous problem in annotated data. Large amounts of label\nerror substantially degrades the quality of deep learning models. Existing\nmethods to tackle the label error problem largely focus on the classification\ntask, and either rely on task specific architecture or require non-trivial\nadditional computations, which is undesirable or even unattainable for industry\nusage. In this paper, we propose LEDO: a model-agnostic and computationally\nefficient framework for Label Error Detection and Overwrite. LEDO is based on\nMonte Carlo Dropout combined with uncertainty metrics, and can be easily\ngeneralized to multiple tasks and data sets. Applying LEDO to an industry\nopinion-based question answering system demonstrates it is effective at\nimproving accuracy in all the core models. Specifically, LEDO brings 1.1% MRR\ngain for the retrieval model, 1.5% PR AUC improvement for the machine reading\ncomprehension model, and 0.9% rise in the Average Precision for the ranker, on\ntop of the strong baselines with a large-scale social media dataset.\nImportantly, LEDO is computationally efficient compared to methods that require\nloss function change, and cost-effective as the resulting data can be used in\nthe same continuous training pipeline for production. Further analysis shows\nthat these gains come from an improved decision boundary after cleaning the\nlabel errors existed in the training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Ahmed K. Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Shashank Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peshterliev_S/0/1/0/all/0/1\">Stanislav Peshterliev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_D/0/1/0/all/0/1\">Debojeet Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1\">Hanwen Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhalla_N/0/1/0/all/0/1\">Nikita Bhalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aneja_G/0/1/0/all/0/1\">Gagan Aneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanty_P/0/1/0/all/0/1\">Pranab Mohanty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adding guardrails to advanced chatbots. (arXiv:2306.07500v1 [cs.CY])","link":"http://arxiv.org/abs/2306.07500","description":"<p>Generative AI models continue to become more powerful. The launch of ChatGPT\nin November 2022 has ushered in a new era of AI. ChatGPT and other similar\nchatbots have a range of capabilities, from answering student homework\nquestions to creating music and art. There are already concerns that humans may\nbe replaced by chatbots for a variety of jobs. Because of the wide spectrum of\ndata chatbots are built on, we know that they will have human errors and human\nbiases built into them. These biases may cause significant harm and/or inequity\ntoward different subpopulations. To understand the strengths and weakness of\nchatbot responses, we present a position paper that explores different use\ncases of ChatGPT to determine the types of questions that are answered fairly\nand the types that still need improvement. We find that ChatGPT is a fair\nsearch engine for the tasks we tested; however, it has biases on both text\ngeneration and code generation. We find that ChatGPT is very sensitive to\nchanges in the prompt, where small changes lead to different levels of\nfairness. This suggests that we need to immediately implement \"corrections\" or\nmitigation strategies in order to improve fairness of these systems. We suggest\ndifferent strategies to improve chatbots and also advocate for an impartial\nreview panel that has access to the model parameters to measure the levels of\ndifferent types of biases and then recommends safeguards that move toward\nresponses that are less discriminatory and more accurate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanchen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_L/0/1/0/all/0/1\">Lisa Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noisy Positive-Unlabeled Learning with Self-Training for Speculative Knowledge Graph Reasoning. (arXiv:2306.07512v1 [cs.LG])","link":"http://arxiv.org/abs/2306.07512","description":"<p>This paper studies speculative reasoning task on real-world knowledge graphs\n(KG) that contain both \\textit{false negative issue} (i.e., potential true\nfacts being excluded) and \\textit{false positive issue} (i.e., unreliable or\noutdated facts being included). State-of-the-art methods fall short in the\nspeculative reasoning ability, as they assume the correctness of a fact is\nsolely determined by its presence in KG, making them vulnerable to false\nnegative/positive issues. The new reasoning task is formulated as a noisy\nPositive-Unlabeled learning problem. We propose a variational framework, namely\nnPUGraph, that jointly estimates the correctness of both collected and\nuncollected facts (which we call \\textit{label posterior}) and updates model\nparameters during training. The label posterior estimation facilitates\nspeculative reasoning from two perspectives. First, it improves the robustness\nof a label posterior-aware graph encoder against false positive links. Second,\nit identifies missing facts to provide high-quality grounds of reasoning. They\nare unified in a simple yet effective self-training procedure. Empirically,\nextensive experiments on three benchmark KG and one Twitter dataset with\nvarious degrees of false negative/positive cases demonstrate the effectiveness\nof nPUGraph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baoyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yichen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Dachun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yuchen Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1\">Hanghang Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelzaher_T/0/1/0/all/0/1\">Tarek F. Abdelzaher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TART: A plug-and-play Transformer module for task-agnostic reasoning. (arXiv:2306.07536v1 [cs.LG])","link":"http://arxiv.org/abs/2306.07536","description":"<p>Large language models (LLMs) exhibit in-context learning abilities which\nenable the same model to perform several tasks without any task-specific\ntraining. In contrast, traditional adaptation approaches, such as fine-tuning,\nmodify the underlying models for each specific task. In-context learning,\nhowever, consistently underperforms task-specific tuning approaches even when\npresented with the same examples. While most existing approaches (e.g., prompt\nengineering) focus on the LLM's learned representations to patch this\nperformance gap, our analysis actually reveal that LLM representations contain\nsufficient information to make good predictions. As such, we focus on the LLM's\nreasoning abilities and demonstrate that this performance gap exists due to\ntheir inability to perform simple probabilistic reasoning tasks. This raises an\nintriguing question: Are LLMs actually capable of learning how to reason in a\ntask-agnostic manner? We answer this in the affirmative and propose TART which\ngenerically improves an LLM's reasoning abilities using a synthetically trained\nTransformer-based reasoning module. TART trains this reasoning module in a\ntask-agnostic manner using only synthetic logistic regression tasks and\ncomposes it with an arbitrary real-world pre-trained model without any\nadditional training. With a single inference module, TART improves performance\nacross different model families (GPT-Neo, Pythia, BLOOM), model sizes (100M -\n6B), tasks (14 NLP binary classification tasks), and even across different\nmodalities (audio and vision). Additionally, on the RAFT Benchmark, TART\nimproves GPT-Neo (125M)'s performance such that it outperforms BLOOM (176B),\nand is within 4% of GPT-3 (175B). Our code and models are available at\nhttps://github.com/HazyResearch/TART .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_K/0/1/0/all/0/1\">Kush Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_A/0/1/0/all/0/1\">Avanika Narayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1\">Christopher De Sa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HAUSER: Towards Holistic and Automatic Evaluation of Simile Generation. (arXiv:2306.07554v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07554","description":"<p>Similes play an imperative role in creative writing such as story and\ndialogue generation. Proper evaluation metrics are like a beacon guiding the\nresearch of simile generation (SG). However, it remains under-explored as to\nwhat criteria should be considered, how to quantify each criterion into\nmetrics, and whether the metrics are effective for comprehensive, efficient,\nand reliable SG evaluation. To address the issues, we establish HAUSER, a\nholistic and automatic evaluation system for the SG task, which consists of\nfive criteria from three perspectives and automatic metrics for each criterion.\nThrough extensive experiments, we verify that our metrics are significantly\nmore correlated with human ratings from each perspective compared with prior\nautomatic metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qianyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yikai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiaqing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuncheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunwen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Sometimes Generate Purely Negatively-Reinforced Text. (arXiv:2306.07567v1 [cs.LG])","link":"http://arxiv.org/abs/2306.07567","description":"<p>When using adversarial training, it is common practice to train against the\nmost egregious failures. However, this might imply using examples with\nsensitive information (such as leaked passwords or security vulnerabilities) as\ntraining data. One might assume that language models trained with gradient\ndescent never generate text snippets which were only present in examples\nassociated with the lowest possible reward. In this paper, we show that this\nassumption is wrong: in some situations, large language models do learn from\nsuch negatively-reinforced examples. We present a specific training setup that\nenables Pythia-160M to generate passwords with a probability slightly greater\nthan chance, despite only showing it these passwords on examples where the\nmodel is incentivized to not output these passwords. Our code is available at\nhttps://github.com/FabienRoger/Learning-From-Negative-Examples\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roger_F/0/1/0/all/0/1\">Fabien Roger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Question Decomposition Tree for Answering Complex Questions over Knowledge Bases. (arXiv:2306.07597v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07597","description":"<p>Knowledge base question answering (KBQA) has attracted a lot of interest in\nrecent years, especially for complex questions which require multiple facts to\nanswer. Question decomposition is a promising way to answer complex questions.\nExisting decomposition methods split the question into sub-questions according\nto a single compositionality type, which is not sufficient for questions\ninvolving multiple compositionality types. In this paper, we propose Question\nDecomposition Tree (QDT) to represent the structure of complex questions.\nInspired by recent advances in natural language generation (NLG), we present a\ntwo-staged method called Clue-Decipher to generate QDT. It can leverage the\nstrong ability of NLG model and simultaneously preserve the original questions.\nTo verify that QDT can enhance KBQA task, we design a decomposition-based KBQA\nsystem called QDTQA. Extensive experiments show that QDTQA outperforms previous\nstate-of-the-art methods on ComplexWebQuestions dataset. Besides, our\ndecomposition method improves an existing KBQA system by 12% and sets a new\nstate-of-the-art on LC-QuAD 1.0.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Sitao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1\">Yiheng Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yuheng Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yuzhong Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Soft Language Clustering for Multilingual Model Pre-training. (arXiv:2306.07610v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07610","description":"<p>Multilingual pre-trained language models have demonstrated impressive\n(zero-shot) cross-lingual transfer abilities, however, their performance is\nhindered when the target language has distant typology from source languages or\nwhen pre-training data is limited in size. In this paper, we propose XLM-P,\nwhich contextually retrieves prompts as flexible guidance for encoding\ninstances conditionally. Our XLM-P enables (1) lightweight modeling of\nlanguage-invariant and language-specific knowledge across languages, and (2)\neasy integration with other multilingual pre-training methods. On the tasks of\nXTREME including text classification, sequence labeling, question answering,\nand sentence retrieval, both base- and large-size language models pre-trained\nwith our proposed method exhibit consistent performance improvement.\nFurthermore, it provides substantial advantages for low-resource languages in\nunsupervised sentence retrieval and for target languages that differ greatly\nfrom the source language in cross-lingual transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jiali Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yufan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yongjing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_Y/0/1/0/all/0/1\">Yi Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Binghuai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rank-Aware Negative Training for Semi-Supervised Text Classification. (arXiv:2306.07621v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07621","description":"<p>Semi-supervised text classification-based paradigms (SSTC) typically employ\nthe spirit of self-training. The key idea is to train a deep classifier on\nlimited labeled texts and then iteratively predict the unlabeled texts as their\npseudo-labels for further training. However, the performance is largely\naffected by the accuracy of pseudo-labels, which may not be significant in\nreal-world scenarios. This paper presents a Rank-aware Negative Training (RNT)\nframework to address SSTC in learning with noisy label manner. To alleviate the\nnoisy information, we adapt a reasoning with uncertainty-based approach to rank\nthe unlabeled texts based on the evidential support received from the labeled\ntexts. Moreover, we propose the use of negative training to train RNT based on\nthe concept that ``the input instance does not belong to the complementary\nlabel''. A complementary label is randomly selected from all labels except the\nlabel on-target. Intuitively, the probability of a true label serving as a\ncomplementary label is low and thus provides less noisy information during the\ntraining, resulting in better performance on the test data. Finally, we\nevaluate the proposed solution on various text classification benchmark\ndatasets. Our extensive experiments show that it consistently overcomes the\nstate-of-the-art alternatives in most scenarios and achieves competitive\nperformance in the others. The code of RNT is publicly available\nat:https://github.com/amurtadha/RNT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murtadha_A/0/1/0/all/0/1\">Ahmed Murtadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shengfeng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bo_W/0/1/0/all/0/1\">Wen Bo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jianlin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xinxin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenze Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunfeng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models -- and Disappeared in GPT-4. (arXiv:2306.07622v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07622","description":"<p>Large language models (LLMs) are currently at the forefront of intertwining\nAI systems with human communication and everyday life. Therefore, it is of\ngreat importance to evaluate their emerging abilities. In this study, we show\nthat LLMs, most notably GPT-3, exhibit behavior that strikingly resembles\nhuman-like intuition -- and the cognitive errors that come with it. However,\nLLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4,\nlearned to avoid succumbing to these errors and perform in a hyperrational\nmanner. For our experiments, we probe LLMs with the Cognitive Reflection Test\n(CRT) as well as semantic illusions that were originally designed to\ninvestigate intuitive decision-making in humans. Moreover, we probe how sturdy\nthe inclination for intuitive-like decision-making is. Our study demonstrates\nthat investigating LLMs with methods from psychology has the potential to\nreveal otherwise unknown emergent traits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagendorff_T/0/1/0/all/0/1\">Thilo Hagendorff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabi_S/0/1/0/all/0/1\">Sarah Fabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SqueezeLLM: Dense-and-Sparse Quantization. (arXiv:2306.07629v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07629","description":"<p>Generative Large Language Models (LLMs) have demonstrated remarkable results\nfor a wide range of tasks. However, deploying these models for inference has\nbeen a significant challenge due to their unprecedented resource requirements.\nThis has forced existing deployment frameworks to use multi-GPU inference\npipelines, which are often complex and costly, or to use smaller and less\nperformant models. In this work, we demonstrate that the main bottleneck for\ngenerative inference with LLMs is memory bandwidth, rather than compute,\nspecifically for single batch inference. While quantization has emerged as a\npromising solution by representing model weights with reduced precision,\nprevious efforts have often resulted in notable performance degradation. To\naddress this, we introduce SqueezeLLM, a post-training quantization framework\nthat not only enables lossless compression to ultra-low precisions of up to\n3-bit, but also achieves higher quantization performance under the same memory\nconstraint. Our framework incorporates two novel ideas: (i) sensitivity-based\nnon-uniform quantization, which searches for the optimal bit precision\nassignment based on second-order information; and (ii) the Dense-and-Sparse\ndecomposition that stores outliers and sensitive weight values in an efficient\nsparse format. When applied to the LLaMA models, our 3-bit quantization\nsignificantly reduces the perplexity gap from the FP16 baseline by up to 2.1x\nas compared to the state-of-the-art methods with the same memory requirement.\nFurthermore, when deployed on an A6000 GPU, our quantized models achieve up to\n2.3x speedup compared to the baseline. Our code is open-sourced and available\nonline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooper_C/0/1/0/all/0/1\">Coleman Hooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1\">Amir Gholami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiuyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid lemmatization in HuSpaCy. (arXiv:2306.07636v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07636","description":"<p>Lemmatization is still not a trivial task for morphologically rich languages.\nPrevious studies showed that hybrid architectures usually work better for these\nlanguages and can yield great results. This paper presents a hybrid lemmatizer\nutilizing both a neural model, dictionaries and hand-crafted rules. We\nintroduce a hybrid architecture along with empirical results on a widely used\nHungarian dataset. The presented methods are published as three HuSpaCy models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berkecz_P/0/1/0/all/0/1\">P&#xe9;ter Berkecz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orosz_G/0/1/0/all/0/1\">Gy&#xf6;rgy Orosz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szanto_Z/0/1/0/all/0/1\">Zsolt Sz&#xe1;nt&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szabo_G/0/1/0/all/0/1\">Gerg&#x151; Szab&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farkas_R/0/1/0/all/0/1\">Rich&#xe1;rd Farkas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modality Adaption or Regularization? A Case Study on End-to-End Speech Translation. (arXiv:2306.07650v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07650","description":"<p>Pre-training and fine-tuning is a paradigm for alleviating the data scarcity\nproblem in end-to-end speech translation (E2E ST). The commonplace \"modality\ngap\" between speech and text data often leads to inconsistent inputs between\npre-training and fine-tuning. However, we observe that this gap occurs in the\nearly stages of fine-tuning, but does not have a major impact on the final\nperformance. On the other hand, we find that there has another gap, which we\ncall the \"capacity gap\": high resource tasks (such as ASR and MT) always\nrequire a large model to fit, when the model is reused for a low resource task\n(E2E ST), it will get a sub-optimal performance due to the over-fitting. In a\ncase study, we find that the regularization plays a more important role than\nthe well-designed modality adaption method, which achieves 29.0 for en-de and\n40.3 for en-fr on the MuST-C dataset. Code and models are available at\nhttps://github.com/hannlp/TAB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yuchen Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Anisotropy Inherent to Transformers?. (arXiv:2306.07656v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07656","description":"<p>The representation degeneration problem is a phenomenon that is widely\nobserved among self-supervised learning methods based on Transformers. In NLP,\nit takes the form of anisotropy, a singular property of hidden representations\nwhich makes them unexpectedly close to each other in terms of angular distance\n(cosine-similarity). Some recent works tend to show that anisotropy is a\nconsequence of optimizing the cross-entropy loss on long-tailed distributions\nof tokens. We show in this paper that anisotropy can also be observed\nempirically in language models with specific objectives that should not suffer\ndirectly from the same consequences. We also show that the anisotropy problem\nextends to Transformers trained on other modalities. Our observations tend to\ndemonstrate that anisotropy might actually be inherent to Transformers-based\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Godey_N/0/1/0/all/0/1\">Nathan Godey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clergerie_E/0/1/0/all/0/1\">&#xc9;ric de la Clergerie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Beno&#xee;t Sagot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethink the Effectiveness of Text Data Augmentation: An Empirical Analysis. (arXiv:2306.07664v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07664","description":"<p>In recent years, language models (LMs) have made remarkable progress in\nadvancing the field of natural language processing (NLP). However, the impact\nof data augmentation (DA) techniques on the fine-tuning (FT) performance of\nthese LMs has been a topic of ongoing debate. In this study, we evaluate the\neffectiveness of three different FT methods in conjugation with\nback-translation across an array of 7 diverse NLP tasks, including\nclassification and regression types, covering single-sentence and sentence-pair\ntasks. Contrary to prior assumptions that DA does not contribute to the\nenhancement of LMs' FT performance, our findings reveal that continued\npre-training on augmented data can effectively improve the FT performance of\nthe downstream tasks. In the most favourable case, continued pre-training\nimproves the performance of FT by more than 10% in the few-shot learning\nsetting. Our finding highlights the potential of DA as a powerful tool for\nbolstering LMs' performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhengxiang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1\">Aldo Lipani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models. (arXiv:2306.07691v1 [eess.AS])","link":"http://arxiv.org/abs/2306.07691","description":"<p>In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that\nleverages style diffusion and adversarial training with large speech language\nmodels (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its\npredecessor by modeling styles as a latent random variable through diffusion\nmodels to generate the most suitable style for the text without requiring\nreference speech, achieving efficient latent diffusion while benefiting from\nthe diverse speech synthesis offered by diffusion models. Furthermore, we\nemploy large pre-trained SLMs, such as WavLM, as discriminators with our novel\ndifferentiable duration modeling for end-to-end training, resulting in improved\nspeech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker\nLJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by\nnative English speakers. Moreover, when trained on the LibriTTS dataset, our\nmodel outperforms previous publicly available models for zero-shot speaker\nadaptation. This work achieves the first human-level TTS on both single and\nmultispeaker datasets, showcasing the potential of style diffusion and\nadversarial training with large SLMs. The audio demos and source code are\navailable at https://styletts2.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yinghao Aaron Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1\">Cong Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raghavan_V/0/1/0/all/0/1\">Vinay S. Raghavan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mischler_G/0/1/0/all/0/1\">Gavin Mischler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mesgarani_N/0/1/0/all/0/1\">Nima Mesgarani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NAVER LABS Europe's Multilingual Speech Translation Systems for the IWSLT 2023 Low-Resource Track. (arXiv:2306.07763v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07763","description":"<p>This paper presents NAVER LABS Europe's systems for Tamasheq-French and\nQuechua-Spanish speech translation in the IWSLT 2023 Low-Resource track. Our\nwork attempts to maximize translation quality in low-resource settings using\nmultilingual parameter-efficient solutions that leverage strong pre-trained\nmodels. Our primary submission for Tamasheq outperforms the previous state of\nthe art by 7.5 BLEU points on the IWSLT 2022 test set, and achieves 23.6 BLEU\non this year's test set, outperforming the second best participant by 7.7\npoints. For Quechua, we also rank first and achieve 17.7 BLEU, despite having\nonly two hours of translation data. Finally, we show that our proposed\nmultilingual architecture is also competitive for high-resource languages,\noutperforming the best unconstrained submission to the IWSLT 2021 Multilingual\ntrack, despite using much less training data and compute.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gow_Smith_E/0/1/0/all/0/1\">Edward Gow-Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berard_A/0/1/0/all/0/1\">Alexandre Berard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boito_M/0/1/0/all/0/1\">Marcely Zanon Boito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calapodescu_I/0/1/0/all/0/1\">Ioan Calapodescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tokenization with Factorized Subword Encoding. (arXiv:2306.07764v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07764","description":"<p>In recent years, language models have become increasingly larger and more\ncomplex. However, the input representations for these models continue to rely\non simple and greedy subword tokenization methods. In this paper, we propose a\nnovel tokenization method that factorizes subwords onto discrete triplets using\na VQ-VAE model. The effectiveness of the proposed tokenization method, referred\nto as the Factorizer, is evaluated on language modeling and morpho-syntactic\ntasks for 7 diverse languages. Results indicate that this method is more\nappropriate and robust for morphological tasks than the commonly used byte-pair\nencoding (BPE) tokenization algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Samuel_D/0/1/0/all/0/1\">David Samuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovrelid_L/0/1/0/all/0/1\">Lilja &#xd8;vrelid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Cloud-based Machine Learning Pipeline for the Efficient Extraction of Insights from Customer Reviews. (arXiv:2306.07786v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07786","description":"<p>The efficiency of natural language processing has improved dramatically with\nthe advent of machine learning models, particularly neural network-based\nsolutions. However, some tasks are still challenging, especially when\nconsidering specific domains. In this paper, we present a cloud-based system\nthat can extract insights from customer reviews using machine learning methods\nintegrated into a pipeline. For topic modeling, our composite model uses\ntransformer-based neural networks designed for natural language processing,\nvector embedding-based keyword extraction, and clustering. The elements of our\nmodel have been integrated and further developed to meet better the\nrequirements of efficient information extraction, topic modeling of the\nextracted information, and user needs. Furthermore, our system can achieve\nbetter results than this task's existing topic modeling and keyword extraction\nsolutions. Our approach is validated and compared with other state-of-the-art\nmethods using publicly available datasets for benchmarking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lakatos_R/0/1/0/all/0/1\">Robert Lakatos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogacsovics_G/0/1/0/all/0/1\">Gergo Bogacsovics</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harangi_B/0/1/0/all/0/1\">Balazs Harangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakatos_I/0/1/0/all/0/1\">Istvan Lakatos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiba_A/0/1/0/all/0/1\">Attila Tiba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toth_J/0/1/0/all/0/1\">Janos Toth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szabo_M/0/1/0/all/0/1\">Marianna Szabo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajdu_A/0/1/0/all/0/1\">Andras Hajdu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NoCoLA: The Norwegian Corpus of Linguistic Acceptability. (arXiv:2306.07790v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07790","description":"<p>While there has been a surge of large language models for Norwegian in recent\nyears, we lack any tool to evaluate their understanding of grammaticality. We\npresent two new Norwegian datasets for this task. NoCoLA_class is a supervised\nbinary classification task where the goal is to discriminate between acceptable\nand non-acceptable sentences. On the other hand, NoCoLA_zero is a purely\ndiagnostic task for evaluating the grammatical judgement of a language model in\na completely zero-shot manner, i.e. without any further training. In this\npaper, we describe both datasets in detail, show how to use them for different\nflavors of language models, and conduct a comparative study of the existing\nNorwegian language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jentoft_M/0/1/0/all/0/1\">Matias Jentoft</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samuel_D/0/1/0/all/0/1\">David Samuel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monolingual and Cross-Lingual Knowledge Transfer for Topic Classification. (arXiv:2306.07797v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07797","description":"<p>This article investigates the knowledge transfer from the RuQTopics dataset.\nThis Russian topical dataset combines a large sample number (361,560\nsingle-label, 170,930 multi-label) with extensive class coverage (76 classes).\nWe have prepared this dataset from the \"Yandex Que\" raw data. By evaluating the\nRuQTopics - trained models on the six matching classes of the Russian MASSIVE\nsubset, we have proved that the RuQTopics dataset is suitable for real-world\nconversational tasks, as the Russian-only models trained on this dataset\nconsistently yield an accuracy around 85\\% on this subset. We also have figured\nout that for the multilingual BERT, trained on the RuQTopics and evaluated on\nthe same six classes of MASSIVE (for all MASSIVE languages), the language-wise\naccuracy closely correlates (Spearman correlation 0.773 with p-value 2.997e-11)\nwith the approximate size of the pretraining BERT's data for the corresponding\nlanguage. At the same time, the correlation of the language-wise accuracy with\nthe linguistical distance from Russian is not statistically significant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karpov_D/0/1/0/all/0/1\">Dmitry Karpov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burtsev_M/0/1/0/all/0/1\">Mikhail Burtsev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer. (arXiv:2306.07799v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07799","description":"<p>Large-scale language models, like ChatGPT, have garnered significant media\nattention and stunned the public with their remarkable capacity for generating\ncoherent text from short natural language prompts. In this paper, we aim to\nconduct a systematic inspection of ChatGPT's performance in two controllable\ngeneration tasks, with respect to ChatGPT's ability to adapt its output to\ndifferent target audiences (expert vs. layman) and writing styles (formal vs.\ninformal). Additionally, we evaluate the faithfulness of the generated text,\nand compare the model's performance with human-authored texts. Our findings\nindicate that the stylistic variations produced by humans are considerably\nlarger than those demonstrated by ChatGPT, and the generated texts diverge from\nhuman samples in several characteristics, such as the distribution of word\ntypes. Moreover, we observe that ChatGPT sometimes incorporates factual errors\nor hallucinations when adapting the text to suit a specific style.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pu_D/0/1/0/all/0/1\">Dongqi Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demberg_V/0/1/0/all/0/1\">Vera Demberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Capsule Networks for Romanian Satire Detection and Sentiment Analysis. (arXiv:2306.07845v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07845","description":"<p>Satire detection and sentiment analysis are intensively explored natural\nlanguage processing (NLP) tasks that study the identification of the satirical\ntone from texts and extracting sentiments in relationship with their targets.\nIn languages with fewer research resources, an alternative is to produce\nartificial examples based on character-level adversarial processes to overcome\ndataset size limitations. Such samples are proven to act as a regularization\nmethod, thus improving the robustness of models. In this work, we improve the\nwell-known NLP models (i.e., Convolutional Neural Networks, Long Short-Term\nMemory (LSTM), Bidirectional LSTM, Gated Recurrent Units (GRUs), and\nBidirectional GRUs) with adversarial training and capsule networks. The\nfine-tuned models are used for satire detection and sentiment analysis tasks in\nthe Romanian language. The proposed framework outperforms the existing methods\nfor the two tasks, achieving up to 99.08% accuracy, thus confirming the\nimprovements added by the capsule layers and the adversarial training in NLP\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Echim_S/0/1/0/all/0/1\">Sebastian-Vasile Echim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smadu_R/0/1/0/all/0/1\">R&#x103;zvan-Alexandru Sm&#x103;du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avram_A/0/1/0/all/0/1\">Andrei-Marius Avram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cercel_D/0/1/0/all/0/1\">Dumitru-Clementin Cercel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pop_F/0/1/0/all/0/1\">Florin Pop</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition. (arXiv:2306.07848v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07848","description":"<p>Contrastive Language-Audio Pretraining (CLAP) has recently exhibited\nimpressive success in diverse fields. In this paper, we propose GEmo-CLAP, a\nkind of efficient gender-attribute-enhanced CLAP model for speech emotion\nrecognition (SER). Specifically, we first build an effective emotion CLAP model\ntermed Emo-CLAP for SER, utilizing various self-supervised learning based\npre-trained models. Then, considering the importance of the gender attribute in\nspeech emotion modeling, two GEmo-CLAP approaches are further proposed to\nintegrate the emotion and gender information of speech signals, forming more\nreasonable objectives. Extensive experiments conducted on the IEMOCAP corpus\ndemonstrate that our proposed two GEmo-CLAP approaches consistently outperform\nthe baseline Emo-CLAP with different pre-trained models, while also achieving\nsuperior recognition performance compared with other state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yanni Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuguang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jixun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_W/0/1/0/all/0/1\">Wen Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Heng Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReadProbe: A Demo of Retrieval-Enhanced Large Language Models to Support Lateral Reading. (arXiv:2306.07875v1 [cs.IR])","link":"http://arxiv.org/abs/2306.07875","description":"<p>With the rapid growth and spread of online misinformation, people need tools\nto help them evaluate the credibility and accuracy of online information.\nLateral reading, a strategy that involves cross-referencing information with\nmultiple sources, may be an effective approach to achieving this goal. In this\npaper, we present ReadProbe, a tool to support lateral reading, powered by\ngenerative large language models from OpenAI and the Bing search engine. Our\ntool is able to generate useful questions for lateral reading, scour the web\nfor relevant documents, and generate well-attributed answers to help people\nbetter evaluate online information. We made a web-based application to\ndemonstrate how ReadProbe can help reduce the risk of being misled by false\ninformation. The code is available at\nhttps://github.com/DakeZhang1998/ReadProbe. An earlier version of our tool won\nthe first prize in a national AI misinformation hackathon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dake Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pradeep_R/0/1/0/all/0/1\">Ronak Pradeep</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks. (arXiv:2306.07899v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07899","description":"<p>Large language models (LLMs) are remarkable data annotators. They can be used\nto generate high-fidelity supervised training data, as well as survey and\nexperimental data. With the widespread adoption of LLMs, human gold--standard\nannotations are key to understanding the capabilities of LLMs and the validity\nof their results. However, crowdsourcing, an important, inexpensive way to\nobtain human annotations, may itself be impacted by LLMs, as crowd workers have\nfinancial incentives to use LLMs to increase their productivity and income. To\ninvestigate this concern, we conducted a case study on the prevalence of LLM\nusage by crowd workers. We reran an abstract summarization task from the\nliterature on Amazon Mechanical Turk and, through a combination of keystroke\ndetection and synthetic text classification, estimate that 33-46% of crowd\nworkers used LLMs when completing the task. Although generalization to other,\nless LLM-friendly tasks is unclear, our results call for platforms,\nresearchers, and crowd workers to find new ways to ensure that human data\nremain human, perhaps using the methodology proposed here as a stepping stone.\nCode/data: https://github.com/epfl-dlab/GPTurk\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Veselovsky_V/0/1/0/all/0/1\">Veniamin Veselovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1\">Manoel Horta Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted Sentiment Classification Benchmark. (arXiv:2306.07902v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07902","description":"<p>Despite impressive advancements in multilingual corpora collection and model\ntraining, developing large-scale deployments of multilingual models still\npresents a significant challenge. This is particularly true for language tasks\nthat are culture-dependent. One such example is the area of multilingual\nsentiment analysis, where affective markers can be subtle and deeply ensconced\nin culture. This work presents the most extensive open massively multilingual\ncorpus of datasets for training sentiment models. The corpus consists of 79\nmanually selected datasets from over 350 datasets reported in the scientific\nliterature based on strict quality criteria. The corpus covers 27 languages\nrepresenting 6 language families. Datasets can be queried using several\nlinguistic and functional features. In addition, we present a multi-faceted\nsentiment classification benchmark summarizing hundreds of experiments\nconducted on different base models, training objectives, dataset collections,\nand fine-tuning strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Augustyniak_L/0/1/0/all/0/1\">&#x141;ukasz Augustyniak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wozniak_S/0/1/0/all/0/1\">Szymon Wo&#x17a;niak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gruza_M/0/1/0/all/0/1\">Marcin Gruza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gramacki_P/0/1/0/all/0/1\">Piotr Gramacki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajda_K/0/1/0/all/0/1\">Krzysztof Rajda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morzy_M/0/1/0/all/0/1\">Miko&#x142;aj Morzy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kajdanowicz_T/0/1/0/all/0/1\">Tomasz Kajdanowicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences. (arXiv:2306.07906v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07906","description":"<p>We present WebGLM, a web-enhanced question-answering system based on the\nGeneral Language Model (GLM). Its goal is to augment a pre-trained large\nlanguage model (LLM) with web search and retrieval capabilities while being\nefficient for real-world deployments. To achieve this, we develop WebGLM with\nstrategies for the LLM-augmented retriever, bootstrapped generator, and human\npreference-aware scorer. Specifically, we identify and address the limitations\nof WebGPT (OpenAI), through which WebGLM is enabled with accuracy, efficiency,\nand cost-effectiveness advantages. In addition, we propose systematic criteria\nfor evaluating web-enhanced QA systems. We conduct multi-dimensional human\nevaluation and quantitative ablation studies, which suggest the outperformance\nof the proposed WebGLM designs over existing systems. WebGLM with the\n10-billion-parameter GLM (10B) is shown to perform better than the\nsimilar-sized WebGPT (13B) and even comparably to WebGPT (175B) in human\nevaluation. The code, demo, and data are at\n\\url{https://github.com/THUDM/WebGLM}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1\">Hanyu Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yifan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Aohan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zhengxiao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Theory of Unsupervised Speech Recognition. (arXiv:2306.07926v1 [eess.AS])","link":"http://arxiv.org/abs/2306.07926","description":"<p>Unsupervised speech recognition (ASR-U) is the problem of learning automatic\nspeech recognition (ASR) systems from unpaired speech-only and text-only\ncorpora. While various algorithms exist to solve this problem, a theoretical\nframework is missing from studying their properties and addressing such issues\nas sensitivity to hyperparameters and training instability. In this paper, we\nproposed a general theoretical framework to study the properties of ASR-U\nsystems based on random matrix theory and the theory of neural tangent kernels.\nSuch a framework allows us to prove various learnability conditions and sample\ncomplexity bounds of ASR-U. Extensive ASR-U experiments on synthetic languages\nwith three classes of transition graphs provide strong empirical evidence for\nour theory (code available at cactuswiththoughts/UnsupASRTheory.git).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Liming Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hasegawa_Johnson_M/0/1/0/all/0/1\">Mark Hasegawa-Johnson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoo_C/0/1/0/all/0/1\">Chang D. Yoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Model Is Semi-Parametric Reinforcement Learning Agent. (arXiv:2306.07929v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07929","description":"<p>Inspired by the insights in cognitive science with respect to human memory\nand reasoning mechanism, a novel evolvable LLM-based (Large Language Model)\nagent framework is proposed as REMEMBERER. By equipping the LLM with a\nlong-term experience memory, REMEMBERER is capable of exploiting the\nexperiences from the past episodes even for different task goals, which excels\nan LLM-based agent with fixed exemplars or equipped with a transient working\nmemory. We further introduce Reinforcement Learning with Experience Memory\n(RLEM) to update the memory. Thus, the whole system can learn from the\nexperiences of both success and failure, and evolve its capability without\nfine-tuning the parameters of the LLM. In this way, the proposed REMEMBERER\nconstitutes a semi-parametric RL agent. Extensive experiments are conducted on\ntwo RL task sets to evaluate the proposed framework. The average results with\ndifferent initialization and training sets exceed the prior SOTA by 4% and 2%\nfor the success rate on two task sets and demonstrate the superiority and\nrobustness of REMEMBERER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Danyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Situo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongshen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zihan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Kai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human-in-the-Loop through Chain-of-Thought. (arXiv:2306.07932v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07932","description":"<p>While the emergence of powerful language models along with Chain-of-thought\nprompting has made automation more and more omnipresent, it sometimes\ndemonstrates its weakness in long-term or multi-step logical reasoning. For\nexample, users don't always get desirable answers for complex mathematical\nproblems without human involvement. Against this background, we present the\nManual Correction System (MCS) -- a human-in-the-loop system enhanced by\nChain-of-Thought prompting, which explores how manual correction of sub-logics\nin rationales can improve LLM's reasoning performance. Moving one step forward,\nconsidering a system with human-in-the-loop involves more than having humans\nimprove performance but also controlling the cost. Therefore, we post a\nCost-utility Analysis Model for Human-in-the-Loop systems (CAMLOP) based on\nclassical economics theory to analyze, quantify and balance the utility and the\ncorresponding cost. We conduct experiments of MCS and CAMLOP with twelve\ndatasets. A significant advantage w.r.t cost and utility proves its superiority\nover strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zefan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wenjuan Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Telecom Language Through Large Language Models. (arXiv:2306.07933v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07933","description":"<p>The recent progress of artificial intelligence (AI) opens up new frontiers in\nthe possibility of automating many tasks involved in Telecom networks design,\nimplementation, and deployment. This has been further pushed forward with the\nevolution of generative artificial intelligence (AI), including the emergence\nof large language models (LLMs), which is believed to be the cornerstone toward\nrealizing self-governed, interactive AI agents. Motivated by this, in this\npaper, we aim to adapt the paradigm of LLMs to the Telecom domain. In\nparticular, we fine-tune several LLMs including BERT, distilled BERT, RoBERTa\nand GPT-2, to the Telecom domain languages, and demonstrate a use case for\nidentifying the 3rd Generation Partnership Project (3GPP) standard working\ngroups. We consider training the selected models on 3GPP technical documents\n(Tdoc) pertinent to years 2009-2019 and predict the Tdoc categories in years\n2020-2023. The results demonstrate that fine-tuning BERT and RoBERTa model\nachieves 84.6% accuracy, while GPT-2 model achieves 83% in identifying 3GPP\nworking groups. The distilled BERT model with around 50% less parameters\nachieves similar performance as others. This corroborates that fine-tuning\npretrained LLM can effectively identify the categories of Telecom language. The\ndeveloped framework shows a stepping stone towards realizing intent-driven and\nself-evolving wireless networks from Telecom languages, and paves the way for\nthe implementation of generative AI in the Telecom domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bariah_L/0/1/0/all/0/1\">Lina Bariah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1\">Hang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qiyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouhouche_B/0/1/0/all/0/1\">Belkacem Mouhouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bader_F/0/1/0/all/0/1\">Faouzi Bader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Debbah_M/0/1/0/all/0/1\">Merouane Debbah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information. (arXiv:2306.07934v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07934","description":"<p>Automated reasoning with unstructured natural text is a key requirement for\nmany potential applications of NLP and for developing robust AI systems.\nRecently, Language Models (LMs) have demonstrated complex reasoning capacities\neven without any finetuning. However, existing evaluation for automated\nreasoning assumes access to a consistent and coherent set of information over\nwhich models reason. When reasoning in the real-world, the available\ninformation is frequently inconsistent or contradictory, and therefore models\nneed to be equipped with a strategy to resolve such conflicts when they arise.\nOne widely-applicable way of resolving conflicts is to impose preferences over\ninformation sources (e.g., based on source credibility or information recency)\nand adopt the source with higher preference. In this paper, we formulate the\nproblem of reasoning with contradictory information guided by preferences over\nsources as the classical problem of defeasible reasoning, and develop a dataset\ncalled BoardgameQA for measuring the reasoning capacity of LMs in this setting.\nBoardgameQA also incorporates reasoning with implicit background knowledge, to\nbetter reflect reasoning problems in downstream applications. We benchmark\nvarious LMs on BoardgameQA and the results reveal a significant gap in the\nreasoning capacity of state-of-the-art LMs on this problem, showing that\nreasoning with conflicting information does not surface out-of-the-box in LMs.\nWhile performance can be improved with finetuning, it nevertheless remains\npoor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kazemi_M/0/1/0/all/0/1\">Mehran Kazemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Q/0/1/0/all/0/1\">Quan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_D/0/1/0/all/0/1\">Deepti Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Najoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imbrasaite_V/0/1/0/all/0/1\">Vaiva Imbrasaite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_D/0/1/0/all/0/1\">Deepak Ramachandran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Representation Learning for Social Post Location Inference. (arXiv:2306.07935v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07935","description":"<p>Inferring geographic locations via social posts is essential for many\npractical location-based applications such as product marketing,\npoint-of-interest recommendation, and infector tracking for COVID-19. Unlike\nimage-based location retrieval or social-post text embedding-based location\ninference, the combined effect of multi-modal information (i.e., post images,\ntext, and hashtags) for social post positioning receives less attention. In\nthis work, we collect real datasets of social posts with images, texts, and\nhashtags from Instagram and propose a novel Multi-modal Representation Learning\nFramework (MRLF) capable of fusing different modalities of social posts for\nlocation inference. MRLF integrates a multi-head attention mechanism to enhance\nlocation-salient information extraction while significantly improving location\ninference compared with single domain-based methods. To overcome the noisy\nuser-generated textual content, we introduce a novel attention-based\ncharacter-aware module that considers the relative dependencies between\ncharacters of social post texts and hashtags for flexible multi-model\ninformation fusion. The experimental results show that MRLF can make accurate\nlocation predictions and open a new door to understanding the multi-modal data\nof social posts for online inference tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_R/0/1/0/all/0/1\">Ruiting Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiayi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xucheng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_L/0/1/0/all/0/1\">Lisi Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wanlun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Fan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FOOCTTS: Generating Arabic Speech with Acoustic Environment for Football Commentator. (arXiv:2306.07936v1 [eess.AS])","link":"http://arxiv.org/abs/2306.07936","description":"<p>This paper presents FOOCTTS, an automatic pipeline for a football commentator\nthat generates speech with background crowd noise. The application gets the\ntext from the user, applies text pre-processing such as vowelization, followed\nby the commentator's speech synthesizer. Our pipeline included Arabic automatic\nspeech recognition for data labeling, CTC segmentation, transcription\nvowelization to match speech, and fine-tuning the TTS. Our system is capable of\ngenerating speech with its acoustic environment within limited 15 minutes of\nfootball commentator recording. Our prototype is generalizable and can be\neasily applied to different domains and languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Baali_M/0/1/0/all/0/1\">Massa Baali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-Calls: Enhancing Call Segmentation and Tagging by Generating Synthetic Conversations via Large Language Models. (arXiv:2306.07941v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07941","description":"<p>Transcriptions of phone calls are of significant value across diverse fields,\nsuch as sales, customer service, healthcare, and law enforcement. Nevertheless,\nthe analysis of these recorded conversations can be an arduous and\ntime-intensive process, especially when dealing with extended or multifaceted\ndialogues. In this work, we propose a novel method, GPT-distilled Calls\nSegmentation and Tagging (GPT-Calls), for efficient and accurate call\nsegmentation and topic extraction. GPT-Calls is composed of offline and online\nphases. The offline phase is applied once to a given list of topics and\ninvolves generating a distribution of synthetic sentences for each topic using\na GPT model and extracting anchor vectors. The online phase is applied to every\ncall separately and scores the similarity between the transcripted conversation\nand the topic anchors found in the offline phase. Then, time domain analysis is\napplied to the similarity scores to group utterances into segments and tag them\nwith topics. The proposed paradigm provides an accurate and efficient method\nfor call segmentation and topic extraction that does not require labeled data,\nthus making it a versatile approach applicable to various domains. Our\nalgorithm operates in production under Dynamics 365 Sales Conversation\nIntelligence, and our research is based on real sales conversations gathered\nfrom various Dynamics 365 Sales tenants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malkiel_I/0/1/0/all/0/1\">Itzik Malkiel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1\">Uri Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yehuda_Y/0/1/0/all/0/1\">Yakir Yehuda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keren_S/0/1/0/all/0/1\">Shahar Keren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barkan_O/0/1/0/all/0/1\">Oren Barkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ronen_R/0/1/0/all/0/1\">Royi Ronen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koenigstein_N/0/1/0/all/0/1\">Noam Koenigstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech-to-Text Adapter and Speech-to-Entity Retriever Augmented LLMs for Speech Understanding. (arXiv:2306.07944v1 [eess.AS])","link":"http://arxiv.org/abs/2306.07944","description":"<p>Large Language Models (LLMs) have been applied in the speech domain, often\nincurring a performance drop due to misaligned between speech and language\nrepresentations. To bridge this gap, we propose a joint speech and language\nmodel (SLM) using a Speech2Text adapter, which maps speech into text token\nembedding space without speech information loss. Additionally, using a\nCTC-based blank-filtering, we can reduce the speech sequence length to that of\ntext. In speech MultiWoz dataset (DSTC11 challenge), SLM largely improves the\ndialog state tracking (DST) performance (24.7% to 28.4% accuracy). Further to\naddress errors on rare entities, we augment SLM with a Speech2Entity retriever,\nwhich uses speech to retrieve relevant entities, and then adds them to the\noriginal SLM input as a prefix. With this retrieval-augmented SLM (ReSLM), the\nDST performance jumps to 34.6% accuracy. Moreover, augmenting the ASR task with\nthe dialog understanding task improves the ASR performance from 9.4% to 8.5%\nWER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_M/0/1/0/all/0/1\">Mingqiu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shafran_I/0/1/0/all/0/1\">Izhak Shafran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soltau_H/0/1/0/all/0/1\">Hagen Soltau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shafey_L/0/1/0/all/0/1\">Laurent El Shafey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Questioning the Survey Responses of Large Language Models. (arXiv:2306.07951v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07951","description":"<p>As large language models increase in capability, researchers have started to\nconduct surveys of all kinds on these models with varying scientific\nmotivations. In this work, we examine what we can learn from a model's survey\nresponses on the basis of the well-established American Community Survey (ACS)\nby the U.S. Census Bureau. Evaluating more than a dozen different models,\nvarying in size from a few hundred million to ten billion parameters, hundreds\nof thousands of times each on questions from the ACS, we systematically\nestablish two dominant patterns. First, smaller models have a significant\nposition and labeling bias, for example, towards survey responses labeled with\nthe letter \"A\". This A-bias diminishes, albeit slowly, as model size increases.\nSecond, when adjusting for this labeling bias through randomized answer\nordering, models still do not trend toward US population statistics or those of\nany cognizable population. Rather, models across the board trend toward\nuniformly random aggregate statistics over survey responses. This pattern is\nrobust to various different ways of prompting the model, including what is the\nde-facto standard. Our findings demonstrate that aggregate statistics of a\nlanguage model's survey responses lack the signals found in human populations.\nThis absence of statistical signal cautions about the use of survey responses\nfrom large language models at present time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dominguez_Olmedo_R/0/1/0/all/0/1\">Ricardo Dominguez-Olmedo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardt_M/0/1/0/all/0/1\">Moritz Hardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendler_Dunner_C/0/1/0/all/0/1\">Celestine Mendler-D&#xfc;nner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MOFI: Learning Image Representations from Noisy Entity Annotated Images. (arXiv:2306.07952v1 [cs.CV])","link":"http://arxiv.org/abs/2306.07952","description":"<p>We present MOFI, a new vision foundation model designed to learn image\nrepresentations from noisy entity annotated images. MOFI differs from previous\nwork in two key aspects: ($i$) pre-training data, and ($ii$) training recipe.\nRegarding data, we introduce a new approach to automatically assign entity\nlabels to images from noisy image-text pairs. Our approach involves employing a\nnamed entity recognition model to extract entities from the alt-text, and then\nusing a CLIP model to select the correct entities as labels of the paired\nimage. The approach is simple, does not require costly human annotation, and\ncan be readily scaled up to billions of image-text pairs mined from the web.\nThrough this method, we have created Image-to-Entities (I2E), a new large-scale\ndataset with 1 billion images and 2 million distinct entities, covering rich\nvisual concepts in the wild. Building upon the I2E dataset, we study different\ntraining recipes, including supervised pre-training, contrastive pre-training,\nand multi-task learning. For constrastive pre-training, we treat entity names\nas free-form text, and further enrich them with entity descriptions.\nExperiments show that supervised pre-training with large-scale fine-grained\nentity labels is highly effective for image retrieval tasks, and multi-task\ntraining further improves the performance. The final MOFI model achieves 86.66%\nmAP on the challenging GPR1200 dataset, surpassing the previous\nstate-of-the-art performance of 72.19% from OpenAI's CLIP model. Further\nexperiments on zero-shot and linear probe image classification also show that\nMOFI outperforms a CLIP model trained on the original image-text data,\ndemonstrating the effectiveness of the I2E dataset in learning strong image\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wentao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofeev_A/0/1/0/all/0/1\">Aleksei Timofeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_K/0/1/0/all/0/1\">Kun Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuangning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yantao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1\">Jon Shlens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xianzhi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"arXiVeri: Automatic table verification with GPT. (arXiv:2306.07968v1 [cs.CL])","link":"http://arxiv.org/abs/2306.07968","description":"<p>Without accurate transcription of numerical data in scientific documents, a\nscientist cannot draw accurate conclusions. Unfortunately, the process of\ncopying numerical data from one paper to another is prone to human error. In\nthis paper, we propose to meet this challenge through the novel task of\nautomatic table verification (AutoTV), in which the objective is to verify the\naccuracy of numerical data in tables by cross-referencing cited sources. To\nsupport this task, we propose a new benchmark, arXiVeri, which comprises\ntabular data drawn from open-access academic papers on arXiv. We introduce\nmetrics to evaluate the performance of a table verifier in two key areas: (i)\ntable matching, which aims to identify the source table in a cited document\nthat corresponds to a target table, and (ii) cell matching, which aims to\nlocate shared cells between a target and source table and identify their row\nand column indices accurately. By leveraging the flexible capabilities of\nmodern large language models (LLMs), we propose simple baselines for table\nverification. Our findings highlight the complexity of this task, even for\nstate-of-the-art LLMs like OpenAI's GPT-4. The code and benchmark will be made\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_G/0/1/0/all/0/1\">Gyungin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1\">Samuel Albanie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Trio Neural Model for Dynamic Entity Relatedness Ranking. (arXiv:1808.08316v4 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/1808.08316","description":"<p>Measuring entity relatedness is a fundamental task for many natural language\nprocessing and information retrieval applications. Prior work often studies\nentity relatedness in static settings and an unsupervised manner. However,\nentities in real-world are often involved in many different relationships,\nconsequently entity-relations are very dynamic over time. In this work, we\npropose a neural networkbased approach for dynamic entity relatedness,\nleveraging the collective attention as supervision. Our model is capable of\nlearning rich and different entity representations in a joint framework.\nThrough extensive experiments on large-scale datasets, we demonstrate that our\nmethod achieves better results than competitive baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Tuan Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nejdl_W/0/1/0/all/0/1\">Wolfgang Nejdl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Samanantar: The Largest Publicly Available Parallel Corpora Collection for 11 Indic Languages. (arXiv:2104.05596v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.05596","description":"<p>We present Samanantar, the largest publicly available parallel corpora\ncollection for Indic languages. The collection contains a total of 49.7 million\nsentence pairs between English and 11 Indic languages (from two language\nfamilies). Specifically, we compile 12.4 million sentence pairs from existing,\npublicly-available parallel corpora, and additionally mine 37.4 million\nsentence pairs from the web, resulting in a 4x increase. We mine the parallel\nsentences from the web by combining many corpora, tools, and methods: (a)\nweb-crawled monolingual corpora, (b) document OCR for extracting sentences from\nscanned documents, (c) multilingual representation models for aligning\nsentences, and (d) approximate nearest neighbor search for searching in a large\ncollection of sentences. Human evaluation of samples from the newly mined\ncorpora validate the high quality of the parallel sentences across 11\nlanguages. Further, we extract 83.4 million sentence pairs between all 55 Indic\nlanguage pairs from the English-centric parallel corpus using English as the\npivot language. We trained multilingual NMT models spanning all these languages\non Samanantar, which outperform existing models and baselines on publicly\navailable benchmarks, such as FLORES, establishing the utility of Samanantar.\nOur data and models are available publicly at\nhttps://ai4bharat.iitm.ac.in/samanantar and we hope they will help advance\nresearch in NMT and multilingual NLP for Indic languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_G/0/1/0/all/0/1\">Gowtham Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doddapaneni_S/0/1/0/all/0/1\">Sumanth Doddapaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bheemaraj_A/0/1/0/all/0/1\">Aravinth Bheemaraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jobanputra_M/0/1/0/all/0/1\">Mayank Jobanputra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AK_R/0/1/0/all/0/1\">Raghavan AK</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Ajitesh Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahoo_S/0/1/0/all/0/1\">Sujit Sahoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diddee_H/0/1/0/all/0/1\">Harshita Diddee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+J_M/0/1/0/all/0/1\">Mahalakshmi J</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakwani_D/0/1/0/all/0/1\">Divyanshu Kakwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1\">Navneet Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pradeep_A/0/1/0/all/0/1\">Aswin Pradeep</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagaraj_S/0/1/0/all/0/1\">Srihari Nagaraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deepak_K/0/1/0/all/0/1\">Kumar Deepak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1\">Vivek Raghavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh Shantadevi Khapra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextualized Semantic Distance between Highly Overlapped Texts. (arXiv:2110.01176v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01176","description":"<p>Overlapping frequently occurs in paired texts in natural language processing\ntasks like text editing and semantic similarity evaluation. Better evaluation\nof the semantic distance between the overlapped sentences benefits the language\nsystem's understanding and guides the generation. Since conventional semantic\nmetrics are based on word representations, they are vulnerable to the\ndisturbance of overlapped components with similar representations. This paper\naims to address the issue with a mask-and-predict strategy. We take the words\nin the longest common sequence (LCS) as neighboring words and use masked\nlanguage modeling (MLM) from pre-trained language models (PLMs) to predict the\ndistributions on their positions. Our metric, Neighboring Distribution\nDivergence (NDD), represent the semantic distance by calculating the divergence\nbetween distributions in the overlapped parts. Experiments on Semantic Textual\nSimilarity show NDD to be more sensitive to various semantic differences,\nespecially on highly overlapped paired texts. Based on the discovery, we\nfurther implement an unsupervised and training-free method for text\ncompression, leading to a significant improvement on the previous\nperplexity-based method. The high scalability of our method even enables NDD to\noutperform the supervised state-of-the-art in domain adaption by a huge margin.\nFurther experiments on syntax and semantics analyses verify the awareness of\ninternal sentence structures, indicating the high potential of NDD for further\nstudies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Letian Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HELP ME THINK: A Simple Prompting Strategy for Non-experts to Create Customized Content with Models. (arXiv:2208.08232v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.08232","description":"<p>Controlling the text generated by language models and customizing the content\nhas been a long-standing challenge. Existing prompting techniques proposed in\npursuit of providing control are task-specific and lack generality; this\nprovides overwhelming choices for non-expert users to find a suitable method\nfor their task. The effort associated with those techniques, such as in writing\nexamples, explanations, instructions, etc. further limits their adoption among\nnon-expert users. In this paper, we propose a simple prompting strategy HELP ME\nTHINK where we encourage GPT3 to help non-expert users by asking a set of\nrelevant questions and leveraging user answers to execute the task. We\ndemonstrate the efficacy of our technique HELP ME THINK on a variety of tasks.\nSpecifically, we focus on tasks that are hard for average humans and require\nsignificant thinking to perform. We hope our work will encourage the\ndevelopment of unconventional ways to harness the power of large language\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nouri_E/0/1/0/all/0/1\">Elnaz Nouri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-based Text Classification on Unified Bangla Multi-class Emotion Corpus. (arXiv:2210.06405v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06405","description":"<p>In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sourav_M/0/1/0/all/0/1\">Md Sakib Ullah Sourav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmud_M/0/1/0/all/0/1\">Mohammad Sultan Mahmud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hua Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complementary Explanations for Effective In-Context Learning. (arXiv:2211.13892v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.13892","description":"<p>Large language models (LLMs) have exhibited remarkable capabilities in\nlearning from explanations in prompts, but there has been limited understanding\nof exactly how these explanations function or why they are effective. This work\naims to better understand the mechanisms by which explanations are used for\nin-context learning. We first study the impact of two different factors on the\nperformance of prompts with explanations: the computation trace (the way the\nsolution is decomposed) and the natural language used to express the prompt. By\nperturbing explanations on three controlled tasks, we show that both factors\ncontribute to the effectiveness of explanations. We further study how to form\nmaximally effective sets of explanations for solving a given test query. We\nfind that LLMs can benefit from the complementarity of the explanation set:\ndiverse reasoning skills shown by different exemplars can lead to better\nperformance. Therefore, we propose a maximal marginal relevance-based exemplar\nselection approach for constructing exemplar sets that are both relevant as\nwell as complementary, which successfully improves the in-context learning\nperformance across three real-world tasks on multiple LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_S/0/1/0/all/0/1\">Srinivasan Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_V/0/1/0/all/0/1\">Ves Stoyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1\">Ramakanth Pasunuru</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LENS: A Learnable Evaluation Metric for Text Simplification. (arXiv:2212.09739v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09739","description":"<p>Training learnable metrics using modern language models has recently emerged\nas a promising method for the automatic evaluation of machine translation.\nHowever, existing human evaluation datasets for text simplification have\nlimited annotations that are based on unitary or outdated models, making them\nunsuitable for this approach. To address these issues, we introduce the\nSimpEval corpus that contains: SimpEval_past, comprising 12K human ratings on\n2.4K simplifications of 24 past systems, and SimpEval_2022, a challenging\nsimplification benchmark consisting of over 1K human ratings of 360\nsimplifications including GPT-3.5 generated text. Training on SimpEval, we\npresent LENS, a Learnable Evaluation Metric for Text Simplification. Extensive\nempirical results show that LENS correlates much better with human judgment\nthan existing metrics, paving the way for future progress in the evaluation of\ntext simplification. We also introduce Rank and Rate, a human evaluation\nframework that rates simplifications from several models in a list-wise manner\nusing an interactive interface, which ensures both consistency and accuracy in\nthe evaluation process and is used to create the SimpEval datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maddela_M/0/1/0/all/0/1\">Mounica Maddela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">Yao Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heineman_D/0/1/0/all/0/1\">David Heineman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Are Reasoning Teachers. (arXiv:2212.10071v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10071","description":"<p>Recent works have shown that chain-of-thought (CoT) prompting can elicit\nlanguage models to solve complex reasoning tasks, step-by-step. However,\nprompt-based CoT methods are dependent on very large models such as GPT-3 175B\nwhich are prohibitive to deploy at scale. In this paper, we use these large\nmodels as reasoning teachers to enable complex reasoning in smaller models and\nreduce model size requirements by several orders of magnitude. We propose\nFine-tune-CoT, a method that generates reasoning samples from very large\nteacher models to fine-tune smaller models. We evaluate our method on a wide\nrange of public models and complex tasks. We find that Fine-tune-CoT enables\nsubstantial reasoning capability in small models, far outperforming\nprompt-based baselines and even the teacher model in many tasks. Additionally,\nwe extend our method by leveraging the teacher model's ability to generate\nmultiple distinct rationales for each original sample. Enriching the\nfine-tuning data with such diverse reasoning results in a substantial\nperformance boost across datasets, even for very small models. We conduct\nablations and sample studies to understand the emergence of reasoning\ncapabilities of student models. Our code implementation and data are available\nat https://github.com/itsnamgyu/reasoning-teacher.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1\">Namgyu Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_L/0/1/0/all/0/1\">Laura Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Se-Young Yun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval. (arXiv:2303.03004v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.03004","description":"<p>AI systems that can create codes as solutions to problems or assist\ndevelopers in writing codes can increase productivity and make programming more\naccessible. Recently, pre-trained large language models have shown impressive\nabilities in generating codes from natural language descriptions, repairing\nbuggy codes, translating codes between languages, and retrieving relevant code\nsegments. However, the evaluation of these models has often been performed in a\nscattered way on only one or two specific tasks, in a few languages, at a\npartial granularity (e.g., function) level, and in many cases without proper\ntraining data. Even more concerning is that in most cases the evaluation of\ngenerated codes has been done in terms of mere lexical overlap with a reference\ncode rather than actual execution. We introduce xCodeEval, the largest\nexecutable multilingual multitask benchmark to date consisting of 25M\ndocument-level coding examples (16.5B tokens) from about 7.5K unique problems\ncovering up to 11 programming languages with execution-level parallelism. It\nfeatures a total of seven tasks involving code understanding, generation,\ntranslation and retrieval. xCodeEval adopts an execution-based evaluation and\noffers a multilingual code execution engine, ExecEval that supports unit test\nbased execution in all the 11 languages. To address the challenge of balancing\nthe distributions of text-code samples over multiple attributes in\nvalidation/test sets, we further propose a novel data splitting and a data\nselection schema based on the geometric mean and graph-theoretic principle.\nExperimental results on all the tasks and languages show xCodeEval is a\npromising yet challenging benchmark as per the current advancements in language\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mohammad Abdullah Matin Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1\">M Saiful Bari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_X/0/1/0/all/0/1\">Xuan Long Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weishi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parvez_M/0/1/0/all/0/1\">Md Rizwan Parvez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"eP-ALM: Efficient Perceptual Augmentation of Language Models. (arXiv:2303.11403v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2303.11403","description":"<p>Large Language Models (LLMs) have so far impressed the world, with\nunprecedented capabilities that emerge in models at large scales. On the vision\nside, transformer models (i.e., ViT) are following the same trend, achieving\nthe best performance on challenging benchmarks. With the abundance of such\nunimodal models, a natural question arises; do we need also to follow this\ntrend to tackle multimodal tasks? In this work, we propose to rather direct\neffort to efficient adaptations of existing models, and propose to augment\nLanguage Models with perception. Existing approaches for adapting pretrained\nmodels for vision-language tasks still rely on several key components that\nhinder their efficiency. In particular, they still train a large number of\nparameters, rely on large multimodal pretraining, use encoders (e.g., CLIP)\ntrained on huge image-text datasets, and add significant inference overhead. In\naddition, most of these approaches have focused on Zero-Shot and In Context\nLearning, with little to no effort on direct finetuning. We investigate the\nminimal computational effort needed to adapt unimodal models for multimodal\ntasks and propose a new challenging setup, alongside different approaches, that\nefficiently adapts unimodal pretrained models. We show that by freezing more\nthan 99\\% of total parameters, training only one linear projection layer, and\nprepending only one trainable token, our approach (dubbed eP-ALM) significantly\noutperforms other baselines on VQA and Captioning across Image, Video, and\nAudio modalities, following the proposed setup. The code will be available\nhere: https://github.com/mshukor/eP-ALM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shukor_M/0/1/0/all/0/1\">Mustafa Shukor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dancette_C/0/1/0/all/0/1\">Corentin Dancette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understand Legal Documents with Contextualized Large Language Models. (arXiv:2303.12135v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.12135","description":"<p>The growth of pending legal cases in populous countries, such as India, has\nbecome a major issue. Developing effective techniques to process and understand\nlegal documents is extremely useful in resolving this problem. In this paper,\nwe present our systems for SemEval-2023 Task 6: understanding legal texts (Modi\net al., 2023). Specifically, we first develop the Legal-BERT-HSLN model that\nconsiders the comprehensive context information in both intra- and\ninter-sentence levels to predict rhetorical roles (subtask A) and then train a\nLegal-LUKE model, which is legal-contextualized and entity-aware, to recognize\nlegal entities (subtask B). Our evaluations demonstrate that our designed\nmodels are more accurate than baselines, e.g., with an up to 15.0% better F1\nscore in subtask B. We achieved notable performance in the task leaderboard,\ne.g., 0.834 micro F1 score, and ranked No.5 out of 27 teams in subtask A.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuchen Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To Asymmetry and Beyond: Structured Pruning of Sequence to Sequence Models for Improved Inference Efficiency. (arXiv:2304.02721v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.02721","description":"<p>Sequence-to-sequence language models can be used to produce abstractive\nsummaries which are coherent, relevant, and concise. Still, model sizes can\nmake deployment in latency-sensitive or web-scale implementations difficult.\nThis paper studies the relationship between model size, structured pruning,\ninference efficiency, and summarization accuracy on widely used summarization\ndatasets. We show that model accuracy is tied to the encoder size while\ninference efficiency is connected to the decoder. Using asymmetric pruning can\nlead to nearly 3x improvement in inference latency with ~1 point loss in\nRouge-2. Moreover, we find both the average degradation and the role of\nasymmetry to be consistent across model sizes and variations in datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Campos_D/0/1/0/all/0/1\">Daniel Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark. (arXiv:2304.03279v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2304.03279","description":"<p>Artificial agents have traditionally been trained to maximize reward, which\nmay incentivize power-seeking and deception, analogous to how next-token\nprediction in language models (LMs) may incentivize toxicity. So do agents\nnaturally learn to be Machiavellian? And how do we measure these behaviors in\ngeneral-purpose models such as GPT-4? Towards answering these questions, we\nintroduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games\ncontaining over half a million rich, diverse scenarios that center on social\ndecision-making. Scenario labeling is automated with LMs, which are more\nperformant than human annotators. We mathematize dozens of harmful behaviors\nand use our annotations to evaluate agents' tendencies to be power-seeking,\ncause disutility, and commit ethical violations. We observe some tension\nbetween maximizing reward and behaving ethically. To improve this trade-off, we\ninvestigate LM-based methods to steer agents' towards less harmful behaviors.\nOur results show that agents can both act competently and morally, so concrete\nprogress can currently be made in machine ethics--designing agents that are\nPareto improvements in both safety and capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_A/0/1/0/all/0/1\">Alexander Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1\">Jun Shern Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_A/0/1/0/all/0/1\">Andy Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Nathaniel Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1\">Steven Basart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodside_T/0/1/0/all/0/1\">Thomas Woodside</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_J/0/1/0/all/0/1\">Jonathan Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emmons_S/0/1/0/all/0/1\">Scott Emmons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Energy-based Models are Zero-Shot Planners for Compositional Scene Rearrangement. (arXiv:2304.14391v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2304.14391","description":"<p>Language is compositional; an instruction can express multiple relation\nconstraints to hold among objects in a scene that a robot is tasked to\nrearrange. Our focus in this work is an instructable scene-rearranging\nframework that generalizes to longer instructions and to spatial concept\ncompositions never seen at training time. We propose to represent\nlanguage-instructed spatial concepts with energy functions over relative object\narrangements. A language parser maps instructions to corresponding energy\nfunctions and an open-vocabulary visual-language model grounds their arguments\nto relevant objects in the scene. We generate goal scene configurations by\ngradient descent on the sum of energy functions, one per language predicate in\nthe instruction. Local vision-based policies then re-locate objects to the\ninferred goal locations. We test our model on established instruction-guided\nmanipulation benchmarks, as well as benchmarks of compositional instructions we\nintroduce. We show our model can execute highly compositional instructions\nzero-shot in simulation and in the real world. It outperforms\nlanguage-to-action reactive policies and Large Language Model planners by a\nlarge margin, especially for long instructions that involve compositions of\nmultiple spatial concepts. Simulation and real-world robot execution videos, as\nwell as our code and datasets are publicly available on our website:\nhttps://ebmplanner.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gkanatsios_N/0/1/0/all/0/1\">Nikolaos Gkanatsios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ayush Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xian_Z/0/1/0/all/0/1\">Zhou Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunchu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atkeson_C/0/1/0/all/0/1\">Christopher Atkeson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1\">Katerina Fragkiadaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction. (arXiv:2305.02549v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.02549","description":"<p>The recent advent of self-supervised pre-training techniques has led to a\nsurge in the use of multimodal learning in form document understanding.\nHowever, existing approaches that extend the mask language modeling to other\nmodalities require careful multi-task tuning, complex reconstruction target\ndesigns, or additional pre-training data. In FormNetV2, we introduce a\ncentralized multimodal graph contrastive learning strategy to unify\nself-supervised pre-training for all modalities in one loss. The graph\ncontrastive objective maximizes the agreement of multimodal representations,\nproviding a natural interplay for all modalities without special customization.\nIn addition, we extract image features within the bounding box that joins a\npair of tokens connected by a graph edge, capturing more targeted visual cues\nwithout loading a sophisticated and separately pre-trained image embedder.\nFormNetV2 establishes new state-of-the-art performance on FUNSD, CORD, SROIE\nand Payment benchmarks with a more compact model size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dozat_T/0/1/0/all/0/1\">Timothy Dozat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perot_V/0/1/0/all/0/1\">Vincent Perot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1\">Guolong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kihyuk Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glushnev_N/0/1/0/all/0/1\">Nikolai Glushnev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renshen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Shangbang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1\">Siyang Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1\">Yasuhisa Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_N/0/1/0/all/0/1\">Nan Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiModal-GPT: A Vision and Language Model for Dialogue with Humans. (arXiv:2305.04790v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.04790","description":"<p>We present a vision and language model named MultiModal-GPT to conduct\nmulti-round dialogue with humans. MultiModal-GPT can follow various\ninstructions from humans, such as generating a detailed caption, counting the\nnumber of interested objects, and answering general questions from users.\nMultiModal-GPT is parameter-efficiently fine-tuned from OpenFlamingo, with\nLow-rank Adapter (LoRA) added both in the cross-attention part and the\nself-attention part of the language model. We first construct instruction\ntemplates with vision and language data for multi-modality instruction tuning\nto make the model understand and follow human instructions. We find the quality\nof training data is vital for the dialogue performance, where few data\ncontaining short answers can lead the model to respond shortly to any\ninstructions. To further enhance the ability to chat with humans of the\nMultiModal-GPT, we utilize language-only instruction-following data to train\nthe MultiModal-GPT jointly. The joint training of language-only and\nvisual-language instructions with the \\emph{same} instruction template\neffectively improves dialogue performance. Various demos show the ability of\ncontinuous dialogue of MultiModal-GPT with humans. Code, dataset, and demo are\nat https://github.com/open-mmlab/Multimodal-GPT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_T/0/1/0/all/0/1\">Tao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1\">Chengqi Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shilong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yudong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Miao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kuikun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PaCE: Unified Multi-modal Dialogue Pre-training with Progressive and Compositional Experts. (arXiv:2305.14839v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14839","description":"<p>Perceiving multi-modal information and fulfilling dialogues with humans is a\nlong-term goal of artificial intelligence. Pre-training is commonly regarded as\nan effective approach for multi-modal dialogue. However, due to the limited\navailability of multi-modal dialogue data, there is still scarce research on\nmulti-modal dialogue pre-training. Yet another intriguing challenge emerges\nfrom the encompassing nature of multi-modal dialogue, which involves various\nmodalities and tasks. Moreover, new forms of tasks may arise at unpredictable\npoints in the future. Hence, it is essential for designed multi-modal dialogue\nmodels to possess sufficient flexibility to adapt to such scenarios. This paper\nproposes \\textbf{PaCE}, a unified, structured, compositional multi-modal\ndialogue pre-training framework. It utilizes a combination of several\nfundamental experts to accommodate multiple dialogue-related tasks and can be\npre-trained using limited dialogue and extensive non-dialogue multi-modal data.\nFurthermore, we propose a progressive training method where old experts from\nthe past can assist new experts, facilitating the expansion of their\ncapabilities. Experimental results demonstrate that PaCE achieves\nstate-of-the-art results on eight multi-modal dialog benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunshui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1\">Binyuan Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">ZhiChao Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning. (arXiv:2305.18169v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18169","description":"<p>In recent years, there has been significant progress in developing\npre-trained language models for NLP. However, these models often struggle when\nfine-tuned on small datasets. To address this issue, researchers have proposed\nvarious adaptation approaches. Prompt-based tuning is arguably the most common\nway, especially for larger models. Previous research shows that adding\ncontrastive learning to prompt-based fine-tuning is effective as it helps the\nmodel generate embeddings that are more distinguishable between classes, and it\ncan also be more sample-efficient as the model learns from positive and\nnegative examples simultaneously. One of the most important components of\ncontrastive learning is data augmentation, but unlike computer vision,\neffective data augmentation for NLP is still challenging. This paper proposes\nLM-CPPF, Contrastive Paraphrasing-guided Prompt-based Fine-tuning of Language\nModels, which leverages prompt-based few-shot paraphrasing using generative\nlanguage models, especially large language models such as GPT-3 and OPT-175B,\nfor data augmentation. Our experiments on multiple text classification\nbenchmarks show that this augmentation method outperforms other methods, such\nas easy data augmentation, back translation, and multiple templates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abaskohi_A/0/1/0/all/0/1\">Amirhossein Abaskohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rothe_S/0/1/0/all/0/1\">Sascha Rothe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaghoobzadeh_Y/0/1/0/all/0/1\">Yadollah Yaghoobzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DyGen: Learning from Noisy Labels via Dynamics-Enhanced Generative Modeling. (arXiv:2305.19395v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.19395","description":"<p>Learning from noisy labels is a challenge that arises in many real-world\napplications where training data can contain incorrect or corrupted labels.\nWhen fine-tuning language models with noisy labels, models can easily overfit\nthe label noise, leading to decreased performance. Most existing methods for\nlearning from noisy labels use static input features for denoising, but these\nmethods are limited by the information they can provide on true label\ndistributions and can result in biased or incorrect predictions. In this work,\nwe propose the Dynamics-Enhanced Generative Model (DyGen), which uses dynamic\npatterns in the embedding space during the fine-tuning process of language\nmodels to improve noisy label predictions. DyGen uses the variational\nauto-encoding framework to infer the posterior distributions of true labels\nfrom noisy labels and training dynamics. Additionally, a co-regularization\nmechanism is used to minimize the impact of potentially noisy labels and\npriors. DyGen demonstrates an average accuracy improvement of 3.10% on two\nsynthetic noise datasets and 1.48% on three real-world noise datasets compared\nto the previous state-of-the-art. Extensive experiments and analyses show the\neffectiveness of each component in DyGen. Our code is available for\nreproducibility on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yuchen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingkai Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OWQ: Lessons learned from activation outliers for weight quantization in large language models. (arXiv:2306.02272v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.02272","description":"<p>Large language models (LLMs) with hundreds of billions of parameters show\nimpressive results across various language tasks using simple prompt tuning and\nfew-shot examples, without the need for task-specific fine-tuning. However,\ntheir enormous size requires multiple server-grade GPUs even for inference,\ncreating a significant cost barrier. To address this limitation, we introduce a\nnovel post-training quantization method for weights with minimal quality\ndegradation. While activation outliers are known to be problematic in\nactivation quantization, our theoretical analysis suggests that we can identify\nfactors contributing to weight quantization errors by considering activation\noutliers. We propose an innovative PTQ scheme called outlier-aware weight\nquantization (OWQ), which identifies vulnerable weights and allocates\nhigh-precision to them. Our extensive experiments demonstrate that the 3.01-bit\nmodels produced by OWQ exhibit comparable quality to the 4-bit models generated\nby OPTQ.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Changhun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jungyu Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taesu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyungjun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_E/0/1/0/all/0/1\">Eunhyeok Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gen-IR @ SIGIR 2023: The First Workshop on Generative Information Retrieval. (arXiv:2306.02887v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2306.02887","description":"<p>Generative information retrieval (IR) has experienced substantial growth\nacross multiple research communities (e.g., information retrieval, computer\nvision, natural language processing, and machine learning), and has been highly\nvisible in the popular press. Theoretical, empirical, and actual user-facing\nproducts have been released that retrieve documents (via generation) or\ndirectly generate answers given an input request. We would like to investigate\nwhether end-to-end generative models are just another trend or, as some claim,\na paradigm change for IR. This necessitates new metrics, theoretical grounding,\nevaluation methods, task definitions, models, user interfaces, etc. The goal of\nthis workshop (https://coda.io/@sigir/gen-ir) is to focus on previously\nexplored Generative IR techniques like document retrieval and direct Grounded\nAnswer Generation, while also offering a venue for the discussion and\nexploration of how Generative IR can be applied to new domains like\nrecommendation systems, summarization, etc. The format of the workshop is\ninteractive, including roundtable and keynote sessions and tends to avoid the\none-sided dialogue of a mini-conference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benedict_G/0/1/0/all/0/1\">Gabriel B&#xe9;n&#xe9;dict</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PolyVoice: Language Models for Speech to Speech Translation. (arXiv:2306.02982v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.02982","description":"<p>We propose PolyVoice, a language model-based framework for speech-to-speech\ntranslation (S2ST) system. Our framework consists of two language models: a\ntranslation language model and a speech synthesis language model. We use\ndiscretized speech units, which are generated in a fully unsupervised way, and\nthus our framework can be used for unwritten languages. For the speech\nsynthesis part, we adopt the existing VALL-E X approach and build a unit-based\naudio language model. This grants our framework the ability to preserve the\nvoice characteristics and the speaking style of the original speech. We examine\nour system on Chinese $\\rightarrow$ English and English $\\rightarrow$ Spanish\npairs. Experimental results show that our system can generate speech with high\ntranslation quality and audio quality. Speech samples are available at\nhttps://speechtranslation.github.io/polyvoice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qianqian Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiying Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qiao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_T/0/1/0/all/0/1\">Tom Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yunlong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Siyuan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xuxin Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_F/0/1/0/all/0/1\">Fengpeng Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Ye Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Lu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zejun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts. (arXiv:2306.04528v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.04528","description":"<p>The increasing reliance on Large Language Models (LLMs) across academia and\nindustry necessitates a comprehensive understanding of their robustness to\nprompts. In response to this vital need, we introduce PromptBench, a robustness\nbenchmark designed to measure LLMs' resilience to adversarial prompts. This\nstudy uses a plethora of adversarial textual attacks targeting prompts across\nmultiple levels: character, word, sentence, and semantic. These prompts are\nthen employed in diverse tasks, such as sentiment analysis, natural language\ninference, reading comprehension, machine translation, and math\nproblem-solving. Our study generates 4,032 adversarial prompts, meticulously\nevaluated over 8 tasks and 13 datasets, with 567,084 test samples in total. Our\nfindings demonstrate that contemporary LLMs are vulnerable to adversarial\nprompts. Furthermore, we present comprehensive analysis to understand the\nmystery behind prompt robustness and its transferability. We then offer\ninsightful robustness analysis and pragmatic recommendations for prompt\ncomposition, beneficial to both researchers and everyday users. We make our\ncode, prompts, and methodologies to generate adversarial prompts publicly\naccessible, thereby enabling and encouraging collaborative exploration in this\npivotal field: https://github.com/microsoft/promptbench.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kaijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiaheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zichen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1\">Neil Zhenqiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Training with In-Domain Language Models for Diagnostic Reasoning. (arXiv:2306.04551v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.04551","description":"<p>Generative artificial intelligence (AI) is a promising direction for\naugmenting clinical diagnostic decision support and reducing diagnostic errors,\na leading contributor to medical errors. To further the development of clinical\nAI systems, the Diagnostic Reasoning Benchmark (DR.BENCH) was introduced as a\ncomprehensive generative AI framework, comprised of six tasks representing key\ncomponents in clinical reasoning. We present a comparative analysis of\nin-domain versus out-of-domain language models as well as multi-task versus\nsingle task training with a focus on the problem summarization task in DR.BENCH\n(Gao et al., 2023). We demonstrate that a multi-task, clinically trained\nlanguage model outperforms its general domain counterpart by a large margin,\nestablishing a new state-of-the-art performance, with a ROUGE-L score of 28.55.\nThis research underscores the value of domain-specific training for optimizing\nclinical diagnostic reasoning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_B/0/1/0/all/0/1\">Brihat Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yanjun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_T/0/1/0/all/0/1\">Timothy Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Churpek_M/0/1/0/all/0/1\">Matthew M. Churpek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afshar_M/0/1/0/all/0/1\">Majid Afshar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dligach_D/0/1/0/all/0/1\">Dmitriy Dligach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revealing the Blind Spot of Sentence Encoder Evaluation by HEROS. (arXiv:2306.05083v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.05083","description":"<p>Existing sentence textual similarity benchmark datasets only use a single\nnumber to summarize how similar the sentence encoder's decision is to humans'.\nHowever, it is unclear what kind of sentence pairs a sentence encoder (SE)\nwould consider similar. Moreover, existing SE benchmarks mainly consider\nsentence pairs with low lexical overlap, so it is unclear how the SEs behave\nwhen two sentences have high lexical overlap. We introduce a high-quality SE\ndiagnostic dataset, HEROS. HEROS is constructed by transforming an original\nsentence into a new sentence based on certain rules to form a \\textit{minimal\npair}, and the minimal pair has high lexical overlaps. The rules include\nreplacing a word with a synonym, an antonym, a typo, a random word, and\nconverting the original sentence into its negation. Different rules yield\ndifferent subsets of HEROS. By systematically comparing the performance of over\n60 supervised and unsupervised SEs on HEROS, we reveal that most unsupervised\nsentence encoders are insensitive to negation. We find the datasets used to\ntrain the SE are the main determinants of what kind of sentence pairs an SE\nconsiders similar. We also show that even if two SEs have similar performance\non STS benchmarks, they can have very different behavior on HEROS. Our result\nreveals the blind spot of traditional STS benchmarks when evaluating SEs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_C/0/1/0/all/0/1\">Cheng-Han Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yung-Sung Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EaSyGuide : ESG Issue Identification Framework leveraging Abilities of Generative Large Language Models. (arXiv:2306.06662v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.06662","description":"<p>This paper presents our participation in the FinNLP-2023 shared task on\nmulti-lingual environmental, social, and corporate governance issue\nidentification (ML-ESG). The task's objective is to classify news articles\nbased on the 35 ESG key issues defined by the MSCI ESG rating guidelines. Our\napproach focuses on the English and French subtasks, employing the CerebrasGPT,\nOPT, and Pythia models, along with the zero-shot and GPT3Mix Augmentation\ntechniques. We utilize various encoder models, such as RoBERTa, DeBERTa, and\nFinBERT, subjecting them to knowledge distillation and additional training.\n</p>\n<p>Our approach yielded exceptional results, securing the first position in the\nEnglish text subtask with F1-score 0.69 and the second position in the French\ntext subtask with F1-score 0.78. These outcomes underscore the effectiveness of\nour methodology in identifying ESG issues in news articles across different\nlanguages. Our findings contribute to the exploration of ESG topics and\nhighlight the potential of leveraging advanced language models for ESG issue\nidentification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hanwool Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jonghyun Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1\">Sohyeon Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Sungbum Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Audio-textual Architecture for Robust Spoken Language Understanding. (arXiv:2306.06819v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.06819","description":"<p>Recent voice assistants are usually based on the cascade spoken language\nunderstanding (SLU) solution, which consists of an automatic speech recognition\n(ASR) engine and a natural language understanding (NLU) system. Because such\napproach relies on the ASR output, it often suffers from the so-called ASR\nerror propagation. In this work, we investigate impacts of this ASR error\npropagation on state-of-the-art NLU systems based on pre-trained language\nmodels (PLM), such as BERT and RoBERTa. Moreover, a multimodal language\nunderstanding (MLU) module is proposed to mitigate SLU performance degradation\ncaused by errors present in the ASR transcript. The MLU benefits from\nself-supervised features learned from both audio and text modalities,\nspecifically Wav2Vec for speech and Bert/RoBERTa for language. Our MLU combines\nan encoder network to embed the audio signal and a text encoder to process text\ntranscripts followed by a late fusion layer to fuse audio and text logits. We\nfound that the proposed MLU showed to be robust towards poor quality ASR\ntranscripts, while the performance of BERT and RoBERTa are severely\ncompromised. Our model is evaluated on five tasks from three SLU datasets and\nrobustness is tested using ASR transcripts from three ASR engines. Results show\nthat the proposed approach effectively mitigates the ASR error propagation\nproblem, surpassing the PLM models' performance across all datasets for the\nacademic ASR engine.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avila_A/0/1/0/all/0/1\">Anderson R. Avila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1\">Chao Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LTCR: Long-Text Chinese Rumor Detection Dataset. (arXiv:2306.07201v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.07201","description":"<p>False information can spread quickly on social media, negatively influencing\nthe citizens' behaviors and responses to social events. To better detect all of\nthe fake news, especially long texts which are harder to find completely, a\nLong-Text Chinese Rumor detection dataset named LTCR is proposed. The LTCR\ndataset provides a valuable resource for accurately detecting misinformation,\nespecially in the context of complex fake news related to COVID-19. The dataset\nconsists of 1,729 and 500 pieces of real and fake news, respectively. The\naverage lengths of real and fake news are approximately 230 and 152 characters.\nWe also propose \\method, Salience-aware Fake News Detection Model, which\nachieves the highest accuracy (95.85%), fake news recall (90.91%) and F-score\n(90.60%) on the dataset. (https://github.com/Enderfga/DoubleCheck)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Ziyang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengsha Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_G/0/1/0/all/0/1\">Guian Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-06-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}