{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-09-13T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Studying the impacts of pre-training using ChatGPT-generated text on downstream tasks. (arXiv:2309.05668v1 [cs.CL])","link":"http://arxiv.org/abs/2309.05668","description":"<p>In recent times, significant advancements have been witnessed in the field of\nlanguage models, particularly with the emergence of Large Language Models\n(LLMs) that are trained on vast amounts of data extracted from internet\narchives. These LLMs, such as ChatGPT, have become widely accessible, allowing\nusers to generate text for various purposes including articles, essays, jokes,\nand poetry. Given that LLMs are trained on a diverse range of text sources,\nencompassing platforms like Reddit and Twitter, it is foreseeable that future\ntraining datasets will also incorporate text generated by previous iterations\nof the models themselves. In light of this development, our research aims to\ninvestigate the influence of artificial text in the pre-training phase of\nlanguage models. Specifically, we conducted a comparative analysis between a\nlanguage model, RoBERTa, pre-trained using CNN/DailyMail news articles, and\nChatGPT, which employed the same articles for its training and evaluated their\nperformance on three downstream tasks as well as their potential gender bias,\nusing sentiment analysis as a metric. Through a series of experiments, we\ndemonstrate that the utilization of artificial text during pre-training does\nnot have a significant impact on either the performance of the models in\ndownstream tasks or their gender bias. In conclusion, our findings suggest that\nthe inclusion of text generated by LLMs in their own pre-training process does\nnot yield substantial effects on the subsequent performance of the models in\ndownstream tasks or their potential gender bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anand_S/0/1/0/all/0/1\">Sarthak Anand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Model for Science: A Study on P vs. NP. (arXiv:2309.05689v1 [cs.CL])","link":"http://arxiv.org/abs/2309.05689","description":"<p>In this work, we use large language models (LLMs) to augment and accelerate\nresearch on the P versus NP problem, one of the most important open problems in\ntheoretical computer science and mathematics. Specifically, we propose Socratic\nreasoning, a general framework that promotes in-depth thinking with LLMs for\ncomplex problem-solving. Socratic reasoning encourages LLMs to recursively\ndiscover, solve, and integrate problems while facilitating self-evaluation and\nrefinement. Our pilot study on the P vs. NP problem shows that GPT-4\nsuccessfully produces a proof schema and engages in rigorous reasoning\nthroughout 97 dialogue turns, concluding \"P $\\neq$ NP\", which is in alignment\nwith (Xu and Zhou, 2023). The investigation uncovers novel insights within the\nextensive solution space of LLMs, shedding light on LLM for Science.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qingxiu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guangyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yaru Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hi Model, generating 'nice' instead of 'good' is not as bad as generating 'rice'! Towards Context and Semantic Infused Dialogue Generation Loss Function and Evaluation Metric. (arXiv:2309.05804v1 [cs.CL])","link":"http://arxiv.org/abs/2309.05804","description":"<p>Over the past two decades, dialogue modeling has made significant strides,\nmoving from simple rule-based responses to personalized and persuasive response\ngeneration. However, despite these advancements, the objective functions and\nevaluation metrics for dialogue generation have remained stagnant, i.e.,\ncross-entropy and BLEU, respectively. These lexical-based metrics have the\nfollowing key limitations: (a) word-to-word matching without semantic\nconsideration: It assigns the same credit for failure to generate 'nice' and\n'rice' for 'good'. (b) missing context attribute for evaluating the generated\nresponse: Even if a generated response is relevant to the ongoing dialogue\ncontext, it may still be penalized for not matching the gold utterance provided\nin the corpus. In this paper, we first investigate these limitations\ncomprehensively and propose a new loss function called Semantic Infused\nContextualized diaLogue (SemTextualLogue) loss function. Furthermore, we\nformulate a new evaluation metric called Dialuation, which incorporates both\ncontext relevance and semantic appropriateness while evaluating a generated\nresponse. We conducted experiments on two benchmark dialogue corpora,\nencompassing both task-oriented and open-domain scenarios. We found that the\ndialogue generation model trained with SemTextualLogue loss attained superior\nperformance (in both quantitative and qualitative evaluation) compared to the\ntraditional cross-entropy loss function across the datasets and evaluation\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_A/0/1/0/all/0/1\">Abhisek Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinan_M/0/1/0/all/0/1\">Muhammed Sinan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sriparna Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis. (arXiv:2309.05833v1 [cs.CL])","link":"http://arxiv.org/abs/2309.05833","description":"<p>In recent years, the transition to cloud-based platforms in the IT sector has\nemphasized the significance of cloud incident root cause analysis to ensure\nservice reliability and maintain customer trust. Central to this process is the\nefficient determination of root causes, a task made challenging due to the\ncomplex nature of contemporary cloud infrastructures. Despite the proliferation\nof AI-driven tools for root cause identification, their applicability remains\nlimited by the inconsistent quality of their outputs. This paper introduces a\nmethod for enhancing confidence estimation in root cause analysis tools by\nprompting retrieval-augmented large language models (LLMs). This approach\noperates in two phases. Initially, the model evaluates its confidence based on\nhistorical incident data, considering its assessment of the evidence strength.\nSubsequently, the model reviews the root cause generated by the predictor. An\noptimization step then combines these evaluations to determine the final\nconfidence assignment. Experimental results illustrate that our method enables\nthe model to articulate its confidence effectively, providing a more calibrated\nscore. We address research questions evaluating the ability of our method to\nproduce calibrated confidence scores using LLMs, the impact of domain-specific\nretrieved examples on confidence estimates, and its potential generalizability\nacross various root cause analysis models. Through this, we aim to bridge the\nconfidence estimation gap, aiding on-call engineers in decision-making and\nbolstering the efficiency of cloud incident management.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dylan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_C/0/1/0/all/0/1\">Chetan Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Las_Casas_P/0/1/0/all/0/1\">Pedro Las-Casas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_R/0/1/0/all/0/1\">Rodrigo Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajmohan_S/0/1/0/all/0/1\">Saravan Rajmohan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs. (arXiv:2309.05918v1 [cs.CL])","link":"http://arxiv.org/abs/2309.05918","description":"<p>In our opinion the exuberance surrounding the relative success of data-driven\nlarge language models (LLMs) is slightly misguided and for several reasons (i)\nLLMs cannot be relied upon for factual information since for LLMs all ingested\ntext (factual or non-factual) was created equal; (ii) due to their subsymbolic\nna-ture, whatever 'knowledge' these models acquire about language will always\nbe buried in billions of microfeatures (weights), none of which is meaningful\non its own; and (iii) LLMs will often fail to make the correct inferences in\nseveral linguistic contexts (e.g., nominal compounds, copredication, quantifier\nscope ambi-guities, intensional contexts. Since we believe the relative success\nof data-driven large language models (LLMs) is not a reflection on the symbolic\nvs. subsymbol-ic debate but a reflection on applying the successful strategy of\na bottom-up reverse engineering of language at scale, we suggest in this paper\napplying the effective bottom-up strategy in a symbolic setting resulting in\nsymbolic, explainable, and ontologically grounded language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saba_W/0/1/0/all/0/1\">Walid S. Saba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAGE: Structured Attribute Value Generation for Billion-Scale Product Catalogs. (arXiv:2309.05920v1 [cs.IR])","link":"http://arxiv.org/abs/2309.05920","description":"<p>We introduce SAGE; a Generative LLM for inferring attribute values for\nproducts across world-wide e-Commerce catalogs. We introduce a novel\nformulation of the attribute-value prediction problem as a Seq2Seq\nsummarization task, across languages, product types and target attributes. Our\nnovel modeling approach lifts the restriction of predicting attribute values\nwithin a pre-specified set of choices, as well as, the requirement that the\nsought attribute values need to be explicitly mentioned in the text. SAGE can\ninfer attribute values even when such values are mentioned implicitly using\nperiphrastic language, or not-at-all-as is the case for common-sense defaults.\nAdditionally, SAGE is capable of predicting whether an attribute is\ninapplicable for the product at hand, or non-obtainable from the available\ninformation. SAGE is the first method able to tackle all aspects of the\nattribute-value-prediction task as they arise in practical settings in\ne-Commerce catalogs. A comprehensive set of experiments demonstrates the\neffectiveness of the proposed approach, as well as, its superiority against\nstate-of-the-art competing alternatives. Moreover, our experiments highlight\nSAGE's ability to tackle the task of predicting attribute values in zero-shot\nsetting; thereby, opening up opportunities for significantly reducing the\noverall number of labeled examples required for training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nikolakopoulos_A/0/1/0/all/0/1\">Athanasios N. Nikolakopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaul_S/0/1/0/all/0/1\">Swati Kaul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gade_S/0/1/0/all/0/1\">Siva Karthik Gade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubrov_B/0/1/0/all/0/1\">Bella Dubrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batur_U/0/1/0/all/0/1\">Umit Batur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Suleiman Ali Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Hallucination in Large Foundation Models. (arXiv:2309.05922v1 [cs.AI])","link":"http://arxiv.org/abs/2309.05922","description":"<p>Hallucination in a foundation model (FM) refers to the generation of content\nthat strays from factual reality or includes fabricated information. This\nsurvey paper provides an extensive overview of recent efforts that aim to\nidentify, elucidate, and tackle the problem of hallucination, with a particular\nfocus on ``Large'' Foundation Models (LFMs). The paper classifies various types\nof hallucination phenomena that are specific to LFMs and establishes evaluation\ncriteria for assessing the extent of hallucination. It also examines existing\nstrategies for mitigating hallucination in LFMs and discusses potential\ndirections for future research in this area. Essentially, the paper offers a\ncomprehensive examination of the challenges and solutions related to\nhallucination in LFMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rawte_V/0/1/0/all/0/1\">Vipula Rawte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Amitava Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do PLMs Know and Understand Ontological Knowledge?. (arXiv:2309.05936v1 [cs.CL])","link":"http://arxiv.org/abs/2309.05936","description":"<p>Ontological knowledge, which comprises classes and properties and their\nrelationships, is integral to world knowledge. It is significant to explore\nwhether Pretrained Language Models (PLMs) know and understand such knowledge.\nHowever, existing PLM-probing studies focus mainly on factual knowledge,\nlacking a systematic probing of ontological knowledge. In this paper, we focus\non probing whether PLMs store ontological knowledge and have a semantic\nunderstanding of the knowledge rather than rote memorization of the surface\nform. To probe whether PLMs know ontological knowledge, we investigate how well\nPLMs memorize: (1) types of entities; (2) hierarchical relationships among\nclasses and properties, e.g., Person is a subclass of Animal and Member of\nSports Team is a subproperty of Member of ; (3) domain and range constraints of\nproperties, e.g., the subject of Member of Sports Team should be a Person and\nthe object should be a Sports Team. To further probe whether PLMs truly\nunderstand ontological knowledge beyond memorization, we comprehensively study\nwhether they can reliably perform logical reasoning with given knowledge\naccording to ontological entailment rules. Our probing results show that PLMs\ncan memorize certain ontological knowledge and utilize implicit knowledge in\nreasoning. However, both the memorizing and reasoning performances are less\nthan perfect, indicating incomplete knowledge and understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weiqi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chengyue Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Answering Subjective Induction Questions on Products by Summarizing Multi-sources Multi-viewpoints Knowledge. (arXiv:2309.05938v1 [cs.CL])","link":"http://arxiv.org/abs/2309.05938","description":"<p>This paper proposes a new task in the field of Answering Subjective Induction\nQuestion on Products (SUBJPQA). The answer to this kind of question is\nnon-unique, but can be interpreted from many perspectives. For example, the\nanswer to 'whether the phone is heavy' has a variety of different viewpoints. A\nsatisfied answer should be able to summarize these subjective opinions from\nmultiple sources and provide objective knowledge, such as the weight of a\nphone. That is quite different from the traditional QA task, in which the\nanswer to a factoid question is unique and can be found from a single data\nsource. To address this new task, we propose a three-steps method. We first\nretrieve all answer-related clues from multiple knowledge sources on facts and\nopinions. The implicit commonsense facts are also collected to supplement the\nnecessary but missing contexts. We then capture their relevance with the\nquestions by interactive attention. Next, we design a reinforcement-based\nsummarizer to aggregate all these knowledgeable clues. Based on a\ntemplate-controlled decoder, we can output a comprehensive and\nmulti-perspective answer. Due to the lack of a relevant evaluated benchmark set\nfor the new task, we construct a large-scale dataset, named SupQA, consisting\nof 48,352 samples across 15 product domains. Evaluation results show the\neffectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yufeng Zhang</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng-xiang Wang</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianxing Yu</a> (1, 2 and 4) ((1) School of Artificial Intelligence, Sun Yat-sen University, Zhuhai 519082 (2) Guangdong Key Laboratory of Big Data Analysis and Processing, 510006, China (3) China National Institute of Standardization, 100088, China (4) Pazhou Lab, Guangzhou, 510330, China)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models as Black-Box Optimizers for Vision-Language Models. (arXiv:2309.05950v1 [cs.CL])","link":"http://arxiv.org/abs/2309.05950","description":"<p>Vision-language models (VLMs) pre-trained on web-scale datasets have\ndemonstrated remarkable capabilities across a variety of vision and multimodal\ntasks. Currently, fine-tuning methods for VLMs mainly operate in a white-box\nsetting, requiring access to model parameters for backpropagation. However,\nmany VLMs rely on proprietary data and are not open-source, which restricts the\nuse of white-box approaches for fine-tuning. Given that popular private large\nlanguage models (LLMs) like ChatGPT still offer a language-based user\ninterface, we aim to develop a novel fine-tuning approach for VLMs through\nnatural language prompts, thereby avoiding the need to access model parameters,\nfeature embeddings, or output logits. In this setup, we propose employing\nchat-based LLMs as black-box optimizers to search for the best text prompt on\nthe illustrative task of few-shot image classification using CLIP.\nSpecifically, we adopt an automatic \"hill-climbing\" procedure that converges on\nan effective prompt by evaluating the accuracy of current prompts and asking\nLLMs to refine them based on textual feedback, all within a conversational\nprocess without human-in-the-loop. In a challenging 1-shot learning setup, our\nsimple approach surpasses the white-box continuous prompting method CoOp by an\naverage of 1.5% across 11 datasets including ImageNet. Our approach also\noutperforms OpenAI's manually crafted prompts and is more efficient than other\nblack-box methods like iterative APE. Additionally, we highlight the advantage\nof conversational feedback incorporating both positive and negative prompts,\nsuggesting that LLMs can utilize the implicit \"gradient\" direction in textual\nfeedback for a more efficient search. Lastly, we find that the text prompts\ngenerated through our strategy are not only more interpretable but also\ntransfer well across different CLIP architectures in a black-box manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Samuel Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shihong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhiqiu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balanced and Explainable Social Media Analysis for Public Health with Large Language Models. (arXiv:2309.05951v1 [cs.CL])","link":"http://arxiv.org/abs/2309.05951","description":"<p>As social media becomes increasingly popular, more and more public health\nactivities emerge, which is worth noting for pandemic monitoring and government\ndecision-making. Current techniques for public health analysis involve popular\nmodels such as BERT and large language models (LLMs). Although recent progress\nin LLMs has shown a strong ability to comprehend knowledge by being fine-tuned\non specific domain datasets, the costs of training an in-domain LLM for every\nspecific public health task are especially expensive. Furthermore, such kinds\nof in-domain datasets from social media are generally highly imbalanced, which\nwill hinder the efficiency of LLMs tuning. To tackle these challenges, the data\nimbalance issue can be overcome by sophisticated data augmentation methods for\nsocial media datasets. In addition, the ability of the LLMs can be effectively\nutilised by prompting the model properly. In light of the above discussion, in\nthis paper, a novel ALEX framework is proposed for social media analysis on\npublic health. Specifically, an augmentation pipeline is developed to resolve\nthe data imbalance issue. Furthermore, an LLMs explanation mechanism is\nproposed by prompting an LLM with the predicted results from BERT models.\nExtensive experiments conducted on three tasks at the Social Media Mining for\nHealth 2023 (SMM4H) competition with the first ranking in two tasks demonstrate\nthe superior performance of the proposed ALEX method. Our code has been\nreleased in https://github.com/YanJiangJerry/ALEX.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1\">Ruihong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng-Fei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Moral Machine Experiment on Large Language Models. (arXiv:2309.05958v1 [cs.CL])","link":"http://arxiv.org/abs/2309.05958","description":"<p>As large language models (LLMs) become more deeply integrated into various\nsectors, understanding how they make moral judgments has become crucial,\nparticularly in the realm of autonomous driving. This study utilized the Moral\nMachine framework to investigate the ethical decision-making tendencies of\nprominent LLMs, including GPT-3.5, GPT-4, PaLM 2, and Llama 2, comparing their\nresponses to human preferences. While LLMs' and humans' preferences such as\nprioritizing humans over pets and favoring saving more lives are broadly\naligned, PaLM 2 and Llama 2, especially, evidence distinct deviations.\nAdditionally, despite the qualitative similarities between the LLM and human\npreferences, there are significant quantitative disparities, suggesting that\nLLMs might lean toward more uncompromising decisions, compared to the milder\ninclinations of humans. These insights elucidate the ethical frameworks of LLMs\nand their potential implications for autonomous driving.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Takemoto_K/0/1/0/all/0/1\">Kazuhiro Takemoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms. (arXiv:2309.05961v1 [cs.SI])","link":"http://arxiv.org/abs/2309.05961","description":"<p>Community Question Answering (CQA) platforms steadily gain popularity as they\nprovide users with fast responses to their queries. The swiftness of these\nresponses is contingent on a mixture of query-specific and user-related\nelements. This paper scrutinizes these contributing factors within the context\nof six highly popular CQA platforms, identified through their standout\nanswering speed. Our investigation reveals a correlation between the time taken\nto yield the first response to a question and several variables: the metadata,\nthe formulation of the questions, and the level of interaction among users.\nAdditionally, by employing conventional machine learning models to analyze\nthese metadata and patterns of user interaction, we endeavor to predict which\nqueries will receive their initial responses promptly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hazra_R/0/1/0/all/0/1\">Rima Hazra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1\">Agnik Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1\">Somnath Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Animesh Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Circuit Breaking: Removing Model Behaviors with Targeted Ablation. (arXiv:2309.05973v1 [cs.CL])","link":"http://arxiv.org/abs/2309.05973","description":"<p>Language models often exhibit behaviors that improve performance on a\npre-training objective but harm performance on downstream tasks. We propose a\nnovel approach to removing undesirable behaviors by ablating a small number of\ncausal pathways between model components, with the intention of disabling the\ncomputational circuit responsible for the bad behavior. Given a small dataset\nof inputs where the model behaves poorly, we learn to ablate a small number of\nimportant causal pathways. In the setting of reducing GPT-2 toxic language\ngeneration, we find ablating just 12 of the 11.6K causal edges mitigates toxic\ngeneration with minimal degradation of performance on other inputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Maximilian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davies_X/0/1/0/all/0/1\">Xander Davies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeau_M/0/1/0/all/0/1\">Max Nadeau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Content Reduction, Surprisal and Information Density Estimation for Long Documents. (arXiv:2309.06009v1 [cs.CL])","link":"http://arxiv.org/abs/2309.06009","description":"<p>Many computational linguistic methods have been proposed to study the\ninformation content of languages. We consider two interesting research\nquestions: 1) how is information distributed over long documents, and 2) how\ndoes content reduction, such as token selection and text summarization, affect\nthe information density in long documents. We present four criteria for\ninformation density estimation for long documents, including surprisal,\nentropy, uniform information density, and lexical density. Among those\ncriteria, the first three adopt the measures from information theory. We\npropose an attention-based word selection method for clinical notes and study\nmachine summarization for multiple-domain documents. Our findings reveal the\nsystematic difference in information density of long text in various domains.\nEmpirical results on automated medical coding from long clinical notes show the\neffectiveness of the attention-based word selection method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marttinen_P/0/1/0/all/0/1\">Pekka Marttinen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How does representation impact in-context learning: A exploration on a synthetic task. (arXiv:2309.06054v1 [cs.LG])","link":"http://arxiv.org/abs/2309.06054","description":"<p>In-context learning, i.e., learning from in-context samples, is an impressive\nability of Transformer. However, the mechanism driving the in-context learning\nis not yet fully understood. In this study, we aim to investigate from an\nunderexplored perspective of representation learning. The representation is\nmore complex for in-context learning senario, where the representation can be\nimpacted by both model weights and in-context samples. We refer the above two\nconceptually aspects of representation as in-weight component and in-context\ncomponent, respectively. To study how the two components affect in-context\nlearning capabilities, we construct a novel synthetic task, making it possible\nto device two probes, in-weights probe and in-context probe, to evaluate the\ntwo components, respectively. We demonstrate that the goodness of in-context\ncomponent is highly related to the in-context learning performance, which\nindicates the entanglement between in-context learning and representation\nlearning. Furthermore, we find that a good in-weights component can actually\nbenefit the learning of the in-context component, indicating that in-weights\nlearning should be the foundation of in-context learning. To further understand\nthe the in-context learning mechanism and importance of the in-weights\ncomponent, we proof by construction that a simple Transformer, which uses\npattern matching and copy-past mechanism to perform in-context learning, can\nmatch the in-context learning performance with more complex, best tuned\nTransformer under the perfect in-weights component assumption. In short, those\ndiscoveries from representation learning perspective shed light on new\napproaches to improve the in-context capacity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jingwen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuwang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAP-Gen: Retrieval-Augmented Patch Generation with CodeT5 for Automatic Program Repair. (arXiv:2309.06057v1 [cs.SE])","link":"http://arxiv.org/abs/2309.06057","description":"<p>Automatic program repair (APR) is crucial to reduce manual debugging efforts\nfor developers and improve software reliability. While conventional\nsearch-based techniques typically rely on heuristic rules or a redundancy\nassumption to mine fix patterns, recent years have witnessed the surge of deep\nlearning (DL) based approaches to automate the program repair process in a\ndata-driven manner. However, their performance is often limited by a fixed set\nof parameters to model the highly complex search space of APR. To ease such\nburden on the parametric models, in this work, we propose a novel\nRetrieval-Augmented Patch Generation framework (RAP-Gen) by explicitly\nleveraging relevant fix patterns retrieved from a codebase of previous bug-fix\npairs. Specifically, we build a hybrid patch retriever to account for both\nlexical and semantic matching based on the raw source code in a\nlanguage-agnostic manner, which does not rely on any code-specific features. In\naddition, we adapt a code-aware language model CodeT5 as our foundation model\nto facilitate both patch retrieval and generation tasks in a unified manner. We\nadopt a stage-wise approach where the patch retriever first retrieves a\nrelevant external bug-fix pair to augment the buggy input for the CodeT5 patch\ngenerator, which synthesizes a ranked list of repair patch candidates. Notably,\nRAP-Gen is a generic APR framework that can flexibly integrate different patch\nretrievers and generators to repair various types of bugs. We thoroughly\nevaluate RAP-Gen on three benchmarks in two programming languages, including\nthe TFix benchmark in JavaScript, and Code Refinement and Defects4J benchmarks\nin Java, where the bug localization information may or may not be provided.\nExperimental results show that RAP-Gen significantly outperforms previous\nstate-of-the-art approaches on all benchmarks, e.g., repairing 15 more bugs on\n818 Defects4J bugs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weishi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BHASA: A Holistic Southeast Asian Linguistic and Cultural Evaluation Suite for Large Language Models. (arXiv:2309.06085v1 [cs.CL])","link":"http://arxiv.org/abs/2309.06085","description":"<p>The rapid development of Large Language Models (LLMs) and the emergence of\nnovel abilities with scale have necessitated the construction of holistic,\ndiverse and challenging benchmarks such as HELM and BIG-bench. However, at the\nmoment, most of these benchmarks focus only on performance in English and\nevaluations that include Southeast Asian (SEA) languages are few in number. We\ntherefore propose BHASA, a holistic linguistic and cultural evaluation suite\nfor LLMs in SEA languages. It comprises three components: (1) a NLP benchmark\ncovering eight tasks across Natural Language Understanding (NLU), Generation\n(NLG) and Reasoning (NLR) tasks, (2) LINDSEA, a linguistic diagnostic toolkit\nthat spans the gamut of linguistic phenomena including syntax, semantics and\npragmatics, and (3) a cultural diagnostics dataset that probes for both\ncultural representation and sensitivity. For this preliminary effort, we\nimplement the NLP benchmark only for Indonesian, Vietnamese, Thai and Tamil,\nand we only include Indonesian and Tamil for LINDSEA and the cultural\ndiagnostics dataset. As GPT-4 is purportedly one of the best-performing\nmultilingual LLMs at the moment, we use it as a yardstick to gauge the\ncapabilities of LLMs in the context of SEA languages. Our initial experiments\non GPT-4 with BHASA find it lacking in various aspects of linguistic\ncapabilities, cultural representation and sensitivity in the targeted SEA\nlanguages. BHASA is a work in progress and will continue to be improved and\nexpanded in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leong_W/0/1/0/all/0/1\">Wei Qi Leong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngui_J/0/1/0/all/0/1\">Jian Gang Ngui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susanto_Y/0/1/0/all/0/1\">Yosephine Susanto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rengarajan_H/0/1/0/all/0/1\">Hamsawardhini Rengarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarveswaran_K/0/1/0/all/0/1\">Kengatharaiyer Sarveswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tjhi_W/0/1/0/all/0/1\">William Chandra Tjhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms: Exploring Tuning Strategies. (arXiv:2309.06089v1 [cs.CL])","link":"http://arxiv.org/abs/2309.06089","description":"<p>The cross-lingual transfer is a promising technique to solve tasks in\nless-resourced languages. In this empirical study, we compare two fine-tuning\napproaches combined with zero-shot and full-shot learning approaches for large\nlanguage models in a cross-lingual setting. As fine-tuning strategies, we\ncompare parameter-efficient adapter methods with fine-tuning of all parameters.\nAs cross-lingual transfer strategies, we compare the intermediate-training\n(\\textit{IT}) that uses each language sequentially and cross-lingual validation\n(\\textit{CLV}) that uses a target language already in the validation phase of\nfine-tuning. We assess the success of transfer and the extent of catastrophic\nforgetting in a source language due to cross-lingual transfer, i.e., how much\npreviously acquired knowledge is lost when we learn new information in a\ndifferent language. The results on two different classification problems, hate\nspeech detection and product reviews, each containing datasets in several\nlanguages, show that the \\textit{IT} cross-lingual strategy outperforms\n\\textit{CLV} for the target language. Our findings indicate that, in the\nmajority of cases, the \\textit{CLV} strategy demonstrates superior retention of\nknowledge in the base language (English) compared to the \\textit{IT} strategy,\nwhen evaluating catastrophic forgetting in multiple cross-lingual transfers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koloski_B/0/1/0/all/0/1\">Boshko Koloski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skrlj_B/0/1/0/all/0/1\">Bla&#x17e; &#x160;krlj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robnik_Sikonja_M/0/1/0/all/0/1\">Marko Robnik-&#x160;ikonja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollak_S/0/1/0/all/0/1\">Senja Pollak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Visual Taxonomy Expansion. (arXiv:2309.06105v1 [cs.CV])","link":"http://arxiv.org/abs/2309.06105","description":"<p>Taxonomy expansion task is essential in organizing the ever-increasing volume\nof new concepts into existing taxonomies. Most existing methods focus\nexclusively on using textual semantics, leading to an inability to generalize\nto unseen terms and the \"Prototypical Hypernym Problem.\" In this paper, we\npropose Visual Taxonomy Expansion (VTE), introducing visual features into the\ntaxonomy expansion task. We propose a textual hypernymy learning task and a\nvisual prototype learning task to cluster textual and visual semantics. In\naddition to the tasks on respective modalities, we introduce a hyper-proto\nconstraint that integrates textual and visual semantics to produce fine-grained\nvisual semantics. Our method is evaluated on two datasets, where we obtain\ncompelling results. Specifically, on the Chinese taxonomy dataset, our method\nsignificantly improves accuracy by 8.75 %. Additionally, our approach performs\nbetter than ChatGPT on the Chinese taxonomy dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tinghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiaqing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zongyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Rui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xian_Y/0/1/0/all/0/1\">Yunsen Xian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterizing Latent Perspectives of Media Houses Towards Public Figures. (arXiv:2309.06112v1 [cs.CL])","link":"http://arxiv.org/abs/2309.06112","description":"<p>Media houses reporting on public figures, often come with their own biases\nstemming from their respective worldviews. A characterization of these\nunderlying patterns helps us in better understanding and interpreting news\nstories. For this, we need diverse or subjective summarizations, which may not\nbe amenable for classifying into predefined class labels. This work proposes a\nzero-shot approach for non-extractive or generative characterizations of person\nentities from a corpus using GPT-2. We use well-articulated articles from\nseveral well-known news media houses as a corpus to build a sound argument for\nthis approach. First, we fine-tune a GPT-2 pre-trained language model with a\ncorpus where specific person entities are characterized. Second, we further\nfine-tune this with demonstrations of person entity characterizations, created\nfrom a corpus of programmatically constructed characterizations. This twice\nfine-tuned model is primed with manual prompts consisting of entity names that\nwere not previously encountered in the second fine-tuning, to generate a simple\nsentence about the entity. The results were encouraging, when compared against\nactual characterizations from the corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivatsa_S/0/1/0/all/0/1\">Sharath Srivatsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasa_S/0/1/0/all/0/1\">Srinath Srinivasa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AstroLLaMA: Towards Specialized Foundation Models in Astronomy. (arXiv:2309.06126v1 [astro-ph.IM])","link":"http://arxiv.org/abs/2309.06126","description":"<p>Large language models excel in many human-language tasks but often falter in\nhighly specialized domains like scholarly astronomy. To bridge this gap, we\nintroduce AstroLLaMA, a 7-billion-parameter model fine-tuned from LLaMA-2 using\nover 300,000 astronomy abstracts from arXiv. Optimized for traditional causal\nlanguage modeling, AstroLLaMA achieves a 30% lower perplexity than Llama-2,\nshowing marked domain adaptation. Our model generates more insightful and\nscientifically relevant text completions and embedding extraction than\nstate-of-the-arts foundation models despite having significantly fewer\nparameters. AstroLLaMA serves as a robust, domain-specific model with broad\nfine-tuning potential. Its public release aims to spur astronomy-focused\nresearch, including automatic paper summarization and conversational agent\ndevelopment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan Dung Nguyen</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Ting_Y/0/1/0/all/0/1\">Yuan-Sen Ting</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Ciuca_I/0/1/0/all/0/1\">Ioana Ciuc&#x103;</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+ONeill_C/0/1/0/all/0/1\">Charlie O&#x27;Neill</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Sun_Z/0/1/0/all/0/1\">Ze-Chang Sun</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Jablonska_M/0/1/0/all/0/1\">Maja Jab&#x142;o&#x144;ska</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Kruk_S/0/1/0/all/0/1\">Sandor Kruk</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Perkowski_E/0/1/0/all/0/1\">Ernest Perkowski</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Miller_J/0/1/0/all/0/1\">Jack Miller</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Li_J/0/1/0/all/0/1\">Jason Li</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Peek_J/0/1/0/all/0/1\">Josh Peek</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Iyer_K/0/1/0/all/0/1\">Kartheik Iyer</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Rozanski_T/0/1/0/all/0/1\">Tomasz R&#xf3;&#x17c;a&#x144;ski</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Khetarpal_P/0/1/0/all/0/1\">Pranav Khetarpal</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Zaman_S/0/1/0/all/0/1\">Sharaf Zaman</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Brodrick_D/0/1/0/all/0/1\">David Brodrick</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Mendez_S/0/1/0/all/0/1\">Sergio J. Rodr&#xed;guez M&#xe9;ndez</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Bui_T/0/1/0/all/0/1\">Thang Bui</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Goodman_A/0/1/0/all/0/1\">Alyssa Goodman</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Accomazzi_A/0/1/0/all/0/1\">Alberto Accomazzi</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Naiman_J/0/1/0/all/0/1\">Jill Naiman</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Cranney_J/0/1/0/all/0/1\">Jesse Cranney</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Schawinski_K/0/1/0/all/0/1\">Kevin Schawinski</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+UniverseTBD/0/1/0/all/0/1\">UniverseTBD</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Annotating Data for Fine-Tuning a Neural Ranker? Current Active Learning Strategies are not Better than Random Selection. (arXiv:2309.06131v1 [cs.IR])","link":"http://arxiv.org/abs/2309.06131","description":"<p>Search methods based on Pretrained Language Models (PLM) have demonstrated\ngreat effectiveness gains compared to statistical and early neural ranking\nmodels. However, fine-tuning PLM-based rankers requires a great amount of\nannotated training data. Annotating data involves a large manual effort and\nthus is expensive, especially in domain specific tasks. In this paper we\ninvestigate fine-tuning PLM-based rankers under limited training data and\nbudget. We investigate two scenarios: fine-tuning a ranker from scratch, and\ndomain adaptation starting with a ranker already fine-tuned on general data,\nand continuing fine-tuning on a target dataset. We observe a great variability\nin effectiveness when fine-tuning on different randomly selected subsets of\ntraining data. This suggests that it is possible to achieve effectiveness gains\nby actively selecting a subset of the training data that has the most positive\neffect on the rankers. This way, it would be possible to fine-tune effective\nPLM rankers at a reduced annotation budget. To investigate this, we adapt\nexisting Active Learning (AL) strategies to the task of fine-tuning PLM rankers\nand investigate their effectiveness, also considering annotation and\ncomputational costs. Our extensive analysis shows that AL strategies do not\nsignificantly outperform random selection of training subsets in terms of\neffectiveness. We further find that gains provided by AL strategies come at the\nexpense of more assessments (thus higher annotation costs) and AL strategies\nunderperform random selection when comparing effectiveness given a fixed\nannotation cost. Our results highlight that ``optimal'' subsets of training\ndata that provide high effectiveness at low annotation cost do exist, but\ncurrent mainstream AL strategies applied to PLM rankers are not capable of\nidentifying them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Althammer_S/0/1/0/all/0/1\">Sophia Althammer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccon_G/0/1/0/all/0/1\">Guido Zuccon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofstatter_S/0/1/0/all/0/1\">Sebastian Hofst&#xe4;tter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verberne_S/0/1/0/all/0/1\">Suzan Verberne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1\">Allan Hanbury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring vagueness and subjectivity in texts: from symbolic to neural VAGO. (arXiv:2309.06132v1 [cs.CL])","link":"http://arxiv.org/abs/2309.06132","description":"<p>We present a hybrid approach to the automated measurement of vagueness and\nsubjectivity in texts. We first introduce the expert system VAGO, we illustrate\nit on a small benchmark of fact vs. opinion sentences, and then test it on the\nlarger French press corpus FreSaDa to confirm the higher prevalence of\nsubjective markers in satirical vs. regular texts. We then build a neural clone\nof VAGO, based on a BERT-like architecture, trained on the symbolic VAGO scores\nobtained on FreSaDa. Using explainability tools (LIME), we show the interest of\nthis neural version for the enrichment of the lexicons of the symbolic version,\nand for the production of versions in other languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Icard_B/0/1/0/all/0/1\">Benjamin Icard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Claveau_V/0/1/0/all/0/1\">Vincent Claveau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atemezing_G/0/1/0/all/0/1\">Ghislain Atemezing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egre_P/0/1/0/all/0/1\">Paul &#xc9;gr&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts. (arXiv:2309.06135v1 [cs.CL])","link":"http://arxiv.org/abs/2309.06135","description":"<p>Text-to-image diffusion models, e.g. Stable Diffusion (SD), lately have shown\nremarkable ability in high-quality content generation, and become one of the\nrepresentatives for the recent wave of transformative AI. Nevertheless, such\nadvance comes with an intensifying concern about the misuse of this generative\ntechnology, especially for producing copyrighted or NSFW (i.e. not safe for\nwork) images. Although efforts have been made to filter inappropriate\nimages/prompts or remove undesirable concepts/styles via model fine-tuning, the\nreliability of these safety mechanisms against diversified problematic prompts\nremains largely unexplored. In this work, we propose Prompting4Debugging (P4D)\nas a debugging and red-teaming tool that automatically finds problematic\nprompts for diffusion models to test the reliability of a deployed safety\nmechanism. We demonstrate the efficacy of our P4D tool in uncovering new\nvulnerabilities of SD models with safety mechanisms. Particularly, our result\nshows that around half of prompts in existing safe prompting benchmarks which\nwere originally considered \"safe\" can actually be manipulated to bypass many\ndeployed safety mechanisms, including concept removal, negative prompt, and\nsafety guidance. Our findings suggest that, without comprehensive testing, the\nevaluations on limited safe prompting benchmarks can lead to a false sense of\nsafety for text-to-image models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chin_Z/0/1/0/all/0/1\">Zhi-Yi Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chieh-Ming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Ching-Chun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_W/0/1/0/all/0/1\">Wei-Chen Chiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overview of GUA-SPA at IberLEF 2023: Guarani-Spanish Code Switching Analysis. (arXiv:2309.06163v1 [cs.CL])","link":"http://arxiv.org/abs/2309.06163","description":"<p>We present the first shared task for detecting and analyzing code-switching\nin Guarani and Spanish, GUA-SPA at IberLEF 2023. The challenge consisted of\nthree tasks: identifying the language of a token, NER, and a novel task of\nclassifying the way a Spanish span is used in the code-switched context. We\nannotated a corpus of 1500 texts extracted from news articles and tweets,\naround 25 thousand tokens, with the information for the tasks. Three teams took\npart in the evaluation phase, obtaining in general good results for Task 1, and\nmore mixed results for Tasks 2 and 3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiruzzo_L/0/1/0/all/0/1\">Luis Chiruzzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aguero_Torales_M/0/1/0/all/0/1\">Marvin Ag&#xfc;ero-Torales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimenez_Lugo_G/0/1/0/all/0/1\">Gustavo Gim&#xe9;nez-Lugo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_A/0/1/0/all/0/1\">Aldo Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Y/0/1/0/all/0/1\">Yliana Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gongora_S/0/1/0/all/0/1\">Santiago G&#xf3;ngora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solorio_T/0/1/0/all/0/1\">Thamar Solorio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AKEM: Aligning Knowledge Base to Queries with Ensemble Model for Entity Recognition and Linking. (arXiv:2309.06175v1 [cs.CL])","link":"http://arxiv.org/abs/2309.06175","description":"<p>This paper presents a novel approach to address the Entity Recognition and\nLinking Challenge at NLPCC 2015. The task involves extracting named entity\nmentions from short search queries and linking them to entities within a\nreference Chinese knowledge base. To tackle this problem, we first expand the\nexisting knowledge base and utilize external knowledge to identify candidate\nentities, thereby improving the recall rate. Next, we extract features from the\ncandidate entities and utilize Support Vector Regression and Multiple Additive\nRegression Tree as scoring functions to filter the results. Additionally, we\napply rules to further refine the results and enhance precision. Our method is\ncomputationally efficient and achieves an F1 score of 0.535.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">Di Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhongping Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Caixia Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Glancing Future for Simultaneous Machine Translation. (arXiv:2309.06179v1 [cs.CL])","link":"http://arxiv.org/abs/2309.06179","description":"<p>Simultaneous machine translation (SiMT) outputs translation while reading the\nsource sentence. Unlike conventional sequence-to-sequence (seq2seq) training,\nexisting SiMT methods adopt the prefix-to-prefix (prefix2prefix) training,\nwhere the model predicts target tokens based on partial source tokens. However,\nthe prefix2prefix training diminishes the ability of the model to capture\nglobal information and introduces forced predictions due to the absence of\nessential source information. Consequently, it is crucial to bridge the gap\nbetween the prefix2prefix training and seq2seq training to enhance the\ntranslation capability of the SiMT model. In this paper, we propose a novel\nmethod that glances future in curriculum learning to achieve the transition\nfrom the seq2seq training to prefix2prefix training. Specifically, we gradually\nreduce the available source information from the whole sentence to the prefix\ncorresponding to that latency. Our method is applicable to a wide range of SiMT\nmethods and experiments demonstrate that our method outperforms strong\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shoutao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaolei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving and Evaluating the Detection of Fragmentation in News Recommendations with the Clustering of News Story Chains. (arXiv:2309.06192v1 [cs.CL])","link":"http://arxiv.org/abs/2309.06192","description":"<p>News recommender systems play an increasingly influential role in shaping\ninformation access within democratic societies. However, tailoring\nrecommendations to users' specific interests can result in the divergence of\ninformation streams. Fragmented access to information poses challenges to the\nintegrity of the public sphere, thereby influencing democracy and public\ndiscourse. The Fragmentation metric quantifies the degree of fragmentation of\ninformation streams in news recommendations. Accurate measurement of this\nmetric requires the application of Natural Language Processing (NLP) to\nidentify distinct news events, stories, or timelines. This paper presents an\nextensive investigation of various approaches for quantifying Fragmentation in\nnews recommendations. These approaches are evaluated both intrinsically, by\nmeasuring performance on news story clustering, and extrinsically, by assessing\nthe Fragmentation scores of different simulated news recommender scenarios. Our\nfindings demonstrate that agglomerative hierarchical clustering coupled with\nSentenceBERT text representation is substantially better at detecting\nFragmentation than earlier implementations. Additionally, the analysis of\nsimulated scenarios yields valuable insights and recommendations for\nstakeholders concerning the measurement and interpretation of Fragmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Polimeno_A/0/1/0/all/0/1\">Alessandra Polimeno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reuver_M/0/1/0/all/0/1\">Myrthe Reuver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vrijenhoek_S/0/1/0/all/0/1\">Sanne Vrijenhoek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fokkens_A/0/1/0/all/0/1\">Antske Fokkens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Action Co-occurrence in Lifestyle Vlogs using Graph Link Prediction. (arXiv:2309.06219v1 [cs.CV])","link":"http://arxiv.org/abs/2309.06219","description":"<p>We introduce the task of automatic human action co-occurrence identification,\ni.e., determine whether two human actions can co-occur in the same interval of\ntime. We create and make publicly available the ACE (Action Co-occurrencE)\ndataset, consisting of a large graph of ~12k co-occurring pairs of visual\nactions and their corresponding video clips. We describe graph link prediction\nmodels that leverage visual and textual information to automatically infer if\ntwo actions are co-occurring. We show that graphs are particularly well suited\nto capture relations between human actions, and the learned graph\nrepresentations are effective for our task and capture novel and relevant\ninformation across different data domains. The ACE dataset and the code\nintroduced in this paper are publicly available at\nhttps://github.com/MichiganNLP/vlog_action_co-occurrence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ignat_O/0/1/0/all/0/1\">Oana Ignat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_S/0/1/0/all/0/1\">Santiago Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models. (arXiv:2309.06236v1 [cs.LG])","link":"http://arxiv.org/abs/2309.06236","description":"<p>Large Language Models (LLMs) have demonstrated remarkable generalization\nacross diverse tasks, leading individuals to increasingly use them as personal\nassistants and universal computing engines. Nevertheless, a notable obstacle\nemerges when feeding numerical/temporal data into these models, such as data\nsourced from wearables or electronic health records. LLMs employ tokenizers in\ntheir input that break down text into smaller units. However, tokenizers are\nnot designed to represent numerical values and might struggle to understand\nrepetitive patterns and context, treating consecutive values as separate tokens\nand disregarding their temporal relationships. Here, we discuss recent works\nthat employ LLMs for human-centric tasks such as in mobile health sensing and\npresent a case study showing that popular LLMs tokenize temporal data\nincorrectly. To address that, we highlight potential solutions such as prompt\ntuning with lightweight embedding layers as well as multimodal adapters, that\ncan help bridge this \"modality gap\". While the capability of language models to\ngeneralize to other modalities with minimal or no finetuning is exciting, this\npaper underscores the fact that their outputs cannot be meaningful if they\nstumble over input nuances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Spathis_D/0/1/0/all/0/1\">Dimitris Spathis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawsar_F/0/1/0/all/0/1\">Fahim Kawsar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re-Reading Improves Reasoning in Language Models. (arXiv:2309.06275v1 [cs.CL])","link":"http://arxiv.org/abs/2309.06275","description":"<p>Reasoning presents a significant and challenging issue for Large Language\nModels (LLMs). The predominant focus of research has revolved around developing\ndiverse prompting strategies to guide and structure the reasoning processes of\nLLMs. However, these approaches based on decoder-only causal language models\noften operate the input question in a single forward pass, potentially missing\nthe rich, back-and-forth interactions inherent in human reasoning. Scant\nattention has been paid to a critical dimension, i.e., the input question\nitself embedded within the prompts. In response, we introduce a deceptively\nsimple yet highly effective prompting strategy, termed question \"re-reading\".\nDrawing inspiration from human learning and problem-solving, re-reading entails\nrevisiting the question information embedded within input prompts. This\napproach aligns seamlessly with the cognitive principle of reinforcement,\nenabling LLMs to extract deeper insights, identify intricate patterns,\nestablish more nuanced connections, and ultimately enhance their reasoning\ncapabilities across various tasks. Experiments conducted on a series of\nreasoning benchmarks serve to underscore the effectiveness and generality of\nour method. Moreover, our findings demonstrate that our approach seamlessly\nintegrates with various language models, though-eliciting prompting methods,\nand ensemble techniques, further underscoring its versatility and compatibility\nin the realm of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaohan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongbo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-guang Lou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Data Augmentation using LLMs improves Distributional Robustness in Question Answering. (arXiv:2309.06358v1 [cs.CL])","link":"http://arxiv.org/abs/2309.06358","description":"<p>Robustness in Natural Language Processing continues to be a pertinent issue,\nwhere state of the art models under-perform under naturally shifted\ndistributions. In the context of Question Answering, work on domain adaptation\nmethods continues to be a growing body of research. However, very little\nattention has been given to the notion of domain generalization under natural\ndistribution shifts, where the target domain is unknown. With drastic\nimprovements in the quality and access to generative models, we answer the\nquestion: How do generated datasets influence the performance of QA models\nunder natural distribution shifts? We perform experiments on 4 different\ndatasets under varying amounts of distribution shift, and analyze how\n\"in-the-wild\" generation can help achieve domain generalization. We take a\ntwo-step generation approach, generating both contexts and QA pairs to augment\nexisting datasets. Through our experiments, we demonstrate how augmenting\nreading comprehension datasets with generated data leads to better robustness\ntowards natural distribution shifts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1\">Arijit Ghosh Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Predict Concept Ordering for Common Sense Generation. (arXiv:2309.06363v1 [cs.CL])","link":"http://arxiv.org/abs/2309.06363","description":"<p>Prior work has shown that the ordering in which concepts are shown to a\ncommonsense generator plays an important role, affecting the quality of the\ngenerated sentence. However, it remains a challenge to determine the optimal\nordering of a given set of concepts such that a natural sentence covering all\nthe concepts could be generated from a pretrained generator. To understand the\nrelationship between the ordering of the input concepts and the quality of the\ngenerated sentences, we conduct a systematic study considering multiple\nlanguage models (LMs) and concept ordering strategies. We find that BART-large\nmodel consistently outperforms all other LMs considered in this study when\nfine-tuned using the ordering of concepts as they appear in CommonGen training\ndata as measured using multiple evaluation metrics. Moreover, the larger\nGPT3-based large language models (LLMs) variants do not necessarily outperform\nmuch smaller LMs on this task, even when fine-tuned on task-specific training\ndata. Interestingly, human annotators significantly reorder input concept sets\nwhen manually writing sentences covering those concepts, and this ordering\nprovides the best sentence generations independently of the LM used for the\ngeneration, outperforming a probabilistic concept ordering baseline\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianhui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Bei Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Framework-Based Qualitative Analysis of Free Responses of Large Language Models: Algorithmic Fidelity. (arXiv:2309.06364v1 [cs.CL])","link":"http://arxiv.org/abs/2309.06364","description":"<p>Today, using Large-scale generative Language Models (LLMs) it is possible to\nsimulate free responses to interview questions like those traditionally\nanalyzed using qualitative research methods. Qualitative methodology\nencompasses a broad family of techniques involving manual analysis of\nopen-ended interviews or conversations conducted freely in natural language.\nHere we consider whether artificial \"silicon participants\" generated by LLMs\nmay be productively studied using qualitative methods aiming to produce\ninsights that could generalize to real human populations. The key concept in\nour analysis is algorithmic fidelity, a term introduced by Argyle et al. (2023)\ncapturing the degree to which LLM-generated outputs mirror human\nsub-populations' beliefs and attitudes. By definition, high algorithmic\nfidelity suggests latent beliefs elicited from LLMs may generalize to real\nhumans, whereas low algorithmic fidelity renders such research invalid. Here we\nused an LLM to generate interviews with silicon participants matching specific\ndemographic characteristics one-for-one with a set of human participants. Using\nframework-based qualitative analysis, we showed the key themes obtained from\nboth human and silicon participants were strikingly similar. However, when we\nanalyzed the structure and tone of the interviews we found even more striking\ndifferences. We also found evidence of the hyper-accuracy distortion described\nby Aher et al. (2023). We conclude that the LLM we tested (GPT-3.5) does not\nhave sufficient algorithmic fidelity to expect research on it to generalize to\nhuman populations. However, the rapid pace of LLM research makes it plausible\nthis could change in the future. Thus we stress the need to establish epistemic\nnorms now around how to assess validity of LLM-based qualitative research,\nespecially concerning the need to ensure representation of heterogeneous lived\nexperiences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amirova_A/0/1/0/all/0/1\">Aliya Amirova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fteropoulli_T/0/1/0/all/0/1\">Theodora Fteropoulli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1\">Nafiso Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cowie_M/0/1/0/all/0/1\">Martin R. Cowie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leibo_J/0/1/0/all/0/1\">Joel Z. Leibo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cited Text Spans for Citation Text Generation. (arXiv:2309.06365v1 [cs.CL])","link":"http://arxiv.org/abs/2309.06365","description":"<p>Automatic related work generation must ground their outputs to the content of\nthe cited papers to avoid non-factual hallucinations, but due to the length of\nscientific documents, existing abstractive approaches have conditioned only on\nthe cited paper \\textit{abstracts}. We demonstrate that the abstract is not\nalways the most appropriate input for citation generation and that models\ntrained in this way learn to hallucinate. We propose to condition instead on\nthe \\textit{cited text span} (CTS) as an alternative to the abstract. Because\nmanual CTS annotation is extremely time- and labor-intensive, we experiment\nwith automatic, ROUGE-based labeling of candidate CTS sentences, achieving\nsufficiently strong performance to substitute for expensive human annotations,\nand we propose a human-in-the-loop, keyword-based CTS retrieval approach that\nmakes generating citation texts grounded in the full text of cited papers both\npromising and practical.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangci Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yi-Hui Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_J/0/1/0/all/0/1\">Jessica Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Reliable and Fluent Large Language Models: Incorporating Feedback Learning Loops in QA Systems. (arXiv:2309.06384v1 [cs.CL])","link":"http://arxiv.org/abs/2309.06384","description":"<p>Large language models (LLMs) have emerged as versatile tools in various daily\napplications. However, they are fraught with issues that undermine their\nutility and trustworthiness. These include the incorporation of erroneous\nreferences (citation), the generation of hallucinated information\n(correctness), and the inclusion of superfluous or omission of crucial details\n(fluency). To ameliorate these concerns, this study makes several key\ncontributions. First, we build a dataset to train a critic model capable of\nevaluating the citation, correctness, and fluency of responses generated by\nLLMs in QA systems. Second, we propose an automated feedback mechanism that\nleverages the critic model to offer real-time feedback on heterogeneous aspects\nof generated text. Third, we introduce a feedback learning loop that uses this\ncritic model to iteratively improve the performance of the LLM responsible for\nresponse generation. Experimental results demonstrate the efficacy of our\napproach, showing substantial improvements in citation and fluency metrics for\nChatGPT, including a 4% precision increase in citation and an approximately 8%\nenhancement in the MAUVE metric for fluency, while maintaining high levels of\ncorrectness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongyub Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whang_T/0/1/0/all/0/1\">Taesun Whang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chanhee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Heuiseok Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Down the Toxicity Rabbit Hole: Investigating PaLM 2 Guardrails. (arXiv:2309.06415v1 [cs.CL])","link":"http://arxiv.org/abs/2309.06415","description":"<p>This paper conducts a robustness audit of the safety feedback of PaLM 2\nthrough a novel toxicity rabbit hole framework introduced here. Starting with a\nstereotype, the framework instructs PaLM 2 to generate more toxic content than\nthe stereotype. Every subsequent iteration it continues instructing PaLM 2 to\ngenerate more toxic content than the previous iteration until PaLM 2 safety\nguardrails throw a safety violation. Our experiments uncover highly disturbing\nantisemitic, Islamophobic, racist, homophobic, and misogynistic (to list a few)\ngenerated content that PaLM 2 safety guardrails do not evaluate as highly\nunsafe.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khorramrouz_A/0/1/0/all/0/1\">Adel Khorramrouz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Sujan Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1\">Arka Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+KhudaBukhsh_A/0/1/0/all/0/1\">Ashiqur R. KhudaBukhsh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Radiology-Llama2: Best-in-Class Large Language Model for Radiology. (arXiv:2309.06419v1 [cs.CL])","link":"http://arxiv.org/abs/2309.06419","description":"<p>This paper introduces Radiology-Llama2, a large language model specialized\nfor radiology through a process known as instruction tuning. Radiology-Llama2\nis based on the Llama2 architecture and further trained on a large dataset of\nradiology reports to generate coherent and clinically useful impressions from\nradiological findings. Quantitative evaluations using ROUGE metrics on the\nMIMIC-CXR and OpenI datasets demonstrate that Radiology-Llama2 achieves\nstate-of-the-art performance compared to other generative language models, with\na Rouge-1 score of 0.4834 on MIMIC-CXR and 0.4185 on OpenI. Additional\nassessments by radiology experts highlight the model's strengths in\nunderstandability, coherence, relevance, conciseness, and clinical utility. The\nwork illustrates the potential of localized language models designed and tuned\nfor specialized domains like radiology. When properly evaluated and deployed,\nsuch models can transform fields like radiology by automating rote tasks and\nenhancing human expertise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_P/0/1/0/all/0/1\">Peng Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_A/0/1/0/all/0/1\">Aoxiao Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Longtao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1\">Chao Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sekeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Haixing Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dajiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Quanzheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Think Before You Speak: Explicitly Generating Implicit Commonsense Knowledge for Response Generation. (arXiv:2110.08501v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08501","description":"<p>Implicit knowledge, such as common sense, is key to fluid human\nconversations. Current neural response generation (RG) models are trained to\ngenerate responses directly, omitting unstated implicit knowledge. In this\npaper, we present Think-Before-Speaking (TBS), a generative approach to first\nexternalize implicit commonsense knowledge (think) and use this knowledge to\ngenerate responses (speak). We expect that externalizing implicit knowledge\nallows more efficient learning, produces more informative responses, and\nenables more explainable models. We analyze different choices to collect\nknowledge-aligned dialogues, represent implicit knowledge, and transition\nbetween knowledge and dialogues. Empirical results show TBS models outperform\nend-to-end and knowledge-augmented RG baselines on most automatic metrics and\ngenerate more informative, specific, and commonsense-following responses, as\nevaluated by human annotators. TBS also generates knowledge that makes sense\nand is relevant to the dialogue around 85\\% of the time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Karthik Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedayatnia_B/0/1/0/all/0/1\">Behnam Hedayatnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Testing the limits of natural language models for predicting human language judgments. (arXiv:2204.03592v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.03592","description":"<p>Neural network language models can serve as computational hypotheses about\nhow humans process language. We compared the model-human consistency of diverse\nlanguage models using a novel experimental approach: controversial sentence\npairs. For each controversial sentence pair, two language models disagree about\nwhich sentence is more likely to occur in natural text. Considering nine\nlanguage models (including n-gram, recurrent neural networks, and transformer\nmodels), we created hundreds of such controversial sentence pairs by either\nselecting sentences from a corpus or synthetically optimizing sentence pairs to\nbe highly controversial. Human subjects then provided judgments indicating for\neach pair which of the two sentences is more likely. Controversial sentence\npairs proved highly effective at revealing model failures and identifying\nmodels that aligned most closely with human judgments. The most\nhuman-consistent model tested was GPT-2, although experiments also revealed\nsignificant shortcomings of its alignment with human perception.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Golan_T/0/1/0/all/0/1\">Tal Golan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siegelman_M/0/1/0/all/0/1\">Matthew Siegelman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kriegeskorte_N/0/1/0/all/0/1\">Nikolaus Kriegeskorte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldassano_C/0/1/0/all/0/1\">Christopher Baldassano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Select from Multiple Options. (arXiv:2212.00301v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.00301","description":"<p>Many NLP tasks can be regarded as a selection problem from a set of options,\nsuch as classification tasks, multi-choice question answering, etc. Textual\nentailment (TE) has been shown as the state-of-the-art (SOTA) approach to\ndealing with those selection problems. TE treats input texts as premises (P),\noptions as hypotheses (H), then handles the selection problem by modeling (P,\nH) pairwise. Two limitations: first, the pairwise modeling is unaware of other\noptions, which is less intuitive since humans often determine the best options\nby comparing competing candidates; second, the inference process of pairwise TE\nis time-consuming, especially when the option space is large. To deal with the\ntwo issues, this work first proposes a contextualized TE model (Context-TE) by\nappending other k options as the context of the current (P, H) modeling.\nContext-TE is able to learn more reliable decision for the H since it considers\nvarious context. Second, we speed up Context-TE by coming up with Parallel-TE,\nwhich learns the decisions of multiple options simultaneously. Parallel-TE\nsignificantly improves the inference speed while keeping comparable performance\nwith Context-TE. Our methods are evaluated on three tasks (ultra-fine entity\ntyping, intent detection and multi-choice QA) that are typical selection\nproblems with different sizes of options. Experiments show our models set new\nSOTA performance; particularly, Parallel-TE is faster than the pairwise TE by k\ntimes in inference. Our code is publicly available at\nhttps://github.com/jiangshdd/LearningToSelect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jiangshu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenpeng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Congying Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-world Story Generation with Structured Knowledge Enhancement: A Comprehensive Survey. (arXiv:2212.04634v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.04634","description":"<p>Storytelling and narrative are fundamental to human experience, intertwined\nwith our social and cultural engagement. As such, researchers have long\nattempted to create systems that can generate stories automatically. In recent\nyears, powered by deep learning and massive data resources, automatic story\ngeneration has shown significant advances. However, considerable challenges,\nlike the need for global coherence in generated stories, still hamper\ngenerative models from reaching the same storytelling ability as human\nnarrators. To tackle these challenges, many studies seek to inject structured\nknowledge into the generation process, which is referred to as structured\nknowledge-enhanced story generation. Incorporating external knowledge can\nenhance the logical coherence among story events, achieve better knowledge\ngrounding, and alleviate over-generalization and repetition problems in\nstories. This survey provides the latest and comprehensive review of this\nresearch field: (i) we present a systematic taxonomy regarding how existing\nmethods integrate structured knowledge into story generation; (ii) we summarize\ninvolved story corpora, structured knowledge datasets, and evaluation metrics;\n(iii) we give multidimensional insights into the challenges of\nknowledge-enhanced story generation and cast light on promising directions for\nfuture study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jieru Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_B/0/1/0/all/0/1\">B&#xf6;rje F. Karlsson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning. (arXiv:2212.07919v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.07919","description":"<p>Large language models show improved downstream task performance when prompted\nto generate step-by-step reasoning to justify their final answers. These\nreasoning steps greatly improve model interpretability and verification, but\nobjectively studying their correctness (independent of the final answer) is\ndifficult without reliable methods for automatic evaluation. We simply do not\nknow how often the stated reasoning steps actually support the final end task\npredictions. In this work, we present ROSCOE, a suite of interpretable,\nunsupervised automatic scores that improve and extend previous text generation\nevaluation metrics. To evaluate ROSCOE against baseline metrics, we design a\ntypology of reasoning errors and collect synthetic and human evaluation scores\non commonly used reasoning datasets. In contrast with existing metrics, ROSCOE\ncan measure semantic consistency, logicality, informativeness, fluency, and\nfactuality - among other traits - by leveraging properties of step-by-step\nrationales. We empirically verify the strength of our metrics on five human\nannotated and six programmatically perturbed diagnostics datasets - covering a\ndiverse set of tasks that require reasoning skills and show that ROSCOE can\nconsistently outperform baseline metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Golovneva_O/0/1/0/all/0/1\">Olga Golovneva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Moya Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poff_S/0/1/0/all/0/1\">Spencer Poff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corredor_M/0/1/0/all/0/1\">Martin Corredor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazel_Zarandi_M/0/1/0/all/0/1\">Maryam Fazel-Zarandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting Multilingual Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages. (arXiv:2303.13592v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.13592","description":"<p>While code-mixing is a common linguistic practice in many parts of the world,\ncollecting high-quality and low-cost code-mixed data remains a challenge for\nnatural language processing (NLP) research. The recent proliferation of Large\nLanguage Models (LLMs) compels one to ask: how capable are these systems in\ngenerating code-mixed data? In this paper, we explore prompting multilingual\nLLMs in a zero-shot manner to generate code-mixed data for seven languages in\nSouth East Asia (SEA), namely Indonesian, Malay, Chinese, Tagalog, Vietnamese,\nTamil, and Singlish. We find that publicly available multilingual\ninstruction-tuned models such as BLOOMZ and Flan-T5-XXL are incapable of\nproducing texts with phrases or clauses from different languages. ChatGPT\nexhibits inconsistent capabilities in generating code-mixed texts, wherein its\nperformance varies depending on the prompt template and language pairing. For\ninstance, ChatGPT generates fluent and natural Singlish texts (an English-based\ncreole spoken in Singapore), but for English-Tamil language pair, the system\nmostly produces grammatically incorrect or semantically meaningless utterances.\nFurthermore, it may erroneously introduce languages not specified in the\nprompt. Based on our investigation, existing multilingual LLMs exhibit a wide\nrange of proficiency in code-mixed data generation for SEA languages. As such,\nwe advise against using LLMs in this context without extensive human checks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yong_Z/0/1/0/all/0/1\">Zheng-Xin Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruochen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forde_J/0/1/0/all/0/1\">Jessica Zosa Forde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Skyler Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramonian_A/0/1/0/all/0/1\">Arjun Subramonian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutawika_L/0/1/0/all/0/1\">Lintang Sutawika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_J/0/1/0/all/0/1\">Jan Christian Blaise Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yin Lin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_L/0/1/0/all/0/1\">Long Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_R/0/1/0/all/0/1\">Rowena Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solorio_T/0/1/0/all/0/1\">Thamar Solorio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Galactic ChitChat: Using Large Language Models to Converse with Astronomy Literature. (arXiv:2304.05406v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.05406","description":"<p>We demonstrate the potential of the state-of-the-art OpenAI GPT-4 large\nlanguage model to engage in meaningful interactions with Astronomy papers using\nin-context prompting. To optimize for efficiency, we employ a distillation\ntechnique that effectively reduces the size of the original input paper by\n50\\%, while maintaining the paragraph structure and overall semantic integrity.\nWe then explore the model's responses using a multi-document context (ten\ndistilled documents). Our findings indicate that GPT-4 excels in the\nmulti-document domain, providing detailed answers contextualized within the\nframework of related research findings. Our results showcase the potential of\nlarge language models for the astronomical community, offering a promising\navenue for further exploration, particularly the possibility of utilizing the\nmodels for hypothesis generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ciuca_I/0/1/0/all/0/1\">Ioana Ciuc&#x103;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ting_Y/0/1/0/all/0/1\">Yuan-Sen Ting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PaLM 2 Technical Report. (arXiv:2305.10403v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10403","description":"<p>We introduce PaLM 2, a new state-of-the-art language model that has better\nmultilingual and reasoning capabilities and is more compute-efficient than its\npredecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture\nof objectives. Through extensive evaluations on English and multilingual\nlanguage, and reasoning tasks, we demonstrate that PaLM 2 has significantly\nimproved quality on downstream tasks across different model sizes, while\nsimultaneously exhibiting faster and more efficient inference compared to PaLM.\nThis improved efficiency enables broader deployment while also allowing the\nmodel to respond faster, for a more natural pace of interaction. PaLM 2\ndemonstrates robust reasoning capabilities exemplified by large improvements\nover PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable\nperformance on a suite of responsible AI evaluations, and enables\ninference-time control over toxicity without additional overhead or impact on\nother capabilities. Overall, PaLM 2 achieves state-of-the-art performance\nacross a diverse set of tasks and capabilities.\n</p>\n<p>When discussing the PaLM 2 family, it is important to distinguish between\npre-trained models (of various sizes), fine-tuned variants of these models, and\nthe user-facing products that use these models. In particular, user-facing\nproducts typically include additional pre- and post-processing steps.\nAdditionally, the underlying models may evolve over time. Therefore, one should\nnot expect the performance of user-facing products to exactly match the results\nreported in this report.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Google/0/1/0/all/0/1\">Google</a>, : , <a href=\"http://arxiv.org/find/cs/1/au:+Anil_R/0/1/0/all/0/1\">Rohan Anil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew M. Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Melvin Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepikhin_D/0/1/0/all/0/1\">Dmitry Lepikhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passos_A/0/1/0/all/0/1\">Alexandre Passos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakeri_S/0/1/0/all/0/1\">Siamak Shakeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taropa_E/0/1/0/all/0/1\">Emanuel Taropa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailey_P/0/1/0/all/0/1\">Paige Bailey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_E/0/1/0/all/0/1\">Eric Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jonathan H. Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafey_L/0/1/0/all/0/1\">Laurent El Shafey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meier_Hellstern_K/0/1/0/all/0/1\">Kathy Meier-Hellstern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_G/0/1/0/all/0/1\">Gaurav Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreira_E/0/1/0/all/0/1\">Erica Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omernick_M/0/1/0/all/0/1\">Mark Omernick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_K/0/1/0/all/0/1\">Kevin Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1\">Kefan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanzhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yujing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abrego_G/0/1/0/all/0/1\">Gustavo Hernandez Abrego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_J/0/1/0/all/0/1\">Junwhan Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1\">Jacob Austin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barham_P/0/1/0/all/0/1\">Paul Barham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botha_J/0/1/0/all/0/1\">Jan Botha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradbury_J/0/1/0/all/0/1\">James Bradbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahma_S/0/1/0/all/0/1\">Siddhartha Brahma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brooks_K/0/1/0/all/0/1\">Kevin Brooks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catasta_M/0/1/0/all/0/1\">Michele Catasta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherry_C/0/1/0/all/0/1\">Colin Cherry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choquette_Choo_C/0/1/0/all/0/1\">Christopher A. Choquette-Choo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhery_A/0/1/0/all/0/1\">Aakanksha Chowdhery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crepy_C/0/1/0/all/0/1\">Cl&#xe9;ment Crepy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dave_S/0/1/0/all/0/1\">Shachi Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devlin_J/0/1/0/all/0/1\">Jacob Devlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_M/0/1/0/all/0/1\">Mark D&#xed;az</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1\">Nan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dyer_E/0/1/0/all/0/1\">Ethan Dyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feinberg_V/0/1/0/all/0/1\">Vlad Feinberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fangxiaoyu Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fienber_V/0/1/0/all/0/1\">Vlad Fienber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1\">Markus Freitag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>, et al. (77 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Separation based on Contrastive Learning and Deep Modularization. (arXiv:2305.10652v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2305.10652","description":"<p>The current monaural state of the art tools for speech separation relies on\nsupervised learning. This means that they must deal with permutation problem,\nthey are impacted by the mismatch on the number of speakers used in training\nand inference. Moreover, their performance heavily relies on the presence of\nhigh-quality labelled data. These problems can be effectively addressed by\nemploying a fully unsupervised technique for speech separation. In this paper,\nwe use contrastive learning to establish the representations of frames then use\nthe learned representations in the downstream deep modularization task.\nConcretely, we demonstrate experimentally that in speech separation, different\nframes of a speaker can be viewed as augmentations of a given hidden standard\nframe of that speaker. The frames of a speaker contain enough prosodic\ninformation overlap which is key in speech separation. Based on this, we\nimplement a self-supervised learning to learn to minimize the distance between\nframes belonging to a given speaker. The learned representations are used in a\ndownstream deep modularization task to cluster frames based on speaker\nidentity. Evaluation of the developed technique on WSJ0-2mix and WSJ0-3mix\nshows that the technique attains SI-SNRi and SDRi of 20.8 and 21.0 respectively\nin WSJ0-2mix. In WSJ0-3mix, it attains SI-SNRi and SDRi of 20.7 and 20.7\nrespectively in WSJ0-2mix. Its greatest strength being that as the number of\nspeakers increase, its performance does not degrade significantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ochieng_P/0/1/0/all/0/1\">Peter Ochieng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ModuleFormer: Modularity Emerges from Mixture-of-Experts. (arXiv:2306.04640v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.04640","description":"<p>Large Language Models (LLMs) have achieved remarkable results. However,\nexisting models are expensive to train and deploy, and it is also difficult to\nexpand their knowledge beyond pre-training data without forgetting previous\nknowledge. This paper proposes a new neural network architecture, ModuleFormer,\nthat leverages modularity to improve the efficiency and flexibility of large\nlanguage models. ModuleFormer is based on the Sparse Mixture of Experts (SMoE).\nUnlike the previous SMoE-based modular language model, which requires\ndomain-labeled data to learn domain-specific experts, ModuleFormer can induce\nmodularity from uncurated data with its new load balancing and concentration\nlosses. ModuleFormer is a modular architecture that includes two different\ntypes of modules: new stick-breaking attention heads and feedforward experts.\nDifferent modules are sparsely activated conditions on the input token during\ntraining and inference. In our experiment, we found that the modular\narchitecture enables three important abilities for large pre-trained language\nmodels: 1) Efficiency, since ModuleFormer only activates a subset of its\nmodules for each input token, thus it could achieve the same performance as\ndense LLMs with more than two times throughput; 2) Extendability, ModuleFormer\nis more immune to catastrophic forgetting than dense LLMs and can be easily\nextended with new modules to learn new knowledge that is not included in the\ntraining data; 3) Specialisation, finetuning ModuleFormer could specialize a\nsubset of modules to the finetuning task and the task-unrelated modules could\nbe easily pruned for a lightweight deployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yikang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyou Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shawn Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenfang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards training Bilingual and Code-Switched Speech Recognition models from Monolingual data sources. (arXiv:2306.08753v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2306.08753","description":"<p>Multilingual Automatic Speech Recognition (ASR) models are capable of\ntranscribing audios across multiple languages, eliminating the need for\nseparate models. In addition, they can perform Language Identification (LID)\nand handle code-switched speech. However, training these models requires\nspecial code-switch and multilingual speech corpora which are sparsely\navailable. In this paper, we evaluate different approaches towards training of\nbilingual as well as code-switched ASR models using purely monolingual data\nsources. We introduce the concept of aggregate tokenizers that differs from the\ncurrent prevalent technique of generating LIDs at the boundaries of monolingual\nsamples and produces LID for each emitted token instead. We compare bilingual\nand monolingual model performance, showcase the efficacy of aggregate\ntokenizers, present a synthetic code-switched ASR data generation technique and\ndemonstrate the effectiveness of the proposed code-switched ASR models for the\ntasks of speech recognition and spoken language identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dhawan_K/0/1/0/all/0/1\">Kunal Dhawan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rekesh_D/0/1/0/all/0/1\">Dima Rekesh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ginsburg_B/0/1/0/all/0/1\">Boris Ginsburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modality Multi-Loss Fusion Network. (arXiv:2308.00264v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.00264","description":"<p>In this work we investigate the optimal selection and fusion of features\nacross multiple modalities and combine these in a neural network to improve\nemotion detection. We compare different fusion methods and examine the impact\nof multi-loss training within the multi-modality fusion network, identifying\nuseful findings relating to subnet performance. Our best model achieves\nstate-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and\nCH-SIMS), and outperforms the other methods in most metrics. We have found that\ntraining on multimodal features improves single modality testing and designing\nfusion methods based on dataset annotation schema enhances model performance.\nThese results suggest a roadmap towards an optimized feature selection and\nfusion approach for enhancing emotion detection in neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zehui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Ziwei Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_J/0/1/0/all/0/1\">Jaywon Koo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirschberg_J/0/1/0/all/0/1\">Julia Hirschberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why Do We Need Neuro-symbolic AI to Model Pragmatic Analogies?. (arXiv:2308.01936v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2308.01936","description":"<p>A hallmark of intelligence is the ability to use a familiar domain to make\ninferences about a less familiar domain, known as analogical reasoning. In this\narticle, we delve into the performance of Large Language Models (LLMs) in\ndealing with progressively complex analogies expressed in unstructured text. We\ndiscuss analogies at four distinct levels of complexity: lexical analogies,\nsyntactic analogies, semantic analogies, and pragmatic analogies. As the\nanalogies become more complex, they require increasingly extensive, diverse\nknowledge beyond the textual content, unlikely to be found in the lexical\nco-occurrence statistics that power LLMs. To address this, we discuss the\nnecessity of employing Neuro-symbolic AI techniques that combine statistical\nand symbolic AI, informing the representation of unstructured text to highlight\nand augment relevant content, provide abstraction and guide the mapping\nprocess. Our knowledge-informed approach maintains the efficiency of LLMs while\npreserving the ability to explain analogies for pedagogical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wijesiriwardene_T/0/1/0/all/0/1\">Thilini Wijesiriwardene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalin_V/0/1/0/all/0/1\">Valerie L. Shalin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Amitava Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FonMTL: Towards Multitask Learning for the Fon Language. (arXiv:2308.14280v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.14280","description":"<p>The Fon language, spoken by an average 2 million of people, is a truly\nlow-resourced African language, with a limited online presence, and existing\ndatasets (just to name but a few). Multitask learning is a learning paradigm\nthat aims to improve the generalization capacity of a model by sharing\nknowledge across different but related tasks: this could be prevalent in very\ndata-scarce scenarios. In this paper, we present the first explorative approach\nto multitask learning, for model capabilities enhancement in Natural Language\nProcessing for the Fon language. Specifically, we explore the tasks of Named\nEntity Recognition (NER) and Part of Speech Tagging (POS) for Fon. We leverage\ntwo language model heads as encoders to build shared representations for the\ninputs, and we use linear layers blocks for classification relative to each\ntask. Our results on the NER and POS tasks for Fon, show competitive (or\nbetter) performances compared to several multilingual pretrained language\nmodels finetuned on single tasks. Additionally, we perform a few ablation\nstudies to leverage the efficiency of two different loss combination strategies\nand find out that the equal loss weighting approach works best in our case. Our\ncode is open-sourced at https://github.com/bonaventuredossou/multitask_fon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dossou_B/0/1/0/all/0/1\">Bonaventure F. P. Dossou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houndayi_I/0/1/0/all/0/1\">Iffanice Houndayi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zantou_P/0/1/0/all/0/1\">Pamely Zantou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hacheme_G/0/1/0/all/0/1\">Gilles Hacheme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLaSM: Large Language and Speech Model. (arXiv:2308.15930v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.15930","description":"<p>Multi-modal large language models have garnered significant interest\nrecently. Though, most of the works focus on vision-language multi-modal models\nproviding strong capabilities in following vision-and-language instructions.\nHowever, we claim that speech is also an important modality through which\nhumans interact with the world. Hence, it is crucial for a general-purpose\nassistant to be able to follow multi-modal speech-and-language instructions. In\nthis work, we propose Large Language and Speech Model (LLaSM). LLaSM is an\nend-to-end trained large multi-modal speech-language model with cross-modal\nconversational abilities, capable of following speech-and-language\ninstructions. Our early experiments show that LLaSM demonstrates a more\nconvenient and natural way for humans to interact with artificial intelligence.\nSpecifically, we also release a large Speech Instruction Following dataset\nLLaSM-Audio-Instructions. Code and demo are available at\nhttps://github.com/LinkSoul-AI/LLaSM and\nhttps://huggingface.co/spaces/LinkSoul/LLaSM. The LLaSM-Audio-Instructions\ndataset is available at\nhttps://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1\">Yu Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Siwei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenhao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruihua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Daochen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Q/0/1/0/all/0/1\">Qiqi Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yemin Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Affective Visual Dialog: A Large-Scale Benchmark for Emotional Reasoning Based on Visually Grounded Conversations. (arXiv:2308.16349v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.16349","description":"<p>We introduce Affective Visual Dialog, an emotion explanation and reasoning\ntask as a testbed for research on understanding the formation of emotions in\nvisually grounded conversations. The task involves three skills: (1)\nDialog-based Question Answering (2) Dialog-based Emotion Prediction and (3)\nAffective emotion explanation generation based on the dialog. Our key\ncontribution is the collection of a large-scale dataset, dubbed AffectVisDial,\nconsisting of 50K 10-turn visually grounded dialogs as well as concluding\nemotion attributions and dialog-informed textual emotion explanations,\nresulting in a total of 27,180 working hours. We explain our design decisions\nin collecting the dataset and introduce the questioner and answerer tasks that\nare associated with the participants in the conversation. We train and\ndemonstrate solid Affective Visual Dialog baselines adapted from\nstate-of-the-art models. Remarkably, the responses generated by our models show\npromising emotional reasoning abilities in response to visually grounded\nconversations. Our project page is available at\nhttps://affective-visual-dialog.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haydarov_K/0/1/0/all/0/1\">Kilichbek Haydarov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaoqian Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madasu_A/0/1/0/all/0/1\">Avinash Madasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salem_M/0/1/0/all/0/1\">Mahmoud Salem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li-Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsayed_G/0/1/0/all/0/1\">Gamaleldin Elsayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Resource Hallucination Prevention for Large Language Models. (arXiv:2309.02654v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.02654","description":"<p>The prevalent use of large language models (LLMs) in various domains has\ndrawn attention to the issue of \"hallucination,\" which refers to instances\nwhere LLMs generate factually inaccurate or ungrounded information. Existing\ntechniques for hallucination detection in language assistants rely on intricate\nfuzzy, specific free-language-based chain of thought (CoT) techniques or\nparameter-based methods that suffer from interpretability issues. Additionally,\nthe methods that identify hallucinations post-generation could not prevent\ntheir occurrence and suffer from inconsistent performance due to the influence\nof the instruction format and model style. In this paper, we introduce a novel\npre-detection self-evaluation technique, referred to as SELF-FAMILIARITY, which\nfocuses on evaluating the model's familiarity with the concepts present in the\ninput instruction and withholding the generation of response in case of\nunfamiliar concepts. This approach emulates the human ability to refrain from\nresponding to unfamiliar topics, thus reducing hallucinations. We validate\nSELF-FAMILIARITY across four different large language models, demonstrating\nconsistently superior performance compared to existing techniques. Our findings\npropose a significant shift towards preemptive strategies for hallucination\nmitigation in LLM assistants, promising improvements in reliability,\napplicability, and interpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Junyu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Cao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fenglong Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT Can Solve Mathematical Problems Without a Calculator. (arXiv:2309.03241v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2309.03241","description":"<p>Previous studies have typically assumed that large language models are unable\nto accurately perform arithmetic operations, particularly multiplication of &gt;8\ndigits, and operations involving decimals and fractions, without the use of\ncalculator tools. This paper aims to challenge this misconception. With\nsufficient training data, a 2 billion-parameter language model can accurately\nperform multi-digit arithmetic operations with almost 100% accuracy without\ndata leakage, significantly surpassing GPT-4 (whose multi-digit multiplication\naccuracy is only 4.3%). We also demonstrate that our MathGLM, fine-tuned from\nGLM-10B on a dataset with additional multi-step arithmetic operations and math\nproblems described in text, achieves similar performance to GPT-4 on a\n5,000-samples Chinese math problem test set. Our code and data are public at\nhttps://github.com/THUDM/MathGLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Q/0/1/0/all/0/1\">Qingsong Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhihuan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zehai He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuyi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jinfeng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoDia: A New Dataset for Romanian Dialect Identification from Speech. (arXiv:2309.03378v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.03378","description":"<p>Dialect identification is a critical task in speech processing and language\ntechnology, enhancing various applications such as speech recognition, speaker\nverification, and many others. While most research studies have been dedicated\nto dialect identification in widely spoken languages, limited attention has\nbeen given to dialect identification in low-resource languages, such as\nRomanian. To address this research gap, we introduce RoDia, the first dataset\nfor Romanian dialect identification from speech. The RoDia dataset includes a\nvaried compilation of speech samples from five distinct regions of Romania,\ncovering both urban and rural environments, totaling 2 hours of manually\nannotated speech data. Along with our dataset, we introduce a set of\ncompetitive models to be used as baselines for future research. The top scoring\nmodel achieves a macro F1 score of 59.83% and a micro F1 score of 62.08%,\nindicating that the task is challenging. We thus believe that RoDia is a\nvaluable resource that will stimulate research aiming to address the challenges\nof Romanian dialect identification. We publicly release our dataset and code at\nhttps://github.com/codrut2/RoDia.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rotaru_C/0/1/0/all/0/1\">Codrut Rotaru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ristea_N/0/1/0/all/0/1\">Nicolae-Catalin Ristea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ImageBind-LLM: Multi-modality Instruction Tuning. (arXiv:2309.03905v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2309.03905","description":"<p>We present ImageBind-LLM, a multi-modality instruction tuning method of large\nlanguage models (LLMs) via ImageBind. Existing works mainly focus on language\nand image instruction tuning, different from which, our ImageBind-LLM can\nrespond to multi-modality conditions, including audio, 3D point clouds, video,\nand their embedding-space arithmetic by only image-text alignment training.\nDuring training, we adopt a learnable bind network to align the embedding space\nbetween LLaMA and ImageBind's image encoder. Then, the image features\ntransformed by the bind network are added to word tokens of all layers in\nLLaMA, which progressively injects visual instructions via an attention-free\nand zero-initialized gating mechanism. Aided by the joint embedding of\nImageBind, the simple image-text training enables our model to exhibit superior\nmulti-modality instruction-following capabilities. During inference, the\nmulti-modality inputs are fed into the corresponding ImageBind encoders, and\nprocessed by a proposed visual cache model for further cross-modal embedding\nenhancement. The training-free cache model retrieves from three million image\nfeatures extracted by ImageBind, which effectively mitigates the\ntraining-inference modality discrepancy. Notably, with our approach,\nImageBind-LLM can respond to instructions of diverse modalities and demonstrate\nsignificant language generation quality. Code is released at\nhttps://github.com/OpenGVLab/LLaMA-Adapter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiaming Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wenqi Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1\">Han Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chris Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1\">Song Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Ziyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xudong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuai Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yafei Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiangyu Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The CALLA Dataset: Probing LLMs' Interactive Knowledge Acquisition from Chinese Medical Literature. (arXiv:2309.04198v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.04198","description":"<p>The application of Large Language Models (LLMs) to the medical domain has\nstimulated the interest of researchers. Recent studies have focused on\nconstructing Instruction Fine-Tuning (IFT) data through medical knowledge\ngraphs to enrich the interactive medical knowledge of LLMs. However, the\nmedical literature serving as a rich source of medical knowledge remains\nunexplored. Our work introduces the CALLA dataset to probe LLMs' interactive\nknowledge acquisition from Chinese medical literature. It assesses the\nproficiency of LLMs in mastering medical knowledge through a free-dialogue\nfact-checking task. We identify a phenomenon called the ``fact-following\nresponse``, where LLMs tend to affirm facts mentioned in questions and display\na reluctance to challenge them. To eliminate the inaccurate evaluation caused\nby this phenomenon, for the golden fact, we artificially construct test data\nfrom two perspectives: one consistent with the fact and one inconsistent with\nthe fact. Drawing from the probing experiment on the CALLA dataset, we conclude\nthat IFT data highly correlated with the medical literature corpus serves as a\npotent catalyst for LLMs, enabling themselves to skillfully employ the medical\nknowledge acquired during the pre-training phase within interactive scenarios,\nenhancing accuracy. Furthermore, we design a framework for automatically\nconstructing IFT data based on medical literature and discuss some real-world\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yanrui Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sendong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Muzhen Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haochun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuhan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Haoqiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UQ at #SMM4H 2023: ALEX for Public Health Analysis with Social Media. (arXiv:2309.04213v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.04213","description":"<p>As social media becomes increasingly popular, more and more activities\nrelated to public health emerge. Current techniques for public health analysis\ninvolve popular models such as BERT and large language models (LLMs). However,\nthe costs of training in-domain LLMs for public health are especially\nexpensive. Furthermore, such kinds of in-domain datasets from social media are\ngenerally imbalanced. To tackle these challenges, the data imbalance issue can\nbe overcome by data augmentation and balanced training. Moreover, the ability\nof the LLMs can be effectively utilized by prompting the model properly. In\nthis paper, a novel ALEX framework is proposed to improve the performance of\npublic health analysis on social media by adopting an LLMs explanation\nmechanism. Results show that our ALEX model got the best performance among all\nsubmissions in both Task 2 and Task 4 with a high score in Task 1 in Social\nMedia Mining for Health 2023 (SMM4H)[1]. Our code has been released at https://\ngithub.com/YanJiangJerry/ALEX.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1\">Ruihong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FIAT: Fusing learning paradigms with Instruction-Accelerated Tuning. (arXiv:2309.04663v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.04663","description":"<p>Learning paradigms for large language models (LLMs) currently tend to fall\nwithin either in-context learning (ICL) or full fine-tuning. Each of these\ncomes with their own trade-offs based on available data, model size, compute\ncost, ease-of-use, and final quality with neither solution performing well\nacross-the-board. In this article, we first describe ICL and fine-tuning\nparadigms in a way that highlights their natural connections. Based on these\nconnections, we propose a new learning paradigm called FIAT that fuses the best\nof these paradigms together, enabling prompt-engineered instructions and\nchain-of-thought reasoning with the very largest models while also using\nsimilar methods to perform parameter updates on a modestly-sized LLM with\nparameter-efficient tuning. We evaluate FIAT's effectiveness on a variety of\nmultilingual tasks and observe that FIAT performs better than both ICL and\nfine-tuning at scales ranging from 100-10,000 training examples. We hope that\nFIAT provides a practical way of harnessing the full potential of LLMs without\nneeding to make a hard choice between learning paradigms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieting_J/0/1/0/all/0/1\">John Wieting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jonathan H. Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Large Language Models for Exploiting ASR Uncertainty. (arXiv:2309.04842v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.04842","description":"<p>While large language models excel in a variety of natural language processing\n(NLP) tasks, to perform well on spoken language understanding (SLU) tasks, they\nmust either rely on off-the-shelf automatic speech recognition (ASR) systems\nfor transcription, or be equipped with an in-built speech modality. This work\nfocuses on the former scenario, where LLM's accuracy on SLU tasks is\nconstrained by the accuracy of a fixed ASR system on the spoken input.\nSpecifically, we tackle speech-intent classification task, where a high\nword-error-rate can limit the LLM's ability to understand the spoken intent.\nInstead of chasing a high accuracy by designing complex or specialized\narchitectures regardless of deployment costs, we seek to answer how far we can\ngo without substantially changing the underlying ASR and LLM, which can\npotentially be shared by multiple unrelated tasks. To this end, we propose\nprompting the LLM with an n-best list of ASR hypotheses instead of only the\nerror-prone 1-best hypothesis. We explore prompt-engineering to explain the\nconcept of n-best lists to the LLM; followed by the finetuning of Low-Rank\nAdapters on the downstream tasks. Our approach using n-best lists proves to be\neffective on a device-directed speech detection task as well as on a keyword\nspotting task, where systems using n-best list prompts outperform those using\n1-best ASR hypothesis; thus paving the way for an efficient method to exploit\nASR uncertainty via LLMs for speech-based applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dighe_P/0/1/0/all/0/1\">Pranay Dighe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shangshang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunshu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_V/0/1/0/all/0/1\">Vineet Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xiaochuan Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewfik_A/0/1/0/all/0/1\">Ahmed Tewfik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-document Summarization: A Comparative Evaluation. (arXiv:2309.04951v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.04951","description":"<p>This paper is aimed at evaluating state-of-the-art models for Multi-document\nSummarization (MDS) on different types of datasets in various domains and\ninvestigating the limitations of existing models to determine future research\ndirections. To address this gap, we conducted an extensive literature review to\nidentify state-of-the-art models and datasets. We analyzed the performance of\nPRIMERA and PEGASUS models on BigSurvey-MDS and MS$^2$ datasets, which posed\nunique challenges due to their varied domains. Our findings show that the\nGeneral-Purpose Pre-trained Model LED outperforms PRIMERA and PEGASUS on the\nMS$^2$ dataset. We used the ROUGE score as a performance metric to evaluate the\nidentified models on different datasets. Our study provides valuable insights\ninto the models' strengths and weaknesses, as well as their applicability in\ndifferent domains. This work serves as a reference for future MDS research and\ncontributes to the development of accurate and robust models which can be\nutilized on demanding datasets with academically and/or scientifically complex\ndata as well as generalized, relatively simple datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hewapathirana_K/0/1/0/all/0/1\">Kushan Hewapathirana</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">Nisansa de Silva</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Athuraliya_C/0/1/0/all/0/1\">C.D. Athuraliya</a> (2) ((1) Department of Computer Science &amp; Engineering, University of Moratuwa, Sri Lanka, (2) ConscientAI, Sri Lanka)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of NetOps Capability of Pre-Trained Large Language Models. (arXiv:2309.05557v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.05557","description":"<p>Large language models (LLMs) can respond to human language queries and have\nshown powerful potential applications in network operations (NetOps). Thanks to\nthe large amount of commonsense knowledge inherent, LLMs achieve much better\ninference accuracy than traditional models and emerge with strong abilities in\ngeneralization, reasoning, and code generation. These abilities may have a\ncrucial boost to automated and intelligent NetOps. However, it remains\nunder-explored how well LLMs perform in various NetOps tasks. In this work, we\nmake a systematic assessment of the capabilities, strengths, and limitations of\nselected LLMs in the field of NetOps. The evaluation is conducted on a\ncollection of 5,732 questions about NetOps, encompassing 26 publicly available\ngeneral-domain LLMs, including ChatGPT, LLaMA, Falcon, etc. We also finetune\nsome of these LLMs with our collected NetOps corpus and evaluate the resulting\nmodels. The evaluation method follows the widely adopted benchmarks for\ngeneral-domain LLMs, combined with Chain-of-Thought Prompts and\nRetrieval-Augmented Generation. The results show that only GPT-4 achieves high\naccuracy equivalent to passing the NetOps certification exam for humans, while\nall the other LLMs have much lower accuracy. However, some open models like\nLLaMA 2 still demonstrate significant potential. Furthermore, we evaluate the\nimpact of factors such as model parameters, prompt engineering, instruction\nfine-tuning etc. This work shall be treated as the initial effort to systematic\nevaluation of LLMs in NetOps, and a more rigorous study is required for\nproduction use. The evaluation code and dataset will be released to benefit\nfuture research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1\">Yukai Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haifeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xizheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Ziqiu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Dapeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiuting Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_C/0/1/0/all/0/1\">Chao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinchi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rewriting the Script: Adapting Text Instructions for Voice Interaction. (arXiv:2306.09992v1 [cs.HC] CROSS LISTED)","link":"http://arxiv.org/abs/2306.09992","description":"<p>Voice assistants have sharply risen in popularity in recent years, but their\nuse has been limited mostly to simple applications like music, hands-free\nsearch, or control of internet-of-things devices. What would it take for voice\nassistants to guide people through more complex tasks? In our work, we study\nthe limitations of the dominant approach voice assistants take to complex task\nguidance: reading aloud written instructions. Using recipes as an example, we\nobserve twelve participants cook at home with a state-of-the-art voice\nassistant. We learn that the current approach leads to nine challenges,\nincluding obscuring the bigger picture, overwhelming users with too much\ninformation, and failing to communicate affordances. Instructions delivered by\na voice assistant are especially difficult because they cannot be skimmed as\neasily as written instructions. Alexa in particular did not surface crucial\ndetails to the user or answer questions well. We draw on our observations to\npropose eight ways in which voice assistants can ``rewrite the script'' --\nsummarizing, signposting, splitting, elaborating, volunteering, reordering,\nredistributing, and visualizing -- to transform written sources into forms that\nare readily communicated through spoken conversation. We conclude with a vision\nof how modern advancements in natural language processing can be leveraged for\nintelligent agents to guide users effectively through complex tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_A/0/1/0/all/0/1\">Alyssa Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oza_N/0/1/0/all/0/1\">Natasha Oza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Head_A/0/1/0/all/0/1\">Andrew Head</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-09-12T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}}]}]}