{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-11-11T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Adaptive Multi-Corpora Language Model Training for Speech Recognition. (arXiv:2211.05121v1 [eess.AS])","link":"http://arxiv.org/abs/2211.05121","description":"<p>Neural network language model (NNLM) plays an essential role in automatic\nspeech recognition (ASR) systems, especially in adaptation tasks when text-only\ndata is available. In practice, an NNLM is typically trained on a combination\nof data sampled from multiple corpora. Thus, the data sampling strategy is\nimportant to the adaptation performance. Most existing works focus on designing\nstatic sampling strategies. However, each corpus may show varying impacts at\ndifferent NNLM training stages. In this paper, we introduce a novel adaptive\nmulti-corpora training algorithm that dynamically learns and adjusts the\nsampling probability of each corpus along the training process. The algorithm\nis robust to corpora sizes and domain relevance. Compared with static sampling\nstrategy baselines, the proposed approach yields remarkable improvement by\nachieving up to relative 7% and 9% word error rate (WER) reductions on\nin-domain and out-of-domain adaptation tasks, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ma_Y/0/1/0/all/0/1\">Yingyi Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xuedong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Zero-shot Event Extraction with Context-Definition Alignment. (arXiv:2211.05156v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05156","description":"<p>Event extraction (EE) is the task of identifying interested event mentions\nfrom text. Conventional efforts mainly focus on the supervised setting.\nHowever, these supervised models cannot generalize to event types out of the\npre-defined ontology. To fill this gap, many efforts have been devoted to the\nzero-shot EE problem. This paper follows the trend of modeling event-type\nsemantics but moves one step further. We argue that using the static embedding\nof the event type name might not be enough because a single word could be\nambiguous, and we need a sentence to define the type semantics accurately. To\nmodel the definition semantics, we use two separate transformer models to\nproject the contextualized event mentions and corresponding definitions into\nthe same embedding space and then minimize their embedding distance via\ncontrastive learning. On top of that, we also propose a warming phase to help\nthe model learn the minor difference between similar definitions. We name our\napproach Zero-shot Event extraction with Definition (ZED). Experiments on the\nMAVEN dataset show that our model significantly outperforms all previous\nzero-shot EE methods with fast inference speed due to the disjoint design.\nFurther experiments also show that ZED can be easily applied to the few-shot\nsetting when the annotation is available and consistently outperforms baseline\nsupervised methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wenlin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uni-Parser: Unified Semantic Parser for Question Answering on Knowledge Base and Database. (arXiv:2211.05165v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05165","description":"<p>Parsing natural language questions into executable logical forms is a useful\nand interpretable way to perform question answering on structured data such as\nknowledge bases (KB) or databases (DB). However, existing approaches on\nsemantic parsing cannot adapt to both modalities, as they suffer from the\nexponential growth of the logical form candidates and can hardly generalize to\nunseen data. In this work, we propose Uni-Parser, a unified semantic parser for\nquestion answering (QA) on both KB and DB. We introduce the primitive (relation\nand entity in KB, and table name, column name and cell value in DB) as an\nessential element in our framework. The number of primitives grows linearly\nwith the number of retrieved relations in KB and DB, preventing us from dealing\nwith exponential logic form candidates. We leverage the generator to predict\nfinal logical forms by altering and composing topranked primitives with\ndifferent operations (e.g. select, where, count). With sufficiently pruned\nsearch space by a contrastive primitive ranker, the generator is empowered to\ncapture the composition of primitives enhancing its generalization ability. We\nachieve competitive results on multiple KB and DB QA benchmarks more\nefficiently, especially in the compositional and zero-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1\">Semih Yavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_R/0/1/0/all/0/1\">Rui Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grammatical Error Correction: A Survey of the State of the Art. (arXiv:2211.05166v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05166","description":"<p>Grammatical Error Correction (GEC) is the task of automatically detecting and\ncorrecting errors in text. The task not only includes the correction of\ngrammatical errors, such as missing prepositions and mismatched subject-verb\nagreement, but also orthographic and semantic errors, such as misspellings and\nword choice errors respectively. The field has seen significant progress in the\nlast decade, motivated in part by a series of five shared tasks, which drove\nthe development of rule-based methods, statistical classifiers, statistical\nmachine translation, and finally neural machine translation systems which\nrepresent the current dominant state of the art. In this survey paper, we\ncondense the field into a single article and first outline some of the\nlinguistic challenges of the task, introduce the most popular datasets that are\navailable to researchers (for both English and other languages), and summarise\nthe various methods and techniques that have been developed with a particular\nfocus on artificial error generation. We next describe the many different\napproaches to evaluation as well as concerns surrounding metric reliability,\nespecially in relation to subjective human judgements, before concluding with\nan overview of recent progress and suggestions for future work and remaining\nchallenges. We hope that this survey will serve as comprehensive resource for\nresearchers who are new to the field or who want to be kept apprised of recent\ndevelopments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bryant_C/0/1/0/all/0/1\">Christopher Bryant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qorib_M/0/1/0/all/0/1\">Muhammad Reza Qorib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Hannan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_H/0/1/0/all/0/1\">Hwee Tou Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briscoe_T/0/1/0/all/0/1\">Ted Briscoe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech separation with large-scale self-supervised learning. (arXiv:2211.05172v1 [eess.AS])","link":"http://arxiv.org/abs/2211.05172","description":"<p>Self-supervised learning (SSL) methods such as WavLM have shown promising\nspeech separation (SS) results in small-scale simulation-based experiments. In\nthis work, we extend the exploration of the SSL-based SS by massively scaling\nup both the pre-training data (more than 300K hours) and fine-tuning data (10K\nhours). We also investigate various techniques to efficiently integrate the\npre-trained model with the SS network under a limited computation budget,\nincluding a low frame rate SSL model training setup and a fine-tuning scheme\nusing only the part of the pre-trained model. Compared with a supervised\nbaseline and the WavLM-based SS model using feature embeddings obtained with\nthe previously released 94K hours trained WavLM, our proposed model obtains\n15.9% and 11.2% of relative word error rate (WER) reductions, respectively, for\na simulated far-field speech mixture test set. For conversation transcription\non real meeting recordings using continuous speech separation, the proposed\nmodel achieves 6.8% and 10.6% of relative WER reductions over the purely\nsupervised baseline on AMI and ICSI evaluation sets, respectively, while\nreducing the computational cost by 38%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sivasankaran_S/0/1/0/all/0/1\">Sunit Sivasankaran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eskimez_S/0/1/0/all/0/1\">Sefik Emre Eskimez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Reasoning-Aware Explainable VQA. (arXiv:2211.05190v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05190","description":"<p>The domain of joint vision-language understanding, especially in the context\nof reasoning in Visual Question Answering (VQA) models, has garnered\nsignificant attention in the recent past. While most of the existing VQA models\nfocus on improving the accuracy of VQA, the way models arrive at an answer is\noftentimes a black box. As a step towards making the VQA task more explainable\nand interpretable, our method is built upon the SOTA VQA framework by\naugmenting it with an end-to-end explanation generation module. In this paper,\nwe investigate two network architectures, including Long Short-Term Memory\n(LSTM) and Transformer decoder, as the explanation generator. Our method\ngenerates human-readable textual explanations while maintaining SOTA VQA\naccuracy on the GQA-REX (77.49%) and VQA-E (71.48%) datasets. Approximately\n65.16% of the generated explanations are approved by humans as valid. Roughly\n60.5% of the generated explanations are valid and lead to the correct answers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vaideeswaran_R/0/1/0/all/0/1\">Rakesh Vaideeswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Feng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_A/0/1/0/all/0/1\">Abhinav Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thattai_G/0/1/0/all/0/1\">Govind Thattai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collateral facilitation in humans and language models. (arXiv:2211.05198v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05198","description":"<p>Are the predictions of humans and language models affected by similar things?\nResearch suggests that while comprehending language, humans make predictions\nabout upcoming words, with more predictable words being processed more easily.\nHowever, evidence also shows that humans display a similar processing advantage\nfor highly anomalous words when these words are semantically related to the\npreceding context or to the most probable continuation. Using stimuli from 3\npsycholinguistic experiments, we find that this is also almost always also the\ncase for 8 contemporary transformer language models (BERT, ALBERT, RoBERTa,\nXLM-R, GPT-2, GPT-Neo, GPT-J, and XGLM). We then discuss the implications of\nthis phenomenon for our understanding of both human language comprehension and\nthe predictions made by language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michaelov_J/0/1/0/all/0/1\">James A. Michaelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1\">Benjamin K. Bergen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HilMeMe: A Human-in-the-Loop Machine Translation Evaluation Metric Looking into Multi-Word Expressions. (arXiv:2211.05201v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05201","description":"<p>With the fast development of Machine Translation (MT) systems, especially the\nnew boost from Neural MT (NMT) models, the MT output quality has reached a new\nlevel of accuracy. However, many researchers criticised that the current\npopular evaluation metrics such as BLEU can not correctly distinguish the\nstate-of-the-art NMT systems regarding quality differences. In this short\npaper, we describe the design and implementation of a linguistically motivated\nhuman-in-the-loop evaluation metric looking into idiomatic and terminological\nMulti-word Expressions (MWEs). MWEs have played a bottleneck in many Natural\nLanguage Processing (NLP) tasks including MT. MWEs can be used as one of the\nmain factors to distinguish different MT systems by looking into their\ncapabilities in recognising and translating MWEs in an accurate and meaning\nequivalent manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lifeng Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT-Based Combination of Convolutional and Recurrent Neural Network for Indonesian Sentiment Analysis. (arXiv:2211.05273v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05273","description":"<p>Sentiment analysis is the computational study of opinions and emotions\nex-pressed in text. Deep learning is a model that is currently producing\nstate-of-the-art in various application domains, including sentiment analysis.\nMany researchers are using a hybrid approach that combines different deep\nlearning models and has been shown to improve model performance. In sentiment\nanalysis, input in text data is first converted into a numerical\nrepresentation. The standard method used to obtain a text representation is the\nfine-tuned embedding method. However, this method does not pay attention to\neach word's context in the sentence. Therefore, the Bidirectional Encoder\nRepresentation from Transformer (BERT) model is used to obtain text\nrepresentations based on the context and position of words in sentences. This\nresearch extends the previous hybrid deep learning using BERT representation\nfor Indonesian sentiment analysis. Our simulation shows that the BERT\nrepresentation improves the accuracies of all hybrid architectures. The\nBERT-based LSTM-CNN also reaches slightly better accuracies than other\nBERT-based hybrid architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murfi_H/0/1/0/all/0/1\">Hendri Murfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Syamsyuriani/0/1/0/all/0/1\">Syamsyuriani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gowandi_T/0/1/0/all/0/1\">Theresia Gowandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardaneswari_G/0/1/0/all/0/1\">Gianinna Ardaneswari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nurrohmah_S/0/1/0/all/0/1\">Siti Nurrohmah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FormLM: Recommending Creation Ideas for Online Forms by Modelling Semantic and Structural Information. (arXiv:2211.05284v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05284","description":"<p>Online forms are widely used to collect data from human and have a\nmulti-billion market. Many software products provide online services for\ncreating semi-structured forms where questions and descriptions are organized\nby pre-defined structures. However, the design and creation process of forms is\nstill tedious and requires expert knowledge. To assist form designers, in this\nwork we present FormLM to model online forms (by enhancing pre-trained language\nmodel with form structural information) and recommend form creation ideas\n(including question / options recommendations and block type suggestion). For\nmodel training and evaluation, we collect the first public online form dataset\nwith 62K online forms. Experiment results show that FormLM significantly\noutperforms general-purpose language models on all tasks, with an improvement\nby 4.71 on Question Recommendation and 10.6 on Block Type Suggestion in terms\nof ROUGE-1 and Macro-F1, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yijia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mengyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yifan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Hongwei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gideon Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Classification with Hypersphere Modeling of Prototypes. (arXiv:2211.05319v1 [cs.LG])","link":"http://arxiv.org/abs/2211.05319","description":"<p>Metric-based meta-learning is one of the de facto standards in few-shot\nlearning. It composes of representation learning and metrics calculation\ndesigns. Previous works construct class representations in different ways,\nvarying from mean output embedding to covariance and distributions. However,\nusing embeddings in space lacks expressivity and cannot capture class\ninformation robustly, while statistical complex modeling poses difficulty to\nmetric designs. In this work, we use tensor fields (``areas'') to model classes\nfrom the geometrical perspective for few-shot learning. We present a simple and\neffective method, dubbed hypersphere prototypes (HyperProto), where class\ninformation is represented by hyperspheres with dynamic sizes with two sets of\nlearnable parameters: the hypersphere's center and the radius. Extending from\npoints to areas, hyperspheres are much more expressive than embeddings.\nMoreover, it is more convenient to perform metric-based classification with\nhypersphere prototypes than statistical modeling, as we only need to calculate\nthe distance from a data point to the surface of the hypersphere. Following\nthis idea, we also develop two variants of prototypes under other measurements.\nExtensive experiments and analysis on few-shot learning tasks across NLP and CV\nand comparison with 20+ competitive baselines demonstrate the effectiveness of\nour approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1\">Ganqu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaobin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not Just Plain Text! Fuel Document-Level Relation Extraction with Explicit Syntax Refinement and Subsentence Modeling. (arXiv:2211.05343v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05343","description":"<p>Document-level relation extraction (DocRE) aims to identify semantic labels\namong entities within a single document. One major challenge of DocRE is to dig\ndecisive details regarding a specific entity pair from long text. However, in\nmany cases, only a fraction of text carries required information, even in the\nmanually labeled supporting evidence. To better capture and exploit instructive\ninformation, we propose a novel expLicit syntAx Refinement and Subsentence\nmOdeliNg based framework (LARSON). By introducing extra syntactic information,\nLARSON can model subsentences of arbitrary granularity and efficiently screen\ninstructive ones. Moreover, we incorporate refined syntax into text\nrepresentations which further improves the performance of LARSON. Experimental\nresults on three benchmark datasets (DocRED, CDR, and GDA) demonstrate that\nLARSON significantly outperforms existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1\">Zhichao Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiuxing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LERT: A Linguistically-motivated Pre-trained Language Model. (arXiv:2211.05344v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05344","description":"<p>Pre-trained Language Model (PLM) has become a representative foundation model\nin the natural language processing field. Most PLMs are trained with\nlinguistic-agnostic pre-training tasks on the surface form of the text, such as\nthe masked language model (MLM). To further empower the PLMs with richer\nlinguistic features, in this paper, we aim to propose a simple but effective\nway to learn linguistic features for pre-trained language models. We propose\nLERT, a pre-trained language model that is trained on three types of linguistic\nfeatures along with the original MLM pre-training task, using a\nlinguistically-informed pre-training (LIP) strategy. We carried out extensive\nexperiments on ten Chinese NLU tasks, and the experimental results show that\nLERT could bring significant improvements over various comparable baselines.\nFurthermore, we also conduct analytical experiments in various linguistic\naspects, and the results prove that the design of LERT is valid and effective.\nResources are available at https://github.com/ymcui/LERT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yiming Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shijin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MSDT: Masked Language Model Scoring Defense in Text Domain. (arXiv:2211.05371v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05371","description":"<p>Pre-trained language models allowed us to process downstream tasks with the\nhelp of fine-tuning, which aids the model to achieve fairly high accuracy in\nvarious Natural Language Processing (NLP) tasks. Such easily-downloaded\nlanguage models from various websites empowered the public users as well as\nsome major institutions to give a momentum to their real-life application.\nHowever, it was recently proven that models become extremely vulnerable when\nthey are backdoor attacked with trigger-inserted poisoned datasets by malicious\nusers. The attackers then redistribute the victim models to the public to\nattract other users to use them, where the models tend to misclassify when\ncertain triggers are detected within the training sample. In this paper, we\nwill introduce a novel improved textual backdoor defense method, named MSDT,\nthat outperforms the current existing defensive algorithms in specific\ndatasets. The experimental results illustrate that our method can be effective\nand constructive in terms of defending against backdoor attack in text domain.\nCode is available at https://github.com/jcroh0508/MSDT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roh_J/0/1/0/all/0/1\">Jaechul Roh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Minhao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yajun Fang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EvEntS ReaLM: Event Reasoning of Entity States via Language Models. (arXiv:2211.05392v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05392","description":"<p>This paper investigates models of event implications. Specifically, how well\nmodels predict entity state-changes, by targeting their understanding of\nphysical attributes. Nominally, Large Language models (LLM) have been exposed\nto procedural knowledge about how objects interact, yet our benchmarking shows\nthey fail to reason about the world. Conversely, we also demonstrate that\nexisting approaches often misrepresent the surprising abilities of LLMs via\nimproper task encodings and that proper model prompting can dramatically\nimprove performance of reported baseline results across multiple tasks. In\nparticular, our results indicate that our prompting technique is especially\nuseful for unseen attributes (out-of-domain) or when only limited data is\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Spiliopoulou_E/0/1/0/all/0/1\">Evangelia Spiliopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pagnoni_A/0/1/0/all/0/1\">Artidoro Pagnoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zebra: Deeply Integrating System-Level Provenance Search and Tracking for Efficient Attack Investigation. (arXiv:2211.05403v1 [cs.CR])","link":"http://arxiv.org/abs/2211.05403","description":"<p>System auditing has emerged as a key approach for monitoring system call\nevents and investigating sophisticated attacks. Based on the collected audit\nlogs, research has proposed to search for attack patterns or track the causal\ndependencies of system events to reveal the attack sequence. However, existing\napproaches either cannot reveal long-range attack sequences or suffer from the\ndependency explosion problem due to a lack of focus on attack-relevant parts,\nand thus are insufficient for investigating complex attacks.\n</p>\n<p>To bridge the gap, we propose Zebra, a system that synergistically integrates\nattack pattern search and causal dependency tracking for efficient attack\ninvestigation. With Zebra, security analysts can alternate between search and\ntracking to reveal the entire attack sequence in a progressive, user-guided\nmanner, while mitigating the dependency explosion problem by prioritizing the\nattack-relevant parts. To enable this, Zebra provides (1) an expressive and\nconcise domain-specific language, Tstl, for performing various types of search\nand tracking analyses, and (2) an optimized language execution engine for\nefficient execution over a big amount of auditing data. Evaluations on a broad\nset of attack cases demonstrate the effectiveness of Zebra in facilitating a\ntimely attack investigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haoyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VieCap4H - VLSP 2021: ObjectAoA -- Enhancing performance of Object Relation Transformer with Attention on Attention for Vietnamese image captioning. (arXiv:2211.05405v1 [cs.CV])","link":"http://arxiv.org/abs/2211.05405","description":"<p>Image captioning is currently a challenging task that requires the ability to\nboth understand visual information and use human language to describe this\nvisual information in the image. In this paper, we propose an efficient way to\nimprove the image understanding ability of transformer-based method by\nextending Object Relation Transformer architecture with Attention on Attention\nmechanism. Experiments on the VieCap4H dataset show that our proposed method\nsignificantly outperforms its original structure on both the public test and\nprivate test of the Image Captioning shared task held by VLSP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Nghia Hieu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_D/0/1/0/all/0/1\">Duong T.D. Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_M/0/1/0/all/0/1\">Minh-Quan Ha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UIT-HWDB: Using Transferring Method to Construct A Novel Benchmark for Evaluating Unconstrained Handwriting Image Recognition in Vietnamese. (arXiv:2211.05407v1 [cs.CV])","link":"http://arxiv.org/abs/2211.05407","description":"<p>Recognizing handwriting images is challenging due to the vast variation in\nwriting style across many people and distinct linguistic aspects of writing\nlanguages. In Vietnamese, besides the modern Latin characters, there are accent\nand letter marks together with characters that draw confusion to\nstate-of-the-art handwriting recognition methods. Moreover, as a low-resource\nlanguage, there are not many datasets for researching handwriting recognition\nin Vietnamese, which makes handwriting recognition in this language have a\nbarrier for researchers to approach. Recent works evaluated offline handwriting\nrecognition methods in Vietnamese using images from an online handwriting\ndataset constructed by connecting pen stroke coordinates without further\nprocessing. This approach obviously can not measure the ability of recognition\nmethods effectively, as it is trivial and may be lack of features that are\nessential in offline handwriting images. Therefore, in this paper, we propose\nthe Transferring method to construct a handwriting image dataset that\nassociates crucial natural attributes required for offline handwriting images.\nUsing our method, we provide a first high-quality synthetic dataset which is\ncomplex and natural for efficiently evaluating handwriting recognition methods.\nIn addition, we conduct experiments with various state-of-the-art methods to\nfigure out the challenge to reach the solution for handwriting recognition in\nVietnamese.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Nghia Hieu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_D/0/1/0/all/0/1\">Duong T.D. Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADEPT: A DEbiasing PrompT Framework. (arXiv:2211.05414v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05414","description":"<p>Several works have proven that finetuning is an applicable approach for\ndebiasing contextualized word embeddings. Similarly, discrete prompts with\nsemantic meanings have shown to be effective in debiasing tasks. With unfixed\nmathematical representation at the token level, continuous prompts usually\nsurpass discrete ones at providing a pre-trained language model (PLM) with\nadditional task-specific information. Despite this, relatively few efforts have\nbeen made to debias PLMs by prompt tuning with continuous prompts compared to\nits discrete counterpart. Furthermore, for most debiasing methods that alter a\nPLM's original parameters, a major problem is the need to not only decrease the\nbias in the PLM but also to ensure that the PLM does not lose its\nrepresentation ability. Finetuning methods typically have a hard time\nmaintaining this balance, as they tend to violently remove meanings of\nattribute words. In this paper, we propose ADEPT, a method to debias PLMs using\nprompt tuning while maintaining the delicate balance between removing biases\nand ensuring representation ability. To achieve this, we propose a new training\ncriterion inspired by manifold learning and equip it with an explicit debiasing\nterm to optimize prompt tuning. In addition, we conduct several experiments\nwith regard to the reliability, quality, and quantity of a previously proposed\nattribute training corpus in order to obtain a clearer prototype of a certain\nattribute, which indicates the attribute's position and relative distances to\nother words on the manifold. We evaluate ADEPT on several widely acknowledged\ndebiasing benchmarks and downstream tasks, and find that it achieves\ncompetitive results while maintaining (and in some cases even improving) the\nPLM's representation ability. We further visualize words' correlation before\nand after debiasing a PLM, and give some possible explanations for the visible\neffects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Ke Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Charles Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_Y/0/1/0/all/0/1\">Yi Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Manling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Transformers Reason in Fragments of Natural Language?. (arXiv:2211.05417v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05417","description":"<p>State-of-the-art deep-learning-based approaches to Natural Language\nProcessing (NLP) are credited with various capabilities that involve reasoning\nwith natural language texts. In this paper we carry out a large-scale empirical\nstudy investigating the detection of formally valid inferences in controlled\nfragments of natural language for which the satisfiability problem becomes\nincreasingly complex. We find that, while transformer-based language models\nperform surprisingly well in these scenarios, a deeper analysis re-veals that\nthey appear to overfit to superficial patterns in the data rather than\nacquiring the logical principles governing the reasoning in these fragments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlegel_V/0/1/0/all/0/1\">Viktor Schlegel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlov_K/0/1/0/all/0/1\">Kamen V. Pavlov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratt_Hartmann_I/0/1/0/all/0/1\">Ian Pratt-Hartmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Human-Centred Explainability Benchmarks For Text Classification. (arXiv:2211.05452v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05452","description":"<p>Progress on many Natural Language Processing (NLP) tasks, such as text\nclassification, is driven by objective, reproducible and scalable evaluation\nvia publicly available benchmarks. However, these are not always representative\nof real-world scenarios where text classifiers are employed, such as sentiment\nanalysis or misinformation detection. In this position paper, we put forward\ntwo points that aim to alleviate this problem. First, we propose to extend text\nclassification benchmarks to evaluate the explainability of text classifiers.\nWe review challenges associated with objectively evaluating the capabilities to\nproduce valid explanations which leads us to the second main point: We propose\nto ground these benchmarks in human-centred applications, for example by using\nsocial media, gamification or to learn explainability metrics from human\njudgements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlegel_V/0/1/0/all/0/1\">Viktor Schlegel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendez_Guzman_E/0/1/0/all/0/1\">Erick Mendez-Guzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batista_Navarro_R/0/1/0/all/0/1\">Riza Batista-Navarro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Text Classification Data and Models Using Aggregated Input Salience. (arXiv:2211.05485v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05485","description":"<p>Realizing when a model is right for a wrong reason is not trivial and\nrequires a significant effort by model developers. In some cases, an input\nsalience method, which highlights the most important parts of the input, may\nreveal problematic reasoning. But scrutinizing highlights over many data\ninstances is tedious and often infeasible. Furthermore, analyzing examples in\nisolation does not reveal general patterns in the data or in the model's\nbehavior.In this paper we aim to address these issues and go from understanding\nsingle examples to understanding entire datasets and models. The methodology we\npropose is based on aggregated salience maps. Using this methodology we address\nmultiple distinct but common model developer needs by showing how problematic\ndata and model behavior can be identified -- a necessary first step for\nimproving the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ebert_S/0/1/0/all/0/1\">Sebastian Ebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakobovits_A/0/1/0/all/0/1\">Alice Shoshana Jakobovits</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filippova_K/0/1/0/all/0/1\">Katja Filippova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoNET: Tackle State Momentum via Noise-Enhanced Training for Dialogue State Tracking. (arXiv:2211.05503v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05503","description":"<p>Dialogue state tracking (DST) aims to convert the dialogue history into\ndialogue states which consist of slot-value pairs. As condensed structural\ninformation memorizing all history information, the dialogue state in the last\nturn is typically adopted as the input for predicting the current state by DST\nmodels. However, these models tend to keep the predicted slot values unchanged,\nwhich is defined as state momentum in this paper. Specifically, the models\nstruggle to update slot values that need to be changed and correct wrongly\npredicted slot values in the last turn. To this end, we propose MoNET to tackle\nstate momentum via noise-enhanced training. First, the previous state of each\nturn in the training data is noised via replacing some of its slot values.\nThen, the noised previous state is used as the input to learn to predict the\ncurrent state, improving the model's ability to update and correct slot values.\nFurthermore, a contrastive context matching framework is designed to narrow the\nrepresentation distance between a state and its corresponding noised variant,\nwhich reduces the impact of noised state and makes the model better understand\nthe dialogue history. Experimental results on MultiWOZ datasets show that MoNET\noutperforms previous DST methods. Ablations and analysis verify the\neffectiveness of MoNET in alleviating state momentum and improving anti-noise\nability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haipeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenye Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Impact of Adversarial Training on Robustness and Generalizability of Language Models. (arXiv:2211.05523v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05523","description":"<p>Adversarial training is widely acknowledged as the most effective defense\nagainst adversarial attacks. However, it is also well established that\nachieving both robustness and generalization in adversarially trained models\ninvolves a trade-off. The goal of this work is to provide an in depth\ncomparison of different approaches for adversarial training in language models.\nSpecifically, we study the effect of pre-training data augmentation as well as\ntraining time input perturbations vs. embedding space perturbations on the\nrobustness and generalization of BERT-like language models. Our findings\nsuggest that better robustness can be achieved by pre-training data\naugmentation or by training with input space perturbation. However, training\nwith embedding space perturbation significantly improves generalization. A\nlinguistic correlation analysis of neurons of the learned models reveal that\nthe improved generalization is due to `more specialized' neurons. To the best\nof our knowledge, this is the first work to carry out a deep qualitative\nanalysis of different methods of generating adversarial examples in adversarial\ntraining of language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Altinisik_E/0/1/0/all/0/1\">Enes Altinisik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sajjad_H/0/1/0/all/0/1\">Hassan Sajjad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sencar_H/0/1/0/all/0/1\">Husrev Taha Sencar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messaoud_S/0/1/0/all/0/1\">Safa Messaoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chawla_S/0/1/0/all/0/1\">Sanjay Chawla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GREENER: Graph Neural Networks for News Media Profiling. (arXiv:2211.05533v1 [cs.LG])","link":"http://arxiv.org/abs/2211.05533","description":"<p>We study the problem of profiling news media on the Web with respect to their\nfactuality of reporting and bias. This is an important but under-studied\nproblem related to disinformation and \"fake news\" detection, but it addresses\nthe issue at a coarser granularity compared to looking at an individual article\nor an individual claim. This is useful as it allows to profile entire media\noutlets in advance. Unlike previous work, which has focused primarily on text\n(e.g.,~on the text of the articles published by the target website, or on the\ntextual description in their social media profiles or in Wikipedia), here our\nmain focus is on modeling the similarity between media outlets based on the\noverlap of their audience. This is motivated by homophily considerations,\ni.e.,~the tendency of people to have connections to people with similar\ninterests, which we extend to media, hypothesizing that similar types of media\nwould be read by similar kinds of users. In particular, we propose GREENER\n(GRaph nEural nEtwork for News mEdia pRofiling), a model that builds a graph of\ninter-media connections based on their audience overlap, and then uses graph\nneural networks to represent each medium. We find that such representations are\nquite useful for predicting the factuality and the bias of news media outlets,\nyielding improvements over state-of-the-art results reported on two datasets.\nWhen augmented with conventionally used representations obtained from news\narticles, Twitter, YouTube, Facebook, and Wikipedia, prediction accuracy is\nfound to improve by 2.5-27 macro-F1 points for the two tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Panayotov_P/0/1/0/all/0/1\">Panayot Panayotov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shukla_U/0/1/0/all/0/1\">Utsav Shukla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sencar_H/0/1/0/all/0/1\">Husrev Taha Sencar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabeel_M/0/1/0/all/0/1\">Mohamed Nabeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assistive Completion of Agrammatic Aphasic Sentences: A Transfer Learning Approach using Neurolinguistics-based Synthetic Dataset. (arXiv:2211.05557v1 [q-bio.QM])","link":"http://arxiv.org/abs/2211.05557","description":"<p>Damage to the inferior frontal gyrus (Broca's area) can cause agrammatic\naphasia wherein patients, although able to comprehend, lack the ability to form\ncomplete sentences. This inability leads to communication gaps which cause\ndifficulties in their daily lives. The usage of assistive devices can help in\nmitigating these issues and enable the patients to communicate effectively.\nHowever, due to lack of large scale studies of linguistic deficits in aphasia,\nresearch on such assistive technology is relatively limited. In this work, we\npresent two contributions that aim to re-initiate research and development in\nthis field. Firstly, we propose a model that uses linguistic features from\nsmall scale studies on aphasia patients and generates large scale datasets of\nsynthetic aphasic utterances from grammatically correct datasets. We show that\nthe mean length of utterance, the noun/verb ratio, and the simple/complex\nsentence ratio of our synthetic datasets correspond to the reported features of\naphasic speech. Further, we demonstrate how the synthetic datasets may be\nutilized to develop assistive devices for aphasia patients. The pre-trained T5\ntransformer is fine-tuned using the generated dataset to suggest 5 corrected\nsentences given an aphasic utterance as input. We evaluate the efficacy of the\nT5 model using the BLEU and cosine semantic similarity scores. Affirming\nresults with BLEU score of 0.827/1.00 and semantic similarity of 0.904/1.00\nwere obtained. These results provide a strong foundation for the concept that a\nsynthetic dataset based on small scale studies on aphasia can be used to\ndevelop effective assistive technology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Misra_R/0/1/0/all/0/1\">Rohit Misra</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Mishra_S/0/1/0/all/0/1\">Sapna S Mishra</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gandhi_T/0/1/0/all/0/1\">Tapan K. Gandhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating Soft Labels for Out-of-Domain Intent Detection. (arXiv:2211.05561v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05561","description":"<p>Out-of-Domain (OOD) intent detection is important for practical dialog\nsystems. To alleviate the issue of lacking OOD training samples, some works\npropose synthesizing pseudo OOD samples and directly assigning one-hot OOD\nlabels to these pseudo samples. However, these one-hot labels introduce noises\nto the training process because some hard pseudo OOD samples may coincide with\nIn-Domain (IND) intents. In this paper, we propose an adaptive soft pseudo\nlabeling (ASoul) method that can estimate soft labels for pseudo OOD samples\nwhen training OOD detectors. Semantic connections between pseudo OOD samples\nand IND intents are captured using an embedding graph. A co-training framework\nis further introduced to produce resulting soft labels following the smoothness\nassumption, i.e., close samples are likely to have similar labels. Extensive\nexperiments on three benchmark datasets show that ASoul consistently improves\nthe OOD detection performance and outperforms various competitive baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lang_H/0/1/0/all/0/1\">Hao Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computer-Aided Modelling of the Bilingual Word Indices to the Ninth-Century Uchitel'noe evangelie. (arXiv:2211.05579v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05579","description":"<p>The development of bilingual dictionaries to medieval translations presents\ndiverse difficulties. These result from two types of philological\ncircumstances: a) the asymmetry between the source language and the target\nlanguage; and b) the varying available sources of both the original and\ntranslated texts. In particular, the full critical edition of Tihova of\nConstantine of Preslav's Uchitel'noe evangelie ('Didactic Gospel') gives a\nrelatively good idea of the Old Church Slavonic translation but not of its\nGreek source text. This is due to the fact that Cramer's edition of the catenae\n- used as the parallel text in it - is based on several codices whose text does\nnot fully coincide with the Slavonic. This leads to the addition of the\nnewly-discovered parallels from Byzantine manuscripts and John Chrysostom's\nhomilies. Our approach to these issues is a step-wise process with two main\ngoals: a) to facilitate the philological annotation of input data and b) to\nconsider the manifestations of the mentioned challenges, first, separately in\norder to simplify their resolution, and, then, in their combination. We\ndemonstrate how we model various types of asymmetric translation correlates and\nthe variability resulting from the pluralism of sources. We also demonstrate\nhow all these constructions are being modelled and processed into the final\nindices. Our approach is designed with generalisation in mind and is intended\nto be applicable also for other translations from Greek into Old Church\nSlavonic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruskov_M/0/1/0/all/0/1\">Martin Ruskov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taseva_L/0/1/0/all/0/1\">Lora Taseva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards automatic generation of Piping and Instrumentation Diagrams (P&IDs) with Artificial Intelligence. (arXiv:2211.05583v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05583","description":"<p>Developing Piping and Instrumentation Diagrams (P&amp;IDs) is a crucial step\nduring the development of chemical processes. Currently, this is a tedious,\nmanual, and time-consuming task. We propose a novel, completely data-driven\nmethod for the prediction of control structures. Our methodology is inspired by\nend-to-end transformer-based human language translation models. We cast the\ncontrol structure prediction as a translation task where Process Flow Diagrams\n(PFDs) are translated to P&amp;IDs. To use established transformer-based language\ntranslation models, we represent the P&amp;IDs and PFDs as strings using our\nrecently proposed SFILES 2.0 notation. Model training is performed in a\ntransfer learning approach. Firstly, we pre-train our model using generated\nP&amp;IDs to learn the grammatical structure of the process diagrams. Thereafter,\nthe model is fine-tuned leveraging transfer learning on real P&amp;IDs. The model\nachieved a top-5 accuracy of 74.8% on 10,000 generated P&amp;IDs and 89.2% on\n100,000 generated P&amp;IDs. These promising results show great potential for\nAI-assisted process engineering. The tests on a dataset of 312 real P&amp;IDs\nindicate the need of a larger P&amp;IDs dataset for industry applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hirtreiter_E/0/1/0/all/0/1\">Edwin Hirtreiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balhorn_L/0/1/0/all/0/1\">Lukas Schulze Balhorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schweidtmann_A/0/1/0/all/0/1\">Artur M. Schweidtmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Robustness of Prefix Tuning in Noisy Data: A Case Study in Financial Sentiment Analysis. (arXiv:2211.05584v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05584","description":"<p>The invention of transformer-based models such as BERT, GPT, and RoBERTa has\nenabled researchers and financial companies to finetune these powerful models\nand use them in different downstream tasks to achieve state-of-the-art\nperformance. Recently, a lightweight alternative (approximately 0.1% - 3% of\nthe original model parameters) to fine-tuning, known as prefix tuning has been\nintroduced. This method freezes the model parameters and only updates the\nprefix to achieve performance comparable to full fine-tuning. Prefix tuning\nenables researchers and financial practitioners to achieve similar results with\nmuch fewer parameters. In this paper, we explore the robustness of prefix\ntuning when facing noisy data. Our experiments demonstrate that fine-tuning is\nmore robust to noise than prefix tuning -- the latter method faces a\nsignificant decrease in performance on most corrupted data sets with increasing\nnoise levels. Furthermore, prefix tuning has high variances in the F1 scores\ncompared to fine-tuning in many corruption methods. We strongly advocate that\ncaution should be carefully taken when applying the state-of-the-art prefix\ntuning method to noisy data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balakrishnan_S/0/1/0/all/0/1\">Sudhandar Balakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yihao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xioadan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt Learning for Domain Adaptation in Task-Oriented Dialogue. (arXiv:2211.05596v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05596","description":"<p>Conversation designers continue to face significant obstacles when creating\nproduction quality task-oriented dialogue systems. The complexity and cost\ninvolved in schema development and data collection is often a major barrier for\nsuch designers, limiting their ability to create natural, user-friendly\nexperiences. We frame the classification of user intent as the generation of a\ncanonical form, a lightweight semantic representation using natural language.\nWe show that canonical forms offer a promising alternative to traditional\nmethods for intent classification. By tuning soft prompts for a frozen large\nlanguage model, we show that canonical forms generalize very well to new,\nunseen domains in a zero- or few-shot setting. The method is also\nsample-efficient, reducing the complexity and effort of developing new\ntask-oriented dialogue domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sreedhar_M/0/1/0/all/0/1\">Makesh Narsimhan Sreedhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parisien_C/0/1/0/all/0/1\">Christopher Parisien</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using contradictions to improve QA systems. (arXiv:2211.05598v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05598","description":"<p>Ensuring the safety of question answering (QA) systems is critical for\ndeploying them in biomedical and scientific domains. One approach to improving\nthese systems uses natural language inference (NLI) to determine whether\nanswers are supported, or entailed, by some background context. However, these\nsystems are vulnerable to supporting an answer with a source that is wrong or\nmisleading. Our work proposes a critical approach by selecting answers based on\nwhether they have been contradicted by some background context. We evaluate\nthis system on multiple choice and extractive QA and find that while the\ncontradiction-based systems are competitive with and often better than\nentailment-only systems, models that incorporate contradiction, entailment, and\nQA model confidence scores together are the best. Based on this result, we\nexplore unique opportunities for leveraging contradiction-based approaches such\nfor improving interpretability and selecting better answers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosati_D/0/1/0/all/0/1\">Domenic Rosati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Moving beyond word lists: towards abstractive topic labels for human-like topics of scientific documents. (arXiv:2211.05599v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05599","description":"<p>Topic models represent groups of documents as a list of words (the topic\nlabels). This work asks whether an alternative approach to topic labeling can\nbe developed that is closer to a natural language description of a topic than a\nword list. To this end, we present an approach to generating human-like topic\nlabels using abstractive multi-document summarization (MDS). We investigate our\napproach with an exploratory case study. We model topics in citation sentences\nin order to understand what further research needs to be done to fully\noperationalize MDS for topic labeling. Our case study shows that in addition to\nmore human-like topics there are additional advantages to evaluation by using\nclustering and summarization measures instead of topic model measures. However,\nwe find that there are several developments needed before we can design a\nwell-powered study to evaluate MDS for topic modeling fully. Namely, improving\ncluster cohesion, improving the factuality and faithfulness of MDS, and\nincreasing the number of documents that might be supported by MDS. We present a\nnumber of ideas on how these can be tackled and conclude with some thoughts on\nhow topic modeling can also be used to improve MDS in general.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosati_D/0/1/0/all/0/1\">Domenic Rosati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Inclusive Notion of Text. (arXiv:2211.05604v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05604","description":"<p>Natural language processing researchers develop models of grammar, meaning\nand human communication based on written text. Due to task and data\ndifferences, what is considered text can vary substantially across studies. A\nconceptual framework for systematically capturing these differences is lacking.\nWe argue that clarity on the notion of text is crucial for reproducible and\ngeneralizable NLP. Towards that goal, we propose common terminology to discuss\nthe production and transformation of textual data, and introduce a two-tier\ntaxonomy of linguistic and non-linguistic elements that are available in\ntextual sources and can be used in NLP modeling. We apply this taxonomy to\nsurvey existing work that extends the notion of text beyond the conservative\nlanguage-centered view. We outline key desiderata and challenges of the\nemerging inclusive approach to text in NLP, and suggest systematic\ncommunity-level reporting as a crucial next step to consolidate the discussion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_I/0/1/0/all/0/1\">Ilia Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT on a Data Diet: Finding Important Examples by Gradient-Based Pruning. (arXiv:2211.05610v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05610","description":"<p>Current pre-trained language models rely on large datasets for achieving\nstate-of-the-art performance. However, past research has shown that not all\nexamples in a dataset are equally important during training. In fact, it is\nsometimes possible to prune a considerable fraction of the training set while\nmaintaining the test performance. Established on standard vision benchmarks,\ntwo gradient-based scoring metrics for finding important examples are GraNd and\nits estimated version, EL2N. In this work, we employ these two metrics for the\nfirst time in NLP. We demonstrate that these metrics need to be computed after\nat least one epoch of fine-tuning and they are not reliable in early steps.\nFurthermore, we show that by pruning a small portion of the examples with the\nhighest GraNd/EL2N scores, we can not only preserve the test accuracy, but also\nsurpass it. This paper details adjustments and implementation choices which\nenable GraNd and EL2N to be applied to NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fayyaz_M/0/1/0/all/0/1\">Mohsen Fayyaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghazadeh_E/0/1/0/all/0/1\">Ehsan Aghazadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modarressi_A/0/1/0/all/0/1\">Ali Modarressi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilehvar_M/0/1/0/all/0/1\">Mohammad Taher Pilehvar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaghoobzadeh_Y/0/1/0/all/0/1\">Yadollah Yaghoobzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahou_S/0/1/0/all/0/1\">Samira Ebrahimi Kahou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Debiasing Methods for Fairer Neural Models in Vision and Language Research: A Survey. (arXiv:2211.05617v1 [cs.LG])","link":"http://arxiv.org/abs/2211.05617","description":"<p>Despite being responsible for state-of-the-art results in several computer\nvision and natural language processing tasks, neural networks have faced harsh\ncriticism due to some of their current shortcomings. One of them is that neural\nnetworks are correlation machines prone to model biases within the data instead\nof focusing on actual useful causal relationships. This problem is particularly\nserious in application domains affected by aspects such as race, gender, and\nage. To prevent models from incurring on unfair decision-making, the AI\ncommunity has concentrated efforts in correcting algorithmic biases, giving\nrise to the research area now widely known as fairness in AI. In this survey\npaper, we provide an in-depth overview of the main debiasing methods for\nfairness-aware neural networks in the context of vision and language research.\nWe propose a novel taxonomy to better organize the literature on debiasing\nmethods for fairness, and we discuss the current challenges, trends, and\nimportant future work directions for the interested researcher and\npractitioner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parraga_O/0/1/0/all/0/1\">Ot&#xe1;vio Parraga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+More_M/0/1/0/all/0/1\">Martin D. More</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_C/0/1/0/all/0/1\">Christian M. Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavenski_N/0/1/0/all/0/1\">Nathan S. Gavenski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kupssinsku_L/0/1/0/all/0/1\">Lucas S. Kupssinsk&#xfc;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medronha_A/0/1/0/all/0/1\">Adilson Medronha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moura_L/0/1/0/all/0/1\">Luis V. Moura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simoes_G/0/1/0/all/0/1\">Gabriel S. Sim&#xf5;es</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barros_R/0/1/0/all/0/1\">Rodrigo C. Barros</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering. (arXiv:2211.05655v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05655","description":"<p>Question answering models commonly have access to two sources of \"knowledge\"\nduring inference time: (1) parametric knowledge - the factual knowledge encoded\nin the model weights, and (2) contextual knowledge - external knowledge (e.g.,\na Wikipedia passage) given to the model to generate a grounded answer. Having\nthese two sources of knowledge entangled together is a core issue for\ngenerative QA models as it is unclear whether the answer stems from the given\nnon-parametric knowledge or not. This unclarity has implications on issues of\ntrust, interpretability and factuality. In this work, we propose a new paradigm\nin which QA models are trained to disentangle the two sources of knowledge.\nUsing counterfactual data augmentation, we introduce a model that predicts two\nanswers for a given question: one based on given contextual knowledge and one\nbased on parametric knowledge. Our experiments on the Natural Questions dataset\nshow that this approach improves the performance of QA models by making them\nmore robust to knowledge conflicts between the two knowledge sources, while\ngenerating useful disentangled answers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neeman_E/0/1/0/all/0/1\">Ella Neeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aharoni_R/0/1/0/all/0/1\">Roee Aharoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honovich_O/0/1/0/all/0/1\">Or Honovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szpektor_I/0/1/0/all/0/1\">Idan Szpektor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT in Plutarch's Shadows. (arXiv:2211.05673v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05673","description":"<p>The extensive surviving corpus of the ancient scholar Plutarch of Chaeronea\n(ca. 45-120 CE) also contains several texts which, according to current\nscholarly opinion, did not originate with him and are therefore attributed to\nan anonymous author Pseudo-Plutarch. These include, in particular, the work\nPlacita Philosophorum (Quotations and Opinions of the Ancient Philosophers),\nwhich is extremely important for the history of ancient philosophy. Little is\nknown about the identity of that anonymous author and its relation to other\nauthors from the same period. This paper presents a BERT language model for\nAncient Greek. The model discovers previously unknown statistical properties\nrelevant to these literary, philosophical, and historical problems and can shed\nnew light on this authorship question. In particular, the Placita\nPhilosophorum, together with one of the other Pseudo-Plutarch texts, shows\nsimilarities with the texts written by authors from an Alexandrian context\n(2nd/3rd century CE).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1\">Ivan P. Yamshchikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantis_Y/0/1/0/all/0/1\">Yorgos Pantis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubert_C/0/1/0/all/0/1\">Charlotte Schubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jost_J/0/1/0/all/0/1\">J&#xfc;rgen Jost</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiaASQ: A Benchmark of Conversational Aspect-based Sentiment Quadruple Analysis. (arXiv:2211.05705v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05705","description":"<p>The rapid development of aspect-based sentiment analysis (ABSA) within recent\ndecades shows great potential for real-world society. The current ABSA works,\nhowever, are mostly limited to the scenario of a single text piece, leaving the\nstudy in dialogue contexts unexplored. In this work, we introduce a novel task\nof conversational aspect-based sentiment quadruple analysis, namely DiaASQ,\naiming to detect the sentiment quadruple of target-aspect-opinion-sentiment in\na dialogue. DiaASQ bridges the gap between fine-grained sentiment analysis and\nconversational opinion mining. We manually construct a large-scale,\nhigh-quality Chinese dataset and also obtain the English version dataset via\nmanual translation. We deliberately propose a neural model to benchmark the\ntask. It advances in effectively performing end-to-end quadruple prediction and\nmanages to incorporate rich dialogue-specific and discourse feature\nrepresentations for better cross-utterance quadruple extraction. We finally\npoint out several potential future works to facilitate the follow-up research\nof this new task. The DiaASQ data is open at https://github.com/unikcc/DiaASQ\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bobo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hao Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinsong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shengqiong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingye Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yijiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1\">Lizi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1\">Donghong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation. (arXiv:2211.05719v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05719","description":"<p>Responding with multi-modal content has been recognized as an essential\ncapability for an intelligent conversational agent. In this paper, we introduce\nthe MMDialog dataset to better facilitate multi-modal conversation. MMDialog is\ncomposed of a curated set of 1.08 million real-world dialogues with 1.53\nmillion unique images across 4,184 topics. MMDialog has two main and unique\nadvantages. First, it is the largest multi-modal conversation dataset by the\nnumber of dialogues by 8x. Second, it contains massive topics to generalize the\nopen-domain. To build engaging dialogue system with this dataset, we propose\nand normalize two response producing tasks based on retrieval and generative\nscenarios. In addition, we build two baselines for above tasks with\nstate-of-the-art techniques and report their experimental performance. We also\npropose a novel evaluation metric MM-Relevance to measure the multi-modal\nresponses. Our dataset and scripts are available in\nhttps://github.com/victorsungo/MMDialog.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiazhan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qingfeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Pu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qingwei Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control. (arXiv:2211.05750v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05750","description":"<p>Pretrained language models have demonstrated extraordinary capabilities in\nlanguage generation. However, real-world tasks often require controlling the\ndistribution of generated text in order to mitigate bias, promote fairness, and\nachieve personalization. Existing techniques for controlling the distribution\nof generated text only work with quantified distributions, which require\npre-defined categories, proportions of the distribution, or an existing corpus\nfollowing the desired distributions. However, many important distributions,\nsuch as personal preferences, are unquantified. In this work, we tackle the\nproblem of generating text following arbitrary distributions (quantified and\nunquantified) by proposing Nano, a few-shot human-in-the-loop training\nalgorithm that continuously learns from human feedback. Nano achieves\nstate-of-the-art results on single topic/attribute as well as quantified\ndistribution control compared to previous works. We also show that Nano is able\nto learn unquantified distributions, achieves personalization, and captures\ndifferences between different individuals' personal preferences with high\nsample efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Massively Multilingual ASR on 70 Languages: Tokenization, Architecture, and Generalization Capabilities. (arXiv:2211.05756v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05756","description":"<p>End-to-end multilingual ASR has become more appealing because of several\nreasons such as simplifying the training and deployment process and positive\nperformance transfer from high-resource to low-resource languages. However,\nscaling up the number of languages, total hours, and number of unique tokens is\nnot a trivial task. This paper explores large-scale multilingual ASR models on\n70 languages. We inspect two architectures: (1) Shared embedding and output and\n(2) Multiple embedding and output model. In the shared model experiments, we\nshow the importance of tokenization strategy across different languages. Later,\nwe use our optimal tokenization strategy to train multiple embedding and output\nmodel to further improve our result. Our multilingual ASR achieves 13.9%-15.6%\naverage WER relative improvement compared to monolingual models. We show that\nour multilingual ASR generalizes well on an unseen dataset and domain,\nachieving 9.5% and 7.5% WER on Multilingual Librispeech (MLS) with zero-shot\nand finetuning, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tjandra_A/0/1/0/all/0/1\">Andros Tjandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_N/0/1/0/all/0/1\">Nayan Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Duc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1\">Michael L. Seltzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Machine-Paraphrased Plagiarism. (arXiv:2103.11909v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.11909","description":"<p>Employing paraphrasing tools to conceal plagiarized text is a severe threat\nto academic integrity. To enable the detection of machine-paraphrased text, we\nevaluate the effectiveness of five pre-trained word embedding models combined\nwith machine learning classifiers and state-of-the-art neural language models.\nWe analyze preprints of research papers, graduation theses, and Wikipedia\narticles, which we paraphrased using different configurations of the tools\nSpinBot and SpinnerChief. The best performing technique, Longformer, achieved\nan average F1 score of 80.99% (F1=99.68% for SpinBot and F1=71.64% for\nSpinnerChief cases), while human evaluators achieved F1=78.4% for SpinBot and\nF1=65.6% for SpinnerChief cases. We show that the automated classification\nalleviates shortcomings of widely-used text-matching systems, such as Turnitin\nand PlagScan. To facilitate future research, all data, code, and two web\napplications showcasing our contributions are openly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1\">Jan Philip Wahle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foltynek_T/0/1/0/all/0/1\">Tom&#xe1;&#x161; Folt&#xfd;nek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuschke_N/0/1/0/all/0/1\">Norman Meuschke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Neural Language Models Good Plagiarists? A Benchmark for Neural Paraphrase Detection. (arXiv:2103.12450v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.12450","description":"<p>The rise of language models such as BERT allows for high-quality text\nparaphrasing. This is a problem to academic integrity, as it is difficult to\ndifferentiate between original and machine-generated content. We propose a\nbenchmark consisting of paraphrased articles using recent language models\nrelying on the Transformer architecture. Our contribution fosters future\nresearch of paraphrase detection systems as it offers a large collection of\naligned original and paraphrased documents, a study regarding its structure,\nclassification experiments with state-of-the-art systems, and we make our\nfindings publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1\">Jan Philip Wahle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuschke_N/0/1/0/all/0/1\">Norman Meuschke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Can Be Strong Differentially Private Learners. (arXiv:2110.05679v6 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.05679","description":"<p>Differentially Private (DP) learning has seen limited success for building\nlarge deep learning models of text, and straightforward attempts at applying\nDifferentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have\nresulted in large performance drops and high computational overhead. We show\nthat this performance drop can be mitigated with (1) the use of large\npretrained language models; (2) non-standard hyperparameters that suit DP\noptimization; and (3) fine-tuning objectives which are aligned with the\npretraining procedure. With the above, we obtain NLP models that outperform\nstate-of-the-art DP-trained models under the same privacy budget and strong\nnon-private baselines -- by directly fine-tuning pretrained models with DP\noptimization on moderately-sized corpora. To address the computational\nchallenge of running DP-SGD with large Transformers, we propose a memory saving\ntechnique that allows clipping in DP-SGD to run without instantiating\nper-example gradients for any linear layer in the model. The technique enables\nprivately training Transformers with almost the same memory cost as non-private\ntraining at a modest run-time overhead. Contrary to conventional wisdom that DP\noptimization fails at learning high-dimensional models (due to noise that\nscales with dimension) empirical results reveal that private learning with\npretrained language models doesn't tend to suffer from dimension-dependent\nperformance degradation. Code to reproduce results can be found at\nhttps://github.com/lxuechen/private-transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuechen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1\">Florian Tram&#xe8;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Testing the Generalization of Neural Language Models for COVID-19 Misinformation Detection. (arXiv:2111.07819v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.07819","description":"<p>A drastic rise in potentially life-threatening misinformation has been a\nby-product of the COVID-19 pandemic. Computational support to identify false\ninformation within the massive body of data on the topic is crucial to prevent\nharm. Researchers proposed many methods for flagging online misinformation\nrelated to COVID-19. However, these methods predominantly target specific\ncontent types (e.g., news) or platforms (e.g., Twitter). The methods'\ncapabilities to generalize were largely unclear so far. We evaluate fifteen\nTransformer-based models on five COVID-19 misinformation datasets that include\nsocial media posts, news articles, and scientific papers to fill this gap. We\nshow tokenizers and models tailored to COVID-19 data do not provide a\nsignificant advantage over general-purpose ones. Our study provides a realistic\nassessment of models for detecting COVID-19 misinformation. We expect that\nevaluating a broad spectrum of datasets and models will benefit future research\nin developing misinformation detection systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1\">Jan Philip Wahle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashok_N/0/1/0/all/0/1\">Nischal Ashok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuschke_N/0/1/0/all/0/1\">Norman Meuschke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_T/0/1/0/all/0/1\">Tirthankar Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Learning with Multilingual Language Models. (arXiv:2112.10668v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.10668","description":"<p>Large-scale generative language models such as GPT-3 are competitive few-shot\nlearners. While these models are known to be able to jointly represent many\ndifferent languages, their training data is dominated by English, potentially\nlimiting their cross-lingual generalization. In this work, we train\nmultilingual generative language models on a corpus covering a diverse set of\nlanguages, and study their few- and zero-shot learning capabilities in a wide\nrange of tasks. Our largest model with 7.5 billion parameters sets new state of\nthe art in few-shot learning in more than 20 representative languages,\noutperforming GPT-3 of comparable size in multilingual commonsense reasoning\n(with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in\n4-shot settings) and natural language inference (+5.4% in each of 0-shot and\n4-shot settings). On the FLORES-101 machine translation benchmark, our model\noutperforms GPT-3 on 171 out of 182 directions with 32 training examples, while\nsurpassing the official supervised baseline in 45 directions. We conduct an\nin-depth analysis of different multilingual prompting approaches, showing in\nparticular that strong few-shot learning performance across languages can be\nachieved via cross-lingual transfer through both templates and demonstration\nexamples. Finally, we evaluate our models in social value tasks such as hate\nspeech detection in five languages and find it has limitations similar to\ncomparable sized GPT-3 models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xi Victoria Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihaylov_T/0/1/0/all/0/1\">Todor Mihaylov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianlu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuohui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simig_D/0/1/0/all/0/1\">Daniel Simig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_M/0/1/0/all/0/1\">Myle Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Naman Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhosale_S/0/1/0/all/0/1\">Shruti Bhosale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jingfei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1\">Ramakanth Pasunuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shleifer_S/0/1/0/all/0/1\">Sam Shleifer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koura_P/0/1/0/all/0/1\">Punit Singh Koura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1\">Vishrav Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OHoro_B/0/1/0/all/0/1\">Brian O&#x27;Horo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jeff Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozareva_Z/0/1/0/all/0/1\">Zornitsa Kozareva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1\">Mona Diab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_V/0/1/0/all/0/1\">Veselin Stoyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regression Transformer: Concurrent Conditional Generation and Regression by Blending Numerical and Textual Tokens. (arXiv:2202.01338v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.01338","description":"<p>Despite significant progress of generative models in the natural sciences,\ntheir controllability remains chal-lenging. One fundamentally missing aspect of\nmolecular or protein generative models is an inductive bias that can reflect\ncontinuous properties of interest. To that end, we propose the Regression\nTransformer (RT), a novel method that abstracts regression as a conditional\nsequence modeling problem. This introduces a new paradigm of multitask language\nmodels which seamlessly bridge sequence regression and conditional sequence\ngeneration.\n</p>\n<p>We thoroughly demonstrate that, despite using a nominal-scale training\nobjective, the RT matches or surpasses the performance of conventional\nregression models in property prediction tasks of small molecules, proteins and\nchemical reactions. Critically, priming the same model with continuous\nproperties yields a highly competitive conditional generative model that\noutperforms specialized approaches in a substructure-constrained,\nproperty-driven molecule generation benchmark. Our dichotomous approach is\nfacilitated by a novel, alternating training scheme that enables the model to\ndecorate seed sequences by desired properties, e.g., to optimize reaction\nyield.\n</p>\n<p>In sum, the RT is the first report of a multitask model that concurrently\nexcels at predictive and generative tasks in biochemistry. This finds\nparticular application in property-driven, local exploration of the chemical or\nprotein space and could pave the road toward foundation models in material\ndesign.\n</p>\n<p>The code to reproduce all experiments of the paper is available at:\nhttps://github.com/IBM/ regression-transformer\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Born_J/0/1/0/all/0/1\">Jannis Born</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manica_M/0/1/0/all/0/1\">Matteo Manica</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WikiOmnia: generative QA corpus on the whole Russian Wikipedia. (arXiv:2204.08009v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08009","description":"<p>The General QA field has been developing the methodology referencing the\nStanford Question answering dataset (SQuAD) as the significant benchmark.\nHowever, compiling factual questions is accompanied by time- and\nlabour-consuming annotation, limiting the training data's potential size. We\npresent the WikiOmnia dataset, a new publicly available set of QA-pairs and\ncorresponding Russian Wikipedia article summary sections, composed with a fully\nautomated generative pipeline. The dataset includes every available article\nfrom Wikipedia for the Russian language. The WikiOmnia pipeline is available\nopen-source and is also tested for creating SQuAD-formatted QA on other\ndomains, like news texts, fiction, and social media. The resulting dataset\nincludes two parts: raw data on the whole Russian Wikipedia (7,930,873 QA pairs\nwith paragraphs for ruGPT-3 XL and 7,991,040 QA pairs with paragraphs for\nruT5-large) and cleaned data with strict automatic verification (over 160,000\nQA pairs with paragraphs for ruGPT-3 XL and over 3,400,000 QA pairs with\nparagraphs for ruT5-large).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pisarevskaya_D/0/1/0/all/0/1\">Dina Pisarevskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shavrina_T/0/1/0/all/0/1\">Tatiana Shavrina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Span-level Bidirectional Network for Aspect Sentiment Triplet Extraction. (arXiv:2204.12674v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.12674","description":"<p>Aspect Sentiment Triplet Extraction (ASTE) is a new fine-grained sentiment\nanalysis task that aims to extract triplets of aspect terms, sentiments, and\nopinion terms from review sentences. Recently, span-level models achieve\ngratifying results on ASTE task by taking advantage of the predictions of all\npossible spans. Since all possible spans significantly increases the number of\npotential aspect and opinion candidates, it is crucial and challenging to\nefficiently extract the triplet elements among them. In this paper, we present\na span-level bidirectional network which utilizes all possible spans as input\nand extracts triplets from spans bidirectionally. Specifically, we devise both\nthe aspect decoder and opinion decoder to decode the span representations and\nextract triples from aspect-to-opinion and opinion-to-aspect directions. With\nthese two decoders complementing with each other, the whole network can extract\ntriplets from spans more comprehensively. Moreover, considering that mutual\nexclusion cannot be guaranteed between the spans, we design a similar span\nseparation loss to facilitate the downstream task of distinguishing the correct\nspan by expanding the KL divergence of similar spans during the training\nprocess; in the inference process, we adopt an inference strategy to remove\nconflicting triplets from the results base on their confidence scores.\nExperimental results show that our framework not only significantly outperforms\nstate-of-the-art methods, but achieves better performance in predicting\ntriplets with multi-token entities and extracting triplets in sentences contain\nmulti-triplets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Keming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zequn Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D3: A Massive Dataset of Scholarly Metadata for Analyzing the State of Computer Science Research. (arXiv:2204.13384v4 [cs.DL] UPDATED)","link":"http://arxiv.org/abs/2204.13384","description":"<p>DBLP is the largest open-access repository of scientific articles on computer\nscience and provides metadata associated with publications, authors, and\nvenues. We retrieved more than 6 million publications from DBLP and extracted\npertinent metadata (e.g., abstracts, author affiliations, citations) from the\npublication texts to create the DBLP Discovery Dataset (D3). D3 can be used to\nidentify trends in research activity, productivity, focus, bias, accessibility,\nand impact of computer science research. We present an initial analysis focused\non the volume of computer science research (e.g., number of papers, authors,\nresearch activity), trends in topics of interest, and citation patterns. Our\nfindings show that computer science is a growing research field (approx. 15%\nannually), with an active and collaborative researcher community. While papers\nin recent years present more bibliographical entries in comparison to previous\ndecades, the average number of citations has been declining. Investigating\npapers' abstracts reveals that recent topic trends are clearly reflected in D3.\nFinally, we list further applications of D3 and pose supplemental research\nquestions. The D3 dataset, our findings, and source code are publicly available\nfor research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1\">Jan Philip Wahle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Training for High-Stakes Reliability. (arXiv:2205.01663v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.01663","description":"<p>In the future, powerful AI systems may be deployed in high-stakes settings,\nwhere a single failure could be catastrophic. One technique for improving AI\nsafety in high-stakes settings is adversarial training, which uses an adversary\nto generate examples to train on in order to achieve better worst-case\nperformance.\n</p>\n<p>In this work, we used a safe language generation task (``avoid injuries'') as\na testbed for achieving high reliability through adversarial training. We\ncreated a series of adversarial training techniques -- including a tool that\nassists human adversaries -- to find and eliminate failures in a classifier\nthat filters text completions suggested by a generator. In our task, we\ndetermined that we can set very conservative classifier thresholds without\nsignificantly impacting the quality of the filtered outputs. We found that\nadversarial training increased robustness to the adversarial attacks that we\ntrained on -- doubling the time for our contractors to find adversarial\nexamples both with our tool (from 13 to 26 minutes) and without (from 20 to 44\nminutes) -- without affecting in-distribution performance.\n</p>\n<p>We hope to see further work in the high-stakes reliability setting, including\nmore powerful tools for enhancing human adversaries and better ways to measure\nhigh levels of reliability, until we can confidently rule out the possibility\nof catastrophic deployment-time failures of powerful models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ziegler_D/0/1/0/all/0/1\">Daniel M. Ziegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nix_S/0/1/0/all/0/1\">Seraphina Nix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_L/0/1/0/all/0/1\">Lawrence Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauman_T/0/1/0/all/0/1\">Tim Bauman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_Nielsen_P/0/1/0/all/0/1\">Peter Schmidt-Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherlis_A/0/1/0/all/0/1\">Adam Scherlis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabeshima_N/0/1/0/all/0/1\">Noa Nabeshima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinstein_Raun_B/0/1/0/all/0/1\">Ben Weinstein-Raun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haas_D/0/1/0/all/0/1\">Daniel de Haas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shlegeris_B/0/1/0/all/0/1\">Buck Shlegeris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_N/0/1/0/all/0/1\">Nate Thomas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"T-Modules: Translation Modules for Zero-Shot Cross-Modal Machine Translation. (arXiv:2205.12216v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12216","description":"<p>We present a new approach to perform zero-shot cross-modal transfer between\nspeech and text for translation tasks. Multilingual speech and text are encoded\nin a joint fixed-size representation space. Then, we compare different\napproaches to decode these multimodal and multilingual fixed-size\nrepresentations, enabling zero-shot translation between languages and\nmodalities. All our models are trained without the need of cross-modal labeled\ntranslation data. Despite a fixed-size representation, we achieve very\ncompetitive results on several text and speech translation tasks. In\nparticular, we significantly improve the state-of-the-art for zero-shot speech\ntranslation on Must-C. Incorporating a speech decoder in our framework, we\nintroduce the first results for zero-shot direct speech-to-speech and\ntext-to-speech translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duquenne_P/0/1/0/all/0/1\">Paul-Ambroise Duquenne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Beno&#xee;t Sagot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwenk_H/0/1/0/all/0/1\">Holger Schwenk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Translation Robustness to Natural Asemantic Variation. (arXiv:2205.12514v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12514","description":"<p>Current Machine Translation (MT) models still struggle with more challenging\ninput, such as noisy data and tail-end words and phrases. Several works have\naddressed this robustness issue by identifying specific categories of noise and\nvariation then tuning models to perform better on them. An important yet\nunder-studied category involves minor variations in nuance (non-typos) that\npreserve meaning w.r.t. the target language. We introduce and formalize this\ncategory as Natural Asemantic Variation (NAV) and investigate it in the context\nof MT robustness. We find that existing MT models fail when presented with NAV\ndata, but we demonstrate strategies to improve performance on NAV by\nfine-tuning them with human-generated variations. We also show that NAV\nrobustness can be transferred across languages and find that synthetic\nperturbations can achieve some but not all of the benefits of organic NAV data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bremerman_J/0/1/0/all/0/1\">Jacob Bremerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Large Language Models are Transforming Machine-Paraphrased Plagiarism. (arXiv:2210.03568v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03568","description":"<p>The recent success of large language models for text generation poses a\nsevere threat to academic integrity, as plagiarists can generate realistic\nparaphrases indistinguishable from original work. However, the role of large\nautoregressive transformers in generating machine-paraphrased plagiarism and\ntheir detection is still developing in the literature. This work explores T5\nand GPT-3 for machine-paraphrase generation on scientific articles from arXiv,\nstudent theses, and Wikipedia. We evaluate the detection performance of six\nautomated solutions and one commercial plagiarism detection software and\nperform a human study with 105 participants regarding their detection\nperformance and the quality of generated examples. Our results suggest that\nlarge models can rewrite text humans have difficulty identifying as\nmachine-paraphrased (53% mean acc.). Human experts rate the quality of\nparaphrases generated by GPT-3 as high as original texts (clarity 4.0/5,\nfluency 4.2/5, coherence 3.8/5). The best-performing detection model (GPT-3)\nachieves a 66% F1-score in detecting paraphrases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1\">Jan Philip Wahle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirstein_F/0/1/0/all/0/1\">Frederic Kirstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters. (arXiv:2210.04284v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04284","description":"<p>Adapter Tuning, which freezes the pretrained language models (PLMs) and only\nfine-tunes a few extra modules, becomes an appealing efficient alternative to\nthe full model fine-tuning. Although computationally efficient, the recent\nAdapters often increase parameters (e.g. bottleneck dimension) for matching the\nperformance of full model fine-tuning, which we argue goes against their\noriginal intention. In this work, we re-examine the parameter-efficiency of\nAdapters through the lens of network pruning (we name such plug-in concept as\n\\texttt{SparseAdapter}) and find that SparseAdapter can achieve comparable or\nbetter performance than standard Adapters when the sparse ratio reaches up to\n80\\%. Based on our findings, we introduce an easy but effective setting\n``\\textit{Large-Sparse}'' to improve the model capacity of Adapters under the\nsame parameter budget. Experiments on five competitive Adapters upon three\nadvanced PLMs show that with proper sparse method (e.g. SNIP) and ratio (e.g.\n40\\%) SparseAdapter can consistently outperform their corresponding\ncounterpart. Encouragingly, with the \\textit{Large-Sparse} setting, we can\nobtain further appealing gains, even outperforming the full fine-tuning by a\nlarge margin. Our code will be released at:\nhttps://github.com/Shwai-He/SparseAdapter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shwai He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1\">Daize Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tracing Semantic Variation in Slang. (arXiv:2210.08635v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08635","description":"<p>The meaning of a slang term can vary in different communities. However, slang\nsemantic variation is not well understood and under-explored in the natural\nlanguage processing of slang. One existing view argues that slang semantic\nvariation is driven by culture-dependent communicative needs. An alternative\nview focuses on slang's social functions suggesting that the desire to foster\nsemantic distinction may have led to the historical emergence of\ncommunity-specific slang senses. We explore these theories using computational\nmodels and test them against historical slang dictionary entries, with a focus\non characterizing regularity in the geographical variation of slang usages\nattested in the US and the UK over the past two centuries. We show that our\nmodels are able to predict the regional identity of emerging slang word\nmeanings from historical slang records. We offer empirical evidence that both\ncommunicative need and semantic distinction play a role in the variation of\nslang meaning yet their relative importance fluctuates over the course of\nhistory. Our work offers an opportunity for incorporating historical cultural\nelements into the natural language processing of slang.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhewei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Searching for a higher power in the human evaluation of MT. (arXiv:2210.11612v2 [stat.AP] UPDATED)","link":"http://arxiv.org/abs/2210.11612","description":"<p>In MT evaluation, pairwise comparisons are conducted to identify the better\nsystem. In conducting the comparison, the experimenter must allocate a budget\nto collect Direct Assessment (DA) judgments. We provide a cost effective way to\nspend the budget, but show that typical budget sizes often do not allow for\nsolid comparison. Taking the perspective that the basis of solid comparison is\nin achieving statistical significance, we study the power (rate of achieving\nsignificance) on a large collection of pairwise DA comparisons. Due to the\nnature of statistical estimation, power is low for differentiating less than\n1-2 DA points, and to achieve a notable increase in power requires at least\n2-3x more samples. Applying variance reduction alone will not yield these\ngains, so we must face the reality of undetectable differences and spending\nincreases. In this context, we propose interim testing, an \"early stopping\"\ncollection procedure that yields more power per judgment collected, which\nadaptively focuses the budget on pairs that are borderline significant. Interim\ntesting can achieve up to a 27% efficiency gain when spending 3x the current\nbudget, or 18% savings at the current evaluation power.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Wei_J/0/1/0/all/0/1\">Johnny Tian-Zheng Wei</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kocmi_T/0/1/0/all/0/1\">Tom Kocmi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Federmann_C/0/1/0/all/0/1\">Christian Federmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KnowGL: Knowledge Generation and Linking from Text. (arXiv:2210.13952v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.13952","description":"<p>We propose KnowGL, a tool that allows converting text into structured\nrelational data represented as a set of ABox assertions compliant with the TBox\nof a given Knowledge Graph (KG), such as Wikidata. We address this problem as a\nsequence generation task by leveraging pre-trained sequence-to-sequence\nlanguage models, e.g. BART. Given a sentence, we fine-tune such models to\ndetect pairs of entity mentions and jointly generate a set of facts consisting\nof the full set of semantic annotations for a KG, such as entity labels, entity\ntypes, and their relationships. To showcase the capabilities of our tool, we\nbuild a web application consisting of a set of UI widgets that help users to\nnavigate through the semantic data extracted from a given input text. We make\nthe KnowGL model available at https://huggingface.co/ibm/knowgl-large.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rossiello_G/0/1/0/all/0/1\">Gaetano Rossiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_F/0/1/0/all/0/1\">Faisal Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1\">Nandana Mihindukulasooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornec_O/0/1/0/all/0/1\">Owen Cornec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Gliozzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-learning Pathologies from Radiology Reports using Variance Aware Prototypical Networks. (arXiv:2210.13979v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.13979","description":"<p>Large pretrained Transformer-based language models like BERT and GPT have\nchanged the landscape of Natural Language Processing (NLP). However, fine\ntuning such models still requires a large number of training examples for each\ntarget task, thus annotating multiple datasets and training these models on\nvarious downstream tasks becomes time consuming and expensive. In this work, we\npropose a simple extension of the Prototypical Networks for few-shot text\nclassification. Our main idea is to replace the class prototypes by Gaussians\nand introduce a regularization term that encourages the examples to be\nclustered near the appropriate class centroids. Experimental results show that\nour method outperforms various strong baselines on 13 public and 4 internal\ndatasets. Furthermore, we use the class distributions as a tool for detecting\npotential out-of-distribution (OOD) data points during deployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sehanobish_A/0/1/0/all/0/1\">Arijit Sehanobish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannan_K/0/1/0/all/0/1\">Kawshik Kannan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abraham_N/0/1/0/all/0/1\">Nabila Abraham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Anasuya Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Odry_B/0/1/0/all/0/1\">Benjamin Odry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Where to start? Analyzing the potential value of intermediate models. (arXiv:2211.00107v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.00107","description":"<p>Previous studies observed that finetuned models may be better base models\nthan the vanilla pretrained model. Such a model, finetuned on some source\ndataset, may provide a better starting point for a new finetuning process on a\ndesired target dataset. Here, we perform a systematic analysis of this\nintertraining scheme, over a wide range of English classification tasks.\nSurprisingly, our analysis suggests that the potential intertraining gain can\nbe analyzed independently for the target dataset under consideration, and for a\nbase model being considered as a starting point. This is in contrast to current\nperception that the alignment between the target dataset and the source dataset\nused to generate the base model is a major factor in determining intertraining\nsuccess. We analyze different aspects that contribute to each. Furthermore, we\nleverage our analysis to propose a practical and efficient approach to\ndetermine if and how to select a base model in real-world settings. Last, we\nrelease an updating ranking of best models in the HuggingFace hub per\narchitecture https://ibm.github.io/model-recycling/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venezian_E/0/1/0/all/0/1\">Elad Venezian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Don_Yehia_S/0/1/0/all/0/1\">Shachar Don-Yehia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1\">Yoav Katz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Processing Long Legal Documents with Pre-trained Transformers: Modding LegalBERT and Longformer. (arXiv:2211.00974v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.00974","description":"<p>Pre-trained Transformers currently dominate most NLP tasks. They impose,\nhowever, limits on the maximum input length (512 sub-words in BERT), which are\ntoo restrictive in the legal domain. Even sparse-attention models, such as\nLongformer and BigBird, which increase the maximum input length to 4,096\nsub-words, severely truncate texts in three of the six datasets of LexGLUE.\nSimpler linear classifiers with TF-IDF features can handle texts of any length,\nrequire far less resources to train and deploy, but are usually outperformed by\npre-trained Transformers. We explore two directions to cope with long legal\ntexts: (i) modifying a Longformer warm-started from LegalBERT to handle even\nlonger texts (up to 8,192 sub-words), and (ii) modifying LegalBERT to use\nTF-IDF representations. The first approach is the best in terms of performance,\nsurpassing a hierarchical version of LegalBERT, which was the previous state of\nthe art in LexGLUE. The second approach leads to computationally more efficient\nmodels at the expense of lower performance, but the resulting models still\noutperform overall a linear SVM with TF-IDF features in long legal document\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mamakas_D/0/1/0/all/0/1\">Dimitris Mamakas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsotsi_P/0/1/0/all/0/1\">Petros Tsotsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1\">Ion Androutsopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RQUGE: Reference-Free Metric for Evaluating Question Generation by Answering the Question. (arXiv:2211.01482v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.01482","description":"<p>Existing metrics for evaluating the quality of automatically generated\nquestions such as BLEU, ROUGE, BERTScore, and BLEURT compare the reference and\npredicted questions, providing a high score when there is a considerable\nlexical overlap or semantic similarity between the candidate and the reference\nquestions. This approach has two major shortcomings. First, we need expensive\nhuman-provided reference questions. Second, it penalises valid questions that\nmay not have high lexical or semantic similarity to the reference questions. In\nthis paper, we propose a new metric, RQUGE, based on the answerability of the\ncandidate question given the context. The metric consists of a\nquestion-answering and a span scorer module, in which we use pre-trained models\nfrom the existing literature, and therefore, our metric can be used without\nfurther training. We show that RQUGE has a higher correlation with human\njudgment without relying on the reference question. RQUGE is shown to be\nsignificantly more robust to several adversarial corruptions. Additionally, we\nillustrate that we can significantly improve the performance of QA models on\nout-of-domain datasets by fine-tuning on the synthetic data generated by a\nquestion generation model and re-ranked by RQUGE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadshahi_A/0/1/0/all/0/1\">Alireza Mohammadshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazdani_M/0/1/0/all/0/1\">Majid Yazdani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yanki_P/0/1/0/all/0/1\">Pouya Yanki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1\">Angela Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeidi_M/0/1/0/all/0/1\">Marzieh Saeidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Cross-modal Interactions in V&L Models that Generate Scene Descriptions. (arXiv:2211.04971v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.04971","description":"<p>Image captioning models tend to describe images in an object-centric way,\nemphasising visible objects. But image descriptions can also abstract away from\nobjects and describe the type of scene depicted. In this paper, we explore the\npotential of a state-of-the-art Vision and Language model, VinVL, to caption\nimages at the scene level using (1) a novel dataset which pairs images with\nboth object-centric and scene descriptions. Through (2) an in-depth analysis of\nthe effect of the fine-tuning, we show (3) that a small amount of curated data\nsuffices to generate scene descriptions without losing the capability to\nidentify object-level concepts in the scene; the model acquires a more holistic\nview of the image compared to when object-centric descriptions are generated.\nWe discuss the parallels between these results and insights from computational\nand cognitive science research on scene perception.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cafagna_M/0/1/0/all/0/1\">Michele Cafagna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deemter_K/0/1/0/all/0/1\">Kees van Deemter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1\">Albert Gatt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What is Wrong with Language Models that Can Not Tell a Story?. (arXiv:2211.05044v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05044","description":"<p>This paper argues that a deeper understanding of narrative and the successful\ngeneration of longer subjectively interesting texts is a vital bottleneck that\nhinders the progress in modern Natural Language Processing (NLP) and may even\nbe in the whole field of Artificial Intelligence. We demonstrate that there are\nno adequate datasets, evaluation methods, and even operational concepts that\ncould be used to start working on narrative processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1\">Ivan P. Yamshchikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fusing ASR Outputs in Joint Training for Speech Emotion Recognition. (arXiv:2110.15684v2 [eess.AS] CROSS LISTED)","link":"http://arxiv.org/abs/2110.15684","description":"<p>Alongside acoustic information, linguistic features based on speech\ntranscripts have been proven useful in Speech Emotion Recognition (SER).\nHowever, due to the scarcity of emotion labelled data and the difficulty of\nrecognizing emotional speech, it is hard to obtain reliable linguistic features\nand models in this research area. In this paper, we propose to fuse Automatic\nSpeech Recognition (ASR) outputs into the pipeline for joint training SER. The\nrelationship between ASR and SER is understudied, and it is unclear what and\nhow ASR features benefit SER. By examining various ASR outputs and fusion\nmethods, our experiments show that in joint ASR-SER training, incorporating\nboth ASR hidden and text output using a hierarchical co-attention fusion\napproach improves the SER performance the most. On the IEMOCAP corpus, our\napproach achieves 63.4% weighted accuracy, which is close to the baseline\nresults achieved by combining ground-truth transcripts. In addition, we also\npresent novel word error rate analysis on IEMOCAP and layer-difference analysis\nof the Wav2vec 2.0 model to better understand the relationship between ASR and\nSER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuanchao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bell_P/0/1/0/all/0/1\">Peter Bell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lai_C/0/1/0/all/0/1\">Catherine Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation for Sparse Retrieval by Filling Vocabulary and Word Frequency Gaps. (arXiv:2211.03988v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2211.03988","description":"<p>IR models using a pretrained language model significantly outperform lexical\napproaches like BM25. In particular, SPLADE, which encodes texts to sparse\nvectors, is an effective model for practical use because it shows robustness to\nout-of-domain datasets. However, SPLADE still struggles with exact matching of\nlow-frequency words in training data. In addition, domain shifts in vocabulary\nand word frequencies deteriorate the IR performance of SPLADE. Because\nsupervision data are scarce in the target domain, addressing the domain shifts\nwithout supervision data is necessary. This paper proposes an unsupervised\ndomain adaptation method by filling vocabulary and word-frequency gaps. First,\nwe expand a vocabulary and execute continual pretraining with a masked language\nmodel on a corpus of the target domain. Then, we multiply SPLADE-encoded sparse\nvectors by inverse document frequency weights to consider the importance of\ndocuments with lowfrequency words. We conducted experiments using our method on\ndatasets with a large vocabulary gap from a source domain. We show that our\nmethod outperforms the present stateof-the-art domain adaptation method. In\naddition, our method achieves state-of-the-art results, combined with BM25.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iida_H/0/1/0/all/0/1\">Hiroki Iida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1\">Naoaki Okazaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-11-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}