{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-10-20T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Alibaba-Translate China's Submission for WMT 2022 Quality Estimation Shared Task. (arXiv:2210.10049v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10049","description":"<p>In this paper, we present our submission to the sentence-level MQM benchmark\nat Quality Estimation Shared Task, named UniTE (Unified Translation\nEvaluation). Specifically, our systems employ the framework of UniTE, which\ncombined three types of input formats during training with a pre-trained\nlanguage model. First, we apply the pseudo-labeled data examples for the\ncontinuously pre-training phase. Notably, to reduce the gap between\npre-training and fine-tuning, we use data pruning and a ranking-based score\nnormalization strategy. For the fine-tuning phase, we use both Direct\nAssessment (DA) and Multidimensional Quality Metrics (MQM) data from past\nyears' WMT competitions. Finally, we collect the source-only evaluation\nresults, and ensemble the predictions generated by two UniTE models, whose\nbackbones are XLM-R and InfoXLM, respectively. Results show that our models\nreach 1st overall ranking in the Multilingual and English-Russian settings, and\n2nd overall ranking in English-German and Chinese-English settings, showing\nrelatively strong performances in this year's quality estimation competition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_K/0/1/0/all/0/1\">Keqin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yu Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dayiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baosong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenqiang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek F.Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting and analyzing missing citations to published scientific entities. (arXiv:2210.10073v1 [cs.DL])","link":"http://arxiv.org/abs/2210.10073","description":"<p>Proper citation is of great importance in academic writing for it enables\nknowledge accumulation and maintains academic integrity. However, citing\nproperly is not an easy task. For published scientific entities, the\never-growing academic publications and over-familiarity of terms easily lead to\nmissing citations. To deal with this situation, we design a special method\nCitation Recommendation for Published Scientific Entity (CRPSE) based on the\ncooccurrences between published scientific entities and in-text citations in\nthe same sentences from previous researchers. Experimental outcomes show the\neffectiveness of our method in recommending the source papers for published\nscientific entities. We further conduct a statistical analysis on missing\ncitations among papers published in prestigious computer science conferences in\n2020. In the 12,278 papers collected, 475 published scientific entities of\ncomputer science and mathematics are found to have missing citations. Many\nentities mentioned without citations are found to be well-accepted research\nresults. On a median basis, the papers proposing these published scientific\nentities with missing citations were published 8 years ago, which can be\nconsidered the time frame for a published scientific entity to develop into a\nwell-accepted concept. For published scientific entities, we appeal for\naccurate and full citation of their source papers as required by academic\nstandards.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jialiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiaxin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaodong Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELASTIC: Numerical Reasoning with Adaptive Symbolic Compiler. (arXiv:2210.10105v1 [cs.AI])","link":"http://arxiv.org/abs/2210.10105","description":"<p>Numerical reasoning over text is a challenging task of Artificial\nIntelligence (AI), requiring reading comprehension and numerical reasoning\nabilities. Previous approaches use numerical reasoning programs to represent\nthe reasoning process. However, most works do not separate the generation of\noperators and operands, which are key components of a numerical reasoning\nprogram, thus limiting their ability to generate such programs for complicated\ntasks. In this paper, we introduce the numEricaL reASoning with adapTive\nsymbolIc Compiler (ELASTIC) model, which is constituted of the RoBERTa as the\nEncoder and a Compiler with four modules: Reasoning Manager, Operator\nGenerator, Operands Generator, and Memory Register. ELASTIC is robust when\nconducting complicated reasoning. Also, it is domain agnostic by supporting the\nexpansion of diverse operators without caring about the number of operands it\ncontains. Experiments show that ELASTIC achieves 68.96 and 65.21 of execution\naccuracy and program accuracy on the FinQA dataset and 83.00 program accuracy\non the MathQA dataset, outperforming previous state-of-the-art models\nsignificantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moshfeghi_Y/0/1/0/all/0/1\">Yashar Moshfeghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Active Learning for Natural Language Processing. (arXiv:2210.10109v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10109","description":"<p>In this work, we provide a survey of active learning (AL) for its\napplications in natural language processing (NLP). In addition to a\nfine-grained categorization of query strategies, we also investigate several\nother important aspects of applying AL to NLP problems. These include AL for\nstructured prediction tasks, annotation cost, model learning (especially with\ndeep neural models), and starting and stopping AL. Finally, we conclude with a\ndiscussion of related topics and future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhisong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1\">Emma Strubell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Domain Aspect Extraction using Transformers Augmented with Knowledge Graphs. (arXiv:2210.10144v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10144","description":"<p>The extraction of aspect terms is a critical step in fine-grained sentiment\nanalysis of text. Existing approaches for this task have yielded impressive\nresults when the training and testing data are from the same domain. However,\nthese methods show a drastic decrease in performance when applied to\ncross-domain settings where the domain of the testing data differs from that of\nthe training data. To address this lack of extensibility and robustness, we\npropose a novel approach for automatically constructing domain-specific\nknowledge graphs that contain information relevant to the identification of\naspect terms. We introduce a methodology for injecting information from these\nknowledge graphs into Transformer models, including two alternative mechanisms\nfor knowledge insertion: via query enrichment and via manipulation of attention\npatterns. We demonstrate state-of-the-art performance on benchmark datasets for\ncross-domain aspect term extraction using our approach and investigate how the\namount of external knowledge available to the Transformer impacts model\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Howard_P/0/1/0/all/0/1\">Phillip Howard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_A/0/1/0/all/0/1\">Arden Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1\">Vasudev Lal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simoes_A/0/1/0/all/0/1\">Ana Paula Simoes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korat_D/0/1/0/all/0/1\">Daniel Korat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereg_O/0/1/0/all/0/1\">Oren Pereg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasserblat_M/0/1/0/all/0/1\">Moshe Wasserblat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singer_G/0/1/0/all/0/1\">Gadi Singer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MedCLIP: Contrastive Learning from Unpaired Medical Images and Text. (arXiv:2210.10163v1 [cs.CV])","link":"http://arxiv.org/abs/2210.10163","description":"<p>Existing vision-text contrastive learning like CLIP aims to match the paired\nimage and caption embeddings while pushing others apart, which improves\nrepresentation transferability and supports zero-shot prediction. However,\nmedical image-text datasets are orders of magnitude below the general images\nand captions from the internet. Moreover, previous methods encounter many false\nnegatives, i.e., images and reports from separate patients probably carry the\nsame semantics but are wrongly treated as negatives. In this paper, we decouple\nimages and texts for multimodal contrastive learning thus scaling the usable\ntraining data in a combinatorial magnitude with low cost. We also propose to\nreplace the InfoNCE loss with semantic matching loss based on medical knowledge\nto eliminate false negatives in contrastive learning. We prove that MedCLIP is\na simple yet effective framework: it outperforms state-of-the-art methods on\nzero-shot prediction, supervised classification, and image-text retrieval.\nSurprisingly, we observe that with only 20K pre-training data, MedCLIP wins\nover the state-of-the-art method (using around 200K data). Our code is\navailable at https://github.com/RyanWangZf/MedCLIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhenbang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1\">Dinesh Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jimeng Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity-Focused Dense Passage Retrieval for Outside-Knowledge Visual Question Answering. (arXiv:2210.10176v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10176","description":"<p>Most Outside-Knowledge Visual Question Answering (OK-VQA) systems employ a\ntwo-stage framework that first retrieves external knowledge given the visual\nquestion and then predicts the answer based on the retrieved content. However,\nthe retrieved knowledge is often inadequate. Retrievals are frequently too\ngeneral and fail to cover specific knowledge needed to answer the question.\nAlso, the naturally available supervision (whether the passage contains the\ncorrect answer) is weak and does not guarantee question relevancy. To address\nthese issues, we propose an Entity-Focused Retrieval (EnFoRe) model that\nprovides stronger supervision during training and recognizes question-relevant\nentities to help retrieve more specific knowledge. Experiments show that our\nEnFoRe model achieves superior retrieval performance on OK-VQA, the currently\nlargest outside-knowledge VQA dataset. We also combine the retrieved knowledge\nwith state-of-the-art VQA models, and achieve a new state-of-the-art\nperformance on OK-VQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jialin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1\">Raymond J. Mooney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple and Effective Unsupervised Speech Translation. (arXiv:2210.10191v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10191","description":"<p>The amount of labeled data to train models for speech tasks is limited for\nmost languages, however, the data scarcity is exacerbated for speech\ntranslation which requires labeled data covering two different languages. To\naddress this issue, we study a simple and effective approach to build speech\ntranslation systems without labeled data by leveraging recent advances in\nunsupervised speech recognition, machine translation and speech synthesis,\neither in a pipeline approach, or to generate pseudo-labels for training\nend-to-end speech translation models. Furthermore, we present an unsupervised\ndomain adaptation technique for pre-trained speech models which improves the\nperformance of downstream unsupervised speech recognition, especially for\nlow-resource settings. Experiments show that unsupervised speech-to-text\ntranslation outperforms the previous unsupervised state of the art by 3.2 BLEU\non the Libri-Trans benchmark, on CoVoST 2, our best systems outperform the best\nsupervised end-to-end models (without pre-training) from only two years ago by\nan average of 5.0 BLEU over five X-En directions. We also report competitive\nresults on MuST-C and CVSS benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng-Jen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulikov_I/0/1/0/all/0/1\">Ilia Kulikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Helpful Neighbors: Leveraging Neighbors in Geographic Feature Pronunciation. (arXiv:2210.10200v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10200","description":"<p>If one sees the place name Houston Mercer Dog Run in New York, how does one\nknow how to pronounce it? Assuming one knows that Houston in New York is\npronounced \"how-ston\" and not like the Texas city, then one can probably guess\nthat \"how-ston\" is also used in the name of the dog park. We present a novel\narchitecture that learns to use the pronunciations of neighboring names in\norder to guess the pronunciation of a given target feature. Applied to Japanese\nplace names, we demonstrate the utility of the model to finding and proposing\ncorrections for errors in Google Maps.\n</p>\n<p>To demonstrate the utility of this approach to structurally similar problems,\nwe also report on an application to a totally different task: Cognate reflex\nprediction in comparative historical linguistics. A version of the code has\nbeen open-sourced\n(https://github.com/google-research/google-research/tree/master/cognate_inpaint_neighbors).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jones_L/0/1/0/all/0/1\">Llion Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sproat_R/0/1/0/all/0/1\">Richard Sproat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishikawa_H/0/1/0/all/0/1\">Haruko Ishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutkin_A/0/1/0/all/0/1\">Alexander Gutkin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exclusive Supermask Subnetwork Training for Continual Learning. (arXiv:2210.10209v1 [cs.CV])","link":"http://arxiv.org/abs/2210.10209","description":"<p>Continual Learning (CL) methods mainly focus on avoiding catastrophic\nforgetting and learning representations that are transferable to new tasks.\nRecently, Wortsman et al. (2020) proposed a CL method, SupSup, which uses a\nrandomly initialized, fixed base network (model) and finds a supermask for each\nnew task that selectively keeps or removes each weight to produce a subnetwork.\nThey prevent forgetting as the network weights are not being updated. Although\nthere is no forgetting, the performance of the supermask is sub-optimal because\nfixed weights restrict its representational power. Furthermore, there is no\naccumulation or transfer of knowledge inside the model when new tasks are\nlearned. Hence, we propose ExSSNeT (Exclusive Supermask SubNEtwork Training),\nwhich performs exclusive and non-overlapping subnetwork weight training. This\navoids conflicting updates to the shared weights by subsequent tasks to improve\nperformance while still preventing forgetting. Furthermore, we propose a novel\nKNN-based Knowledge Transfer (KKT) module that dynamically initializes a new\ntask's mask based on previous tasks for improving knowledge transfer. We\ndemonstrate that ExSSNeT outperforms SupSup and other strong previous methods\non both text classification and vision tasks while preventing forgetting.\nMoreover, ExSSNeT is particularly advantageous for sparse masks that activate\n2-10% of the model parameters, resulting in an average improvement of 8.3% over\nSupSup. Additionally, ExSSNeT scales to a large number of tasks (100), and our\nKKT module helps to learn new tasks faster while improving overall performance.\nOur code is available at https://github.com/prateeky2806/exessnet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1\">Prateek Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable Slot Type Attentions to Improve Joint Intent Detection and Slot Filling. (arXiv:2210.10227v1 [cs.LG])","link":"http://arxiv.org/abs/2210.10227","description":"<p>Joint intent detection and slot filling is a key research topic in natural\nlanguage understanding (NLU). Existing joint intent and slot filling systems\nanalyze and compute features collectively for all slot types, and importantly,\nhave no way to explain the slot filling model decisions. In this work, we\npropose a novel approach that: (i) learns to generate additional slot type\nspecific features in order to improve accuracy and (ii) provides explanations\nfor slot filling decisions for the first time in a joint NLU model. We perform\nan additional constrained supervision using a set of binary classifiers for the\nslot type specific feature learning, thus ensuring appropriate attention\nweights are learned in the process to explain slot filling decisions for\nutterances. Our model is inherently explainable and does not need any post-hoc\nprocessing. We evaluate our approach on two widely used datasets and show\naccuracy improvements. Moreover, a detailed analysis is also provided for the\nexclusive slot explainability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gunaratna_K/0/1/0/all/0/1\">Kalpa Gunaratna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_V/0/1/0/all/0/1\">Vijay Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yerukola_A/0/1/0/all/0/1\">Akhila Yerukola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hongxia Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speaker- and Age-Invariant Training for Child Acoustic Modeling Using Adversarial Multi-Task Learning. (arXiv:2210.10231v1 [cs.SD])","link":"http://arxiv.org/abs/2210.10231","description":"<p>One of the major challenges in acoustic modelling of child speech is the\nrapid changes that occur in the children's articulators as they grow up, their\ndiffering growth rates and the subsequent high variability in the same age\ngroup. These high acoustic variations along with the scarcity of child speech\ncorpora have impeded the development of a reliable speech recognition system\nfor children. In this paper, a speaker- and age-invariant training approach\nbased on adversarial multi-task learning is proposed. The system consists of\none generator shared network that learns to generate speaker- and age-invariant\nfeatures connected to three discrimination networks, for phoneme, age, and\nspeaker. The generator network is trained to minimize the\nphoneme-discrimination loss and maximize the speaker- and age-discrimination\nlosses in an adversarial multi-task learning fashion. The generator network is\na Time Delay Neural Network (TDNN) architecture while the three discriminators\nare feed-forward networks. The system was applied to the OGI speech corpora and\nachieved a 13% reduction in the WER of the ASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahin_M/0/1/0/all/0/1\">Mostafa Shahin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_B/0/1/0/all/0/1\">Beena Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Epps_J/0/1/0/all/0/1\">Julien Epps</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Type-supervised sequence labeling based on the heterogeneous star graph for named entity recognition. (arXiv:2210.10240v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10240","description":"<p>Named entity recognition is a fundamental task in natural language\nprocessing, identifying the span and category of entities in unstructured\ntexts. The traditional sequence labeling methodology ignores the nested\nentities, i.e. entities included in other entity mentions. Many approaches\nattempt to address this scenario, most of which rely on complex structures or\nhave high computation complexity. The representation learning of the\nheterogeneous star graph containing text nodes and type nodes is investigated\nin this paper. In addition, we revise the graph attention mechanism into a\nhybrid form to address its unreasonableness in specific topologies. The model\nperforms the type-supervised sequence labeling after updating nodes in the\ngraph. The annotation scheme is an extension of the single-layer sequence\nlabeling and is able to cope with the vast majority of nested entities.\nExtensive experiments on public NER datasets reveal the effectiveness of our\nmodel in extracting both flat and nested entities. The method achieved\nstate-of-the-art performance on both flat and nested datasets. The significant\nimprovement in accuracy reflects the superiority of the multi-layer labeling\nstrategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xueru Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Changjiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haotian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Luguang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Hong Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Data-Driven Investigation of Noise-Adaptive Utterance Generation with Linguistic Modification. (arXiv:2210.10252v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10252","description":"<p>In noisy environments, speech can be hard to understand for humans. Spoken\ndialog systems can help to enhance the intelligibility of their output, either\nby modifying the speech synthesis (e.g., imitate Lombard speech) or by\noptimizing the language generation. We here focus on the second type of\napproach, by which an intended message is realized with words that are more\nintelligible in a specific noisy environment. By conducting a speech perception\nexperiment, we created a dataset of 900 paraphrases in babble noise, perceived\nby native English speakers with normal hearing. We find that careful selection\nof paraphrases can improve intelligibility by 33% at SNR -5 dB. Our analysis of\nthe data shows that the intelligibility differences between paraphrases are\nmainly driven by noise-robust acoustic cues. Furthermore, we propose an\nintelligibility-aware paraphrase ranking model, which outperforms baseline\nmodels with a relative improvement of 31.37% at SNR -5 dB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chingacham_A/0/1/0/all/0/1\">Anupama Chingacham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demberg_V/0/1/0/all/0/1\">Vera Demberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continued Pretraining for Better Zero- and Few-Shot Promptability. (arXiv:2210.10258v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10258","description":"<p>Recently introduced language model prompting methods can achieve high\naccuracy in zero- and few-shot settings while requiring few to no learned\ntask-specific parameters. Nevertheless, these methods still often trail behind\nfull model finetuning. In this work, we investigate if a dedicated continued\npretraining stage could improve \"promptability\", i.e., zero-shot performance\nwith natural language prompts or few-shot performance with prompt tuning. We\nreveal settings where existing continued pretraining methods lack\npromptability. We also identify current methodological gaps, which we fill with\nthorough large-scale experiments. We demonstrate that a simple recipe,\ncontinued pretraining that incorporates a trainable prompt during multi-task\nlearning, leads to improved promptability in both zero- and few-shot settings\ncompared to existing methods, up to 31% relative. On the other hand, we find\nthat continued pretraining using MAML-style meta-learning, a method that\ndirectly optimizes few-shot promptability, yields subpar performance. We\nvalidate our findings with two prompt tuning methods, and, based on our\nresults, we provide concrete recommendations to optimize promptability for\ndifferent use cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhaofeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Logan_R/0/1/0/all/0/1\">Robert L. Logan IV</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walsh_P/0/1/0/all/0/1\">Pete Walsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagia_A/0/1/0/all/0/1\">Akshita Bhagia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groeneveld_D/0/1/0/all/0/1\">Dirk Groeneveld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Entity Detection with Proposer and Regressor. (arXiv:2210.10260v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10260","description":"<p>Named entity recognition is a traditional task in natural language\nprocessing. In particular, nested entity recognition receives extensive\nattention for the widespread existence of the nesting scenario. The latest\nresearch migrates the well-established paradigm of set prediction in object\ndetection to cope with entity nesting. However, the manual creation of query\nvectors, which fail to adapt to the rich semantic information in the context,\nlimits these approaches. An end-to-end entity detection approach with proposer\nand regressor is presented in this paper to tackle the issues. First, the\nproposer utilizes the feature pyramid network to generate high-quality entity\nproposals. Then, the regressor refines the proposals for generating the final\nprediction. The model adopts encoder-only architecture and thus obtains the\nadvantages of the richness of query semantics, high precision of entity\nlocalization, and easiness for model training. Moreover, we introduce the novel\nspatially modulated attention and progressive refinement for further\nimprovement. Extensive experiments demonstrate that our model achieves advanced\nperformance in flat and nested NER, achieving a new state-of-the-art F1 score\nof 80.74 on the GENIA dataset and 72.38 on the WeiboNER dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xueru Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Changjiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haotian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Luguang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Hong Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Model Decomposition: Quantifying the Dependency and Correlation of Language Models. (arXiv:2210.10289v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10289","description":"<p>Pre-trained language models (LMs), such as BERT (Devlin et al., 2018) and its\nvariants, have led to significant improvements on various NLP tasks in past\nyears. However, a theoretical framework for studying their relationships is\nstill missing. In this paper, we fill this gap by investigating the linear\ndependency between pre-trained LMs. The linear dependency of LMs is defined\nanalogously to the linear dependency of vectors. We propose Language Model\nDecomposition (LMD) to represent a LM using a linear combination of other LMs\nas basis, and derive the closed-form solution. A goodness-of-fit metric for LMD\nsimilar to the coefficient of determination is defined and used to measure the\nlinear dependency of a set of LMs. In experiments, we find that BERT and eleven\n(11) BERT-like LMs are 91% linearly dependent. This observation suggests that\ncurrent state-of-the-art (SOTA) LMs are highly \"correlated\". To further advance\nSOTA we need more diverse and novel LMs that are less dependent on existing\nLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Aspect Sentiment Quad Prediction via Template-Order Data Augmentation. (arXiv:2210.10291v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10291","description":"<p>Recently, aspect sentiment quad prediction (ASQP) has become a popular task\nin the field of aspect-level sentiment analysis. Previous work utilizes a\npredefined template to paraphrase the original sentence into a structure target\nsequence, which can be easily decoded as quadruplets of the form (aspect\ncategory, aspect term, opinion term, sentiment polarity). The template involves\nthe four elements in a fixed order. However, we observe that this solution\ncontradicts with the order-free property of the ASQP task, since there is no\nneed to fix the template order as long as the quadruplet is extracted\ncorrectly. Inspired by the observation, we study the effects of template orders\nand find that some orders help the generative model achieve better performance.\nIt is hypothesized that different orders provide various views of the\nquadruplet. Therefore, we propose a simple but effective method to identify the\nmost proper orders, and further combine multiple proper templates as data\naugmentation to improve the ASQP task. Specifically, we use the pre-trained\nlanguage model to select the orders with minimal entropy. By fine-tuning the\npre-trained language model with these template orders, our approach improves\nthe performance of quad prediction, and outperforms state-of-the-art methods\nsignificantly in low-resource settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Mengting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yike Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Hang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yinhao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shiwan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forging Multiple Training Objectives for Pre-trained Language Models via Meta-Learning. (arXiv:2210.10293v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10293","description":"<p>Multiple pre-training objectives fill the vacancy of the understanding\ncapability of single-objective language modeling, which serves the ultimate\npurpose of pre-trained language models (PrLMs), generalizing well on a mass of\nscenarios. However, learning multiple training objectives in a single model is\nchallenging due to the unknown relative significance as well as the potential\ncontrariety between them. Empirical studies have shown that the current\nobjective sampling in an ad-hoc manual setting makes the learned language\nrepresentation barely converge to the desired optimum. Thus, we propose\n\\textit{MOMETAS}, a novel adaptive sampler based on meta-learning, which learns\nthe latent sampling pattern on arbitrary pre-training objectives. Such a design\nis lightweight with negligible additional training overhead. To validate our\napproach, we adopt five objectives and conduct continual pre-training with\nBERT-base and BERT-large models, where MOMETAS demonstrates universal\nperformance gain over other rule-based sampling strategies on 14 natural\nlanguage processing tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hongqiu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1\">Ruixue Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boli Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Neural Network Model for Readability Assessment with Feature Projection and Length-Balanced Loss. (arXiv:2210.10305v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10305","description":"<p>For readability assessment, traditional methods mainly employ machine\nlearning classifiers with hundreds of linguistic features. Although the deep\nlearning model has become the prominent approach for almost all NLP tasks, it\nis less explored for readability assessment. In this paper, we propose a\nBERT-based model with feature projection and length-balanced loss (BERT-FP-LBL)\nfor readability assessment. Specially, we present a new difficulty knowledge\nguided semi-supervised method to extract topic features to complement the\ntraditional linguistic features. From the linguistic features, we employ\nprojection filtering to extract orthogonal features to supplement BERT\nrepresentations. Furthermore, we design a new length-balanced loss to handle\nthe greatly varying length distribution of data. Our model achieves\nstate-of-the-art performances on two English benchmark datasets and one dataset\nof Chinese textbooks, and also achieves the near-perfect accuracy of 99\\% on\none English dataset. Moreover, our proposed model obtains comparable results\nwith human experts in consistency test.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from the Dictionary: Heterogeneous Knowledge Guided Fine-tuning for Chinese Spell Checking. (arXiv:2210.10320v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10320","description":"<p>Chinese Spell Checking (CSC) aims to detect and correct Chinese spelling\nerrors. Recent researches start from the pretrained knowledge of language\nmodels and take multimodal information into CSC models to improve the\nperformance. However, they overlook the rich knowledge in the dictionary, the\nreference book where one can learn how one character should be pronounced,\nwritten, and used. In this paper, we propose the LEAD framework, which renders\nthe CSC model to learn heterogeneous knowledge from the dictionary in terms of\nphonetics, vision, and meaning. LEAD first constructs positive and negative\nsamples according to the knowledge of character phonetics, glyphs, and\ndefinitions in the dictionary. Then a unified contrastive learning-based\ntraining scheme is employed to refine the representations of the CSC models.\nExtensive experiments and detailed analyses on the SIGHAN benchmark datasets\ndemonstrate the effectiveness of our proposed methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shirong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhongli Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yangning_L/0/1/0/all/0/1\">Li Yangning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shulin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haitao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Stability of Fine-Tuning Pretrained Language Models via Component-Wise Gradient Norm Clipping. (arXiv:2210.10325v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10325","description":"<p>Fine-tuning over large pretrained language models (PLMs) has established many\nstate-of-the-art results. Despite its superior performance, such fine-tuning\ncan be unstable, resulting in significant variance in performance and potential\nrisks for practical applications. Previous works have attributed such\ninstability to the catastrophic forgetting problem in the top layers of PLMs,\nwhich indicates iteratively that fine-tuning layers in a top-down manner is a\npromising solution. In this paper, we first point out that this method does not\nalways work out due to the different convergence speeds of different\nlayers/modules. Inspired by this observation, we propose a simple\ncomponent-wise gradient norm clipping method to adjust the convergence speed\nfor different components. Experiment results demonstrate that our method\nachieves consistent improvements in terms of generalization performance,\nconvergence speed, and training stability. The codebase can be found at\nhttps://github.com/yangalan123/FineTuningStability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chenghao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xuezhe Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Detoxification with Attribute-Discriminative Latent Space. (arXiv:2210.10329v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10329","description":"<p>Transformer-based Language Models (LMs) achieve remarkable performances on a\nvariety of NLU tasks, but are also prone to generating toxic texts such as\ninsults, threats, and profanities which limit their adaptations to the\nreal-world applications. To overcome this issue, a few text generation\napproaches aim to detoxify toxic texts with additional LMs or perturbations.\nHowever, previous methods require excessive memory, computations, and time\nwhich are serious bottlenecks in their real-world application. To address such\nlimitations, we propose an effective yet efficient method for language\ndetoxification using an attribute-discriminative latent space. Specifically, we\nproject the latent space of an original Transformer LM to a discriminative\nlatent space on which the texts are well-separated by their attributes, with\nthe help of a projection block and a discriminator. This allows the LM to\ncontrol the text generation to be non-toxic with minimal memory and computation\noverhead. We validate our model, Attribute-Discriminative Language Model (ADLM)\non detoxified language and dialogue generation tasks, on which our method\nsignificantly outperforms baselines both in performance and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwak_J/0/1/0/all/0/1\">Jin Myung Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minseon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revision Transformers: Getting RiT of No-Nos. (arXiv:2210.10332v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10332","description":"<p>Current transformer language models (LM) are large-scale models with billions\nof parameters. They have been shown to provide high performances on a variety\nof tasks but are also prone to shortcut learning and bias. Addressing such\nincorrect model behavior via parameter adjustments is very costly. This is\nparticularly problematic for updating dynamic concepts, such as moral values,\nwhich vary culturally or interpersonally. In this work, we question the current\ncommon practice of storing all information in the model parameters and propose\nthe Revision Transformer (RiT) employing information retrieval to facilitate\neasy model updating. The specific combination of a large-scale pre-trained LM\nthat inherently but also diffusely encodes world knowledge with a\nclear-structured revision engine makes it possible to update the model's\nknowledge with little effort and the help of user interaction. We exemplify RiT\non a moral dataset and simulate user feedback demonstrating strong performance\nin model revision even with small data. This way, users can easily design a\nmodel regarding their preferences, paving the way for more transparent and\npersonalized AI models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_F/0/1/0/all/0/1\">Felix Friedrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stammer_W/0/1/0/all/0/1\">Wolfgang Stammer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1\">Patrick Schramowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Devil in Linear Transformer. (arXiv:2210.10340v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10340","description":"<p>Linear transformers aim to reduce the quadratic space-time complexity of\nvanilla transformers. However, they usually suffer from degraded performances\non various tasks and corpus. In this paper, we examine existing kernel-based\nlinear transformers and identify two key issues that lead to such performance\ngaps: 1) unbounded gradients in the attention computation adversely impact the\nconvergence of linear transformer models; 2) attention dilution which trivially\ndistributes attention scores over long sequences while neglecting neighbouring\nstructures. To address these issues, we first identify that the scaling of\nattention matrices is the devil in unbounded gradients, which turns out\nunnecessary in linear attention as we show theoretically and empirically. To\nthis end, we propose a new linear attention that replaces the scaling operation\nwith a normalization to stabilize gradients. For the issue of attention\ndilution, we leverage a diagonal attention to confine attention to only\nneighbouring tokens in early layers. Benefiting from the stable gradients and\nimproved attention, our new linear transformer model, transNormer, demonstrates\nsuperior performance on text classification and language modeling tasks, as\nwell as on the challenging Long-Range Arena benchmark, surpassing vanilla\ntransformer and existing linear variants by a clear margin while being\nsignificantly more space-time efficient. The code is available at\nhttps://github.com/OpenNLPLab/Transnormer .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">XiaoDong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weixuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongxu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiran Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining. (arXiv:2210.10341v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10341","description":"<p>Pre-trained language models have attracted increasing attention in the\nbiomedical domain, inspired by their great success in the general natural\nlanguage domain. Among the two main branches of pre-trained language models in\nthe general language domain, i.e., BERT (and its variants) and GPT (and its\nvariants), the first one has been extensively studied in the biomedical domain,\nsuch as BioBERT and PubMedBERT. While they have achieved great success on a\nvariety of discriminative downstream biomedical tasks, the lack of generation\nability constrains their application scope. In this paper, we propose BioGPT, a\ndomain-specific generative Transformer language model pre-trained on large\nscale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and\ndemonstrate that our model outperforms previous models on most tasks.\nEspecially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI\nend-to-end relation extraction tasks respectively, and 78.2% accuracy on\nPubMedQA, creating a new record. Our case study on text generation further\ndemonstrates the advantage of BioGPT on biomedical literature to generate\nfluent descriptions for biomedical terms. Code is available at\nhttps://github.com/microsoft/BioGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Liai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yingce Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EnTDA: Entity-to-Text based Data Augmentation Approach for Named Entity Recognition Tasks. (arXiv:2210.10343v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10343","description":"<p>Data augmentation techniques have been used to improve the generalization\ncapability of models in the named entity recognition (NER) tasks. Existing\naugmentation methods either manipulate the words in the original text that\nrequire hand-crafted in-domain knowledge, or leverage generative models which\nsolicit dependency order among entities. To alleviate the excessive reliance on\nthe dependency order among entities in existing augmentation paradigms, we\ndevelop an entity-to-text instead of text-to-entity based data augmentation\nmethod named: EnTDA to decouple the dependencies between entities by adding,\ndeleting, replacing and swapping entities, and adopt these augmented data to\nbootstrap the generalization ability of the NER model. Furthermore, we\nintroduce a diversity beam search to increase the diversity of the augmented\ndata. Experiments on thirteen NER datasets across three tasks (flat NER, nested\nNER, and discontinuous NER) and two settings (full data NER and low resource\nNER) show that EnTDA could consistently outperform the baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Museformer: Transformer with Fine- and Coarse-Grained Attention for Music Generation. (arXiv:2210.10349v1 [cs.SD])","link":"http://arxiv.org/abs/2210.10349","description":"<p>Symbolic music generation aims to generate music scores automatically. A\nrecent trend is to use Transformer or its variants in music generation, which\nis, however, suboptimal, because the full attention cannot efficiently model\nthe typically long music sequences (e.g., over 10,000 tokens), and the existing\nmodels have shortcomings in generating musical repetition structures. In this\npaper, we propose Museformer, a Transformer with a novel fine- and\ncoarse-grained attention for music generation. Specifically, with the\nfine-grained attention, a token of a specific bar directly attends to all the\ntokens of the bars that are most relevant to music structures (e.g., the\nprevious 1st, 2nd, 4th and 8th bars, selected via similarity statistics); with\nthe coarse-grained attention, a token only attends to the summarization of the\nother bars rather than each token of them so as to reduce the computational\ncost. The advantages are two-fold. First, it can capture both music\nstructure-related correlations via the fine-grained attention, and other\ncontextual information via the coarse-grained attention. Second, it is\nefficient and can model over 3X longer music sequences compared to its\nfull-attention counterpart. Both objective and subjective experimental results\ndemonstrate its ability to generate long music sequences with high quality and\nbetter structures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Botao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Peiling Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shikun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MuGER$^2$: Multi-Granularity Evidence Retrieval and Reasoning for Hybrid Question Answering. (arXiv:2210.10350v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10350","description":"<p>Hybrid question answering (HQA) aims to answer questions over heterogeneous\ndata, including tables and passages linked to table cells. The heterogeneous\ndata can provide different granularity evidence to HQA models, e.t., column,\nrow, cell, and link. Conventional HQA models usually retrieve coarse- or\nfine-grained evidence to reason the answer. Through comparison, we find that\ncoarse-grained evidence is easier to retrieve but contributes less to the\nreasoner, while fine-grained evidence is the opposite. To preserve the\nadvantage and eliminate the disadvantage of different granularity evidence, we\npropose MuGER$^2$, a Multi-Granularity Evidence Retrieval and Reasoning\napproach. In evidence retrieval, a unified retriever is designed to learn the\nmulti-granularity evidence from the heterogeneous data. In answer reasoning, an\nevidence selector is proposed to navigate the fine-grained evidence for the\nanswer reader based on the learned multi-granularity evidence. Experiment\nresults on the HybridQA dataset show that MuGER$^2$ significantly boosts the\nHQA performance. Further ablation analysis verifies the effectiveness of both\nthe retrieval and reasoning designs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chaoqun Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging a New Spanish Corpus for Multilingual and Crosslingual Metaphor Detection. (arXiv:2210.10358v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10358","description":"<p>The lack of wide coverage datasets annotated with everyday metaphorical\nexpressions for languages other than English is striking. This means that most\nresearch on supervised metaphor detection has been published only for that\nlanguage. In order to address this issue, this work presents the first corpus\nannotated with naturally occurring metaphors in Spanish large enough to develop\nsystems to perform metaphor detection. The presented dataset, CoMeta, includes\ntexts from various domains, namely, news, political discourse, Wikipedia and\nreviews. In order to label CoMeta, we apply the MIPVU method, the guidelines\nmost commonly used to systematically annotate metaphor on real data. We use our\nnewly created dataset to provide competitive baselines by fine-tuning several\nmultilingual and monolingual state-of-the-art large language models.\nFurthermore, by leveraging the existing VUAM English data in addition to\nCoMeta, we present the, to the best of our knowledge, first cross-lingual\nexperiments on supervised metaphor detection. Finally, we perform a detailed\nerror analysis that explores the seemingly high transfer of everyday metaphor\nacross these two languages and datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Bayona_E/0/1/0/all/0/1\">Elisa Sanchez-Bayona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agerri_R/0/1/0/all/0/1\">Rodrigo Agerri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Group is better than individual: Exploiting Label Topologies and Label Relations for Joint Multiple Intent Detection and Slot Filling. (arXiv:2210.10369v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10369","description":"<p>Recent joint multiple intent detection and slot filling models employ label\nembeddings to achieve the semantics-label interactions. However, they treat all\nlabels and label embeddings as uncorrelated individuals, ignoring the\ndependencies among them. Besides, they conduct the decoding for the two tasks\nindependently, without leveraging the correlations between them. Therefore, in\nthis paper, we first construct a Heterogeneous Label Graph (HLG) containing two\nkinds of topologies: (1) statistical dependencies based on labels'\nco-occurrence patterns and hierarchies in slot labels; (2) rich relations among\nthe label nodes. Then we propose a novel model termed ReLa-Net. It can capture\nbeneficial correlations among the labels from HLG. The label correlations are\nleveraged to enhance semantic-label interactions. Moreover, we also propose the\nlabel-aware inter-dependent decoding mechanism to further exploit the label\ncorrelations for decoding. Experiment results show that our ReLa-Net\nsignificantly outperforms previous models. Remarkably, ReLa-Net surpasses the\nprevious best model by over 20\\% in terms of overall accuracy on MixATIS\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xing_B/0/1/0/all/0/1\">Bowen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1\">Ivor W. Tsang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Co-guiding Net: Achieving Mutual Guidances between Multiple Intent Detection and Slot Filling via Heterogeneous Semantics-Label Graphs. (arXiv:2210.10375v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10375","description":"<p>Recent graph-based models for joint multiple intent detection and slot\nfilling have obtained promising results through modeling the guidance from the\nprediction of intents to the decoding of slot filling. However, existing\nmethods (1) only model the \\textit{unidirectional guidance} from intent to\nslot; (2) adopt \\textit{homogeneous graphs} to model the interactions between\nthe slot semantics nodes and intent label nodes, which limit the performance.\nIn this paper, we propose a novel model termed Co-guiding Net, which implements\na two-stage framework achieving the \\textit{mutual guidances} between the two\ntasks. In the first stage, the initial estimated labels of both tasks are\nproduced, and then they are leveraged in the second stage to model the mutual\nguidances. Specifically, we propose two \\textit{heterogeneous graph attention\nnetworks} working on the proposed two \\textit{heterogeneous semantics-label\ngraphs}, which effectively represent the relations among the semantics nodes\nand label nodes. Experiment results show that our model outperforms existing\nmodels by a large margin, obtaining a relative improvement of 19.3\\% over the\nprevious best model on MixATIS dataset in overall accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xing_B/0/1/0/all/0/1\">Bowen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1\">Ivor W. Tsang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tourist Guidance Robot Based on HyperCLOVA. (arXiv:2210.10400v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10400","description":"<p>This paper describes our system submitted to Dialogue Robot Competition 2022.\nOur proposed system is a combined model of rule-based and generation-based\ndialog systems. The system utilizes HyperCLOVA, a Japanese foundation model,\nnot only to generate responses but also summarization, search information, etc.\nWe also used our original speech recognition system, which was fine-tuned for\nthis dialog task. As a result, our system ranked second in the preliminary\nround and moved on to the finals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamazaki_T/0/1/0/all/0/1\">Takato Yamazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshikawa_K/0/1/0/all/0/1\">Katsumasa Yoshikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawamoto_T/0/1/0/all/0/1\">Toshiki Kawamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohagi_M/0/1/0/all/0/1\">Masaya Ohagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mizumoto_T/0/1/0/all/0/1\">Tomoya Mizumoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichimura_S/0/1/0/all/0/1\">Shuta Ichimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kida_Y/0/1/0/all/0/1\">Yusuke Kida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_T/0/1/0/all/0/1\">Toshinori Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid-Regressive Neural Machine Translation. (arXiv:2210.10416v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10416","description":"<p>In this work, we empirically confirm that non-autoregressive translation with\nan iterative refinement mechanism (IR-NAT) suffers from poor acceleration\nrobustness because it is more sensitive to decoding batch size and computing\ndevice setting than autoregressive translation (AT). Inspired by it, we attempt\nto investigate how to combine the strengths of autoregressive and\nnon-autoregressive translation paradigms better. To this end, we demonstrate\nthrough synthetic experiments that prompting a small number of AT's predictions\ncan promote one-shot non-autoregressive translation to achieve the equivalent\nperformance of IR-NAT. Following this line, we propose a new two-stage\ntranslation prototype called hybrid-regressive translation (HRT). Specifically,\nHRT first generates discontinuous sequences via autoregression (e.g., make a\nprediction every k tokens, k&gt;1) and then fills in all previously skipped tokens\nat once in a non-autoregressive manner. We also propose a bag of techniques to\neffectively and efficiently train HRT without adding any model parameters. HRT\nachieves the state-of-the-art BLEU score of 28.49 on the WMT En-De task and is\nat least 1.5x faster than AT, regardless of batch size and device. In addition,\nanother bonus of HRT is that it successfully inherits the good characteristics\nof AT in the deep-encoder-shallow-decoder architecture. Concretely, compared to\nthe vanilla HRT with a 6-layer encoder and 6-layer decoder, the inference speed\nof HRT with a 12-layer encoder and 1-layer decoder is further doubled on both\nGPU and CPU without BLEU loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinhui Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Ming Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Linguistic Investigation of Machine Learning based Contradiction Detection Models: An Empirical Analysis and Future Perspectives. (arXiv:2210.10434v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10434","description":"<p>We analyze two Natural Language Inference data sets with respect to their\nlinguistic features. The goal is to identify those syntactic and semantic\nproperties that are particularly hard to comprehend for a machine learning\nmodel. To this end, we also investigate the differences between a\ncrowd-sourced, machine-translated data set (SNLI) and a collection of text\npairs from internet sources. Our main findings are, that the model has\ndifficulty recognizing the semantic importance of prepositions and verbs,\nemphasizing the importance of linguistically aware pre-training tasks.\nFurthermore, it often does not comprehend antonyms and homonyms, especially if\nthose are depending on the context. Incomplete sentences are another problem,\nas well as longer paragraphs and rare words or phrases. The study shows that\nautomated language understanding requires a more informed approach, utilizing\nas much external knowledge as possible throughout the training process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pielka_M/0/1/0/all/0/1\">Maren Pielka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rode_F/0/1/0/all/0/1\">Felix Rode</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pucknat_L/0/1/0/all/0/1\">Lisa Pucknat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deusser_T/0/1/0/all/0/1\">Tobias Deu&#xdf;er</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifa_R/0/1/0/all/0/1\">Rafet Sifa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LightEA: A Scalable, Robust, and Interpretable Entity Alignment Framework via Three-view Label Propagation. (arXiv:2210.10436v1 [cs.AI])","link":"http://arxiv.org/abs/2210.10436","description":"<p>Entity Alignment (EA) aims to find equivalent entity pairs between KGs, which\nis the core step of bridging and integrating multi-source KGs. In this paper,\nwe argue that existing GNN-based EA methods inherit the inborn defects from\ntheir neural network lineage: weak scalability and poor interpretability.\nInspired by recent studies, we reinvent the Label Propagation algorithm to\neffectively run on KGs and propose a non-neural EA framework -- LightEA,\nconsisting of three efficient components: (i) Random Orthogonal Label\nGeneration, (ii) Three-view Label Propagation, and (iii) Sparse Sinkhorn\nIteration. According to the extensive experiments on public datasets, LightEA\nhas impressive scalability, robustness, and interpretability. With a mere tenth\nof time consumption, LightEA achieves comparable results to state-of-the-art\nmethods across all datasets and even surpasses them on many.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xin Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuanbin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_M/0/1/0/all/0/1\">Man Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linguistic Rules-Based Corpus Generation for Native Chinese Grammatical Error Correction. (arXiv:2210.10442v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10442","description":"<p>Chinese Grammatical Error Correction (CGEC) is both a challenging NLP task\nand a common application in human daily life. Recently, many data-driven\napproaches are proposed for the development of CGEC research. However, there\nare two major limitations in the CGEC field: First, the lack of high-quality\nannotated training corpora prevents the performance of existing CGEC models\nfrom being significantly improved. Second, the grammatical errors in widely\nused test sets are not made by native Chinese speakers, resulting in a\nsignificant gap between the CGEC models and the real application. In this\npaper, we propose a linguistic rules-based approach to construct large-scale\nCGEC training corpora with automatically generated grammatical errors.\nAdditionally, we present a challenging CGEC benchmark derived entirely from\nerrors made by native Chinese speakers in real-world scenarios. Extensive\nexperiments and detailed analyses not only demonstrate that the training data\nconstructed by our method effectively improves the performance of CGEC models,\nbut also reflect that our benchmark is an excellent resource for further\ndevelopment of the CGEC field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shirong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Rongyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shulin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Ding Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yangning_L/0/1/0/all/0/1\">Li Yangning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhongli Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haitao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GCDT: A Chinese RST Treebank for Multigenre and Multilingual Discourse Parsing. (arXiv:2210.10449v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10449","description":"<p>A lack of large-scale human-annotated data has hampered the hierarchical\ndiscourse parsing of Chinese. In this paper, we present GCDT, the largest\nhierarchical discourse treebank for Mandarin Chinese in the framework of\nRhetorical Structure Theory (RST). GCDT covers over 60K tokens across five\ngenres of freely available text, using the same relation inventory as\ncontemporary RST treebanks for English. We also report on this dataset's\nparsing experiments, including state-of-the-art (SOTA) scores for Chinese RST\nparsing and RST parsing on the English GUM dataset, using cross-lingual\ntraining in Chinese and English with multilingual embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Siyao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Janet Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeldes_A/0/1/0/all/0/1\">Amir Zeldes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attribution and Obfuscation of Neural Text Authorship: A Data Mining Perspective. (arXiv:2210.10488v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10488","description":"<p>Two interlocking research questions of growing interest and importance in\nprivacy research are Authorship Attribution (AA) and Authorship Obfuscation\n(AO). Given an artifact, especially a text t in question, an AA solution aims\nto accurately attribute t to its true author out of many candidate authors\nwhile an AO solution aims to modify t to hide its true authorship.\nTraditionally, the notion of authorship and its accompanying privacy concern is\nonly toward human authors. However, in recent years, due to the explosive\nadvancements in Neural Text Generation (NTG) techniques in NLP, capable of\nsynthesizing human-quality open-ended texts (so-called \"neural texts\"), one has\nto now consider authorships by humans, machines, or their combination. Due to\nthe implications and potential threats of neural texts when used maliciously,\nit has become critical to understand the limitations of traditional AA/AO\nsolutions and develop novel AA/AO solutions in dealing with neural texts. In\nthis survey, therefore, we make a comprehensive review of recent literature on\nthe attribution and obfuscation of neural text authorship from a Data Mining\nperspective, and share our view on their limitations and promising research\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uchendu_A/0/1/0/all/0/1\">Adaku Uchendu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thai Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongwon Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a neural architecture of language: Deep learning versus logistics of access in neural architectures for compositional processing. (arXiv:2210.10543v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10543","description":"<p>Recently, a number of articles have argued that deep learning models such as\nGPT could also capture key aspects of language processing in the human mind and\nbrain. However, I will argue that these models are not suitable as neural\nmodels of human language. Firstly, because they fail on fundamental boundary\nconditions, such as the amount of learning they require. This would in fact\nimply that the mechanisms of GPT and brain language processing are\nfundamentally different. Secondly, because they do not possess the logistics of\naccess needed for compositional and productive human language processing.\nNeural architectures could possess logistics of access based on small-world\nlike network structures, in which processing does not consist of symbol\nmanipulation but of controlling the flow of activation. In this view, two\ncomplementary approaches would be needed to investigate the relation between\nbrain and cognition. Investigating learning methods could reveal how 'learned\ncognition' as found in deep learning could develop in the brain. However,\nneural architectures with logistics of access should also be developed to\naccount for 'productive cognition' as required for natural or artificial human\nlanguage processing. Later on, these approaches could perhaps be combined to\nsee how such architectures could develop by learning and development from a\nsimpler basis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Velde_F/0/1/0/all/0/1\">Frank van der Velde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Does More Than Describe: On The Lack Of Figurative Speech in Text-To-Image Models. (arXiv:2210.10578v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10578","description":"<p>The impressive capacity shown by recent text-to-image diffusion models to\ngenerate high-quality pictures from textual input prompts has leveraged the\ndebate about the very definition of art. Nonetheless, these models have been\ntrained using text data collected from content-based labelling protocols that\nfocus on describing the items and actions in an image but neglect any\nsubjective appraisal. Consequently, these automatic systems need rigorous\ndescriptions of the elements and the pictorial style of the image to be\ngenerated, otherwise failing to deliver. As potential indicators of the actual\nartistic capabilities of current generative models, we characterise the\nsentimentality, objectiveness and degree of abstraction of publicly available\ntext data used to train current text-to-image diffusion models. Considering the\nsharp difference observed between their language style and that typically\nemployed in artistic contexts, we suggest generative models should incorporate\nadditional sources of subjective information in their training in order to\novercome (or at least to alleviate) some of their current limitations, thus\neffectively unleashing a truly artistic and creative generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kleinlein_R/0/1/0/all/0/1\">Ricardo Kleinlein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luna_Jimenez_C/0/1/0/all/0/1\">Cristina Luna-Jim&#xe9;nez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Martinez_F/0/1/0/all/0/1\">Fernando Fern&#xe1;ndez-Mart&#xed;nez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CEntRE: A paragraph-level Chinese dataset for Relation Extraction among Enterprises. (arXiv:2210.10581v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10581","description":"<p>Enterprise relation extraction aims to detect pairs of enterprise entities\nand identify the business relations between them from unstructured or\nsemi-structured text data, and it is crucial for several real-world\napplications such as risk analysis, rating research and supply chain security.\nHowever, previous work mainly focuses on getting attribute information about\nenterprises like personnel and corporate business, and pays little attention to\nenterprise relation extraction. To encourage further progress in the research,\nwe introduce the CEntRE, a new dataset constructed from publicly available\nbusiness news data with careful human annotation and intelligent data\nprocessing. Extensive experiments on CEntRE with six excellent models\ndemonstrate the challenges of our proposed dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peipei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yimo Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_F/0/1/0/all/0/1\">Fei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongsong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Limin Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced vectors for top-k document retrieval in Question Answering. (arXiv:2210.10584v1 [cs.IR])","link":"http://arxiv.org/abs/2210.10584","description":"<p>Modern day applications, especially information retrieval webapps that\ninvolve \"search\" as their use cases are gradually moving towards \"answering\"\nmodules. Conversational chatbots which have been proved to be more engaging to\nusers, use Question Answering as their core. Since, precise answering is\ncomputationally expensive, several approaches have been developed to prefetch\nthe most relevant documents/passages from the database that contain the answer.\nWe propose a different approach that retrieves the evidence documents\nefficiently and accurately, making sure that the relevant document for a given\nuser query is not missed. We do so by assigning each document (or passage in\nour case), a unique identifier and using them to create dense vectors which can\nbe efficiently indexed. More precisely, we use the identifier to predict\nrandomly sampled context window words of the relevant question corresponding to\nthe passage along with the words of passage itself. This naturally embeds the\npassage identifier into the vector space in such a way that the embedding is\ncloser to the question without compromising he information content. This\napproach enables efficient creation of real-time query vectors in ~4\nmilliseconds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hammad_M/0/1/0/all/0/1\">Mohammed Hammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Graph Masking Pre-training for Graph-to-Text Generation. (arXiv:2210.10599v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10599","description":"<p>Large-scale pre-trained language models (PLMs) have advanced Graph-to-Text\n(G2T) generation by processing the linearised version of a graph. However, the\nlinearisation is known to ignore the structural information. Additionally, PLMs\nare typically pre-trained on free text which introduces domain mismatch between\npre-training and downstream G2T generation tasks. To address these\nshortcomings, we propose graph masking pre-training strategies that neither\nrequire supervision signals nor adjust the architecture of the underlying\npre-trained encoder-decoder model. When used with a pre-trained T5, our\napproach achieves new state-of-the-art results on WebNLG+2020 and\nEventNarrative G2T generation datasets. Our method also shows to be very\neffective in the low-resource setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiuzhou Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shareghi_E/0/1/0/all/0/1\">Ehsan Shareghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NGEP: A Graph-based Event Planning Framework for Story Generation. (arXiv:2210.10602v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10602","description":"<p>To improve the performance of long text generation, recent studies have\nleveraged automatically planned event structures (i.e. storylines) to guide\nstory generation. Such prior works mostly employ end-to-end neural generation\nmodels to predict event sequences for a story. However, such generation models\nstruggle to guarantee the narrative coherence of separate events due to the\nhallucination problem, and additionally the generated event sequences are often\nhard to control due to the end-to-end nature of the models. To address these\nchallenges, we propose NGEP, an novel event planning framework which generates\nan event sequence by performing inference on an automatically constructed event\ngraph and enhances generalisation ability through a neural event advisor. We\nconduct a range of experiments on multiple criteria, and the results\ndemonstrate that our graph-based neural framework outperforms the\nstate-of-the-art (SOTA) event planning approaches, considering both the\nperformance of event sequence generation and the effectiveness on the\ndownstream task of story generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loakman_T/0/1/0/all/0/1\">Tyler Loakman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1\">Frank Guerin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DALLE-2 is Seeing Double: Flaws in Word-to-Concept Mapping in Text2Image Models. (arXiv:2210.10606v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10606","description":"<p>We study the way DALLE-2 maps symbols (words) in the prompt to their\nreferences (entities or properties of entities in the generated image). We show\nthat in stark contrast to the way human process language, DALLE-2 does not\nfollow the constraint that each word has a single role in the interpretation,\nand sometimes re-use the same symbol for different purposes. We collect a set\nof stimuli that reflect the phenomenon: we show that DALLE-2 depicts both\nsenses of nouns with multiple senses at once; and that a given word can modify\nthe properties of two distinct entities in the image, or can be depicted as one\nobject and also modify the properties of another object, creating a semantic\nleakage of properties between entities. Taken together, our study highlights\nthe differences between DALLE-2 and human language processing and opens an\navenue for future study on the inductive biases of text-to-image models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rassin_R/0/1/0/all/0/1\">Royi Rassin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Chinese Story Generation via Awareness of Syntactic Dependencies and Semantics. (arXiv:2210.10618v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10618","description":"<p>Story generation aims to generate a long narrative conditioned on a given\ninput. In spite of the success of prior works with the application of\npre-trained models, current neural models for Chinese stories still struggle to\ngenerate high-quality long text narratives. We hypothesise that this stems from\nambiguity in syntactically parsing the Chinese language, which does not have\nexplicit delimiters for word segmentation. Consequently, neural models suffer\nfrom the inefficient capturing of features in Chinese narratives. In this\npaper, we present a new generation framework that enhances the feature\ncapturing mechanism by informing the generation model of dependencies between\nwords and additionally augmenting the semantic representation learning through\nsynonym denoising training. We conduct a range of experiments, and the results\ndemonstrate that our framework outperforms the state-of-the-art Chinese\ngeneration models on all evaluation metrics, demonstrating the benefits of\nenhanced dependency and semantic representation learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Henglin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loakman_T/0/1/0/all/0/1\">Tyler Loakman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1\">Frank Guerin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RankT5: Fine-Tuning T5 for Text Ranking with Ranking Losses. (arXiv:2210.10634v1 [cs.IR])","link":"http://arxiv.org/abs/2210.10634","description":"<p>Recently, substantial progress has been made in text ranking based on\npretrained language models such as BERT. However, there are limited studies on\nhow to leverage more powerful sequence-to-sequence models such as T5. Existing\nattempts usually formulate text ranking as classification and rely on\npostprocessing to obtain a ranked list. In this paper, we propose RankT5 and\nstudy two T5-based ranking model structures, an encoder-decoder and an\nencoder-only one, so that they not only can directly output ranking scores for\neach query-document pair, but also can be fine-tuned with \"pairwise\" or\n\"listwise\" ranking losses to optimize ranking performances. Our experiments\nshow that the proposed models with ranking losses can achieve substantial\nranking performance gains on different public text ranking data sets. Moreover,\nwhen fine-tuned with listwise ranking losses, the ranking model appears to have\nbetter zero-shot ranking performance on out-of-domain data sets compared to the\nmodel fine-tuned with classification losses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_H/0/1/0/all/0/1\">Honglei Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagerman_R/0/1/0/all/0/1\">Rolf Jagerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_K/0/1/0/all/0/1\">Kai Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Ji Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jianmo Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuanhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bendersky_M/0/1/0/all/0/1\">Michael Bendersky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"N-Best Hypotheses Reranking for Text-To-SQL Systems. (arXiv:2210.10668v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10668","description":"<p>Text-to-SQL task maps natural language utterances to structured queries that\ncan be issued to a database. State-of-the-art (SOTA) systems rely on finetuning\nlarge, pre-trained language models in conjunction with constrained decoding\napplying a SQL parser. On the well established Spider dataset, we begin with\nOracle studies: specifically, choosing an Oracle hypothesis from a SOTA model's\n10-best list, yields a $7.7\\%$ absolute improvement in both exact match (EM)\nand execution (EX) accuracy, showing significant potential improvements with\nreranking. Identifying coherence and correctness as reranking approaches, we\ndesign a model generating a query plan and propose a heuristic schema linking\nalgorithm. Combining both approaches, with T5-Large, we obtain a consistent\n$1\\% $ improvement in EM accuracy, and a $~2.5\\%$ improvement in EX,\nestablishing a new SOTA for this task. Our comprehensive error studies on DEV\ndata show the underlying difficulty in making progress on this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1\">Lu Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathi_S/0/1/0/all/0/1\">Sree Hari Krishnan Parthasarathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Learning for Analyzing Political Campaigns on Facebook. (arXiv:2210.10669v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10669","description":"<p>Social media platforms are currently the main channel for political\nmessaging, allowing politicians to target specific demographics and adapt based\non their reactions. However, making this communication transparent is\nchallenging, as the messaging is tightly coupled with its intended audience and\noften echoed by multiple stakeholders interested in advancing specific\npolicies. Our goal in this paper is to take a first step towards understanding\nthese highly decentralized settings. We propose a weakly supervised approach to\nidentify the stance and issue of political ads on Facebook and analyze how\npolitical campaigns use some kind of demographic targeting by location, gender,\nor age. Furthermore, we analyze the temporal dynamics of the political ads on\nelection polls.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tunazzina Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Shamik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arabic Word-level Readability Visualization for Assisted Text Simplification. (arXiv:2210.10672v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10672","description":"<p>This demo paper presents a Google Docs add-on for automatic Arabic word-level\nreadability visualization. The add-on includes a lemmatization component that\nis connected to a five-level readability lexicon and Arabic WordNet-based\nsubstitution suggestions. The add-on can be used for assessing the reading\ndifficulty of a text and identifying difficult words as part of the task of\nmanual text simplification. We make our add-on and its code publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hazim_R/0/1/0/all/0/1\">Reem Hazim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saddiki_H/0/1/0/all/0/1\">Hind Saddiki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhafni_B/0/1/0/all/0/1\">Bashar Alhafni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalil_M/0/1/0/all/0/1\">Muhamed Al Khalil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study. (arXiv:2210.10678v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10678","description":"<p>This paper presents an empirical study to build relation extraction systems\nin low-resource settings. Based upon recent pre-trained language models, we\ncomprehensively investigate three schemes to evaluate the performance in\nlow-resource settings: (i) different types of prompt-based methods with\nfew-shot labeled data; (ii) diverse balancing methods to address the\nlong-tailed distribution issue; (iii) data augmentation technologies and\nself-training to generate more labeled in-domain data. We create a benchmark\nwith 8 relation extraction (RE) datasets covering different languages, domains\nand contexts and perform extensive comparisons over the proposed schemes with\ncombinations. Our experiments illustrate: (i) Though prompt-based tuning is\nbeneficial in low-resource RE, there is still much potential for improvement,\nespecially in extracting relations from cross-sentence contexts with multiple\nrelational triples; (ii) Balancing methods are not always helpful for RE with\nlong-tailed distribution; (iii) Data augmentation complements existing\nbaselines and can bring much performance gain, while self-training may not\nconsistently achieve advancement to low-resource RE. Code and datasets are in\nhttps://github.com/zjunlp/LREBench.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why Should Adversarial Perturbations be Imperceptible? Rethink the Research Paradigm in Adversarial NLP. (arXiv:2210.10683v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10683","description":"<p>Textual adversarial samples play important roles in multiple subfields of NLP\nresearch, including security, evaluation, explainability, and data\naugmentation. However, most work mixes all these roles, obscuring the problem\ndefinitions and research goals of the security role that aims to reveal the\npractical concerns of NLP models. In this paper, we rethink the research\nparadigm of textual adversarial samples in security scenarios. We discuss the\ndeficiencies in previous work and propose our suggestions that the research on\nthe Security-oriented adversarial NLP (SoadNLP) should: (1) evaluate their\nmethods on security tasks to demonstrate the real-world concerns; (2) consider\nreal-world attackers' goals, instead of developing impractical methods. To this\nend, we first collect, process, and release a security datasets collection\nAdvbench. Then, we reformalize the task and adjust the emphasis on different\ngoals in SoadNLP. Next, we propose a simple method based on heuristic rules\nthat can easily fulfill the actual adversarial goals to simulate real-world\nattack methods. We conduct experiments on both the attack and the defense sides\non Advbench. Experimental results show that our method has higher practical\nvalue, indicating that the research paradigm in SoadNLP may start from our new\nbenchmark. All the code and data of Advbench can be obtained at\n\\url{https://github.com/thunlp/Advbench}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Hongcheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1\">Ganqu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1\">Fanchao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Longtao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models Understand Us, Poorly. (arXiv:2210.10684v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10684","description":"<p>Some claim language models understand us. Others won't hear it. To clarify, I\ninvestigate three views of human language understanding: as-mapping,\nas-reliability and as-representation. I argue that while behavioral reliability\nis necessary for understanding, internal representations are sufficient; they\nclimb the right hill. I review state-of-the-art language and multi-modal\nmodels: they are pragmatically challenged by under-specification of form. I\nquestion the Scaling Paradigm: limits on resources may prohibit scaled-up\nmodels from approaching understanding. Last, I describe how as-representation\nadvances a science of understanding. We need work which probes model internals,\nadds more of human language, and measures what models can learn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1\">Jared Moore</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Procedural Fairness: Uncovering Biases in How a Toxic Language Classifier Uses Sentiment Information. (arXiv:2210.10689v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10689","description":"<p>Previous works on the fairness of toxic language classifiers compare the\noutput of models with different identity terms as input features but do not\nconsider the impact of other important concepts present in the context. Here,\nbesides identity terms, we take into account high-level latent features learned\nby the classifier and investigate the interaction between these features and\nidentity terms. For a multi-class toxic language classifier, we leverage a\nconcept-based explanation framework to calculate the sensitivity of the model\nto the concept of sentiment, which has been used before as a salient feature\nfor toxic language detection. Our results show that although for some classes,\nthe classifier has learned the sentiment information as expected, this\ninformation is outweighed by the influence of identity terms as input features.\nThis work is a step towards evaluating procedural fairness, where unfair\nprocesses lead to unfair outcomes. The produced knowledge can guide debiasing\ntechniques to ensure that important concepts besides identity terms are\nwell-represented in training datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nejadgholi_I/0/1/0/all/0/1\">Isar Nejadgholi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balkir_E/0/1/0/all/0/1\">Esma Balk&#x131;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_K/0/1/0/all/0/1\">Kathleen C. Fraser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiritchenko_S/0/1/0/all/0/1\">Svetlana Kiritchenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Separating Grains from the Chaff: Using Data Filtering to Improve Multilingual Translation for Low-Resourced African Languages. (arXiv:2210.10692v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10692","description":"<p>We participated in the WMT 2022 Large-Scale Machine Translation Evaluation\nfor the African Languages Shared Task. This work describes our approach, which\nis based on filtering the given noisy data using a sentence-pair classifier\nthat was built by fine-tuning a pre-trained language model. To train the\nclassifier, we obtain positive samples (i.e. high-quality parallel sentences)\nfrom a gold-standard curated dataset and extract negative samples (i.e.\nlow-quality parallel sentences) from automatically aligned parallel data by\nchoosing sentences with low alignment scores. Our final machine translation\nmodel was then trained on filtered data, instead of the entire noisy dataset.\nWe empirically validate our approach by evaluating on two common datasets and\nshow that data filtering generally improves overall translation quality, in\nsome cases even significantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdulmumin_I/0/1/0/all/0/1\">Idris Abdulmumin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beukman_M/0/1/0/all/0/1\">Michael Beukman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba O. Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1\">Chris Emezue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asiko_E/0/1/0/all/0/1\">Everlyn Asiko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adewumi_T/0/1/0/all/0/1\">Tosin Adewumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen Hassan Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeyemi_M/0/1/0/all/0/1\">Mofetoluwa Adeyemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yousuf_O/0/1/0/all/0/1\">Oreen Yousuf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sahib Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwadabe_T/0/1/0/all/0/1\">Tajuddeen Rabiu Gwadabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness of Demonstration-based Learning Under Limited Data Scenario. (arXiv:2210.10693v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10693","description":"<p>Demonstration-based learning has shown great potential in stimulating\npretrained language models' ability under limited data scenario. Simply\naugmenting the input with some demonstrations can significantly improve\nperformance on few-shot NER. However, why such demonstrations are beneficial\nfor the learning process remains unclear since there is no explicit alignment\nbetween the demonstrations and the predictions. In this paper, we design\npathological demonstrations by gradually removing intuitively useful\ninformation from the standard ones to take a deep dive of the robustness of\ndemonstration-based sequence labeling and show that (1) demonstrations composed\nof random tokens still make the model a better few-shot learner; (2) the length\nof random demonstrations and the relevance of random tokens are the main\nfactors affecting the performance; (3) demonstrations increase the confidence\nof model predictions on captured superficial patterns. We have publicly\nreleased our code at https://github.com/SALT-NLP/RobustDemo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanzhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruiyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Relevance Feedback for Information-Seeking Retrieval using Few-Shot Document Re-Ranking. (arXiv:2210.10695v1 [cs.IR])","link":"http://arxiv.org/abs/2210.10695","description":"<p>Pairing a lexical retriever with a neural re-ranking model has set\nstate-of-the-art performance on large-scale information retrieval datasets.\nThis pipeline covers scenarios like question answering or navigational queries,\nhowever, for information-seeking scenarios, users often provide information on\nwhether a document is relevant to their query in form of clicks or explicit\nfeedback. Therefore, in this work, we explore how relevance feedback can be\ndirectly integrated into neural re-ranking models by adopting few-shot and\nparameter-efficient learning techniques. Specifically, we introduce a kNN\napproach that re-ranks documents based on their similarity with the query and\nthe documents the user considers relevant. Further, we explore Cross-Encoder\nmodels that we pre-train using meta-learning and subsequently fine-tune for\neach query, training only on the feedback documents. To evaluate our different\nintegration strategies, we transform four existing information retrieval\ndatasets into the relevance feedback scenario. Extensive experiments\ndemonstrate that integrating relevance feedback directly in neural re-ranking\nmodels improves their performance, and fusing lexical ranking with our best\nperforming neural re-ranker outperforms all other methods by 5.2 nDCG@20.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baumgartner_T/0/1/0/all/0/1\">Tim Baumg&#xe4;rtner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_L/0/1/0/all/0/1\">Leonardo F. R. Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1\">Nils Reimers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Schema-aware Reference as Prompt Improves Data-Efficient Relational Triple and Event Extraction. (arXiv:2210.10709v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10709","description":"<p>Information Extraction, which aims to extract structural relational triple or\nevent from unstructured texts, often suffers from data scarcity issues. With\nthe development of pre-trained language models, many prompt-based approaches to\ndata-efficient information extraction have been proposed and achieved\nimpressive performance. However, existing prompt learning methods for\ninformation extraction are still susceptible to several potential limitations:\n(i) semantic gap between natural language and output structure knowledge with\npre-defined schema; (ii) representation learning with locally individual\ninstances limits the performance given the insufficient features. In this\npaper, we propose a novel approach of schema-aware Reference As Prompt (RAP),\nwhich dynamically leverage schema and knowledge inherited from global\n(few-shot) training data for each sample. Specifically, we propose a\nschema-aware reference store, which unifies symbolic schema and relevant\ntextual instances. Then, we employ a dynamic reference integration module to\nretrieve pertinent knowledge from the datastore as prompts during training and\ninference. Experimental results demonstrate that RAP can be plugged into\nvarious existing models and outperforms baselines in low-resource settings on\nfive datasets of relational triple extraction and event extraction. In\naddition, we provide comprehensive empirical ablations and case analysis\nregarding different types and scales of knowledge in order to better understand\nthe mechanisms of RAP. Code is available in https://github.com/zjunlp/RAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shengyu Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniNL: Aligning Representation Learning with Scoring Function for OOD Detection via Unified Neighborhood Learning. (arXiv:2210.10722v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10722","description":"<p>Detecting out-of-domain (OOD) intents from user queries is essential for\navoiding wrong operations in task-oriented dialogue systems. The key challenge\nis how to distinguish in-domain (IND) and OOD intents. Previous methods ignore\nthe alignment between representation learning and scoring function, limiting\nthe OOD detection performance. In this paper, we propose a unified neighborhood\nlearning framework (UniNL) to detect OOD intents. Specifically, we design a\nK-nearest neighbor contrastive learning (KNCL) objective for representation\nlearning and introduce a KNN-based scoring function for OOD detection. We aim\nto align representation learning with scoring function. Experiments and\nanalysis on two benchmark datasets show the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mou_Y/0/1/0/all/0/1\">Yutao Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Keqing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiran Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TabLLM: Few-shot Classification of Tabular Data with Large Language Models. (arXiv:2210.10723v1 [cs.CL])","link":"http://arxiv.org/abs/2210.10723","description":"<p>We study the application of large language models to zero-shot and few-shot\nclassification of tabular data. We prompt the large language model with a\nserialization of the tabular data to a natural-language string, together with a\nshort description of the classification problem. In the few-shot setting, we\nfine-tune the large language model using some labeled examples. We evaluate\nseveral serialization methods including templates, table-to-text models, and\nlarge language models. Despite its simplicity, we find that this technique\noutperforms prior deep-learning-based tabular classification methods on several\nbenchmark datasets. In most cases, even zero-shot classification obtains\nnon-trivial performance, illustrating the method's ability to exploit prior\nknowledge encoded in large language models. Unlike many deep learning methods\nfor tabular datasets, this approach is also competitive with strong traditional\nbaselines like gradient-boosted trees, especially in the very-few-shot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hegselmann_S/0/1/0/all/0/1\">Stefan Hegselmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buendia_A/0/1/0/all/0/1\">Alejandro Buendia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_H/0/1/0/all/0/1\">Hunter Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_M/0/1/0/all/0/1\">Monica Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1\">David Sontag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D-REX: Dialogue Relation Extraction with Explanations. (arXiv:2109.05126v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05126","description":"<p>Existing research studies on cross-sentence relation extraction in long-form\nmulti-party conversations aim to improve relation extraction without\nconsidering the explainability of such methods. This work addresses that gap by\nfocusing on extracting explanations that indicate that a relation exists while\nusing only partially labeled data. We propose our model-agnostic framework,\nD-REX, a policy-guided semi-supervised algorithm that explains and ranks\nrelations. We frame relation extraction as a re-ranking task and include\nrelation- and entity-specific explanations as an intermediate step of the\ninference process. We find that about 90% of the time, human annotators prefer\nD-REX's explanations over a strong BERT-based joint relation extraction and\nexplanation model. Finally, our evaluations on a dialogue relation extraction\ndataset show that our method is simple yet effective and achieves a\nstate-of-the-art F1 score on relation extraction, improving upon existing\nmethods by 13.5%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albalak_A/0/1/0/all/0/1\">Alon Albalak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Embar_V/0/1/0/all/0/1\">Varun Embar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuan_Y/0/1/0/all/0/1\">Yi-Lin Tuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Getoor_L/0/1/0/all/0/1\">Lise Getoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation. (arXiv:2109.09519v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.09519","description":"<p>To explore the limit of dialogue generation pre-training, we present the\nmodels of PLATO-XL with up to 11 billion parameters, trained on both Chinese\nand English social media conversations. To train such large models, we adopt\nthe architecture of unified transformer with high computation and parameter\nefficiency. In addition, we carry out multi-party aware pre-training to better\ndistinguish the characteristic information in social media conversations. With\nsuch designs, PLATO-XL successfully achieves superior performances as compared\nto other approaches in both Chinese and English chitchat. We further explore\nthe capacity of PLATO-XL on other conversational tasks, such as knowledge\ngrounded dialogue and task-oriented conversation. The experimental results\nindicate that PLATO-XL obtains state-of-the-art results across multiple\nconversational tasks, verifying its potential as a foundation model of\nconversational AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1\">Siqi Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Huang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenquan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhihua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hua Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinxian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xinchao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yingzhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zheng-Yu Niu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Textual Backdoor Attacks Can Be More Harmful via Two Simple Tricks. (arXiv:2110.08247v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2110.08247","description":"<p>Backdoor attacks are a kind of emergent security threat in deep learning.\nAfter being injected with a backdoor, a deep neural model will behave normally\non standard inputs but give adversary-specified predictions once the input\ncontains specific backdoor triggers. In this paper, we find two simple tricks\nthat can make existing textual backdoor attacks much more harmful. The first\ntrick is to add an extra training task to distinguish poisoned and clean data\nduring the training of the victim model, and the second one is to use all the\nclean training data rather than remove the original clean data corresponding to\nthe poisoned data. These two tricks are universally applicable to different\nattack models. We conduct experiments in three tough situations including clean\ndata fine-tuning, low-poisoning-rate, and label-consistent attacks.\nExperimental results show that the two tricks can significantly improve attack\nperformance. This paper exhibits the great potential harmfulness of backdoor\nattacks. All the code and data can be obtained at\n\\url{https://github.com/thunlp/StyleAttack}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1\">Fanchao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Hongcheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples. (arXiv:2111.10962v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.10962","description":"<p>Knowledge-enhanced language representation learning has shown promising\nresults across various knowledge-intensive NLP tasks. However, prior methods\nare limited in efficient utilization of multilingual knowledge graph (KG) data\nfor language model (LM) pretraining. They often train LMs with KGs in indirect\nways, relying on extra entity/relation embeddings to facilitate knowledge\ninjection. In this work, we explore methods to make better use of the\nmultilingual annotation and language agnostic property of KG triples, and\npresent novel knowledge based multilingual language models (KMLMs) trained\ndirectly on the knowledge triples. We first generate a large amount of\nmultilingual synthetic sentences using the Wikidata KG triples. Then based on\nthe intra- and inter-sentence structures of the generated data, we design\npretraining tasks to enable the LMs to not only memorize the factual knowledge\nbut also learn useful logical patterns. Our pretrained KMLMs demonstrate\nsignificant performance improvements on a wide range of knowledge-intensive\ncross-lingual tasks, including named entity recognition (NER), factual\nknowledge retrieval, relation classification, and a newly designed logical\nreasoning task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruidan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VIRT: Improving Representation-based Models for Text Matching through Virtual Interaction. (arXiv:2112.04195v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.04195","description":"<p>With the booming of pre-trained transformers, representation-based models\nbased on Siamese transformer encoders have become mainstream techniques for\nefficient text matching. However, these models suffer from severe performance\ndegradation due to the lack of interaction between the text pair, compared with\ninteraction-based models. Prior arts attempt to address this through performing\nextra interaction for Siamese encoded representations, while the interaction\nduring encoding is still ignored. To remedy this, we propose a \\textit{Virtual}\nInteRacTion mechanism (VIRT) to transfer interactive knowledge from\ninteraction-based models into Siamese encoders through attention map\ndistillation. As a train-time-only component, VIRT could completely maintain\nthe high efficiency of the Siamese structure and brings no extra computation\ncost during inference. To fully utilize the learned interactive knowledge, we\nfurther design a VIRT-adapted interaction strategy. Experimental results on\nmultiple text matching datasets demonstrate that our method outperforms\nstate-of-the-art representation-based models. What's more, VIRT can be easily\nintegrated into existing representation-based methods to achieve further\nimprovements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hongyin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Video Summarization via Multimodal Self-supervised Learning. (arXiv:2201.02494v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02494","description":"<p>Modern video summarization methods are based on deep neural networks that\nrequire a large amount of annotated data for training. However, existing\ndatasets for video summarization are small-scale, easily leading to\nover-fitting of the deep models. Considering that the annotation of large-scale\ndatasets is time-consuming, we propose a multimodal self-supervised learning\nframework to obtain semantic representations of videos, which benefits the\nvideo summarization task. Specifically, the self-supervised learning is\nconducted by exploring the semantic consistency between the videos and text in\nboth coarse-grained and fine-grained fashions, as well as recovering masked\nframes in the videos. The multimodal framework is trained on a newly-collected\ndataset that consists of video-text pairs. Additionally, we introduce a\nprogressive video summarization method, where the important content in a video\nis pinpointed progressively to generate better summaries. Extensive experiments\nhave proved the effectiveness and superiority of our method in rank correlation\ncoefficients and F-score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haopeng_L/0/1/0/all/0/1\">Li Haopeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiuhong_K/0/1/0/all/0/1\">Ke Qiuhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mingming_G/0/1/0/all/0/1\">Gong Mingming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drummond_T/0/1/0/all/0/1\">Tom Drummond</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COLD: A Benchmark for Chinese Offensive Language Detection. (arXiv:2201.06025v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.06025","description":"<p>Offensive language detection is increasingly crucial for maintaining a\ncivilized social media platform and deploying pre-trained language models.\nHowever, this task in Chinese is still under exploration due to the scarcity of\nreliable datasets. To this end, we propose a benchmark --COLD for Chinese\noffensive language analysis, including a Chinese Offensive Language Dataset\n--COLDATASET and a baseline detector --COLDETECTOR which is trained on the\ndataset. We show that the COLD benchmark contributes to Chinese offensive\nlanguage detection which is challenging for existing resources. We then deploy\nthe COLDETECTOR and conduct detailed analyses on popular Chinese pre-trained\nlanguage models. We first analyze the offensiveness of existing generative\nmodels and show that these models inevitably expose varying degrees of\noffensive issues. Furthermore, we investigate the factors that influence the\noffensive generations, and we find that anti-bias contents and keywords\nreferring to certain groups or revealing negative attitudes trigger offensive\noutputs easier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiawen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chujie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpreting Arabic Transformer Models. (arXiv:2201.07434v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.07434","description":"<p>Arabic is a Semitic language which is widely spoken with many dialects. Given\nthe success of pre-trained language models, many transformer models trained on\nArabic and its dialects have surfaced. While these models have been compared\nwith respect to downstream NLP tasks, no evaluation has been carried out to\ndirectly compare the internal representations. We probe how linguistic\ninformation is encoded in Arabic pretrained models, trained on different\nvarieties of Arabic language. We perform a layer and neuron analysis on the\nmodels using three intrinsic tasks: two morphological tagging tasks based on\nMSA (modern standard Arabic) and dialectal POS-tagging and a dialectal\nidentification task. Our analysis enlightens interesting findings such as: i)\nword morphology is learned at the lower and middle layers ii) dialectal\nidentification necessitate more knowledge and hence preserved even in the final\nlayers, iii) despite a large overlap in their vocabulary, the MSA-based models\nfail to capture the nuances of Arabic dialects, iv) we found that neurons in\nembedding layers are polysemous in nature, while the neurons in middle layers\nare exclusive to specific properties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelali_A/0/1/0/all/0/1\">Ahmed Abdelali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrani_N/0/1/0/all/0/1\">Nadir Durrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalvi_F/0/1/0/all/0/1\">Fahim Dalvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sajjad_H/0/1/0/all/0/1\">Hassan Sajjad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Two-Step Hybrid Policy for Graph-Based Interpretable Reinforcement Learning. (arXiv:2201.08520v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.08520","description":"<p>We present a two-step hybrid reinforcement learning (RL) policy that is\ndesigned to generate interpretable and robust hierarchical policies on the RL\nproblem with graph-based input. Unlike prior deep reinforcement learning\npolicies parameterized by an end-to-end black-box graph neural network, our\napproach disentangles the decision-making process into two steps. The first\nstep is a simplified classification problem that maps the graph input to an\naction group where all actions share a similar semantic meaning. The second\nstep implements a sophisticated rule-miner that conducts explicit one-hop\nreasoning over the graph and identifies decisive edges in the graph input\nwithout the necessity of heavy domain knowledge. This two-step hybrid policy\npresents human-friendly interpretations and achieves better performance in\nterms of generalization and robustness. Extensive experimental studies on four\nlevels of complex text-based games have demonstrated the superiority of the\nproposed method compared to the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1\">Tongzhou Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kaixiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_F/0/1/0/all/0/1\">Feiyang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thattai_G/0/1/0/all/0/1\">Govind Thattai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeRetriever: Unimodal and Bimodal Contrastive Learning for Code Search. (arXiv:2201.10866v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.10866","description":"<p>In this paper, we propose the CodeRetriever model, which learns the\nfunction-level code semantic representations through large-scale code-text\ncontrastive pre-training. We adopt two contrastive learning schemes in\nCodeRetriever: unimodal contrastive learning and bimodal contrastive learning.\nFor unimodal contrastive learning, we design an unsupervised learning approach\nto build semantic-related code pairs based on the documentation and function\nname. For bimodal contrastive learning, we leverage the documentation and\nin-line comments of code to build code-text pairs. Both contrastive objectives\ncan fully leverage large-scale code corpus for pre-training. Extensive\nexperimental results show that CodeRetriever achieves new state-of-the-art with\nsignificant improvement over existing code pre-trained models, on eleven\ndomain/language-specific code search tasks with six programming languages in\ndifferent code granularity (function-level, snippet-level and statement-level).\nThese results demonstrate the effectiveness and robustness of CodeRetriever.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaonan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bolun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1\">Weizhen Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PCL: Peer-Contrastive Learning with Diverse Augmentations for Unsupervised Sentence Embeddings. (arXiv:2201.12093v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.12093","description":"<p>Learning sentence embeddings in an unsupervised manner is fundamental in\nnatural language processing. Recent common practice is to couple pre-trained\nlanguage models with unsupervised contrastive learning, whose success relies on\naugmenting a sentence with a semantically-close positive instance to construct\ncontrastive pairs. Nonetheless, existing approaches usually depend on a\nmono-augmenting strategy, which causes learning shortcuts towards the\naugmenting biases and thus corrupts the quality of sentence embeddings. A\nstraightforward solution is resorting to more diverse positives from a\nmulti-augmenting strategy, while an open question remains about how to\nunsupervisedly learn from the diverse positives but with uneven augmenting\nqualities in the text field. As one answer, we propose a novel Peer-Contrastive\nLearning (PCL) with diverse augmentations. PCL constructs diverse contrastive\npositives and negatives at the group level for unsupervised sentence\nembeddings. PCL performs peer-positive contrast as well as peer-network\ncooperation, which offers an inherent anti-bias ability and an effective way to\nlearn from diverse augmentations. Experiments on STS benchmarks verify the\neffectiveness of PCL against its competitors in unsupervised sentence\nembeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings. (arXiv:2202.06671v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.06671","description":"<p>Learning scientific document representations can be substantially improved\nthrough contrastive learning objectives, where the challenge lies in creating\npositive and negative training samples that encode the desired similarity\nsemantics. Prior work relies on discrete citation relations to generate\ncontrast samples. However, discrete citations enforce a hard cut-off to\nsimilarity. This is counter-intuitive to similarity-based learning, and ignores\nthat scientific papers can be very similar despite lacking a direct citation -\na core problem of finding related research. Instead, we use controlled nearest\nneighbor sampling over citation graph embeddings for contrastive learning. This\ncontrol allows us to learn continuous similarity, to sample hard-to-learn\nnegatives and positives, and also to avoid collisions between negative and\npositive samples by controlling the sampling margin between them. The resulting\nmethod SciNCL outperforms the state-of-the-art on the SciDocs benchmark.\nFurthermore, we demonstrate that it can train (or tune) models\nsample-efficiently, and that it can be combined with recent training-efficient\nmethods. Perhaps surprisingly, even training a general-domain language model\nthis way outperforms baselines pretrained in-domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ostendorff_M/0/1/0/all/0/1\">Malte Ostendorff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rethmeier_N/0/1/0/all/0/1\">Nils Rethmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehm_G/0/1/0/all/0/1\">Georg Rehm</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Saving Dense Retriever from Shortcut Dependency in Conversational Search. (arXiv:2202.07280v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.07280","description":"<p>Conversational search (CS) needs a holistic understanding of conversational\ninputs to retrieve relevant passages. In this paper, we demonstrate the\nexistence of a retrieval shortcut in CS, which causes models to retrieve\npassages solely relying on partial history while disregarding the latest\nquestion. With in-depth analysis, we first show that naively trained dense\nretrievers heavily exploit the shortcut and hence perform poorly when asked to\nanswer history-independent questions. To build more robust models against\nshortcut dependency, we explore various hard negative mining strategies.\nExperimental results show that training with the model-based hard negatives\neffectively mitigates the dependency on the shortcut, significantly improving\ndense retrievers on recent CS benchmarks. In particular, our retriever\noutperforms the previous state-of-the-art model by 11.0 in Recall@10 on QReCC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungdong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gangwoo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Morphology Without Borders: Clause-Level Morphology. (arXiv:2202.12832v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.12832","description":"<p>Morphological tasks use large multi-lingual datasets that organize words into\ninflection tables, which then serve as training and evaluation data for various\ntasks. However, a closer inspection of these data reveals profound\ncross-linguistic inconsistencies, that arise from the lack of a clear\nlinguistic and operational definition of what is a word, and that severely\nimpair the universality of the derived tasks. To overcome this deficiency, we\npropose to view morphology as a clause-level phenomenon, rather than\nword-level. It is anchored in a fixed yet inclusive set of features, that\nencapsulates all functions realized in a saturated clause. We deliver\nMightyMorph, a novel dataset for clause-level morphology covering 4\ntypologically-different languages: English, German, Turkish and Hebrew. We use\nthis dataset to derive 3 clause-level morphological tasks: inflection,\nreinflection and analysis. Our experiments show that the clause-level tasks are\nsubstantially harder than the respective word-level tasks, while having\ncomparable complexity across languages. Furthermore, redefining morphology to\nthe clause-level provides a neat interface with contextualized language models\n(LMs) and allows assessing the morphological knowledge encoded in these models\nand their usability for morphological tasks. Taken together, this work opens up\nnew horizons in the study of computational morphology, leaving ample space for\nstudying neural morphology cross-linguistically.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goldman_O/0/1/0/all/0/1\">Omer Goldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geodesic Multi-Modal Mixup for Robust Fine-Tuning. (arXiv:2203.03897v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03897","description":"<p>Pre-trained large-scale models provide a transferable embedding, and they\nshow promising performance on diverse downstream tasks. However, the analysis\nof learned embedding has not been explored well, and the transferability for\ncross-modal tasks can be improved. This paper provides a perspective to\nunderstand multi-modal embedding in terms of uniformity and alignment. We newly\nfind that the representation learned by multi-modal learning models such as\nCLIP has two separated embedding spaces for each heterogeneous dataset with\nless alignment. Besides, there are unexplored large intermediate areas between\nthe two modalities with less uniformity. As a result, lack of alignment and\nuniformity might restrict the robustness and transferability of the\nrepresentation for the downstream task. To this end, we provide a new\nend-to-end fine-tuning method for robust representation that encourages better\nuniformity and alignment score. First, we propose a \\textit{Geodesic\nMulti-Modal Mixup} that mixes the representation of image and text to generate\nthe hard negative samples on the hyperspherical embedding space. Second, we\nfine-tune the multi-modal model on hard negative samples as well as normal\nnegatives and positive samples with contrastive loss. Through extensive\nexperiments on retrieval, classification, and structure-awareness task, we\ndemonstrate that our geodesic multi-modal Mixup learns a robust representation\nand provides improved performance on various downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+So_J/0/1/0/all/0/1\">Junhyuk So</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1\">Changdae Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_Y/0/1/0/all/0/1\">Yongtaek Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1\">Hoyoon Byun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1\">Minchul Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kyungwoo Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Demonstration Tuning for Pre-trained Language Models. (arXiv:2204.04392v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04392","description":"<p>Pretrained language models can be effectively stimulated by textual prompts\nor demonstrations, especially in low-data scenarios. Recent works have focused\non automatically searching discrete or continuous prompts or optimized\nverbalizers, yet studies for the demonstration are still limited. Concretely,\nthe demonstration examples are crucial for an excellent final performance of\nprompt-tuning. In this paper, we propose a novel pluggable, extensible, and\nefficient approach named contrastive demonstration tuning, which is free of\ndemonstration sampling. Furthermore, the proposed approach can be: (i) Plugged\ninto any previous prompt-tuning approaches; (ii) Extended to widespread\nclassification tasks with a large number of categories. Experimental results on\n16 datasets illustrate that our method integrated with previous approaches\nLM-BFF and P-tuning can yield better performance. Code is available in\nhttps://github.com/zjunlp/PromptKG/tree/main/research/Demo-Tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hierarchical N-Gram Framework for Zero-Shot Link Prediction. (arXiv:2204.10293v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10293","description":"<p>Due to the incompleteness of knowledge graphs (KGs), zero-shot link\nprediction (ZSLP) which aims to predict unobserved relations in KGs has\nattracted recent interest from researchers. A common solution is to use textual\nfeatures of relations (e.g., surface name or textual descriptions) as auxiliary\ninformation to bridge the gap between seen and unseen relations. Current\napproaches learn an embedding for each word token in the text. These methods\nlack robustness as they suffer from the out-of-vocabulary (OOV) problem.\nMeanwhile, models built on character n-grams have the capability of generating\nexpressive representations for OOV words. Thus, in this paper, we propose a\nHierarchical N-Gram framework for Zero-Shot Link Prediction (HNZSLP), which\nconsiders the dependencies among character n-grams of the relation surface name\nfor ZSLP. Our approach works by first constructing a hierarchical n-gram graph\non the surface name to model the organizational structure of n-grams that leads\nto the surface name. A GramTransformer, based on the Transformer is then\npresented to model the hierarchical n-gram graph to construct the relation\nembedding for ZSLP. Experimental results show the proposed HNZSLP achieved\nstate-of-the-art performance on two ZSLP datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingchen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junfan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensah_S/0/1/0/all/0/1\">Samuel Mensah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiulong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yang Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NMTScore: A Multilingual Analysis of Translation-based Text Similarity Measures. (arXiv:2204.13692v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.13692","description":"<p>Being able to rank the similarity of short text segments is an interesting\nbonus feature of neural machine translation. Translation-based similarity\nmeasures include direct and pivot translation probability, as well as\ntranslation cross-likelihood, which has not been studied so far. We analyze\nthese measures in the common framework of multilingual NMT, releasing the\nNMTScore library (available at https://github.com/ZurichNLP/nmtscore). Compared\nto baselines such as sentence embeddings, translation-based measures prove\ncompetitive in paraphrase identification and are more robust against\nadversarial or multilingual input, especially if proper normalization is\napplied. When used for reference-based evaluation of data-to-text generation in\n2 tasks and 17 languages, translation-based measures show a relatively high\ncorrelation to human judgments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vamvas_J/0/1/0/all/0/1\">Jannis Vamvas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unifying the Convergences in Multilingual Neural Machine Translation. (arXiv:2205.01620v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.01620","description":"<p>Although all-in-one-model multilingual neural machine translation\n(multilingual NMT) has achieved remarkable progress, the convergence\ninconsistency in the joint training is ignored, i.e., different language pairs\nreaching convergence in different epochs. This leads to the trained MNMT model\nover-fitting low-resource language translations while under-fitting\nhigh-resource ones. In this paper, we propose a novel training strategy named\nLSSD (Language-Specific Self-Distillation), which can alleviate the convergence\ninconsistency and help MNMT models achieve the best performance on each\nlanguage pair simultaneously. Specifically, LSSD picks up language-specific\nbest checkpoints for each language pair to teach the current model on the fly.\nFurthermore, we systematically explore three sample-level manipulations of\nknowledge transferring. Experimental results on three datasets show that LSSD\nobtains consistent improvements towards all language pairs and achieves the\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yichong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiaocheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xinwei Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Outliers Dimensions that Disrupt Transformers Are Driven by Frequency. (arXiv:2205.11380v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11380","description":"<p>While Transformer-based language models are generally very robust to pruning,\nthere is the recently discovered outlier phenomenon: disabling only 48 out of\n110M parameters in BERT-base drops its performance by nearly 30% on MNLI. We\nreplicate the original evidence for the outlier phenomenon and we link it to\nthe geometry of the embedding space. We find that in both BERT and RoBERTa the\nmagnitude of hidden state coefficients corresponding to outlier dimensions\ncorrelates with the frequency of encoded tokens in pre-training data, and it\nalso contributes to the \"vertical\" self-attention pattern enabling the model to\nfocus on the special tokens. This explains the drop in performance from\ndisabling the outliers, and it suggests that to decrease anisotropicity in\nfuture models we need pre-training schemas that would better take into account\nthe skewed token distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Puccetti_G/0/1/0/all/0/1\">Giovanni Puccetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_A/0/1/0/all/0/1\">Anna Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozd_A/0/1/0/all/0/1\">Aleksandr Drozd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DellOrletta_F/0/1/0/all/0/1\">Felice Dell&#x27;Orletta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuned Language Models are Continual Learners. (arXiv:2205.12393v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12393","description":"<p>Recent work on large language models relies on the intuition that most\nnatural language processing tasks can be described via natural language\ninstructions. Language models trained on these instructions show strong\nzero-shot performance on several standard datasets. However, these models even\nthough impressive still perform poorly on a wide range of tasks outside of\ntheir respective training and evaluation sets. To address this limitation, we\nargue that a model should be able to keep extending its knowledge and\nabilities, without forgetting previous skills. In spite of the limited success\nof Continual Learning we show that Language Models can be continual learners.\nWe empirically investigate the reason for this success and conclude that\nContinual Learning emerges from self-supervision pre-training. Our resulting\nmodel Continual-T0 (CT0) is able to learn diverse new tasks, while still\nmaintaining good performance on previous tasks, spanning remarkably through 70\ndatasets in total. Finally, we show that CT0 is able to combine instructions in\nways it was never trained for, demonstrating some compositionality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transcormer: Transformer for Sentence Scoring with Sliding Language Modeling. (arXiv:2205.12986v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12986","description":"<p>Sentence scoring aims at measuring the likelihood score of a sentence and is\nwidely used in many natural language processing scenarios, like reranking,\nwhich is to select the best sentence from multiple candidates. Previous works\non sentence scoring mainly adopted either causal language modeling (CLM) like\nGPT or masked language modeling (MLM) like BERT, which have some limitations:\n1) CLM only utilizes unidirectional information for the probability estimation\nof a sentence without considering bidirectional context, which affects the\nscoring quality; 2) MLM can only estimate the probability of partial tokens at\na time and thus requires multiple forward passes to estimate the probability of\nthe whole sentence, which incurs large computation and time cost. In this\npaper, we propose \\textit{Transcormer} -- a Transformer model with a novel\n\\textit{sliding language modeling} (SLM) for sentence scoring. Specifically,\nour SLM adopts a triple-stream self-attention mechanism to estimate the\nprobability of all tokens in a sentence with bidirectional context and only\nrequires a single forward pass. SLM can avoid the limitations of CLM (only\nunidirectional context) and MLM (multiple forward passes) and inherit their\nadvantages, and thus achieve high effectiveness and efficiency in scoring.\nExperimental results on multiple tasks demonstrate that our method achieves\nbetter performance than other language modelings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yicheng Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Generator-Ranker Learning for Natural Language Generation. (arXiv:2206.13974v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.13974","description":"<p>Generate-then-rank is a widely used mechanism for text generation, where a\ngenerator produces multiple candidates and a ranker chooses the best one.\nHowever, existing methods usually train the generator and the ranker\nseparately, which causes a lack of mutual feedback and a misalignment of their\nobjectives. This results in suboptimal generation quality. To address this\nissue, we propose JGR, a novel joint training algorithm that integrates the\ngenerator and the ranker in a single framework. JGR optimizes the generator\nwith a hybrid objective that combines data likelihood and ranker reward, and\ntrains the ranker with a contrastive loss that compares the generator outputs.\nBy alternately updating the generator and the ranker, JGR can effectively\nharmonize their learning and enhance their quality jointly. We evaluate JGR on\nvarious text generation tasks and demonstrate that it surpasses existing\nmethods on four public datasets across three common generation scenarios. Our\ncode, data, and models are available at https://github.com/microsoft/AdvNLG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Weizhou Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1\">Xiaojun Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PoeticTTS -- Controllable Poetry Reading for Literary Studies. (arXiv:2207.05549v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2207.05549","description":"<p>Speech synthesis for poetry is challenging due to specific intonation\npatterns inherent to poetic speech. In this work, we propose an approach to\nsynthesise poems with almost human like naturalness in order to enable literary\nscholars to systematically examine hypotheses on the interplay between text,\nspoken realisation, and the listener's perception of poems. To meet these\nspecial requirements for literary studies, we resynthesise poems by cloning\nprosodic values from a human reference recitation, and afterwards make use of\nfine-grained prosody control to manipulate the synthetic speech in a\nhuman-in-the-loop setting to alter the recitation w.r.t. specific phenomena. We\nfind that finetuning our TTS model on poetry captures poetic intonation\npatterns to a large extent which is beneficial for prosody cloning and\nmanipulation and verify the success of our approach both in an objective\nevaluation as well as in human studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Koch_J/0/1/0/all/0/1\">Julia Koch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lux_F/0/1/0/all/0/1\">Florian Lux</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schauffler_N/0/1/0/all/0/1\">Nadja Schauffler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bernhart_T/0/1/0/all/0/1\">Toni Bernhart</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dieterle_F/0/1/0/all/0/1\">Felix Dieterle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuhn_J/0/1/0/all/0/1\">Jonas Kuhn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Richter_S/0/1/0/all/0/1\">Sandra Richter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Viehhauser_G/0/1/0/all/0/1\">Gabriel Viehhauser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Testing Pre-trained Language Models' Understanding of Distributivity via Causal Mediation Analysis. (arXiv:2209.04761v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.04761","description":"<p>To what extent do pre-trained language models grasp semantic knowledge\nregarding the phenomenon of distributivity? In this paper, we introduce\nDistNLI, a new diagnostic dataset for natural language inference that targets\nthe semantic difference arising from distributivity, and employ the causal\nmediation analysis framework to quantify the model behavior and explore the\nunderlying mechanism in this semantically-related task. We find that the extent\nof models' understanding is associated with model size and vocabulary size. We\nalso provide insights into how models encode such high-level semantic\nknowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ban_P/0/1/0/all/0/1\">Pangbo Ban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianran Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinert_Threlkeld_S/0/1/0/all/0/1\">Shane Steinert-Threlkeld</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Distributional Lens for Multi-Aspect Controllable Text Generation. (arXiv:2210.02889v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.02889","description":"<p>Multi-aspect controllable text generation is a more challenging and practical\ntask than single-aspect control. Existing methods achieve complex multi-aspect\ncontrol by fusing multiple controllers learned from single-aspect, but suffer\nfrom attribute degeneration caused by the mutual interference of these\ncontrollers. To address this, we provide observations on attribute fusion from\na distributional perspective and propose to directly search for the\nintersection areas of multiple attribute distributions as their combination for\ngeneration. Our method first estimates the attribute space with an autoencoder\nstructure. Afterward, we iteratively approach the intersections by jointly\nminimizing distances to points representing different attributes. Finally, we\nmap them to attribute-relevant sentences with a prefix-tuning-based decoder.\nExperiments on the three-aspect control task, including sentiment, topic, and\ndetoxification aspects, reveal that our method outperforms several strong\nbaselines on attribute relevance and text quality and achieves the SOTA.\nFurther analysis also supplies some explanatory support for the effectiveness\nof our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuxuan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiaocheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Sicheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lingyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Heng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Framework based Query Generation for Temporal Question Answering over Knowledge Graphs. (arXiv:2210.04490v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04490","description":"<p>Answering factual questions with temporal intent over knowledge graphs\n(temporal KGQA) attracts rising attention in recent years. In the generation of\ntemporal queries, existing KGQA methods ignore the fact that some intrinsic\nconnections between events can make them temporally related, which may limit\ntheir capability. We systematically analyze the possible interpretation of\ntemporal constraints and conclude the interpretation structures as the Semantic\nFramework of Temporal Constraints, SF-TCons. Based on the semantic framework,\nwe propose a temporal question answering method, SF-TQA, which generates query\ngraphs by exploring the relevant facts of mentioned entities, where the\nexploring process is restricted by SF-TCons. Our evaluations show that SF-TQA\nsignificantly outperforms existing methods on two benchmarks over different\nknowledge graphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Weantao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huayu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yuzhong Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capturing Global Structural Information in Long Document Question Answering with Compressive Graph Selector Network. (arXiv:2210.05499v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05499","description":"<p>Long document question answering is a challenging task due to its demands for\ncomplex reasoning over long text. Previous works usually take long documents as\nnon-structured flat texts or only consider the local structure in long\ndocuments. However, these methods usually ignore the global structure of the\nlong document, which is essential for long-range understanding. To tackle this\nproblem, we propose Compressive Graph Selector Network (CGSN) to capture the\nglobal structure in a compressive and iterative manner. The proposed model\nmainly focuses on the evidence selection phase of long document question\nanswering. Specifically, it consists of three modules: local graph network,\nglobal graph network and evidence memory network. Firstly, the local graph\nnetwork builds the graph structure of the chunked segment in token, sentence,\nparagraph and segment levels to capture the short-term dependency of the text.\nSecondly, the global graph network selectively receives the information of each\nlevel from the local graph, compresses them into the global graph nodes and\napplies graph attention to the global graph nodes to build the long-range\nreasoning over the entire text in an iterative way. Thirdly, the evidence\nmemory network is designed to alleviate the redundancy problem in the evidence\nselection by saving the selected result in the previous steps. Extensive\nexperiments show that the proposed model outperforms previous methods on two\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yuxiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MTet: Multi-domain Translation for English and Vietnamese. (arXiv:2210.05610v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05610","description":"<p>We introduce MTet, the largest publicly available parallel corpus for\nEnglish-Vietnamese translation. MTet consists of 4.2M high-quality training\nsentence pairs and a multi-domain test set refined by the Vietnamese research\ncommunity. Combining with previous works on English-Vietnamese translation, we\ngrow the existing parallel dataset to 6.2M sentence pairs. We also release the\nfirst pretrained model EnViT5 for English and Vietnamese languages. Combining\nboth resources, our model significantly outperforms previous state-of-the-art\nresults by up to 2 points in translation BLEU score, while being 1.6 times\nsmaller.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1\">Chinh Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trinh_T/0/1/0/all/0/1\">Trieu H. Trinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_L/0/1/0/all/0/1\">Long Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hieu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1\">Tai Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hieu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luong_M/0/1/0/all/0/1\">Minh-Thang Luong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Foundation Transformers. (arXiv:2210.06423v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.06423","description":"<p>A big convergence of model architectures across language, vision, speech, and\nmultimodal is emerging. However, under the same name \"Transformers\", the above\nareas use different implementations for better performance, e.g.,\nPost-LayerNorm for BERT, and Pre-LayerNorm for GPT and vision Transformers. We\ncall for the development of Foundation Transformer for true general-purpose\nmodeling, which serves as a go-to architecture for various tasks and modalities\nwith guaranteed training stability. In this work, we introduce a Transformer\nvariant, named Magneto, to fulfill the goal. Specifically, we propose\nSub-LayerNorm for good expressivity, and the initialization strategy\ntheoretically derived from DeepNet for stable scaling up. Extensive experiments\ndemonstrate its superior performance and better stability than the de facto\nTransformer variants designed for various applications, including language\nmodeling (i.e., BERT, and GPT), machine translation, vision pretraining (i.e.,\nBEiT), speech recognition, and multimodal pretraining (i.e., BEiT-3).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhiliang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_P/0/1/0/all/0/1\">Payal Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1\">Saksham Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benhaim_A/0/1/0/all/0/1\">Alon Benhaim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patra_B/0/1/0/all/0/1\">Barun Patra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1\">Vishrav Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SubeventWriter: Iterative Sub-event Sequence Generation with Coherence Controller. (arXiv:2210.06694v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06694","description":"<p>In this paper, we propose a new task of sub-event generation for an unseen\nprocess to evaluate the understanding of the coherence of sub-event actions and\nobjects. To solve the problem, we design SubeventWriter, a sub-event sequence\ngeneration framework with a coherence controller. Given an unseen process, the\nframework can iteratively construct the sub-event sequence by generating one\nsub-event at each iteration. We also design a very effective coherence\ncontroller to decode more coherent sub-events. As our extensive experiments and\nanalysis indicate, SubeventWriter can generate more reliable and meaningful\nsub-event sequences for unseen processes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1\">Tianqing Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_G/0/1/0/all/0/1\">Ginny Y. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_S/0/1/0/all/0/1\">Simon See</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Algorithms for Weighted Pushdown Automata. (arXiv:2210.06884v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06884","description":"<p>Weighted pushdown automata (WPDAs) are at the core of many natural language\nprocessing tasks, like syntax-based statistical machine translation and\ntransition-based dependency parsing. As most existing dynamic programming\nalgorithms are designed for context-free grammars (CFGs), algorithms for PDAs\noften resort to a PDA-to-CFG conversion. In this paper, we develop novel\nalgorithms that operate directly on WPDAs. Our algorithms are inspired by\nLang's algorithm, but use a more general definition of pushdown automaton and\neither reduce the space requirements by a factor of $|\\Gamma|$ (the size of the\nstack alphabet) or reduce the runtime by a factor of more than $|Q|$ (the\nnumber of states). When run on the same class of PDAs as Lang's algorithm, our\nalgorithm is both more space-efficient by a factor of $|\\Gamma|$ and more\ntime-efficient by a factor of $|Q| \\cdot |\\Gamma|$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Butoi_A/0/1/0/all/0/1\">Alexandra Butoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DuSell_B/0/1/0/all/0/1\">Brian DuSell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vieira_T/0/1/0/all/0/1\">Tim Vieira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1\">David Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Character-Centric Story Visualization via Visual Planning and Token Alignment. (arXiv:2210.08465v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.08465","description":"<p>Story visualization advances the traditional text-to-image generation by\nenabling multiple image generation based on a complete story. This task\nrequires machines to 1) understand long text inputs and 2) produce a globally\nconsistent image sequence that illustrates the contents of the story. A key\nchallenge of consistent story visualization is to preserve characters that are\nessential in stories. To tackle the challenge, we propose to adapt a recent\nwork that augments Vector-Quantized Variational Autoencoders (VQ-VAE) with a\ntext-tovisual-token (transformer) architecture. Specifically, we modify the\ntext-to-visual-token module with a two-stage framework: 1) character token\nplanning model that predicts the visual tokens for characters only; 2) visual\ntoken completion model that generates the remaining visual token sequence,\nwhich is sent to VQ-VAE for finalizing image generations. To encourage\ncharacters to appear in the images, we further train the two-stage framework\nwith a character-token alignment objective. Extensive experiments and\nevaluations demonstrate that the proposed method excels at preserving\ncharacters and can produce higher quality image sequences compared with the\nstrong baselines. Codes can be found in https://github.com/sairin1202/VP-CSV\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1\">Rujun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Te-Lin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakayama_H/0/1/0/all/0/1\">Hideki Nakayama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MCP: Self-supervised Pre-training for Personalized Chatbots with Multi-level Contrastive Sampling. (arXiv:2210.08753v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08753","description":"<p>Personalized chatbots focus on endowing the chatbots with a consistent\npersonality to behave like real users and further act as personal assistants.\nPrevious studies have explored generating implicit user profiles from the\nuser's dialogue history for building personalized chatbots. However, these\nstudies only use the response generation loss to train the entire model, thus\nit is prone to suffer from the problem of data sparsity. Besides, they\noveremphasize the final generated response's quality while ignoring the\ncorrelations and fusions between the user's dialogue history, leading to rough\ndata representations and performance degradation. To tackle these problems, we\npropose a self-supervised learning framework MCP for capturing better\nrepresentations from users' dialogue history for personalized chatbots.\nSpecifically, we apply contrastive sampling methods to leverage the supervised\nsignals hidden in user dialog history, and generate the pre-training samples\nfor enhancing the model. We design three pre-training tasks based on three\ntypes of contrastive pairs from user dialogue history, namely response pairs,\nsequence augmentation pairs, and user pairs. We pre-train the utterance encoder\nand the history encoder towards the contrastive objectives and use these\npre-trained encoders for generating user profiles while personalized response\ngeneration. Experimental results on two real-world datasets show a significant\nimprovement in our proposed model MCP compared with the existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhaoheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yutao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhengyi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Symbol Guided Hindsight Priors for Reward Learning from Human Preferences. (arXiv:2210.09151v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.09151","description":"<p>Specifying rewards for reinforcement learned (RL) agents is challenging.\nPreference-based RL (PbRL) mitigates these challenges by inferring a reward\nfrom feedback over sets of trajectories. However, the effectiveness of PbRL is\nlimited by the amount of feedback needed to reliably recover the structure of\nthe target reward. We present the PRIor Over Rewards (PRIOR) framework, which\nincorporates priors about the structure of the reward function and the\npreference feedback into the reward learning process. Imposing these priors as\nsoft constraints on the reward learning objective reduces the amount of\nfeedback required by half and improves overall reward recovery. Additionally,\nwe demonstrate that using an abstract state space for the computation of the\npriors further improves the reward learning and the agent's performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Verma_M/0/1/0/all/0/1\">Mudit Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metcalf_K/0/1/0/all/0/1\">Katherine Metcalf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Bidirectional Language-Knowledge Graph Pretraining. (arXiv:2210.09338v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.09338","description":"<p>Pretraining a language model (LM) on text has been shown to help various\ndownstream NLP tasks. Recent works show that a knowledge graph (KG) can\ncomplement text data, offering structured background knowledge that provides a\nuseful scaffold for reasoning. However, these works are not pretrained to learn\na deep fusion of the two modalities at scale, limiting the potential to acquire\nfully joint representations of text and KG. Here we propose DRAGON (Deep\nBidirectional Language-Knowledge Graph Pretraining), a self-supervised approach\nto pretraining a deeply joint language-knowledge foundation model from text and\nKG at scale. Specifically, our model takes pairs of text segments and relevant\nKG subgraphs as input and bidirectionally fuses information from both\nmodalities. We pretrain this model by unifying two self-supervised reasoning\ntasks, masked language modeling and KG link prediction. DRAGON outperforms\nexisting LM and LM+KG models on diverse downstream tasks including question\nanswering across general and biomedical domains, with +5% absolute gain on\naverage. In particular, DRAGON achieves notable performance on complex\nreasoning about language and knowledge (+10% on questions involving long\ncontexts or multi-step reasoning) and low-resource QA (+8% on OBQA and\nRiddleSense), and new state-of-the-art results on various BioNLP tasks. Our\ncode and trained models are available at\nhttps://github.com/michiyasunaga/dragon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xikun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D Manning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-granularity Argument Mining in Legal Texts. (arXiv:2210.09472v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.09472","description":"<p>In this paper, we explore legal argument mining using multiple levels of\ngranularity. Argument mining has usually been conceptualized as a sentence\nclassification problem. In this work, we conceptualize argument mining as a\ntoken-level (i.e., word-level) classification problem. We use a Longformer\nmodel to classify the tokens. Results show that token-level text classification\nidentifies certain legal argument elements more accurately than sentence-level\ntext classification. Token-level classification also provides greater\nflexibility to analyze legal texts and to gain more insight into what the model\nfocuses on when processing a large amount of input data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huihui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashley_K/0/1/0/all/0/1\">Kevin Ashley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment-Aware Word and Sentence Level Pre-training for Sentiment Analysis. (arXiv:2210.09803v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.09803","description":"<p>Most existing pre-trained language representation models (PLMs) are\nsub-optimal in sentiment analysis tasks, as they capture the sentiment\ninformation from word-level while under-considering sentence-level information.\nIn this paper, we propose SentiWSP, a novel Sentiment-aware pre-trained\nlanguage model with combined Word-level and Sentence-level Pre-training tasks.\nThe word level pre-training task detects replaced sentiment words, via a\ngenerator-discriminator framework, to enhance the PLM's knowledge about\nsentiment words. The sentence level pre-training task further strengthens the\ndiscriminator via a contrastive learning framework, with similar sentences as\nnegative samples, to encode sentiments in a sentence. Extensive experimental\nresults show that SentiWSP achieves new state-of-the-art performance on various\nsentence-level and aspect-level sentiment classification benchmarks. We have\nmade our code and model publicly available at\nhttps://github.com/XMUDM/SentiWSP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1\">Shuai Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haonan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhenghao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jian Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hidden State Variability of Pretrained Language Models Can Guide Computation Reduction for Transfer Learning. (arXiv:2210.10041v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10041","description":"<p>While transferring a pretrained language model, common approaches\nconventionally attach their task-specific classifiers to the top layer and\nadapt all the pretrained layers. We investigate whether one could make a\ntask-specific selection on which subset of the layers to adapt and where to\nplace the classifier. The goal is to reduce the computation cost of transfer\nlearning methods (e.g. fine-tuning or adapter-tuning) without sacrificing its\nperformance.\n</p>\n<p>We propose to select layers based on the variability of their hidden states\ngiven a task-specific corpus. We say a layer is already \"well-specialized\" in a\ntask if the within-class variability of its hidden states is low relative to\nthe between-class variability. Our variability metric is cheap to compute and\ndoesn't need any training or hyperparameter tuning. It is robust to data\nimbalance and data scarcity. Extensive experiments on the GLUE benchmark\ndemonstrate that selecting layers based on our metric can yield significantly\nstronger performance than using the same number of top layers and often match\nthe performance of fine-tuning or adapter-tuning the entire language model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Shuo Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jiahao Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasad_A/0/1/0/all/0/1\">Ankita Pasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Li Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Q/0/1/0/all/0/1\">Qing Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1\">Hongyuan Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-10-19T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}}]}]}