{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-02-21T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A Pilot Evaluation of ChatGPT and DALL-E 2 on Decision Making and Spatial Reasoning. (arXiv:2302.09068v1 [cs.AI])","link":"http://arxiv.org/abs/2302.09068","description":"<p>We conduct a pilot study selectively evaluating the cognitive abilities\n(decision making and spatial reasoning) of two recently released generative\ntransformer models, ChatGPT and DALL-E 2. Input prompts were constructed\nfollowing neutral a priori guidelines, rather than adversarial intent. Post hoc\nqualitative analysis of the outputs shows that DALL-E 2 is able to generate at\nleast one correct image for each spatial reasoning prompt, but most images\ngenerated are incorrect (even though the model seems to have a clear\nunderstanding of the objects mentioned in the prompt). Similarly, in evaluating\nChatGPT on the rationality axioms developed under the classical Von\nNeumann-Morgenstern utility theorem, we find that, although it demonstrates\nsome level of rational decision-making, many of its decisions violate at least\none of the axioms even under reasonable constructions of preferences, bets, and\ndecision-making prompts. ChatGPT's outputs on such problems generally tended to\nbe unpredictable: even as it made irrational decisions (or employed an\nincorrect reasoning process) for some simpler decision-making problems, it was\nable to draw correct conclusions for more complex bet structures. We briefly\ncomment on the nuances and challenges involved in scaling up such a 'cognitive'\nevaluation or conducting it with a closed set of answer keys ('ground truth'),\ngiven that these models are inherently generative and open-ended in responding\nto prompts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhisheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kejriwal_M/0/1/0/all/0/1\">Mayank Kejriwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conveying the Predicted Future to Users: A Case Study of Story Plot Prediction. (arXiv:2302.09122v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09122","description":"<p>Creative writing is hard: Novelists struggle with writer's block daily. While\nautomatic story generation has advanced recently, it is treated as a \"toy task\"\nfor advancing artificial intelligence rather than helping people. In this\npaper, we create a system that produces a short description that narrates a\npredicted plot using existing story generation approaches. Our goal is to\nassist writers in crafting a consistent and compelling story arc. We conducted\nexperiments on Amazon Mechanical Turk (AMT) to examine the quality of the\ngenerated story plots in terms of consistency and storiability. The results\nshow that short descriptions produced by our frame-enhanced GPT-2 (FGPT-2) were\nrated as the most consistent and storiable among all models; FGPT-2's outputs\neven beat some random story snippets written by humans. Next, we conducted a\npreliminary user study using a story continuation task where AMT workers were\ngiven access to machine-generated story plots and asked to write a follow-up\nstory. FGPT-2 could positively affect the writing process, though people favor\nother baselines more. Our study shed some light on the possibilities of future\ncreative writing support systems beyond the scope of completing sentences. Our\ncode is available at: https://github.com/appleternity/Story-Plot-Generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chieh-Yang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naphade_S/0/1/0/all/0/1\">Saniya Naphade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_K/0/1/0/all/0/1\">Kavya Laalasa Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Hao &#x27;Kenneth&#x27; Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cluster-Guided Label Generation in Extreme Multi-Label Classification. (arXiv:2302.09150v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09150","description":"<p>For extreme multi-label classification (XMC), existing classification-based\nmodels poorly perform for tail labels and often ignore the semantic relations\namong labels, like treating \"Wikipedia\" and \"Wiki\" as independent and separate\nlabels. In this paper, we cast XMC as a generation task (XLGen), where we\nbenefit from pre-trained text-to-text models. However, generating labels from\nthe extremely large label space is challenging without any constraints or\nguidance. We, therefore, propose to guide label generation using label cluster\ninformation to hierarchically generate lower-level labels. We also find that\nfrequency-based label ordering and using decoding ensemble methods are critical\nfactors for the improvements in XLGen. XLGen with cluster guidance\nsignificantly outperforms the classification and generation baselines on tail\nlabels, and also generally improves the overall performance in four popular XMC\nbenchmarks. In human evaluation, we also find XLGen generates unseen but\nplausible labels. Our code is now available at\nhttps://github.com/alexa/xlgen-eacl-2023.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_T/0/1/0/all/0/1\">Taehee Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joo-Kyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungjin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Med-EASi: Finely Annotated Dataset and Models for Controllable Simplification of Medical Texts. (arXiv:2302.09155v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09155","description":"<p>Automatic medical text simplification can assist providers with\npatient-friendly communication and make medical texts more accessible, thereby\nimproving health literacy. But curating a quality corpus for this task requires\nthe supervision of medical experts. In this work, we present\n$\\textbf{Med-EASi}$ ($\\underline{\\textbf{Med}}$ical dataset for\n$\\underline{\\textbf{E}}$laborative and $\\underline{\\textbf{A}}$bstractive\n$\\underline{\\textbf{Si}}$mplification), a uniquely crowdsourced and finely\nannotated dataset for supervised simplification of short medical texts. Its\n$\\textit{expert-layman-AI collaborative}$ annotations facilitate\n$\\textit{controllability}$ over text simplification by marking four kinds of\ntextual transformations: elaboration, replacement, deletion, and insertion. To\nlearn medical text simplification, we fine-tune T5-large with four different\nstyles of input-output combinations, leading to two control-free and two\ncontrollable versions of the model. We add two types of\n$\\textit{controllability}$ into text simplification, by using a multi-angle\ntraining approach: $\\textit{position-aware}$, which uses in-place annotated\ninputs and outputs, and $\\textit{position-agnostic}$, where the model only\nknows the contents to be edited, but not their positions. Our results show that\nour fine-grained annotations improve learning compared to the unannotated\nbaseline. Furthermore, $\\textit{position-aware}$ control generates better\nsimplification than the $\\textit{position-agnostic}$ one. The data and code are\navailable at https://github.com/Chandrayee/CTRL-SIMP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basu_C/0/1/0/all/0/1\">Chandrayee Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasu_R/0/1/0/all/0/1\">Rosni Vasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KILM: Knowledge Injection into Encoder-Decoder Language Models. (arXiv:2302.09170v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09170","description":"<p>Large pre-trained language models (PLMs) have been shown to retain implicit\nknowledge within their parameters. To enhance this implicit knowledge, we\npropose Knowledge Injection into Language Models (KILM), a novel approach that\ninjects entity-related knowledge into encoder-decoder PLMs, via a generative\nknowledge infilling objective through continued pre-training. This is done\nwithout architectural modifications to the PLMs or adding additional\nparameters. Experimental results over a suite of knowledge-intensive tasks\nspanning numerous datasets show that KILM enables models to retain more\nknowledge and hallucinate less, while preserving their original performance on\ngeneral NLU and NLG tasks. KILM also demonstrates improved zero-shot\nperformances on tasks such as entity disambiguation, outperforming\nstate-of-the-art models having 30x more parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namazifar_M/0/1/0/all/0/1\">Mahdi Namazifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1\">Devamanyu Hazarika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_A/0/1/0/all/0/1\">Aishwarya Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-T&#xfc;r</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Task Graph Generation from Instructional Video Transcripts. (arXiv:2302.09173v1 [cs.AI])","link":"http://arxiv.org/abs/2302.09173","description":"<p>This work explores the problem of generating task graphs of real-world\nactivities. Different from prior formulations, we consider a setting where text\ntranscripts of instructional videos performing a real-world activity (e.g.,\nmaking coffee) are provided and the goal is to identify the key steps relevant\nto the task as well as the dependency relationship between these key steps. We\npropose a novel task graph generation approach that combines the reasoning\ncapabilities of instruction-tuned language models along with clustering and\nranking components to generate accurate task graphs in a completely\nunsupervised manner. We show that the proposed approach generates more accurate\ntask graphs compared to a supervised learning approach on tasks from the ProceL\nand CrossTask datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Logeswaran_L/0/1/0/all/0/1\">Lajanugen Logeswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1\">Sungryull Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Yunseok Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Moontae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints. (arXiv:2302.09185v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09185","description":"<p>The limits of open-ended generative models are unclear, yet increasingly\nimportant. What causes them to succeed and what causes them to fail? In this\npaper, we take a prompt-centric approach to analyzing and bounding the\nabilities of open-ended generative models. We present a generic methodology of\nanalysis with two challenging prompt constraint types: structural and\nstylistic. These constraint types are categorized into a set of well-defined\nconstraints that are analyzable by a single prompt. We then systematically\ncreate a diverse set of simple, natural, and useful prompts to robustly analyze\neach individual constraint. Using the GPT-3 text-davinci-002 model as a case\nstudy, we generate outputs from our collection of prompts and analyze the\nmodel's generative failures. We also show the generalizability of our proposed\nmethod on other large models like BLOOM and OPT. Our results and our in-context\nmitigation strategies reveal open challenges for future research. We have\npublicly released our code at https://github.com/SALT-NLP/Bound-Cap-LLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1\">Albert Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanzhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extraction of Constituent Factors of Digestion Efficiency in Information Transfer by Media Composed of Texts and Images. (arXiv:2302.09189v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09189","description":"<p>The development and spread of information and communication technologies have\nincreased and diversified information. However, the increase in the volume and\nthe selection of information does not necessarily promote understanding. In\naddition, conventional evaluations of information transfer have focused only on\nthe arrival of information to the receivers. They need to sufficiently take\ninto account the receivers' understanding of the information after it has been\nacquired, which is the original purpose of the evaluation. In this study, we\npropose the concept of \"information digestion,\" which refers to the receivers'\ncorrect understanding of the acquired information, its contents, and its\npurpose. In the experiment, we proposed an evaluation model of information\ndigestibility using hierarchical factor analysis and extracted factors that\nconstitute digestibility by four types of media.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hiroaki_K/0/1/0/all/0/1\">Koike Hiroaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_T/0/1/0/all/0/1\">Teruaki Hayashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RetVec: Resilient and Efficient Text Vectorizer. (arXiv:2302.09207v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09207","description":"<p>This paper describes RetVec, a resilient multilingual embedding scheme\ndesigned for neural-based text processing, including small-text classification\nand large-language models. RetVec combines a novel character encoding with an\noptional small model to embed words into a 256-dimensional vector space. These\nembeddings enable training competitive multilingual text models resilient to\ntypos and adversarial attacks. In this paper, we evaluate and compare RetVec to\nstate-of-the-art tokenizers and word embeddings on common model architectures.\nThese comparisons demonstrate that RetVec leads to competitive models that are\nsignificantly more resilient to text perturbations across a variety of common\ntasks. RetVec is available under Apache 2 license at\n\\url{https://github.com/[anonymized]}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bursztein_E/0/1/0/all/0/1\">Elie Bursztein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Marina Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vallis_O/0/1/0/all/0/1\">Owen Vallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xinyu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurakin_A/0/1/0/all/0/1\">Alexey Kurakin</a>,"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation. (arXiv:2302.09210v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09210","description":"<p>Generative Pre-trained Transformer (GPT) models have shown remarkable\ncapabilities for natural language generation, but their performance for machine\ntranslation has not been thoroughly investigated. In this paper, we present a\ncomprehensive evaluation of GPT models for machine translation, covering\nvarious aspects such as quality of different GPT models in comparison with\nstate-of-the-art research and commercial systems, effect of prompting\nstrategies, robustness towards domain shifts and document-level translation. We\nexperiment with eighteen different translation directions involving high and\nlow resource languages, as well as non English-centric translations, and\nevaluate the performance of three GPT models: ChatGPT, GPT3.5\n(text-davinci-003), and text-davinci-002. Our results show that GPT models\nachieve very competitive translation quality for high resource languages, while\nhaving limited capabilities for low resource languages. We also show that\nhybrid approaches, which combine GPT models with other translation systems, can\nfurther enhance the translation quality. We perform comprehensive analysis and\nhuman evaluation to further understand the characteristics of GPT translations.\nWe hope that our paper provides valuable insights for researchers and\npractitioners in the field and helps to better understand the potential and\nlimitations of GPT models for translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hendy_A/0/1/0/all/0/1\">Amr Hendy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelrehim_M/0/1/0/all/0/1\">Mohamed Abdelrehim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharaf_A/0/1/0/all/0/1\">Amr Sharaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raunak_V/0/1/0/all/0/1\">Vikas Raunak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabr_M/0/1/0/all/0/1\">Mohamed Gabr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsushita_H/0/1/0/all/0/1\">Hitokazu Matsushita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afify_M/0/1/0/all/0/1\">Mohamed Afify</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadalla_H/0/1/0/all/0/1\">Hany Hassan Awadalla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLN-Trans: Translator for the Vision and Language Navigation Agent. (arXiv:2302.09230v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09230","description":"<p>Language understanding is essential for the navigation agent to follow\ninstructions. We observe two kinds of issues in the instructions that can make\nthe navigation task challenging: 1. The mentioned landmarks are not\nrecognizable by the navigation agent due to the different vision abilities of\nthe instructor and the modeled agent. 2. The mentioned landmarks are applicable\nto multiple targets, thus not distinctive for selecting the target among the\ncandidate viewpoints. To deal with these issues, we design a translator module\nfor the navigation agent to convert the original instructions into\neasy-to-follow sub-instruction representations at each step. The translator\nneeds to focus on the recognizable and distinctive landmarks based on the\nagent's visual abilities and the observed visual environment. To achieve this\ngoal, we create a new synthetic sub-instruction dataset and design specific\ntasks to train the translator and the navigation agent. We evaluate our\napproach on Room2Room~(R2R), Room4room~(R4R), and Room2Room Last (R2R-Last)\ndatasets and achieve state-of-the-art results on multiple benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kordjamshidi_P/0/1/0/all/0/1\">Parisa Kordjamshidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable Prompt Generation for Semi-supervised Learning with Language Models. (arXiv:2302.09236v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09236","description":"<p>Prompt-based learning methods in semi-supervised learning (SSL) settings have\nbeen shown to be effective on multiple natural language understanding (NLU)\ndatasets and tasks in the literature. However, manually designing multiple\nprompts and verbalizers requires domain knowledge and human effort, making it\ndifficult and expensive to scale across different datasets. In this paper, we\npropose two methods to automatically design multiple prompts and integrate\nautomatic verbalizer in SSL settings without sacrificing performance. The first\nmethod uses various demonstration examples with learnable continuous prompt\ntokens to create diverse prompt models. The second method uses a varying number\nof soft prompt tokens to encourage language models to learn different prompts.\nFor the verbalizer, we use the prototypical verbalizer to replace the manual\none. In summary, we obtained the best average accuracy of 73.2% (a relative\nimprovement of 2.52% over even the previous state-of-the-art SSL method with\nmanual prompts and verbalizers) in different few-shot learning settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maharjan_S/0/1/0/all/0/1\">Suraj Maharjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Beiye Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Federated Approach for Hate Speech Detection. (arXiv:2302.09243v1 [cs.LG])","link":"http://arxiv.org/abs/2302.09243","description":"<p>Hate speech detection has been the subject of high research attention, due to\nthe scale of content created on social media. In spite of the attention and the\nsensitive nature of the task, privacy preservation in hate speech detection has\nremained under-studied. The majority of research has focused on centralised\nmachine learning infrastructures which risk leaking data. In this paper, we\nshow that using federated machine learning can help address privacy the\nconcerns that are inherent to hate speech detection while obtaining up to 6.81%\nimprovement in terms of F1-score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gala_J/0/1/0/all/0/1\">Jay Gala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_D/0/1/0/all/0/1\">Deep Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_J/0/1/0/all/0/1\">Jash Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talat_Z/0/1/0/all/0/1\">Zeerak Talat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bag of Tricks for Effective Language Model Pretraining and Downstream Adaptation: A Case Study on GLUE. (arXiv:2302.09268v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09268","description":"<p>This technical report briefly describes our JDExplore d-team's submission\nVega v1 on the General Language Understanding Evaluation (GLUE) leaderboard,\nwhere GLUE is a collection of nine natural language understanding tasks,\nincluding question answering, linguistic acceptability, sentiment analysis,\ntext similarity, paraphrase detection, and natural language inference. [Method]\nWe investigate several effective strategies and choose their best combination\nsetting as the training recipes. As for model structure, we employ the vanilla\nTransformer with disentangled attention as the basic block encoder. For\nself-supervised training, we employ the representative denoising objective\n(i.e., replaced token detection) in phase 1 and combine the contrastive\nobjective (i.e., sentence embedding contrastive learning) with it in phase 2.\nDuring fine-tuning, several advanced techniques such as transductive\nfine-tuning, self-calibrated fine-tuning, and adversarial fine-tuning are\nadopted. [Results] According to our submission record (Jan. 2022), with our\noptimized pretraining and fine-tuning strategies, our 1.3 billion model sets\nnew state-of-the-art on 4/9 tasks, achieving the best average score of 91.3.\nEncouragingly, our Vega v1 is the first to exceed powerful human performance on\nthe two challenging tasks, i.e., SST-2 and WNLI. We believe our empirically\nsuccessful recipe with a bag of tricks could shed new light on developing\nefficient discriminative large language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1\">Qihuang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Keqin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Juhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Search-Engine-augmented Dialogue Response Generation with Cheaply Supervised Query Production. (arXiv:2302.09300v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09300","description":"<p>Knowledge-aided dialogue response generation aims at augmenting chatbots with\nrelevant external knowledge in the hope of generating more informative\nresponses. The majority of previous work assumes that the relevant knowledge is\ngiven as input or retrieved from a static pool of knowledge. However, this\nassumption violates the real-world situation, where knowledge is continually\nupdated and a chatbot has to dynamically retrieve useful knowledge. We propose\na dialogue model that can access the vast and dynamic information from any\nsearch engine for response generation. As the core module, a query producer is\nused to generate queries from a dialogue context to interact with a search\nengine. We design a training algorithm using cheap noisy supervision for the\nquery producer, where the signals are obtained by comparing retrieved articles\nwith the next dialogue response. As the result, the query producer is adjusted\nwithout any human annotation of gold queries, making it easily transferable to\nother domains and search engines. Experiments show that our query producer can\nachieve R@1 and R@5 rates of 62.4% and 74.8% for retrieving gold knowledge, and\nthe overall model generates better responses over strong knowledge-aided\nbaselines using BART and other typical systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Ante Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linfeng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1\">Haitao Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Representation Manifolds of Stable Diffusion Through the Lens of Intrinsic Dimension. (arXiv:2302.09301v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09301","description":"<p>Prompting has become an important mechanism by which users can more\neffectively interact with many flavors of foundation model. Indeed, the last\nseveral years have shown that well-honed prompts can sometimes unlock emergent\ncapabilities within such models. While there has been a substantial amount of\nempirical exploration of prompting within the community, relatively few works\nhave studied prompting at a mathematical level. In this work we aim to take a\nfirst step towards understanding basic geometric properties induced by prompts\nin Stable Diffusion, focusing on the intrinsic dimension of internal\nrepresentations within the model. We find that choice of prompt has a\nsubstantial impact on the intrinsic dimension of representations at both layers\nof the model which we explored, but that the nature of this impact depends on\nthe layer being considered. For example, in certain bottleneck layers of the\nmodel, intrinsic dimension of representations is correlated with prompt\nperplexity (measured using a surrogate model), while this correlation is not\napparent in the latent layers. Our evidence suggests that intrinsic dimension\ncould be a useful tool for future studies of the impact of different prompts on\ntext-to-image models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1\">Henry Kvinge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1\">Davis Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godfrey_C/0/1/0/all/0/1\">Charles Godfrey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridge the Gap between Language models and Tabular Understanding. (arXiv:2302.09302v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09302","description":"<p>Table pretrain-then-finetune paradigm has been proposed and employed at a\nrapid pace after the success of pre-training in the natural language domain.\nDespite the promising findings in tabular pre-trained language models (TPLMs),\nthere is an input gap between pre-training and fine-tuning phases. For\ninstance, TPLMs jointly pre-trained with table and text input could be\neffective for tasks also with table-text joint input like table question\nanswering, but it may fail for tasks with only tables or text as input such as\ntable retrieval. To this end, we propose UTP, an approach that dynamically\nsupports three types of multi-modal inputs: table-text, table, and text.\nSpecifically, UTP is pre-trained with two strategies: (1) We first utilize a\nuniversal mask language modeling objective on each kind of input, enforcing the\nmodel to adapt various inputs. (2) We then present Cross-Modal Contrastive\nRegularization (CMCR), which utilizes contrastive learning to encourage the\nconsistency between table-text cross-modality representations via unsupervised\ninstance-wise training signals during pre-training. By these means, the\nresulting model not only bridges the input gap between pre-training and\nfine-tuning but also advances in the alignment of table and text. Extensive\nresults show UTP achieves superior results on uni-modal input tasks (e.g.,\ntable retrieval) and cross-modal input tasks (e.g., table question answering).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_L/0/1/0/all/0/1\">Linjun Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Ming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jianhui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stress Test for BERT and Deep Models: Predicting Words from Italian Poetry. (arXiv:2302.09303v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09303","description":"<p>In this paper we present a set of experiments carried out with BERT on a\nnumber of Italian sentences taken from poetry domain. The experiments are\norganized on the hypothesis of a very high level of difficulty in\npredictability at the three levels of linguistic complexity that we intend to\nmonitor: lexical, syntactic and semantic level. To test this hypothesis we ran\nthe Italian version of BERT with 80 sentences for a total of 900 tokens mostly\nextracted from Italian poetry of the first half of last century. Then we\nalternated canonical and noncanonical versions of the same sentence before\nprocessing them with the same DL model. We used then sentences from the\nnewswire domain containing similar syntactic structures. The results show that\nthe DL model is highly sensitive to presence of noncanonical structures.\nHowever, DLs are also very sensitive to word frequency and to local non literal\nmeaning compositional effect. This is also apparent by the preference for\npredicting function vs content words, collocates vs infrequent word phrases. In\nthe paper, we focused our attention on the use of subword units done by BERT\nfor out of vocabulary words.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Delmonte_R/0/1/0/all/0/1\">Rodolfo Delmonte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busetto_N/0/1/0/all/0/1\">Nicol&#xf2; Busetto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretability in Activation Space Analysis of Transformers: A Focused Survey. (arXiv:2302.09304v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09304","description":"<p>The field of natural language processing has reached breakthroughs with the\nadvent of transformers. They have remained state-of-the-art since then, and\nthere also has been much research in analyzing, interpreting, and evaluating\nthe attention layers and the underlying embedding space. In addition to the\nself-attention layers, the feed-forward layers in the transformer are a\nprominent architectural component. From extensive research, we observe that its\nrole is under-explored. We focus on the latent space, known as the Activation\nSpace, that consists of the neuron activations from these feed-forward layers.\nIn this survey paper, we review interpretability methods that examine the\nlearnings that occurred in this activation space. Since there exists only\nlimited research in this direction, we conduct a detailed examination of each\nwork and point out potential future directions of research. We hope our work\nprovides a step towards strengthening activation space analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vijayakumar_S/0/1/0/all/0/1\">Soniya Vijayakumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimising Human-Machine Collaboration for Efficient High-Precision Information Extraction from Text Documents. (arXiv:2302.09324v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09324","description":"<p>While humans can extract information from unstructured text with high\nprecision and recall, this is often too time-consuming to be practical.\nAutomated approaches, on the other hand, produce nearly-immediate results, but\nmay not be reliable enough for high-stakes applications where precision is\nessential. In this work, we consider the benefits and drawbacks of various\nhuman-only, human-machine, and machine-only information extraction approaches.\nWe argue for the utility of a human-in-the-loop approach in applications where\nhigh precision is required, but purely manual extraction is infeasible. We\npresent a framework and an accompanying tool for information extraction using\nweak-supervision labelling with human validation. We demonstrate our approach\non three criminal justice datasets. We find that the combination of computer\nspeed and human understanding yields precision comparable to manual annotation\nwhile requiring only a fraction of time, and significantly outperforms fully\nautomated baselines in terms of precision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Butcher_B/0/1/0/all/0/1\">Bradley Butcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zilka_M/0/1/0/all/0/1\">Miri Zilka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cook_D/0/1/0/all/0/1\">Darren Cook</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hron_J/0/1/0/all/0/1\">Jiri Hron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformadores: Fundamentos teoricos y Aplicaciones. (arXiv:2302.09327v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09327","description":"<p>Transformers are a neural network architecture originally designed for\nnatural language processing that it is now a mainstream tool for solving a wide\nvariety of problems, including natural language processing, sound, image,\nreinforcement learning, and other problems with heterogeneous input data. Its\ndistinctive feature is its self-attention system, based on attention to one's\nown sequence, which derives from the previously introduced attention system.\nThis article provides the reader with the necessary context to understand the\nmost recent research articles and presents the mathematical and algorithmic\nfoundations of the elements that make up this type of network. The different\ncomponents that make up this architecture and the variations that may exist are\nalso studied, as well as some applications of the transformer models. This\narticle is in Spanish to bring this scientific knowledge to the\nSpanish-speaking community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Torre_J/0/1/0/all/0/1\">Jordi de la Torre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Front-End Adapter: Adapting Front-End Input of Speech based Self-Supervised Learning for Speech Recognition. (arXiv:2302.09331v1 [eess.AS])","link":"http://arxiv.org/abs/2302.09331","description":"<p>Recent years have witnessed a boom in self-supervised learning (SSL) in\nvarious areas including speech processing. Speech based SSL models present\npromising performance in a range of speech related tasks. However, the training\nof SSL models is computationally expensive and a common practice is to\nfine-tune a released SSL model on the specific task. It is essential to use\nconsistent front-end input during pre-training and fine-tuning. This\nconsistency may introduce potential issues when the optimal front-end is not\nthe same as that used in pre-training. In this paper, we propose a simple but\neffective front-end adapter to address this front-end discrepancy. By\nminimizing the distance between the outputs of different front-ends, the\nfilterbank feature (Fbank) can be compatible with SSL models which are\npre-trained with waveform. The experiment results demonstrate the effectiveness\nof our proposed front-end adapter on several popular SSL models for the speech\nrecognition task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xie Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_Z/0/1/0/all/0/1\">Ziyang Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_C/0/1/0/all/0/1\">Changli Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yujin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhisheng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Out-Of-Distribution Generalization Capability of Language Models: Counterfactually-Augmented Data is not Enough. (arXiv:2302.09345v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09345","description":"<p>Counterfactually-Augmented Data (CAD) has the potential to improve language\nmodels' Out-Of-Distribution (OOD) generalization capability, as CAD induces\nlanguage models to exploit causal features and exclude spurious correlations.\nHowever, the empirical results of OOD generalization on CAD are not as\nefficient as expected. In this paper, we attribute the inefficiency to Myopia\nPhenomenon caused by CAD: language models only focus on causal features that\nare edited in the augmentation and exclude other non-edited causal features. As\na result, the potential of CAD is not fully exploited. Based on the structural\nproperties of CAD, we design two additional constraints to help language models\nextract more complete causal features contained in CAD, thus improving the OOD\ngeneralization capability. We evaluate our method on two tasks: Sentiment\nAnalysis and Natural Language Inference, and the experimental results\ndemonstrate that our method could unlock CAD's potential and improve language\nmodels' OOD generalization capability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Caoyun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jidong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yaohui Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT is not The Count: Learning to Match Mathematical Statements with Proofs. (arXiv:2302.09350v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09350","description":"<p>We introduce a task consisting in matching a proof to a given mathematical\nstatement. The task fits well within current research on Mathematical\nInformation Retrieval and, more generally, mathematical article analysis\n(Mathematical Sciences, 2014). We present a dataset for the task (the MATcH\ndataset) consisting of over 180k statement-proof pairs extracted from modern\nmathematical research articles. We find this dataset highly representative of\nour task, as it consists of relatively new findings useful to mathematicians.\nWe propose a bilinear similarity model and two decoding methods to match\nstatements to proofs effectively. While the first decoding method matches a\nproof to a statement without being aware of other statements or proofs, the\nsecond method treats the task as a global matching problem. Through a symbol\nreplacement procedure, we analyze the \"insights\" that pre-trained language\nmodels have in such mathematical article analysis and show that while these\nmodels perform well on this task with the best performing mean reciprocal rank\nof 73.7, they follow a relatively shallow symbolic analysis and matching to\nachieve that performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weixian Waylon Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziser_Y/0/1/0/all/0/1\">Yftah Ziser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coavoux_M/0/1/0/all/0/1\">Maximin Coavoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Shay B. Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language-conditioned Reinforcement Learning with Inside-out Task Language Development and Translation. (arXiv:2302.09368v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09368","description":"<p>Natural Language-conditioned reinforcement learning (RL) enables the agents\nto follow human instructions. Previous approaches generally implemented\nlanguage-conditioned RL by providing human instructions in natural language\n(NL) and training a following policy. In this outside-in approach, the policy\nneeds to comprehend the NL and manage the task simultaneously. However, the\nunbounded NL examples often bring much extra complexity for solving concrete RL\ntasks, which can distract policy learning from completing the task. To ease the\nlearning burden of the policy, we investigate an inside-out scheme for natural\nlanguage-conditioned RL by developing a task language (TL) that is task-related\nand unique. The TL is used in RL to achieve highly efficient and effective\npolicy training. Besides, a translator is trained to translate NL into TL. We\nimplement this scheme as TALAR (TAsk Language with predicAte Representation)\nthat learns multiple predicates to model object relationships as the TL.\nExperiments indicate that TALAR not only better comprehends NL instructions but\nalso leads to a better instruction-following policy that improves 13.4% success\nrate and adapts to unseen expressions of NL instruction. The TL can also be an\neffective task abstraction, naturally compatible with hierarchical RL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jing-Cheng Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin-Yu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Si-Hang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yang Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speaker and Language Change Detection using Wav2vec2 and Whisper. (arXiv:2302.09381v1 [eess.AS])","link":"http://arxiv.org/abs/2302.09381","description":"<p>We investigate recent transformer networks pre-trained for automatic speech\nrecognition for their ability to detect speaker and language changes in speech.\nWe do this by simply adding speaker (change) or language targets to the labels.\nFor Wav2vec2 pre-trained networks, we also investigate if the representation\nfor the speaker change symbol can be conditioned to capture speaker identity\ncharacteristics. Using a number of constructed data sets we show that these\ncapabilities are definitely there, with speaker recognition equal error rates\nof the order of 10% and language detection error rates of a few percent. We\nwill publish the code for reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Berns_T/0/1/0/all/0/1\">Tijn Berns</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vaessen_N/0/1/0/all/0/1\">Nik Vaessen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leeuwen_D/0/1/0/all/0/1\">David A. van Leeuwen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M-SENSE: Modeling Narrative Structure in Short Personal Narratives Using Protagonist's Mental Representations. (arXiv:2302.09418v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09418","description":"<p>Narrative is a ubiquitous component of human communication. Understanding its\nstructure plays a critical role in a wide variety of applications, ranging from\nsimple comparative analyses to enhanced narrative retrieval, comprehension, or\nreasoning capabilities. Prior research in narratology has highlighted the\nimportance of studying the links between cognitive and linguistic aspects of\nnarratives for effective comprehension. This interdependence is related to the\ntextual semantics and mental language in narratives, referring to characters'\nmotivations, feelings or emotions, and beliefs. However, this interdependence\nis hardly explored for modeling narratives. In this work, we propose the task\nof automatically detecting prominent elements of the narrative structure by\nanalyzing the role of characters' inferred mental state along with linguistic\ninformation at the syntactic and semantic levels. We introduce a STORIES\ndataset of short personal narratives containing manual annotations of key\nelements of narrative structure, specifically climax and resolution. To this\nend, we implement a computational model that leverages the protagonist's mental\nstate information obtained from a pre-trained model trained on social\ncommonsense knowledge and integrates their representations with contextual\nsemantic embed-dings using a multi-feature fusion approach. Evaluating against\nprior zero-shot and supervised baselines, we find that our model is able to\nachieve significant improvements in the task of identifying climax and\nresolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vijayaraghavan_P/0/1/0/all/0/1\">Prashanth Vijayaraghavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1\">Deb Roy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT. (arXiv:2302.09419v1 [cs.AI])","link":"http://arxiv.org/abs/2302.09419","description":"<p>The Pretrained Foundation Models (PFMs) are regarded as the foundation for\nvarious downstream tasks with different data modalities. A pretrained\nfoundation model, such as BERT, GPT-3, MAE, DALLE-E, and ChatGPT, is trained on\nlarge-scale data which provides a reasonable parameter initialization for a\nwide range of downstream applications. The idea of pretraining behind PFMs\nplays an important role in the application of large models. Different from\nprevious methods that apply convolution and recurrent modules for feature\nextractions, the generative pre-training (GPT) method applies Transformer as\nthe feature extractor and is trained on large datasets with an autoregressive\nparadigm. Similarly, the BERT apples transformers to train on large datasets as\na contextual language model. Recently, the ChatGPT shows promising success on\nlarge language models, which applies an autoregressive language model with zero\nshot or few show prompting. With the extraordinary success of PFMs, AI has made\nwaves in a variety of fields over the past few years. Considerable methods,\ndatasets, and evaluation metrics have been proposed in the literature, the need\nis raising for an updated survey. This study provides a comprehensive review of\nrecent research advancements, current and future challenges, and opportunities\nfor PFMs in text, image, graph, as well as other data modalities. We first\nreview the basic components and existing pretraining in natural language\nprocessing, computer vision, and graph learning. We then discuss other advanced\nPFMs for other data modalities and unified PFMs considering the data quality\nand quantity. Besides, we discuss relevant research about the fundamentals of\nthe PFM, including model efficiency and compression, security, and privacy.\nFinally, we lay out key implications, future research directions, challenges,\nand open problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Ce Zhou</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangjing Wang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Ji_C/0/1/0/all/0/1\">Cheng Ji</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qiben Yan</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jia Wu</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a> (5), <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengtao Xie</a> (6), <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a> (9), <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a> (7), <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a> (8), <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a> (3) ((1) Michigan State University, (2) Beihang University, (3) Lehigh University, (4) Macquarie University, (5) Nanyang Technological University, (6) University of California San Diego, (7) Duke University, (8) University of Illinois at Chicago, (9) Salesforce AI Research)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero and Few-Shot Localization of Task-Oriented Dialogue Agents with a Distilled Representation. (arXiv:2302.09424v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09424","description":"<p>Task-oriented Dialogue (ToD) agents are mostly limited to a few widely-spoken\nlanguages, mainly due to the high cost of acquiring training data for each\nlanguage. Existing low-cost approaches that rely on cross-lingual embeddings or\nnaive machine translation sacrifice a lot of accuracy for data efficiency, and\nlargely fail in creating a usable dialogue agent. We propose automatic methods\nthat use ToD training data in a source language to build a high-quality\nfunctioning dialogue agent in another target language that has no training data\n(i.e. zero-shot) or a small training set (i.e. few-shot). Unlike most prior\nwork in cross-lingual ToD that only focuses on Dialogue State Tracking (DST),\nwe build an end-to-end agent.\n</p>\n<p>We show that our approach closes the accuracy gap between few-shot and\nexisting full-shot methods for ToD agents. We achieve this by (1) improving the\ndialogue data representation, (2) improving entity-aware machine translation,\nand (3) automatic filtering of noisy translations.\n</p>\n<p>We evaluate our approach on the recent bilingual dialogue dataset BiToD. In\nChinese to English transfer, in the zero-shot setting, our method achieves\n46.7% and 22.0% in Task Success Rate (TSR) and Dialogue Success Rate (DSR)\nrespectively. In the few-shot setting where 10% of the data in the target\nlanguage is used, we improve the state-of-the-art by 15.2% and 14.0%, coming\nwithin 5% of full-shot training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moradshahi_M/0/1/0/all/0/1\">Mehrad Moradshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semnani_S/0/1/0/all/0/1\">Sina J. Semnani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1\">Monica S. Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BBT-Fin: Comprehensive Construction of Chinese Financial Domain Pre-trained Language Model, Corpus and Benchmark. (arXiv:2302.09432v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09432","description":"<p>To advance Chinese financial natural language processing (NLP), we introduce\nBBT-FinT5, a new Chinese financial pre-training language model based on the T5\nmodel. To support this effort, we have built BBT-FinCorpus, a large-scale\nfinancial corpus with approximately 300GB of raw text from four different\nsources. In general domain NLP, comprehensive benchmarks like GLUE and\nSuperGLUE have driven significant advancements in language model pre-training\nby enabling head-to-head comparisons among models. Drawing inspiration from\nthese benchmarks, we propose BBT-CFLEB, a Chinese Financial Language\nunderstanding and generation Evaluation Benchmark, which includes six datasets\ncovering both understanding and generation tasks. Our aim is to facilitate\nresearch in the development of NLP within the Chinese financial domain. Our\nmodel, corpus and benchmark are released at\nhttps://github.com/ssymmetry/BBT-FinCUGE-Applications. Our work belongs to the\nBig Bang Transformer (BBT), a large-scale pre-trained language model project.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">Dakuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiaqing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yipei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qianyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1\">Yipeng Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Mengkun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_Y/0/1/0/all/0/1\">Yingsi Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hengkui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Language Representations with Logical Inductive Bias. (arXiv:2302.09458v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09458","description":"<p>Transformer architectures have achieved great success in solving natural\nlanguage tasks, which learn strong language representations from large-scale\nunlabeled texts. In this paper, we seek to go further beyond and explore a new\nlogical inductive bias for better language representation learning. Logic\nreasoning is known as a formal methodology to reach answers from given\nknowledge and facts. Inspired by such a view, we develop a novel neural\narchitecture named FOLNet (First-Order Logic Network), to encode this new\ninductive bias. We construct a set of neural logic operators as learnable Horn\nclauses, which are further forward-chained into a fully differentiable neural\narchitecture (FOLNet). Interestingly, we find that the self-attention module in\ntransformers can be composed by two of our neural logic operators, which\nprobably explains their strong reasoning performance. Our proposed FOLNet has\nthe same input and output interfaces as other pretrained models and thus could\nbe pretrained/finetuned by using similar losses. It also allows FOLNet to be\nused in a plug-and-play manner when replacing other pretrained models. With our\nlogical inductive bias, the same set of ``logic deduction skills'' learned\nthrough pretraining are expected to be equally capable of solving diverse\ndownstream tasks. For this reason, FOLNet learns language representations that\nhave much stronger transfer capabilities. Experimental results on several\nlanguage understanding tasks show that our pretrained FOLNet model outperforms\nthe existing strong transformer-based approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianshu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video-Text Retrieval by Supervised Multi-Space Multi-Grained Alignment. (arXiv:2302.09473v1 [cs.CV])","link":"http://arxiv.org/abs/2302.09473","description":"<p>While recent progress in video-text retrieval has been advanced by the\nexploration of better representation learning, in this paper, we present a\nnovel multi-space multi-grained supervised learning framework, SUMA, to learn\nan aligned representation space shared between the video and the text for\nvideo-text retrieval. The shared aligned space is initialized with a finite\nnumber of concept clusters, each of which refers to a number of basic concepts\n(words). With the text data at hand, we are able to update the shared aligned\nspace in a supervised manner using the proposed similarity and alignment\nlosses. Moreover, to enable multi-grained alignment, we incorporate frame\nrepresentations for better modeling the video modality and calculating\nfine-grained and coarse-grained similarity. Benefiting from learned shared\naligned space and multi-grained similarity, extensive experiments on several\nvideo-text retrieval benchmarks demonstrate the superiority of SUMA over\nexisting methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yimu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Peng Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Classification in the Wild: a Large-scale Long-tailed Name Normalization Dataset. (arXiv:2302.09509v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09509","description":"<p>Real-world data usually exhibits a long-tailed distribution,with a few\nfrequent labels and a lot of few-shot labels. The study of institution name\nnormalization is a perfect application case showing this phenomenon. There are\nmany institutions worldwide with enormous variations of their names in the\npublicly available literature. In this work, we first collect a large-scale\ninstitution name normalization dataset LoT-insts1, which contains over 25k\nclasses that exhibit a naturally long-tailed distribution. In order to isolate\nthe few-shot and zero-shot learning scenarios from the massive many-shot\nclasses, we construct our test set from four different subsets: many-, medium-,\nand few-shot sets, as well as a zero-shot open set. We also replicate several\nimportant baseline methods on our data, covering a wide range from search-based\nmethods to neural network methods that use the pretrained BERT model. Further,\nwe propose our specially pretrained, BERT-based model that shows better\nout-of-distribution generalization on few-shot and zero-shot test sets.\nCompared to other datasets focusing on the long-tailed phenomenon, our dataset\nhas one order of magnitude more training data than the largest existing\nlong-tailed datasets and is naturally long-tailed rather than manually\nsynthesized. We believe it provides an important and different scenario to\nstudy this problem. To our best knowledge, this is the first natural language\ndataset that focuses on long-tailed and open-set classification problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jiexing Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhixin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yusheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chenghu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinbing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SanskritShala: A Neural Sanskrit NLP Toolkit with Web-Based Interface for Pedagogical and Annotation Purposes. (arXiv:2302.09527v1 [cs.CL])","link":"http://arxiv.org/abs/2302.09527","description":"<p>We present a neural Sanskrit Natural Language Processing (NLP) toolkit named\nSanskritShala (a school of Sanskrit) to facilitate computational linguistic\nanalyses for several tasks such as word segmentation, morphological tagging,\ndependency parsing, and compound type identification. Our systems currently\nreport state-of-the-art performance on available benchmark datasets for all\ntasks. SanskritShala is deployed as a web-based application, which allows a\nuser to get real-time analysis for the given input. It is built with\neasy-to-use interactive data annotation features that allow annotators to\ncorrect the system predictions when it makes mistakes. We publicly release the\nsource codes of the 4 modules included in the toolkit, 7 word embedding models\nthat have been trained on publicly available Sanskrit corpora and multiple\nannotated datasets such as word similarity, relatedness, categorization,\nanalogy prediction to assess intrinsic properties of word embeddings. So far as\nwe know, this is the first neural-based Sanskrit NLP toolkit that has a\nweb-based interface and a number of NLP modules. We are sure that the people\nwho are willing to work with Sanskrit will find it useful for pedagogical and\nannotative purposes. SanskritShala is available at:\nhttps://cnerg.iitkgp.ac.in/sanskritshala. The demo video of our platform can be\naccessed at: https://youtu.be/x0X31Y9k0mw4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sandhan_J/0/1/0/all/0/1\">Jivnesh Sandhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Anshul Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behera_L/0/1/0/all/0/1\">Laxmidhar Behera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandhan_T/0/1/0/all/0/1\">Tushar Sandhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Upvotes? Downvotes? No Votes? Understanding the relationship between reaction mechanisms and political discourse on Reddit. (arXiv:2302.09540v1 [cs.CY])","link":"http://arxiv.org/abs/2302.09540","description":"<p>A significant share of political discourse occurs online on social media\nplatforms. Policymakers and researchers try to understand the role of social\nmedia design in shaping the quality of political discourse around the globe. In\nthe past decades, scholarship on political discourse theory has produced\ndistinct characteristics of different types of prominent political rhetoric\nsuch as deliberative, civic, or demagogic discourse. This study investigates\nthe relationship between social media reaction mechanisms (i.e., upvotes,\ndownvotes) and political rhetoric in user discussions by engaging in an\nin-depth conceptual analysis of political discourse theory. First, we analyze\n155 million user comments in 55 political subforums on Reddit between 2010 and\n2018 to explore whether users' style of political discussion aligns with the\nessential components of deliberative, civic, and demagogic discourse. Second,\nwe perform a quantitative study that combines confirmatory factor analysis with\ndifference in differences models to explore whether different reaction\nmechanism schemes (e.g., upvotes only, upvotes and downvotes, no reaction\nmechanisms) correspond with political user discussion that is more or less\ncharacteristic of deliberative, civic, or demagogic discourse. We produce three\nmain takeaways. First, despite being \"ideal constructs of political rhetoric,\"\nwe find that political discourse theories describe political discussions on\nReddit to a large extent. Second, we find that discussions in subforums with\nonly upvotes, or both up- and downvotes are associated with user discourse that\nis more deliberate and civic. Third, social media discussions are most\ndemagogic in subreddits with no reaction mechanisms at all. These findings\noffer valuable contributions for ongoing policy discussions on the relationship\nbetween social media interface design and respectful political discussion among\nusers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papakyriakopoulos_O/0/1/0/all/0/1\">Orestis Papakyriakopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelmann_S/0/1/0/all/0/1\">Severin Engelmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winecoff_A/0/1/0/all/0/1\">Amy Winecoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Social Media Manipulation in Low-Resource Languages. (arXiv:2011.05367v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2011.05367","description":"<p>Social media have been deliberately used for malicious purposes, including\npolitical manipulation and disinformation. Most research focuses on\nhigh-resource languages. However, malicious actors share content across\ncountries and languages, including low-resource ones. Here, we investigate\nwhether and to what extent malicious actors can be detected in low-resource\nlanguage settings. We discovered that a high number of accounts posting in\nTagalog were suspended as part of Twitter's crackdown on interference\noperations after the 2016 US Presidential election. By combining text embedding\nand transfer learning, our framework can detect, with promising accuracy,\nmalicious users posting in Tagalog without any prior knowledge or training on\nmalicious content in that language. We first learn an embedding model for each\nlanguage, namely a high-resource language (English) and a low-resource one\n(Tagalog), independently. Then, we learn a mapping between the two latent\nspaces to transfer the detection model. We demonstrate that the proposed\napproach significantly outperforms state-of-the-art models, including BERT, and\nyields marked advantages in settings with very limited training data -- the\nnorm when dealing with detecting malicious activity in online platforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haider_S/0/1/0/all/0/1\">Samar Haider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luceri_L/0/1/0/all/0/1\">Luca Luceri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deb_A/0/1/0/all/0/1\">Ashok Deb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badawy_A/0/1/0/all/0/1\">Adam Badawy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrara_E/0/1/0/all/0/1\">Emilio Ferrara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-adaptive Pre-training and Self-training are Complementary for Natural Language Understanding. (arXiv:2109.06466v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06466","description":"<p>Task-adaptive pre-training (TAPT) and Self-training (ST) have emerged as the\nmajor semi-supervised approaches to improve natural language understanding\n(NLU) tasks with massive amount of unlabeled data. However, it's unclear\nwhether they learn similar representations or they can be effectively combined.\nIn this paper, we show that TAPT and ST can be complementary with simple TFS\nprotocol by following TAPT -&gt; Finetuning -&gt; Self-training (TFS) process.\nExperimental results show that TFS protocol can effectively utilize unlabeled\ndata to achieve strong combined gains consistently across six datasets covering\nsentiment classification, paraphrase identification, natural language\ninference, named entity recognition and dialogue slot classification. We\ninvestigate various semi-supervised settings and consistently show that gains\nfrom TAPT and ST can be strongly additive by following TFS procedure. We hope\nthat TFS could serve as an important semi-supervised baseline for future NLP\nstudies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shiyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1\">Semih Yavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xifeng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Explainable Phrasal Reasoning with Neural Fuzzy Logic. (arXiv:2109.08927v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08927","description":"<p>Natural language inference (NLI) aims to determine the logical relationship\nbetween two sentences, such as Entailment, Contradiction, and Neutral. In\nrecent years, deep learning models have become a prevailing approach to NLI,\nbut they lack interpretability and explainability. In this work, we address the\nexplainability of NLI by weakly supervised logical reasoning, and propose an\nExplainable Phrasal Reasoning (EPR) approach. Our model first detects phrases\nas the semantic unit and aligns corresponding phrases in the two sentences.\nThen, the model predicts the NLI label for the aligned phrases, and induces the\nsentence label by fuzzy logic formulas. Our EPR is almost everywhere\ndifferentiable and thus the system can be trained end to end. In this way, we\nare able to provide explicit explanations of phrasal logical relationships in a\nweakly supervised manner. We further show that such reasoning results help\ntextual explanation generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zijun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zi Xuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1\">Atharva Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_Z/0/1/0/all/0/1\">Zhijian Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firdaus_M/0/1/0/all/0/1\">Mauajama Firdaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lili Mou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Semantic Parsing for Multilingual Task-Oriented Dialogues. (arXiv:2111.02574v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.02574","description":"<p>Robust state tracking for task-oriented dialogue systems currently remains\nrestricted to a few popular languages. This paper shows that given a\nlarge-scale dialogue data set in one language, we can automatically produce an\neffective semantic parser for other languages using machine translation. We\npropose automatic translation of dialogue datasets with alignment to ensure\nfaithful translation of slot values and eliminate costly human supervision used\nin previous benchmarks. We also propose a new contextual semantic parsing\nmodel, which encodes the formal slots and values, and only the last agent and\nuser utterances. We show that the succinct representation reduces the\ncompounding effect of translation errors, without harming the accuracy in\npractice.\n</p>\n<p>We evaluate our approach on several dialogue state tracking benchmarks. On\nRiSAWOZ, CrossWOZ, CrossWOZ-EN, and MultiWOZ-ZH datasets we improve the state\nof the art by 11%, 17%, 20%, and 0.3% in joint goal accuracy. We present a\ncomprehensive error analysis for all three datasets showing erroneous\nannotations can lead to misguided judgments on the quality of the model.\n</p>\n<p>Finally, we present RiSAWOZ English and German datasets, created using our\ntranslation methodology. On these datasets, accuracy is within 11% of the\noriginal showing that high-accuracy multilingual dialogue datasets are possible\nwithout relying on expensive human annotations. We release our datasets and\nsoftware open source.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moradshahi_M/0/1/0/all/0/1\">Mehrad Moradshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_V/0/1/0/all/0/1\">Victoria Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campagna_G/0/1/0/all/0/1\">Giovanni Campagna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1\">Monica S. Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploration into Translation-Equivariant Image Quantization. (arXiv:2112.00384v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00384","description":"<p>This is an exploratory study that discovers the current image quantization\n(vector quantization) do not satisfy translation equivariance in the quantized\nspace due to aliasing. Instead of focusing on anti-aliasing, we propose a\nsimple yet effective way to achieve translation-equivariant image quantization\nby enforcing orthogonality among the codebook embeddings. To explore the\nadvantages of translation-equivariant image quantization, we conduct three\nproof-of-concept experiments with a carefully controlled dataset: (1)\ntext-to-image generation, where the quantized image indices are the target to\npredict, (2) image-to-text generation, where the quantized image indices are\ngiven as a condition, (3) using a smaller training set to analyze sample\nefficiency. From the strictly controlled experiments, we empirically verify\nthat the translation-equivariant image quantizer improves not only sample\nefficiency but also the accuracy over VQGAN up to +11.9% in text-to-image\ngeneration and +3.9% in image-to-text generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1\">Woncheol Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gyubok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jiyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyou_E/0/1/0/all/0/1\">Eunyi Lyou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joonseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-assisted prompt editing to improve GPT-3 after deployment. (arXiv:2201.06009v7 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.06009","description":"<p>Large LMs such as GPT-3 are powerful, but can commit mistakes that are\nobvious to humans. For example, GPT-3 would mistakenly interpret \"What word is\nsimilar to good?\" to mean a homophone, while the user intended a synonym. Our\ngoal is to effectively correct such errors via user interactions with the\nsystem but without retraining, which will be prohibitively costly. We pair\nGPT-3 with a growing memory of recorded cases where the model misunderstood the\nuser's intents, along with user feedback for clarification. Such a memory\nallows our system to produce enhanced prompts for any new query based on the\nuser feedback for error correction on similar cases in the past. On four tasks\n(two lexical tasks, two advanced ethical reasoning tasks), we show how a\n(simulated) user can interactively teach a deployed GPT-3, substantially\nincreasing its accuracy over the queries with different kinds of\nmisunderstandings by the GPT-3. Our approach is a step towards the low-cost\nutility enhancement for very large pre-trained LMs. Code, data, and\ninstructions to implement MEMPROMPT for a new task at\nhttps://www.memprompt.com/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_N/0/1/0/all/0/1\">Niket Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HiURE: Hierarchical Exemplar Contrastive Learning for Unsupervised Relation Extraction. (arXiv:2205.02225v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.02225","description":"<p>Unsupervised relation extraction aims to extract the relationship between\nentities from natural language sentences without prior information on\nrelational scope or distribution. Existing works either utilize self-supervised\nschemes to refine relational feature signals by iteratively leveraging adaptive\nclustering and classification that provoke gradual drift problems, or adopt\ninstance-wise contrastive learning which unreasonably pushes apart those\nsentence pairs that are semantically similar. To overcome these defects, we\npropose a novel contrastive learning framework named HiURE, which has the\ncapability to derive hierarchical signals from relational feature space using\ncross hierarchy attention and effectively optimize relation representation of\nsentences under exemplar-wise contrastive learning. Experimental results on two\npublic datasets demonstrate the advanced effectiveness and robustness of HiURE\non unsupervised relation extraction when compared with state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shu`ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unveiling Transformers with LEGO: a synthetic reasoning task. (arXiv:2206.04301v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.04301","description":"<p>We propose a synthetic reasoning task, LEGO (Learning Equality and Group\nOperations), that encapsulates the problem of following a chain of reasoning,\nand we study how the Transformer architectures learn this task. We pay special\nattention to data effects such as pretraining (on seemingly unrelated NLP\ntasks) and dataset composition (e.g., differing chain length at training and\ntest time), as well as architectural variants such as weight-tied layers or\nadding convolutional components. We study how the trained models eventually\nsucceed at the task, and in particular, we manage to understand some of the\nattention heads as well as how the information flows in the network. In\nparticular, we have identified a novel \\emph{association} pattern that globally\nattends only to identical tokens. Based on these observations we propose a\nhypothesis that here pretraining helps for LEGO tasks due to certain structured\nattention patterns, and we experimentally verify this hypothesis. We also\nobserve that in some data regime the trained transformer finds ``shortcut\"\nsolutions to follow the chain of reasoning, which impedes the model's\nrobustness, and moreover we propose ways to prevent it. Motivated by our\nfindings on structured attention patterns, we propose the LEGO attention\nmodule, a drop-in replacement for vanilla attention heads. This architectural\nchange significantly reduces Flops and maintains or even \\emph{improves} the\nmodel's performance at large-scale pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Backurs_A/0/1/0/all/0/1\">Arturs Backurs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1\">S&#xe9;bastien Bubeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eldan_R/0/1/0/all/0/1\">Ronen Eldan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunasekar_S/0/1/0/all/0/1\">Suriya Gunasekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_T/0/1/0/all/0/1\">Tal Wagner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiViz: Towards Visualizing and Understanding Multimodal Models. (arXiv:2207.00056v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2207.00056","description":"<p>The promise of multimodal models for real-world applications has inspired\nresearch in visualizing and understanding their internal mechanics with the end\ngoal of empowering stakeholders to visualize model behavior, perform model\ndebugging, and promote trust in machine learning models. However, modern\nmultimodal models are typically black-box neural networks, which makes it\nchallenging to understand their internal mechanics. How can we visualize the\ninternal modeling of multimodal interactions in these models? Our paper aims to\nfill this gap by proposing MultiViz, a method for analyzing the behavior of\nmultimodal models by scaffolding the problem of interpretability into 4 stages:\n(1) unimodal importance: how each modality contributes towards downstream\nmodeling and prediction, (2) cross-modal interactions: how different modalities\nrelate with each other, (3) multimodal representations: how unimodal and\ncross-modal interactions are represented in decision-level features, and (4)\nmultimodal prediction: how decision-level features are composed to make a\nprediction. MultiViz is designed to operate on diverse modalities, models,\ntasks, and research areas. Through experiments on 8 trained models across 6\nreal-world tasks, we show that the complementary stages in MultiViz together\nenable users to (1) simulate model predictions, (2) assign interpretable\nconcepts to features, (3) perform error analysis on model misclassifications,\nand (4) use insights from error analysis to debug models. MultiViz is publicly\navailable, will be regularly updated with new interpretation tools and metrics,\nand welcomes inputs from the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1\">Nihal Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zihao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DocPrompting: Generating Code by Retrieving the Docs. (arXiv:2207.05987v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.05987","description":"<p>Publicly available source-code libraries are continuously growing and\nchanging. This makes it impossible for models of code to keep current with all\navailable APIs by simply training these models on existing code repositories.\nThus, existing models inherently cannot generalize to using unseen functions\nand libraries, because these would never appear in the training data. In\ncontrast, when human programmers use functions and libraries for the first\ntime, they frequently refer to textual resources such as code manuals and\ndocumentation, to explore and understand the available functionality. Inspired\nby this observation, we introduce DocPrompting: a natural-language-to-code\ngeneration approach that explicitly leverages documentation by (1) retrieving\nthe relevant documentation pieces given an NL intent, and (2) generating code\nbased on the NL intent and the retrieved documentation. DocPrompting is\ngeneral: it can be applied to any programming language and is agnostic to the\nunderlying neural model. We demonstrate that DocPrompting consistently improves\nNL-to-code models: DocPrompting improves strong base models such as CodeT5 by\n2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in\nexecution-based evaluation on the popular Python CoNaLa benchmark; on a new\nBash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to\nabsolute 6.9% exact match.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1\">Uri Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Frank F. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiruo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengbao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpreting Embedding Spaces by Conceptualization. (arXiv:2209.00445v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.00445","description":"<p>One of the main methods for semantic interpretation of text is mapping it\ninto a vector in some embedding space. Such vectors can then be used for a\nvariety of text processing tasks. Recently, most embedding spaces are a product\nof training large language models. One major drawback of this type of\nrepresentation is its incomprehensibility to humans. Understanding the\nembedding space is crucial for several important needs, including the need to\nexplain the decision of a system that uses the embedding, the need to debug the\nembedding method and compare it to alternatives, and the need to detect biases\nhidden in the model. In this paper, we present a novel method of transforming\nany embedding space into a comprehensible conceptual space. We first present an\nalgorithm for deriving a conceptual space with dynamic on-demand granularity.\nWe then show a method for transferring any vector in the original\nincomprehensible space to an understandable vector in the conceptual space. We\ncombine human tests with cross-model tests to show that the conceptualized\nvectors indeed represent the semantics of the original vectors. We also show\nhow the conceptualized vectors can be used for various tasks including\nidentifying weaknesses in the semantics underlying the original spaces and\ndifferences in the semantics of alternative models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Simhi_A/0/1/0/all/0/1\">Adi Simhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markovitch_S/0/1/0/all/0/1\">Shaul Markovitch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Foundations and Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions. (arXiv:2209.03430v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2209.03430","description":"<p>Multimodal machine learning is a vibrant multi-disciplinary research field\nthat aims to design computer agents with intelligent capabilities such as\nunderstanding, reasoning, and learning through integrating multiple\ncommunicative modalities, including linguistic, acoustic, visual, tactile, and\nphysiological messages. With the recent interest in video understanding,\nembodied autonomous agents, text-to-image generation, and multisensor fusion in\napplication domains such as healthcare and robotics, multimodal machine\nlearning has brought unique computational and theoretical challenges to the\nmachine learning community given the heterogeneity of data sources and the\ninterconnections often found between modalities. However, the breadth of\nprogress in multimodal research has made it difficult to identify the common\nthemes and open questions in the field. By synthesizing a broad range of\napplication domains and theoretical frameworks from both historical and recent\nperspectives, this paper is designed to provide an overview of the\ncomputational and theoretical foundations of multimodal machine learning. We\nstart by defining three key principles of modality heterogeneity, connections,\nand interactions that have driven subsequent innovations, and propose a\ntaxonomy of six core technical challenges: representation, alignment,\nreasoning, generation, transference, and quantification covering historical and\nrecent trends. Recent technical achievements will be presented through the lens\nof this taxonomy, allowing researchers to understand the similarities and\ndifferences across new approaches. We end by motivating several open problems\nfor future research as identified by our taxonomy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zadeh_A/0/1/0/all/0/1\">Amir Zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaXM: Towards Multilingual Visual Question Answering. (arXiv:2209.05401v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.05401","description":"<p>Visual Question Answering (VQA) has been primarily studied through the lens\nof the English language. Yet, tackling VQA in other languages in the same\nmanner would require a considerable amount of resources. In this paper, we\npropose scalable solutions to multilingual visual question answering (mVQA), on\nboth data and modeling fronts. We first propose a translation-based framework\nto mVQA data generation that requires much less human annotation efforts than\nthe conventional approach of directly collection questions and answers. Then,\nwe apply our framework to the multilingual captions in the Crossmodal-3600\ndataset and develop an efficient annotation protocol to create MaXM, a\ntest-only VQA benchmark in 7 diverse languages. Finally, we propose an approach\nto unified, extensible, open-ended, and end-to-end mVQA modeling and\ndemonstrate strong performance in 13 languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Changpinyo_S/0/1/0/all/0/1\">Soravit Changpinyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1\">Linting Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szpektor_I/0/1/0/all/0/1\">Idan Szpektor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thapliyal_A/0/1/0/all/0/1\">Ashish V. Thapliyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amelot_J/0/1/0/all/0/1\">Julien Amelot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yarom_M/0/1/0/all/0/1\">Michal Yarom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Relation between Sensitivity and Accuracy in In-context Learning. (arXiv:2209.07661v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.07661","description":"<p>In-context learning (ICL) suffers from oversensitivity to the prompt, making\nit unreliable in real-world scenarios. We study the sensitivity of ICL with\nrespect to multiple perturbation types. First, we find that label bias obscures\nthe true sensitivity, and therefore prior work may have significantly\nunderestimated ICL sensitivity. Second, we observe a strong negative\ncorrelation between ICL sensitivity and accuracy: predictions sensitive to\nperturbations are less likely to be correct. Motivated by these findings, we\npropose \\textsc{SenSel}, a few-shot selective prediction method that abstains\nfrom sensitive predictions. Experiments on ten classification datasets show\nthat \\textsc{SenSel} consistently outperforms two commonly used\nconfidence-based and entropy-based baselines on abstention decisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanda Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViLPAct: A Benchmark for Compositional Generalization on Multimodal Human Activities. (arXiv:2210.05556v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.05556","description":"<p>We introduce ViLPAct, a novel vision-language benchmark for human activity\nplanning. It is designed for a task where embodied AI agents can reason and\nforecast future actions of humans based on video clips about their initial\nactivities and intents in text. The dataset consists of 2.9k videos from\n\\charades extended with intents via crowdsourcing, a multi-choice question test\nset, and four strong baselines. One of the baselines implements a neurosymbolic\napproach based on a multi-modal knowledge base (MKB), while the other ones are\ndeep generative models adapted from recent state-of-the-art (SOTA) methods.\nAccording to our extensive experiments, the key challenges are compositional\ngeneralization and effective use of information from both modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1\">Terry Yue Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yaqing Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yuecheng Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lizhen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1\">Gerard de Melo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yazhou Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Translation Memories into Non-Autoregressive Machine Translation. (arXiv:2210.06020v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06020","description":"<p>Non-autoregressive machine translation (NAT) has recently made great\nprogress. However, most works to date have focused on standard translation\ntasks, even though some edit-based NAT models, such as the Levenshtein\nTransformer (LevT), seem well suited to translate with a Translation Memory\n(TM). This is the scenario considered here. We first analyze the vanilla LevT\nmodel and explain why it does not do well in this setting. We then propose a\nnew variant, TM-LevT, and show how to effectively train this model. By\nmodifying the data presentation and introducing an extra deletion operation, we\nobtain performance that are on par with an autoregressive approach, while\nreducing the decoding load. We also show that incorporating TMs during training\ndispenses to use knowledge distillation, a well-known trick used to mitigate\nthe multimodality issue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jitao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crego_J/0/1/0/all/0/1\">Josep Crego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yvon_F/0/1/0/all/0/1\">Fran&#xe7;ois Yvon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Non-transferable Text Classification. (arXiv:2210.12651v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12651","description":"<p>Training a good deep learning model requires substantial data and computing\nresources, which makes the resulting neural model a valuable intellectual\nproperty. To prevent the neural network from being undesirably exploited,\nnon-transferable learning has been proposed to reduce the model generalization\nability in specific target domains. However, existing approaches require\nlabeled data for the target domain which can be difficult to obtain.\nFurthermore, they do not have the mechanism to still recover the model's\nability to access the target domain. In this paper, we propose a novel\nunsupervised non-transferable learning method for the text classification task\nthat does not require annotated target domain data. We further introduce a\nsecret key component in our approach for recovering the access to the target\ndomain, where we design both an explicit and an implicit method for doing so.\nExtensive experiments demonstrate the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Guangtao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEBERT: Efficient and Robust Binary Ensemble BERT. (arXiv:2210.15976v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.15976","description":"<p>Pre-trained BERT models have achieved impressive accuracy on natural language\nprocessing (NLP) tasks. However, their excessive amount of parameters hinders\nthem from efficient deployment on edge devices. Binarization of the BERT models\ncan significantly alleviate this issue but comes with a severe accuracy drop\ncompared with their full-precision counterparts. In this paper, we propose an\nefficient and robust binary ensemble BERT (BEBERT) to bridge the accuracy gap.\nTo the best of our knowledge, this is the first work employing ensemble\ntechniques on binary BERTs, yielding BEBERT, which achieves superior accuracy\nwhile retaining computational efficiency. Furthermore, we remove the knowledge\ndistillation procedures during ensemble to speed up the training process\nwithout compromising accuracy. Experimental results on the GLUE benchmark show\nthat the proposed BEBERT significantly outperforms the existing binary BERT\nmodels in accuracy and robustness with a 2x speedup on training time. Moreover,\nour BEBERT has only a negligible accuracy loss of 0.3% compared to the\nfull-precision baseline while saving 15x and 13x in FLOPs and model size,\nrespectively. In addition, BEBERT also outperforms other compressed BERTs in\naccuracy by up to 6.7%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jiayi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Chao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haonan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongfeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Multimodal Entity-Relation Extraction Based on Edge-enhanced Graph Alignment Network and Word-pair Relation Tagging. (arXiv:2211.15028v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.15028","description":"<p>Multimodal named entity recognition (MNER) and multimodal relation extraction\n(MRE) are two fundamental subtasks in the multimodal knowledge graph\nconstruction task. However, the existing methods usually handle two tasks\nindependently, which ignores the bidirectional interaction between them. This\npaper is the first to propose jointly performing MNER and MRE as a joint\nmultimodal entity-relation extraction task (JMERE). Besides, the current MNER\nand MRE models only consider aligning the visual objects with textual entities\nin visual and textual graphs but ignore the entity-entity relationships and\nobject-object relationships. To address the above challenges, we propose an\nedge-enhanced graph alignment network and a word-pair relation tagging (EEGA)\nfor JMERE task. Specifically, we first design a word-pair relation tagging to\nexploit the bidirectional interaction between MNER and MRE and avoid the error\npropagation. Then, we propose an edge-enhanced graph alignment network to\nenhance the JMERE task by aligning nodes and edges in the cross-graph. Compared\nwith previous methods, the proposed method can leverage the edge information to\nauxiliary alignment between objects and entities and find the correlations\nbetween entity-entity relationships and object-object relationships.\nExperiments are conducted to show the effectiveness of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Li Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLAM: Selective Clarification for Ambiguous Questions with Generative Language Models. (arXiv:2212.07769v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.07769","description":"<p>Users often ask dialogue systems ambiguous questions that require\nclarification. We show that current language models rarely ask users to clarify\nambiguous questions and instead provide incorrect answers. To address this, we\nintroduce CLAM: a framework for getting language models to selectively ask for\nclarification about ambiguous user questions. In particular, we show that we\ncan prompt language models to detect whether a given question is ambiguous,\ngenerate an appropriate clarifying question to ask the user, and give a final\nanswer after receiving clarification. We also show that we can simulate users\nby providing language models with privileged information. This lets us\nautomatically evaluate multi-turn clarification dialogues. Finally, CLAM\nsignificantly improves language models' accuracy on mixed ambiguous and\nunambiguous questions relative to SotA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuhn_L/0/1/0/all/0/1\">Lorenz Kuhn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farquhar_S/0/1/0/all/0/1\">Sebastian Farquhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How would Stance Detection Techniques Evolve after the Launch of ChatGPT?. (arXiv:2212.14548v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.14548","description":"<p>Stance detection refers to the task of extracting the standpoint (Favor,\nAgainst or Neither) towards a target in given texts. Such research gains\nincreasing attention with the proliferation of social media contents. The\nconventional framework of handling stance detection is converting it into text\nclassification tasks. Deep learning models have already replaced rule-based\nmodels and traditional machine learning models in solving such problems.\nCurrent deep neural networks are facing two main challenges which are\ninsufficient labeled data and information in social media posts and the\nunexplainable nature of deep learning models. A new pre-trained language model\nchatGPT was launched on Nov 30, 2022. For the stance detection tasks, our\nexperiments show that ChatGPT can achieve SOTA or similar performance for\ncommonly used datasets including SemEval-2016 and P-Stance. At the same time,\nChatGPT can provide explanation for its own prediction, which is beyond the\ncapability of any existing model. The explanations for the cases it cannot\nprovide classification results are especially useful. ChatGPT has the potential\nto be the best AI model for stance detection tasks in NLP, or at least change\nthe research paradigm of this field. ChatGPT also opens up the possibility of\nbuilding explanatory AI for stance detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">Daijun Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Liwen Jing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring AI Ethics of ChatGPT: A Diagnostic Analysis. (arXiv:2301.12867v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.12867","description":"<p>Recent breakthroughs in natural language processing (NLP) have permitted the\nsynthesis and comprehension of coherent text in an open-ended way, therefore\ntranslating the theoretical algorithms into practical applications. The large\nlanguage-model (LLM) has significantly impacted businesses such as report\nsummarization softwares and copywriters. Observations indicate, however, that\nLLMs may exhibit social prejudice and toxicity, posing ethical and societal\ndangers of consequences resulting from irresponsibility. Large-scale benchmarks\nfor accountable LLMs should consequently be developed. Although several\nempirical investigations reveal the existence of a few ethical difficulties in\nadvanced LLMs, there is no systematic examination and user study of the ethics\nof current LLMs use. To further educate future efforts on constructing ethical\nLLMs responsibly, we perform a qualitative research method on OpenAI's ChatGPT\nto better understand the practical features of ethical dangers in recent LLMs.\nWe analyze ChatGPT comprehensively from four perspectives: 1) \\textit{Bias} 2)\n\\textit{Reliability} 3) \\textit{Robustness} 4) \\textit{Toxicity}. In accordance\nwith our stated viewpoints, we empirically benchmark ChatGPT on multiple sample\ndatasets. We find that a significant number of ethical risks cannot be\naddressed by existing benchmarks, and hence illustrate them via additional case\nstudies. In addition, we examine the implications of our findings on the AI\nethics of ChatGPT, as well as future problems and practical design\nconsiderations for LLMs. We believe that our findings may give light on future\nefforts to determine and mitigate the ethical hazards posed by machines in LLM\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1\">Terry Yue Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yujin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chunyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1\">Zhenchang Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Greedy Ordering of Layer Weight Matrices in Transformers Improves Translation. (arXiv:2302.02123v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.02123","description":"<p>Prior work has attempted to understand the internal structures and\nfunctionalities of Transformer-based encoder-decoder architectures on the level\nof multi-head attention and feed-forward sublayers. Interpretations have\nfocused on the encoder and decoder, along with the combinatorial possibilities\nof the self-attention, cross-attention, and feed-forward sublayers. However,\nwithout examining the low-level structures, one gains limited understanding of\nthe motivation behind sublayer reordering. Could we dive into the sublayer\nabstraction and permute layer weight matrices to improve the quality of\ntranslation? We propose AEIUOrder to greedily reorder layer weight matrices in\nthe encoder by their well-trainedness, as measured by Heavy-Tailed\nSelf-Regularization (HT-SR) metrics, and order the decoder matrices\ncorrespondingly. Our results suggest that greedily reordering layer weight\nmatrices to maximize Total well-trainedness facilitates the model to learn\nrepresentations and generate translations more effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_E/0/1/0/all/0/1\">Elicia Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain of Hindsight Aligns Language Models with Feedback. (arXiv:2302.02676v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.02676","description":"<p>Learning from human preferences is important for language models to be\nhelpful and useful for humans, and to align with human and social values. Prior\nwork have achieved remarkable successes by learning from human feedback to\nunderstand and follow instructions. They belong to two categories supervised\nfinetuning and RLHF. Supervised finetuning is based on curated model\ngenerations that are preferred by human labelers, a key limitation of them is\nthat supervised finetuning cannot learn from negative ratings; models are only\ntrained on positive feedback, which makes it data inefficient and difficult to\ngeneralize. While RLHF can learn from all feedback by learning a reward\nfunction and RL optimization, it suffers from imperfect reward function and RL\nis very hard to tune. In this work, we propose a novel technique that addresses\nthe limitations of both supervised finetuning and RLHF, our method, Chain of\nHindsight, aligns language models with all feedback without using reinforcement\nlearning. Our idea is motivated by how humans learn from hindsight experience,\nand we turn all feedback into a sentence to finetune model in order to leverage\nthe language understanding abilities of language models. We condition the model\non a sequence of model generations paired with hindsight feedback, and finetune\nthe model to predict the most preferred output. By doing so, models can learn\nto identify and correct negative attributes or errors. Applying our method to\nGPT-J, we observe that it substantially outperforms both supervised finetuning\nand RLHF on summarization and dialogue tasks and is significantly more\npreferred in human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sferrazza_C/0/1/0/all/0/1\">Carmelo Sferrazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient and Flexible Topic Modeling using Pretrained Embeddings and Bag of Sentences. (arXiv:2302.03106v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.03106","description":"<p>Pre-trained language models have led to a new state-of-the-art in many NLP\ntasks. However, for topic modeling, statistical generative models such as LDA\nare still prevalent, which do not easily allow incorporating contextual word\nvectors. They might yield topics that do not align very well with human\njudgment. In this work, we propose a novel topic modeling and inference\nalgorithm. We suggest a bag of sentences (BoS) approach using sentences as the\nunit of analysis. We leverage pre-trained sentence embeddings by combining\ngenerative process models with clustering. We derive a fast inference algorithm\nbased on expectation maximization, hard assignments, and an annealing process.\nOur evaluation shows that our method yields state-of-the art results with\nrelatively little computational demands. Our methods is more flexible compared\nto prior works leveraging word embeddings, since it provides the possibility to\ncustomize topic-document distributions using priors. Code is at\n\\url{https://github.com/JohnTailor/BertSenClu}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1\">Johannes Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linguistic ambiguity analysis in ChatGPT. (arXiv:2302.06426v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.06426","description":"<p>Linguistic ambiguity is and has always been one of the main challenges in\nNatural Language Processing (NLP) systems. Modern Transformer architectures\nlike BERT, T5 or more recently InstructGPT have achieved some impressive\nimprovements in many NLP fields, but there is still plenty of work to do.\nMotivated by the uproar caused by ChatGPT, in this paper we provide an\nintroduction to linguistic ambiguity, its varieties and their relevance in\nmodern NLP, and perform an extensive empiric analysis. ChatGPT strengths and\nweaknesses are revealed, as well as strategies to get the most of this model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ortega_Martin_M/0/1/0/all/0/1\">Miguel Ortega-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Sierra_O/0/1/0/all/0/1\">&#xd3;scar Garc&#xed;a-Sierra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardoiz_A/0/1/0/all/0/1\">Alfonso Ardoiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jorge &#xc1;lvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armenteros_J/0/1/0/all/0/1\">Juan Carlos Armenteros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_A/0/1/0/all/0/1\">Adri&#xe1;n Alonso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Capacity for Moral Self-Correction in Large Language Models. (arXiv:2302.07459v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.07459","description":"<p>We test the hypothesis that language models trained with reinforcement\nlearning from human feedback (RLHF) have the capability to \"morally\nself-correct\" -- to avoid producing harmful outputs -- if instructed to do so.\nWe find strong evidence in support of this hypothesis across three different\nexperiments, each of which reveal different facets of moral self-correction. We\nfind that the capability for moral self-correction emerges at 22B model\nparameters, and typically improves with increasing model size and RLHF\ntraining. We believe that at this level of scale, language models obtain two\ncapabilities that they can use for moral self-correction: (1) they can follow\ninstructions and (2) they can learn complex normative concepts of harm like\nstereotyping, bias, and discrimination. As such, they can follow instructions\nto avoid certain kinds of morally harmful outputs. We believe our results are\ncause for cautious optimism regarding the ability to train language models to\nabide by ethical principles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_D/0/1/0/all/0/1\">Deep Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askell_A/0/1/0/all/0/1\">Amanda Askell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiefer_N/0/1/0/all/0/1\">Nicholas Schiefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_T/0/1/0/all/0/1\">Thomas I. Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukosiute_K/0/1/0/all/0/1\">Kamil&#x117; Luko&#x161;i&#x16b;t&#x117;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anna Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldie_A/0/1/0/all/0/1\">Anna Goldie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirhoseini_A/0/1/0/all/0/1\">Azalia Mirhoseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsson_C/0/1/0/all/0/1\">Catherine Olsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_D/0/1/0/all/0/1\">Danny Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1\">Dawn Drain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dustin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Johnson_E/0/1/0/all/0/1\">Eli Tran-Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kernion_J/0/1/0/all/0/1\">Jackson Kernion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerr_J/0/1/0/all/0/1\">Jamie Kerr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_J/0/1/0/all/0/1\">Jared Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landau_J/0/1/0/all/0/1\">Joshua Landau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ndousse_K/0/1/0/all/0/1\">Kamal Ndousse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Karina Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovitt_L/0/1/0/all/0/1\">Liane Lovitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellitto_M/0/1/0/all/0/1\">Michael Sellitto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhage_N/0/1/0/all/0/1\">Nelson Elhage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mercado_N/0/1/0/all/0/1\">Noemi Mercado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DasSarma_N/0/1/0/all/0/1\">Nova DasSarma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rausch_O/0/1/0/all/0/1\">Oliver Rausch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasenby_R/0/1/0/all/0/1\">Robert Lasenby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larson_R/0/1/0/all/0/1\">Robin Larson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ringer_S/0/1/0/all/0/1\">Sam Ringer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1\">Sandipan Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1\">Saurav Kadavath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnston_S/0/1/0/all/0/1\">Scott Johnston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kravec_S/0/1/0/all/0/1\">Shauna Kravec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Showk_S/0/1/0/all/0/1\">Sheer El Showk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanham_T/0/1/0/all/0/1\">Tamera Lanham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Telleen_Lawton_T/0/1/0/all/0/1\">Timothy Telleen-Lawton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henighan_T/0/1/0/all/0/1\">Tom Henighan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hume_T/0/1/0/all/0/1\">Tristan Hume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yuntao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hatfield_Dodds_Z/0/1/0/all/0/1\">Zac Hatfield-Dodds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mann_B/0/1/0/all/0/1\">Ben Mann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amodei_D/0/1/0/all/0/1\">Dario Amodei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_N/0/1/0/all/0/1\">Nicholas Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCandlish_S/0/1/0/all/0/1\">Sam McCandlish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_T/0/1/0/all/0/1\">Tom Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olah_C/0/1/0/all/0/1\">Christopher Olah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jack Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1\">Jared Kaplan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Whats New? Identifying the Unfolding of New Events in Narratives. (arXiv:2302.07748v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.07748","description":"<p>Narratives include a rich source of events unfolding over time and context.\nAutomatic understanding of these events may provide a summarised comprehension\nof the narrative for further computation (such as reasoning). In this paper, we\nstudy the Information Status (IS) of the events and propose a novel challenging\ntask: the automatic identification of new events in a narrative. We define an\nevent as a triplet of subject, predicate, and object. The event is categorized\nas new with respect to the discourse context and whether it can be inferred\nthrough commonsense reasoning. We annotated a publicly available corpus of\nnarratives with the new events at sentence level using human annotators. We\npresent the annotation protocol and a study aiming at validating the quality of\nthe annotation and the difficulty of the task. We publish the annotated\ndataset, annotation materials, and machine learning baseline models for the\ntask of new event extraction for narrative understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mousavi_S/0/1/0/all/0/1\">Seyed Mahed Mousavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_S/0/1/0/all/0/1\">Shohei Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roccabruna_G/0/1/0/all/0/1\">Gabriel Roccabruna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshino_K/0/1/0/all/0/1\">Koichiro Yoshino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1\">Satoshi Nakamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riccardi_G/0/1/0/all/0/1\">Giuseppe Riccardi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks. (arXiv:2302.08399v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2302.08399","description":"<p>Intuitive psychology is a pillar of common-sense reasoning. The replication\nof this reasoning in machine intelligence is an important stepping-stone on the\nway to human-like artificial intelligence. Several recent tasks and benchmarks\nfor examining this reasoning in Large-Large Models have focused in particular\non belief attribution in Theory-of-Mind tasks. These tasks have shown both\nsuccesses and failures. We consider in particular a recent purported success\ncase, and show that small variations that maintain the principles of ToM turn\nthe results on their head. We argue that in general, the zero-hypothesis for\nmodel evaluation in intuitive psychology should be skeptical, and that outlying\nfailure cases should outweigh average success rates. We also consider what\npossible future successes on Theory-of-Mind tasks by more powerful LLMs would\nmean for ToM tasks with people.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ullman_T/0/1/0/all/0/1\">Tomer Ullman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-02-20T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}