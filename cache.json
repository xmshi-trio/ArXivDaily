{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-01-18T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Infusing Commonsense World Models with Graph Knowledge. (arXiv:2301.05746v1 [cs.CL])","link":"http://arxiv.org/abs/2301.05746","description":"<p>While language models have become more capable of producing compelling\nlanguage, we find there are still gaps in maintaining consistency, especially\nwhen describing events in a dynamically changing world. We study the setting of\ngenerating narratives in an open world text adventure game, where a graph\nrepresentation of the underlying game state can be used to train models that\nconsume and output both grounded graph representations and natural language\ndescriptions and actions. We build a large set of tasks by combining\ncrowdsourced and simulated gameplays with a novel dataset of complex actions in\norder to to construct such models. We find it is possible to improve the\nconsistency of action narration models by training on graph contexts and\ntargets, even if graphs are not present at test time. This is shown both in\nautomatic metrics and human evaluations. We plan to release our code, the new\nset of tasks, and best performing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gurung_A/0/1/0/all/0/1\">Alexander Gurung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komeili_M/0/1/0/all/0/1\">Mojtaba Komeili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urbanek_J/0/1/0/all/0/1\">Jack Urbanek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data. (arXiv:2301.05843v1 [cs.HC])","link":"http://arxiv.org/abs/2301.05843","description":"<p>Large language models (LLMs) provide a new way to build chatbots by accepting\nnatural language prompts. Yet, it is unclear how to design prompts to power\nchatbots to carry on naturalistic conversations while pursuing a given goal,\nsuch as collecting self-report data from users. We explore what design factors\nof prompts can help steer chatbots to talk naturally and collect data reliably.\nTo this aim, we formulated four prompt designs with different structures and\npersonas. Through an online study (N = 48) where participants conversed with\nchatbots driven by different designs of prompts, we assessed how prompt designs\nand conversation topics affected the conversation flows and users' perceptions\nof chatbots. Our chatbots covered 79% of the desired information slots during\nconversations, and the designs of prompts and topics significantly influenced\nthe conversation flows and the data collection performance. We discuss the\nopportunities and challenges of building chatbots with LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jing Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungdong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">Hyunhoon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Ho Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Stance of Authorities towards Rumors in Arabic Tweets: A Preliminary Study. (arXiv:2301.05863v1 [cs.CL])","link":"http://arxiv.org/abs/2301.05863","description":"<p>A myriad of studies addressed the problem of rumor verification in Twitter by\neither utilizing evidence from the propagation networks or external evidence\nfrom the Web. However, none of these studies exploited evidence from trusted\nauthorities. In this paper, we define the task of detecting the stance of\nauthorities towards rumors in tweets, i.e., whether a tweet from an authority\nagrees, disagrees, or is unrelated to the rumor. We believe the task is useful\nto augment the sources of evidence utilized by existing rumor verification\nsystems. We construct and release the first Authority STance towards Rumors\n(AuSTR) dataset, where evidence is retrieved from authority timelines in Arabic\nTwitter. Due to the relatively limited size of our dataset, we study the\nusefulness of existing datasets for stance detection in our task. We show that\nexisting datasets are somewhat useful for the task; however, they are clearly\ninsufficient, which motivates the need to augment them with annotated data\nconstituting stance of authorities from Twitter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haouari_F/0/1/0/all/0/1\">Fatima Haouari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsayed_T/0/1/0/all/0/1\">Tamer Elsayed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TikTalk: A Multi-Modal Dialogue Dataset for Real-World Chitchat. (arXiv:2301.05880v1 [cs.CL])","link":"http://arxiv.org/abs/2301.05880","description":"<p>We present a novel multi-modal chitchat dialogue dataset-TikTalk aimed at\nfacilitating the research of intelligent chatbots. It consists of the videos\nand corresponding dialogues users generate on video social applications. In\ncontrast to existing multi-modal dialogue datasets, we construct dialogue\ncorpora based on video comment-reply pairs, which is more similar to chitchat\nin real-world dialogue scenarios. Our dialogue context includes three\nmodalities: text, vision, and audio. Compared with previous image-based\ndialogue datasets, the richer sources of context in TikTalk lead to a greater\ndiversity of conversations. TikTalk contains over 38K videos and 367K\ndialogues. Data analysis shows that responses in TikTalk are in correlation\nwith various contexts and external knowledge. It poses a great challenge for\nthe deep understanding of multi-modal information and the generation of\nresponses. We evaluate several baselines on three types of automatic metrics\nand conduct case studies. Experimental results demonstrate that there is still\na large room for future improvement on TikTalk. Our dataset is available at\n\\url{https://github.com/RUC-AIMind/TikTalk}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongpeng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_L/0/1/0/all/0/1\">Ludan Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wenke Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jingyuan Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yixin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Di Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Ruihua Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiwu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$\\texttt{tasksource}$: Structured Dataset Preprocessing Annotations for Frictionless Extreme Multi-Task Learning and Evaluation. (arXiv:2301.05948v1 [cs.CL])","link":"http://arxiv.org/abs/2301.05948","description":"<p>The HuggingFace Datasets Hub hosts thousands of datasets. This provides\nexciting opportunities for language model training and evaluation. However, the\ndatasets for a given type of task are stored with different schemas, and\nharmonization is harder than it seems (https://xkcd.com/927/). Multi-task\ntraining or evaluation requires manual work to fit data into task templates.\nVarious initiatives independently address this problem by releasing the\nharmonized datasets or harmonization codes to preprocess datasets to the same\nformat. We identify patterns across previous preprocessings, e.g. mapping of\ncolumn names, and extraction of a specific sub-field from structured data in a\ncolumn, and propose a structured annotation framework that makes our\nannotations fully exposed and not buried in unstructured code. We release a\ndataset annotation framework and dataset annotations for more than 400 English\ntasks (https://github.com/sileod/tasksource). These annotations provide\nmetadata, like the name of the columns that should be used as input or labels\nfor all datasets, and can save time for future dataset preprocessings, even if\nthey do not use our framework. We fine-tune a multi-task text encoder on all\ntasksource tasks, outperforming every publicly available text encoder of\ncomparable size on an external evaluation\nhttps://hf.co/sileod/deberta-v3-base-tasksource-nli.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sileo_D/0/1/0/all/0/1\">Damien Sileo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rationalizing Predictions by Adversarial Information Calibration. (arXiv:2301.06009v1 [cs.CL])","link":"http://arxiv.org/abs/2301.06009","description":"<p>Explaining the predictions of AI models is paramount in safety-critical\napplications, such as in legal or medical domains. One form of explanation for\na prediction is an extractive rationale, i.e., a subset of features of an\ninstance that lead the model to give its prediction on that instance. For\nexample, the subphrase ``he stole the mobile phone'' can be an extractive\nrationale for the prediction of ``Theft''. Previous works on generating\nextractive rationales usually employ a two-phase model: a selector that selects\nthe most important features (i.e., the rationale) followed by a predictor that\nmakes the prediction based exclusively on the selected features. One\ndisadvantage of these works is that the main signal for learning to select\nfeatures comes from the comparison of the answers given by the predictor to the\nground-truth answers. In this work, we propose to squeeze more information from\nthe predictor via an information calibration method. More precisely, we train\ntwo models jointly: one is a typical neural model that solves the task at hand\nin an accurate but black-box manner, and the other is a selector-predictor\nmodel that additionally produces a rationale for its prediction. The first\nmodel is used as a guide for the second model. We use an adversarial technique\nto calibrate the information extracted by the two models such that the\ndifference between them is an indicator of the missed or over-selected\nfeatures. In addition, for natural language tasks, we propose a\nlanguage-model-based regularizer to encourage the extraction of fluent\nrationales. Experimental results on a sentiment analysis task, a hate speech\nrecognition task as well as on three tasks from the legal domain show the\neffectiveness of our approach to rationale extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1\">Lei Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A data science and machine learning approach to continuous analysis of Shakespeare's plays. (arXiv:2301.06024v1 [cs.CL])","link":"http://arxiv.org/abs/2301.06024","description":"<p>The availability of quantitative methods that can analyze text has provided\nnew ways of examining literature in a manner that was not available in the\npre-information era. Here we apply comprehensive machine learning analysis to\nthe work of William Shakespeare. The analysis shows clear change in style of\nwriting over time, with the most significant changes in the sentence length,\nfrequency of adjectives and adverbs, and the sentiments expressed in the text.\nApplying machine learning to make a stylometric prediction of the year of the\nplay shows a Pearson correlation of 0.71 between the actual and predicted year,\nindicating that Shakespeare's writing style as reflected by the quantitative\nmeasurements changed over time. Additionally, it shows that the stylometrics of\nsome of the plays is more similar to plays written either before or after the\nyear they were written. For instance, Romeo and Juliet is dated 1596, but is\nmore similar in stylometrics to plays written by Shakespeare after 1600. The\nsource code for the analysis is available for free download.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Swisher_C/0/1/0/all/0/1\">Charles Swisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_L/0/1/0/all/0/1\">Lior Shamir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hawk: An Industrial-strength Multi-label Document Classifier. (arXiv:2301.06057v1 [cs.CL])","link":"http://arxiv.org/abs/2301.06057","description":"<p>There are a plethora of methods and algorithms that solve the classical\nmulti-label document classification. However, when it comes to deployment and\nusage in an industry setting, most, if not all the contemporary approaches fail\nto address some of the vital aspects or requirements of an ideal solution: i.\nability to operate on variable-length texts and rambling documents. ii.\ncatastrophic forgetting problem. iii. modularity when it comes to online\nlearning and updating the model. iv. ability to spotlight relevant text while\nproducing the prediction, i.e. visualizing the predictions. v. ability to\noperate on imbalanced or skewed datasets. vi. scalability. The paper describes\nthe significance of these problems in detail and proposes a unique neural\nnetwork architecture that addresses the above problems. The proposed\narchitecture views documents as a sequence of sentences and leverages\nsentence-level embeddings for input representation. A hydranet-like\narchitecture is designed to have granular control over and improve the\nmodularity, coupled with a weighted loss driving task-specific heads. In\nparticular, two specific mechanisms are compared: Bi-LSTM and\nTransformer-based. The architecture is benchmarked on some of the popular\nbenchmarking datasets such as Web of Science - 5763, Web of Science - 11967,\nBBC Sports, and BBC News datasets. The experimental results reveal that the\nproposed model outperforms the existing methods by a substantial margin. The\nablation study includes comparisons of the impact of the attention mechanism\nand the application of weighted loss functions to train the task-specific heads\nin the hydranet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Javeed_A/0/1/0/all/0/1\">Arshad Javeed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on the Evolution of Stream Processing Systems. (arXiv:2008.00842v2 [cs.DC] UPDATED)","link":"http://arxiv.org/abs/2008.00842","description":"<p>Stream processing has been an active research field for more than 20 years,\nbut it is now witnessing its prime time due to recent successful efforts by the\nresearch community and numerous worldwide open-source communities. This survey\nprovides a comprehensive overview of fundamental aspects of stream processing\nsystems and their evolution in the functional areas of out-of-order data\nmanagement, state management, fault tolerance, high availability, load\nmanagement, elasticity, and reconfiguration. We review noteworthy past research\nfindings, outline the similarities and differences between early ('00-'10) and\nmodern ('11-'22) streaming systems, and discuss recent trends and open\nproblems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fragkoulis_M/0/1/0/all/0/1\">Marios Fragkoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carbone_P/0/1/0/all/0/1\">Paris Carbone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalavri_V/0/1/0/all/0/1\">Vasiliki Kalavri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsifodimos_A/0/1/0/all/0/1\">Asterios Katsifodimos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Studying Fake News Spreading, Polarisation Dynamics, and Manipulation by Bots: a Tale of Networks and Language. (arXiv:2109.07909v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2109.07909","description":"<p>With the explosive growth of online social media, the ancient problem of\ninformation disorders interfering with news diffusion has surfaced with a\nrenewed intensity threatening our democracies, public health, and news outlets'\ncredibility. Therefore, thousands of scientific papers have been published in a\nrelatively short period, making researchers of different disciplines struggle\nwith an information overload problem. The aim of this survey is threefold: (1)\nwe present the results of a network-based analysis of the existing\nmultidisciplinary literature to support the search for relevant trends and\ncentral publications; (2) we describe the main results and necessary background\nto attack the problem under a computational perspective; (3) we review selected\ncontributions using network science as a unifying framework and computational\nlinguistics as the tool to make sense of the shared content. Despite scholars\nworking on computational linguistics and networks traditionally belong to\ndifferent scientific communities, we expect that those interested in the area\nof fake news should be aware of crucial aspects of both disciplines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruffo_G/0/1/0/all/0/1\">Giancarlo Ruffo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semeraro_A/0/1/0/all/0/1\">Alfonso Semeraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giachanou_A/0/1/0/all/0/1\">Anastasia Giachanou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosso_P/0/1/0/all/0/1\">Paolo Rosso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm. (arXiv:2110.08190v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08190","description":"<p>Conventional wisdom in pruning Transformer-based language models is that\npruning reduces the model expressiveness and thus is more likely to underfit\nrather than overfit. However, under the trending pretrain-and-finetune\nparadigm, we postulate a counter-traditional hypothesis, that is: pruning\nincreases the risk of overfitting when performed at the fine-tuning phase. In\nthis paper, we aim to address the overfitting problem and improve pruning\nperformance via progressive knowledge distillation with error-bound properties.\nWe show for the first time that reducing the risk of overfitting can help the\neffectiveness of pruning under the pretrain-and-finetune paradigm. Ablation\nstudies and experiments on the GLUE benchmark show that our method outperforms\nthe leading competitors across different tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaoyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dongkuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_I/0/1/0/all/0/1\">Ian E.H. Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yijue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Sung-en Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingbing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shiyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1\">Mimi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasekaran_S/0/1/0/all/0/1\">Sanguthevar Rajasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Caiwen Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Dementia from Speech and Transcripts using Transformers. (arXiv:2110.14769v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.14769","description":"<p>Alzheimer's disease (AD) constitutes a neurodegenerative disease with serious\nconsequences to peoples' everyday lives, if it is not diagnosed early since\nthere is no available cure. Alzheimer's is the most common cause of dementia,\nwhich constitutes a general term for loss of memory. Due to the fact that\ndementia affects speech, existing research initiatives focus on detecting\ndementia from spontaneous speech. However, little work has been done regarding\nthe conversion of speech data to Log-Mel spectrograms and Mel-frequency\ncepstral coefficients (MFCCs) and the usage of pretrained models. Concurrently,\nlittle work has been done in terms of both the usage of transformer networks\nand the way the two modalities, i.e., speech and transcripts, are combined in a\nsingle neural network. To address these limitations, first we represent speech\nsignal as an image and employ several pretrained models, with Vision\nTransformer (ViT) achieving the highest evaluation results. Secondly, we\npropose multimodal models. More specifically, our introduced models include\nGated Multimodal Unit in order to control the influence of each modality\ntowards the final classification and crossmodal attention so as to capture in\nan effective way the relationships between the two modalities. Extensive\nexperiments conducted on the ADReSS Challenge dataset demonstrate the\neffectiveness of the proposed models and their superiority over\nstate-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ilias_L/0/1/0/all/0/1\">Loukas Ilias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askounis_D/0/1/0/all/0/1\">Dimitris Askounis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Psarras_J/0/1/0/all/0/1\">John Psarras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Disentangled Semantic Representations for Zero-Shot Cross-Lingual Transfer in Multilingual Machine Reading Comprehension. (arXiv:2204.00996v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.00996","description":"<p>Multilingual pre-trained models are able to zero-shot transfer knowledge from\nrich-resource to low-resource languages in machine reading comprehension (MRC).\nHowever, inherent linguistic discrepancies in different languages could make\nanswer spans predicted by zero-shot transfer violate syntactic constraints of\nthe target language. In this paper, we propose a novel multilingual MRC\nframework equipped with a Siamese Semantic Disentanglement Model (SSDM) to\ndisassociate semantics from syntax in representations learned by multilingual\npre-trained models. To explicitly transfer only semantic knowledge to the\ntarget language, we propose two groups of losses tailored for semantic and\nsyntactic encoding and disentanglement. Experimental results on three\nmultilingual MRC datasets (i.e., XQuAD, MLQA, and TyDi QA) demonstrate the\neffectiveness of our proposed approach over models based on mBERT and XLM-100.\nCode is available at:https://github.com/wulinjuan/SSDM_MRC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Linjuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shaojuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaowang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Deyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shizhan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1\">Zhiqiang Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhiyong Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Vocal Fatigue with Neural Embeddings. (arXiv:2204.03428v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2204.03428","description":"<p>Vocal fatigue refers to the feeling of tiredness and weakness of voice due to\nextended utilization. This paper investigates the effectiveness of neural\nembeddings for the detection of vocal fatigue. We compare x-vectors,\nECAPA-TDNN, and wav2vec 2.0 embeddings on a corpus of academic spoken English.\nLow-dimensional mappings of the data reveal that neural embeddings capture\ninformation about the change in vocal characteristics of a speaker during\nprolonged voice usage. We show that vocal fatigue can be reliably predicted\nusing all three kinds of neural embeddings after only 50 minutes of continuous\nspeaking when temporal smoothing and normalization are applied to the extracted\nembeddings. We employ support vector machines for classification and achieve\naccuracy scores of 81% using x-vectors, 85% using ECAPA-TDNN embeddings, and\n82% using wav2vec 2.0 embeddings as input features. We obtain an accuracy score\nof 76%, when the trained system is applied to a different speaker and recording\nenvironment without any adaptation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bayerl_S/0/1/0/all/0/1\">Sebastian P. Bayerl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wagner_D/0/1/0/all/0/1\">Dominik Wagner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baumann_I/0/1/0/all/0/1\">Ilja Baumann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riedhammer_K/0/1/0/all/0/1\">Korbinian Riedhammer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bocklet_T/0/1/0/all/0/1\">Tobias Bocklet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent Communication through Metropolis-Hastings Naming Game with Deep Generative Models. (arXiv:2205.12392v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2205.12392","description":"<p>Constructive studies on symbol emergence systems seek to investigate\ncomputational models that can better explain human language evolution, the\ncreation of symbol systems, and the construction of internal representations.\nThis study provides a new model for emergent communication, which is based on a\nprobabilistic generative model (PGM) instead of a discriminative model based on\ndeep reinforcement learning. We define the Metropolis-Hastings (MH) naming game\nby generalizing previously proposed models. It is not a referential game with\nexplicit feedback, as assumed by many emergent communication studies. Instead,\nit is a game based on joint attention without explicit feedback.\nMathematically, the MH naming game is proved to be a type of MH algorithm for\nan integrative PGM that combines two agents that play the naming game. From\nthis viewpoint, symbol emergence is regarded as decentralized Bayesian\ninference, and semiotic communication is regarded as inter-personal cross-modal\ninference. This notion leads to the collective predictive coding hypothesis}\nregarding language evolution and, in general, the emergence of symbols. We also\npropose the inter-Gaussian mixture model (GMM)+ variational autoencoder (VAE),\na deep generative model for emergent communication based on the MH naming game.\nThe model has been validated on MNIST and Fruits 360 datasets. Experimental\nfindings demonstrate that categories are formed from real images observed by\nagents, and signs are correctly shared across agents by successfully utilizing\nboth of the observations of agents via the MH naming game. Furthermore,\nscholars verified that visual images were recalled from signs uttered by\nagents. Notably, emergent communication without supervision and reward feedback\nimproved the performance of the unsupervised representation learning of agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taniguchi_T/0/1/0/all/0/1\">Tadahiro Taniguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshida_Y/0/1/0/all/0/1\">Yuto Yoshida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taniguchi_A/0/1/0/all/0/1\">Akira Taniguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagiwara_Y/0/1/0/all/0/1\">Yoshinobu Hagiwara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Generation Meets Real People: Building a Social, Informative Open-Domain Dialogue Agent. (arXiv:2207.12021v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.12021","description":"<p>We present Chirpy Cardinal, an open-domain social chatbot. Aiming to be both\ninformative and conversational, our bot chats with users in an authentic,\nemotionally intelligent way. By integrating controlled neural generation with\nscaffolded, hand-written dialogue, we let both the user and bot take turns\ndriving the conversation, producing an engaging and socially fluent experience.\nDeployed in the fourth iteration of the Alexa Prize Socialbot Grand Challenge,\nChirpy Cardinal handled thousands of conversations per day, placing second out\nof nine bots with an average user rating of 3.58/5.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ethan A. Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paranjape_A/0/1/0/all/0/1\">Ashwin Paranjape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_A/0/1/0/all/0/1\">Abigail See</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiam_C/0/1/0/all/0/1\">Caleb Chiam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Trenton Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kenealy_K/0/1/0/all/0/1\">Kathleen Kenealy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Swee Kiat Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardy_A/0/1/0/all/0/1\">Amelia Hardy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_C/0/1/0/all/0/1\">Chetanya Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haojun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyabor_A/0/1/0/all/0/1\">Alexander Iyabor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yutong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sowrirajan_H/0/1/0/all/0/1\">Hari Sowrirajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_P/0/1/0/all/0/1\">Peng Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadagopan_K/0/1/0/all/0/1\">Kaushik Ram Sadagopan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phu_N/0/1/0/all/0/1\">Nguyet Minh Phu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soylu_D/0/1/0/all/0/1\">Dilara Soylu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jillian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_A/0/1/0/all/0/1\">Avanika Narayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campagna_G/0/1/0/all/0/1\">Giovanni Campagna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Can Transformers Learn In-Context? A Case Study of Simple Function Classes. (arXiv:2208.01066v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.01066","description":"<p>In-context learning refers to the ability of a model to condition on a prompt\nsequence consisting of in-context examples (input-output pairs corresponding to\nsome task) along with a new query input, and generate the corresponding output.\nCrucially, in-context learning happens only at inference time without any\nparameter updates to the model. While large language models such as GPT-3\nexhibit some ability to perform in-context learning, it is unclear what the\nrelationship is between tasks on which this succeeds and what is present in the\ntraining data. To make progress towards understanding in-context learning, we\nconsider the well-defined problem of training a model to in-context learn a\nfunction class (e.g., linear functions): that is, given data derived from some\nfunctions in the class, can we train a model to in-context learn \"most\"\nfunctions from this class? We show empirically that standard Transformers can\nbe trained from scratch to perform in-context learning of linear functions --\nthat is, the trained model is able to learn unseen linear functions from\nin-context examples with performance comparable to the optimal least squares\nestimator. In fact, in-context learning is possible even under two forms of\ndistribution shift: (i) between the training data of the model and\ninference-time prompts, and (ii) between the in-context examples and the query\ninput during inference. We also show that we can train Transformers to\nin-context learn more complex function classes -- namely sparse linear\nfunctions, two-layer neural networks, and decision trees -- with performance\nthat matches or exceeds task-specific learning algorithms. Our code and models\nare available at https://github.com/dtsip/in-context-learning .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Shivam Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsipras_D/0/1/0/all/0/1\">Dimitris Tsipras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valiant_G/0/1/0/all/0/1\">Gregory Valiant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative pseudo-forced alignment by acoustic CTC loss for self-supervised ASR domain adaptation. (arXiv:2210.15226v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.15226","description":"<p>High-quality data labeling from specific domains is costly and human\ntime-consuming. In this work, we propose a self-supervised domain adaptation\nmethod, based upon an iterative pseudo-forced alignment algorithm. The produced\nalignments are employed to customize an end-to-end Automatic Speech Recognition\n(ASR) and iteratively refined. The algorithm is fed with frame-wise character\nposteriors produced by a seed ASR, trained with out-of-domain data, and\noptimized throughout a Connectionist Temporal Classification (CTC) loss. The\nalignments are computed iteratively upon a corpus of broadcast TV. The process\nis repeated by reducing the quantity of text to be aligned or expanding the\nalignment window until finding the best possible audio-text alignment. The\nstarting timestamps, or temporal anchors, are produced uniquely based on the\nconfidence score of the last aligned utterance. This score is computed with the\npaths of the CTC-alignment matrix. With this methodology, no human-revised text\nreferences are required. Alignments from long audio files with low-quality\ntranscriptions, like TV captions, are filtered out by confidence score and\nready for further ASR adaptation. The obtained results, on both the Spanish\nRTVE2022 and CommonVoice databases, underpin the feasibility of using CTC-based\nsystems to perform: highly accurate audio-text alignments, domain adaptation\nand semi-supervised training of end-to-end ASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lopez_F/0/1/0/all/0/1\">Fernando L&#xf3;pez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luque_J/0/1/0/all/0/1\">Jordi Luque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Categorical Framework for Modeling with Stock and Flow Diagrams. (arXiv:2211.01290v3 [cs.LO] UPDATED)","link":"http://arxiv.org/abs/2211.01290","description":"<p>Stock and flow diagrams are already an important tool in epidemiology, but\ncategory theory lets us go further and treat these diagrams as mathematical\nentities in their own right. In this chapter we use communicable disease models\ncreated with our software, StockFlow.jl, to explain the benefits of the\ncategorical approach. We first explain the category of stock-flow diagrams and\nnote the clear separation between the syntax of these diagrams and their\nsemantics, demonstrating three examples of semantics already implemented in the\nsoftware: ODEs, causal loop diagrams, and system structure diagrams. We then\nturn to two methods for building large stock-flow diagrams from smaller ones in\na modular fashion: composition and stratification. Finally, we introduce the\nopen-source ModelCollab software for diagram-based collaborative modeling. The\ngraphical user interface of this web-based software lets modelers take\nadvantage of the ideas discussed here without any knowledge of their\ncategorical foundations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baez_J/0/1/0/all/0/1\">John C. Baez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Libkind_S/0/1/0/all/0/1\">Sophie Libkind</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osgood_N/0/1/0/all/0/1\">Nathaniel D. Osgood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Redekopp_E/0/1/0/all/0/1\">Eric Redekopp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extending Logic Explained Networks to Text Classification. (arXiv:2211.09732v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.09732","description":"<p>Recently, Logic Explained Networks (LENs) have been proposed as\nexplainable-by-design neural models providing logic explanations for their\npredictions. However, these models have only been applied to vision and tabular\ndata, and they mostly favour the generation of global explanations, while local\nones tend to be noisy and verbose. For these reasons, we propose LENp,\nimproving local explanations by perturbing input words, and we test it on text\nclassification. Our results show that (i) LENp provides better local\nexplanations than LIME in terms of sensitivity and faithfulness, and (ii) logic\nexplanations are more useful and user-friendly than feature scoring provided by\nLIME as attested by a human survey.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Rishabh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciravegna_G/0/1/0/all/0/1\">Gabriele Ciravegna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1\">Pietro Barbiero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannini_F/0/1/0/all/0/1\">Francesco Giannini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buffelli_D/0/1/0/all/0/1\">Davide Buffelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Lio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Programming by Example and Text-to-Code Translation for Conversational Code Generation. (arXiv:2211.11554v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.11554","description":"<p>Dialogue systems is an increasingly popular task of natural language\nprocessing. However, the dialogue paths tend to be deterministic, restricted to\nthe system rails, regardless of the given request or input text. Recent\nadvances in program synthesis have led to systems which can synthesize programs\nfrom very general search spaces, e.g. Programming by Example, and to systems\nwith very accessible interfaces for writing programs, e.g. text-to-code\ntranslation, but have not achieved both of these qualities in the same system.\nWe propose Modular Programs for Text-guided Hierarchical Synthesis (MPaTHS), a\nmethod for integrating Programming by Example and text-to-code systems which\noffers an accessible natural language interface for synthesizing general\nprograms. We present a program representation that allows our method to be\napplied to the problem of task-oriented dialogue. Finally, we demo MPaTHS using\nour program representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Whitehouse_E/0/1/0/all/0/1\">Eli Whitehouse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerard_W/0/1/0/all/0/1\">William Gerard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klimovich_Y/0/1/0/all/0/1\">Yauhen Klimovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franco_Salvador_M/0/1/0/all/0/1\">Marc Franco-Salvador</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEAformer: Multi-modal Entity Alignment Transformer for Meta Modality Hybrid. (arXiv:2212.14454v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2212.14454","description":"<p>As an important variant of entity alignment (EA), multi-modal entity\nalignment (MMEA) aims to discover identical entities across different knowledge\ngraphs (KGs) with relevant images attached. We noticed that current MMEA\nalgorithms all globally adopt the KG-level modality fusion strategies for\nmulti-modal entity representation but ignore the variation in modality\npreferences for individual entities, hurting the robustness to potential noise\ninvolved in modalities (e.g., blurry images and relations). In this paper we\npresent MEAformer, a multi-modal entity alignment transformer approach for meta\nmodality hybrid, which dynamically predicts the mutual correlation coefficients\namong modalities for entity-level feature aggregation. A modal-aware hard\nentity replay strategy is further proposed for addressing vague entity details.\nExperimental results show that our model not only achieves SOTA performance on\nmultiple training scenarios including supervised, unsupervised, iterative, and\nlow resource, but also has comparable number of parameters, optimistic speed,\nand good interpretability. Our code and data will be available soon for\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Lingbing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1\">Yuxia Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jeff Z. Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Wenting Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Design Principle of Blockchain: An Initiative for the SoK of SoKs. (arXiv:2301.00479v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2301.00479","description":"<p>Blockchain, also coined as decentralized AI, has the potential to empower AI\nto be more trustworthy by creating a decentralized trust of privacy, security,\nand audibility. However, systematic studies on the design principle of\nblockchain as a trust engine for an integrated society of\ncyber-physical-social-system (CPSS) are still absent. In this article, we\nprovide an initiative for seeking the design principle of blockchain for a\nbetter digital world. Using a hybrid method of qualitative and quantitative\nstudies, we examine the past origin, the current development, and the future\ndirections of blockchain design principles. We have three findings. First, the\nanswer to whether blockchain lives up to its original design principle as a\ndistributed database is controversial. Second, the current development of the\nblockchain community reveals a taxonomy of 7 categories, namely, privacy and\nsecurity, scalability, decentralization, applicability, governance and\nregulation, system design, and cross-chain interoperability. Both research and\npractice are more centered around the first category of privacy and security\nand the fourth category of applicability. Future scholars, practitioners, and\npolicy-makers have vast opportunities in other, much less exploited facets and\nthe synthesis at the interface of multiple aspects. Finally, in\ncounter-examples, we conclude that a synthetic solution that crosses discipline\nboundaries is necessary to close the gaps between the current design of\nblockchain and the design principle of a trust engine for a truly intelligent\nworld.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Luyao Sunshine Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models as Corporate Lobbyists. (arXiv:2301.01181v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.01181","description":"<p>We demonstrate a proof-of-concept of a large language model conducting\ncorporate lobbying related activities. An autoregressive large language model\n(OpenAI's text-davinci-003) determines if proposed U.S. Congressional bills are\nrelevant to specific public companies and provides explanations and confidence\nlevels. For the bills the model deems as relevant, the model drafts a letter to\nthe sponsor of the bill in an attempt to persuade the congressperson to make\nchanges to the proposed legislation. We use hundreds of novel ground-truth\nlabels of the relevance of a bill to a company to benchmark the performance of\nthe model, which outperforms the baseline of predicting the most common outcome\nof irrelevance. We also benchmark the performance of the previous OpenAI GPT-3\nmodel (text-davinci-002), which was the state-of-the-art model on many academic\nnatural language tasks until text-davinci-003 was recently released. The\nperformance of text-davinci-002 is worse than a simple benchmark. These results\nsuggest that, as large language models continue to exhibit improved natural\nlanguage understanding capabilities, performance on lobbying related tasks will\ncontinue to improve. Longer-term, if AI begins to influence law in a manner\nthat is not a direct extension of human intentions, this threatens the critical\nrole that law as information could play in aligning AI with humans. Initially,\nAI is being used to simply augment human lobbyists for a small portion of their\ndaily tasks. However, firms have an incentive to use less and less human\noversight over automated assessments of policy ideas and the written\ncommunication to regulatory agencies and Congressional staffers. The core\nquestion raised is where to draw the line between human-driven and AI-driven\npolicy influence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nay_J/0/1/0/all/0/1\">John J. Nay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Categorization of Mental Health Posts using Transformers. (arXiv:2301.02589v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.02589","description":"<p>With recent developments in digitization of clinical psychology, NLP research\ncommunity has revolutionized the field of mental health detection on social\nmedia. Existing research in mental health analysis revolves around the\ncross-sectional studies to classify users' intent on social media. For in-depth\nanalysis, we investigate existing classifiers to solve the problem of causal\ncategorization which suggests the inefficiency of learning based methods due to\nlimited training samples. To handle this challenge, we use transformer models\nand demonstrate the efficacy of a pre-trained transfer learning on \"CAMS\"\ndataset. The experimental result improves the accuracy and depicts the\nimportance of identifying cause-and-effect relationships in the underlying\ntext.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaur_S/0/1/0/all/0/1\">Simranjeet Kaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhardwaj_R/0/1/0/all/0/1\">Ritika Bhardwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Aastha Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_M/0/1/0/all/0/1\">Muskan Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_C/0/1/0/all/0/1\">Chandni Saxena</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why do Nearest Neighbor Language Models Work?. (arXiv:2301.02828v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.02828","description":"<p>Language models (LMs) compute the probability of a text by sequentially\ncomputing a representation of an already-seen context and using this\nrepresentation to predict the next word. Currently, most LMs calculate these\nrepresentations through a neural network consuming the immediate previous\ncontext. However recently, retrieval-augmented LMs have shown to improve over\nstandard neural LMs, by accessing information retrieved from a large datastore,\nin addition to their standard, parametric, next-word prediction. In this paper,\nwe set out to understand why retrieval-augmented language models, and\nspecifically why k-nearest neighbor language models (kNN-LMs) perform better\nthan standard parametric LMs, even when the k-nearest neighbor component\nretrieves examples from the same training set that the LM was originally\ntrained on. To this end, we perform a careful analysis of the various\ndimensions over which kNN-LM diverges from standard LMs, and investigate these\ndimensions one by one. Empirically, we identify three main reasons why kNN-LM\nperforms better than standard LMs: using a different input representation for\npredicting the next tokens, approximate kNN search, and the importance of\nsoftmax temperature for the kNN distribution. Further, we incorporate these\ninsights into the model architecture or the training procedure of the standard\nparametric LM, improving its results without the need for an explicit retrieval\ncomponent. The code is available at https://github.com/frankxu2004/knnlm-why.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Frank F. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1\">Uri Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word-Graph2vec: An efficient word embedding approach on word co-occurrence graph using random walk sampling. (arXiv:2301.04312v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.04312","description":"<p>Word embedding has become ubiquitous and is widely used in various text\nmining and natural language processing (NLP) tasks, such as information\nretrieval, semantic analysis, and machine translation, among many others.\nUnfortunately, it is prohibitively expensive to train the word embedding in a\nrelatively large corpus. We propose a graph-based word embedding algorithm,\ncalled Word-Graph2vec, which converts the large corpus into a word\nco-occurrence graph, then takes the word sequence samples from this graph by\nrandomly traveling and trains the word embedding on this sampling corpus in the\nend. We posit that because of the stable vocabulary, relative idioms, and fixed\nexpressions in English, the size and density of the word co-occurrence graph\nchange slightly with the increase in the training corpus. So that\nWord-Graph2vec has stable runtime on the large scale data set, and its\nperformance advantage becomes more and more obvious with the growth of the\ntraining corpus. Extensive experiments conducted on real-world datasets show\nthat the proposed algorithm outperforms traditional Skip-Gram by four-five\ntimes in terms of efficiency, while the error generated by the random walk\nsampling is small.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenting Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuanzhe Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jiahong Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-01-17T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}