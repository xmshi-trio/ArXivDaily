{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-06-02T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A Two-Stage Decoder for Efficient ICD Coding. (arXiv:2306.00005v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00005","description":"<p>Clinical notes in healthcare facilities are tagged with the International\nClassification of Diseases (ICD) code; a list of classification codes for\nmedical diagnoses and procedures. ICD coding is a challenging multilabel text\nclassification problem due to noisy clinical document inputs and long-tailed\nlabel distribution. Recent automated ICD coding efforts improve performance by\nencoding medical notes and codes with additional data and knowledge bases.\nHowever, most of them do not reflect how human coders generate the code: first,\nthe coders select general code categories and then look for specific\nsubcategories that are relevant to a patient's condition. Inspired by this, we\npropose a two-stage decoding mechanism to predict ICD codes. Our model uses the\nhierarchical properties of the codes to split the prediction into two steps: At\nfirst, we predict the parent code and then predict the child code based on the\nprevious prediction. Experiments on the public MIMIC-III data set show that our\nmodel performs well in single-model settings without external data or\nknowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thanh-Tung Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlegel_V/0/1/0/all/0/1\">Viktor Schlegel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashyap_A/0/1/0/all/0/1\">Abhinav Kashyap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winkler_S/0/1/0/all/0/1\">Stefan Winkler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Datasets for Portuguese Legal Semantic Textual Similarity: Comparing weak supervision and an annotation process approaches. (arXiv:2306.00007v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00007","description":"<p>The Brazilian judiciary has a large workload, resulting in a long time to\nfinish legal proceedings. Brazilian National Council of Justice has established\nin Resolution 469/2022 formal guidance for document and process digitalization\nopening up the possibility of using automatic techniques to help with everyday\ntasks in the legal field, particularly in a large number of texts yielded on\nthe routine of law procedures. Notably, Artificial Intelligence (AI) techniques\nallow for processing and extracting useful information from textual data,\npotentially speeding up the process. However, datasets from the legal domain\nrequired by several AI techniques are scarce and difficult to obtain as they\nneed labels from experts. To address this challenge, this article contributes\nwith four datasets from the legal domain, two with documents and metadata but\nunlabeled, and another two labeled with a heuristic aiming at its use in\ntextual semantic similarity tasks. Also, to evaluate the effectiveness of the\nproposed heuristic label process, this article presents a small ground truth\ndataset generated from domain expert annotations. The analysis of ground truth\nlabels highlights that semantic analysis of domain text can be challenging even\nfor domain experts. Also, the comparison between ground truth and heuristic\nlabels shows that heuristic labels are useful.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Junior_D/0/1/0/all/0/1\">Daniel da Silva Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corval_P/0/1/0/all/0/1\">Paulo Roberto dos S. Corval</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paes_A/0/1/0/all/0/1\">Aline Paes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1\">Daniel de Oliveira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Brainformers: Trading Simplicity for Efficiency. (arXiv:2306.00008v1 [cs.LG])","link":"http://arxiv.org/abs/2306.00008","description":"<p>Transformers are central to recent successes in natural language processing\nand computer vision. Transformers have a mostly uniform backbone where layers\nalternate between feed-forward and self-attention in order to build a deep\nnetwork. Here we investigate this design choice and find that more complex\nblocks that have different permutations of layer primitives can be more\nefficient. Using this insight, we develop a complex block, named Brainformer,\nthat consists of a diverse sets of layers such as sparsely gated feed-forward\nlayers, dense feed-forward layers, attention layers, and various forms of layer\nnormalization and activation functions. Brainformer consistently outperforms\nthe state-of-the-art dense and sparse Transformers, in terms of both quality\nand efficiency. A Brainformer model with 8 billion activated parameters per\ntoken demonstrates 2x faster training convergence and 5x faster step time\ncompared to its GLaM counterpart. In downstream task evaluation, Brainformer\nalso demonstrates a 3% higher SuperGLUE score with fine-tuning compared to GLaM\nwith a similar number of activated parameters. Finally, Brainformer largely\noutperforms a Primer dense model derived with NAS with similar computation per\ntoken on fewshot evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yanqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1\">Nan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Daiyi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Chang Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Da Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakeri_S/0/1/0/all/0/1\">Siamak Shakeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+So_D/0/1/0/all/0/1\">David So</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yifeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Claire Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laundon_J/0/1/0/all/0/1\">James Laundon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dean_J/0/1/0/all/0/1\">Jeff Dean</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning Approach for Cancer Entities Association and Classification. (arXiv:2306.00013v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00013","description":"<p>According to the World Health Organization (WHO), cancer is the second\nleading cause of death globally. Scientific research on different types of\ncancers grows at an ever-increasing rate, publishing large volumes of research\narticles every year. The insight information and the knowledge of the drug,\ndiagnostics, risk, symptoms, treatments, etc., related to genes are significant\nfactors that help explore and advance the cancer research progression. Manual\nscreening of such a large volume of articles is very laborious and\ntime-consuming to formulate any hypothesis. The study uses the two most\nnon-trivial NLP, Natural Language Processing functions, Entity Recognition, and\ntext classification to discover knowledge from biomedical literature. Named\nEntity Recognition (NER) recognizes and extracts the predefined entities\nrelated to cancer from unstructured text with the support of a user-friendly\ninterface and built-in dictionaries. Text classification helps to explore the\ninsights into the text and simplifies data categorization, querying, and\narticle screening. Machine learning classifiers are also used to build the\nclassification model and Structured Query Languages (SQL) is used to identify\nthe hidden relations that may lead to significant predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeyakodi_G/0/1/0/all/0/1\">G. Jeyakodi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_A/0/1/0/all/0/1\">Arkadeep Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Debapratim Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarukeswari_K/0/1/0/all/0/1\">K. Sarukeswari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amouda_V/0/1/0/all/0/1\">V. Amouda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PreQuant: A Task-agnostic Quantization Approach for Pre-trained Language Models. (arXiv:2306.00014v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00014","description":"<p>While transformer-based pre-trained language models (PLMs) have dominated a\nnumber of NLP applications, these models are heavy to deploy and expensive to\nuse. Therefore, effectively compressing large-scale PLMs becomes an\nincreasingly important problem. Quantization, which represents high-precision\ntensors with low-bit fix-point format, is a viable solution. However, most\nexisting quantization methods are task-specific, requiring customized training\nand quantization with a large number of trainable parameters on each individual\ntask. Inspired by the observation that the over-parameterization nature of PLMs\nmakes it possible to freeze most of the parameters during the fine-tuning\nstage, in this work, we propose a novel ``quantize before fine-tuning''\nframework, PreQuant, that differs from both quantization-aware training and\npost-training quantization. PreQuant is compatible with various quantization\nstrategies, with outlier-aware parameter-efficient fine-tuning incorporated to\ncorrect the induced quantization error. We demonstrate the effectiveness of\nPreQuant on the GLUE benchmark using BERT, RoBERTa, and T5. We also provide an\nempirical investigation into the workflow of PreQuant, which sheds light on its\nefficacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhuocheng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiahao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xian_Y/0/1/0/all/0/1\">Yunsen Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale. (arXiv:2306.00017v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00017","description":"<p>Large language models (LLMs) have achieved a milestone that undenia-bly\nchanged many held beliefs in artificial intelligence (AI). However, there\nremains many limitations of these LLMs when it comes to true language\nunderstanding, limitations that are a byproduct of the under-lying architecture\nof deep neural networks. Moreover, and due to their subsymbolic nature,\nwhatever knowledge these models acquire about how language works will always be\nburied in billions of microfeatures (weights), none of which is meaningful on\nits own, making such models hopelessly unexplainable. To address these\nlimitations, we suggest com-bining the strength of symbolic representations\nwith what we believe to be the key to the success of LLMs, namely a successful\nbottom-up re-verse engineering of language at scale. As such we argue for a\nbottom-up reverse engineering of language in a symbolic setting. Hints on what\nthis project amounts to have been suggested by several authors, and we discuss\nin some detail here how this project could be accomplished.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saba_W/0/1/0/all/0/1\">Walid S. Saba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Utilization of Multinomial Naive Bayes Algorithm and Term Frequency Inverse Document Frequency (TF-IDF Vectorizer) in Checking the Credibility of News Tweet in the Philippines. (arXiv:2306.00018v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00018","description":"<p>The digitalization of news media become a good indicator of progress and\nsignal to more threats. Media disinformation or fake news is one of these\nthreats, and it is necessary to take any action in fighting disinformation.\nThis paper utilizes ground truth-based annotations and TF-IDF as feature\nextraction for the news articles which is then used as a training data set for\nMultinomial Naive Bayes. The model has an accuracy of 99.46% in training and\n88.98% in predicting unseen data. Tagging fake news as real news is a\nconcerning point on the prediction that is indicated in the F1 score of 89.68%.\nThis could lead to a negative impact. To prevent this to happen it is suggested\nto further improve the corpus collection, and use an ensemble machine learning\nto reinforce the prediction\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Riego_N/0/1/0/all/0/1\">Neil Christian R. Riego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villarba_D/0/1/0/all/0/1\">Danny Bell Villarba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT4GEO: How a Language Model Sees the World's Geography. (arXiv:2306.00020v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00020","description":"<p>Large language models (LLMs) have shown remarkable capabilities across a\nbroad range of tasks involving question answering and the generation of\ncoherent text and code. Comprehensively understanding the strengths and\nweaknesses of LLMs is beneficial for safety, downstream applications and\nimproving performance. In this work, we investigate the degree to which GPT-4\nhas acquired factual geographic knowledge and is capable of using this\nknowledge for interpretative reasoning, which is especially important for\napplications that involve geographic data, such as geospatial analysis, supply\nchain management, and disaster response. To this end, we design and conduct a\nseries of diverse experiments, starting from factual tasks such as location,\ndistance and elevation estimation to more complex questions such as generating\ncountry outlines and travel networks, route finding under constraints and\nsupply chain analysis. We provide a broad characterisation of what GPT-4\n(without plugins or Internet access) knows about the world, highlighting both\npotentially surprising capabilities but also limitations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roberts_J/0/1/0/all/0/1\">Jonathan Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luddecke_T/0/1/0/all/0/1\">Timo L&#xfc;ddecke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sowmen Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1\">Samuel Albanie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Hate Speech Classification with Model Agnostic Methods. (arXiv:2306.00021v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00021","description":"<p>There have been remarkable breakthroughs in Machine Learning and Artificial\nIntelligence, notably in the areas of Natural Language Processing and Deep\nLearning. Additionally, hate speech detection in dialogues has been gaining\npopularity among Natural Language Processing researchers with the increased use\nof social media. However, as evidenced by the recent trends, the need for the\ndimensions of explainability and interpretability in AI models has been deeply\nrealised. Taking note of the factors above, the research goal of this paper is\nto bridge the gap between hate speech prediction and the explanations generated\nby the system to support its decision. This has been achieved by first\npredicting the classification of a text and then providing a posthoc, model\nagnostic and surrogate interpretability approach for explainability and to\nprevent model bias. The bidirectional transformer model BERT has been used for\nprediction because of its state of the art efficiency over other Machine\nLearning models. The model agnostic algorithm LIME generates explanations for\nthe output of a trained classifier and predicts the features that influence the\nmodel decision. The predictions generated from the model were evaluated\nmanually, and after thorough evaluation, we observed that the model performs\nefficiently in predicting and explaining its prediction. Lastly, we suggest\nfurther directions for the expansion of the provided research work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nandini_D/0/1/0/all/0/1\">Durgesh Nandini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_U/0/1/0/all/0/1\">Ute Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Verification Improves Few-Shot Clinical Information Extraction. (arXiv:2306.00024v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00024","description":"<p>Extracting patient information from unstructured text is a critical task in\nhealth decision-support and clinical research. Large language models (LLMs)\nhave shown the potential to accelerate clinical curation via few-shot\nin-context learning, in contrast to supervised learning which requires much\nmore costly human annotations. However, despite drastic advances in modern LLMs\nsuch as GPT-4, they still struggle with issues regarding accuracy and\ninterpretability, especially in mission-critical domains such as health. Here,\nwe explore a general mitigation framework using self-verification, which\nleverages the LLM to provide provenance for its own extraction and check its\nown outputs. This is made possible by the asymmetry between verification and\ngeneration, where the latter is often much easier than the former. Experimental\nresults show that our method consistently improves accuracy for various LLMs in\nstandard clinical information extraction tasks. Additionally, self-verification\nyields interpretations in the form of a short text span corresponding to each\noutput, which makes it very efficient for human experts to audit the results,\npaving the way towards trustworthy extraction of clinical information in\nresource-constrained scenarios. To facilitate future research in this\ndirection, we release our code and prompts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gero_Z/0/1/0/all/0/1\">Zelalem Gero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_C/0/1/0/all/0/1\">Chandan Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1\">Michel Galley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaXLR -- Mixed Language Meta Representation Transformation for Low-resource Cross-lingual Learning based on Multi-Armed Bandit. (arXiv:2306.00100v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00100","description":"<p>Transfer learning for extremely low resource languages is a challenging task\nas there is no large scale monolingual corpora for pre training or sufficient\nannotated data for fine tuning. We follow the work of MetaXL which suggests\nusing meta learning for transfer learning from a single source language to an\nextremely low resource one. We propose an enhanced approach which uses multiple\nsource languages chosen in a data driven manner. In addition, we introduce a\nsample selection strategy for utilizing the languages in training by using a\nmulti armed bandit algorithm. Using both of these improvements we managed to\nachieve state of the art results on the NER task for the extremely low resource\nlanguages while using the same amount of data, making the representations\nbetter generalized. Also, due to the method ability to use multiple languages\nit allows the framework to use much larger amounts of data, while still having\nsuperior results over the former MetaXL method even with the same amounts of\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bezalel_L/0/1/0/all/0/1\">Liat Bezalel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orgad_E/0/1/0/all/0/1\">Eyal Orgad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning. (arXiv:2306.00103v1 [cs.CV])","link":"http://arxiv.org/abs/2306.00103","description":"<p>Two-Tower Vision-Language (VL) models have shown promising improvements on\nvarious downstream VL tasks. Although the most advanced work improves\nperformance by building bridges between encoders, it suffers from ineffective\nlayer-by-layer utilization of uni-modal representations and cannot flexibly\nexploit different levels of uni-modal semantic knowledge. In this work, we\npropose ManagerTower, a novel VL model architecture that gathers and combines\nthe insights of pre-trained uni-modal experts at different levels. The managers\nintroduced in each cross-modal layer can adaptively aggregate uni-modal\nsemantic knowledge to facilitate more comprehensive cross-modal alignment and\nfusion. ManagerTower outperforms previous strong baselines both with and\nwithout Vision-Language Pre-training (VLP). With only 4M VLP data, ManagerTower\nachieves superior performances on various downstream VL tasks, especially\n79.15% accuracy on VQAv2 Test-Std, 86.56% IR@1 and 95.64% TR@1 on Flickr30K.\nCode and checkpoints are available at https://github.com/LooperXX/ManagerTower.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_S/0/1/0/all/0/1\">Shao-Yen Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhiwandiwalla_A/0/1/0/all/0/1\">Anahita Bhiwandiwalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenman_S/0/1/0/all/0/1\">Shachar Rosenman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1\">Vasudev Lal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training. (arXiv:2306.00107v1 [cs.SD])","link":"http://arxiv.org/abs/2306.00107","description":"<p>Self-supervised learning (SSL) has recently emerged as a promising paradigm\nfor training generalisable models on large-scale data in the fields of vision,\ntext, and speech. Although SSL has been proven effective in speech and audio,\nits application to music audio has yet to be thoroughly explored. This is\nprimarily due to the distinctive challenges associated with modelling musical\nknowledge, particularly its tonal and pitched characteristics of music. To\naddress this research gap, we propose an acoustic Music undERstanding model\nwith large-scale self-supervised Training (MERT), which incorporates teacher\nmodels to provide pseudo labels in the masked language modelling (MLM) style\nacoustic pre-training. In our exploration, we identified a superior combination\nof teacher models, which outperforms conventional speech and audio approaches\nin terms of performance. This combination includes an acoustic teacher based on\nResidual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a musical\nteacher based on the Constant-Q Transform (CQT). These teachers effectively\nguide our student model, a BERT-style transformer encoder, to better model\nmusic audio. In addition, we introduce an in-batch noise mixture augmentation\nto enhance the representation robustness. Furthermore, we explore a wide range\nof settings to overcome the instability in acoustic language model\npre-training, which allows our designed paradigm to scale from 95M to 330M\nparameters. Experimental results indicate that our model can generalise and\nperform well on 14 music understanding tasks and attains state-of-the-art\n(SOTA) overall scores. The code and models are online:\nhttps://github.com/yizhilll/MERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yizhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1\">Ruibin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Ge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yinghao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hanzhi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragni_A/0/1/0/all/0/1\">Anton Ragni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benetos_E/0/1/0/all/0/1\">Emmanouil Benetos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gyenge_N/0/1/0/all/0/1\">Norbert Gyenge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dannenberg_R/0/1/0/all/0/1\">Roger Dannenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gus Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yemin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenhao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yike Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MuseCoco: Generating Symbolic Music from Text. (arXiv:2306.00110v1 [cs.SD])","link":"http://arxiv.org/abs/2306.00110","description":"<p>Generating music from text descriptions is a user-friendly mode since the\ntext is a relatively easy interface for user engagement. While some approaches\nutilize texts to control music audio generation, editing musical elements in\ngenerated audio is challenging for users. In contrast, symbolic music offers\nease of editing, making it more accessible for users to manipulate specific\nmusical elements. In this paper, we propose MuseCoco, which generates symbolic\nmusic from text descriptions with musical attributes as the bridge to break\ndown the task into text-to-attribute understanding and attribute-to-music\ngeneration stages. MuseCoCo stands for Music Composition Copilot that empowers\nmusicians to generate music directly from given text descriptions, offering a\nsignificant improvement in efficiency compared to creating music entirely from\nscratch. The system has two main advantages: Firstly, it is data efficient. In\nthe attribute-to-music generation stage, the attributes can be directly\nextracted from music sequences, making the model training self-supervised. In\nthe text-to-attribute understanding stage, the text is synthesized and refined\nby ChatGPT based on the defined attribute templates. Secondly, the system can\nachieve precise control with specific attributes in text descriptions and\noffers multiple control options through attribute-conditioned or\ntext-conditioned approaches. MuseCoco outperforms baseline systems in terms of\nmusicality, controllability, and overall score by at least 1.27, 1.08, and 1.32\nrespectively. Besides, there is a notable enhancement of about 20% in objective\ncontrol accuracy. In addition, we have developed a robust large-scale model\nwith 1.2 billion parameters, showcasing exceptional controllability and\nmusicality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Peiling Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_C/0/1/0/all/0/1\">Chenfei Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Botao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1\">Chengyi Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Multi-Figurative Language Detection. (arXiv:2306.00121v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00121","description":"<p>Figures of speech help people express abstract concepts and evoke stronger\nemotions than literal expressions, thereby making texts more creative and\nengaging. Due to its pervasive and fundamental character, figurative language\nunderstanding has been addressed in Natural Language Processing, but it's\nhighly understudied in a multilingual setting and when considering more than\none figure of speech at the same time. To bridge this gap, we introduce\nmultilingual multi-figurative language modelling, and provide a benchmark for\nsentence-level figurative language detection, covering three common figures of\nspeech and seven languages. Specifically, we develop a framework for figurative\nlanguage detection based on template-based prompt learning. In so doing, we\nunify multiple detection tasks that are interrelated across multiple figures of\nspeech and languages, without requiring task- or language-specific modules.\nExperimental results show that our framework outperforms several strong\nbaselines and may serve as a blueprint for the joint modelling of other\ninterrelated tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1\">Huiyuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toral_A/0/1/0/all/0/1\">Antonio Toral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nissim_M/0/1/0/all/0/1\">Malvina Nissim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-Trained Language-Meaning Models for Multilingual Parsing and Generation. (arXiv:2306.00124v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00124","description":"<p>Pre-trained language models (PLMs) have achieved great success in NLP and\nhave recently been used for tasks in computational semantics. However, these\ntasks do not fully benefit from PLMs since meaning representations are not\nexplicitly included in the pre-training stage. We introduce multilingual\npre-trained language-meaning models based on Discourse Representation\nStructures (DRSs), including meaning representations besides natural language\ntexts in the same model, and design a new strategy to reduce the gap between\nthe pre-training and fine-tuning objectives. Since DRSs are language neutral,\ncross-lingual transfer learning is adopted to further improve the performance\nof non-English tasks. Automatic evaluation results show that our approach\nachieves the best performance on both the multilingual DRS parsing and\nDRS-to-text generation tasks. Correlation analysis between automatic metrics\nand human judgements on the generation task further validates the effectiveness\nof our model. Human inspection reveals that out-of-vocabulary tokens are the\nmain cause of erroneous results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunliu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1\">Huiyuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nissim_M/0/1/0/all/0/1\">Malvina Nissim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bos_J/0/1/0/all/0/1\">Johan Bos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Sequence-to-Sequence&Set Model for Text-to-Table Generation. (arXiv:2306.00137v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00137","description":"<p>Recently, the text-to-table generation task has attracted increasing\nattention due to its wide applications. In this aspect, the dominant model\nformalizes this task as a sequence-to-sequence generation task and serializes\neach table into a token sequence during training by concatenating all rows in a\ntop-down order. However, it suffers from two serious defects: 1) the predefined\norder introduces a wrong bias during training, which highly penalizes shifts in\nthe order between rows; 2) the error propagation problem becomes serious when\nthe model outputs a long token sequence. In this paper, we first conduct a\npreliminary study to demonstrate the generation of most rows is\norder-insensitive. Furthermore, we propose a novel sequence-to-sequence&amp;set\ntext-to-table generation model. Specifically, in addition to a text encoder\nencoding the input text, our model is equipped with a table header generator to\nfirst output a table header, i.e., the first row of the table, in the manner of\nsequence generation. Then we use a table body generator with learnable row\nembeddings and column embeddings to generate a set of table body rows in\nparallel. Particularly, to deal with the issue that there is no correspondence\nbetween each generated table body row and target during training, we propose a\ntarget assignment strategy based on the bipartite matching between the first\ncells of generated table body rows and targets. Experiment results show that\nour model significantly surpasses the baselines, achieving state-of-the-art\nperformance on commonly-used datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Liangying Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xuling Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring the Robustness of Natural Language Processing Models to Domain Shifts. (arXiv:2306.00168v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00168","description":"<p>Large Language Models have shown promising performance on various tasks,\nincluding fine-tuning, few-shot learning, and zero-shot learning. However,\ntheir performance on domains without labeled data still lags behind those with\nlabeled data, which we refer as the Domain Robustness (DR) challenge. Existing\nresearch on DR suffers from disparate setups, lack of evaluation task variety,\nand reliance on challenge sets. In this paper, we explore the DR challenge of\nboth fine-tuned and few-shot learning models in natural domain shift settings.\nWe introduce a DR benchmark comprising diverse NLP tasks, including sentence\nand token-level classification, QA, and generation, each task consists of\nseveral domains. We propose two views of the DR challenge: Source Drop (SD) and\nTarget Drop (TD), which alternate between the source and target in-domain\nperformance as reference points. We find that in significant proportions of\ndomain shifts, either SD or TD is positive, but not both, emphasizing the\nimportance of considering both measures as diagnostic tools. Our experimental\nresults demonstrate the persistent existence of the DR challenge in both\nfine-tuning and few-shot learning models, though it is less pronounced in the\nlatter. We also find that increasing the fine-tuned model size improves\nperformance, particularly in classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Calderon_N/0/1/0/all/0/1\">Nitay Calderon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porat_N/0/1/0/all/0/1\">Naveh Porat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_David_E/0/1/0/all/0/1\">Eyal Ben-David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gekhman_Z/0/1/0/all/0/1\">Zorik Gekhman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oved_N/0/1/0/all/0/1\">Nadav Oved</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Annotation with Generative AI Requires Validation. (arXiv:2306.00176v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00176","description":"<p>Generative large language models (LLMs) can be a powerful tool for augmenting\ntext annotation procedures, but their performance varies across annotation\ntasks due to prompt quality, text data idiosyncrasies, and conceptual\ndifficulty. Because these challenges will persist even as LLM technology\nimproves, we argue that any automated annotation process using an LLM must\nvalidate the LLM's performance against labels generated by humans. To this end,\nwe outline a workflow to harness the annotation potential of LLMs in a\nprincipled, efficient way. Using GPT-4, we validate this approach by\nreplicating 27 annotation tasks across 11 datasets from recent social science\narticles in high-impact journals. We find that LLM performance for text\nannotation is promising but highly contingent on both the dataset and the type\nof annotation task, which reinforces the necessity to validate on a\ntask-by-task basis. We make available easy-to-use software designed to\nimplement our workflow and streamline the deployment of LLMs for automated\nannotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pangakis_N/0/1/0/all/0/1\">Nicholas Pangakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolken_S/0/1/0/all/0/1\">Samuel Wolken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fasching_N/0/1/0/all/0/1\">Neil Fasching</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Hierarchical Discourse Graph for Scientific Document Summarization. (arXiv:2306.00177v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00177","description":"<p>The extended structural context has made scientific paper summarization a\nchallenging task. This paper proposes CHANGES, a contrastive hierarchical graph\nneural network for extractive scientific paper summarization. CHANGES\nrepresents a scientific paper with a hierarchical discourse graph and learns\neffective sentence representations with dedicated designed hierarchical graph\ninformation aggregation. We also propose a graph contrastive learning module to\nlearn global theme-aware sentence representations. Extensive experiments on the\nPubMed and arXiv benchmark datasets prove the effectiveness of CHANGES and the\nimportance of capturing hierarchical structure information in modeling\nscientific papers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback. (arXiv:2306.00186v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00186","description":"<p>Despite the seeming success of contemporary grounded text generation systems,\nthey often tend to generate factually inconsistent text with respect to their\ninput. This phenomenon is emphasized in tasks like summarization, in which the\ngenerated summaries should be corroborated by their source article. In this\nwork, we leverage recent progress on textual entailment models to directly\naddress this problem for abstractive summarization systems. We use\nreinforcement learning with reference-free, textual entailment rewards to\noptimize for factual consistency and explore the ensuing trade-offs, as\nimproved consistency may come at the cost of less informative or more\nextractive summaries. Our results, according to both automatic metrics and\nhuman evaluation, show that our method considerably improves the faithfulness,\nsalience, and conciseness of the generated summaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roit_P/0/1/0/all/0/1\">Paul Roit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferret_J/0/1/0/all/0/1\">Johan Ferret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shani_L/0/1/0/all/0/1\">Lior Shani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aharoni_R/0/1/0/all/0/1\">Roee Aharoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cideron_G/0/1/0/all/0/1\">Geoffrey Cideron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dadashi_R/0/1/0/all/0/1\">Robert Dadashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1\">Matthieu Geist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girgin_S/0/1/0/all/0/1\">Sertan Girgin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussenot_L/0/1/0/all/0/1\">L&#xe9;onard Hussenot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_O/0/1/0/all/0/1\">Orgad Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momchev_N/0/1/0/all/0/1\">Nikola Momchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramos_S/0/1/0/all/0/1\">Sabela Ramos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanczyk_P/0/1/0/all/0/1\">Piotr Stanczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vieillard_N/0/1/0/all/0/1\">Nino Vieillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachem_O/0/1/0/all/0/1\">Olivier Bachem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elidan_G/0/1/0/all/0/1\">Gal Elidan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassidim_A/0/1/0/all/0/1\">Avinatan Hassidim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietquin_O/0/1/0/all/0/1\">Olivier Pietquin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szpektor_I/0/1/0/all/0/1\">Idan Szpektor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Invariant Learning Characterization of Controlled Text Generation. (arXiv:2306.00198v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00198","description":"<p>Controlled generation refers to the problem of creating text that contains\nstylistic or semantic attributes of interest. Many approaches reduce this\nproblem to training a predictor of the desired attribute. For example,\nresearchers hoping to deploy a large language model to produce non-toxic\ncontent may use a toxicity classifier to filter generated text. In practice,\nthe generated text to classify, which is determined by user prompts, may come\nfrom a wide range of distributions. In this paper, we show that the performance\nof controlled generation may be poor if the distributions of text in response\nto user prompts differ from the distribution the predictor was trained on. To\naddress this problem, we cast controlled generation under distribution shift as\nan invariant learning problem: the most effective predictor should be invariant\nacross multiple text environments. We then discuss a natural solution that\narises from this characterization and propose heuristics for selecting natural\nenvironments. We study this characterization and the proposed method\nempirically using both synthetic and real data. Experiments demonstrate both\nthe challenge of distribution shift in controlled generation and the potential\nof invariance methods in this setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Carolina Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Claudia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vafa_K/0/1/0/all/0/1\">Keyon Vafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1\">Amir Feder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blei_D/0/1/0/all/0/1\">David M. Blei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Strategies for improving low resource speech to text translation relying on pre-trained ASR models. (arXiv:2306.00208v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00208","description":"<p>This paper presents techniques and findings for improving the performance of\nlow-resource speech to text translation (ST). We conducted experiments on both\nsimulated and real-low resource setups, on language pairs English - Portuguese,\nand Tamasheq - French respectively. Using the encoder-decoder framework for ST,\nour results show that a multilingual automatic speech recognition system acts\nas a good initialization under low-resource scenarios. Furthermore, using the\nCTC as an additional objective for translation during training and decoding\nhelps to reorder the internal representations and improves the final\ntranslation. Through our experiments, we try to identify various factors\n(initializations, objectives, and hyper-parameters) that contribute the most\nfor improvements in low-resource setups. With only 300 hours of pre-training\ndata, our model achieved 7.3 BLEU score on Tamasheq - French data,\noutperforming prior published works from IWSLT 2022 by 1.6 points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kesiraju_S/0/1/0/all/0/1\">Santosh Kesiraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvas_M/0/1/0/all/0/1\">Marek Sarvas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlicek_T/0/1/0/all/0/1\">Tomas Pavlicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macaire_C/0/1/0/all/0/1\">Cecile Macaire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciuba_A/0/1/0/all/0/1\">Alejandro Ciuba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FEED PETs: Further Experimentation and Expansion on the Disambiguation of Potentially Euphemistic Terms. (arXiv:2306.00217v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00217","description":"<p>Transformers have been shown to work well for the task of English euphemism\ndisambiguation, in which a potentially euphemistic term (PET) is classified as\neuphemistic or non-euphemistic in a particular context. In this study, we\nexpand on the task in two ways. First, we annotate PETs for vagueness, a\nlinguistic property associated with euphemisms, and find that transformers are\ngenerally better at classifying vague PETs, suggesting linguistic differences\nin the data that impact performance. Second, we present novel euphemism corpora\nin three different languages: Yoruba, Spanish, and Mandarin Chinese. We perform\neuphemism disambiguation experiments in each language using multilingual\ntransformer models mBERT and XLM-RoBERTa, establishing preliminary results from\nwhich to launch future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1\">Patrick Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shode_I/0/1/0/all/0/1\">Iyanuoluwa Shode</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trujillo_A/0/1/0/all/0/1\">Alain Chirino Trujillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ojo_O/0/1/0/all/0/1\">Olumide Ebenezer Ojo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plancarte_D/0/1/0/all/0/1\">Diana Cuervas Plancarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_A/0/1/0/all/0/1\">Anna Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jing Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images. (arXiv:2306.00219v1 [cs.CV])","link":"http://arxiv.org/abs/2306.00219","description":"<p>Text-to-image generative models have made remarkable advancements in\ngenerating high-quality images. However, generated images often contain\nundesirable artifacts or other errors due to model limitations. Existing\ntechniques to fine-tune generated images are time-consuming (manual editing),\nproduce poorly-integrated results (inpainting), or result in unexpected changes\nacross the entire image (variation selection and prompt fine-tuning). In this\nwork, we present Diffusion Brush, a Latent Diffusion Model-based (LDM) tool to\nefficiently fine-tune desired regions within an AI-synthesized image. Our\nmethod introduces new random noise patterns at targeted regions during the\nreverse diffusion process, enabling the model to efficiently make changes to\nthe specified regions while preserving the original context for the rest of the\nimage. We evaluate our method's usability and effectiveness through a user\nstudy with artists, comparing our technique against other state-of-the-art\nimage inpainting techniques and editing software for fine-tuning AI-generated\nimagery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gholami_P/0/1/0/all/0/1\">Peyman Gholami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_R/0/1/0/all/0/1\">Robert Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Visual Cropping to Enhance Fine-Detail Question Answering of BLIP-Family Models. (arXiv:2306.00228v1 [cs.CV])","link":"http://arxiv.org/abs/2306.00228","description":"<p>Visual Question Answering is a challenging task, as it requires seamless\ninteraction between perceptual, linguistic, and background knowledge systems.\nWhile the recent progress of visual and natural language models like BLIP has\nled to improved performance on this task, we lack understanding of the ability\nof such models to perform on different kinds of questions and reasoning types.\nAs our initial analysis of BLIP-family models revealed difficulty with\nanswering fine-detail questions, we investigate the following question: Can\nvisual cropping be employed to improve the performance of state-of-the-art\nvisual question answering models on fine-detail questions? Given the recent\nsuccess of the BLIP-family models, we study a zero-shot and a fine-tuned BLIP\nmodel. We define three controlled subsets of the popular VQA-v2 benchmark to\nmeasure whether cropping can help model performance. Besides human cropping, we\ndevise two automatic cropping strategies based on multi-modal embedding by CLIP\nand BLIP visual QA model gradients. Our experiments demonstrate that the\nperformance of BLIP model variants can be significantly improved through human\ncropping, and automatic cropping methods can produce comparable benefits. A\ndeeper dive into our findings indicates that the performance enhancement is\nmore pronounced in zero-shot models than in fine-tuned models and more salient\nwith smaller bounding boxes than larger ones. We perform case studies to\nconnect quantitative differences with qualitative observations across question\ntypes and datasets. Finally, we see that the cropping enhancement is robust, as\nwe gain an improvement of 4.59% (absolute) in the general VQA-random task by\nsimply inputting a concatenation of the original and gradient-based cropped\nimages. We make our code available to facilitate further innovation on visual\ncropping methods for question answering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiarui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khayatkhoei_M/0/1/0/all/0/1\">Mahyar Khayatkhoei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhikara_P/0/1/0/all/0/1\">Prateek Chhikara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces. (arXiv:2306.00245v1 [cs.LG])","link":"http://arxiv.org/abs/2306.00245","description":"<p>Much of the previous work towards digital agents for graphical user\ninterfaces (GUIs) has relied on text-based representations (derived from HTML\nor other structured data sources), which are not always readily available.\nThese input representations have been often coupled with custom, task-specific\naction spaces. This paper focuses on creating agents that interact with the\ndigital world using the same conceptual interface that humans commonly use --\nvia pixel-based screenshots and a generic action space corresponding to\nkeyboard and mouse actions. Building upon recent progress in pixel-based\npretraining, we show, for the first time, that it is possible for such agents\nto outperform human crowdworkers on the MiniWob++ benchmark of GUI-based\ninstruction following tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaw_P/0/1/0/all/0/1\">Peter Shaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1\">Mandar Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_J/0/1/0/all/0/1\">James Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasupat_P/0/1/0/all/0/1\">Panupong Pasupat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hexiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_U/0/1/0/all/0/1\">Urvashi Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kenton Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toutanova_K/0/1/0/all/0/1\">Kristina Toutanova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AfriNames: Most ASR models \"butcher\" African Names. (arXiv:2306.00253v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00253","description":"<p>Useful conversational agents must accurately capture named entities to\nminimize error for downstream tasks, for example, asking a voice assistant to\nplay a track from a certain artist, initiating navigation to a specific\nlocation, or documenting a laboratory result for a patient. However, where\nnamed entities such as ``Ukachukwu`` (Igbo), ``Lakicia`` (Swahili), or\n``Ingabire`` (Rwandan) are spoken, automatic speech recognition (ASR) models'\nperformance degrades significantly, propagating errors to downstream systems.\nWe model this problem as a distribution shift and demonstrate that such model\nbias can be mitigated through multilingual pre-training, intelligent data\naugmentation strategies to increase the representation of African-named\nentities, and fine-tuning multilingual ASR models on multiple African accents.\nThe resulting fine-tuned models show an 81.5\\% relative WER improvement\ncompared with the baseline on samples with African-named entities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Olatunji_T/0/1/0/all/0/1\">Tobi Olatunji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afonja_T/0/1/0/all/0/1\">Tejumade Afonja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dossou_B/0/1/0/all/0/1\">Bonaventure F. P. Dossou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonja_A/0/1/0/all/0/1\">Atnafu Lambebo Tonja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1\">Chris Chinenye Emezue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rufai_A/0/1/0/all/0/1\">Amina Mardiyyah Rufai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sahib Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training-free Neural Architecture Search for RNNs and Transformers. (arXiv:2306.00288v1 [cs.LG])","link":"http://arxiv.org/abs/2306.00288","description":"<p>Neural architecture search (NAS) has allowed for the automatic creation of\nnew and effective neural network architectures, offering an alternative to the\nlaborious process of manually designing complex architectures. However,\ntraditional NAS algorithms are slow and require immense amounts of computing\npower. Recent research has investigated training-free NAS metrics for image\nclassification architectures, drastically speeding up search algorithms. In\nthis paper, we investigate training-free NAS metrics for recurrent neural\nnetwork (RNN) and BERT-based transformer architectures, targeted towards\nlanguage modeling tasks. First, we develop a new training-free metric, named\nhidden covariance, that predicts the trained performance of an RNN architecture\nand significantly outperforms existing training-free metrics. We experimentally\nevaluate the effectiveness of the hidden covariance metric on the NAS-Bench-NLP\nbenchmark. Second, we find that the current search space paradigm for\ntransformer architectures is not optimized for training-free neural\narchitecture search. Instead, a simple qualitative analysis can effectively\nshrink the search space to the best performing architectures. This conclusion\nis based on our investigation of existing training-free metrics and new metrics\ndeveloped from recent transformer pruning literature, evaluated on our own\nbenchmark of trained BERT architectures. Ultimately, our analysis shows that\nthe architecture search space and the training-free metric must be developed\ntogether in order to achieve effective results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Serianni_A/0/1/0/all/0/1\">Aaron Serianni</a> (Princeton University), <a href=\"http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1\">Jugal Kalita</a> (University of Colorado at Colorado Springs)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CapText: Large Language Model-based Caption Generation From Image Context and Description. (arXiv:2306.00301v1 [cs.LG])","link":"http://arxiv.org/abs/2306.00301","description":"<p>While deep-learning models have been shown to perform well on image-to-text\ndatasets, it is difficult to use them in practice for captioning images. This\nis because \\textit{captions} traditionally tend to be context-dependent and\noffer complementary information about an image, while models tend to produce\n\\textit{descriptions} that describe the visual features of the image. Prior\nresearch in caption generation has explored the use of models that generate\ncaptions when provided with the images alongside their respective descriptions\nor contexts. We propose and evaluate a new approach, which leverages existing\nlarge language models to generate captions from textual descriptions and\ncontext alone, without ever processing the image directly. We demonstrate that\nafter fine-tuning, our approach outperforms current state-of-the-art image-text\nalignment models like OSCAR-VinVL on this task on the CIDEr metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shinjini Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anupam_S/0/1/0/all/0/1\">Sagnik Anupam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAISA at SemEval-2023 Task 8: Counterfactual Data Augmentation for Mitigating Class Imbalance in Causal Claim Identification. (arXiv:2306.00346v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00346","description":"<p>The class imbalance problem can cause machine learning models to produce an\nundesirable performance on the minority class as well as the whole dataset.\nUsing data augmentation techniques to increase the number of samples is one way\nto tackle this problem. We introduce a novel counterfactual data augmentation\nby verb replacement for the identification of medical claims. In addition, we\ninvestigate the impact of this method and compare it with 3 other data\naugmentation techniques, showing that the proposed method can result in a\nsignificant (relative) improvement in the minority class.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karimi_A/0/1/0/all/0/1\">Akbar Karimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flek_L/0/1/0/all/0/1\">Lucie Flek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focused Prefix Tuning for Controllable Text Generation. (arXiv:2306.00369v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00369","description":"<p>In a controllable text generation dataset, there exist unannotated attributes\nthat could provide irrelevant learning signals to models that use it for\ntraining and thus degrade their performance. We propose focused prefix\ntuning(FPT) to mitigate the problem and to enable the control to focus on the\ndesired attribute. Experimental results show that FPT can achieve better\ncontrol accuracy and text fluency than baseline models in single-attribute\ncontrol tasks. In multi-attribute control tasks, FPT achieves comparable\ncontrol accuracy with the state-of-the-art approach while keeping the\nflexibility to control new attributes without retraining existing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Congda Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shing_M/0/1/0/all/0/1\">Makoto Shing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawada_K/0/1/0/all/0/1\">Kei Sawada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okumura_M/0/1/0/all/0/1\">Manabu Okumura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CFL: Causally Fair Language Models Through Token-level Attribute Controlled Generation. (arXiv:2306.00374v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00374","description":"<p>We propose a method to control the attributes of Language Models (LMs) for\nthe text generation task using Causal Average Treatment Effect (ATE) scores and\ncounterfactual augmentation. We explore this method, in the context of LM\ndetoxification, and propose the Causally Fair Language (CFL) architecture for\ndetoxifying pre-trained LMs in a plug-and-play manner. Our architecture is\nbased on a Structural Causal Model (SCM) that is mathematically transparent and\ncomputationally efficient as compared with many existing detoxification\ntechniques. We also propose several new metrics that aim to better understand\nthe behaviour of LMs in the context of toxic text generation. Further, we\nachieve state of the art performance for toxic degeneration, which are computed\nusing \\RTP (RTP) benchmark. Our experiments show that CFL achieves such a\ndetoxification without much impact on the model perplexity. We also show that\nCFL mitigates the unintended bias problem through experiments on the BOLD\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madhavan_R/0/1/0/all/0/1\">Rahul Madhavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_R/0/1/0/all/0/1\">Rishabh Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadhawan_K/0/1/0/all/0/1\">Kahini Wadhawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1\">Sameep Mehta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Developing and Building Ontologies in Cyber Security. (arXiv:2306.00377v1 [cs.CR])","link":"http://arxiv.org/abs/2306.00377","description":"<p>Cyber Security is one of the most arising disciplines in our modern society.\nWe work on Cybersecurity domain and in this the topic we chose is Cyber\nSecurity Ontologies. In this we gather all latest and previous ontologies and\ncompare them on the basis of different analyzing factors to get best of them.\nReason to select this topic is to assemble different ontologies from different\nera of time. Because, researches that included in this SLR is mostly studied\nsingle ontology. If any researcher wants to study ontologies, he has to study\nevery single ontology and select which one is best for his research. So, we\nassemble different types of ontology and compare them against each other to get\nbest of them. A total 24 papers between years 2010-2020 are carefully selected\nthrough systematic process and classified accordingly. Lastly, this SLR have\nbeen presented to provide the researchers promising future directions in the\ndomain of cybersecurity ontologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farooq_M/0/1/0/all/0/1\">Muhammad Shoaib Farooq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waseem_M/0/1/0/all/0/1\">Muhammad Talha Waseem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preference-grounded Token-level Guidance for Language Model Fine-tuning. (arXiv:2306.00398v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00398","description":"<p>Aligning language models (LMs) with preferences is an important problem in\nnatural language generation. A key challenge is that preferences are typically\nprovided at the sequence level while LM training and generation both occur at\nthe token level. There is, therefore, a granularity mismatch between the\npreference and the LM training losses, which may complicate the learning\nproblem. In this paper, we address this issue by developing an alternate\ntraining process, where we iterate between grounding the sequence-level\npreference into token-level training guidance, and improving the LM with the\nlearned guidance. For guidance learning, we design a framework that extends the\npairwise-preference learning in imitation learning to both variable-length LM\ngeneration and utilizing the preference among multiple generations. For LM\ntraining, based on the amount of supervised data, we present two minimalist\nlearning objectives that utilize the learned guidance. In experiments, our\nmethod performs competitively on two distinct representative LM tasks --\ndiscrete-prompt generation and text summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shentao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shujian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Congying Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yihao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyuan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiSync: A Bilingual Editor for Synchronized Monolingual Texts. (arXiv:2306.00400v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00400","description":"<p>In our globalized world, a growing number of situations arise where people\nare required to communicate in one or several foreign languages. In the case of\nwritten communication, users with a good command of a foreign language may find\nassistance from computer-aided translation (CAT) technologies. These\ntechnologies often allow users to access external resources, such as\ndictionaries, terminologies or bilingual concordancers, thereby interrupting\nand considerably hindering the writing process. In addition, CAT systems assume\nthat the source sentence is fixed and also restrict the possible changes on the\ntarget side. In order to make the writing process smoother, we present BiSync,\na bilingual writing assistant that allows users to freely compose text in two\nlanguages, while maintaining the two monolingual texts synchronized. We also\ninclude additional functionalities, such as the display of alternative prefix\ntranslations and paraphrases, which are intended to facilitate the authoring of\ntexts. We detail the model architecture used for synchronization and evaluate\nthe resulting tool, showing that high accuracy can be attained with limited\ncomputational resources. The interface and models are publicly available at\nhttps://github.com/jmcrego/BiSync and a demonstration video can be watched on\nYouTube at https://youtu.be/_l-ugDHfNgU .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Crego_J/0/1/0/all/0/1\">Josep Crego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jitao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yvon_F/0/1/0/all/0/1\">Fran&#xe7;ois Yvon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards hate speech detection in low-resource languages: Comparing ASR to acoustic word embeddings on Wolof and Swahili. (arXiv:2306.00410v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00410","description":"<p>We consider hate speech detection through keyword spotting on radio\nbroadcasts. One approach is to build an automatic speech recognition (ASR)\nsystem for the target low-resource language. We compare this to using acoustic\nword embedding (AWE) models that map speech segments to a space where matching\nwords have similar vectors. We specifically use a multilingual AWE model\ntrained on labelled data from well-resourced languages to spot keywords in data\nin the unseen target language. In contrast to ASR, the AWE approach only\nrequires a few keyword exemplars. In controlled experiments on Wolof and\nSwahili where training and test data are from the same domain, an ASR model\ntrained on just five minutes of data outperforms the AWE approach. But in an\nin-the-wild test on Swahili radio broadcasts with actual hate speech keywords,\nthe AWE model (using one minute of template data) is more robust, giving\nsimilar performance to an ASR system trained on 30 hours of labelled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_C/0/1/0/all/0/1\">Christiaan Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakotonirina_N/0/1/0/all/0/1\">Nathana&#xeb;l Carraz Rakotonirina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chimoto_E/0/1/0/all/0/1\">Everlyn Asiko Chimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bassett_B/0/1/0/all/0/1\">Bruce A. Bassett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1\">Herman Kamper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Aware Unlikelihood Learning Improves Generative Aspect Sentiment Quad Prediction. (arXiv:2306.00418v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00418","description":"<p>Recently, aspect sentiment quad prediction has received widespread attention\nin the field of aspect-based sentiment analysis. Existing studies extract\nquadruplets via pre-trained generative language models to paraphrase the\noriginal sentence into a templated target sequence. However, previous works\nonly focus on what to generate but ignore what not to generate. We argue that\nconsidering the negative samples also leads to potential benefits. In this\nwork, we propose a template-agnostic method to control the token-level\ngeneration, which boosts original learning and reduces mistakes simultaneously.\nSpecifically, we introduce Monte Carlo dropout to understand the built-in\nuncertainty of pre-trained language models, acquiring the noises and errors. We\nfurther propose marginalized unlikelihood learning to suppress the\nuncertainty-aware mistake tokens. Finally, we introduce minimization entropy to\nbalance the effects of marginalized unlikelihood learning. Extensive\nexperiments on four public datasets demonstrate the effectiveness of our\napproach on various generation templates1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Mengting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yinhao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yike Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Hang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shiwan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Knowledge Retrieval with Multi-modal Queries. (arXiv:2306.00424v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00424","description":"<p>We investigate knowledge retrieval with multi-modal queries, i.e. queries\ncontaining information split across image and text inputs, a challenging task\nthat differs from previous work on cross-modal retrieval. We curate a new\ndataset called ReMuQ for benchmarking progress on this task. ReMuQ requires a\nsystem to retrieve knowledge from a large corpus by integrating contents from\nboth text and image queries. We introduce a retriever model ``ReViz'' that can\ndirectly process input text and images to retrieve relevant knowledge in an\nend-to-end fashion without being dependent on intermediate modules such as\nobject detectors or caption generators. We introduce a new pretraining task\nthat is effective for learning knowledge retrieval with multimodal queries and\nalso improves performance on downstream tasks. We demonstrate superior\nperformance in retrieval on two datasets (ReMuQ and OK-VQA) under zero-shot\nsettings as well as further improvements when finetuned on these datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Man Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhiyuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1\">Tejas Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Divide, Conquer, and Combine: Mixture of Semantic-Independent Experts for Zero-Shot Dialogue State Tracking. (arXiv:2306.00434v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00434","description":"<p>Zero-shot transfer learning for Dialogue State Tracking (DST) helps to handle\na variety of task-oriented dialogue domains without the cost of collecting\nin-domain data. Existing works mainly study common data- or model-level\naugmentation methods to enhance the generalization but fail to effectively\ndecouple the semantics of samples, limiting the zero-shot performance of DST.\nIn this paper, we present a simple and effective \"divide, conquer and combine\"\nsolution, which explicitly disentangles the semantics of seen data, and\nleverages the performance and robustness with the mixture-of-experts mechanism.\nSpecifically, we divide the seen data into semantically independent subsets and\ntrain corresponding experts, the newly unseen samples are mapped and inferred\nwith mixture-of-experts with our designed ensemble inference. Extensive\nexperiments on MultiWOZ2.1 upon the T5-Adapter show our schema significantly\nand consistently improves the zero-shot performance, achieving the SOTA on\nsettings without external knowledge, with only 10M trainable parameters1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yanan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Li Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Many Answers Should I Give? An Empirical Study of Multi-Answer Reading Comprehension. (arXiv:2306.00435v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00435","description":"<p>The multi-answer phenomenon, where a question may have multiple answers\nscattered in the document, can be well handled by humans but is challenging\nenough for machine reading comprehension (MRC) systems. Despite recent progress\nin multi-answer MRC, there lacks a systematic analysis of how this phenomenon\narises and how to better address it. In this work, we design a taxonomy to\ncategorize commonly-seen multi-answer MRC instances, with which we inspect\nthree multi-answer datasets and analyze where the multi-answer challenge comes\nfrom. We further analyze how well different paradigms of current multi-answer\nMRC models deal with different types of multi-answer instances. We find that\nsome paradigms capture well the key information in the questions while others\nbetter model the relationship between questions and contexts. We thus explore\nstrategies to make the best of the strengths of different paradigms.\nExperiments show that generation models can be a promising platform to\nincorporate different paradigms. Our annotations and code are released for\nfurther research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jiuheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yuxuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yansong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Responsibility Perspective Transfer for Italian Femicide News. (arXiv:2306.00437v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00437","description":"<p>Different ways of linguistically expressing the same real-world event can\nlead to different perceptions of what happened. Previous work has shown that\ndifferent descriptions of gender-based violence (GBV) influence the reader's\nperception of who is to blame for the violence, possibly reinforcing\nstereotypes which see the victim as partly responsible, too. As a contribution\nto raise awareness on perspective-based writing, and to facilitate access to\nalternative perspectives, we introduce the novel task of automatically\nrewriting GBV descriptions as a means to alter the perceived level of\nresponsibility on the perpetrator. We present a quasi-parallel dataset of\nsentences with low and high perceived responsibility levels for the\nperpetrator, and experiment with unsupervised (mBART-based), zero-shot and\nfew-shot (GPT3-based) methods for rewriting sentences. We evaluate our models\nusing a questionnaire study and a suite of automatic metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Minnema_G/0/1/0/all/0/1\">Gosse Minnema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1\">Huiyuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muscato_B/0/1/0/all/0/1\">Benedetta Muscato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nissim_M/0/1/0/all/0/1\">Malvina Nissim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A big data approach towards sarcasm detection in Russian. (arXiv:2306.00445v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00445","description":"<p>We present a set of deterministic algorithms for Russian inflection and\nautomated text synthesis. These algorithms are implemented in a publicly\navailable web-service www.passare.ru. This service provides functions for\ninflection of single words, word matching and synthesis of grammatically\ncorrect Russian text. Selected code and datasets are available at\nhttps://github.com/passare-ru/PassareFunctions/ Performance of the inflectional\nfunctions has been tested against the annotated corpus of Russian language\nOpenCorpora, compared with that of other solutions, and used for estimating the\nmorphological variability and complexity of different parts of speech in\nRussian.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gurin_A/0/1/0/all/0/1\">A.A. Gurin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadykov_T/0/1/0/all/0/1\">T.M. Sadykov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhukov_T/0/1/0/all/0/1\">T.A. Zhukov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Anisotropy and Outliers in Multilingual Language Models for Cross-Lingual Semantic Sentence Similarity. (arXiv:2306.00458v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00458","description":"<p>Previous work has shown that the representations output by contextual\nlanguage models are more anisotropic than static type embeddings, and typically\ndisplay outlier dimensions. This seems to be true for both monolingual and\nmultilingual models, although much less work has been done on the multilingual\ncontext. Why these outliers occur and how they affect the representations is\nstill an active area of research. We investigate outlier dimensions and their\nrelationship to anisotropy in multiple pre-trained multilingual language\nmodels. We focus on cross-lingual semantic similarity tasks, as these are\nnatural tasks for evaluating multilingual representations. Specifically, we\nexamine sentence representations. Sentence transformers which are fine-tuned on\nparallel resources (that are not always available) perform better on this task,\nand we show that their representations are more isotropic. However, we aim to\nimprove multilingual representations in general. We investigate how much of the\nperformance difference can be made up by only transforming the embedding space\nwithout fine-tuning, and visualise the resulting spaces. We test different\noperations: Removing individual outlier dimensions, cluster-based isotropy\nenhancement, and ZCA whitening. We publish our code for reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hammerl_K/0/1/0/all/0/1\">Katharina H&#xe4;mmerl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fastowski_A/0/1/0/all/0/1\">Alina Fastowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Libovicky_J/0/1/0/all/0/1\">Jind&#x159;ich Libovick&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. (arXiv:2306.00477v1 [cs.CL])","link":"http://arxiv.org/abs/2306.00477","description":"<p>Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs)\nhas emerged as a highly successful approach, with training only a small number\nof parameters without sacrificing performance and becoming the de-facto\nlearning paradigm with the increasing size of PLMs. However, existing PEFT\nmethods are not memory-efficient, because they still require caching most of\nthe intermediate activations for the gradient calculation, akin to fine-tuning.\nOne effective way to reduce the activation memory is to apply a reversible\nmodel, so the intermediate activations are not necessary to be cached and can\nbe recomputed. Nevertheless, modifying a PLM to its reversible variant with\nPEFT is not straightforward, since the reversible model has a distinct\narchitecture from the currently released PLMs. In this paper, we first\ninvestigate what is a key factor for the success of existing PEFT methods, and\nrealize that it's essential to preserve the PLM's starting point when\ninitializing a PEFT method. With this finding, we propose memory-efficient\nfine-tuning (MEFT) that inserts adapters into a PLM, preserving the PLM's\nstarting point and making it reversible without additional pre-training. We\nevaluate MEFT on the GLUE benchmark and five question-answering tasks with\nvarious backbones, BERT, RoBERTa, BART and OPT. MEFT significantly reduces the\nactivation memory up to 84% of full fine-tuning with a negligible amount of\ntrainable parameters. Moreover, MEFT achieves the same score on GLUE and a\ncomparable score on the question-answering tasks as full fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1\">Baohao Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shaomu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monz_C/0/1/0/all/0/1\">Christof Monz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parallel Neurosymbolic Integration with Concordia. (arXiv:2306.00480v1 [cs.AI])","link":"http://arxiv.org/abs/2306.00480","description":"<p>Parallel neurosymbolic architectures have been applied effectively in NLP by\ndistilling knowledge from a logic theory into a deep model.However, prior art\nfaces several limitations including supporting restricted forms of logic\ntheories and relying on the assumption of independence between the logic and\nthe deep network. We present Concordia, a framework overcoming the limitations\nof prior art. Concordia is agnostic both to the deep network and the logic\ntheory offering support for a wide range of probabilistic theories. Our\nframework can support supervised training of both components and unsupervised\ntraining of the neural component. Concordia has been successfully applied to\ntasks beyond NLP and data classification, improving the accuracy of\nstate-of-the-art on collective activity detection, entity linking and\nrecommendation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feldstein_J/0/1/0/all/0/1\">Jonathan Feldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurcius_M/0/1/0/all/0/1\">Modestas Jur&#x10d;ius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsamoura_E/0/1/0/all/0/1\">Efthymia Tsamoura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inspecting Spoken Language Understanding from Kids for Basic Math Learning at Home. (arXiv:2306.00482v1 [cs.CY])","link":"http://arxiv.org/abs/2306.00482","description":"<p>Enriching the quality of early childhood education with interactive math\nlearning at home systems, empowered by recent advances in conversational AI\ntechnologies, is slowly becoming a reality. With this motivation, we implement\na multimodal dialogue system to support play-based learning experiences at\nhome, guiding kids to master basic math concepts. This work explores Spoken\nLanguage Understanding (SLU) pipeline within a task-oriented dialogue system\ndeveloped for Kid Space, with cascading Automatic Speech Recognition (ASR) and\nNatural Language Understanding (NLU) components evaluated on our home\ndeployment data with kids going through gamified math learning activities. We\nvalidate the advantages of a multi-task architecture for NLU and experiment\nwith a diverse set of pretrained language representations for Intent\nRecognition and Entity Extraction tasks in the math learning domain. To\nrecognize kids' speech in realistic home environments, we investigate several\nASR systems, including the commercial Google Cloud and the latest open-source\nWhisper solutions with varying model sizes. We evaluate the SLU pipeline by\ntesting our best-performing NLU models on noisy ASR output to inspect the\nchallenges of understanding children for math learning in authentic homes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Okur_E/0/1/0/all/0/1\">Eda Okur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alba_R/0/1/0/all/0/1\">Roddy Fuentes Alba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahay_S/0/1/0/all/0/1\">Saurav Sahay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachman_L/0/1/0/all/0/1\">Lama Nachman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMARAGD: Learning SMatch for Accurate and Rapid Approximate Graph Distance. (arXiv:2203.13226v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.13226","description":"<p>The similarity of graph structures, such as Meaning Representations (MRs), is\noften assessed via structural matching algorithms, such as Smatch (Cai and\nKnight, 2013). However, Smatch involves a combinatorial problem that suffers\nfrom NP-completeness, making large-scale applications, e.g., graph clustering\nor search, infeasible. To alleviate this issue, we learn SMARAGD: Semantic\nMatch for Accurate and Rapid Approximate Graph Distance. We show the potential\nof neural networks to approximate Smatch scores, i) in linear time using a\nmachine translation framework to predict alignments, or ii) in constant time\nusing a Siamese CNN to directly predict Smatch scores. We show that the\napproximation error can be substantially reduced through data augmentation and\ngraph anonymization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Opitz_J/0/1/0/all/0/1\">Juri Opitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meier_P/0/1/0/all/0/1\">Philipp Meier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1\">Anette Frank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aerial Vision-and-Dialog Navigation. (arXiv:2205.12219v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12219","description":"<p>The ability to converse with humans and follow natural language commands is\ncrucial for intelligent unmanned aerial vehicles (a.k.a. drones). It can\nrelieve people's burden of holding a controller all the time, allow\nmultitasking, and make drone control more accessible for people with\ndisabilities or with their hands occupied. To this end, we introduce Aerial\nVision-and-Dialog Navigation (AVDN), to navigate a drone via natural language\nconversation. We build a drone simulator with a continuous photorealistic\nenvironment and collect a new AVDN dataset of over 3k recorded navigation\ntrajectories with asynchronous human-human dialogs between commanders and\nfollowers. The commander provides initial navigation instruction and further\nguidance by request, while the follower navigates the drone in the simulator\nand asks questions when needed. During data collection, followers' attention on\nthe drone's visual observation is also recorded. Based on the AVDN dataset, we\nstudy the tasks of aerial navigation from (full) dialog history and propose an\neffective Human Attention Aided Transformer model (HAA-Transformer), which\nlearns to predict both navigation waypoints and human attention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yue Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Winson Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tongzhou Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GENEVA: Benchmarking Generalizability for Event Argument Extraction with Hundreds of Event Types and Argument Roles. (arXiv:2205.12505v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12505","description":"<p>Recent works in Event Argument Extraction (EAE) have focused on improving\nmodel generalizability to cater to new events and domains. However, standard\nbenchmarking datasets like ACE and ERE cover less than 40 event types and 25\nentity-centric argument roles. Limited diversity and coverage hinder these\ndatasets from adequately evaluating the generalizability of EAE models. In this\npaper, we first contribute by creating a large and diverse EAE ontology. This\nontology is created by transforming FrameNet, a comprehensive semantic role\nlabeling (SRL) dataset for EAE, by exploiting the similarity between these two\ntasks. Then, exhaustive human expert annotations are collected to build the\nontology, concluding with 115 events and 220 argument roles, with a significant\nportion of roles not being entities. We utilize this ontology to further\nintroduce GENEVA, a diverse generalizability benchmarking dataset comprising\nfour test suites, aimed at evaluating models' ability to handle limited data\nand unseen event type generalization. We benchmark six EAE models from various\nfamilies. The results show that owing to non-entity argument roles, even the\nbest-performing model can only achieve 39% F1 score, indicating how GENEVA\nprovides new challenges for generalization in EAE. Overall, our large and\ndiverse EAE ontology can aid in creating more comprehensive future resources,\nwhile GENEVA is a challenging benchmarking dataset encouraging further research\nfor improving generalizability in EAE. The code and data can be found at\nhttps://github.com/PlusLabNLP/GENEVA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parekh_T/0/1/0/all/0/1\">Tanmay Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1\">I-Hung Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction. (arXiv:2207.14116v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.14116","description":"<p>We present Claim-Dissector: a novel latent variable model for fact-checking\nand analysis, which given a claim and a set of retrieved evidences jointly\nlearns to identify: (i) the relevant evidences to the given claim, (ii) the\nveracity of the claim. We propose to disentangle the per-evidence relevance\nprobability and its contribution to the final veracity probability in an\ninterpretable way -- the final veracity probability is proportional to a linear\nensemble of per-evidence relevance probabilities. In this way, the individual\ncontributions of evidences towards the final predicted probability can be\nidentified. In per-evidence relevance probability, our model can further\ndistinguish whether each relevant evidence is supporting (S) or refuting (R)\nthe claim. This allows to quantify how much the S/R probability contributes to\nthe final verdict or to detect disagreeing evidence.\n</p>\n<p>Despite its interpretable nature, our system achieves results competitive\nwith state-of-the-art on the FEVER dataset, as compared to typical two-stage\nsystem pipelines, while using significantly fewer parameters. It also sets new\nstate-of-the-art on FAVIQ and RealFC datasets. Furthermore, our analysis shows\nthat our model can learn fine-grained relevance cues while using coarse-grained\nsupervision, and we demonstrate it in 2 ways. (i) We show that our model can\nachieve competitive sentence recall while using only paragraph-level relevance\nsupervision. (ii) Traversing towards the finest granularity of relevance, we\nshow that our model is capable of identifying relevance at the token level. To\ndo this, we present a new benchmark TLR-FEVER focusing on token-level\ninterpretability -- humans annotate tokens in relevant evidences they\nconsidered essential when making their judgment. Then we measure how similar\nare these annotations to the tokens our model is focusing on.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fajcik_M/0/1/0/all/0/1\">Martin Fajcik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smrz_P/0/1/0/all/0/1\">Pavel Smrz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Large Language Models know what humans know?. (arXiv:2209.01515v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.01515","description":"<p>Humans can attribute beliefs to others. However, it is unknown to what extent\nthis ability results from an innate biological endowment or from experience\naccrued through child development, particularly exposure to language describing\nothers' mental states. We test the viability of the language exposure\nhypothesis by assessing whether models exposed to large quantities of human\nlanguage display sensitivity to the implied knowledge states of characters in\nwritten passages. In pre-registered analyses, we present a linguistic version\nof the False Belief Task to both human participants and a Large Language Model,\nGPT-3. Both are sensitive to others' beliefs, but while the language model\nsignificantly exceeds chance behavior, it does not perform as well as the\nhumans, nor does it explain the full extent of their behavior -- despite being\nexposed to more language than a human would in a lifetime. This suggests that\nwhile statistical learning from language exposure may in part explain how\nhumans develop the ability to reason about the mental states of others, other\nmechanisms are also responsible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trott_S/0/1/0/all/0/1\">Sean Trott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_C/0/1/0/all/0/1\">Cameron Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tyler Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michaelov_J/0/1/0/all/0/1\">James Michaelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1\">Benjamin Bergen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Best Prompts for Text-to-Image Models and How to Find Them. (arXiv:2209.11711v3 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2209.11711","description":"<p>Recent progress in generative models, especially in text-guided diffusion\nmodels, has enabled the production of aesthetically-pleasing imagery resembling\nthe works of professional human artists. However, one has to carefully compose\nthe textual description, called the prompt, and augment it with a set of\nclarifying keywords. Since aesthetics are challenging to evaluate\ncomputationally, human feedback is needed to determine the optimal prompt\nformulation and keyword combination. In this paper, we present a\nhuman-in-the-loop approach to learning the most useful combination of prompt\nkeywords using a genetic algorithm. We also show how such an approach can\nimprove the aesthetic appeal of images depicting the same descriptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pavlichenko_N/0/1/0/all/0/1\">Nikita Pavlichenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ustalov_D/0/1/0/all/0/1\">Dmitry Ustalov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SQuId: Measuring Speech Naturalness in Many Languages. (arXiv:2210.06324v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06324","description":"<p>Much of text-to-speech research relies on human evaluation, which incurs\nheavy costs and slows down the development process. The problem is particularly\nacute in heavily multilingual applications, where recruiting and polling judges\ncan take weeks. We introduce SQuId (Speech Quality Identification), a\nmultilingual naturalness prediction model trained on over a million ratings and\ntested in 65 locales-the largest effort of this type to date. The main insight\nis that training one model on many locales consistently outperforms mono-locale\nbaselines. We present our task, the model, and show that it outperforms a\ncompetitive baseline based on w2v-BERT and VoiceMOS by 50.0%. We then\ndemonstrate the effectiveness of cross-locale transfer during fine-tuning and\nhighlight its effect on zero-shot locales, i.e., locales for which there is no\nfine-tuning data. Through a series of analyses, we highlight the role of\nnon-linguistic effects such as sound artifacts in cross-locale transfer.\nFinally, we present the effect of our design decision, e.g., model size,\npre-training diversity, and language rebalancing with several ablation\nexperiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sellam_T/0/1/0/all/0/1\">Thibault Sellam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camp_J/0/1/0/all/0/1\">Joshua Camp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mackinnon_D/0/1/0/all/0/1\">Diana Mackinnon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_A/0/1/0/all/0/1\">Ankur P. Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riesa_J/0/1/0/all/0/1\">Jason Riesa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Creation of Named Entity Recognition Datasets by Querying Phrase Representations. (arXiv:2210.07586v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07586","description":"<p>Most weakly supervised named entity recognition (NER) models rely on\ndomain-specific dictionaries provided by experts. This approach is infeasible\nin many domains where dictionaries do not exist. While a phrase retrieval model\nwas used to construct pseudo-dictionaries with entities retrieved from\nWikipedia automatically in a recent study, these dictionaries often have\nlimited coverage because the retriever is likely to retrieve popular entities\nrather than rare ones. In this study, we present a novel framework, HighGEN,\nthat generates NER datasets with high-coverage pseudo-dictionaries.\nSpecifically, we create entity-rich dictionaries with a novel search method,\ncalled phrase embedding search, which encourages the retriever to search a\nspace densely populated with various entities. In addition, we use a new\nverification process based on the embedding distance between candidate entity\nmentions and entity types to reduce the false-positive noise in weak labels\ngenerated by high-coverage dictionaries. We demonstrate that HighGEN\noutperforms the previous best model by an average F1 score of 4.7 across five\nNER benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunjae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jaehyo Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models. (arXiv:2210.15458v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.15458","description":"<p>Decoding methods for large language models often trade-off between diversity\nof outputs and parallelism of computation. Methods such as beam search and\nGumbel top-k sampling can guarantee a different output for each element of the\nbeam, but are not easy to parallelize. Alternatively, methods such as\ntemperature sampling and its modifications (top-k sampling, nucleus sampling,\ntypical decoding, and others), are embarrassingly parallel, but have no\nguarantees about duplicate samples. We present a framework for sampling\naccording to an arithmetic code book implicitly defined by a large language\nmodel, compatible with common sampling variations, with provable beam diversity\nunder certain conditions, as well as being embarrassingly parallel and\nproviding unbiased and consistent expectations from the original model. We\ndemonstrate the effectiveness of our approach on WMT machine translation, more\nthan halving the standard deviation when estimating expected BLEU score reward,\nand closing the BLEU score gap between independent sampling and beam search by\nup to 63%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vilnis_L/0/1/0/all/0/1\">Luke Vilnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemlyanskiy_Y/0/1/0/all/0/1\">Yury Zemlyanskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_P/0/1/0/all/0/1\">Patrick Murray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passos_A/0/1/0/all/0/1\">Alexandre Passos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghai_S/0/1/0/all/0/1\">Sumit Sanghai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent Linguistic Structures in Neural Networks are Fragile. (arXiv:2210.17406v8 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.17406","description":"<p>Large Language Models (LLMs) have been reported to have strong performance on\nnatural language processing tasks. However, performance metrics such as\naccuracy do not measure the quality of the model in terms of its ability to\nrobustly represent complex linguistic structures. In this paper, focusing on\nthe ability of language models to represent syntax, we propose a framework to\nassess the consistency and robustness of linguistic representations. To this\nend, we introduce measures of robustness of neural network models that leverage\nrecent advances in extracting linguistic constructs from LLMs via probing\ntasks, i.e., simple tasks used to extract meaningful information about a single\nfacet of a language model, such as syntax reconstruction and root\nidentification. Empirically, we study the performance of four LLMs across six\ndifferent corpora on the proposed robustness measures by analysing their\nperformance and robustness with respect to syntax-preserving perturbations. We\nprovide evidence that context-free representation (e.g., GloVe) are in some\ncases competitive with context-dependent representations from modern LLMs\n(e.g., BERT), yet equally brittle to syntax-preserving perturbations. Our key\nobservation is that emergent syntactic representations in neural networks are\nbrittle. We make the code, trained models and logs available to the community\nas a contribution to the debate about the capabilities of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malfa_E/0/1/0/all/0/1\">Emanuele La Malfa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wicker_M/0/1/0/all/0/1\">Matthew Wicker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwiatkowska_M/0/1/0/all/0/1\">Marta Kwiatkowska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speaking Multiple Languages Affects the Moral Bias of Language Models. (arXiv:2211.07733v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07733","description":"<p>Pre-trained multilingual language models (PMLMs) are commonly used when\ndealing with data from multiple languages and cross-lingual transfer. However,\nPMLMs are trained on varying amounts of data for each language. In practice\nthis means their performance is often much better on English than many other\nlanguages. We explore to what extent this also applies to moral norms. Do the\nmodels capture moral norms from English and impose them on other languages? Do\nthe models exhibit random and thus potentially harmful beliefs in certain\nlanguages? Both these issues could negatively impact cross-lingual transfer and\npotentially lead to harmful outcomes. In this paper, we (1) apply the\nMoralDirection framework to multilingual models, comparing results in German,\nCzech, Arabic, Chinese, and English, (2) analyse model behaviour on filtered\nparallel subtitles corpora, and (3) apply the models to a Moral Foundations\nQuestionnaire, comparing with human responses from different countries. Our\nexperiments demonstrate that, indeed, PMLMs encode differing moral biases, but\nthese do not necessarily correspond to cultural differences or commonalities in\nhuman opinions. We release our code and models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hammerl_K/0/1/0/all/0/1\">Katharina H&#xe4;mmerl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deiseroth_B/0/1/0/all/0/1\">Bj&#xf6;rn Deiseroth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1\">Patrick Schramowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Libovicky_J/0/1/0/all/0/1\">Jind&#x159;ich Libovick&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rothkopf_C/0/1/0/all/0/1\">Constantin A. Rothkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MT Metrics Correlate with Human Ratings of Simultaneous Speech Translation. (arXiv:2211.08633v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08633","description":"<p>There have been several meta-evaluation studies on the correlation between\nhuman ratings and offline machine translation (MT) evaluation metrics such as\nBLEU, chrF2, BertScore and COMET. These metrics have been used to evaluate\nsimultaneous speech translation (SST) but their correlations with human ratings\nof SST, which has been recently collected as Continuous Ratings (CR), are\nunclear. In this paper, we leverage the evaluations of candidate systems\nsubmitted to the English-German SST task at IWSLT 2022 and conduct an extensive\ncorrelation analysis of CR and the aforementioned metrics. Our study reveals\nthat the offline metrics are well correlated with CR and can be reliably used\nfor evaluating machine translation in simultaneous mode, with some limitations\non the test set size. We conclude that given the current quality levels of SST,\nthese metrics can be used as proxies for CR, alleviating the need for large\nscale human evaluation. Additionally, we observe that correlations of the\nmetrics with translation as a reference is significantly higher than with\nsimultaneous interpreting, and thus we recommend the former for reliable\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Machacek_D/0/1/0/all/0/1\">Dominik Mach&#xe1;&#x10d;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1\">Raj Dabre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reward Gaming in Conditional Text Generation. (arXiv:2211.08714v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08714","description":"<p>To align conditional text generation model outputs with desired behaviors,\nthere has been an increasing focus on training the model using reinforcement\nlearning (RL) with reward functions learned from human annotations. Under this\nframework, we identify three common cases where high rewards are incorrectly\nassigned to undesirable patterns: noise-induced spurious correlation, naturally\noccurring spurious correlation, and covariate shift. We show that even though\nlearned metrics achieve high performance on the distribution of the data used\nto train the reward function, the undesirable patterns may be amplified during\nRL training of the text generation model. While there has been discussion about\nreward gaming in the RL or safety community, in this discussion piece, we would\nlike to highlight reward gaming in the natural language generation (NLG)\ncommunity using concrete conditional text generation examples and discuss\npotential fixes and areas for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1\">Richard Yuanzhe Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_V/0/1/0/all/0/1\">Vishakh Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellam_T/0/1/0/all/0/1\">Thibault Sellam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_A/0/1/0/all/0/1\">Ankur P. Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Super-CLEVR: A Virtual Benchmark to Diagnose Domain Robustness in Visual Reasoning. (arXiv:2212.00259v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2212.00259","description":"<p>Visual Question Answering (VQA) models often perform poorly on\nout-of-distribution data and struggle on domain generalization. Due to the\nmulti-modal nature of this task, multiple factors of variation are intertwined,\nmaking generalization difficult to analyze. This motivates us to introduce a\nvirtual benchmark, Super-CLEVR, where different factors in VQA domain shifts\ncan be isolated in order that their effects can be studied independently. Four\nfactors are considered: visual complexity, question redundancy, concept\ndistribution and concept compositionality. With controllably generated data,\nSuper-CLEVR enables us to test VQA methods in situations where the test data\ndiffers from the training data along each of these axes. We study four existing\nmethods, including two neural symbolic methods NSCL and NSVQA, and two\nnon-symbolic methods FiLM and mDETR; and our proposed method, probabilistic\nNSVQA (P-NSVQA), which extends NSVQA with uncertainty reasoning. P-NSVQA\noutperforms other methods on three of the four domain shift factors. Our\nresults suggest that disentangling reasoning and perception, combined with\nprobabilistic uncertainty, form a strong VQA model that is more robust to\ndomain shifts. The dataset and code are released at\nhttps://github.com/Lizw14/Super-CLEVR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuowan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingrui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stengel_Eskin_E/0/1/0/all/0/1\">Elias Stengel-Eskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1\">Adam Kortylewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wufei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teaching Small Language Models to Reason. (arXiv:2212.08410v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08410","description":"<p>Chain of thought prompting successfully improves the reasoning capabilities\nof large language models, achieving state of the art results on a range of\ndatasets. However, these reasoning capabilities only appear to emerge in models\nwith a size of over 100 billion parameters. In this paper, we explore the\ntransfer of such reasoning capabilities to models with less than 100 billion\nparameters via knowledge distillation. Specifically, we finetune a student\nmodel on the chain of thought outputs generated by a larger teacher model. Our\nexperiments show that the proposed method improves task performance across\narithmetic, commonsense and symbolic reasoning datasets. For example, the\naccuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned on\nPaLM-540B generated chains of thought.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Magister_L/0/1/0/all/0/1\">Lucie Charlotte Magister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallinson_J/0/1/0/all/0/1\">Jonathan Mallinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adamek_J/0/1/0/all/0/1\">Jakub Adamek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malmi_E/0/1/0/all/0/1\">Eric Malmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Severyn_A/0/1/0/all/0/1\">Aliaksei Severyn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09597","description":"<p>Reasoning, as an essential ability for complex problem-solving, can provide\nback-end support for various real-world applications, such as medical\ndiagnosis, negotiation, etc. This paper provides a comprehensive survey of\ncutting-edge research on reasoning with language model prompting. We introduce\nresearch works with comparisons and summaries and provide systematic resources\nto help beginners. We also discuss the potential reasons for emerging such\nreasoning abilities and highlight future research directions. Resources are\navailable at https://github.com/zjunlp/Prompt4ReasoningPapers (updated\nperiodically).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1\">Shuofei Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1\">Yixin Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters. (arXiv:2212.10001v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10001","description":"<p>Chain-of-Thought (CoT) prompting can dramatically improve the multi-step\nreasoning abilities of large language models (LLMs). CoT explicitly encourages\nthe LLM to generate intermediate rationales for solving a problem, by providing\na series of reasoning steps in the demonstrations. Despite its success, there\nis still little understanding of what makes CoT prompting effective and which\naspects of the demonstrated reasoning steps contribute to its performance. In\nthis paper, we show that CoT reasoning is possible even with invalid\ndemonstrations - prompting with invalid reasoning steps can achieve over 80-90%\nof the performance obtained using CoT under various metrics, while still\ngenerating coherent lines of reasoning during inference. Further experiments\nshow that other aspects of the rationales, such as being relevant to the query\nand correctly ordering the reasoning steps, are much more important for\neffective CoT reasoning. Overall, these findings both deepen our understanding\nof CoT prompting, and open up new questions regarding LLMs' capability to learn\nto reason in context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boshi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiaming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">You Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DISCO: Distilling Phrasal Counterfactuals with Large Language Models. (arXiv:2212.10534v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10534","description":"<p>Models trained with counterfactually augmented data learn representations of\nthe causal structure of tasks, enabling robust generalization. However,\nhigh-quality counterfactual data is scarce for most tasks and not easily\ngenerated at scale. When crowdsourced, such data is typically limited in scale\nand diversity; when generated using supervised methods, it is computationally\nexpensive to extend to new counterfactual dimensions. In this work, we\nintroduce DISCO (DIStilled COunterfactual Data), a new method for automatically\ngenerating high quality counterfactual data at scale. DISCO engineers prompts\nto generate phrasal perturbations with a large general language model. Then, a\ntask-specific teacher model filters these generations to distill high-quality\ncounterfactual data. While task-agnostic, we apply our pipeline to the task of\nnatural language inference (NLI) and find that on challenging evaluations such\nas the NLI stress test, comparatively smaller student models trained with DISCO\ngenerated counterfactuals are more robust (6% absolute) and generalize better\nacross distributions (2%) compared to models trained without data augmentation.\nFurthermore, DISCO augmented models are 10% more consistent between\ncounterfactual pairs on three evaluation sets, demonstrating that DISCO\naugmentation enables models to more reliably learn causal representations. Our\nrepository is available at: https://github.com/eric11eca/disco\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiyue Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richardson_K/0/1/0/all/0/1\">Kyle Richardson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on In-context Learning. (arXiv:2301.00234v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.00234","description":"<p>With the increasing ability of large language models (LLMs), in-context\nlearning (ICL) has become a new paradigm for natural language processing (NLP),\nwhere LLMs make predictions only based on contexts augmented with a few\nexamples. It has been a new trend to explore ICL to evaluate and extrapolate\nthe ability of LLMs. In this paper, we aim to survey and summarize the progress\nand challenges of ICL. We first present a formal definition of ICL and clarify\nits correlation to related studies. Then, we organize and discuss advanced\ntechniques, including training strategies, demonstration designing strategies,\nas well as related analysis. Finally, we discuss the challenges of ICL and\nprovide potential directions for further research. We hope that our work can\nencourage more research on uncovering how ICL works and improving ICL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qingxiu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Damai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Ce Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Communication Drives the Emergence of Language Universals in Neural Agents: Evidence from the Word-order/Case-marking Trade-off. (arXiv:2301.13083v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.13083","description":"<p>Artificial learners often behave differently from human learners in the\ncontext of neural agent-based simulations of language emergence and change. A\ncommon explanation is the lack of appropriate cognitive biases in these\nlearners. However, it has also been proposed that more naturalistic settings of\nlanguage learning and use could lead to more human-like results. We investigate\nthis latter account focusing on the word-order/case-marking trade-off, a widely\nattested language universal that has proven particularly hard to simulate. We\npropose a new Neural-agent Language Learning and Communication framework\n(NeLLCom) where pairs of speaking and listening agents first learn a miniature\nlanguage via supervised learning, and then optimize it for communication via\nreinforcement learning. Following closely the setup of earlier human\nexperiments, we succeed in replicating the trade-off with the new framework\nwithout hard-coding specific biases in the agents. We see this as an essential\nstep towards the investigation of language universals with neural learners.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lian_Y/0/1/0/all/0/1\">Yuchen Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisazza_A/0/1/0/all/0/1\">Arianna Bisazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verhoef_T/0/1/0/all/0/1\">Tessa Verhoef</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding Language Models to Images for Multimodal Inputs and Outputs. (arXiv:2301.13823v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.13823","description":"<p>We propose an efficient method to ground pretrained text-only language models\nto the visual domain, enabling them to process arbitrarily interleaved\nimage-and-text data, and generate text interleaved with retrieved images. Our\nmethod leverages the abilities of language models learnt from large scale\ntext-only pretraining, such as in-context learning and free-form text\ngeneration. We keep the language model frozen, and finetune input and output\nlinear layers to enable cross-modality interactions. This allows our model to\nprocess arbitrarily interleaved image-and-text inputs, and generate free-form\ntext interleaved with retrieved images. We achieve strong zero-shot performance\non grounded tasks such as contextual image retrieval and multimodal dialogue,\nand showcase compelling interactive abilities. Our approach works with any\noff-the-shelf language model and paves the way towards an effective, general\nsolution for leveraging pretrained language models in visually grounded\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koh_J/0/1/0/all/0/1\">Jing Yu Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1\">Daniel Fried</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery. (arXiv:2302.03668v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.03668","description":"<p>The strength of modern generative models lies in their ability to be\ncontrolled through text-based prompts. Typical \"hard\" prompts are made from\ninterpretable words and tokens, and must be hand-crafted by humans. There are\nalso \"soft\" prompts, which consist of continuous feature vectors. These can be\ndiscovered using powerful optimization methods, but they cannot be easily\ninterpreted, re-used across models, or plugged into a text-based interface.\n</p>\n<p>We describe an approach to robustly optimize hard text prompts through\nefficient gradient-based optimization. Our approach automatically generates\nhard text-based prompts for both text-to-image and text-to-text applications.\nIn the text-to-image setting, the method creates hard prompts for diffusion\nmodels, allowing API users to easily generate, discover, and mix and match\nimage concepts without prior knowledge on how to prompt the model. In the\ntext-to-text setting, we show that hard prompts can be automatically discovered\nthat are effective in tuning LMs for classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yuxin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1\">Neel Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchenbauer_J/0/1/0/all/0/1\">John Kirchenbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1\">Jonas Geiping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bag of Tricks for Training Data Extraction from Language Models. (arXiv:2302.04460v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.04460","description":"<p>With the advance of language models, privacy protection is receiving more\nattention. Training data extraction is therefore of great importance, as it can\nserve as a potential tool to assess privacy leakage. However, due to the\ndifficulty of this task, most of the existing methods are proof-of-concept and\nstill not effective enough. In this paper, we investigate and benchmark tricks\nfor improving training data extraction using a publicly available dataset.\nBecause most existing extraction methods use a pipeline of\ngenerating-then-ranking, i.e., generating text candidates as potential training\ndata and then ranking them based on specific criteria, our research focuses on\nthe tricks for both text generation (e.g., sampling strategy) and text ranking\n(e.g., token-level criteria). The experimental results show that several\npreviously overlooked tricks can be crucial to the success of training data\nextraction. Based on the GPT-Neo 1.3B evaluation results, our proposed tricks\noutperform the baseline by a large margin in most cases, providing a much\nstronger baseline for future research. The code is available at\nhttps://github.com/weichen-yu/LM-Extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Weichen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1\">Chao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1\">Bingyi Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Min Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Level Generation Through Large Language Models. (arXiv:2302.05817v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2302.05817","description":"<p>Large Language Models (LLMs) are powerful tools, capable of leveraging their\ntraining on natural language to write stories, generate code, and answer\nquestions. But can they generate functional video game levels? Game levels,\nwith their complex functional constraints and spatial relationships in more\nthan one dimension, are very different from the kinds of data an LLM typically\nsees during training. Datasets of game levels are also hard to come by,\npotentially taxing the abilities of these data-hungry models. We investigate\nthe use of LLMs to generate levels for the game Sokoban, finding that LLMs are\nindeed capable of doing so, and that their performance scales dramatically with\ndataset size. We also perform preliminary experiments on controlling LLM level\ngenerators and discuss promising areas for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Todd_G/0/1/0/all/0/1\">Graham Todd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Earle_S/0/1/0/all/0/1\">Sam Earle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasir_M/0/1/0/all/0/1\">Muhammad Umair Nasir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_M/0/1/0/all/0/1\">Michael Cerny Green</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1\">Julian Togelius</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProsAudit, a prosodic benchmark for self-supervised speech models. (arXiv:2302.12057v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.12057","description":"<p>We present ProsAudit, a benchmark in English to assess structural prosodic\nknowledge in self-supervised learning (SSL) speech models. It consists of two\nsubtasks, their corresponding metrics, and an evaluation dataset. In the\nprotosyntax task, the model must correctly identify strong versus weak prosodic\nboundaries. In the lexical task, the model needs to correctly distinguish\nbetween pauses inserted between words and within words. We also provide human\nevaluation scores on this benchmark. We evaluated a series of SSL models and\nfound that they were all able to perform above chance on both tasks, even when\nevaluated on an unseen language. However, non-native models performed\nsignificantly worse than native ones on the lexical task, highlighting the\nimportance of lexical knowledge in this task. We also found a clear effect of\nsize with models trained on more data performing better in the two subtasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seyssel_M/0/1/0/all/0/1\">Maureen de Seyssel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavechin_M/0/1/0/all/0/1\">Marvin Lavechin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titeux_H/0/1/0/all/0/1\">Hadrien Titeux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_A/0/1/0/all/0/1\">Arthur Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Virlet_G/0/1/0/all/0/1\">Gwendal Virlet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revilla_A/0/1/0/all/0/1\">Andrea Santos Revilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wisniewski_G/0/1/0/all/0/1\">Guillaume Wisniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludusan_B/0/1/0/all/0/1\">Bogdan Ludusan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Let's have a chat! A Conversation with ChatGPT: Technology, Applications, and Limitations. (arXiv:2302.13817v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.13817","description":"<p>The emergence of an AI-powered chatbot that can generate human-like sentences\nand write coherent essays has caught the world's attention. This paper\ndiscusses the historical overview of chatbots and the technology behind Chat\nGenerative Pre-trained Transformer, better known as ChatGPT. Moreover,\npotential applications of ChatGPT in various domains, including healthcare,\neducation, and research, are highlighted. Despite promising results, there are\nseveral privacy and ethical concerns surrounding ChatGPT. In addition, we\nhighlight some of the important limitations of the current version of ChatGPT.\nWe also ask ChatGPT to provide its point of view and present its responses to\nseveral questions we attempt to answer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1\">Sakib Shahriar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayawi_K/0/1/0/all/0/1\">Kadhim Hayawi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Speech Recognition for Language-Guided Embodied Agents. (arXiv:2302.14030v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.14030","description":"<p>Benchmarks for language-guided embodied agents typically assume text-based\ninstructions, but deployed agents will encounter spoken instructions. While\nAutomatic Speech Recognition (ASR) models can bridge the input gap, erroneous\nASR transcripts can hurt the agents' ability to complete tasks. In this work,\nwe propose training a multimodal ASR model to reduce errors in transcribing\nspoken instructions by considering the accompanying visual context. We train\nour model on a dataset of spoken instructions, synthesized from the ALFRED task\ncompletion dataset, where we simulate acoustic noise by systematically masking\nspoken words. We find that utilizing visual observations facilitates masked\nword recovery, with multimodal ASR models recovering up to 30% more masked\nwords than unimodal baselines. We also find that a text-trained embodied agent\nsuccessfully completes tasks more often by following transcribed instructions\nfrom multimodal ASR models. github.com/Cylumn/embodied-multimodal-asr\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1\">Allen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoyuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monga_A/0/1/0/all/0/1\">Aarav Monga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Seoho Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_T/0/1/0/all/0/1\">Tejas Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Are State-of-the-Art Evaluators of Translation Quality. (arXiv:2302.14520v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.14520","description":"<p>We describe GEMBA, a GPT-based metric for assessment of translation quality,\nwhich works both with a reference translation and without. In our evaluation,\nwe focus on zero-shot prompting, comparing four prompt variants in two modes,\nbased on the availability of the reference. We investigate nine versions of GPT\nmodels, including ChatGPT and GPT-4. We show that our method for translation\nquality assessment only works with GPT~3.5 and larger models. Comparing to\nresults from WMT22's Metrics shared task, our method achieves state-of-the-art\naccuracy in both modes when compared to MQM-based human labels. Our results are\nvalid on the system level for all three WMT22 Metrics shared task language\npairs, namely English into German, English into Russian, and Chinese into\nEnglish. This provides a first glimpse into the usefulness of pre-trained,\ngenerative large language models for quality assessment of translations. We\npublicly release all our code and prompt templates used for the experiments\ndescribed in this work, as well as all corresponding scoring results, to allow\nfor external validation and reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kocmi_T/0/1/0/all/0/1\">Tom Kocmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Federmann_C/0/1/0/all/0/1\">Christian Federmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Almanac: Retrieval-Augmented Language Models for Clinical Medicine. (arXiv:2303.01229v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.01229","description":"<p>Large-language models have recently demonstrated impressive zero-shot\ncapabilities in a variety of natural language tasks such as summarization,\ndialogue generation, and question-answering. Despite many promising\napplications in clinical medicine, adoption of these models in real-world\nsettings has been largely limited by their tendency to generate incorrect and\nsometimes even toxic statements. In this study, we develop Almanac, a large\nlanguage model framework augmented with retrieval capabilities for medical\nguideline and treatment recommendations. Performance on a novel dataset of\nclinical scenarios (n = 130) evaluated by a panel of 5 board-certified and\nresident physicians demonstrates significant increases in factuality (mean of\n18% at p-value &lt; 0.05) across all specialties, with improvements in\ncompleteness and safety. Our results demonstrate the potential for large\nlanguage models to be effective tools in the clinical decision-making process,\nwhile also emphasizing the importance of careful testing and deployment to\nmitigate their shortcomings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zakka_C/0/1/0/all/0/1\">Cyril Zakka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaurasia_A/0/1/0/all/0/1\">Akash Chaurasia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shad_R/0/1/0/all/0/1\">Rohan Shad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalal_A/0/1/0/all/0/1\">Alex R. Dalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jennifer L. Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moor_M/0/1/0/all/0/1\">Michael Moor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexander_K/0/1/0/all/0/1\">Kevin Alexander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashley_E/0/1/0/all/0/1\">Euan Ashley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_J/0/1/0/all/0/1\">Jack Boyd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_K/0/1/0/all/0/1\">Kathleen Boyd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirsch_K/0/1/0/all/0/1\">Karen Hirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curt Langlotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nelson_J/0/1/0/all/0/1\">Joanna Nelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hiesinger_W/0/1/0/all/0/1\">William Hiesinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query-Utterance Attention with Joint modeling for Query-Focused Meeting Summarization. (arXiv:2303.04487v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.04487","description":"<p>Query-focused meeting summarization (QFMS) aims to generate summaries from\nmeeting transcripts in response to a given query. Previous works typically\nconcatenate the query with meeting transcripts and implicitly model the query\nrelevance only at the token level with attention mechanism. However, due to the\ndilution of key query-relevant information caused by long meeting transcripts,\nthe original transformer-based model is insufficient to highlight the key parts\nrelated to the query. In this paper, we propose a query-aware framework with\njoint modeling token and utterance based on Query-Utterance Attention. It\ncalculates the utterance-level relevance to the query with a dense retrieval\nmodule. Then both token-level query relevance and utterance-level query\nrelevance are combined and incorporated into the generation process with\nattention mechanism explicitly. We show that the query relevance of different\ngranularities contributes to generating a summary more related to the query.\nExperimental results on the QMSum dataset show that the proposed model achieves\nnew state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingxian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_B/0/1/0/all/0/1\">Bin Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bo Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yajing Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent and Predictable Memorization in Large Language Models. (arXiv:2304.11158v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.11158","description":"<p>Memorization, or the tendency of large language models (LLMs) to output\nentire sequences from their training data verbatim, is a key concern for safely\ndeploying language models. In particular, it is vital to minimize a model's\nmemorization of sensitive datapoints such as those containing personal\nidentifiable information (PII). The prevalence of such undesirable memorization\ncan pose issues for model trainers, and may even require discarding an\notherwise functional model. We therefore seek to predict which sequences will\nbe memorized before a large model's full train-time by extrapolating the\nmemorization behavior of lower-compute trial runs. We measure memorization of\nthe Pythia model suite and plot scaling laws for forecasting memorization,\nallowing us to provide equi-compute recommendations to maximize the reliability\n(recall) of such predictions. We additionally provide further novel discoveries\non the distribution of memorization scores across models and data. We release\nall code and data necessary to reproduce the results in this paper at\nhttps://github.com/EleutherAI/pythia\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prashanth_U/0/1/0/all/0/1\">USVSN Sai Prashanth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutawika_L/0/1/0/all/0/1\">Lintang Sutawika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoelkopf_H/0/1/0/all/0/1\">Hailey Schoelkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anthony_Q/0/1/0/all/0/1\">Quentin Anthony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purohit_S/0/1/0/all/0/1\">Shivanshu Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1\">Edward Raff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Challenges of Deploying BERT-based NLP Models in Resource-Constrained Embedded Devices. (arXiv:2304.11520v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.11520","description":"<p>BERT-based neural architectures have established themselves as popular\nstate-of-the-art baselines for many downstream NLP tasks. However, these\narchitectures are data-hungry and consume a lot of memory and energy, often\nhindering their deployment in many real-time, resource-constrained\napplications. Existing lighter versions of BERT (eg. DistilBERT and TinyBERT)\noften cannot perform well on complex NLP tasks. More importantly, from a\ndesigner's perspective, it is unclear what is the \"right\" BERT-based\narchitecture to use for a given NLP task that can strike the optimal trade-off\nbetween the resources available and the minimum accuracy desired by the end\nuser. System engineers have to spend a lot of time conducting trial-and-error\nexperiments to find a suitable answer to this question. This paper presents an\nexploratory study of BERT-based models under different resource constraints and\naccuracy budgets to derive empirical observations about this resource/accuracy\ntrade-offs. Our findings can help designers to make informed choices among\nalternative BERT-based architectures for embedded systems, thus saving\nsignificant development time and effort.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Souvika Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babar_M/0/1/0/all/0/1\">Mohammad Fakhruddin Babar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_M/0/1/0/all/0/1\">Md Mahadi Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Monowar Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santu_S/0/1/0/all/0/1\">Shubhra Kanti Karmaker Santu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation. (arXiv:2305.00955v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.00955","description":"<p>Many recent advances in natural language generation have been fueled by\ntraining large language models on internet-scale data. However, this paradigm\ncan lead to models that generate toxic, inaccurate, and unhelpful content, and\nautomatic evaluation metrics often fail to identify these behaviors. As models\nbecome more capable, human feedback is an invaluable signal for evaluating and\nimproving models. This survey aims to provide an overview of the recent\nresearch that has leveraged human feedback to improve natural language\ngeneration. First, we introduce an encompassing formalization of feedback, and\nidentify and organize existing research into a taxonomy following this\nformalization. Next, we discuss how feedback can be described by its format and\nobjective, and cover the two approaches proposed to use feedback (either for\ntraining or decoding): directly using the feedback or training feedback models.\nWe also discuss existing datasets for human-feedback data collection, and\nconcerns surrounding feedback collection. Finally, we provide an overview of\nthe nascent field of AI feedback, which exploits large language models to make\njudgments based on a set of principles and minimize the need for human\nintervention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1\">Patrick Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1\">Emmy Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farinhas_A/0/1/0/all/0/1\">Ant&#xf3;nio Farinhas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_P/0/1/0/all/0/1\">Pedro Henrique Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertsch_A/0/1/0/all/0/1\">Amanda Bertsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Souza_J/0/1/0/all/0/1\">Jos&#xe9; G. C. de Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01555","description":"<p>Scaling language models have revolutionized widespread NLP tasks, yet little\ncomprehensively explored few-shot relation extraction with large language\nmodels. In this paper, we investigate principal methodologies, in-context\nlearning and data generation, for few-shot relation extraction via GPT-3.5\nthrough exhaustive experiments. To enhance few-shot performance, we further\npropose task-related instructions and schema-constrained data generation. We\nobserve that in-context learning can achieve performance on par with previous\nprompt learning approaches, and data generation with the large language model\ncan boost previous solutions to obtain new state-of-the-art few-shot results on\nfour widely-studied relation extraction datasets. We hope our work can inspire\nfuture research for the capabilities of large language models in few-shot\nrelation extraction. Code is available in\nhttps://github.com/zjunlp/DeepKE/tree/main/example/llm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuqi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT as a Text Simplification Tool to Remove Bias. (arXiv:2305.06166v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.06166","description":"<p>The presence of specific linguistic signals particular to a certain sub-group\nof people can be picked up by language models during training. If the model\nbegins to associate specific language with a distinct group, any decisions made\nbased upon this language would hold a strong correlation to a decision based\nupon their protected characteristic, leading to possible discrimination. We\nexplore a potential technique for bias mitigation in the form of simplification\nof text. The driving force of this idea is that simplifying text should\nstandardise language between different sub-groups to one way of speaking while\nkeeping the same meaning. The experiment shows promising results as the\nclassifier accuracy for predicting the sensitive attribute drops by up to 17%\nfor the simplified data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barker_C/0/1/0/all/0/1\">Charmaine Barker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazakov_D/0/1/0/all/0/1\">Dimitar Kazakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Methods for Extracting Metaphorical Names of Flowers and Plants. (arXiv:2305.10833v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10833","description":"<p>The domain of Botany is rich with metaphorical terms. Those terms play an\nimportant role in the description and identification of flowers and plants.\nHowever, the identification of such terms in discourse is an arduous task. This\nleads in some cases to committing errors during translation processes and\nlexicographic tasks. The process is even more challenging when it comes to\nmachine translation, both in the cases of single-word terms and multi-word\nterms. One of the recent concerns of Natural Language Processing (NLP)\napplications and Machine Translation (MT) technologies is the automatic\nidentification of metaphor-based words in discourse through Deep Learning (DL).\nIn this study, we seek to fill this gap through the use of thirteen popular\ntransformer based models, as well as ChatGPT, and we show that discriminative\nmodels perform better than GPT-3.5 model with our best performer reporting\n92.2349% F1 score in metaphoric flower and plant names identification task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haddad_A/0/1/0/all/0/1\">Amal Haddad Haddad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Premasiri_D/0/1/0/all/0/1\">Damith Premasiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitkov_R/0/1/0/all/0/1\">Ruslan Mitkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Off-Target Problem of Zero-Shot Multilingual Neural Machine Translation. (arXiv:2305.10930v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10930","description":"<p>While multilingual neural machine translation has achieved great success, it\nsuffers from the off-target issue, where the translation is in the wrong\nlanguage. This problem is more pronounced on zero-shot translation tasks. In\nthis work, we find that failing in encoding discriminative target language\nsignal will lead to off-target and a closer lexical distance (i.e.,\nKL-divergence) between two languages' vocabularies is related with a higher\noff-target rate. We also find that solely isolating the vocab of different\nlanguages in the decoder can alleviate the problem. Motivated by the findings,\nwe propose Language Aware Vocabulary Sharing (LAVS), a simple and effective\nalgorithm to construct the multilingual vocabulary, that greatly alleviates the\noff-target problem of the translation model by increasing the KL-divergence\nbetween languages. We conduct experiments on a multilingual machine translation\nbenchmark in 11 languages. Experiments show that the off-target rate for 90\ntranslation tasks is reduced from 29\\% to 8\\%, while the overall BLEU score is\nimproved by an average of 1.9 points without extra training cost or sacrificing\nthe supervised directions' performance. We release the code at\n\\href{https://github.com/chenllliang/Off-Target-MNMT}{https://github.com/chenllliang/Off-Target-MNMT}\nfor reproduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Pilot Study on Dialogue-Level Dependency Parsing for Chinese. (arXiv:2305.12441v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.12441","description":"<p>Dialogue-level dependency parsing has received insufficient attention,\nespecially for Chinese. To this end, we draw on ideas from syntactic dependency\nand rhetorical structure theory (RST), developing a high-quality\nhuman-annotated corpus, which contains 850 dialogues and 199,803 dependencies.\nConsidering that such tasks suffer from high annotation costs, we investigate\nzero-shot and few-shot scenarios. Based on an existing syntactic treebank, we\nadopt a signal-based method to transform seen syntactic dependencies into\nunseen ones between elementary discourse units (EDUs), where the signals are\ndetected by masked language modeling. Besides, we apply single-view and\nmulti-view data selection to access reliable pseudo-labeled instances.\nExperimental results show the effectiveness of these baselines. Moreover, we\ndiscuss several crucial points about our dataset and approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1\">Gongyao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation. (arXiv:2305.14330v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.14330","description":"<p>In the paradigm of AI-generated content (AIGC), there has been increasing\nattention in extending pre-trained text-to-image (T2I) models to text-to-video\n(T2V) generation. Despite their effectiveness, these frameworks face challenges\nin maintaining consistent narratives and handling rapid shifts in scene\ncomposition or object placement from a single user prompt. This paper\nintroduces a new framework, dubbed DirecT2V, which leverages instruction-tuned\nlarge language models (LLMs) to generate frame-by-frame descriptions from a\nsingle abstract user prompt. DirecT2V utilizes LLM directors to divide user\ninputs into separate prompts for each frame, enabling the inclusion of\ntime-varying content and facilitating consistent video generation. To maintain\ntemporal consistency and prevent object collapse, we propose a novel value\nmapping method and dual-softmax filtering. Extensive experimental results\nvalidate the effectiveness of the DirecT2V framework in producing visually\ncoherent and consistent videos from abstract user prompts, addressing the\nchallenges of zero-shot video generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Susung Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Junyoung Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Sunghwan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_H/0/1/0/all/0/1\">Heeseong Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BeamSearchQA: Large Language Models are Strong Zero-Shot QA Solver. (arXiv:2305.14766v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14766","description":"<p>Open-domain question answering is a crucial task that often requires\naccessing external information. Existing methods typically adopt a single-turn\nretrieve-then-read approach, where relevant documents are first retrieved, and\nquestions are then answered based on the retrieved information. However, there\nare cases where answering a question requires implicit knowledge that is not\ndirectly retrievable from the question itself. In this work, we propose a novel\nquestion-answering pipeline called BeamSearchQA. Our approach leverages large\nlanguage models to iteratively generate new questions about the original\nquestion, enabling an iterative reasoning process. By iteratively refining and\nexpanding the scope of the question, our method aims to capture and utilize\nhidden knowledge that may not be directly obtainable through retrieval. We\nevaluate our approach on the widely-used open-domain NQ and WebQ datasets. The\nexperimental results demonstrate that BeamSearchQA significantly outperforms\nother zero-shot baselines, indicating its effectiveness in tackling the\nchallenges of open-domain question answering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_A/0/1/0/all/0/1\">Anlei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jingwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linjun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_R/0/1/0/all/0/1\">Rangan Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LMs with a Voice: Spoken Language Modeling beyond Speech Tokens. (arXiv:2305.15255v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.15255","description":"<p>We present SPECTRON, a novel approach to adapting pre-trained language models\n(LMs) to perform speech continuation. By leveraging pre-trained speech\nencoders, our model generates both text and speech outputs with the entire\nsystem being trained end-to-end operating directly on spectrograms. Training\nthe entire model in the spectrogram domain simplifies our speech continuation\nsystem versus existing cascade methods which use discrete speech\nrepresentations. We further show our method surpasses existing spoken language\nmodels both in semantic content and speaker preservation while also benefiting\nfrom the knowledge transferred from pre-existing models. Audio samples can be\nfound in our website https://michelleramanovich.github.io/spectron/spectron\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nachmani_E/0/1/0/all/0/1\">Eliya Nachmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levkovitch_A/0/1/0/all/0/1\">Alon Levkovitch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salazar_J/0/1/0/all/0/1\">Julian Salazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asawaroengchai_C/0/1/0/all/0/1\">Chulayuth Asawaroengchai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mariooryad_S/0/1/0/all/0/1\">Soroosh Mariooryad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skerry_Ryan_R/0/1/0/all/0/1\">RJ Skerry-Ryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1\">Michelle Tadmor Ramanovich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective. (arXiv:2305.15408v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.15408","description":"<p>Recent studies have discovered that Chain-of-Thought prompting (CoT) can\ndramatically improve the performance of Large Language Models (LLMs),\nparticularly when dealing with complex tasks involving mathematics or\nreasoning. Despite the enormous empirical success, the underlying mechanisms\nbehind CoT and how it unlocks the potential of LLMs remain elusive. In this\npaper, we take a first step towards theoretically answering these questions.\nSpecifically, we examine the expressivity of LLMs with CoT in solving\nfundamental mathematical and decision-making problems. We start by giving an\nimpossibility result showing that bounded-depth Transformers are unable to\ndirectly produce correct answers for basic arithmetic/equation tasks unless the\nmodel size grows super-polynomially with respect to the input length. In\ncontrast, we then prove by construction that autoregressive Transformers of\nconstant size suffice to solve both tasks by generating CoT derivations using a\ncommonly-used math language format. Moreover, we show LLMs with CoT are capable\nof solving a general class of decision-making problems known as Dynamic\nProgramming, thus justifying its power in tackling complex real-world tasks.\nFinally, extensive experiments on four tasks show that, while Transformers\nalways fail to predict the answers directly, they can consistently learn to\ngenerate correct solutions step-by-step given sufficient CoT demonstrations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_G/0/1/0/all/0/1\">Guhao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bohang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuntian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Haotian Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LFTK: Handcrafted Features in Computational Linguistics. (arXiv:2305.15878v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.15878","description":"<p>Past research has identified a rich set of handcrafted linguistic features\nthat can potentially assist various tasks. However, their extensive number\nmakes it difficult to effectively select and utilize existing handcrafted\nfeatures. Coupled with the problem of inconsistent implementation across\nresearch works, there has been no categorization scheme or generally-accepted\nfeature names. This creates unwanted confusion. Also, most existing handcrafted\nfeature extraction libraries are not open-source or not actively maintained. As\na result, a researcher often has to build such an extraction system from the\nground up.\n</p>\n<p>We collect and categorize more than 220 popular handcrafted features grounded\non past literature. Then, we conduct a correlation analysis study on several\ntask-specific datasets and report the potential use cases of each feature.\nLastly, we devise a multilingual handcrafted linguistic feature extraction\nsystem in a systematically expandable manner. We open-source our system for\npublic access to a rich set of pre-implemented handcrafted features. Our system\nis coined LFTK and is the largest of its kind. Find it at\ngithub.com/brucewlee/lftk.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bruce W. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jason Hyung-Jong Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art. (arXiv:2305.16259v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16259","description":"<p>The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural\nLanguage Processing (NLP) during the past decade. However, the demands of long\ndocument analysis are quite different from those of shorter texts, while the\never increasing size of documents uploaded on-line renders automated\nunderstanding of long texts a critical area of research. This article has two\ngoals: a) it overviews the relevant neural building blocks, thus serving as a\nshort tutorial, and b) it surveys the state-of-the-art in long document NLP,\nmainly focusing on two central tasks: document classification and document\nsummarization. Sentiment analysis for long texts is also covered, since it is\ntypically treated as a particular case of document classification.\nAdditionally, this article discusses the main challenges, issues and current\nsolutions related to long document NLP. Finally, the relevant, publicly\navailable, annotated datasets are presented, in order to facilitate further\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsirmpas_D/0/1/0/all/0/1\">Dimitrios Tsirmpas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkionis_I/0/1/0/all/0/1\">Ioannis Gkionis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mademlis_I/0/1/0/all/0/1\">Ioannis Mademlis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLP Reproducibility For All: Understanding Experiences of Beginners. (arXiv:2305.16579v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16579","description":"<p>As natural language processing (NLP) has recently seen an unprecedented level\nof excitement, and more people are eager to enter the field, it is unclear\nwhether current research reproducibility efforts are sufficient for this group\nof beginners to apply the latest developments. To understand their needs, we\nconducted a study with 93 students in an introductory NLP course, where\nstudents reproduced the results of recent NLP papers. Surprisingly, we find\nthat their programming skill and comprehension of research papers have a\nlimited impact on their effort spent completing the exercise. Instead, we find\naccessibility efforts by research authors to be the key to success, including\ncomplete documentation, better coding practice, and easier access to data\nfiles. Going forward, we recommend that NLP researchers pay close attention to\nthese simple aspects of open-sourcing their work, and use insights from\nbeginners' feedback to provide actionable ideas on how to better support them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Storks_S/0/1/0/all/0/1\">Shane Storks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Keunwoo Peter Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Ziqiao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Emotion Experiencer Recognition. (arXiv:2305.16731v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16731","description":"<p>The most prominent subtask in emotion analysis is emotion classification; to\nassign a category to a textual unit, for instance a social media post. Many\nresearch questions from the social sciences do, however, not only require the\ndetection of the emotion of an author of a post but to understand who is\nascribed an emotion in text. This task is tackled by emotion role labeling\nwhich aims at extracting who is described in text to experience an emotion,\nwhy, and towards whom. This could, however, be considered overly sophisticated\nif the main question to answer is who feels which emotion. A targeted approach\nfor such setup is to classify emotion experiencer mentions (aka \"emoters\")\nregarding the emotion they presumably perceive. This task is similar to named\nentity recognition of person names with the difference that not every mentioned\nentity name is an emoter. While, very recently, data with emoter annotations\nhas been made available, no experiments have yet been performed to detect such\nmentions. With this paper, we provide baseline experiments to understand how\nchallenging the task is. We further evaluate the impact on experiencer-specific\nemotion categorization and appraisal detection in a pipeline, when gold\nmentions are not available. We show that experiencer detection in text is a\nchallenging task, with a precision of .82 and a recall of .56 (F1 =.66). These\nresults motivate future work of jointly modeling emoter spans and\nemotion/appraisal predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wegge_M/0/1/0/all/0/1\">Maximilian Wegge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory. (arXiv:2305.17144v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2305.17144","description":"<p>The captivating realm of Minecraft has attracted substantial research\ninterest in recent years, serving as a rich platform for developing intelligent\nagents capable of functioning in open-world environments. However, the current\nresearch landscape predominantly focuses on specific objectives, such as the\npopular \"ObtainDiamond\" task, and has not yet shown effective generalization to\na broader spectrum of tasks. Furthermore, the current leading success rate for\nthe \"ObtainDiamond\" task stands at around 20%, highlighting the limitations of\nReinforcement Learning (RL) based controllers used in existing methods. To\ntackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel\nframework integrates Large Language Models (LLMs) with text-based knowledge and\nmemory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These\nagents, equipped with the logic and common sense capabilities of LLMs, can\nskillfully navigate complex, sparse-reward environments with text-based\ninteractions. We develop a set of structured actions and leverage LLMs to\ngenerate action plans for the agents to execute. The resulting LLM-based agent\nmarkedly surpasses previous methods, achieving a remarkable improvement of\n+47.5% in success rate on the \"ObtainDiamond\" task, demonstrating superior\nrobustness compared to traditional RL-based controllers. Notably, our agent is\nthe first to procure all items in the Minecraft Overworld technology tree,\ndemonstrating its extensive capabilities. GITM does not need any GPU for\ntraining, but a single CPU node with 32 CPU cores is enough. This research\nshows the potential of LLMs in developing capable agents for handling\nlong-horizon, complex tasks and adapting to uncertainties in open-world\nenvironments. See the project website at https://github.com/OpenGVLab/GITM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xizhou Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuntao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1\">Hao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chenxin Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weijie Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chenyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Lewei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaoxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heterogeneous Value Evaluation for Large Language Models. (arXiv:2305.17147v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17147","description":"<p>The emergent capabilities of Large Language Models (LLMs) have made it\ncrucial to align their values with those of humans. Current methodologies\ntypically attempt alignment with a homogeneous human value and requires human\nverification, yet lack consensus on the desired aspect and depth of alignment\nand resulting human biases. In this paper, we propose A2EHV, an Automated\nAlignment Evaluation with a Heterogeneous Value system that (1) is automated to\nminimize individual human biases, and (2) allows assessments against various\ntarget values to foster heterogeneous agents. Our approach pivots on the\nconcept of value rationality, which represents the ability for agents to\nexecute behaviors that satisfy a target value the most. The quantification of\nvalue rationality is facilitated by the Social Value Orientation framework from\nsocial psychology, which partitions the value space into four categories to\nassess social preferences from agents' behaviors. We evaluate the value\nrationality of eight mainstream LLMs and observe that large models are more\ninclined to align neutral values compared to those with strong personal values.\nBy examining the behavior of these LLMs, we contribute to a deeper\nunderstanding of value alignment within a heterogeneous value system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaowei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1\">Siyuan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ceyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_Z/0/1/0/all/0/1\">Ziqi Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaodong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FACTUAL: A Benchmark for Faithful and Consistent Textual Scene Graph Parsing. (arXiv:2305.17497v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17497","description":"<p>Textual scene graph parsing has become increasingly important in various\nvision-language applications, including image caption evaluation and image\nretrieval. However, existing scene graph parsers that convert image captions\ninto scene graphs often suffer from two types of errors. First, the generated\nscene graphs fail to capture the true semantics of the captions or the\ncorresponding images, resulting in a lack of faithfulness. Second, the\ngenerated scene graphs have high inconsistency, with the same semantics\nrepresented by different annotations.\n</p>\n<p>To address these challenges, we propose a novel dataset, which involves\nre-annotating the captions in Visual Genome (VG) using a new intermediate\nrepresentation called FACTUAL-MR. FACTUAL-MR can be directly converted into\nfaithful and consistent scene graph annotations. Our experimental results\nclearly demonstrate that the parser trained on our dataset outperforms existing\napproaches in terms of faithfulness and consistency. This improvement leads to\na significant performance boost in both image caption evaluation and zero-shot\nimage retrieval tasks. Furthermore, we introduce a novel metric for measuring\nscene graph similarity, which, when combined with the improved scene graph\nparser, achieves state-of-the-art (SOTA) results on multiple benchmark datasets\nfor the aforementioned tasks. The code and dataset are available at\nhttps://github.com/zhuang-li/FACTUAL .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1\">Yuyang Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1\">Terry Yue Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lizhen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1\">Donghong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quan Hung Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translatotron 3: Speech to Speech Translation with Monolingual Data. (arXiv:2305.17547v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17547","description":"<p>This paper presents Translatotron 3, a novel approach to train a direct\nspeech-to-speech translation model from monolingual speech-text datasets only\nin a fully unsupervised manner. Translatotron 3 combines masked autoencoder,\nunsupervised embedding mapping, and back-translation to achieve this goal.\nExperimental results in speech-to-speech translation tasks between Spanish and\nEnglish show that Translatotron 3 outperforms a baseline cascade system,\nreporting 18.14 BLEU points improvement on the synthesized\nUnpaired-Conversational dataset. In contrast to supervised approaches that\nnecessitate real paired data, which is unavailable, or specialized modeling to\nreplicate para-/non-linguistic information, Translatotron 3 showcases its\ncapability to retain para-/non-linguistic such as pauses, speaking rates, and\nspeaker identity. Audio samples can be found in our website\n<a href=\"http://google-research.github.io/lingvo-lab/translatotron3\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nachmani_E/0/1/0/all/0/1\">Eliya Nachmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levkovitch_A/0/1/0/all/0/1\">Alon Levkovitch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yifan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asawaroengchai_C/0/1/0/all/0/1\">Chulayuth Asawaroengchai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zen_H/0/1/0/all/0/1\">Heiga Zen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1\">Michelle Tadmor Ramanovich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Task Synthesis for Visual Programming. (arXiv:2305.18342v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.18342","description":"<p>Generative neural models hold great promise in enhancing programming\neducation by synthesizing new content for students. We seek to design neural\nmodels that can automatically generate programming tasks for a given\nspecification in the context of visual programming domains. Despite the recent\nsuccesses of large generative models like GPT-4, our initial results show that\nthese models are ineffective in synthesizing visual programming tasks and\nstruggle with logical and spatial reasoning. We propose a novel neuro-symbolic\ntechnique, NeurTaskSyn, that can synthesize programming tasks for a\nspecification given in the form of desired programming concepts exercised by\nits solution code and constraints on the visual task. NeurTaskSyn has two\ncomponents: the first component is trained via imitation learning procedure to\ngenerate possible solution codes, and the second component is trained via\nreinforcement learning procedure to guide an underlying symbolic execution\nengine that generates visual tasks for these codes. We demonstrate the\neffectiveness of NeurTaskSyn through an extensive empirical evaluation and a\nqualitative study on reference tasks taken from the Hour of Code: Classic Maze\nchallenge by Code-dot-org and the Intro to Programming with Karel course by\nCodeHS-dot-com.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Padurean_V/0/1/0/all/0/1\">Victor-Alexandru P&#x103;durean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzannetos_G/0/1/0/all/0/1\">Georgios Tzannetos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_A/0/1/0/all/0/1\">Adish Singla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conformal Prediction with Large Language Models for Multi-Choice Question Answering. (arXiv:2305.18404v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18404","description":"<p>As large language models continue to be widely developed, robust uncertainty\nquantification techniques will become crucial for their safe deployment in\nhigh-stakes scenarios. In this work, we explore how conformal prediction can be\nused to provide uncertainty quantification in language models for the specific\ntask of multiple-choice question-answering. We find that the uncertainty\nestimates from conformal prediction are tightly correlated with prediction\naccuracy. This observation can be useful for downstream applications such as\nselective classification and filtering out low-quality predictions. We also\ninvestigate the exchangeability assumption required by conformal prediction to\nout-of-subject questions, which may be a more realistic scenario for many\npractical applications. Our work contributes towards more trustworthy and\nreliable usage of large language models in safety-critical situations, where\nrobust guarantees of error rate are required.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_B/0/1/0/all/0/1\">Bhawesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Charlie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1\">Gauri Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palepu_A/0/1/0/all/0/1\">Anil Palepu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellamy_D/0/1/0/all/0/1\">David Bellamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raskar_R/0/1/0/all/0/1\">Ramesh Raskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beam_A/0/1/0/all/0/1\">Andrew Beam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation Of Lexical Stylistic Features In Language Models' Embedding Space. (arXiv:2305.18657v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18657","description":"<p>The representation space of pretrained Language Models (LMs) encodes rich\ninformation about words and their relationships (e.g., similarity, hypernymy,\npolysemy) as well as abstract semantic notions (e.g., intensity). In this\npaper, we demonstrate that lexical stylistic notions such as complexity,\nformality, and figurativeness, can also be identified in this space. We show\nthat it is possible to derive a vector representation for each of these\nstylistic notions from only a small number of seed pairs. Using these vectors,\nwe can characterize new texts in terms of these dimensions by performing simple\ncalculations in the corresponding embedding space. We conduct experiments on\nfive datasets and find that static embeddings encode these features more\naccurately at the level of words and phrases, whereas contextualized LMs\nperform better on sentences. The lower performance of contextualized\nrepresentations at the word level is partially attributable to the anisotropy\nof their vector space, which can be corrected to some extent using techniques\nlike standardization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apidianaki_M/0/1/0/all/0/1\">Marianna Apidianaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DC CoMix TTS: An End-to-End Expressive TTS with Discrete Code Collaborated with Mixer. (arXiv:2305.19567v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2305.19567","description":"<p>Despite the huge successes made in neutral TTS, content-leakage remains a\nchallenge. In this paper, we propose a new input representation and simple\narchitecture to achieve improved prosody modeling. Inspired by the recent\nsuccess in the use of discrete code in TTS, we introduce discrete code to the\ninput of the reference encoder. Specifically, we leverage the vector quantizer\nfrom the audio compression model to exploit the diverse acoustic information it\nhas already been trained on. In addition, we apply the modified MLP-Mixer to\nthe reference encoder, making the architecture lighter. As a result, we train\nthe prosody transfer TTS in an end-to-end manner. We prove the effectiveness of\nour method through both subjective and objective evaluations. We demonstrate\nthat the reference encoder learns better speaker-independent prosody when\ndiscrete code is utilized as input in the experiments. In addition, we obtain\ncomparable results even when fewer parameters are inputted.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yerin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_M/0/1/0/all/0/1\">Myoung-Wan Koo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Global Context Mechanism for Sequence Labeling. (arXiv:2305.19928v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.19928","description":"<p>Sequential labeling tasks necessitate the computation of sentence\nrepresentations for each word within a given sentence. With the advent of\nadvanced pretrained language models; one common approach involves incorporating\na BiLSTM layer to bolster the sequence structure information at the output\nlevel. Nevertheless, it has been empirically demonstrated (P.-H. Li et al.,\n2020) that the potential of BiLSTM for generating sentence representations for\nsequence labeling tasks is constrained, primarily due to the amalgamation of\nfragments form past and future sentence representations to form a complete\nsentence representation. In this study, we discovered that strategically\nintegrating the whole sentence representation, which existing in the first cell\nand last cell of BiLSTM, into sentence representation of ecah cell, could\nmarkedly enhance the F1 score and accuracy. Using BERT embedded within BiLSTM\nas illustration, we conducted exhaustive experiments on nine datasets for\nsequence labeling tasks, encompassing named entity recognition (NER), part of\nspeech (POS) tagging and End-to-End Aspect-Based sentiment analysis (E2E-ABSA).\nWe noted significant improvements in F1 scores and accuracy across all examined\ndatasets .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Conglei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1\">Kun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongguang Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beam Tree Recursive Cells. (arXiv:2305.19999v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.19999","description":"<p>We propose Beam Tree Recursive Cell (BT-Cell) - a backpropagation-friendly\nframework to extend Recursive Neural Networks (RvNNs) with beam search for\nlatent structure induction. We further extend this framework by proposing a\nrelaxation of the hard top-k operators in beam search for better propagation of\ngradient signals. We evaluate our proposed models in different\nout-of-distribution splits in both synthetic and realistic data. Our\nexperiments show that BTCell achieves near-perfect performance on several\nchallenging structure-sensitive synthetic tasks like ListOps and logical\ninference while maintaining comparable performance in realistic data against\nother RvNN-based models. Additionally, we identify a previously unknown failure\ncase for neural models in generalization to unseen number of arguments in\nListOps. The code is available at:\nhttps://github.com/JRC1995/BeamTreeRecursiveCells.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_J/0/1/0/all/0/1\">Jishnu Ray Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1\">Cornelia Caragea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decision-Oriented Dialogue for Human-AI Collaboration. (arXiv:2305.20076v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.20076","description":"<p>We describe a class of tasks called decision-oriented dialogues, in which AI\nassistants must collaborate with one or more humans via natural language to\nhelp them make complex decisions. We formalize three domains in which users\nface everyday decisions: (1) choosing an assignment of reviewers to conference\npapers, (2) planning a multi-step itinerary in a city, and (3) negotiating\ntravel plans for a group of friends. In each of these settings, AI assistants\nand users have disparate abilities that they must combine to arrive at the best\ndecision: assistants can access and process large amounts of information, while\nusers have preferences and constraints external to the system. For each task,\nwe build a dialogue environment where agents receive a reward based on the\nquality of the final decision they reach. Using these environments, we collect\nhuman-human dialogues with humans playing the role of assistant. To compare how\ncurrent AI assistants communicate in these settings, we present baselines using\nlarge language models in self-play. Finally, we highlight a number of\nchallenges models face in decision-oriented dialogues, ranging from efficient\ncommunication to reasoning and optimization, and release our environments as a\ntestbed for future modeling work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jessy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomlin_N/0/1/0/all/0/1\">Nicholas Tomlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisner_J/0/1/0/all/0/1\">Jason Eisner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards an Understanding and Explanation for Mixed-Initiative Artificial Scientific Text Detection. (arXiv:2304.05011v1 [cs.HC] CROSS LISTED)","link":"http://arxiv.org/abs/2304.05011","description":"<p>Large language models (LLMs) have gained popularity in various fields for\ntheir exceptional capability of generating human-like text. Their potential\nmisuse has raised social concerns about plagiarism in academic contexts.\nHowever, effective artificial scientific text detection is a non-trivial task\ndue to several challenges, including 1) the lack of a clear understanding of\nthe differences between machine-generated and human-written scientific text, 2)\nthe poor generalization performance of existing methods caused by\nout-of-distribution issues, and 3) the limited support for human-machine\ncollaboration with sufficient interpretability during the detection process. In\nthis paper, we first identify the critical distinctions between\nmachine-generated and human-written scientific text through a quantitative\nexperiment. Then, we propose a mixed-initiative workflow that combines human\nexperts' prior knowledge with machine intelligence, along with a visual\nanalytics prototype to facilitate efficient and trustworthy scientific text\ndetection. Finally, we demonstrate the effectiveness of our approach through\ntwo case studies and a controlled user study with proficient researchers. We\nalso provide design implications for interactive artificial text detection\ntools in high-stakes decision-making scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weng_L/0/1/0/all/0/1\">Luoxuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Minfeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam Kwai Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiashun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Dongming Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-06-01T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}