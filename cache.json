{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-04-28T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model. (arXiv:2304.13731v1 [eess.AS])","link":"http://arxiv.org/abs/2304.13731","description":"<p>The immense scale of the recent large language models (LLM) allows many\ninteresting properties, such as, instruction- and chain-of-thought-based\nfine-tuning, that has significantly improved zero- and few-shot performance in\nmany natural language processing (NLP) tasks. Inspired by such successes, we\nadopt such an instruction-tuned LLM Flan-T5 as the text encoder for\ntext-to-audio (TTA) generation -- a task where the goal is to generate an audio\nfrom its textual description. The prior works on TTA either pre-trained a joint\ntext-audio encoder or used a non-instruction-tuned model, such as, T5.\nConsequently, our latent diffusion model (LDM)-based approach TANGO outperforms\nthe state-of-the-art AudioLDM on most metrics and stays comparable on the rest\non AudioCaps test set, despite training the LDM on a 63 times smaller dataset\nand keeping the text encoder frozen. This improvement might also be attributed\nto the adoption of audio pressure level-based sound mixing for training set\naugmentation, whereas the prior methods take a random mix.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ghosal_D/0/1/0/all/0/1\">Deepanway Ghosal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mehrish_A/0/1/0/all/0/1\">Ambuj Mehrish</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Internal State of an LLM Knows When its Lying. (arXiv:2304.13734v1 [cs.CL])","link":"http://arxiv.org/abs/2304.13734","description":"<p>While Large Language Models (LLMs) have shown exceptional performance in\nvarious tasks, their (arguably) most prominent drawback is generating\ninaccurate or false information with a confident tone. In this paper, we\nhypothesize that the LLM's internal state can be used to reveal the\ntruthfulness of a statement. Therefore, we introduce a simple yet effective\nmethod to detect the truthfulness of LLM-generated statements, which utilizes\nthe LLM's hidden layer activations to determine the veracity of statements. To\ntrain and evaluate our method, we compose a dataset of true and false\nstatements in six different topics. A classifier is trained to detect which\nstatement is true or false based on an LLM's activation values. Specifically,\nthe classifier receives as input the activation values from the LLM for each of\nthe statements in the dataset. Our experiments demonstrate that our method for\ndetecting statement veracity significantly outperforms even few-shot prompting\nmethods, highlighting its potential to enhance the reliability of LLM-generated\ncontent and its practical applicability in real-world scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azaria_A/0/1/0/all/0/1\">Amos Azaria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_T/0/1/0/all/0/1\">Tom Mitchell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine Tuning with Abnormal Examples. (arXiv:2304.13783v1 [cs.CL])","link":"http://arxiv.org/abs/2304.13783","description":"<p>Given the prevalence of crowd sourced labor in creating Natural Language\nprocessing datasets, these aforementioned sets have become increasingly large.\nFor instance, the SQUAD dataset currently sits at over 80,000 records. However,\nbecause the English language is rather repetitive in structure, the\ndistribution of word frequencies in the SQUAD dataset's contexts are relatively\nunchanged. By measuring each sentences distance from the co-variate distance of\nfrequencies of all sentences in the dataset, we identify 10,500 examples that\ncreate a more uniform distribution for training. While fine-tuning ELECTRA [4]\non this subset of examples reaches better performance to a model trained on all\n87,000 examples. Herein we introduce a methodology for systematically pruning\ndatasets for fine tuning reaching better out of sample performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rieger_W/0/1/0/all/0/1\">Will Rieger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translate to Disambiguate: Zero-shot Multilingual Word Sense Disambiguation with Pretrained Language Models. (arXiv:2304.13803v1 [cs.CL])","link":"http://arxiv.org/abs/2304.13803","description":"<p>Pretrained Language Models (PLMs) learn rich cross-lingual knowledge and can\nbe finetuned to perform well on diverse tasks such as translation and\nmultilingual word sense disambiguation (WSD). However, they often struggle at\ndisambiguating word sense in a zero-shot setting. To better understand this\ncontrast, we present a new study investigating how well PLMs capture\ncross-lingual word sense with Contextual Word-Level Translation (C-WLT), an\nextension of word-level translation that prompts the model to translate a given\nword in context. We find that as the model size increases, PLMs encode more\ncross-lingual word sense knowledge and better use context to improve WLT\nperformance. Building on C-WLT, we introduce a zero-shot approach for WSD,\ntested on 18 languages from the XL-WSD dataset. Our method outperforms fully\nsupervised baselines on recall for many evaluation languages without additional\ntraining or finetuning. This study presents a first step towards understanding\nhow to best leverage the cross-lingual knowledge inside PLMs for robust\nzero-shot reasoning in any language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Haoqiang Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blevins_T/0/1/0/all/0/1\">Terra Blevins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models. (arXiv:2304.13835v1 [cs.CL])","link":"http://arxiv.org/abs/2304.13835","description":"<p>Current dialogue research primarily studies pairwise (two-party)\nconversations, and does not address the everyday setting where more than two\nspeakers converse together. In this work, we both collect and evaluate\nmulti-party conversations to study this more general case. We use the LIGHT\nenvironment to construct grounded conversations, where each participant has an\nassigned character to role-play. We thus evaluate the ability of language\nmodels to act as one or more characters in such conversations. Models require\ntwo skills that pairwise-trained models appear to lack: (1) being able to\ndecide when to talk; (2) producing coherent utterances grounded on multiple\ncharacters. We compare models trained on our new dataset to existing\npairwise-trained dialogue models, as well as large language models with\nfew-shot prompting. We find that our new dataset, MultiLIGHT, which we will\npublicly release, can help bring significant improvements in the group setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jimmy Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuster_K/0/1/0/all/0/1\">Kurt Shuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urbanek_J/0/1/0/all/0/1\">Jack Urbanek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komeili_M/0/1/0/all/0/1\">Mojtaba Komeili</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Structured Seed-Mediated Gold Nanorod Growth Procedures from Literature with GPT-3. (arXiv:2304.13846v1 [physics.app-ph])","link":"http://arxiv.org/abs/2304.13846","description":"<p>Although gold nanorods have been the subject of much research, the pathways\nfor controlling their shape and thereby their optical properties remain largely\nheuristically understood. Although it is apparent that the simultaneous\npresence of and interaction between various reagents during synthesis control\nthese properties, computational and experimental approaches for exploring the\nsynthesis space can be either intractable or too time-consuming in practice.\nThis motivates an alternative approach leveraging the wealth of synthesis\ninformation already embedded in the body of scientific literature by developing\ntools to extract relevant structured data in an automated, high-throughput\nmanner. To that end, we present an approach using the powerful GPT-3 language\nmodel to extract structured multi-step seed-mediated growth procedures and\noutcomes for gold nanorods from unstructured scientific text. GPT-3 prompt\ncompletions are fine-tuned to predict synthesis templates in the form of JSON\ndocuments from unstructured text input with an overall accuracy of $86\\%$. The\nperformance is notable, considering the model is performing simultaneous entity\nrecognition and relation extraction. We present a dataset of 11,644 entities\nextracted from 1,137 papers, resulting in 268 papers with at least one complete\nseed-mediated gold nanorod growth procedure and outcome for a total of 332\ncomplete procedures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Walker_N/0/1/0/all/0/1\">Nicholas Walker</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Dagdelen_J/0/1/0/all/0/1\">John Dagdelen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Cruse_K/0/1/0/all/0/1\">Kevin Cruse</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lee_S/0/1/0/all/0/1\">Sanghoon Lee</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gleason_S/0/1/0/all/0/1\">Samuel Gleason</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Dunn_A/0/1/0/all/0/1\">Alexander Dunn</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ceder_G/0/1/0/all/0/1\">Gerbrand Ceder</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Alivisatos_A/0/1/0/all/0/1\">A. Paul Alivisatos</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Persson_K/0/1/0/all/0/1\">Kristin A. Persson</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jain_A/0/1/0/all/0/1\">Anubhav Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is a prompt and a few samples all you need? Using GPT-4 for data augmentation in low-resource classification tasks. (arXiv:2304.13861v1 [cs.CL])","link":"http://arxiv.org/abs/2304.13861","description":"<p>Obtaining and annotating data can be expensive and time-consuming, especially\nin complex, low-resource domains. We use GPT-4 and ChatGPT to augment small\nlabeled datasets with synthetic data via simple prompts, in three different\nclassification tasks with varying complexity. For each task, we randomly select\na base sample of 500 texts to generate 5,000 new synthetic samples. We explore\ntwo augmentation strategies: one that preserves original label distribution and\nanother that balances the distribution. Using a progressively larger training\nsample size, we train and evaluate a 110M parameter multilingual language model\non the real and synthetic data separately. We also test GPT-4 and ChatGPT in a\nzero-shot setting on the test sets. We observe that GPT-4 and ChatGPT have\nstrong zero-shot performance across all tasks. We find that data augmented with\nsynthetic samples yields a good downstream performance, and particularly aids\nin low-resource settings, such as in identifying rare classes. Human-annotated\ndata exhibits a strong predictive power, overtaking synthetic data in two out\nof the three tasks. This finding highlights the need for more complex prompts\nfor synthetic datasets to consistently surpass human-generated ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moller_A/0/1/0/all/0/1\">Anders Giovanni M&#xf8;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalsgaard_J/0/1/0/all/0/1\">Jacob Aarup Dalsgaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pera_A/0/1/0/all/0/1\">Arianna Pera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aiello_L/0/1/0/all/0/1\">Luca Maria Aiello</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferring Procedural Knowledge across Commonsense Tasks. (arXiv:2304.13867v1 [cs.CL])","link":"http://arxiv.org/abs/2304.13867","description":"<p>Stories about everyday situations are an essential part of human\ncommunication, motivating the need to develop AI agents that can reliably\nunderstand these stories. Despite the long list of supervised methods for story\ncompletion and procedural understanding, current AI has no mechanisms to\nautomatically track and explain procedures in unseen stories. To bridge this\ngap, we study the ability of AI models to transfer procedural knowledge to\nnovel narrative tasks in a transparent manner. We design LEAP: a comprehensive\nframework that integrates state-of-the-art modeling architectures, training\nregimes, and augmentation strategies based on both natural and synthetic\nstories. To address the lack of densely annotated training data, we devise a\nrobust automatic labeler based on few-shot prompting to enhance the augmented\ndata. Our experiments with in- and out-of-domain tasks reveal insights into the\ninterplay of different architectures, training regimes, and augmentation\nstrategies. LEAP's labeler has a clear positive impact on out-of-domain\ndatasets, while the resulting dense annotation provides native explainability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaixin Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MasonNLP+ at SemEval-2023 Task 8: Extracting Medical Questions, Experiences and Claims from Social Media using Knowledge-Augmented Pre-trained Language Models. (arXiv:2304.13875v1 [cs.CL])","link":"http://arxiv.org/abs/2304.13875","description":"<p>In online forums like Reddit, users share their experiences with medical\nconditions and treatments, including making claims, asking questions, and\ndiscussing the effects of treatments on their health. Building systems to\nunderstand this information can effectively monitor the spread of\nmisinformation and verify user claims. The Task-8 of the 2023 International\nWorkshop on Semantic Evaluation focused on medical applications, specifically\nextracting patient experience- and medical condition-related entities from user\nposts on social media. The Reddit Health Online Talk (RedHot) corpus contains\nposts from medical condition-related subreddits with annotations characterizing\nthe patient experience and medical conditions. In Subtask-1, patient experience\nis characterized by personal experience, questions, and claims. In Subtask-2,\nmedical conditions are characterized by population, intervention, and outcome.\nFor the automatic extraction of patient experiences and medical condition\ninformation, as a part of the challenge, we proposed language-model-based\nextraction systems that ranked $3^{rd}$ on both subtasks' leaderboards. In this\nwork, we describe our approach and, in addition, explore the automatic\nextraction of this information using domain-specific language models and the\ninclusion of external knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_G/0/1/0/all/0/1\">Giridhar Kaushik Ramachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangavarapu_H/0/1/0/all/0/1\">Haritha Gangavarapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lybarger_K/0/1/0/all/0/1\">Kevin Lybarger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uzuner_O/0/1/0/all/0/1\">Ozlem Uzuner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Keyphrase Generation: Analysis and Evaluation. (arXiv:2304.13883v1 [cs.CL])","link":"http://arxiv.org/abs/2304.13883","description":"<p>Keyphrase generation aims at generating topical phrases from a given text\neither by copying from the original text (present keyphrases) or by producing\nnew keyphrases (absent keyphrases) that capture the semantic meaning of the\ntext. Encoder-decoder models are most widely used for this task because of\ntheir capabilities for absent keyphrase generation. However, there has been\nlittle to no analysis on the performance and behavior of such models for\nkeyphrase generation. In this paper, we study various tendencies exhibited by\nthree strong models: T5 (based on a pre-trained transformer),\nCatSeq-Transformer (a non-pretrained Transformer), and ExHiRD (based on a\nrecurrent neural network). We analyze prediction confidence scores, model\ncalibration, and the effect of token position on keyphrases generation.\nMoreover, we motivate and propose a novel metric framework, SoftKeyScore, to\nevaluate the similarity between two sets of keyphrases by using softscores to\naccount for partial matching and semantic similarity. We find that SoftKeyScore\nis more suitable than the standard F1 metric for evaluating two sets of given\nkeyphrases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kundu_T/0/1/0/all/0/1\">Tuhin Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_J/0/1/0/all/0/1\">Jishnu Ray Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1\">Cornelia Caragea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Data Augmentation for Context-Dependent Text-to-SQL. (arXiv:2304.13902v1 [cs.CL])","link":"http://arxiv.org/abs/2304.13902","description":"<p>The limited scale of annotated data constraints existing context-dependent\ntext-to-SQL models because of the complexity of labeling. The data augmentation\nmethod is a commonly used method to solve this problem. However, the data\ngenerated by current augmentation methods often lack diversity. In this paper,\nwe introduce ConDA, which generates interactive questions and corresponding SQL\nresults. We designed the SQL dialogue state to enhance the data diversity\nthrough the state transition. Meanwhile, we also present a filter method to\nensure the data quality by a grounding model. Additionally, we utilize a\ngrounding model to identify and filter low-quality questions that mismatch the\nstate information. Experimental results on the SParC and CoSQL datasets show\nthat ConDA boosts the baseline model to achieve an average improvement of\n$3.3\\%$ on complex questions. Moreover, we analyze the augmented data, which\nreveals that the data generated by ConDA are of high quality in both SQL\ntemplate hardness and types, turns, and question consistency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dingzirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_L/0/1/0/all/0/1\">Longxu Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval-based Knowledge Augmented Vision Language Pre-training. (arXiv:2304.13923v1 [cs.CV])","link":"http://arxiv.org/abs/2304.13923","description":"<p>With recent progress in large-scale vision and language representation\nlearning, Vision Language Pretraining (VLP) models have achieved promising\nimprovements on various multi-modal downstream tasks. Albeit powerful, these\npre-training models still do not take advantage of world knowledge, which is\nimplicit in multi-modal data but comprises abundant and complementary\ninformation. In this work, we propose a REtrieval-based knowledge Augmented\nVision Language Pre-training model (REAVL), which retrieves world knowledge\nfrom knowledge graphs (KGs) and incorporates them in vision-language\npre-training. REAVL has two core components: a knowledge retriever that\nretrieves knowledge given multi-modal data, and a knowledge-augmented model\nthat fuses multi-modal data and knowledge. By novelly unifying four\nknowledge-aware self-supervised tasks, REAVL promotes the mutual integration of\nmulti-modal data and knowledge by fusing explicit knowledge with\nvision-language pairs for masked multi-modal data modeling and KG relational\nreasoning. Empirical experiments show that REAVL achieves new state-of-the-art\nperformance uniformly on knowledge-based vision-language understanding and\nmultimodal entity linking tasks, and competitive results on general\nvision-language tasks while only using 0.2% pre-training data of the best\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jiahua Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Z/0/1/0/all/0/1\">Zifei Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Longpo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuedong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning and Reasoning Multifaceted and Longitudinal Data for Poverty Estimates and Livelihood Capabilities of Lagged Regions in Rural India. (arXiv:2304.13958v1 [cs.CL])","link":"http://arxiv.org/abs/2304.13958","description":"<p>Poverty is a multifaceted phenomenon linked to the lack of capabilities of\nhouseholds to earn a sustainable livelihood, increasingly being assessed using\nmultidimensional indicators. Its spatial pattern depends on social, economic,\npolitical, and regional variables. Artificial intelligence has shown immense\nscope in analyzing the complexities and nuances of poverty. The proposed\nproject aims to examine the poverty situation of rural India for the period of\n1990-2022 based on the quality of life and livelihood indicators. The districts\nwill be classified into `advanced', `catching up', `falling behind', and\n`lagged' regions. The project proposes to integrate multiple data sources,\nincluding conventional national-level large sample household surveys, census\nsurveys, and proxy variables like daytime, and nighttime data from satellite\nimages, and communication networks, to name a few, to provide a comprehensive\nview of poverty at the district level. The project also intends to examine\ncausation and longitudinal analysis to examine the reasons for poverty. Poverty\nand inequality could be widening in developing countries due to demographic and\ngrowth-agglomerating policies. Therefore, targeting the lagging regions and the\nvulnerable population is essential to eradicate poverty and improve the quality\nof life to achieve the goal of `zero poverty'. Thus, the study also focuses on\nthe districts with a higher share of the marginal section of the population\ncompared to the national average to trace the performance of development\nindicators and their association with poverty in these regions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1\">Atharva Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1\">Raya Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_R/0/1/0/all/0/1\">Ravi S. Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Domain Evaluation of POS Taggers: From Wall Street Journal to Fandom Wiki. (arXiv:2304.13989v1 [cs.CL])","link":"http://arxiv.org/abs/2304.13989","description":"<p>The Wall Street Journal section of the Penn Treebank has been the de-facto\nstandard for evaluating POS taggers for a long time, and accuracies over 97\\%\nhave been reported. However, less is known about out-of-domain tagger\nperformance, especially with fine-grained label sets. Using data from Elder\nScrolls Fandom, a wiki about the \\textit{Elder Scrolls} video game universe, we\ncreate a modest dataset for qualitatively evaluating the cross-domain\nperformance of two POS taggers: the Stanford tagger (Toutanova et al. 2003) and\nBilty (Plank et al. 2016), both trained on WSJ. Our analyses show that\nperformance on tokens seen during training is almost as good as in-domain\nperformance, but accuracy on unknown tokens decreases from 90.37% to 78.37%\n(Stanford) and 87.84\\% to 80.41\\% (Bilty) across domains. Both taggers struggle\nwith proper nouns and inconsistent capitalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hansen_K/0/1/0/all/0/1\">Kia Kirstein Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goot_R/0/1/0/all/0/1\">Rob van der Goot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SweCTRL-Mini: a data-transparent Transformer-based large language model for controllable text generation in Swedish. (arXiv:2304.13994v1 [cs.CL])","link":"http://arxiv.org/abs/2304.13994","description":"<p>We present SweCTRL-Mini, a large Swedish language model that can be used for\ninference and fine-tuning on a single consumer-grade GPU. The model is based on\nthe CTRL architecture by Keskar, McCann, Varshney, Xiong, and Socher (2019),\nwhich means that users of the SweCTRL-Mini model can control the genre of the\ngenerated text by inserting special tokens in the generation prompts.\nSweCTRL-Mini is trained on a subset of the Swedish part of the mC4 corpus and a\nset of Swedish novels. In this article, we provide (1) a detailed account of\nthe utilized training data and text pre-processing steps, to the extent that it\nis possible to check whether a specific phrase/source was a part of the\ntraining data, and (2) an evaluation of the model on both discriminative tasks,\nusing automatic evaluation methods, and generative tasks, using human referees.\nWe also compare the generative capabilities of the model with those of GPT-3.\nSweCTRL-Mini is fully open and available for download.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalpakchi_D/0/1/0/all/0/1\">Dmytro Kalpakchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boye_J/0/1/0/all/0/1\">Johan Boye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Origin Tracing and Detecting of LLMs. (arXiv:2304.14072v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14072","description":"<p>The extraordinary performance of large language models (LLMs) heightens the\nimportance of detecting whether the context is generated by an AI system. More\nimportantly, while more and more companies and institutions release their LLMs,\nthe origin can be hard to trace. Since LLMs are heading towards the time of\nAGI, similar to the origin tracing in anthropology, it is of great importance\nto trace the origin of LLMs. In this paper, we first raise the concern of the\norigin tracing of LLMs and propose an effective method to trace and detect\nAI-generated contexts. We introduce a novel algorithm that leverages the\ncontrastive features between LLMs and extracts model-wise features to trace the\ntext origins. Our proposed method works under both white-box and black-box\nsettings therefore can be widely generalized to detect various LLMs.(e.g. can\nbe generalized to detect GPT-3 models without the GPT-3 models). Also, our\nproposed method requires only limited data compared with the supervised\nlearning methods and can be extended to trace new-coming model origins. We\nconstruct extensive experiments to examine whether we can trace the origins of\ngiven texts. We provide valuable observations based on the experimental\nresults, such as the difficulty level of AI origin tracing, and the AI origin\nsimilarities, and call for ethical concerns of LLM providers. We are releasing\nall codes and data as a toolkit and benchmark for future AI origin tracing and\ndetecting studies. \\footnote{We are releasing all available resource at\n\\url{https://github.com/OpenLMLab/}.}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1\">Ke Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Human-Human Interactions in Images from Weak Textual Supervision. (arXiv:2304.14104v1 [cs.CV])","link":"http://arxiv.org/abs/2304.14104","description":"<p>Interactions between humans are diverse and context-dependent, but previous\nworks have treated them as categorical, disregarding the heavy tail of possible\ninteractions. We propose a new paradigm of learning human-human interactions as\nfree text from a single still image, allowing for flexibility in modeling the\nunlimited space of situations and relationships between people. To overcome the\nabsence of data labelled specifically for this task, we use knowledge\ndistillation applied to synthetic caption data produced by a large language\nmodel without explicit supervision. We show that the pseudo-labels produced by\nthis procedure can be used to train a captioning model to effectively\nunderstand human-human interactions in images, as measured by a variety of\nmetrics that measure textual and semantic faithfulness and factual groundedness\nof our predictions. We further show that our approach outperforms SOTA image\ncaptioning and situation recognition models on this task. We will release our\ncode and pseudo-labels along with Waldo and Wenda, a manually-curated test set\nfor still image human-human interaction understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alper_M/0/1/0/all/0/1\">Morris Alper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1\">Hadar Averbuch-Elor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatLog: Recording and Analyzing ChatGPT Across Time. (arXiv:2304.14106v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14106","description":"<p>While there are abundant researches about evaluating ChatGPT on natural\nlanguage understanding and generation tasks, few studies have investigated how\nChatGPT's behavior changes over time. In this paper, we collect a\ncoarse-to-fine temporal dataset called ChatLog, consisting of two parts that\nupdate monthly and daily: ChatLog-Monthly is a dataset of 38,730\nquestion-answer pairs collected every month including questions from both the\nreasoning and classification tasks. ChatLog-Daily, on the other hand, consists\nof ChatGPT's responses to 1000 identical questions for long-form generation\nevery day. We conduct comprehensive automatic and human evaluation to provide\nthe evidence for the existence of ChatGPT evolving patterns. We further analyze\nthe unchanged characteristics of ChatGPT over time by extracting its knowledge\nand linguistic features. We find some stable features to improve the robustness\nof a RoBERTa-based detector on new versions of ChatGPT. We will continuously\nmaintain our project at https://github.com/THU-KEG/ChatLog.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_S/0/1/0/all/0/1\">Shangqing Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jifan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DataComp: In search of the next generation of multimodal datasets. (arXiv:2304.14108v1 [cs.CV])","link":"http://arxiv.org/abs/2304.14108","description":"<p>Large multimodal datasets have been instrumental in recent breakthroughs such\nas CLIP, Stable Diffusion, and GPT-4. At the same time, datasets rarely receive\nthe same research attention as model architectures or training algorithms. To\naddress this shortcoming in the machine learning ecosystem, we introduce\nDataComp, a benchmark where the training code is fixed and researchers innovate\nby proposing new training sets. We provide a testbed for dataset experiments\ncentered around a new candidate pool of 12.8B image-text pairs from Common\nCrawl. Participants in our benchmark design new filtering techniques or curate\nnew data sources and then evaluate their new dataset by running our\nstandardized CLIP training code and testing on 38 downstream test sets. Our\nbenchmark consists of multiple scales, with four candidate pool sizes and\nassociated compute budgets ranging from 12.8M to 12.8B samples seen during\ntraining. This multi-scale design facilitates the study of scaling trends and\nmakes the benchmark accessible to researchers with varying resources.\n</p>\n<p>Our baseline experiments show that the DataComp workflow is a promising way\nof improving multimodal datasets. We introduce DataComp-1B, a dataset created\nby applying a simple filtering algorithm to the 12.8B candidate pool. The\nresulting 1.4B subset enables training a CLIP ViT-L/14 from scratch to 79.2%\nzero-shot accuracy on ImageNet. Our new ViT-L/14 model outperforms a larger\nViT-g/14 trained on LAION-2B by 0.7 percentage points while requiring 9x less\ntraining compute. We also outperform OpenAI's CLIP ViT-L/14 by 3.7 percentage\npoints, which is trained with the same compute budget as our model. These gains\nhighlight the potential for improving model performance by carefully curating\ntraining sets. We view DataComp-1B as only the first step and hope that\nDataComp paves the way toward the next generation of multimodal datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1\">Samir Yitzhak Gadre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_A/0/1/0/all/0/1\">Alex Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayase_J/0/1/0/all/0/1\">Jonathan Hayase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smyrnis_G/0/1/0/all/0/1\">Georgios Smyrnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thao Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marten_R/0/1/0/all/0/1\">Ryan Marten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1\">Mitchell Wortsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1\">Dhruba Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jieyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orgad_E/0/1/0/all/0/1\">Eyal Orgad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Entezari_R/0/1/0/all/0/1\">Rahim Entezari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daras_G/0/1/0/all/0/1\">Giannis Daras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratt_S/0/1/0/all/0/1\">Sarah Pratt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanujan_V/0/1/0/all/0/1\">Vivek Ramanujan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1\">Yonatan Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marathe_K/0/1/0/all/0/1\">Kalyani Marathe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mussmann_S/0/1/0/all/0/1\">Stephen Mussmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vencu_R/0/1/0/all/0/1\">Richard Vencu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherti_M/0/1/0/all/0/1\">Mehdi Cherti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1\">Ranjay Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1\">Pang Wei Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saukh_O/0/1/0/all/0/1\">Olga Saukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratner_A/0/1/0/all/0/1\">Alexander Ratner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuran Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaumont_R/0/1/0/all/0/1\">Romain Beaumont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Sewoong Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1\">Alex Dimakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jitsev_J/0/1/0/all/0/1\">Jenia Jitsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmon_Y/0/1/0/all/0/1\">Yair Carmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1\">Vaishaal Shankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT vs State-of-the-Art Models: A Benchmarking Study in Keyphrase Generation Task. (arXiv:2304.14177v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14177","description":"<p>Transformer-based language models, including ChatGPT, have demonstrated\nexceptional performance in various natural language generation tasks. However,\nthere has been limited research evaluating ChatGPT's keyphrase generation\nability, which involves identifying informative phrases that accurately reflect\na document's content. This study seeks to address this gap by comparing\nChatGPT's keyphrase generation performance with state-of-the-art models, while\nalso testing its potential as a solution for two significant challenges in the\nfield: domain adaptation and keyphrase generation from long documents. We\nconducted experiments on six publicly available datasets from scientific\narticles and news domains, analyzing performance on both short and long\ndocuments. Our results show that ChatGPT outperforms current state-of-the-art\nmodels in all tested datasets and environments, generating high-quality\nkeyphrases that adapt well to diverse domains and document lengths.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Cruz_R/0/1/0/all/0/1\">Roberto Mart&#xed;nez-Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Lopez_A/0/1/0/all/0/1\">Alvaro J. L&#xf3;pez-L&#xf3;pez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portela_J/0/1/0/all/0/1\">Jos&#xe9; Portela</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality. (arXiv:2304.14178v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14178","description":"<p>Large language models (LLMs) have demonstrated impressive zero-shot abilities\non a variety of open-ended tasks, while recent research has also explored the\nuse of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl,\na novel training paradigm that equips LLMs with multi-modal abilities through\nmodularized learning of foundation LLM, a visual knowledge module, and a visual\nabstractor module. This approach can support multiple modalities and facilitate\ndiverse unimodal and multimodal abilities through modality collaboration. The\ntraining paradigm of mPLUG-Owl involves a two-stage method for aligning image\nand text, which learns visual knowledge with the assistance of LLM while\nmaintaining and even improving the generation abilities of LLM. In the first\nstage, the visual knowledge module and abstractor module are trained with a\nfrozen LLM module to align the image and text. In the second stage,\nlanguage-only and multi-modal supervised datasets are used to jointly fine-tune\na low-rank adaption (LoRA) module on LLM and the abstractor module by freezing\nthe visual knowledge module. We carefully build a visually-related instruction\nevaluation set OwlEval. Experimental results show that our model outperforms\nexisting multi-modal models, demonstrating mPLUG-Owl's impressive instruction\nand visual understanding ability, multi-turn conversation ability, and\nknowledge reasoning ability. Besides, we observe some unexpected and exciting\nabilities such as multi-image correlation and scene text understanding, which\nmakes it possible to leverage it for harder real scenarios, such as vision-only\ndocument comprehension. Our code, pre-trained model, instruction-tuned models,\nand evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The\nonline demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinghao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guohai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiabo Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1\">Anwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Pengcheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yaya Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hehong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Junfeng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1\">Qian Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NAP at SemEval-2023 Task 3: Is Less Really More? (Back-)Translation as Data Augmentation Strategies for Detecting Persuasion Techniques. (arXiv:2304.14179v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14179","description":"<p>Persuasion techniques detection in news in a multi-lingual setup is\nnon-trivial and comes with challenges, including little training data. Our\nsystem successfully leverages (back-)translation as data augmentation\nstrategies with multi-lingual transformer models for the task of detecting\npersuasion techniques. The automatic and human evaluation of our augmented data\nallows us to explore whether (back-)translation aid or hinder performance. Our\nin-depth analyses indicate that both data augmentation strategies boost\nperformance; however, balancing human-produced and machine-generated data seems\nto be crucial.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Falk_N/0/1/0/all/0/1\">Neele Falk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eichel_A/0/1/0/all/0/1\">Annerose Eichel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccirilli_P/0/1/0/all/0/1\">Prisca Piccirilli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UIO at SemEval-2023 Task 12: Multilingual fine-tuning for sentiment classification in low-resource languages. (arXiv:2304.14189v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14189","description":"<p>Our contribution to the 2023 AfriSenti-SemEval shared task 12: Sentiment\nAnalysis for African Languages, provides insight into how a multilingual large\nlanguage model can be a resource for sentiment analysis in languages not seen\nduring pretraining. The shared task provides datasets of a variety of African\nlanguages from different language families. The languages are to various\ndegrees related to languages used during pretraining, and the language data\ncontain various degrees of code-switching. We experiment with both monolingual\nand multilingual datasets for the final fine-tuning, and find that with the\nprovided datasets that contain samples in the thousands, monolingual\nfine-tuning yields the best results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ronningstad_E/0/1/0/all/0/1\">Egil R&#xf8;nningstad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Modular Approach for Multilingual Timex Detection and Normalization using Deep Learning and Grammar-based methods. (arXiv:2304.14221v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14221","description":"<p>Detecting and normalizing temporal expressions is an essential step for many\nNLP tasks. While a variety of methods have been proposed for detection, best\nnormalization approaches rely on hand-crafted rules. Furthermore, most of them\nhave been designed only for English. In this paper we present a modular\nmultilingual temporal processing system combining a fine-tuned Masked Language\nModel for detection, and a grammar-based normalizer. We experiment in Spanish\nand English and compare with HeidelTime, the state-of-the-art in multilingual\ntemporal processing. We obtain best results in gold timex normalization, timex\ndetection and type recognition, and competitive performance in the combined\nTempEval-3 relaxed value metric. A detailed error analysis shows that detecting\nonly those timexes for which it is feasible to provide a normalization is\nhighly beneficial in this last metric. This raises the question of which is the\nbest strategy for timex processing, namely, leaving undetected those timexes\nfor which is not easy to provide normalization rules or aiming for high\ncoverage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Escribano_N/0/1/0/all/0/1\">Nayla Escribano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigau_G/0/1/0/all/0/1\">German Rigau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agerri_R/0/1/0/all/0/1\">Rodrigo Agerri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models are Strong Zero-Shot Retriever. (arXiv:2304.14233v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14233","description":"<p>In this work, we propose a simple method that applies a large language model\n(LLM) to large-scale retrieval in zero-shot scenarios. Our method, Language\nlanguage model as Retriever (LameR) is built upon no other neural models but an\nLLM, while breaking up brute-force combinations of retrievers with LLMs and\nlifting the performance of zero-shot retrieval to be very competitive on\nbenchmark datasets. Essentially, we propose to augment a query with its\npotential answers by prompting LLMs with a composition of the query and the\nquery's in-domain candidates. The candidates, regardless of correct or wrong,\nare obtained by a vanilla retrieval procedure on the target collection. Such\ncandidates, as a part of prompts, are likely to help LLM generate more precise\nanswers by pattern imitation or candidate summarization. Even if all the\ncandidates are wrong, the prompts at least make LLM aware of in-collection\npatterns and genres. Moreover, due to the low performance of a self-supervised\nretriever, the LLM-based query augmentation becomes less effective as the\nretriever bottlenecks the whole pipeline. So, we propose to leverage a\nnon-parametric lexicon-based method (e.g., BM25) as the retrieval module to\ncapture query-document overlap in a literal fashion. As such, LameR makes the\nretrieval procedure transparent to the LLM, so it circumvents the performance\nbottleneck.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Intended Uses of Automated Fact-Checking Artefacts: Why, How and Who. (arXiv:2304.14238v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14238","description":"<p>Automated fact-checking is often presented as an epistemic tool that\nfact-checkers, social media consumers, and other stakeholders can use to fight\nmisinformation. Nevertheless, few papers thoroughly discuss how. We document\nthis by analysing 100 highly-cited papers, and annotating epistemic elements\nrelated to intended use, i.e., means, ends, and stakeholders. We find that\nnarratives leaving out some of these aspects are common, that many papers\npropose inconsistent means and ends, and that the feasibility of suggested\nstrategies rarely has empirical backing. We argue that this vagueness actively\nhinders the technology from reaching its goals, as it encourages overclaiming,\nlimits criticism, and prevents stakeholder feedback. Accordingly, we provide\nseveral recommendations for thinking and writing about the use of fact-checking\nartefacts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1\">Michael Schlichtkrull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ousidhoum_N/0/1/0/all/0/1\">Nedjma Ousidhoum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity-Level Sentiment Analysis (ELSA): An exploratory task survey. (arXiv:2304.14241v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14241","description":"<p>This paper explores the task of identifying the overall sentiment expressed\ntowards volitional entities (persons and organizations) in a document -- what\nwe refer to as Entity-Level Sentiment Analysis (ELSA). While identifying\nsentiment conveyed towards an entity is well researched for shorter texts like\ntweets, we find little to no research on this specific task for longer texts\nwith multiple mentions and opinions towards the same entity. This lack of\nresearch would be understandable if ELSA can be derived from existing tasks and\nmodels. To assess this, we annotate a set of professional reviews for their\noverall sentiment towards each volitional entity in the text. We sample from\ndata already annotated for document-level, sentence-level, and target-level\nsentiment in a multi-domain review corpus, and our results indicate that there\nis no single proxy task that provides this overall sentiment we seek for the\nentities at a satisfactory level of performance. We present a suite of\nexperiments aiming to assess the contribution towards ELSA provided by\ndocument-, sentence-, and target-level sentiment analysis, and provide a\ndiscussion of their shortcomings. We show that sentiment in our dataset is\nexpressed not only with an entity mention as target, but also towards targets\nwith a sentiment-relevant relation to a volitional entity. In our data, these\nrelations extend beyond anaphoric coreference resolution, and our findings call\nfor further research of the topic. Finally, we also present a survey of\nprevious relevant work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ronningstad_E/0/1/0/all/0/1\">Egil R&#xf8;nningstad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velldal_E/0/1/0/all/0/1\">Erik Velldal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovrelid_L/0/1/0/all/0/1\">Lilja &#xd8;vrelid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What's in a Name? Evaluating Assembly-Part Semantic Knowledge in Language Models through User-Provided Names in CAD Files. (arXiv:2304.14275v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14275","description":"<p>Semantic knowledge of part-part and part-whole relationships in assemblies is\nuseful for a variety of tasks from searching design repositories to the\nconstruction of engineering knowledge bases. In this work we propose that the\nnatural language names designers use in Computer Aided Design (CAD) software\nare a valuable source of such knowledge, and that Large Language Models (LLMs)\ncontain useful domain-specific information for working with this data as well\nas other CAD and engineering-related tasks.\n</p>\n<p>In particular we extract and clean a large corpus of natural language part,\nfeature and document names and use this to quantitatively demonstrate that a\npre-trained language model can outperform numerous benchmarks on three\nself-supervised tasks, without ever having seen this data before. Moreover, we\nshow that fine-tuning on the text data corpus further boosts the performance on\nall tasks, thus demonstrating the value of the text data which until now has\nbeen largely ignored. We also identify key limitations to using LLMs with text\ndata alone, and our findings provide a strong motivation for further work into\nmulti-modal text-geometry models.\n</p>\n<p>To aid and encourage further work in this area we make all our data and code\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meltzer_P/0/1/0/all/0/1\">Peter Meltzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambourne_J/0/1/0/all/0/1\">Joseph G. Lambourne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grandi_D/0/1/0/all/0/1\">Daniele Grandi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI, write an essay for me: A large-scale comparison of human-written versus ChatGPT-generated essays. (arXiv:2304.14276v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14276","description":"<p>Background: Recently, ChatGPT and similar generative AI models have attracted\nhundreds of millions of users and become part of the public discourse. Many\nbelieve that such models will disrupt society and will result in a significant\nchange in the education system and information generation in the future. So\nfar, this belief is based on either colloquial evidence or benchmarks from the\nowners of the models -- both lack scientific rigour.\n</p>\n<p>Objective: Through a large-scale study comparing human-written versus\nChatGPT-generated argumentative student essays, we systematically assess the\nquality of the AI-generated content.\n</p>\n<p>Methods: A large corpus of essays was rated using standard criteria by a\nlarge number of human experts (teachers). We augment the analysis with a\nconsideration of the linguistic characteristics of the generated essays.\n</p>\n<p>Results: Our results demonstrate that ChatGPT generates essays that are rated\nhigher for quality than human-written essays. The writing style of the AI\nmodels exhibits linguistic characteristics that are different from those of the\nhuman-written essays, e.g., it is characterized by fewer discourse and\nepistemic markers, but more nominalizations and greater lexical diversity.\n</p>\n<p>Conclusions: Our results clearly demonstrate that models like ChatGPT\noutperform humans in generating argumentative essays. Since the technology is\nreadily available for anyone to use, educators must act immediately. We must\nre-invent homework and develop teaching concepts that utilize these AI models\nin the same way as math utilized the calculator: teach the general concepts\nfirst and then use AI tools to free up time for other learning objectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Herbold_S/0/1/0/all/0/1\">Steffen Herbold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hautli_Janisz_A/0/1/0/all/0/1\">Annette Hautli-Janisz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heuer_U/0/1/0/all/0/1\">Ute Heuer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kikteva_Z/0/1/0/all/0/1\">Zlata Kikteva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trautsch_A/0/1/0/all/0/1\">Alexander Trautsch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Frame Induction with Deep Metric Learning. (arXiv:2304.14286v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14286","description":"<p>Recent studies have demonstrated the usefulness of contextualized word\nembeddings in unsupervised semantic frame induction. However, they have also\nrevealed that generic contextualized embeddings are not always consistent with\nhuman intuitions about semantic frames, which causes unsatisfactory performance\nfor frame induction based on contextualized embeddings. In this paper, we\naddress supervised semantic frame induction, which assumes the existence of\nframe-annotated data for a subset of predicates in a corpus and aims to build a\nframe induction model that leverages the annotated data. We propose a model\nthat uses deep metric learning to fine-tune a contextualized embedding model,\nand we apply the fine-tuned contextualized embeddings to perform semantic frame\ninduction. Our experiments on FrameNet show that fine-tuning with deep metric\nlearning considerably improves the clustering evaluation scores, namely, the\nB-cubed F-score and Purity F-score, by about 8 points or more. We also\ndemonstrate that our approach is effective even when the number of training\ninstances is small.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamada_K/0/1/0/all/0/1\">Kosuke Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasano_R/0/1/0/all/0/1\">Ryohei Sasano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1\">Koichi Takeda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlled Text Generation with Natural Language Instructions. (arXiv:2304.14293v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14293","description":"<p>Large language models generate fluent texts and can follow natural language\ninstructions to solve a wide range of tasks without task-specific training.\nNevertheless, it is notoriously difficult to control their generation to\nsatisfy the various constraints required by different applications. In this\nwork, we present InstructCTG, a controlled text generation framework that\nincorporates different constraints by conditioning on natural language\ndescriptions and demonstrations of the constraints. In particular, we first\nextract the underlying constraints of natural texts through a combination of\noff-the-shelf NLP tools and simple heuristics. We then verbalize the\nconstraints into natural language instructions to form weakly supervised\ntraining data. By prepending natural language descriptions of the constraints\nand a few demonstrations, we fine-tune a pre-trained language model to\nincorporate various types of constraints. Compared to existing search-based or\nscore-based methods, InstructCTG is more flexible to different constraint types\nand has a much smaller impact on the generation quality and speed because it\ndoes not modify the decoding procedure. Additionally, InstructCTG allows the\nmodel to adapt to new constraints without re-training through the use of\nfew-shot task generalization and in-context learning abilities of\ninstruction-tuned language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuchen Eleanor Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilcox_E/0/1/0/all/0/1\">Ethan Wilcox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Are State-of-the-Art Evaluators of Code Generation. (arXiv:2304.14317v1 [cs.AI])","link":"http://arxiv.org/abs/2304.14317","description":"<p>Recent advancements in the field of natural language generation have\nfacilitated the use of large language models to assess the quality of generated\ntext. Although these models have shown promising results in tasks such as\nmachine translation and summarization, their applicability in code generation\ntasks remains limited without human involvement. The complexity of programming\nconcepts required for such tasks makes it difficult to develop evaluation\nmetrics that align with human judgment. Token-matching-based metrics, such as\nBLEU, have demonstrated weak correlations with human practitioners in code\ngeneration tasks. Moreover, the utilization of human-written test suites to\nevaluate functional correctness can be challenging in domains with low\nresources. To overcome these obstacles, we propose a new evaluation framework\nbased on the GPT-3.5 (\\texttt{GPT-3.5-turbo}), for code generation assessments.\nOur framework addresses the limitations of existing approaches by achieving\nsuperior correlations with functional correctness and human preferences,\nwithout the need for test oracles or references. We evaluate the efficacy of\nour framework on two different tasks and four programming languages, comparing\nits performance with the state-of-the-art CodeBERTScore metric, which relies on\na pre-trained model. Our results demonstrate that our framework surpasses\nCodeBERTScore, delivering high levels of accuracy and consistency across\nvarious programming languages and tasks. We also make our evaluation framework\nand datasets available to the public at\n\\url{https://github.com/terryyz/llm-code-eval}, encouraging further research in\nthe evaluation of code generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1\">Terry Yue Zhuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"q2d: Turning Questions into Dialogs to Teach Models How to Search. (arXiv:2304.14318v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14318","description":"<p>One of the exciting capabilities of recent language models for dialog is\ntheir ability to independently search for relevant information to ground a\ngiven dialog response. However, obtaining training data to teach models how to\nissue search queries is time and resource consuming. In this work, we propose\nq2d: an automatic data generation pipeline that generates information-seeking\ndialogs from questions. We prompt a large language model (PaLM) to create\nconversational versions of question answering datasets, and use it to improve\nquery generation models that communicate with external search APIs to ground\ndialog responses. Unlike previous approaches which relied on human written\ndialogs with search queries, our method allows to automatically generate\nquery-based grounded dialogs with better control and scale. Our experiments\ndemonstrate that: (1) For query generation on the QReCC dataset, models trained\non our synthetically-generated data achieve 90%--97% of the performance of\nmodels trained on the human-generated data; (2) We can successfully generate\ndata for training dialog models in new domains without any existing dialog data\nas demonstrated on the multi-hop MuSiQue and Bamboogle QA datasets. (3) We\nperform a thorough analysis of the generated dialogs showing that humans find\nthem of high quality and struggle to distinguish them from human-written\ndialogs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1\">Yonatan Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Ganor_S/0/1/0/all/0/1\">Shlomi Cohen-Ganor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakimi_I/0/1/0/all/0/1\">Ido Hakimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewenberg_Y/0/1/0/all/0/1\">Yoad Lewenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aharoni_R/0/1/0/all/0/1\">Roee Aharoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinreb_E/0/1/0/all/0/1\">Enav Weinreb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Idioms, Probing and Dangerous Things: Towards Structural Probing for Idiomaticity in Vector Space. (arXiv:2304.14333v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14333","description":"<p>The goal of this paper is to learn more about how idiomatic information is\nstructurally encoded in embeddings, using a structural probing method. We\nrepurpose an existing English verbal multi-word expression (MWE) dataset to\nsuit the probing framework and perform a comparative probing study of static\n(GloVe) and contextual (BERT) embeddings. Our experiments indicate that both\nencode some idiomatic information to varying degrees, but yield conflicting\nevidence as to whether idiomaticity is encoded in the vector norm, leaving this\nan open question. We also identify some limitations of the used dataset and\nhighlight important directions for future work in improving its suitability for\na probing analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klubicka_F/0/1/0/all/0/1\">Filip Klubi&#x10d;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nedumpozhimana_V/0/1/0/all/0/1\">Vasudevan Nedumpozhimana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelleher_J/0/1/0/all/0/1\">John D. Kelleher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MarsEclipse at SemEval-2023 Task 3: Multi-Lingual and Multi-Label Framing Detection with Contrastive Learning. (arXiv:2304.14339v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14339","description":"<p>This paper describes our system for SemEval-2023 Task 3 Subtask 2 on Framing\nDetection. We used a multi-label contrastive loss for fine-tuning large\npre-trained language models in a multi-lingual setting, achieving very\ncompetitive results: our system was ranked first on the official test set and\non the official shared task leaderboard for five of the six languages for which\nwe had training data and for which we could perform fine-tuning. Here, we\ndescribe our experimental setup, as well as various ablation studies. The code\nof our system is available at https://github.com/QishengL/SemEval2023\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1\">Qisheng Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_M/0/1/0/all/0/1\">Meiting Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Dark Side of ChatGPT: Legal and Ethical Challenges from Stochastic Parrots and Hallucination. (arXiv:2304.14347v1 [cs.CY])","link":"http://arxiv.org/abs/2304.14347","description":"<p>With the launch of ChatGPT, Large Language Models (LLMs) are shaking up our\nwhole society, rapidly altering the way we think, create and live. For\ninstance, the GPT integration in Bing has altered our approach to online\nsearching. While nascent LLMs have many advantages, new legal and ethical risks\nare also emerging, stemming in particular from stochastic parrots and\nhallucination. The EU is the first and foremost jurisdiction that has focused\non the regulation of AI models. However, the risks posed by the new LLMs are\nlikely to be underestimated by the emerging EU regulatory paradigm. Therefore,\nthis correspondence warns that the European AI regulatory paradigm must evolve\nfurther to mitigate such risks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zihao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Industrial Engineering with Large Language Models: A case study of ChatGPT's performance on Oil & Gas problems. (arXiv:2304.14354v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14354","description":"<p>Large Language Models (LLMs) have shown great potential in solving complex\nproblems in various fields, including oil and gas engineering and other\nindustrial engineering disciplines like factory automation, PLC programming\netc. However, automatic identification of strong and weak solutions to\nfundamental physics equations governing several industrial processes remain a\nchallenging task. This paper identifies the limitation of current LLM\napproaches, particularly ChatGPT in selected practical problems native to oil\nand gas engineering but not exclusively. The performance of ChatGPT in solving\ncomplex problems in oil and gas engineering is discussed and the areas where\nLLMs are most effective are presented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ogundare_O/0/1/0/all/0/1\">Oluwatosin Ogundare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madasu_S/0/1/0/all/0/1\">Srinath Madasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiggins_N/0/1/0/all/0/1\">Nathanial Wiggins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to Guardrail Models for Virtual Assistants. (arXiv:2304.14364v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14364","description":"<p>A wave of new task-based virtual assistants has been fueled by increasingly\npowerful large language models, such as GPT-4. These conversational agents can\nbe customized to serve customer-specific use cases, but ensuring that\nagent-generated text conforms to designer-specified rules included in prompt\ninstructions alone is challenging. Therefore, chatbot designers often use\nanother model, called a guardrail model, to verify that the agent output aligns\nwith their rules and constraints. We explore using a distillation approach to\nguardrail models to monitor the output of the first model using training data\nfrom GPT-4. We find two crucial steps to our CONSCENDI process:\nscenario-augmented generation and contrastive training examples. When\ngenerating conversational data, we generate a set of rule-breaking scenarios,\nwhich enumerate a diverse set of high-level ways a rule can be violated. This\nscenario-guided approach produces a diverse training set of rule-violating\nconversations, and it provides chatbot designers greater control over the\nclassification process. We also prompt GPT-4 to also generate contrastive\nexamples by altering conversations with violations into acceptable\nconversations. This set of borderline, contrastive examples enables the\ndistilled model to learn finer-grained distinctions between what is acceptable\nand what is not. We find that CONSCENDI results in guardrail models that\nimprove over baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Albert Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_V/0/1/0/all/0/1\">Varun Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schumacher_E/0/1/0/all/0/1\">Elliot Schumacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannan_A/0/1/0/all/0/1\">Anitha Kannan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Energy-based Models as Zero-Shot Planners for Compositional Scene Rearrangement. (arXiv:2304.14391v1 [cs.RO])","link":"http://arxiv.org/abs/2304.14391","description":"<p>Language is compositional; an instruction can express multiple relation\nconstraints to hold among objects in a scene that a robot is tasked to\nrearrange. Our focus in this work is an instructable scene rearranging\nframework that generalizes to longer instructions and to spatial concept\ncompositions never seen at training time. We propose to represent\nlanguage-instructed spatial concepts with energy functions over relative object\narrangements. A language parser maps instructions to corresponding energy\nfunctions and an open-vocabulary visual-language model grounds their arguments\nto relevant objects in the scene. We generate goal scene configurations by\ngradient descent on the sum of energy functions, one per language predicate in\nthe instruction. Local vision-based policies then relocate objects to the\ninferred goal locations. We test our model on established instruction-guided\nmanipulation benchmarks, as well as benchmarks of compositional instructions we\nintroduce. We show our model can execute highly compositional instructions\nzero-shot in simulation and in the real world. It outperforms\nlanguage-to-action reactive policies and Large Language Model planners by a\nlarge margin, especially for long instructions that involve compositions of\nmultiple spatial concepts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gkanatsios_N/0/1/0/all/0/1\">Nikolaos Gkanatsios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ayush Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xian_Z/0/1/0/all/0/1\">Zhou Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunchu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atkeson_C/0/1/0/all/0/1\">Christopher Atkeson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1\">Katerina Fragkiadaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"string2string: A Modern Python Library for String-to-String Algorithms. (arXiv:2304.14395v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14395","description":"<p>We introduce string2string, an open-source library that offers a\ncomprehensive suite of efficient algorithms for a broad range of\nstring-to-string problems. It includes traditional algorithmic solutions as\nwell as recent advanced neural approaches to tackle various problems in string\nalignment, distance measurement, lexical and semantic search, and similarity\nanalysis -- along with several helpful visualization tools and metrics to\nfacilitate the interpretation and analysis of these methods. Notable algorithms\nfeatured in the library include the Smith-Waterman algorithm for pairwise local\nalignment, the Hirschberg algorithm for global alignment, the Wagner-Fisher\nalgorithm for edit distance, BARTScore and BERTScore for similarity analysis,\nthe Knuth-Morris-Pratt algorithm for lexical search, and Faiss for semantic\nsearch. Besides, it wraps existing efficient and widely-used implementations of\ncertain frameworks and metrics, such as sacreBLEU and ROUGE, whenever it is\nappropriate and suitable. Overall, the library aims to provide extensive\ncoverage and increased flexibility in comparison to existing libraries for\nstrings. It can be used for many downstream applications, tasks, and problems\nin natural-language processing, bioinformatics, and computational social\nsciences. It is implemented in Python, easily installable via pip, and\naccessible through a simple API. Source code, documentation, and tutorials are\nall available on our GitHub page: https://github.com/stanfordnlp/string2string.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suzgun_M/0/1/0/all/0/1\">Mirac Suzgun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shieber_S/0/1/0/all/0/1\">Stuart M. Shieber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"We're Afraid Language Models Aren't Modeling Ambiguity. (arXiv:2304.14399v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14399","description":"<p>Ambiguity is an intrinsic feature of natural language. Managing ambiguity is\na key part of human language understanding, allowing us to anticipate\nmisunderstanding as communicators and revise our interpretations as listeners.\nAs language models (LMs) are increasingly employed as dialogue interfaces and\nwriting aids, handling ambiguous language is critical to their success. We\ncharacterize ambiguity in a sentence by its effect on entailment relations with\nanother sentence, and collect AmbiEnt, a linguist-annotated benchmark of 1,645\nexamples with diverse kinds of ambiguity. We design a suite of tests based on\nAmbiEnt, presenting the first evaluation of pretrained LMs to recognize\nambiguity and disentangle possible meanings. We find that the task remains\nextremely challenging, including for the recent GPT-4, whose generated\ndisambiguations are considered correct only 32% of the time in human\nevaluation, compared to 90% for disambiguations in our dataset. Finally, to\nillustrate the value of ambiguity-sensitive tools, we show that a multilabel\nNLI model can flag political claims in the wild that are misleading due to\nambiguity. We encourage the field to rediscover the importance of ambiguity for\nNLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alisa Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhaofeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michael_J/0/1/0/all/0/1\">Julian Michael</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhr_A/0/1/0/all/0/1\">Alane Suhr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koller_A/0/1/0/all/0/1\">Alexander Koller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions. (arXiv:2304.14402v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14402","description":"<p>Large language models (LLMs) with instruction finetuning demonstrate superior\ngenerative capabilities. However, these models are resource intensive. To\nalleviate this issue, we explore distilling knowledge from instruction-tuned\nLLMs to much smaller ones. To this end, we carefully develop a large set of\n2.58M instructions based on both existing and newly-generated instructions. In\naddition to being sizeable, we design our instructions to cover a broad set of\ntopics to ensure. A thorough investigation of our instruction data demonstrate\ntheir diversity, and we generate responses for these instructions using\ngpt-3.5-turbo. We then exploit the instructions to tune a host of models,\ndubbed LaMini-LM, of varying sizes, both from the encoder-decoder as well as\nthe decoder-only families. We evaluate our models both automatically (on 15\ndifferent NLP benchmarks) and manually. Results show that our proposed\nLaMini-LM are on par with competitive baselines while being nearly 10 times\nsmaller in size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waheed_A/0/1/0/all/0/1\">Abdul Waheed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViMQ: A Vietnamese Medical Question Dataset for Healthcare Dialogue System Development. (arXiv:2304.14405v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14405","description":"<p>Existing medical text datasets usually take the form of ques- tion and answer\npairs that support the task of natural language gener- ation, but lacking the\ncomposite annotations of the medical terms. In this study, we publish a\nVietnamese dataset of medical questions from patients with sentence-level and\nentity-level annotations for the Intent Classification and Named Entity\nRecognition tasks. The tag sets for two tasks are in medical domain and can\nfacilitate the development of task- oriented healthcare chatbots with better\ncomprehension of queries from patients. We train baseline models for the two\ntasks and propose a simple self-supervised training strategy with span-noise\nmodelling that substan- tially improves the performance. Dataset and code will\nbe published at https://github.com/tadeephuy/ViMQ\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huy_T/0/1/0/all/0/1\">Ta Duc Huy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_N/0/1/0/all/0/1\">Nguyen Anh Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tran Hoang Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minh_N/0/1/0/all/0/1\">Nguyen Phuc Minh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_N/0/1/0/all/0/1\">Nguyen Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung H. Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1\">Steven Q. H. Truong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models. (arXiv:2203.07281v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07281","description":"<p>Providing natural language instructions in prompts is a useful new paradigm\nfor improving task performance of large language models in a zero-shot setting.\nRecent work has aimed to improve such prompts via manual rewriting or\ngradient-based tuning. However, manual rewriting is time-consuming and requires\nsubjective interpretation, while gradient-based tuning can be extremely\ncomputationally demanding for large models and may not be feasible for\nAPI-based models. In this work, we introduce Gradient-free Instructional Prompt\nSearch (GrIPS), a gradient-free, edit-based search approach for improving task\ninstructions for large language models. GrIPS takes in instructions designed\nfor humans and automatically returns an improved, edited prompt, while allowing\nfor API-based tuning. With InstructGPT models, GrIPS improves the average task\nperformance by up to 4.30 percentage points on eight classification tasks from\nthe Natural Instructions dataset (with similar improvements for OPT, BLOOM, and\nFLAN-T5). We see improvements for both instruction-only prompts and instruction\n+ k-shot examples prompts. Notably, GrIPS outperforms manual rewriting and\npurely example-based prompts while controlling for the available compute and\ndata budget. Further, performance of GrIPS is comparable to select\ngradient-based tuning approaches. Qualitatively, we show our edits can simplify\ninstructions and at times make them incoherent but nonetheless improve\naccuracy. Our code is available at: https://github.com/archiki/GrIPS\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1\">Archiki Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hase_P/0/1/0/all/0/1\">Peter Hase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiTimeBERT: Extending Pre-Trained Language Representations with Bi-Temporal Information. (arXiv:2204.13032v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.13032","description":"<p>Time is an important aspect of documents and is used in a range of NLP and IR\ntasks. In this work, we investigate methods for incorporating temporal\ninformation during pre-training to further improve the performance on\ntime-related tasks. Compared with common pre-trained language models like BERT\nwhich utilize synchronic document collections (e.g., BookCorpus and Wikipedia)\nas the training corpora, we use long-span temporal news article collection for\nbuilding word representations. We introduce BiTimeBERT, a novel language\nrepresentation model trained on a temporal collection of news articles via two\nnew pre-training tasks, which harnesses two distinct temporal signals to\nconstruct time-aware language representations. The experimental results show\nthat BiTimeBERT consistently outperforms BERT and other existing pre-trained\nmodels with substantial gains on different downstream NLP tasks and\napplications for which time is of importance (e.g., the accuracy improvement\nover BERT is 155\\% on the event time estimation task).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1\">Adam Jatowt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshikawa_M/0/1/0/all/0/1\">Masatoshi Yoshikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yi Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PREME: Preference-based Meeting Exploration through an Interactive Questionnaire. (arXiv:2205.02370v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.02370","description":"<p>The recent increase in the volume of online meetings necessitates automated\ntools for managing and organizing the material, especially when an attendee has\nmissed the discussion and needs assistance in quickly exploring it. In this\nwork, we propose a novel end-to-end framework for generating interactive\nquestionnaires for preference-based meeting exploration. As a result, users are\nsupplied with a list of suggested questions reflecting their preferences. Since\nthe task is new, we introduce an automatic evaluation strategy. Namely, it\nmeasures how much the generated questions via questionnaire are answerable to\nensure factual correctness and covers the source meeting for the depth of\npossible exploration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arabzadeh_N/0/1/0/all/0/1\">Negar Arabzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmadvand_A/0/1/0/all/0/1\">Ali Ahmadvand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiseleva_J/0/1/0/all/0/1\">Julia Kiseleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shokouhi_M/0/1/0/all/0/1\">Milad Shokouhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Parallelism Tradeoff: Limitations of Log-Precision Transformers. (arXiv:2207.00729v4 [cs.CC] UPDATED)","link":"http://arxiv.org/abs/2207.00729","description":"<p>Despite their omnipresence in modern NLP, characterizing the computational\npower of transformer neural nets remains an interesting open question. We prove\nthat transformers whose arithmetic precision is logarithmic in the number of\ninput tokens (and whose feedforward nets are computable using space linear in\ntheir input) can be simulated by constant-depth logspace-uniform threshold\ncircuits. This provides insight on the power of transformers using known\nresults in complexity theory. For example, if $\\mathsf L \\neq \\mathsf P$ (i.e.,\nnot all poly-time problems can be solved using logarithmic space), then\ntransformers cannot even accurately solve linear equalities or check membership\nin an arbitrary context-free grammar with empty productions. Our result\nintuitively emerges from the transformer architecture's high parallelizability.\nWe thus speculatively introduce the idea of a fundamental parallelism tradeoff:\nany model architecture as parallelizable as the transformer will obey\nlimitations similar to it. Since parallelism is key to training models at\nmassive scale, this suggests a potential inherent weakness of the scaling\nparadigm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context Generation Improves Open Domain Question Answering. (arXiv:2210.06349v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.06349","description":"<p>Closed-book question answering (QA) requires a model to directly answer an\nopen-domain question without access to any external knowledge. Prior work on\nclosed-book QA either directly finetunes or prompts a pretrained language model\n(LM) to leverage the stored knowledge. However, they do not fully exploit the\nparameterized knowledge. To address this issue, we propose a two-stage,\nclosed-book QA framework which employs a coarse-to-fine approach to extract\nrelevant knowledge and answer a question. Our approach first generates a\nrelated context for a given question by prompting a pretrained LM. We then\nprompt the same LM for answer prediction using the generated context and the\nquestion. Additionally, to eliminate failure caused by context uncertainty, we\nmarginalize over generated contexts. Experimental results on three QA\nbenchmarks show that our method significantly outperforms previous closed-book\nQA methods (e.g. exact matching 68.6% vs. 55.3%), and is on par with open-book\nmethods that exploit external knowledge sources (e.g. 68.6% vs. 68.0%). Our\nmethod is able to better exploit the stored knowledge in pretrained LMs without\nadding extra learnable parameters or needing finetuning, and paves the way for\nhybrid models that integrate pretrained LMs with external knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwary_M/0/1/0/all/0/1\">Mostofa Patwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhumoye_S/0/1/0/all/0/1\">Shrimai Prabhumoye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prenger_R/0/1/0/all/0/1\">Ryan Prenger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Communication breakdown: On the low mutual intelligibility between human and neural captioning. (arXiv:2210.11512v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.11512","description":"<p>We compare the 0-shot performance of a neural caption-based image retriever\nwhen given as input either human-produced captions or captions generated by a\nneural captioner. We conduct this comparison on the recently introduced\nImageCoDe data-set (Krojer et al., 2022) which contains hard distractors nearly\nidentical to the images to be retrieved. We find that the neural retriever has\nmuch higher performance when fed neural rather than human captions, despite the\nfact that the former, unlike the latter, were generated without awareness of\nthe distractors that make the task hard. Even more remarkably, when the same\nneural captions are given to human subjects, their retrieval performance is\nalmost at chance level. Our results thus add to the growing body of evidence\nthat, even when the ``language'' of neural models resembles English, this\nsuperficial resemblance might be deeply misleading.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dessi_R/0/1/0/all/0/1\">Roberto Dess&#xec;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gualdoni_E/0/1/0/all/0/1\">Eleonora Gualdoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franzon_F/0/1/0/all/0/1\">Francesca Franzon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boleda_G/0/1/0/all/0/1\">Gemma Boleda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baroni_M/0/1/0/all/0/1\">Marco Baroni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model and Data Transfer for Cross-Lingual Sequence Labelling in Zero-Resource Settings. (arXiv:2210.12623v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12623","description":"<p>Zero-resource cross-lingual transfer approaches aim to apply supervised\nmodels from a source language to unlabelled target languages. In this paper we\nperform an in-depth study of the two main techniques employed so far for\ncross-lingual zero-resource sequence labelling, based either on data or model\ntransfer. Although previous research has proposed translation and annotation\nprojection (data-based cross-lingual transfer) as an effective technique for\ncross-lingual sequence labelling, in this paper we experimentally demonstrate\nthat high capacity multilingual language models applied in a zero-shot\n(model-based cross-lingual transfer) setting consistently outperform data-based\ncross-lingual transfer approaches. A detailed analysis of our results suggests\nthat this might be due to important differences in language use. More\nspecifically, machine translation often generates a textual signal which is\ndifferent to what the models are exposed to when using gold standard data,\nwhich affects both the fine-tuning and evaluation processes. Our results also\nindicate that data-based cross-lingual transfer approaches remain a competitive\noption when high-capacity multilingual language models are not available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Ferrero_I/0/1/0/all/0/1\">Iker Garc&#xed;a-Ferrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agerri_R/0/1/0/all/0/1\">Rodrigo Agerri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigau_G/0/1/0/all/0/1\">German Rigau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Democratizing Neural Machine Translation with OPUS-MT. (arXiv:2212.01936v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.01936","description":"<p>This paper presents the OPUS ecosystem with a focus on the development of\nopen machine translation models and tools, and their integration into end-user\napplications, development platforms and professional workflows. We discuss our\non-going mission of increasing language coverage and translation quality, and\nalso describe on-going work on the development of modular translation models\nand speed-optimized compact solutions for real-time translation on regular\ndesktops and small devices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tiedemann_J/0/1/0/all/0/1\">J&#xf6;rg Tiedemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aulamo_M/0/1/0/all/0/1\">Mikko Aulamo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakshandaeva_D/0/1/0/all/0/1\">Daria Bakshandaeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boggia_M/0/1/0/all/0/1\">Michele Boggia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gronroos_S/0/1/0/all/0/1\">Stig-Arne Gr&#xf6;nroos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nieminen_T/0/1/0/all/0/1\">Tommi Nieminen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raganato_A/0/1/0/all/0/1\">Alessandro Raganato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherrer_Y/0/1/0/all/0/1\">Yves Scherrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazquez_R/0/1/0/all/0/1\">Raul Vazquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Virpioja_S/0/1/0/all/0/1\">Sami Virpioja</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling. (arXiv:2301.12050v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2301.12050","description":"<p>Reinforcement learning (RL) agents typically learn tabula rasa, without prior\nknowledge of the world. However, if initialized with knowledge of high-level\nsubgoals and transitions between subgoals, RL agents could utilize this\nAbstract World Model (AWM) for planning and exploration. We propose using\nfew-shot large language models (LLMs) to hypothesize an AWM, that will be\nverified through world experience, to improve sample efficiency of RL agents.\nOur DECKARD agent applies LLM-guided exploration to item crafting in Minecraft\nin two phases: (1) the Dream phase where the agent uses an LLM to decompose a\ntask into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase\nwhere the agent learns a modular policy for each subgoal and verifies or\ncorrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and\nthen verifying the AWM based on agent experience not only increases sample\nefficiency over contemporary methods by an order of magnitude but is also\nrobust to and corrects errors in the LLM, successfully blending noisy\ninternet-scale information from LLMs with knowledge grounded in environment\ndynamics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nottingham_K/0/1/0/all/0/1\">Kolby Nottingham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhr_A/0/1/0/all/0/1\">Alane Suhr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_R/0/1/0/all/0/1\">Roy Fox</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning. (arXiv:2301.13808v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.13808","description":"<p>Table-based reasoning has shown remarkable progress in combining deep models\nwith discrete reasoning, which requires reasoning over both free-form natural\nlanguage (NL) questions and structured tabular data. However, previous\ntable-based reasoning solutions usually suffer from significant performance\ndegradation on huge evidence (tables). In addition, most existing methods\nstruggle to reason over complex questions since the required information is\nscattered in different places. To alleviate the above challenges, we exploit\nlarge language models (LLMs) as decomposers for effective table-based\nreasoning, which (i) decompose huge evidence (a huge table) into sub-evidence\n(a small table) to mitigate the interference of useless information for table\nreasoning; and (ii) decompose complex questions into simpler sub-questions for\ntext reasoning. Specifically, we first use the LLMs to break down the evidence\n(tables) involved in the current question, retaining the relevant evidence and\nexcluding the remaining irrelevant evidence from the huge table. In addition,\nwe propose a \"parsing-execution-filling\" strategy to alleviate the\nhallucination dilemma of the chain of thought by decoupling logic and numerical\ncomputation in each step. Extensive experiments show that our method can\neffectively leverage decomposed evidence and questions and outperforms the\nstrong baselines on TabFact, WikiTableQuestion, and FetaQA datasets. Notably,\nour model outperforms human performance for the first time on the TabFact\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yunhu Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1\">Binyuan Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Binhua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Symbolic Discovery of Optimization Algorithms. (arXiv:2302.06675v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.06675","description":"<p>We present a method to formulate algorithm discovery as program search, and\napply it to discover optimization algorithms for deep neural network training.\nWe leverage efficient search techniques to explore an infinite and sparse\nprogram space. To bridge the large generalization gap between proxy and target\ntasks, we also introduce program selection and simplification strategies. Our\nmethod discovers a simple and effective optimization algorithm, $\\textbf{Lion}$\n($\\textit{Evo$\\textbf{L}$ved S$\\textbf{i}$gn M$\\textbf{o}$me$\\textbf{n}$tum}$).\nIt is more memory-efficient than Adam as it only keeps track of the momentum.\nDifferent from adaptive optimizers, its update has the same magnitude for each\nparameter calculated through the sign operation. We compare Lion with widely\nused optimizers, such as Adam and Adafactor, for training a variety of models\non different tasks. On image classification, Lion boosts the accuracy of ViT by\nup to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On\nvision-language contrastive learning, we achieve 88.3% $\\textit{zero-shot}$ and\n91.1% $\\textit{fine-tuning}$ accuracy on ImageNet, surpassing the previous best\nresults by 2% and 0.1%, respectively. On diffusion models, Lion outperforms\nAdam by achieving a better FID score and reducing the training compute by up to\n2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion\nexhibits a similar or better performance compared to Adam. Our analysis of Lion\nreveals that its performance gain grows with the training batch size. It also\nrequires a smaller learning rate than Adam due to the larger norm of the update\nproduced by the sign function. Additionally, we examine the limitations of Lion\nand identify scenarios where its improvements are small or not statistically\nsignificant. The implementation of Lion is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangning Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Da Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Real_E/0/1/0/all/0/1\">Esteban Real</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaiyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1\">Hieu Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xuanyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luong_T/0/1/0/all/0/1\">Thang Luong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yifeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PheME: A deep ensemble framework for improving phenotype prediction from multi-modal data. (arXiv:2303.10794v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2303.10794","description":"<p>Detailed phenotype information is fundamental to accurate diagnosis and risk\nestimation of diseases. As a rich source of phenotype information, electronic\nhealth records (EHRs) promise to empower diagnostic variant interpretation.\nHowever, how to accurately and efficiently extract phenotypes from the\nheterogeneous EHR data remains a challenge. In this work, we present PheME, an\nEnsemble framework using Multi-modality data of structured EHRs and\nunstructured clinical notes for accurate Phenotype prediction. Firstly, we\nemploy multiple deep neural networks to learn reliable representations from the\nsparse structured EHR data and redundant clinical notes. A multi-modal model\nthen aligns multi-modal features onto the same latent space to predict\nphenotypes. Secondly, we leverage ensemble learning to combine outputs from\nsingle-modal models and multi-modal models to improve phenotype predictions. We\nchoose seven diseases to evaluate the phenotyping performance of the proposed\nframework. Experimental results show that using multi-modal data significantly\nimproves phenotype prediction in all diseases, the proposed ensemble learning\nframework can further boost the performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shenghan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Ruixiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Sirui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasmy_L/0/1/0/all/0/1\">Laila Rasmy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhi_D/0/1/0/all/0/1\">Degui Zhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_N/0/1/0/all/0/1\">Na Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Large Language Models. (arXiv:2303.18223v8 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.18223","description":"<p>Language is essentially a complex, intricate system of human expressions\ngoverned by grammatical rules. It poses a significant challenge to develop\ncapable AI algorithms for comprehending and grasping a language. As a major\napproach, language modeling has been widely studied for language understanding\nand generation in the past two decades, evolving from statistical language\nmodels to neural language models. Recently, pre-trained language models (PLMs)\nhave been proposed by pre-training Transformer models over large-scale corpora,\nshowing strong capabilities in solving various NLP tasks. Since researchers\nhave found that model scaling can lead to performance improvement, they further\nstudy the scaling effect by increasing the model size to an even larger size.\nInterestingly, when the parameter scale exceeds a certain level, these enlarged\nlanguage models not only achieve a significant performance improvement but also\nshow some special abilities that are not present in small-scale language\nmodels. To discriminate the difference in parameter scale, the research\ncommunity has coined the term large language models (LLM) for the PLMs of\nsignificant size. Recently, the research on LLMs has been largely advanced by\nboth academia and industry, and a remarkable progress is the launch of ChatGPT,\nwhich has attracted widespread attention from society. The technical evolution\nof LLMs has been making an important impact on the entire AI community, which\nwould revolutionize the way how we develop and use AI algorithms. In this\nsurvey, we review the recent advances of LLMs by introducing the background,\nkey findings, and mainstream techniques. In particular, we focus on four major\naspects of LLMs, namely pre-training, adaptation tuning, utilization, and\ncapacity evaluation. Besides, we also summarize the available resources for\ndeveloping LLMs and discuss the remaining issues for future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yupeng Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1\">Yingqian Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Beichen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zican Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yifan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yushuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhipeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jinhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1\">Ruiyang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xinyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zikang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction. (arXiv:2304.11015v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.11015","description":"<p>We study the problem of decomposing a complex text-to-sql task into smaller\nsub-tasks and how such a decomposition can significantly improve the\nperformance of Large Language Models (LLMs) in the reasoning process. There is\ncurrently a significant gap between the performance of fine-tuned models and\nprompting approaches using LLMs on challenging text-to-sql datasets such as\nSpider. We show that SQL queries, despite their declarative structure, can be\nbroken down into sub-problems and the solutions of those sub-problems can be\nfed into LLMs to significantly improve their performance. Our experiments with\nthree LLMs show that this approach consistently improves their performance by\nroughly 10%, pushing the accuracy of LLMs towards state-of-the-art, and even\nbeating large fine-tuned models on the holdout Spider dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pourreza_M/0/1/0/all/0/1\">Mohammadreza Pourreza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafiei_D/0/1/0/all/0/1\">Davood Rafiei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Neural Networks for Text Classification: A Survey. (arXiv:2304.11534v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.11534","description":"<p>Text Classification is the most essential and fundamental problem in Natural\nLanguage Processing. While numerous recent text classification models applied\nthe sequential deep learning technique, graph neural network-based models can\ndirectly deal with complex structured text data and exploit global information.\nMany real text classification applications can be naturally cast into a graph,\nwhich captures words, documents, and corpus global features. In this survey, we\nbring the coverage of methods up to 2023, including corpus-level and\ndocument-level graph neural networks. We discuss each of these methods in\ndetail, dealing with the graph construction mechanisms and the graph-based\nlearning process. As well as the technological survey, we look at issues behind\nand future directions addressed in text classification using graph neural\nnetworks. We also cover datasets, evaluation metrics, and experiment design and\npresent a summary of published performance on the publicly available\nbenchmarks. Note that we present a comprehensive comparison between different\ntechniques and identify the pros and cons of various evaluation metrics in this\nsurvey.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kunze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yihao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extreme Classification for Answer Type Prediction in Question Answering. (arXiv:2304.12395v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.12395","description":"<p>Semantic answer type prediction (SMART) is known to be a useful step towards\neffective question answering (QA) systems. The SMART task involves predicting\nthe top-$k$ knowledge graph (KG) types for a given natural language question.\nThis is challenging due to the large number of types in KGs. In this paper, we\npropose use of extreme multi-label classification using Transformer models\n(XBERT) by clustering KG types using structural and semantic features based on\nquestion text. We specifically improve the clustering stage of the XBERT\npipeline using textual and structural features derived from KGs. We show that\nthese features can improve end-to-end performance for the SMART task, and yield\nstate-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Setty_V/0/1/0/all/0/1\">Vinay Setty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping. (arXiv:2304.13273v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2304.13273","description":"<p>With the development of Vision-Language Pre-training Models (VLPMs)\nrepresented by CLIP and ALIGN, significant breakthroughs have been achieved for\nassociation-based visual tasks such as image classification and image-text\nretrieval by the zero-shot capability of CLIP without fine-tuning. However,\nCLIP is hard to apply to generation-based tasks. This is due to the lack of\ndecoder architecture and pre-training tasks for generation. Although previous\nworks have created generation capacity for CLIP through additional language\nmodels, a modality gap between the CLIP representations of different modalities\nand the inability of CLIP to model the offset of this gap, which fails the\nconcept to transfer across modalities. To solve the problem, we try to map\nimages/videos to the language modality and generate captions from the language\nmodality. In this paper, we propose the K-nearest-neighbor Cross-modality\nMapping (Knight), a zero-shot method from association to generation. With\ntext-only unsupervised training, Knight achieves state-of-the-art performance\nin zero-shot methods for image captioning and video captioning. Our code is\navailable at https://github.com/junyangwang0410/Knight.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1\">Jitao Sang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond. (arXiv:2304.13712v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.13712","description":"<p>This paper presents a comprehensive and practical guide for practitioners and\nend-users working with Large Language Models (LLMs) in their downstream natural\nlanguage processing (NLP) tasks. We provide discussions and insights into the\nusage of LLMs from the perspectives of models, data, and downstream tasks.\nFirstly, we offer an introduction and brief summary of current GPT- and\nBERT-style LLMs. Then, we discuss the influence of pre-training data, training\ndata, and test data. Most importantly, we provide a detailed discussion about\nthe use and non-use cases of large language models for various natural language\nprocessing tasks, such as knowledge-intensive tasks, traditional natural\nlanguage understanding tasks, natural language generation tasks, emergent\nabilities, and considerations for specific tasks.We present various use cases\nand non-use cases to illustrate the practical applications and limitations of\nLLMs in real-world scenarios. We also try to understand the importance of data\nand the specific challenges associated with each NLP task. Furthermore, we\nexplore the impact of spurious biases on LLMs and delve into other essential\nconsiderations, such as efficiency, cost, and latency, to ensure a\ncomprehensive understanding of deploying LLMs in practice. This comprehensive\nguide aims to provide researchers and practitioners with valuable insights and\nbest practices for working with LLMs, thereby enabling the successful\nimplementation of these models in a wide range of NLP tasks. A curated list of\npractical guide resources of LLMs, regularly updated, can be found at\n\\url{https://github.com/Mooler0410/LLMsPracticalGuide}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingfeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hongye Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Ruixiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaotian Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Qizhang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-04-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}