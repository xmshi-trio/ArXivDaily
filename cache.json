{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-07-13T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"GPT4 is Slightly Helpful for Peer-Review Assistance: A Pilot Study. (arXiv:2307.05492v1 [cs.HC])","link":"http://arxiv.org/abs/2307.05492","description":"<p>In this pilot study, we investigate the use of GPT4 to assist in the\npeer-review process. Our key hypothesis was that GPT-generated reviews could\nachieve comparable helpfulness to human reviewers. By comparing reviews\ngenerated by both human reviewers and GPT models for academic papers submitted\nto a major machine learning conference, we provide initial evidence that\nartificial intelligence can contribute effectively to the peer-review process.\nWe also perform robustness experiments with inserted errors to understand which\nparts of the paper the model tends to focus on. Our findings open new avenues\nfor leveraging machine learning tools to address resource constraints in peer\nreview. The results also shed light on potential enhancements to the review\nprocess and lay the groundwork for further research on scaling oversight in a\ndomain where human-feedback is increasingly a scarce resource.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Robertson_Z/0/1/0/all/0/1\">Zachary Robertson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cases of EFL Secondary Students' Prompt Engineering Pathways to Complete a Writing Task with ChatGPT. (arXiv:2307.05493v1 [cs.HC])","link":"http://arxiv.org/abs/2307.05493","description":"<p>ChatGPT is a state-of-the-art (SOTA) chatbot. Although it has potential to\nsupport English as a foreign language (EFL) students' writing, to effectively\ncollaborate with it, a student must learn to engineer prompts, that is, the\nskill of crafting appropriate instructions so that ChatGPT produces desired\noutputs. However, writing an appropriate prompt for ChatGPT is not\nstraightforward for non-technical users who suffer a trial-and-error process.\nThis paper examines the content of EFL students' ChatGPT prompts when\ncompleting a writing task and explores patterns in the quality and quantity of\nthe prompts. The data come from iPad screen recordings of secondary school EFL\nstudents who used ChatGPT and other SOTA chatbots for the first time to\ncomplete the same writing task. The paper presents a case study of four\ndistinct pathways that illustrate the trial-and-error process and show\ndifferent combinations of prompt content and quantity. The cases contribute\nevidence for the need to provide prompt engineering education in the context of\nthe EFL writing classroom, if students are to move beyond an individual\ntrial-and-error process, learning a greater variety of prompt content and more\nsophisticated prompts to support their writing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Woo_D/0/1/0/all/0/1\">David James Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1\">Kai Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susanto_H/0/1/0/all/0/1\">Hengky Susanto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators. (arXiv:2307.05532v1 [cs.CL])","link":"http://arxiv.org/abs/2307.05532","description":"<p>Large language models that exhibit instruction-following behaviour represent\none of the biggest recent upheavals in conversational interfaces, a trend in\nlarge part fuelled by the release of OpenAI's ChatGPT, a proprietary large\nlanguage model for text generation fine-tuned through reinforcement learning\nfrom human feedback (LLM+RLHF). We review the risks of relying on proprietary\nsoftware and survey the first crop of open-source projects of comparable\narchitecture and functionality. The main contribution of this paper is to show\nthat openness is differentiated, and to offer scientific documentation of\ndegrees of openness in this fast-moving field. We evaluate projects in terms of\nopenness of code, training data, model weights, RLHF data, licensing,\nscientific documentation, and access methods. We find that while there is a\nfast-growing list of projects billing themselves as 'open source', many inherit\nundocumented data of dubious legality, few share the all-important\ninstruction-tuning (a key site where human annotation labour is involved), and\ncareful scientific documentation is exceedingly rare. Degrees of openness are\nrelevant to fairness and accountability at all points, from data collection and\ncuration to model architecture, and from training and fine-tuning to release\nand deployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liesenfeld_A/0/1/0/all/0/1\">Andreas Liesenfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1\">Alianda Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dingemanse_M/0/1/0/all/0/1\">Mark Dingemanse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advancements in Scientific Controllable Text Generation Methods. (arXiv:2307.05538v1 [cs.CL])","link":"http://arxiv.org/abs/2307.05538","description":"<p>The previous work on controllable text generation is organized using a new\nschema we provide in this study. Seven components make up the schema, and each\none is crucial to the creation process. To accomplish controlled generation for\nscientific literature, we describe the various modulation strategies utilised\nto modulate each of the seven components. We also offer a theoretical study and\nqualitative examination of these methods. This insight makes possible new\narchitectures based on combinations of these components. Future research will\ncompare these methods empirically to learn more about their strengths and\nutility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goel_A/0/1/0/all/0/1\">Arnav Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hira_M/0/1/0/all/0/1\">Medha Hira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1\">Avinash Anand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bangar_S/0/1/0/all/0/1\">Siddhesh Bangar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1\">Dr. Rajiv Ratn Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Review of feedback in Automated Essay Scoring. (arXiv:2307.05553v1 [cs.CL])","link":"http://arxiv.org/abs/2307.05553","description":"<p>The first automated essay scoring system was developed 50 years ago.\nAutomated essay scoring systems are developing into systems with richer\nfunctions than the previous simple scoring systems. Its purpose is not only to\nscore essays but also as a learning tool to improve the writing skill of users.\nFeedback is the most important aspect of making an automated essay scoring\nsystem useful in real life. The importance of feedback was already emphasized\nin the first AES system. This paper reviews research on feedback including\ndifferent feedback types and essay traits on automated essay scoring. We also\nreviewed the latest case studies of the automated essay scoring system that\nprovides feedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jong_Y/0/1/0/all/0/1\">You-Jin Jong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yong-Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ri_O/0/1/0/all/0/1\">Ok-Chol Ri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Coding at Scale: Design and Deployment of a Nationwide System for Normalizing Referrals in the Chilean Public Healthcare System. (arXiv:2307.05560v1 [cs.CL])","link":"http://arxiv.org/abs/2307.05560","description":"<p>The disease coding task involves assigning a unique identifier from a\ncontrolled vocabulary to each disease mentioned in a clinical document. This\ntask is relevant since it allows information extraction from unstructured data\nto perform, for example, epidemiological studies about the incidence and\nprevalence of diseases in a determined context. However, the manual coding\nprocess is subject to errors as it requires medical personnel to be competent\nin coding rules and terminology. In addition, this process consumes a lot of\ntime and energy, which could be allocated to more clinically relevant tasks.\nThese difficulties can be addressed by developing computational systems that\nautomatically assign codes to diseases. In this way, we propose a two-step\nsystem for automatically coding diseases in referrals from the Chilean public\nhealthcare system. Specifically, our model uses a state-of-the-art NER model\nfor recognizing disease mentions and a search engine system based on\nElasticsearch for assigning the most relevant codes associated with these\ndisease mentions. The system's performance was evaluated on referrals manually\ncoded by clinical experts. Our system obtained a MAP score of 0.63 for the\nsubcategory level and 0.83 for the category level, close to the best-performing\nmodels in the literature. This system could be a support tool for health\nprofessionals, optimizing the coding and management process. Finally, to\nguarantee reproducibility, we publicly release the code of our models and\nexperiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Villena_F/0/1/0/all/0/1\">Fabi&#xe1;n Villena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rojas_M/0/1/0/all/0/1\">Mat&#xed;as Rojas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arias_F/0/1/0/all/0/1\">Felipe Arias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacheco_J/0/1/0/all/0/1\">Jorge Pacheco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vera_P/0/1/0/all/0/1\">Paulina Vera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunstan_J/0/1/0/all/0/1\">Jocelyn Dunstan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenters at SemEval-2023 Task 1: Enhancing CLIP in Handling Compositionality and Ambiguity for Zero-Shot Visual WSD through Prompt Augmentation and Text-To-Image Diffusion. (arXiv:2307.05564v1 [cs.CL])","link":"http://arxiv.org/abs/2307.05564","description":"<p>This paper describes our zero-shot approaches for the Visual Word Sense\nDisambiguation (VWSD) Task in English. Our preliminary study shows that the\nsimple approach of matching candidate images with the phrase using CLIP suffers\nfrom the many-to-many nature of image-text pairs. We find that the CLIP text\nencoder may have limited abilities in capturing the compositionality in natural\nlanguage. Conversely, the descriptive focus of the phrase varies from instance\nto instance. We address these issues in our two systems, Augment-CLIP and\nStable Diffusion Sampling (SD Sampling). Augment-CLIP augments the text prompt\nby generating sentences that contain the context phrase with the help of large\nlanguage models (LLMs). We further explore CLIP models in other languages, as\nthe an ambiguous word may be translated into an unambiguous one in the other\nlanguage. SD Sampling uses text-to-image Stable Diffusion to generate multiple\nimages from the given phrase, increasing the likelihood that a subset of images\nmatch the one that paired with the text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie S. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiue_Y/0/1/0/all/0/1\">Yow-Ting Shiue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shih_Y/0/1/0/all/0/1\">Yong-Siang Shih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1\">Jonas Geiping</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event Extraction as Question Generation and Answering. (arXiv:2307.05567v1 [cs.CL])","link":"http://arxiv.org/abs/2307.05567","description":"<p>Recent work on Event Extraction has reframed the task as Question Answering\n(QA), with promising results. The advantage of this approach is that it\naddresses the error propagation issue found in traditional token-based\nclassification approaches by directly predicting event arguments without\nextracting candidates first. However, the questions are typically based on\nfixed templates and they rarely leverage contextual information such as\nrelevant arguments. In addition, prior QA-based approaches have difficulty\nhandling cases where there are multiple arguments for the same role. In this\npaper, we propose QGA-EE, which enables a Question Generation (QG) model to\ngenerate questions that incorporate rich contextual information instead of\nusing fixed templates. We also propose dynamic templates to assist the training\nof QG model. Experiments show that QGA-EE outperforms all prior\nsingle-task-based models on the ACE05 English dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">Di Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ran_S/0/1/0/all/0/1\">Shihao Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tetreault_J/0/1/0/all/0/1\">Joel Tetreault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaimes_A/0/1/0/all/0/1\">Alejandro Jaimes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hate Speech Detection via Dual Contrastive Learning. (arXiv:2307.05578v1 [cs.CL])","link":"http://arxiv.org/abs/2307.05578","description":"<p>The fast spread of hate speech on social media impacts the Internet\nenvironment and our society by increasing prejudice and hurting people.\nDetecting hate speech has aroused broad attention in the field of natural\nlanguage processing. Although hate speech detection has been addressed in\nrecent work, this task still faces two inherent unsolved challenges. The first\nchallenge lies in the complex semantic information conveyed in hate speech,\nparticularly the interference of insulting words in hate speech detection. The\nsecond challenge is the imbalanced distribution of hate speech and non-hate\nspeech, which may significantly deteriorate the performance of models. To\ntackle these challenges, we propose a novel dual contrastive learning (DCL)\nframework for hate speech detection. Our framework jointly optimizes the\nself-supervised and the supervised contrastive learning loss for capturing\nspan-level information beyond the token-level emotional semantics used in\nexisting models, particularly detecting speech containing abusive and insulting\nwords. Moreover, we integrate the focal loss into the dual contrastive learning\nframework to alleviate the problem of data imbalance. We conduct experiments on\ntwo publicly available English datasets, and experimental results show that the\nproposed model outperforms the state-of-the-art models and precisely detects\nhate speeches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Junyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongfei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaokun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaoqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tongyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_L/0/1/0/all/0/1\">Linlin Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fenglong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SITTA: A Semantic Image-Text Alignment for Image Captioning. (arXiv:2307.05591v1 [cs.CV])","link":"http://arxiv.org/abs/2307.05591","description":"<p>Textual and semantic comprehension of images is essential for generating\nproper captions. The comprehension requires detection of objects, modeling of\nrelations between them, an assessment of the semantics of the scene and,\nfinally, representing the extracted knowledge in a language space. To achieve\nrich language capabilities while ensuring good image-language mappings,\npretrained language models (LMs) were conditioned on pretrained multi-modal\n(image-text) models that allow for image inputs. This requires an alignment of\nthe image representation of the multi-modal model with the language\nrepresentations of a generative LM. However, it is not clear how to best\ntransfer semantics detected by the vision encoder of the multi-modal model to\nthe LM. We introduce two novel ways of constructing a linear mapping that\nsuccessfully transfers semantics between the embedding spaces of the two\npretrained models. The first aligns the embedding space of the multi-modal\nlanguage encoder with the embedding space of the pretrained LM via token\ncorrespondences. The latter leverages additional data that consists of\nimage-text pairs to construct the mapping directly from vision to language\nspace. Using our semantic mappings, we unlock image captioning for LMs without\naccess to gradient information. By using different sources of data we achieve\nstrong captioning performance on MS-COCO and Flickr30k datasets. Even in the\nface of limited data, our method partly exceeds the performance of other\nzero-shot and even finetuned competitors. Our ablation studies show that even\nLMs at a scale of merely 250M parameters can generate decent captions employing\nour semantic mappings. Our approach makes image captioning more accessible for\ninstitutions with restricted computational resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paischer_F/0/1/0/all/0/1\">Fabian Paischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adler_T/0/1/0/all/0/1\">Thomas Adler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmarcher_M/0/1/0/all/0/1\">Markus Hofmarcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1\">Sepp Hochreiter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Separate-and-Aggregate: A Transformer-based Patch Refinement Model for Knowledge Graph Completion. (arXiv:2307.05627v1 [cs.CL])","link":"http://arxiv.org/abs/2307.05627","description":"<p>Knowledge graph completion (KGC) is the task of inferencing missing facts\nfrom any given knowledge graphs (KG). Previous KGC methods typically represent\nknowledge graph entities and relations as trainable continuous embeddings and\nfuse the embeddings of the entity $h$ (or $t$) and relation $r$ into hidden\nrepresentations of query $(h, r, ?)$ (or $(?, r, t$)) to approximate the\nmissing entities. To achieve this, they either use shallow linear\ntransformations or deep convolutional modules. However, the linear\ntransformations suffer from the expressiveness issue while the deep\nconvolutional modules introduce unnecessary inductive bias, which could\npotentially degrade the model performance. Thus, we propose a novel\nTransformer-based Patch Refinement Model (PatReFormer) for KGC. PatReFormer\nfirst segments the embedding into a sequence of patches and then employs\ncross-attention modules to allow bi-directional embedding feature interaction\nbetween the entities and relations, leading to a better understanding of the\nunderlying KG. We conduct experiments on four popular KGC benchmarks, WN18RR,\nFB15k-237, YAGO37 and DB100K. The experimental results show significant\nperformance improvement from existing KGC methods on standard KGC evaluation\nmetrics, e.g., MRR and H@n. Our analysis first verifies the effectiveness of\nour model design choices in PatReFormer. We then find that PatReFormer can\nbetter capture KG information from a large relation embedding dimension.\nFinally, we demonstrate that the strength of PatReFormer is at complex relation\ntypes, compared to other KGC models\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1\">Quan Z. Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1\">Kwok-Yan Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better Handling Coreference Resolution in Aspect Level Sentiment Classification by Fine-Tuning Language Models. (arXiv:2307.05646v1 [cs.CL])","link":"http://arxiv.org/abs/2307.05646","description":"<p>Customer feedback is invaluable to companies as they refine their products.\nMonitoring customer feedback can be automated with Aspect Level Sentiment\nClassification (ALSC) which allows us to analyse specific aspects of the\nproducts in reviews. Large Language Models (LLMs) are the heart of many\nstate-of-the-art ALSC solutions, but they perform poorly in some scenarios\nrequiring Coreference Resolution (CR). In this work, we propose a framework to\nimprove an LLM's performance on CR-containing reviews by fine tuning on highly\ninferential tasks. We show that the performance improvement is likely\nattributed to the improved model CR ability. We also release a new dataset that\nfocuses on CR in ALSC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mullick_D/0/1/0/all/0/1\">Dhruv Mullick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bilal Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fyshe_A/0/1/0/all/0/1\">Alona Fyshe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stack More Layers Differently: High-Rank Training Through Low-Rank Updates. (arXiv:2307.05695v1 [cs.CL])","link":"http://arxiv.org/abs/2307.05695","description":"<p>Despite the dominance and effectiveness of scaling, resulting in large\nnetworks with hundreds of billions of parameters, the necessity to train\noverparametrized models remains poorly understood, and alternative approaches\ndo not necessarily make it cheaper to train high-performance models. In this\npaper, we explore low-rank training techniques as an alternative approach to\ntraining large neural networks. We introduce a novel method called ReLoRA,\nwhich utilizes low-rank updates to train high-rank networks. We apply ReLoRA to\npre-training transformer language models with up to 350M parameters and\ndemonstrate comparable performance to regular neural network training.\nFurthermore, we observe that the efficiency of ReLoRA increases with model\nsize, making it a promising approach for training multi-billion-parameter\nnetworks efficiently. Our findings shed light on the potential of low-rank\ntraining techniques and their implications for scaling laws.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lialin_V/0/1/0/all/0/1\">Vladislav Lialin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shivagunde_N/0/1/0/all/0/1\">Namrata Shivagunde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muckatira_S/0/1/0/all/0/1\">Sherin Muckatira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1\">Anna Rumshisky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Personalized Reinforcement Learning Summarization Service for Learning Structure from Unstructured Data. (arXiv:2307.05696v1 [cs.IR])","link":"http://arxiv.org/abs/2307.05696","description":"<p>The exponential growth of textual data has created a crucial need for tools\nthat assist users in extracting meaningful insights. Traditional document\nsummarization approaches often fail to meet individual user requirements and\nlack structure for efficient information processing. To address these\nlimitations, we propose Summation, a hierarchical personalized concept-based\nsummarization approach. It synthesizes documents into a concise hierarchical\nconcept map and actively engages users by learning and adapting to their\npreferences. Using a Reinforcement Learning algorithm, Summation generates\npersonalized summaries for unseen documents on specific topics. This framework\nenhances comprehension, enables effective navigation, and empowers users to\nextract meaningful insights from large document collections aligned with their\nunique requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghodratnama_S/0/1/0/all/0/1\">Samira Ghodratnama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beheshti_A/0/1/0/all/0/1\">Amin Beheshti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakershahrak_M/0/1/0/all/0/1\">Mehrdad Zakershahrak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations. (arXiv:2307.05722v1 [cs.AI])","link":"http://arxiv.org/abs/2307.05722","description":"<p>Large Language Models (LLMs) have revolutionized natural language processing\ntasks, demonstrating their exceptional capabilities in various domains.\nHowever, their potential for behavior graph understanding in job\nrecommendations remains largely unexplored. This paper focuses on unveiling the\ncapability of large language models in understanding behavior graphs and\nleveraging this understanding to enhance recommendations in online recruitment,\nincluding the promotion of out-of-distribution (OOD) application. We present a\nnovel framework that harnesses the rich contextual information and semantic\nrepresentations provided by large language models to analyze behavior graphs\nand uncover underlying patterns and relationships. Specifically, we propose a\nmeta-path prompt constructor that leverages LLM recommender to understand\nbehavior graphs for the first time and design a corresponding path augmentation\nmodule to alleviate the prompt bias introduced by path-based sequence input. By\nleveraging this capability, our framework enables personalized and accurate job\nrecommendations for individual users. We evaluate the effectiveness of our\napproach on a comprehensive dataset and demonstrate its ability to improve the\nrelevance and quality of recommended quality. This research not only sheds\nlight on the untapped potential of large language models but also provides\nvaluable insights for developing advanced recommendation systems in the\nrecruitment market. The findings contribute to the growing field of natural\nlanguage processing and offer practical implications for enhancing job search\nexperiences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Likang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhaopeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hengshu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust and Efficient Continual Language Learning. (arXiv:2307.05741v1 [cs.CL])","link":"http://arxiv.org/abs/2307.05741","description":"<p>As the application space of language models continues to evolve, a natural\nquestion to ask is how we can quickly adapt models to new tasks. We approach\nthis classic question from a continual learning perspective, in which we aim to\ncontinue fine-tuning models trained on past tasks on new tasks, with the goal\nof \"transferring\" relevant knowledge. However, this strategy also runs the risk\nof doing more harm than good, i.e., negative transfer. In this paper, we\nconstruct a new benchmark of task sequences that target different possible\ntransfer scenarios one might face, such as a sequence of tasks with high\npotential of positive transfer, high potential for negative transfer, no\nexpected effect, or a mixture of each. An ideal learner should be able to\nmaximally exploit information from all tasks that have any potential for\npositive transfer, while also avoiding the negative effects of any distracting\ntasks that may confuse it. We then propose a simple, yet effective, learner\nthat satisfies many of our desiderata simply by leveraging a selective strategy\nfor initializing new models from past task checkpoints. Still, limitations\nremain, and we hope this benchmark can help the community to further build and\nanalyze such learners.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fisch_A/0/1/0/all/0/1\">Adam Fisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rannen_Triki_A/0/1/0/all/0/1\">Amal Rannen-Triki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1\">Razvan Pascanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bornschein_J/0/1/0/all/0/1\">J&#xf6;rg Bornschein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazaridou_A/0/1/0/all/0/1\">Angeliki Lazaridou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gribovskaya_E/0/1/0/all/0/1\">Elena Gribovskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranzato_M/0/1/0/all/0/1\">Marc&#x27;Aurelio Ranzato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Machine Translation Data Generation and Augmentation using ChatGPT. (arXiv:2307.05779v1 [cs.CL])","link":"http://arxiv.org/abs/2307.05779","description":"<p>Neural models have revolutionized the field of machine translation, but\ncreating parallel corpora is expensive and time-consuming. We investigate an\nalternative to manual parallel corpora - hallucinated parallel corpora created\nby generative language models. Although these models are themselves trained on\nparallel data, they can leverage a multilingual vector space to create data,\nand may be able to supplement small manually-procured corpora. Our experiments\nhighlight two key findings - despite a lack of diversity in their output, the\nhallucinated data improves the translation signal, even when the domain clashes\nwith the original dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wayne Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicolai_G/0/1/0/all/0/1\">Garrett Nicolai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models. (arXiv:2307.05782v1 [cs.CL])","link":"http://arxiv.org/abs/2307.05782","description":"<p>Artificial intelligence is making spectacular progress, and one of the best\nexamples is the development of large language models (LLMs) such as OpenAI's\nGPT series. In these lectures, written for readers with a background in\nmathematics or physics, we give a brief history and survey of the state of the\nart, and describe the underlying transformer architecture in detail. We then\nexplore some current ideas on how LLMs work and how models trained to predict\nthe next word in a text are able to perform other tasks displaying\nintelligence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Douglas_M/0/1/0/all/0/1\">Michael R. Douglas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved POS tagging for spontaneous, clinical speech using data augmentation. (arXiv:2307.05796v1 [cs.CL])","link":"http://arxiv.org/abs/2307.05796","description":"<p>This paper addresses the problem of improving POS tagging of transcripts of\nspeech from clinical populations. In contrast to prior work on parsing and POS\ntagging of transcribed speech, we do not make use of an in domain treebank for\ntraining. Instead, we train on an out of domain treebank of newswire using data\naugmentation techniques to make these structures resemble natural, spontaneous\nspeech. We trained a parser with and without the augmented data and tested its\nperformance using manually validated POS tags in clinical speech produced by\npatients with various types of neurodegenerative conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulick_S/0/1/0/all/0/1\">Seth Kulick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryant_N/0/1/0/all/0/1\">Neville Ryant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irwin_D/0/1/0/all/0/1\">David J. Irwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nevler_N/0/1/0/all/0/1\">Naomi Nevler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sunghye Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relational Extraction on Wikipedia Tables using Convolutional and Memory Networks. (arXiv:2307.05827v1 [cs.CL])","link":"http://arxiv.org/abs/2307.05827","description":"<p>Relation extraction (RE) is the task of extracting relations between entities\nin text. Most RE methods extract relations from free-form running text and\nleave out other rich data sources, such as tables. We explore RE from the\nperspective of applying neural methods on tabularly organized data. We\nintroduce a new model consisting of Convolutional Neural Network (CNN) and\nBidirectional-Long Short Term Memory (BiLSTM) network to encode entities and\nlearn dependencies among them, respectively. We evaluate our model on a large\nand recent dataset and compare results with previous neural methods.\nExperimental results show that our model consistently outperforms the previous\nmodel for the task of relation extraction on tabular data. We perform\ncomprehensive error analyses and ablation study to show the contribution of\nvarious components of our model. Finally, we discuss the usefulness and\ntrade-offs of our approach, and provide suggestions for fostering further\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahriar_A/0/1/0/all/0/1\">Arif Shahriar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_R/0/1/0/all/0/1\">Rohan Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbosa_D/0/1/0/all/0/1\">Denilson Barbosa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding. (arXiv:2307.05908v1 [cs.CL])","link":"http://arxiv.org/abs/2307.05908","description":"<p>This paper presents \"Predictive Pipelined Decoding (PPD),\" an approach that\nspeeds up greedy decoding in Large Language Models (LLMs) while maintaining the\nexact same output as the original decoding. Unlike conventional strategies, PPD\nemploys additional compute resources to parallelize the initiation of\nsubsequent token decoding during the current token decoding. This innovative\nmethod reduces decoding latency and reshapes the understanding of trade-offs in\nLLM decoding strategies. We have developed a theoretical framework that allows\nus to analyze the trade-off between computation and latency. Using this\nframework, we can analytically estimate the potential reduction in latency\nassociated with our proposed method, achieved through the assessment of the\nmatch rate, represented as p_correct. The results demonstrate that the use of\nextra computational resources has the potential to accelerate LLM greedy\ndecoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Seongjun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gibbeum Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaewoong Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1\">Dimitris Papailiopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kangwook Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prototypical Contrastive Transfer Learning for Multimodal Language Understanding. (arXiv:2307.05942v1 [cs.RO])","link":"http://arxiv.org/abs/2307.05942","description":"<p>Although domestic service robots are expected to assist individuals who\nrequire support, they cannot currently interact smoothly with people through\nnatural language. For example, given the instruction \"Bring me a bottle from\nthe kitchen,\" it is difficult for such robots to specify the bottle in an\nindoor environment. Most conventional models have been trained on real-world\ndatasets that are labor-intensive to collect, and they have not fully leveraged\nsimulation data through a transfer learning framework. In this study, we\npropose a novel transfer learning approach for multimodal language\nunderstanding called Prototypical Contrastive Transfer Learning (PCTL), which\nuses a new contrastive loss called Dual ProtoNCE. We introduce PCTL to the task\nof identifying target objects in domestic environments according to free-form\nnatural language instructions. To validate PCTL, we built new real-world and\nsimulation datasets. Our experiment demonstrated that PCTL outperformed\nexisting methods. Specifically, PCTL achieved an accuracy of 78.1%, whereas\nsimple fine-tuning achieved an accuracy of 73.4%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Otsuki_S/0/1/0/all/0/1\">Seitaro Otsuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishikawa_S/0/1/0/all/0/1\">Shintaro Ishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Komei Sugiura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models. (arXiv:2307.05972v1 [cs.CL])","link":"http://arxiv.org/abs/2307.05972","description":"<p>We investigate the effects of post-training quantization and\nquantization-aware training on the generalization of Transformer language\nmodels. We present a new method called self-distilled quantization (SDQ) that\nminimizes accumulative quantization errors and outperforms baselines. We apply\nSDQ to multilingual models XLM-R-Base and InfoXLM-Base and demonstrate that\nboth models can be reduced from 32-bit floating point weights to 8-bit integer\nweights while maintaining a high level of performance on the XGLUE benchmark.\nOur results also highlight the challenges of quantizing multilingual models,\nwhich must generalize to languages they were not fine-tuned on.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neill_J/0/1/0/all/0/1\">James O&#x27; Neill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Sourav Dutta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models. (arXiv:2307.05973v1 [cs.RO])","link":"http://arxiv.org/abs/2307.05973","description":"<p>Large language models (LLMs) are shown to possess a wealth of actionable\nknowledge that can be extracted for robot manipulation in the form of reasoning\nand planning. Despite the progress, most still rely on pre-defined motion\nprimitives to carry out the physical interactions with the environment, which\nremains a major bottleneck. In this work, we aim to synthesize robot\ntrajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a\nlarge variety of manipulation tasks given an open-set of instructions and an\nopen-set of objects. We achieve this by first observing that LLMs excel at\ninferring affordances and constraints given a free-form language instruction.\nMore importantly, by leveraging their code-writing capabilities, they can\ninteract with a visual-language model (VLM) to compose 3D value maps to ground\nthe knowledge into the observation space of the agent. The composed value maps\nare then used in a model-based planning framework to zero-shot synthesize\nclosed-loop robot trajectories with robustness to dynamic perturbations. We\nfurther demonstrate how the proposed framework can benefit from online\nexperiences by efficiently learning a dynamics model for scenes that involve\ncontact-rich interactions. We present a large-scale study of the proposed\nmethod in both simulated and real-robot environments, showcasing the ability to\nperform a large variety of everyday manipulation tasks specified in free-form\nnatural language. Project website: https://voxposer.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenlong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunzhu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DDNAS: Discretized Differentiable Neural Architecture Search for Text Classification. (arXiv:2307.06005v1 [cs.CL])","link":"http://arxiv.org/abs/2307.06005","description":"<p>Neural Architecture Search (NAS) has shown promising capability in learning\ntext representation. However, existing text-based NAS neither performs a\nlearnable fusion of neural operations to optimize the architecture, nor encodes\nthe latent hierarchical categorization behind text input. This paper presents a\nnovel NAS method, Discretized Differentiable Neural Architecture Search\n(DDNAS), for text representation learning and classification. With the\ncontinuous relaxation of architecture representation, DDNAS can use gradient\ndescent to optimize the search. We also propose a novel discretization layer\nvia mutual information maximization, which is imposed on every search node to\nmodel the latent hierarchical categorization in text representation. Extensive\nexperiments conducted on eight diverse real datasets exhibit that DDNAS can\nconsistently outperform the state-of-the-art NAS methods. While DDNAS relies on\nonly three basic operations, i.e., convolution, pooling, and none, to be the\ncandidates of NAS building blocks, its promising performance is noticeable and\nextensible to obtain further improvement by adding more different operations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kuan-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cheng-Te Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kuo-Jung Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PolyLM: An Open Source Polyglot Large Language Model. (arXiv:2307.06018v1 [cs.CL])","link":"http://arxiv.org/abs/2307.06018","description":"<p>Large language models (LLMs) demonstrate remarkable ability to comprehend,\nreason, and generate following nature language instructions. However, the\ndevelopment of LLMs has been primarily focused on high-resource languages, such\nas English, thereby limiting their applicability and research in other\nlanguages. Consequently, we present PolyLM, a multilingual LLM trained on 640\nbillion (B) tokens, avaliable in two model sizes: 1.7B and 13B. To enhance its\nmultilingual capabilities, we 1) integrate bilingual data into training data;\nand 2) adopt a curriculum learning strategy that increases the proportion of\nnon-English data from 30% in the first stage to 60% in the final stage during\npre-training. Further, we propose a multilingual self-instruct method which\nautomatically generates 132.7K diverse multilingual instructions for model\nfine-tuning. To assess the model's performance, we collect several existing\nmultilingual tasks, including multilingual understanding, question answering,\ngeneration, and translation. Extensive experiments show that PolyLM surpasses\nother open-source models such as LLaMA and BLOOM on multilingual tasks while\nmaintaining comparable performance in English. Our models, alone with the\ninstruction data and multilingual benchmark, are available at:\n\\url{https://modelscope.cn/models/damo/nlp_polylm_13b_text_generation}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiangpeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Haoran Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Huan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xingzhang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yu Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhiwei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_B/0/1/0/all/0/1\">Binbin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tianxiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shangjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1\">Binyuan Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bowen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dayiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baosong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pluggable Neural Machine Translation Models via Memory-augmented Adapters. (arXiv:2307.06029v1 [cs.CL])","link":"http://arxiv.org/abs/2307.06029","description":"<p>Although neural machine translation (NMT) models perform well in the general\ndomain, it remains rather challenging to control their generation behavior to\nsatisfy the requirement of different users. Given the expensive training cost\nand the data scarcity challenge of learning a new model from scratch for each\nuser requirement, we propose a memory-augmented adapter to steer pretrained NMT\nmodels in a pluggable manner. Specifically, we construct a multi-granular\nmemory based on the user-provided text samples and propose a new adapter\narchitecture to combine the model representations and the retrieved results. We\nalso propose a training strategy using memory dropout to reduce spurious\ndependencies between the NMT model and the memory. We validate our approach on\nboth style- and domain-specific experiments and the results indicate that our\nmethod can outperform several representative pluggable baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuzhuang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weidong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Study on the Appropriate size of the Mongolian general corpus. (arXiv:2307.06050v1 [cs.CL])","link":"http://arxiv.org/abs/2307.06050","description":"<p>This study aims to determine the appropriate size of the Mongolian general\ncorpus. This study used the Heaps function and Type Token Ratio to determine\nthe appropriate size of the Mongolian general corpus. The sample corpus of\n906,064 tokens comprised texts from 10 domains of newspaper politics, economy,\nsociety, culture, sports, world articles and laws, middle and high school\nliterature textbooks, interview articles, and podcast transcripts. First, we\nestimated the Heaps function with this sample corpus. Next, we observed changes\nin the number of types and TTR values while increasing the number of tokens by\none million using the estimated Heaps function. As a result of observation, we\nfound that the TTR value hardly changed when the number of tokens exceeded from\n39 to 42 million. Thus, we conclude that an appropriate size for a Mongolian\ngeneral corpus is from 39 to 42 million tokens.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Sunsoo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsend_G/0/1/0/all/0/1\">Ganbat Tsend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpreting deep embeddings for disease progression clustering. (arXiv:2307.06060v1 [stat.ML])","link":"http://arxiv.org/abs/2307.06060","description":"<p>We propose a novel approach for interpreting deep embeddings in the context\nof patient clustering. We evaluate our approach on a dataset of participants\nwith type 2 diabetes from the UK Biobank, and demonstrate clinically meaningful\ninsights into disease progression patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Munoz_Farre_A/0/1/0/all/0/1\">Anna Munoz-Farre</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Poulakakis_Daktylidis_A/0/1/0/all/0/1\">Antonios Poulakakis-Daktylidis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kothalawala_D/0/1/0/all/0/1\">Dilini Mahesha Kothalawala</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rodriguez_Martinez_A/0/1/0/all/0/1\">Andrea Rodriguez-Martinez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View. (arXiv:2307.06082v1 [cs.AI])","link":"http://arxiv.org/abs/2307.06082","description":"<p>Incremental decision making in real-world environments is one of the most\nchallenging tasks in embodied artificial intelligence. One particularly\ndemanding scenario is Vision and Language Navigation~(VLN) which requires\nvisual and natural language understanding as well as spatial and temporal\nreasoning capabilities. The embodied agent needs to ground its understanding of\nnavigation instructions in observations of a real-world environment like Street\nView. Despite the impressive results of LLMs in other research areas, it is an\nongoing problem of how to best connect them with an interactive visual\nenvironment. In this work, we propose VELMA, an embodied LLM agent that uses a\nverbalization of the trajectory and of visual environment observations as\ncontextual prompt for the next action. Visual information is verbalized by a\npipeline that extracts landmarks from the human written navigation instructions\nand uses CLIP to determine their visibility in the current panorama view. We\nshow that VELMA is able to successfully follow navigation instructions in\nStreet View with only two in-context examples. We further finetune the LLM\nagent on a few thousand examples and achieve 25%-30% relative improvement in\ntask completion over the previous state-of-the-art for two datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schumann_R/0/1/0/all/0/1\">Raphael Schumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanrong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Weixi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1\">Tsu-Jui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1\">Stefan Riezler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Portuguese Sign Language Animation with Dynamic Timing and Mouthing. (arXiv:2307.06124v1 [cs.CL])","link":"http://arxiv.org/abs/2307.06124","description":"<p>Current signing avatars are often described as unnatural as they cannot\naccurately reproduce all the subtleties of synchronized body behaviors of a\nhuman signer. In this paper, we propose a new dynamic approach for transitions\nbetween signs, focusing on mouthing animations for Portuguese Sign Language.\nAlthough native signers preferred animations with dynamic transitions, we did\nnot find significant differences in comprehension and perceived naturalness\nscores. On the other hand, we show that including mouthing behaviors improved\ncomprehension and perceived naturalness for novice sign language learners.\nResults have implications in computational linguistics, human-computer\ninteraction, and synthetic animation of signing avatars.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lacerda_I/0/1/0/all/0/1\">In&#xea;s Lacerda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicolau_H/0/1/0/all/0/1\">Hugo Nicolau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coheur_L/0/1/0/all/0/1\">Luisa Coheur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems. (arXiv:2307.06187v1 [cs.MA])","link":"http://arxiv.org/abs/2307.06187","description":"<p>In autonomic computing, self-adaptation has been proposed as a fundamental\nparadigm to manage the complexity of multiagent systems (MASs). This achieved\nby extending a system with support to monitor and adapt itself to achieve\nspecific concerns of interest. Communication in these systems is key given that\nin scenarios involving agent interaction, it enhances cooperation and reduces\ncoordination challenges by enabling direct, clear information exchange.\nHowever, improving the expressiveness of the interaction communication with\nMASs is not without challenges. In this sense, the interplay between\nself-adaptive systems and effective communication is crucial for future MAS\nadvancements. In this paper, we propose the integration of large language\nmodels (LLMs) such as GPT-based technologies into multiagent systems. We anchor\nour methodology on the MAPE-K model, which is renowned for its robust support\nin monitoring, analyzing, planning, and executing system adaptations in\nresponse to dynamic environments. We also present a practical illustration of\nthe proposed approach, in which we implement and assess a basic MAS-based\napplication. The approach significantly advances the state-of-the-art of\nself-adaptive systems by proposing a new paradigm for MAS self-adaptation of\nautonomous systems based on LLM capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nascimento_N/0/1/0/all/0/1\">Nathalia Nascimento</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alencar_P/0/1/0/all/0/1\">Paulo Alencar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cowan_D/0/1/0/all/0/1\">Donald Cowan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ashaar: Automatic Analysis and Generation of Arabic Poetry Using Deep Learning Approaches. (arXiv:2307.06218v1 [cs.CL])","link":"http://arxiv.org/abs/2307.06218","description":"<p>Poetry holds immense significance within the cultural and traditional fabric\nof any nation. It serves as a vehicle for poets to articulate their emotions,\npreserve customs, and convey the essence of their culture. Arabic poetry is no\nexception, having played a cherished role in the heritage of the Arabic\ncommunity throughout history and maintaining its relevance in the present era.\nTypically, comprehending Arabic poetry necessitates the expertise of a linguist\nwho can analyze its content and assess its quality. This paper presents the\nintroduction of a framework called \\textit{Ashaar}\nhttps://github.com/ARBML/Ashaar, which encompasses a collection of datasets and\npre-trained models designed specifically for the analysis and generation of\nArabic poetry. The pipeline established within our proposed approach\nencompasses various aspects of poetry, such as meter, theme, and era\nclassification. It also incorporates automatic poetry diacritization, enabling\nmore intricate analyses like automated extraction of the \\textit{Arudi} style.\nAdditionally, we explore the feasibility of generating conditional poetry\nthrough the pre-training of a character-based GPT model. Furthermore, as part\nof this endeavor, we provide four datasets: one for poetry generation, another\nfor diacritization, and two for Arudi-style prediction. These datasets aim to\nfacilitate research and development in the field of Arabic poetry by enabling\nresearchers and enthusiasts to delve into the nuances of this rich literary\ntradition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alyafeai_Z/0/1/0/all/0/1\">Zaid Alyafeai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Shaibani_M/0/1/0/all/0/1\">Maged S. Al-Shaibani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1\">Moataz Ahmed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMBench: Is Your Multi-modal Model an All-around Player?. (arXiv:2307.06281v1 [cs.CV])","link":"http://arxiv.org/abs/2307.06281","description":"<p>Large vision-language models have recently achieved remarkable progress,\nexhibiting great perception and reasoning abilities concerning visual\ninformation. However, how to effectively evaluate these large vision-language\nmodels remains a major obstacle, hindering future model development.\nTraditional benchmarks like VQAv2 or COCO Caption provide quantitative\nperformance measurements but suffer from a lack of fine-grained ability\nassessment and non-robust evaluation metrics. Recent subjective benchmarks,\nsuch as OwlEval, offer comprehensive evaluations of a model's abilities by\nincorporating human labor, but they are not scalable and display significant\nbias. In response to these challenges, we propose MMBench, a novel\nmulti-modality benchmark. MMBench methodically develops a comprehensive\nevaluation pipeline, primarily comprised of two elements. The first element is\na meticulously curated dataset that surpasses existing similar benchmarks in\nterms of the number and variety of evaluation questions and abilities. The\nsecond element introduces a novel CircularEval strategy and incorporates the\nuse of ChatGPT. This implementation is designed to convert free-form\npredictions into pre-defined choices, thereby facilitating a more robust\nevaluation of the model's predictions. MMBench is a systematically-designed\nobjective benchmark for robustly evaluating the various abilities of\nvision-language models. We hope MMBench will assist the research community in\nbetter evaluating their models and encourage future advancements in this\ndomain. Project page: https://opencompass.org.cn/mmbench.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1\">Haodong Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wangbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yike Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Conghui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instruction Mining: High-Quality Instruction Data Selection for Large Language Models. (arXiv:2307.06290v1 [cs.CL])","link":"http://arxiv.org/abs/2307.06290","description":"<p>Large language models typically undergo two training stages, pretraining and\nfinetuning. Despite that large-scale pretraining endows the model with strong\ncapabilities to generate natural language responses, these pretrained models\ncan still fail to understand human instructions at times. To enhance language\nmodels' ability of interpreting and responding to instructions, instruction\nfinetuning has emerged as a critical method in this area. Recent studies found\nthat large language models can be finetuned to perform well even with a small\namount of high-quality instruction-following data. However, the selection of\nhigh-quality datasets for finetuning language models still lacks clear\nguidelines to follow. In this paper, we propose InstructMining, a linear rule\nfor evaluating instruction-following data quality. We formulate InstructMining\nusing specific natural language indicators. To investigate the relationship\nbetween data quality and these indicators, we further conduct extensive\nfinetuning experiments. The experiment results are then applied to estimating\nparameters in InstructMining. To further investigate its performance, we use\nInstructMining to select high-quality data from unseen datasets. Results\ndemonstrate that InstructMining can help select relatively high-quality samples\nfrom various instruction-following datasets. Compared to models finetuned on\nunfiltered datasets, models finetuned on InstructMining selected datasets\nperform better on 42.5% cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yihan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yanbin Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v7 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2108.08614","description":"<p>Question answering over knowledge graphs and other RDF data has been greatly\nadvanced, with a number of good techniques providing crisp answers for natural\nlanguage questions or telegraphic queries. Some of these systems incorporate\ntextual sources as additional evidence for the answering process, but cannot\ncompute answers that are present in text alone. Conversely, techniques from the\nIR and NLP communities have addressed QA over text, but such systems barely\nutilize semantic data and knowledge. This paper presents a method for complex\nquestions that can seamlessly operate over a mixture of RDF datasets and text\ncorpora, or individual sources, in a unified framework. Our method, called\nUNIQORN, builds a context graph on-the-fly, by retrieving question-relevant\nevidences from the RDF data and/or a text corpus, using fine-tuned BERT models.\nThe resulting graph typically contains all question-relevant evidences but also\na lot of noise. UNIQORN copes with this input by a graph algorithm for Group\nSteiner Trees, that identifies the best answer candidates in the context graph.\nExperimental results on several benchmarks of complex questions with multiple\nentities and relations, show that UNIQORN significantly outperforms\nstate-of-the-art methods for heterogeneous QA. The graph-based methodology\nprovides user-interpretable evidence for the complete answering process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pramanik_S/0/1/0/all/0/1\">Soumajit Pramanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba Oluwadara Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discriminative Models Can Still Outperform Generative Models in Aspect Based Sentiment Analysis. (arXiv:2206.02892v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.02892","description":"<p>Aspect-based Sentiment Analysis (ABSA) helps to explain customers' opinions\ntowards products and services. In the past, ABSA models were discriminative,\nbut more recently generative models have been used to generate aspects and\npolarities directly from text. In contrast, discriminative models commonly\nfirst select aspects from the text, and then classify the aspect's polarity.\nPrevious results showed that generative models outperform discriminative models\non several English ABSA datasets. Here, we evaluate and contrast two\nstate-of-the-art discriminative and generative models in several settings:\ncross-lingual, cross-domain, and cross-lingual and domain, to understand\ngeneralizability in settings other than English mono-lingual in-domain. Our\nmore thorough evaluation shows that, contrary to previous studies,\ndiscriminative models can still outperform generative models in almost all\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mullick_D/0/1/0/all/0/1\">Dhruv Mullick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fyshe_A/0/1/0/all/0/1\">Alona Fyshe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bilal Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kencorpus: A Kenyan Language Corpus of Swahili, Dholuo and Luhya for Natural Language Processing Tasks. (arXiv:2208.12081v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.12081","description":"<p>Indigenous African languages are categorized as under-served in Natural\nLanguage Processing. They therefore experience poor digital inclusivity and\ninformation access. The processing challenge with such languages has been how\nto use machine learning and deep learning models without the requisite data.\nThe Kencorpus project intends to bridge this gap by collecting and storing text\nand speech data that is good enough for data-driven solutions in applications\nsuch as machine translation, question answering and transcription in\nmultilingual communities. The Kencorpus dataset is a text and speech corpus for\nthree languages predominantly spoken in Kenya: Swahili, Dholuo and Luhya. Data\ncollection was done by researchers from communities, schools, media, and\npublishers. The Kencorpus' dataset has a collection of 5,594 items - 4,442\ntexts (5.6M words) and 1,152 speech files (177hrs). Based on this data, Part of\nSpeech tagging sets for Dholuo and Luhya (50,000 and 93,000 words respectively)\nwere developed. We developed 7,537 Question-Answer pairs for Swahili and\ncreated a text translation set of 13,400 sentences from Dholuo and Luhya into\nSwahili. The datasets are useful for downstream machine learning tasks such as\nmodel training and translation. We also developed two proof of concept systems:\nfor Kiswahili speech-to-text and machine learning system for Question Answering\ntask, with results of 18.87% word error rate and 80% Exact Match (EM)\nrespectively. These initial results give great promise to the usability of\nKencorpus to the machine learning community. Kencorpus is one of few public\ndomain corpora for these three low resource languages and forms a basis of\nlearning and sharing experiences for similar works especially for low resource\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wanjawa_B/0/1/0/all/0/1\">Barack Wanjawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wanzare_L/0/1/0/all/0/1\">Lilian Wanzare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Indede_F/0/1/0/all/0/1\">Florence Indede</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McOnyango_O/0/1/0/all/0/1\">Owen McOnyango</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ombui_E/0/1/0/all/0/1\">Edward Ombui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muchemi_L/0/1/0/all/0/1\">Lawrence Muchemi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Grammar-Based Coding Revisited. (arXiv:2209.13636v2 [cs.IT] UPDATED)","link":"http://arxiv.org/abs/2209.13636","description":"<p>We revisit the problem of minimal local grammar-based coding. In this\nsetting, the local grammar encoder encodes grammars symbol by symbol, whereas\nthe minimal grammar transform minimizes the grammar length in a preset class of\ngrammars as given by the length of local grammar encoding. It has been known\nthat such minimal codes are strongly universal for a strictly positive entropy\nrate, whereas the number of rules in the minimal grammar constitutes an upper\nbound for the mutual information of the source. Whereas the fully minimal code\nis likely intractable, the constrained minimal block code can be efficiently\ncomputed. In this article, we present a new, simpler, and more general proof of\nstrong universality of the minimal block code, regardless of the entropy rate.\nThe proof is based on a simple Zipfian bound for ranked probabilities. By the\nway, we also show empirically that the number of rules in the minimal block\ncode cannot clearly discriminate between long-memory and memoryless sources,\nsuch as a text in English and a random permutation of its characters. This\ncontradicts our previous expectations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Debowski_L/0/1/0/all/0/1\">&#x141;ukasz D&#x119;bowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Decoding: Open-ended Text Generation as Optimization. (arXiv:2210.15097v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.15097","description":"<p>Given a language model (LM), maximum probability is a poor decoding objective\nfor open-ended generation, because it produces short and repetitive text. On\nthe other hand, sampling can often produce incoherent text that drifts from the\noriginal topics. We propose contrastive decoding (CD), a reliable decoding\napproach that optimizes a contrastive objective subject to a plausibility\nconstraint. The contrastive objective returns the difference between the\nlikelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM\n(called the amateur, e.g. OPT-125M), and the constraint ensures that the\noutputs are plausible. CD is inspired by the fact that the failures of larger\nLMs (e.g., repetition, incoherence) are even more prevalent in smaller LMs, and\nthat this difference signals which texts should be preferred. CD requires zero\nadditional training, and produces higher quality text than decoding from the\nlarger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and\nsignificantly outperforms four strong decoding algorithms (e.g., nucleus,\ntop-k) in automatic and human evaluations across wikipedia, news and story\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Lisa Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1\">Daniel Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisner_J/0/1/0/all/0/1\">Jason Eisner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpeechBlender: Speech Augmentation Framework for Mispronunciation Data Generation. (arXiv:2211.00923v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2211.00923","description":"<p>The lack of labeled second language (L2) speech data is a major challenge in\ndesigning mispronunciation detection models. We introduce SpeechBlender - a\nfine-grained data augmentation pipeline for generating mispronunciation errors\nto overcome such data scarcity. The SpeechBlender utilizes varieties of masks\nto target different regions of phonetic units, and use the mixing factors to\nlinearly interpolate raw speech signals while augmenting pronunciation. The\nmasks facilitate smooth blending of the signals, generating more effective\nsamples than the `Cut/Paste' method. Our proposed technique achieves\nstate-of-the-art results, with Speechocean762, on ASR dependent\nmispronunciation detection models at phoneme level, with a 2.0% gain in Pearson\nCorrelation Coefficient (PCC) compared to the previous state-of-the-art [1].\nAdditionally, we demonstrate a 5.0% improvement at the phoneme level compared\nto our baseline. We also observed a 4.6% increase in F1-score with Arabic\nAraVoiceL2 testset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kheir_Y/0/1/0/all/0/1\">Yassine El Kheir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afzal_S/0/1/0/all/0/1\">Shazia Afzal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Human-Language Model Interaction. (arXiv:2212.09746v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09746","description":"<p>Many real-world applications of language models (LMs), such as writing\nassistance and code autocomplete, involve human-LM interaction. However, most\nbenchmarks are non-interactive in that a model produces output without human\ninvolvement. To evaluate human-LM interaction, we develop a new framework,\nHuman-AI Language-based Interaction Evaluation (HALIE), that defines the\ncomponents of interactive systems and dimensions to consider when designing\nevaluation metrics. Compared to standard, non-interactive evaluation, HALIE\ncaptures (i) the interactive process, not only the final output; (ii) the\nfirst-person subjective experience, not just a third-party assessment; and\n(iii) notions of preference beyond quality (e.g., enjoyment and ownership). We\nthen design five tasks to cover different forms of interaction: social\ndialogue, question answering, crossword puzzles, summarization, and metaphor\ngeneration. With four state-of-the-art LMs (three variants of OpenAI's GPT-3\nand AI21 Labs' Jurassic-1), we find that better non-interactive performance\ndoes not always translate to better human-LM interaction. In particular, we\nhighlight three cases where the results from non-interactive and interactive\nmetrics diverge and underscore the importance of human-LM interaction for LM\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Mina Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_M/0/1/0/all/0/1\">Megha Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardy_A/0/1/0/all/0/1\">Amelia Hardy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thickstun_J/0/1/0/all/0/1\">John Thickstun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1\">Esin Durmus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paranjape_A/0/1/0/all/0/1\">Ashwin Paranjape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerard_Ursin_I/0/1/0/all/0/1\">Ines Gerard-Ursin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Lisa Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ladhak_F/0/1/0/all/0/1\">Faisal Ladhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_F/0/1/0/all/0/1\">Frieda Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rose E. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_M/0/1/0/all/0/1\">Minae Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Joon Sung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Hancheng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tony Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bommasani_R/0/1/0/all/0/1\">Rishi Bommasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernstein_M/0/1/0/all/0/1\">Michael Bernstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?. (arXiv:2212.09747v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09747","description":"<p>The CoNLL-2003 English named entity recognition (NER) dataset has been widely\nused to train and evaluate NER models for almost 20 years. However, it is\nunclear how well models that are trained on this 20-year-old data and developed\nover a period of decades using the same test set will perform when applied on\nmodern data. In this paper, we evaluate the generalization of over 20 different\nmodels trained on CoNLL-2003, and show that NER models have very different\ngeneralization. Surprisingly, we find no evidence of performance degradation in\npre-trained Transformers, such as RoBERTa and T5, even when fine-tuned using\ndecades-old data. We investigate why some models generalize well to new data\nwhile others do not, and attempt to disentangle the effects of temporal drift\nand overfitting due to test reuse. Our analysis suggests that most\ndeterioration is due to temporal mismatch between the pre-training corpora and\nthe downstream test sets. We found that four factors are important for good\ngeneralization: model architecture, number of parameters, time period of the\npre-training corpus, in addition to the amount of fine-tuning data. We suggest\ncurrent evaluation methods have, in some sense, underestimated progress on NER\nover the past 20 years, as NER models have not only improved on the original\nCoNLL-2003 test set, but improved even more on modern data. Our datasets can be\nfound at https://github.com/ShuhengL/acl2023_conllpp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In and Out-of-Domain Text Adversarial Robustness via Label Smoothing. (arXiv:2212.10258v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10258","description":"<p>Recently it has been shown that state-of-the-art NLP models are vulnerable to\nadversarial attacks, where the predictions of a model can be drastically\naltered by slight modifications to the input (such as synonym substitutions).\nWhile several defense techniques have been proposed, and adapted, to the\ndiscrete nature of text adversarial attacks, the benefits of general-purpose\nregularization methods such as label smoothing for language models, have not\nbeen studied. In this paper, we study the adversarial robustness provided by\nvarious label smoothing strategies in foundational models for diverse NLP tasks\nin both in-domain and out-of-domain settings. Our experiments show that label\nsmoothing significantly improves adversarial robustness in pre-trained models\nlike BERT, against various popular attacks. We also analyze the relationship\nbetween prediction confidence and robustness, showing that label smoothing\nreduces over-confident errors on adversarial examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yahan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dan_S/0/1/0/all/0/1\">Soham Dan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">Insup Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A data science and machine learning approach to continuous analysis of Shakespeare's plays. (arXiv:2301.06024v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.06024","description":"<p>The availability of quantitative text analysis methods has provided new ways\nof analyzing literature in a manner that was not available in the\npre-information era. Here we apply comprehensive machine learning analysis to\nthe work of William Shakespeare. The analysis shows clear changes in the style\nof writing over time, with the most significant changes in the sentence length,\nfrequency of adjectives and adverbs, and the sentiments expressed in the text.\nApplying machine learning to make a stylometric prediction of the year of the\nplay shows a Pearson correlation of 0.71 between the actual and predicted year,\nindicating that Shakespeare's writing style as reflected by the quantitative\nmeasurements changed over time. Additionally, it shows that the stylometrics of\nsome of the plays is more similar to plays written either before or after the\nyear they were written. For instance, Romeo and Juliet is dated 1596, but is\nmore similar in stylometrics to plays written by Shakespeare after 1600. The\nsource code for the analysis is available for free download.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Swisher_C/0/1/0/all/0/1\">Charles Swisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_L/0/1/0/all/0/1\">Lior Shamir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference. (arXiv:2302.09582v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2302.09582","description":"<p>Understanding how language supports emotion inference remains a topic of\ndebate in emotion science. The present study investigated whether\nlanguage-derived emotion-concept knowledge would causally support emotion\ninference by manipulating the language-specific knowledge representations in\nlarge language models. Using the prompt technique, 14 attributes of emotion\nconcepts were found to be represented by distinct artificial neuron\npopulations. By manipulating these attribute-related neurons, the majority of\nthe emotion inference tasks showed performance deterioration compared to random\nmanipulations. The attribute-specific performance deterioration was related to\nthe importance of different attributes in human mental space. Our findings\nprovide causal evidence in support of a language-based mechanism for emotion\ninference and highlight the contributions of emotion-concept knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Ming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yusheng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hsiu-Yuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jiali Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinmiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huadong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT detectors are biased against non-native English writers. (arXiv:2304.02819v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.02819","description":"<p>The rapid adoption of generative language models has brought about\nsubstantial advancements in digital communication, while simultaneously raising\nconcerns regarding the potential misuse of AI-generated content. Although\nnumerous detection methods have been proposed to differentiate between AI and\nhuman-generated content, the fairness and robustness of these detectors remain\nunderexplored. In this study, we evaluate the performance of several\nwidely-used GPT detectors using writing samples from native and non-native\nEnglish writers. Our findings reveal that these detectors consistently\nmisclassify non-native English writing samples as AI-generated, whereas native\nwriting samples are accurately identified. Furthermore, we demonstrate that\nsimple prompting strategies can not only mitigate this bias but also\neffectively bypass GPT detectors, suggesting that GPT detectors may\nunintentionally penalize writers with constrained linguistic expressions. Our\nresults call for a broader conversation about the ethical implications of\ndeploying ChatGPT content detectors and caution against their use in evaluative\nor educational settings, particularly when they may inadvertently penalize or\nexclude non-native English speakers from the global discourse. The published\nversion of this study can be accessed at:\nwww.cell.com/patterns/fulltext/S2666-3899(23)00130-7\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1\">Weixin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuksekgonul_M/0/1/0/all/0/1\">Mert Yuksekgonul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yining Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_E/0/1/0/all/0/1\">Eric Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DataComp: In search of the next generation of multimodal datasets. (arXiv:2304.14108v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2304.14108","description":"<p>Multimodal datasets are a critical component in recent breakthroughs such as\nStable Diffusion and GPT-4, yet their design does not receive the same research\nattention as model architectures or training algorithms. To address this\nshortcoming in the ML ecosystem, we introduce DataComp, a testbed for dataset\nexperiments centered around a new candidate pool of 12.8 billion image-text\npairs from Common Crawl. Participants in our benchmark design new filtering\ntechniques or curate new data sources and then evaluate their new dataset by\nrunning our standardized CLIP training code and testing the resulting model on\n38 downstream test sets. Our benchmark consists of multiple compute scales\nspanning four orders of magnitude, which enables the study of scaling trends\nand makes the benchmark accessible to researchers with varying resources. Our\nbaseline experiments show that the DataComp workflow leads to better training\nsets. In particular, our best baseline, DataComp-1B, enables training a CLIP\nViT-L/14 from scratch to 79.2% zero-shot accuracy on ImageNet, outperforming\nOpenAI's CLIP ViT-L/14 by 3.7 percentage points while using the same training\nprocedure and compute. We release DataComp and all accompanying code at\nwww.datacomp.ai.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1\">Samir Yitzhak Gadre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_A/0/1/0/all/0/1\">Alex Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayase_J/0/1/0/all/0/1\">Jonathan Hayase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smyrnis_G/0/1/0/all/0/1\">Georgios Smyrnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thao Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marten_R/0/1/0/all/0/1\">Ryan Marten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1\">Mitchell Wortsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1\">Dhruba Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jieyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orgad_E/0/1/0/all/0/1\">Eyal Orgad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Entezari_R/0/1/0/all/0/1\">Rahim Entezari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daras_G/0/1/0/all/0/1\">Giannis Daras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratt_S/0/1/0/all/0/1\">Sarah Pratt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanujan_V/0/1/0/all/0/1\">Vivek Ramanujan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1\">Yonatan Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marathe_K/0/1/0/all/0/1\">Kalyani Marathe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mussmann_S/0/1/0/all/0/1\">Stephen Mussmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vencu_R/0/1/0/all/0/1\">Richard Vencu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherti_M/0/1/0/all/0/1\">Mehdi Cherti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1\">Ranjay Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1\">Pang Wei Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saukh_O/0/1/0/all/0/1\">Olga Saukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratner_A/0/1/0/all/0/1\">Alexander Ratner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuran Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaumont_R/0/1/0/all/0/1\">Romain Beaumont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Sewoong Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1\">Alex Dimakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jitsev_J/0/1/0/all/0/1\">Jenia Jitsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmon_Y/0/1/0/all/0/1\">Yair Carmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1\">Vaishaal Shankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification. (arXiv:2305.04003v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.04003","description":"<p>Verification of machine learning models used in Natural Language Processing\n(NLP) is known to be a hard problem. In particular, many known neural network\nverification methods that work for computer vision and other numeric datasets\ndo not work for NLP. Here, we study technical reasons that underlie this\nproblem. Based on this analysis, we propose practical methods and heuristics\nfor preparing NLP datasets and models in a way that renders them amenable to\nknown verification methods based on abstract interpretation. We implement these\nmethods as a Python library called ANTONIO that links to the neural network\nverifiers ERAN and Marabou. We perform evaluation of the tool using an NLP\ndataset R-U-A-Robot suggested as a benchmark for verifying legally critical NLP\napplications. We hope that, thanks to its general applicability, this work will\nopen novel possibilities for including NLP verification problems into neural\nnetwork verification competitions, and will popularise NLP problems within this\ncommunity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casadio_M/0/1/0/all/0/1\">Marco Casadio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnaboldi_L/0/1/0/all/0/1\">Luca Arnaboldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daggitt_M/0/1/0/all/0/1\">Matthew L. Daggitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isac_O/0/1/0/all/0/1\">Omri Isac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinkar_T/0/1/0/all/0/1\">Tanvi Dinkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kienitz_D/0/1/0/all/0/1\">Daniel Kienitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1\">Verena Rieser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komendantskaya_E/0/1/0/all/0/1\">Ekaterina Komendantskaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MGR: Multi-generator Based Rationalization. (arXiv:2305.04492v7 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.04492","description":"<p>Rationalization is to employ a generator and a predictor to construct a\nself-explaining NLP model in which the generator selects a subset of\nhuman-intelligible pieces of the input text to the following predictor.\nHowever, rationalization suffers from two key challenges, i.e., spurious\ncorrelation and degeneration, where the predictor overfits the spurious or\nmeaningless pieces solely selected by the not-yet well-trained generator and in\nturn deteriorates the generator. Although many studies have been proposed to\naddress the two challenges, they are usually designed separately and do not\ntake both of them into account. In this paper, we propose a simple yet\neffective method named MGR to simultaneously solve the two problems. The key\nidea of MGR is to employ multiple generators such that the occurrence stability\nof real pieces is improved and more meaningful pieces are delivered to the\npredictor. Empirically, we show that MGR improves the F1 score by up to 20.9%\nas compared to state-of-the-art methods. Codes are available at\nhttps://github.com/jugechengzi/Rationalization-MGR .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haozhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuankai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yang Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs. (arXiv:2305.08844v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08844","description":"<p>Despite their unprecedented success, even the largest language models make\nmistakes. Similar to how humans learn and improve using feedback, previous work\nproposed providing language models with natural language feedback to guide them\nin repairing their outputs. Because human-generated critiques are expensive to\nobtain, researchers have devised learned critique generators in lieu of human\ncritics while assuming one can train downstream models to utilize generated\nfeedback. However, this approach does not apply to black-box or limited access\nmodels such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of\nlarge general-purpose language agents, fine-tuning is neither computationally\nnor spatially efficient as it results in multiple copies of the network. In\nthis work, we introduce RL4F (Reinforcement Learning for Feedback), a\nmulti-agent collaborative framework where the critique generator is trained to\nmaximize end-task performance of GPT-3, a fixed model more than 200 times its\nsize. RL4F produces critiques that help GPT-3 revise its outputs. We study\nthree datasets for action planning, summarization and alphabetization and show\nrelative improvements up to 10% in multiple text similarity metrics over other\nlearned, retrieval-augmented or prompting-based critique generators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akyurek_A/0/1/0/all/0/1\">Afra Feyza Aky&#xfc;rek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akyurek_E/0/1/0/all/0/1\">Ekin Aky&#xfc;rek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1\">Ashwin Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijaya_D/0/1/0/all/0/1\">Derry Wijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_N/0/1/0/all/0/1\">Niket Tandon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network. (arXiv:2305.12493v5 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2305.12493","description":"<p>Contextual information plays a crucial role in speech recognition\ntechnologies and incorporating it into the end-to-end speech recognition models\nhas drawn immense interest recently. However, previous deep bias methods lacked\nexplicit supervision for bias tasks. In this study, we introduce a contextual\nphrase prediction network for an attention-based deep bias method. This network\npredicts context phrases in utterances using contextual embeddings and\ncalculates bias loss to assist in the training of the contextualized model. Our\nmethod achieved a significant word error rate (WER) reduction across various\nend-to-end speech recognition models. Experiments on the LibriSpeech corpus\nshow that our proposed model obtains a 12.1% relative WER improvement over the\nbaseline model, and the WER of the context phrases decreases relatively by\n40.5%. Moreover, by applying a context phrase filtering strategy, we also\neffectively eliminate the WER degradation when using a larger biasing list.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1\">Kaixun Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1\">Zhanheng Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_P/0/1/0/all/0/1\">Pengcheng Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mu_B/0/1/0/all/0/1\">Bingshen Mu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_T/0/1/0/all/0/1\">Tianyi Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey. (arXiv:2305.18703v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18703","description":"<p>Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing (NLP), providing a highly useful, task-agnostic foundation\nfor a wide range of applications. However, directly applying LLMs to solve\nsophisticated problems in specific domains meets many hurdles, caused by the\nheterogeneity of domain data, the sophistication of domain knowledge, the\nuniqueness of domain objectives, and the diversity of the constraints (e.g.,\nvarious social norms, cultural conformity, religious beliefs, and ethical\nstandards in the domain applications). Domain specification techniques are key\nto make large language models disruptive in many applications. Specifically, to\nsolve these hurdles, there has been a notable increase in research and\npractices conducted in recent years on the domain specialization of LLMs. This\nemerging field of study, with its substantial potential for impact,\nnecessitates a comprehensive and systematic review to better summarize and\nguide ongoing work in this area. In this article, we present a comprehensive\nsurvey on domain specification techniques for large language models, an\nemerging direction critical for large language model applications. First, we\npropose a systematic taxonomy that categorizes the LLM domain-specialization\ntechniques based on the accessibility to LLMs and summarizes the framework for\nall the subcategories as well as their relations and differences to each other.\nSecond, we present an extensive taxonomy of critical application domains that\ncan benefit dramatically from specialized LLMs, discussing their practical\nsignificance and open challenges. Last, we offer our insights into the current\nresearch status and future trends in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1\">Chen Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xujiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiaying Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chengyuan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Can Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_T/0/1/0/all/0/1\">Tanmoy Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianjiao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panalkar_A/0/1/0/all/0/1\">Amit Panalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengzhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1\">Chris White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KIT's Multilingual Speech Translation System for IWSLT 2023. (arXiv:2306.05320v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.05320","description":"<p>Many existing speech translation benchmarks focus on native-English speech in\nhigh-quality recording conditions, which often do not match the conditions in\nreal-life use-cases. In this paper, we describe our speech translation system\nfor the multilingual track of IWSLT 2023, which evaluates translation quality\non scientific conference talks. The test condition features accented input\nspeech and terminology-dense contents. The task requires translation into 10\nlanguages of varying amounts of resources. In absence of training data from the\ntarget domain, we use a retrieval-based approach (kNN-MT) for effective\nadaptation (+0.8 BLEU for speech translation). We also use adapters to easily\nintegrate incremental training data from data augmentation, and show that it\nmatches the performance of re-training. We observe that cascaded systems are\nmore easily adaptable towards specific target domains, due to their separate\nmodules. Our cascaded speech system substantially outperforms its end-to-end\ncounterpart on scientific talk translation, although their performance remains\nsimilar on TED talks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Danni Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thai Binh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koneru_S/0/1/0/all/0/1\">Sai Koneru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ugan_E/0/1/0/all/0/1\">Enes Yavuz Ugan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1\">Ngoc-Quan Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan-Nam Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1\">Tu Anh Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullov_C/0/1/0/all/0/1\">Carlos Mullov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alexander Waibel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niehues_J/0/1/0/all/0/1\">Jan Niehues</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. (arXiv:2306.05685v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.05685","description":"<p>Evaluating large language model (LLM) based chat assistants is challenging\ndue to their broad capabilities and the inadequacy of existing benchmarks in\nmeasuring human preferences. To address this, we explore using strong LLMs as\njudges to evaluate these models on more open-ended questions. We examine the\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\nself-enhancement biases, as well as limited reasoning ability, and propose\nsolutions to mitigate some of them. We then verify the agreement between LLM\njudges and human preferences by introducing two benchmarks: MT-bench, a\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\ncrowdsourced human preferences well, achieving over 80\\% agreement, the same\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\nexplainable way to approximate human preferences, which are otherwise very\nexpensive to obtain. Additionally, we show our benchmark and traditional\nbenchmarks complement each other by evaluating several variants of LLaMA and\nVicuna. We will publicly release MT-bench questions, 3K expert votes, and 30K\nconversations with human preferences from Chatbot Arena.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lianmin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_W/0/1/0/all/0/1\">Wei-Lin Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1\">Ying Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1\">Siyuan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhanghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yonghao Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dacheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric. P Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1\">Ion Stoica</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Effectiveness of ChatGPT in Mathematical Reasoning and Problem Solving: Evidence from the Vietnamese National High School Graduation Examination. (arXiv:2306.06331v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.06331","description":"<p>This study offers a complete analysis of ChatGPT's mathematics abilities in\nresponding to multiple-choice questions for the Vietnamese National High School\nGraduation Examination (VNHSGE) on a range of subjects and difficulty levels.\nThe dataset included 250 questions divided into four levels: knowledge (K),\ncomprehension (C), application (A), and high application (H), and it included\nten themes that covered diverse mathematical concepts. The outcomes demonstrate\nthat ChatGPT's performance varies depending on the difficulty level and\nsubject. It performed best on questions at Level (K), with an accuracy rate of\n$83\\%$; but, as the difficulty level rose, it scored poorly, with an accuracy\nrate of $10\\%$. The study has also shown that ChatGPT significantly succeeds in\nproviding responses to questions on subjects including exponential and\nlogarithmic functions, geometric progression, and arithmetic progression. The\nstudy found that ChatGPT had difficulty correctly answering questions on topics\nincluding derivatives and applications, spatial geometry, and Oxyz spatial\ncalculus. Additionally, this study contrasted ChatGPT outcomes with Vietnamese\nstudents in VNHSGE and in other math competitions. ChatGPT dominated in the SAT\nMath competition with a success rate of $70\\%$, followed by VNHSGE mathematics\n($58.8\\%)$. However, its success rates were lower on other exams, such as AP\nStatistics, the GRE Quantitative, AMC 10, AMC 12, and AP Calculus BC. These\nresults suggest that ChatGPT has the potential to be an effective teaching tool\nfor mathematics, but more work is needed to enhance its handling of graphical\ndata and address the challenges presented by questions that are getting more\nchallenging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dao_X/0/1/0/all/0/1\">Xuan-Quy Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngoc-Bich Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does mBERT understand Romansh? Evaluating word embeddings using word alignment. (arXiv:2306.08702v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.08702","description":"<p>We test similarity-based word alignment models (SimAlign and awesome-align)\nin combination with word embeddings from mBERT and XLM-R on parallel sentences\nin German and Romansh. Since Romansh is an unseen language, we are dealing with\na zero-shot setting. Using embeddings from mBERT, both models reach an\nalignment error rate of 0.22, which outperforms fast_align, a statistical\nmodel, and is on par with similarity-based word alignment for seen languages.\nWe interpret these results as evidence that mBERT contains information that can\nbe meaningful and applicable to Romansh.\n</p>\n<p>To evaluate performance, we also present a new trilingual corpus, which we\ncall the DERMIT (DE-RM-IT) corpus, containing press releases made by the Canton\nof Grisons in German, Romansh and Italian in the past 25 years. The corpus\ncontains 4 547 parallel documents and approximately 100 000 sentence pairs in\neach language combination. We additionally present a gold standard for\nGerman-Romansh word alignment. The data is available at\nhttps://github.com/eyldlv/DERMIT-Corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dolev_E/0/1/0/all/0/1\">Eyal Liron Dolev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Modular Activation for Efficient Sequence Modeling. (arXiv:2306.11197v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2306.11197","description":"<p>Linear State Space Models (SSMs) have demonstrated strong performance in a\nvariety of sequence modeling tasks due to their efficient encoding of the\nrecurrent structure. However, in more comprehensive tasks like language\nmodeling and machine translation, self-attention-based models still outperform\nSSMs. Hybrid models employing both SSM and self-attention generally show\npromising performance, but current approaches apply attention modules\nstatically and uniformly to all elements in the input sequences, leading to\nsub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse\nModular Activation (SMA), a general mechanism enabling neural networks to\nsparsely and dynamically activate sub-modules for sequence elements in a\ndifferentiable manner. Through allowing each element to skip non-activated\nsub-modules, SMA reduces computation and memory consumption at both training\nand inference stages of sequence modeling. As a specific instantiation of SMA,\nwe design a novel neural architecture, SeqBoat, which employs SMA to sparsely\nactivate a Gated Attention Unit (GAU) based on the state representations\nlearned from an SSM. By constraining the GAU to only conduct local attention on\nthe activated inputs, SeqBoat can achieve linear inference complexity with\ntheoretically infinite attention span, and provide substantially better\nquality-efficiency trade-off than the chunking-based models. With experiments\non a wide range of tasks, including language modeling, speech classification\nand long-range arena, SeqBoat brings new state-of-the-art results among hybrid\nmodels with linear complexity and reveals the amount of attention needed for\neach task through the learned sparse activation patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1\">Liliang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Zero-shot Robustness of Instruction-tuned Language Models. (arXiv:2306.11270v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.11270","description":"<p>Instruction fine-tuning has recently emerged as a promising approach for\nimproving the zero-shot capabilities of Large Language Models (LLMs) on new\ntasks. This technique has shown particular strength in improving the\nperformance of modestly sized LLMs, sometimes inducing performance competitive\nwith much larger model variants. In this paper we ask two questions: (1) How\nsensitive are instruction-tuned models to the particular phrasings of\ninstructions, and, (2) How can we make them more robust to such natural\nlanguage variation? To answer the former, we collect a set of 319 instructions\nmanually written by NLP practitioners for over 80 unique tasks included in\nwidely used benchmarks, and we evaluate the variance and average performance of\nthese instructions as compared to instruction phrasings observed during\ninstruction fine-tuning. We find that using novel (unobserved) but appropriate\ninstruction phrasings consistently degrades model performance, sometimes\nsubstantially so. Further, such natural instructions yield a wide variance in\ndownstream performance, despite their semantic equivalence. Put another way,\ninstruction-tuned models are not especially robust to instruction re-phrasings.\nWe propose a simple method to mitigate this issue by introducing ``soft\nprompt'' embedding parameters and optimizing these to maximize the similarity\nbetween representations of semantically equivalent instructions. We show that\nthis method consistently improves the robustness of instruction-tuned models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiuding Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaib_C/0/1/0/all/0/1\">Chantal Shaib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identity Construction in a Misogynist Incels Forum. (arXiv:2306.15745v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.15745","description":"<p>Online communities of involuntary celibates (incels) are a prominent source\nof misogynist hate speech. In this paper, we use quantitative text and network\nanalysis approaches to examine how identity groups are discussed on\nincels-dot-is, the largest black-pilled incels forum. We find that this\ncommunity produces a wide range of novel identity terms and, while terms for\nwomen are most common, mentions of other minoritized identities are increasing.\nAn analysis of the associations made with identity groups suggests an\nessentialist ideology where physical appearance, as well as gender and racial\nhierarchies, determine human value. We discuss implications for research into\nautomated misogynist hate speech detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoder_M/0/1/0/all/0/1\">Michael Miller Yoder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perry_C/0/1/0/all/0/1\">Chloe Perry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1\">David West Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carley_K/0/1/0/all/0/1\">Kathleen M. Carley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruden_M/0/1/0/all/0/1\">Meredith L. Pruden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation. (arXiv:2307.00470v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.00470","description":"<p>Large language models(LLMS) have shown excellent text generation\ncapabilities,capable of generating fluent responses for many downstream tasks.\nHowever,applying large language models to real-world critical tasks remains\nchallenging due to their susceptibility to hallucinations and inability to\ndirectly use external knowledge. To address the above challenges,this paper\nproposes PatternGPT, a pattern-driven text generation framework for large\nlanguage models. First,the framework utilizes the extraction capabilities of\nlarge language models to generate rich and diverse patterns and later draws on\nthe idea of federated learning. Using multiple agents to achieve sharing to\nobtain more diverse patterns. Finally, it searches for high-quality patterns\nusing judgment criteria and optimization algorithms and uses the searched\npatterns to guide the model for generation. This framework has the advantages\nof generating diversified patterns, protecting data privacy,combining external\nknowledge, and improving the quality of generation, which provides an effective\nmethod to optimize the text generation capability of large language models,and\nmake it better applied to the field of intelligent dialogue and content\ngeneration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Le Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_X/0/1/0/all/0/1\">Xin Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment. (arXiv:2307.02682v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2307.02682","description":"<p>Dense video captioning, a task of localizing meaningful moments and\ngenerating relevant captions for videos, often requires a large, expensive\ncorpus of annotated video segments paired with text. In an effort to minimize\nthe annotation cost, we propose ZeroTA, a novel method for dense video\ncaptioning in a zero-shot manner. Our method does not require any videos or\nannotations for training; instead, it localizes and describes events within\neach input video at test time by optimizing solely on the input. This is\naccomplished by introducing a soft moment mask that represents a temporal\nsegment in the video and jointly optimizing it with the prefix parameters of a\nlanguage model. This joint optimization aligns a frozen language generation\nmodel (i.e., GPT-2) with a frozen vision-language contrastive model (i.e.,\nCLIP) by maximizing the matching score between the generated text and a moment\nwithin the video. We also introduce a pairwise temporal IoU loss to let a set\nof soft moment masks capture multiple distinct events within the video. Our\nmethod effectively discovers diverse significant events within the video, with\nthe resulting captions appropriately describing these events. The empirical\nresults demonstrate that ZeroTA surpasses zero-shot baselines and even\noutperforms the state-of-the-art few-shot method on the widely-used benchmark\nActivityNet Captions. Moreover, our method shows greater robustness compared to\nsupervised methods when evaluated in out-of-domain scenarios. This research\nprovides insight into the potential of aligning widely-used models, such as\nlanguage generation models and vision-language models, to unlock a new\ncapability: understanding temporal aspects of videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jo_Y/0/1/0/all/0/1\">Yongrae Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seongyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Aiden SJ Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyunji Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1\">Hanseok Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain. (arXiv:2307.03042v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.03042","description":"<p>Adapting pretrained language models to novel domains, such as clinical\napplications, traditionally involves retraining their entire set of parameters.\nHowever, this approach is increasingly proven to be impractical owing to the\nsubstantial computational requirements associated with training such large\nlanguage models. To address this issue, Parameter-Efficient Fine-Tuning (PEFT)\ntechniques offer a viable solution by selectively fine-tuning a small subset of\nadditional parameters, significantly reducing the computational requirements\nfor domain adaptation. In this study, we propose Clinical LLaMA-LoRA, a PEFT\nadapter layer built upon the open-sourced LLaMA model. Clinical LLaMA-LoRA is\ntrained using clinical notes obtained from the MIMIC-IV database, thereby\ncreating a specialised adapter designed for the clinical domain. Additionally,\nwe propose a two-step PEFT framework which fuses Clinical LLaMA-LoRA with\nDownstream LLaMA-LoRA, another PEFT adapter specialised for downstream tasks.\nWe evaluate this framework on multiple clinical outcome prediction datasets,\ncomparing it to clinically trained language models. Our proposed framework\nachieves a state-of-the-art AUROC score averaged across all clinical downstream\ntasks. We observe substantial improvements of 6-9% AUROC score in the\nlarge-scale multilabel classification tasks, such as diagnoses and procedures\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gema_A/0/1/0/all/0/1\">Aryo Pradipta Gema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daines_L/0/1/0/all/0/1\">Luke Daines</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1\">Pasquale Minervini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alex_B/0/1/0/all/0/1\">Beatrice Alex</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.03109","description":"<p>Large language models (LLMs) are gaining increasing popularity in both\nacademia and industry, owing to their unprecedented performance in various\napplications. As LLMs continue to play a vital role in both research and daily\nuse, their evaluation becomes increasingly critical, not only at the task\nlevel, but also at the society level for better understanding of their\npotential risks. Over the past years, significant efforts have been made to\nexamine LLMs from various perspectives. This paper presents a comprehensive\nreview of these evaluation methods for LLMs, focusing on three key dimensions:\nwhat to evaluate, where to evaluate, and how to evaluate. Firstly, we provide\nan overview from the perspective of evaluation tasks, encompassing general\nnatural language processing tasks, reasoning, medical usage, ethics,\neducations, natural and social sciences, agent applications, and other areas.\nSecondly, we answer the `where' and `how' questions by diving into the\nevaluation methods and benchmarks, which serve as crucial components in\nassessing performance of LLMs. Then, we summarize the success and failure cases\nof LLMs in different tasks. Finally, we shed light on several future challenges\nthat lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to\nresearchers in the realm of LLMs evaluation, thereby aiding the development of\nmore proficient LLMs. Our key point is that evaluation should be treated as an\nessential discipline to better assist the development of LLMs. We consistently\nmaintain the related open-source materials at:\nhttps://github.com/MLGroupJLU/LLM-eval-survey.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yupeng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kaijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1\">Xiaoyuan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cunxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yi Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Computational Modeling of Meaning: Embodied Cognition Intertwined with Emotion. (arXiv:2307.04518v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.04518","description":"<p>This document chronicles this author's attempt to explore how words come to\nmean what they do, with a particular focus on child language acquisition and\nwhat that means for models of language understanding.\\footnote{I say\n\\emph{historical} because I synthesize the ideas based on when I discovered\nthem and how those ideas influenced my later thinking.} I explain the setting\nfor child language learning, how embodiment -- being able to perceive and enact\nin the world, including knowledge of concrete and abstract concepts -- is\ncrucial, and how emotion and cognition relate to each other and the language\nlearning process. I end with what I think are some of the requirements for a\nlanguage-learning agent that learns language in a setting similar to that of\nchildren. This paper can act as a potential guide for ongoing and future work\nin modeling language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kennington_C/0/1/0/all/0/1\">Casey Kennington</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Dataset for Evaluating Complex Compositional Knowledge for Natural Language Inference. (arXiv:2307.05034v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.05034","description":"<p>We introduce a synthetic dataset called Sentences Involving Complex\nCompositional Knowledge (SICCK) and a novel analysis that investigates the\nperformance of Natural Language Inference (NLI) models to understand\ncompositionality in logic. We produce 1,304 sentence pairs by modifying 15\nexamples from the SICK dataset (Marelli et al., 2014). To this end, we modify\nthe original texts using a set of phrases - modifiers that correspond to\nuniversal quantifiers, existential quantifiers, negation, and other concept\nmodifiers in Natural Logic (NL) (MacCartney, 2009). We use these phrases to\nmodify the subject, verb, and object parts of the premise and hypothesis.\nLastly, we annotate these modified texts with the corresponding entailment\nlabels following NL rules. We conduct a preliminary verification of how well\nthe change in the structural and semantic composition is captured by neural NLI\nmodels, in both zero-shot and fine-tuned scenarios. We found that the\nperformance of NLI models under the zero-shot setting is poor, especially for\nmodified sentences with negation and existential quantifiers. After fine-tuning\nthis dataset, we observe that models continue to perform poorly over negation,\nexistential and universal modifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akoju_S/0/1/0/all/0/1\">Sushma Anand Akoju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vacareanu_R/0/1/0/all/0/1\">Robert Vacareanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riaz_H/0/1/0/all/0/1\">Haris Riaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanco_E/0/1/0/all/0/1\">Eduardo Blanco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1\">Mihai Surdeanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Customized Text Sanitization Mechanism with Differential Privacy. (arXiv:2207.01193v2 [cs.CR] CROSS LISTED)","link":"http://arxiv.org/abs/2207.01193","description":"<p>As privacy issues are receiving increasing attention within the Natural\nLanguage Processing (NLP) community, numerous methods have been proposed to\nsanitize texts subject to differential privacy. However, the state-of-the-art\ntext sanitization mechanisms based on metric local differential privacy (MLDP)\ndo not apply to non-metric semantic similarity measures and cannot achieve good\ntrade-offs between privacy and utility. To address the above limitations, we\npropose a novel Customized Text (CusText) sanitization mechanism based on the\noriginal $\\epsilon$-differential privacy (DP) definition, which is compatible\nwith any similarity measure. Furthermore, CusText assigns each input token a\ncustomized output set of tokens to provide more advanced privacy protection at\nthe token level. Extensive experiments on several benchmark datasets show that\nCusText achieves a better trade-off between privacy and utility than existing\nmechanisms. The code is available at https://github.com/sai4july/CusText.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huimin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_F/0/1/0/all/0/1\">Fengran Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jamie Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-07-12T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}