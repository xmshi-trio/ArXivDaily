{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-12-08T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Counterfactual reasoning: Do language models need world knowledge for causal understanding?. (arXiv:2212.03278v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03278","description":"<p>Current pre-trained language models have enabled remarkable improvements in\ndownstream tasks, but it remains difficult to distinguish effects of\nstatistical correlation from more systematic logical reasoning grounded on\nunderstanding of the real world. In this paper we tease these factors apart by\nleveraging counterfactual conditionals, which force language models to predict\nunusual consequences based on hypothetical propositions. We introduce a set of\ntests drawn from psycholinguistic experiments, as well as larger-scale\ncontrolled datasets, to probe counterfactual predictions from a variety of\npopular pre-trained language models. We find that models are consistently able\nto override real-world knowledge in counterfactual scenarios, and that this\neffect is more robust in case of stronger baseline world knowledge -- however,\nwe also find that for most models this effect appears largely to be driven by\nsimple lexical cues. When we mitigate effects of both world knowledge and\nlexical cues to test knowledge of linguistic nuances of counterfactuals, we\nfind that only GPT-3 shows sensitivity to these nuances, though this\nsensitivity is also non-trivially impacted by lexical associative factors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ettinger_A/0/1/0/all/0/1\">Allyson Ettinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Knowledge Augmentation to Multi-tasking: Towards Human-like Dialogue Systems. (arXiv:2212.03279v1 [cs.AI])","link":"http://arxiv.org/abs/2212.03279","description":"<p>The goal of building dialogue agents that can converse with humans naturally\nhas been a long-standing dream of researchers since the early days of\nartificial intelligence. The well-known Turing Test proposed to judge the\nultimate validity of an artificial intelligence agent on the\nindistinguishability of its dialogues from humans'. It should come as no\nsurprise that human-level dialogue systems are very challenging to build. But,\nwhile early effort on rule-based systems found limited success, the emergence\nof deep learning enabled great advance on this topic.\n</p>\n<p>In this thesis, we focus on methods that address the numerous issues that\nhave been imposing the gap between artificial conversational agents and\nhuman-level interlocutors. These methods were proposed and experimented with in\nways that were inspired by general state-of-the-art AI methodologies. But they\nalso targeted the characteristics that dialogue systems possess.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianji Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cheater's Bowl: Human vs. Computer Search Strategies for Open-Domain Question Answering. (arXiv:2212.03296v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03296","description":"<p>For humans and computers, the first step in answering an open-domain question\nis retrieving a set of relevant documents from a large corpus. However, the\nstrategies that computers use fundamentally differ from those of humans. To\nbetter understand these differences, we design a gamified interface for data\ncollection -- Cheater's Bowl -- where a human answers complex questions with\naccess to both traditional and modern search tools. We collect a dataset of\nhuman search sessions, analyze human search strategies, and compare them to\nstate-of-the-art multi-hop QA models. Humans query logically, apply dynamic\nsearch chains, and use world knowledge to boost searching. We demonstrate how\nhuman queries can improve the accuracy of existing systems and propose\nimproving the future design of QA models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wanrong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_A/0/1/0/all/0/1\">Andrew Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1\">Jordan Boyd-Graber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Grained Emotional Paraphrasing along Emotion Gradients. (arXiv:2212.03297v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03297","description":"<p>Paraphrase generation, a.k.a. paraphrasing, is a common and important task in\nnatural language processing. Emotional paraphrasing, which changes the emotion\nembodied in a piece of text while preserving its meaning, has many potential\napplications, e.g., moderating online dialogues and preventing cyberbullying.\nWe introduce a new task of fine-grained emotional paraphrasing along emotion\ngradients, that is, altering the emotional intensities of the paraphrases in\nfine grain following smooth variations in affective dimensions while preserving\nthe meanings of the originals. We propose a framework for addressing this task\nby fine-tuning text-to-text Transformers through multi-task training. We\nenhance several widely used paraphrasing corpus by annotating the input and\ntarget texts with their fine-grained emotion labels. With these labels,\nfine-tuning text-to-text Transformers on these corpus entails multi-task\ntraining. Evaluations of the fine-tuned Transformers on separate test sets show\nthat including fine-grained emotion labels in the paraphrase task significantly\nimprove the chance of obtaining high-quality paraphrases of the desired\nemotions, i.e., more than doubling the number of exact matches of desired\nemotions while achieving consistently better scores in paraphrase metrics such\nas BLEU, ROGUE, and METEOR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Justin Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KATSum: Knowledge-aware Abstractive Text Summarization. (arXiv:2212.03371v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03371","description":"<p>Text Summarization is recognised as one of the NLP downstream tasks and it\nhas been extensively investigated in recent years. It can assist people with\nperceiving the information rapidly from the Internet, including news articles,\nsocial posts, videos, etc. Most existing research works attempt to develop\nsummarization models to produce a better output. However, advent limitations of\nmost existing models emerge, including unfaithfulness and factual errors. In\nthis paper, we propose a novel model, named as Knowledge-aware Abstractive Text\nSummarization, which leverages the advantages offered by Knowledge Graph to\nenhance the standard Seq2Seq model. On top of that, the Knowledge Graph\ntriplets are extracted from the source text and utilised to provide keywords\nwith relational information, producing coherent and factually errorless\nsummaries. We conduct extensive experiments by using real-world data sets. The\nresults reveal that the proposed framework can effectively utilise the\ninformation from Knowledge Graph and significantly reduce the factual errors in\nthe summary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weihua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_E/0/1/0/all/0/1\">Edmund Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jianhua Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis and Utilization of Entrainment on Acoustic and Emotion Features in User-agent Dialogue. (arXiv:2212.03398v1 [eess.AS])","link":"http://arxiv.org/abs/2212.03398","description":"<p>Entrainment is the phenomenon by which an interlocutor adapts their speaking\nstyle to align with their partner in conversations. It has been found in\ndifferent dimensions as acoustic, prosodic, lexical or syntactic. In this work,\nwe explore and utilize the entrainment phenomenon to improve spoken dialogue\nsystems for voice assistants. We first examine the existence of the entrainment\nphenomenon in human-to-human dialogues in respect to acoustic feature and then\nextend the analysis to emotion features. The analysis results show strong\nevidence of entrainment in terms of both acoustic and emotion features. Based\non this findings, we implement two entrainment policies and assess if the\nintegration of entrainment principle into a Text-to-Speech (TTS) system\nimproves the synthesis performance and the user experience. It is found that\nthe integration of the entrainment principle into a TTS system brings\nperformance improvement when considering acoustic features, while no obvious\nimprovement is observed when considering emotion features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tan_D/0/1/0/all/0/1\">Daxin Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kargas_N/0/1/0/all/0/1\">Nikos Kargas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McHardy_D/0/1/0/all/0/1\">David McHardy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Papayiannis_C/0/1/0/all/0/1\">Constantinos Papayiannis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bonafonte_A/0/1/0/all/0/1\">Antonio Bonafonte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strelec_M/0/1/0/all/0/1\">Marek Strelec</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rohnke_J/0/1/0/all/0/1\">Jonas Rohnke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Filandras_A/0/1/0/all/0/1\">Agis Oikonomou Filandras</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wood_T/0/1/0/all/0/1\">Trevor Wood</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards using Few-Shot Prompt Learning for Automating Model Completion. (arXiv:2212.03404v1 [cs.SE])","link":"http://arxiv.org/abs/2212.03404","description":"<p>We propose a simple yet a novel approach to improve completion in domain\nmodeling activities. Our approach exploits the power of large language models\nby using few-shot prompt learning without the need to train or fine-tune those\nmodels with large datasets that are scarce in this field. We implemented our\napproach and tested it on the completion of static and dynamic domain diagrams.\nOur initial evaluation shows that such an approach is effective and can be\nintegrated in different ways during the modeling activities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaaben_M/0/1/0/all/0/1\">Meriem Ben Chaaben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgueno_L/0/1/0/all/0/1\">Lola Burgue&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahraoui_H/0/1/0/all/0/1\">Houari Sahraoui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JamPatoisNLI: A Jamaican Patois Natural Language Inference Dataset. (arXiv:2212.03419v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03419","description":"<p>JamPatoisNLI provides the first dataset for natural language inference in a\ncreole language, Jamaican Patois. Many of the most-spoken low-resource\nlanguages are creoles. These languages commonly have a lexicon derived from a\nmajor world language and a distinctive grammar reflecting the languages of the\noriginal speakers and the process of language birth by creolization. This gives\nthem a distinctive place in exploring the effectiveness of transfer from large\nmonolingual or multilingual pretrained models. While our work, along with\nprevious work, shows that transfer from these models to low-resource languages\nthat are unrelated to languages in their training set is not very effective, we\nwould expect stronger results from transfer to creoles. Indeed, our experiments\nshow considerably better results from few-shot learning of JamPatoisNLI than\nfor such unrelated languages, and help us begin to understand how the unique\nrelationship between creoles and their high-resource base languages affect\ncross-lingual transfer. JamPatoisNLI, which consists of naturally-occurring\npremises and expert-written hypotheses, is a step towards steering research\ninto a traditionally underserved language and a useful benchmark for\nunderstanding cross-lingual NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Armstrong_R/0/1/0/all/0/1\">Ruth-Ann Armstrong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hewitt_J/0/1/0/all/0/1\">John Hewitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher Manning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improve Bilingual TTS Using Dynamic Language and Phonology Embedding. (arXiv:2212.03435v1 [cs.SD])","link":"http://arxiv.org/abs/2212.03435","description":"<p>In most cases, bilingual TTS needs to handle three types of input scripts:\nfirst language only, second language only, and second language embedded in the\nfirst language. In the latter two situations, the pronunciation and intonation\nof the second language are usually quite different due to the influence of the\nfirst language. Therefore, it is a big challenge to accurately model the\npronunciation and intonation of the second language in different contexts\nwithout mutual interference. This paper builds a Mandarin-English TTS system to\nacquire more standard spoken English speech from a monolingual Chinese speaker.\nWe introduce phonology embedding to capture the English differences between\ndifferent phonology. Embedding mask is applied to language embedding for\ndistinguishing information between different languages and to phonology\nembedding for focusing on English expression. We specially design an embedding\nstrength modulator to capture the dynamic strength of language and phonology.\nExperiments show that our approach can produce significantly more natural and\nstandard spoken English speech of the monolingual Chinese speaker. From\nanalysis, we find that suitable phonology control contributes to better\nperformance in different scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fengyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_J/0/1/0/all/0/1\">Jian Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Self-Supervised Multilingual Speech Representation Learning Combined with Auxiliary Language Information. (arXiv:2212.03476v1 [eess.AS])","link":"http://arxiv.org/abs/2212.03476","description":"<p>Multilingual end-to-end models have shown great improvement over monolingual\nsystems. With the development of pre-training methods on speech,\nself-supervised multilingual speech representation learning like XLSR has shown\nsuccess in improving the performance of multilingual automatic speech\nrecognition (ASR). However, similar to the supervised learning, multilingual\npre-training may also suffer from language interference and further affect the\napplication of multilingual system. In this paper, we introduce several\ntechniques for improving self-supervised multilingual pre-training by\nleveraging auxiliary language information, including the language adversarial\ntraining, language embedding and language adaptive training during the\npre-training stage. We conduct experiments on a multilingual ASR task\nconsisting of 16 languages. Our experimental results demonstrate 14.3% relative\ngain over the standard XLSR model, and 19.8% relative gain over the no\npre-training multilingual model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ding_F/0/1/0/all/0/1\">Fenglin Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wan_G/0/1/0/all/0/1\">Genshun Wan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_P/0/1/0/all/0/1\">Pengcheng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_J/0/1/0/all/0/1\">Jia Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Cong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Generative Approach for Script Event Prediction via Contrastive Fine-tuning. (arXiv:2212.03496v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03496","description":"<p>Script event prediction aims to predict the subsequent event given the\ncontext. This requires the capability to infer the correlations between events.\nRecent works have attempted to improve event correlation reasoning by using\npretrained language models and incorporating external knowledge~(e.g.,\ndiscourse relations). Though promising results have been achieved, some\nchallenges still remain. First, the pretrained language models adopted by\ncurrent works ignore event-level knowledge, resulting in an inability to\ncapture the correlations between events well. Second, modeling correlations\nbetween events with discourse relations is limited because it can only capture\nexplicit correlations between events with discourse markers, and cannot capture\nmany implicit correlations. To this end, we propose a novel generative approach\nfor this task, in which a pretrained language model is fine-tuned with an\nevent-centric pretraining objective and predicts the next event within a\ngenerative paradigm. Specifically, we first introduce a novel event-level blank\ninfilling strategy as the learning objective to inject event-level knowledge\ninto the pretrained language model, and then design a likelihood-based\ncontrastive loss for fine-tuning the generative model. Instead of using an\nadditional prediction layer, we perform prediction by using sequence\nlikelihoods generated by the generative model. Our approach models correlations\nbetween events in a soft way without any external knowledge. The\nlikelihood-based prediction eliminates the need to use additional networks to\nmake predictions and is somewhat interpretable since it scores each word in the\nevent. Experimental results on the multi-choice narrative cloze~(MCNC) task\ndemonstrate that our approach achieves better results than other\nstate-of-the-art baselines. Our code will be available at\n\\url{https://github.com/zhufq00/mcnc}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fangqi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Changlong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_X/0/1/0/all/0/1\">Xin Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WIDER & CLOSER: Mixture of Short-channel Distillers for Zero-shot Cross-lingual Named Entity Recognition. (arXiv:2212.03506v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03506","description":"<p>Zero-shot cross-lingual named entity recognition (NER) aims at transferring\nknowledge from annotated and rich-resource data in source languages to\nunlabeled and lean-resource data in target languages. Existing mainstream\nmethods based on the teacher-student distillation framework ignore the rich and\ncomplementary information lying in the intermediate layers of pre-trained\nlanguage models, and domain-invariant information is easily lost during\ntransfer. In this study, a mixture of short-channel distillers (MSD) method is\nproposed to fully interact the rich hierarchical information in the teacher\nmodel and to transfer knowledge to the student model sufficiently and\nefficiently. Concretely, a multi-channel distillation framework is designed for\nsufficient information transfer by aggregating multiple distillers as a\nmixture. Besides, an unsupervised method adopting parallel domain adaptation is\nproposed to shorten the channels between the teacher and student models to\npreserve domain-invariant features. Experiments on four datasets across nine\nlanguages demonstrate that the proposed method achieves new state-of-the-art\nperformance on zero-shot cross-lingual NER and shows great generalization and\ncompatibility across languages and fields.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun-Yu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Beiduo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jia-Chen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhen-Hua Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Embeddings by Weakly-Supervised Contrastive Pre-training. (arXiv:2212.03533v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03533","description":"<p>This paper presents E5, a family of state-of-the-art text embeddings that\ntransfer well to a wide range of tasks. The model is trained in a contrastive\nmanner with weak supervision signals from our curated large-scale text pair\ndataset (called CCPairs). E5 can be readily used as a general-purpose embedding\nmodel for any tasks requiring a single-vector representation of texts such as\nretrieval, clustering, and classification, achieving strong performance in both\nzero-shot and fine-tuned settings. We conduct extensive evaluations on 56\ndatasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the\nfirst model that outperforms the strong BM25 baseline on the BEIR retrieval\nbenchmark without using any labeled data. When fine-tuned, E5 obtains the best\nresults on the MTEB benchmark, beating existing embedding models with 40x more\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1\">Nan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_B/0/1/0/all/0/1\">Binxing Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linjun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_R/0/1/0/all/0/1\">Rangan Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Talking About Large Language Models. (arXiv:2212.03551v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03551","description":"<p>Thanks to rapid progress in artificial intelligence, we have entered an era\nwhen technology and philosophy intersect in interesting ways. Sitting squarely\nat the centre of this intersection are large language models (LLMs). The more\nadept LLMs become at mimicking human language, the more vulnerable we become to\nanthropomorphism, to seeing the systems in which they are embedded as more\nhuman-like than they really are. This trend is amplified by the natural\ntendency to use philosophically loaded terms, such as \"knows\", \"believes\", and\n\"thinks\", when describing these systems. To mitigate this trend, this paper\nadvocates the practice of repeatedly stepping back to remind ourselves of how\nLLMs, and the systems of which they form a part, actually work. The hope is\nthat increased scientific precision will encourage more philosophical nuance in\nthe discourse around artificial intelligence, both within the field and in the\npublic sphere.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shanahan_M/0/1/0/all/0/1\">Murray Shanahan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Resource End-to-end Sanskrit TTS using Tacotron2, WaveGlow and Transfer Learning. (arXiv:2212.03558v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03558","description":"<p>End-to-end text-to-speech (TTS) systems have been developed for European\nlanguages like English and Spanish with state-of-the-art speech quality,\nprosody, and naturalness. However, development of end-to-end TTS for Indian\nlanguages is lagging behind in terms of quality. The challenges involved in\nsuch a task are: 1) scarcity of quality training data; 2) low efficiency during\ntraining and inference; 3) slow convergence in the case of large vocabulary\nsize. In our work reported in this paper, we have investigated the use of\nfine-tuning the English-pretrained Tacotron2 model with limited Sanskrit data\nto synthesize natural sounding speech in Sanskrit in low resource settings. Our\nexperiments show encouraging results, achieving an overall MOS of 3.38 from 37\nevaluators with good Sanskrit spoken knowledge. This is really a very good\nresult, considering the fact that the speech data we have used is of duration\n2.5 hours only.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Debnath_A/0/1/0/all/0/1\">Ankur Debnath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_S/0/1/0/all/0/1\">Shridevi S Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadiger_G/0/1/0/all/0/1\">Gangotri Nadiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesan_R/0/1/0/all/0/1\">Ramakrishnan Angarai Ganesan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tag Embedding and Well-defined Intermediate Representation improve Auto-Formulation of Problem Description. (arXiv:2212.03575v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03575","description":"<p>In this report, I address auto-formulation of problem description, the task\nof converting an optimization problem into a canonical representation. I first\nsimplify the auto-formulation task by defining an intermediate representation,\nthen introduce entity tag embedding to utilize a given entity tag information.\nThe ablation study demonstrate the effectiveness of the proposed method, which\nfinally took second place in NeurIPS 2022 NL4Opt competition subtask 2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_S/0/1/0/all/0/1\">Sanghwan Jang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"G-MAP: General Memory-Augmented Pre-trained Language Model for Domain Tasks. (arXiv:2212.03613v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03613","description":"<p>Recently, domain-specific PLMs have been proposed to boost the task\nperformance of specific domains (e.g., biomedical and computer science) by\ncontinuing to pre-train general PLMs with domain-specific corpora. However,\nthis Domain-Adaptive Pre-Training (DAPT; Gururangan et al. (2020)) tends to\nforget the previous general knowledge acquired by general PLMs, which leads to\na catastrophic forgetting phenomenon and sub-optimal performance. To alleviate\nthis problem, we propose a new framework of General Memory Augmented\nPre-trained Language Model (G-MAP), which augments the domain-specific PLM by a\nmemory representation built from the frozen general PLM without losing any\ngeneral knowledge. Specifically, we propose a new memory-augmented layer, and\nbased on it, different augmented strategies are explored to build the memory\nrepresentation and then adaptively fuse it into the domain-specific PLM. We\ndemonstrate the effectiveness of G-MAP on various domains (biomedical and\ncomputer science publications, news, and reviews) and different kinds (text\nclassification, QA, NER) of tasks, and the extensive results show that the\nproposed G-MAP can achieve SOTA results on all tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zhongwei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yichun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiaxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M3ST: Mix at Three Levels for Speech Translation. (arXiv:2212.03657v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03657","description":"<p>How to solve the data scarcity problem for end-to-end speech-to-text\ntranslation (ST)? It's well known that data augmentation is an efficient method\nto improve performance for many tasks by enlarging the dataset. In this paper,\nwe propose Mix at three levels for Speech Translation (M^3ST) method to\nincrease the diversity of the augmented training corpus. Specifically, we\nconduct two phases of fine-tuning based on a pre-trained model using external\nmachine translation (MT) data. In the first stage of fine-tuning, we mix the\ntraining corpus at three levels, including word level, sentence level and frame\nlevel, and fine-tune the entire model with mixed data. At the second stage of\nfine-tuning, we take both original speech sequences and original text sequences\nin parallel into the model to fine-tune the network, and use Jensen-Shannon\ndivergence to regularize their outputs. Experiments on MuST-C speech\ntranslation benchmark and analysis show that M^3ST outperforms current strong\nbaselines and achieves state-of-the-art results on eight directions with an\naverage BLEU of 29.9.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xuxin Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qianqian Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_F/0/1/0/all/0/1\">Fengpeng Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_T/0/1/0/all/0/1\">Tom Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-Based Named Entity Recognition for French Using Adversarial Adaptation to Similar Domain Corpora. (arXiv:2212.03692v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03692","description":"<p>Named Entity Recognition (NER) involves the identification and classification\nof named entities in unstructured text into predefined classes. NER in\nlanguages with limited resources, like French, is still an open problem due to\nthe lack of large, robust, labelled datasets. In this paper, we propose a\ntransformer-based NER approach for French using adversarial adaptation to\nsimilar domain or general corpora for improved feature extraction and better\ngeneralization. We evaluate our approach on three labelled datasets and show\nthat our adaptation framework outperforms the corresponding non-adaptive models\nfor various combinations of transformer models, source datasets and target\ncorpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choudhry_A/0/1/0/all/0/1\">Arjun Choudhry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Pankaj Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khatri_I/0/1/0/all/0/1\">Inder Khatri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aaryan Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicol_M/0/1/0/all/0/1\">Maxime Nicol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meurs_M/0/1/0/all/0/1\">Marie-Jean Meurs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vishwakarma_D/0/1/0/all/0/1\">Dinesh Kumar Vishwakarma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Persona-Based Conversational AI: State of the Art and Challenges. (arXiv:2212.03699v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03699","description":"<p>Conversational AI has become an increasingly prominent and practical\napplication of machine learning. However, existing conversational AI techniques\nstill suffer from various limitations. One such limitation is a lack of\nwell-developed methods for incorporating auxiliary information that could help\na model understand conversational context better. In this paper, we explore how\npersona-based information could help improve the quality of response generation\nin conversations. First, we provide a literature review focusing on the current\nstate-of-the-art methods that utilize persona information. We evaluate two\nstrong baseline methods, the Ranking Profile Memory Network and the\nPoly-Encoder, on the NeurIPS ConvAI2 benchmark dataset. Our analysis elucidates\nthe importance of incorporating persona information into conversational\nsystems. Additionally, our study highlights several limitations with current\nstate-of-the-art methods and outlines challenges and future research directions\nfor advancing personalized conversational AI technology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junfeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Symons_C/0/1/0/all/0/1\">Christopher Symons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vatsavai_R/0/1/0/all/0/1\">Ranga Raju Vatsavai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intent Recognition in Conversational Recommender Systems. (arXiv:2212.03721v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03721","description":"<p>Any organization needs to improve their products, services, and processes. In\nthis context, engaging with customers and understanding their journey is\nessential. Organizations have leveraged various techniques and technologies to\nsupport customer engagement, from call centres to chatbots and virtual agents.\nRecently, these systems have used Machine Learning (ML) and Natural Language\nProcessing (NLP) to analyze large volumes of customer feedback and engagement\ndata. The goal is to understand customers in context and provide meaningful\nanswers across various channels. Despite multiple advances in Conversational\nArtificial Intelligence (AI) and Recommender Systems (RS), it is still\nchallenging to understand the intent behind customer questions during the\ncustomer journey. To address this challenge, in this paper, we study and\nanalyze the recent work in Conversational Recommender Systems (CRS) in general\nand, more specifically, in chatbot-based CRS. We introduce a pipeline to\ncontextualize the input utterances in conversations. We then take the next step\ntowards leveraging reverse feature engineering to link the contextualized input\nand learning model to support intent recognition. Since performance evaluation\nis achieved based on different ML models, we use transformer base models to\nevaluate the proposed approach using a labelled dialogue dataset (MSDialogue)\nof question-answering interactions between information seekers and answer\nproviders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moradizeyveh_S/0/1/0/all/0/1\">Sahar Moradizeyveh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harnessing Knowledge and Reasoning for Human-Like Natural Language Generation: A Brief Review. (arXiv:2212.03747v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03747","description":"<p>The rapid development and application of natural language generation (NLG)\ntechniques has revolutionized the field of automatic text production. However,\nthese techniques are still limited in their ability to produce human-like text\nthat is truly reasonable and informative. In this paper, we explore the\nimportance of NLG being guided by knowledge, in order to convey human-like\nreasoning through language generation. We propose ten goals for intelligent NLG\nsystems to pursue, and briefly review the achievement of NLG techniques guided\nby knowledge and reasoning. We also conclude by envisioning future directions\nand challenges in the pursuit of these goals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiangjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Study on Extracting Named Entities from Fine-tuned vs. Differentially Private Fine-tuned BERT Models. (arXiv:2212.03749v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03749","description":"<p>Privacy preserving deep learning is an emerging field in machine learning\nthat aims to mitigate the privacy risks in the use of deep neural networks. One\nsuch risk is training data extraction from language models that have been\ntrained on datasets , which contain personal and privacy sensitive information.\nIn our study, we investigate the extent of named entity memorization in\nfine-tuned BERT models. We use single-label text classification as\nrepresentative downstream task and employ three different fine-tuning setups in\nour experiments, including one with Differentially Privacy (DP). We create a\nlarge number of text samples from the fine-tuned BERT models utilizing a custom\nsequential sampling strategy with two prompting strategies. We search in these\nsamples for named entities and check if they are also present in the\nfine-tuning datasets. We experiment with two benchmark datasets in the domains\nof emails and blogs. We show that the application of DP has a huge effect on\nthe text generation capabilities of BERT. Furthermore, we show that a\nfine-tuned BERT does not generate more named entities entities specific to the\nfine-tuning dataset than a BERT model that is pre-trained only. This suggests\nthat BERT is unlikely to emit personal or privacy sensitive named entities.\nOverall, our results are important to understand to what extent BERT-based\nservices are prone to training data extraction attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diera_A/0/1/0/all/0/1\">Andor Diera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lell_N/0/1/0/all/0/1\">Nicolas Lell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garifullina_A/0/1/0/all/0/1\">Aygul Garifullina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1\">Ansgar Scherp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning. (arXiv:2212.03760v1 [cs.IR])","link":"http://arxiv.org/abs/2212.03760","description":"<p>Recent studies have proposed a unified user modeling framework that leverages\nuser behavior data from various applications. Most benefit from utilizing\nusers' behavior sequences as plain texts, representing rich information in any\ndomain or system without losing generality. Hence, a question arises: Can\nlanguage modeling for user history corpus help improve recommender systems?\nWhile its versatile usability has been widely investigated in many domains, its\napplications to recommender systems still remain underexplored. We show that\nlanguage modeling applied directly to task-specific user histories achieves\nexcellent results on diverse recommendation tasks. Also, leveraging additional\ntask-agnostic user histories delivers significant performance benefits. We\nfurther demonstrate that our approach can provide promising transfer learning\ncapabilities for a broad spectrum of real-world recommender systems, even on\nunseen domains and services.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1\">Kyuyong Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_H/0/1/0/all/0/1\">Hanock Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1\">Wonjae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jisu Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Seungjae Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyung-Min Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Overview of Indian Spoken Language Recognition from Machine Learning Perspective. (arXiv:2212.03812v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03812","description":"<p>Automatic spoken language identification (LID) is a very important research\nfield in the era of multilingual voice-command-based human-computer interaction\n(HCI). A front-end LID module helps to improve the performance of many\nspeech-based applications in the multilingual scenario. India is a populous\ncountry with diverse cultures and languages. The majority of the Indian\npopulation needs to use their respective native languages for verbal\ninteraction with machines. Therefore, the development of efficient Indian\nspoken language recognition systems is useful for adapting smart technologies\nin every section of Indian society. The field of Indian LID has started gaining\nmomentum in the last two decades, mainly due to the development of several\nstandard multilingual speech corpora for the Indian languages. Even though\nsignificant research progress has already been made in this field, to the best\nof our knowledge, there are not many attempts to analytically review them\ncollectively. In this work, we have conducted one of the very first attempts to\npresent a comprehensive review of the Indian spoken language recognition\nresearch field. In-depth analysis has been presented to emphasize the unique\nchallenges of low-resource and mutual influences for developing LID systems in\nthe Indian contexts. Several essential aspects of the Indian LID research, such\nas the detailed description of the available speech corpora, the major research\ncontributions, including the earlier attempts based on statistical modeling to\nthe recent approaches based on different neural network architectures, and the\nfuture research trends are discussed. This review work will help assess the\nstate of the present Indian LID research by any active researcher or any\nresearch enthusiasts from related fields.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1\">Spandan Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahidullah_M/0/1/0/all/0/1\">Md Sahidullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_G/0/1/0/all/0/1\">Goutam Saha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness of Learning from Task Instructions. (arXiv:2212.03813v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03813","description":"<p>Traditional supervised learning mostly works on individual tasks and requires\ntraining on a large set of task-specific examples. This paradigm seriously\nhinders the development of task generalization since preparing a task-specific\nexample set is costly. To build a system that can quickly and easily generalize\nto new tasks, task instructions have been adopted as an emerging trend of\nsupervision recently. These instructions give the model the definition of the\ntask and allow the model to output the appropriate answer based on the\ninstructions and inputs. However, task instructions are often expressed in\ndifferent forms, which can be interpreted from two threads: first, some\ninstructions are short sentences and are pretrained language model (PLM)\noriented, such as prompts, while other instructions are paragraphs and are\nhuman-oriented, such as those in Amazon MTurk; second, different end-users very\nlikely explain the same task with instructions of different textual\nexpressions. A robust system for task generalization should be able to handle\nany new tasks regardless of the variability of instructions.\n</p>\n<p>However, the system robustness in dealing with instruction-driven task\ngeneralization is still unexplored. This work investigates the system\nrobustness when the instructions of new tasks are (i) maliciously manipulated,\n(ii) paraphrased, or (iii) from different levels of conciseness. To our\nknowledge, this is the first work that systematically studies how robust a PLM\nis when it is supervised by instructions with different factors of variability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiasheng Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hanzi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liangyu Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenpeng Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transformer-Based User Satisfaction Prediction for Proactive Interaction Mechanism in DuerOS. (arXiv:2212.03817v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03817","description":"<p>Recently, spoken dialogue systems have been widely deployed in a variety of\napplications, serving a huge number of end-users. A common issue is that the\nerrors resulting from noisy utterances, semantic misunderstandings, or lack of\nknowledge make it hard for a real system to respond properly, possibly leading\nto an unsatisfactory user experience. To avoid such a case, we consider a\nproactive interaction mechanism where the system predicts the user satisfaction\nwith the candidate response before giving it to the user. If the user is not\nlikely to be satisfied according to the prediction, the system will ask the\nuser a suitable question to determine the real intent of the user instead of\nproviding the response directly. With such an interaction with the user, the\nsystem can give a better response to the user. Previous models that predict the\nuser satisfaction are not applicable to DuerOS which is a large-scale\ncommercial dialogue system. They are based on hand-crafted features and thus\ncan hardly learn the complex patterns lying behind millions of conversations\nand temporal dependency in multiple turns of the conversation. Moreover, they\nare trained and evaluated on the benchmark datasets with adequate labels, which\nare expensive to obtain in a commercial dialogue system. To face these\nchallenges, we propose a pipeline to predict the user satisfaction to help\nDuerOS decide whether to ask for clarification in each turn. Specifically, we\npropose to first generate a large number of weak labels and then train a\ntransformer-based model to predict the user satisfaction with these weak\nlabels. Empirically, we deploy and evaluate our model on DuerOS, and observe a\n19% relative improvement on the accuracy of user satisfaction prediction and\n2.3% relative improvement on user experience.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaonan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chuheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuyun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+XIe_J/0/1/0/all/0/1\">Jian XIe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Latent Knowledge in Language Models Without Supervision. (arXiv:2212.03827v1 [cs.CL])","link":"http://arxiv.org/abs/2212.03827","description":"<p>Existing techniques for training language models can be misaligned with the\ntruth: if we train models with imitation learning, they may reproduce errors\nthat humans make; if we train them to generate text that humans rate highly,\nthey may output errors that human evaluators can't detect. We propose\ncircumventing this issue by directly finding latent knowledge inside the\ninternal activations of a language model in a purely unsupervised way.\nSpecifically, we introduce a method for accurately answering yes-no questions\ngiven only unlabeled model activations. It works by finding a direction in\nactivation space that satisfies logical consistency properties, such as that a\nstatement and its negation have opposite truth values. We show that despite\nusing no supervision and no model outputs, our method can recover diverse\nknowledge represented in large language models: across 6 models and 10\nquestion-answering datasets, it outperforms zero-shot accuracy by 4\\% on\naverage. We also find that it cuts prompt sensitivity in half and continues to\nmaintain high accuracy even when models are prompted to generate incorrect\nanswers. Our results provide an initial step toward discovering what language\nmodels know, distinct from what they say, even when we don't have access to\nexplicit ground truth labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burns_C/0/1/0/all/0/1\">Collin Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Haotian Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues. (arXiv:2103.00820v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2103.00820","description":"<p>Compared to traditional visual question answering, video-grounded dialogues\nrequire additional reasoning over dialogue context to answer questions in a\nmulti-turn setting. Previous approaches to video-grounded dialogues mostly use\ndialogue context as a simple text input without modelling the inherent\ninformation flows at the turn level. In this paper, we propose a novel\nframework of Reasoning Paths in Dialogue Context (PDC). PDC model discovers\ninformation flows among dialogue turns through a semantic graph constructed\nbased on lexical components in each question and answer. PDC model then learns\nto predict reasoning paths over this semantic graph. Our path prediction model\npredicts a path from the current turn through past dialogue turns that contain\nadditional visual cues to answer the current question. Our reasoning model\nsequentially processes both visual and textual information through this\nreasoning path and the propagated features are used to generate the answer. Our\nexperimental results demonstrate the effectiveness of our method and provide\nadditional insights on how models use semantic dependencies in a dialogue\ncontext to retrieve visual cues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BookSum: A Collection of Datasets for Long-form Narrative Summarization. (arXiv:2105.08209v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.08209","description":"<p>The majority of available text summarization datasets include short-form\nsource documents that lack long-range causal and temporal dependencies, and\noften contain strong layout and stylistic biases. While relevant, such datasets\nwill offer limited challenges for future generations of text summarization\nsystems. We address these issues by introducing BookSum, a collection of\ndatasets for long-form narrative summarization. Our dataset covers source\ndocuments from the literature domain, such as novels, plays and stories, and\nincludes highly abstractive, human written summaries on three levels of\ngranularity of increasing difficulty: paragraph-, chapter-, and book-level. The\ndomain and structure of our dataset poses a unique set of challenges for\nsummarization systems, which include: processing very long documents,\nnon-trivial causal and temporal dependencies, and rich discourse structures. To\nfacilitate future work, we trained and evaluated multiple extractive and\nabstractive summarization models as baselines for our dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kryscinski_W/0/1/0/all/0/1\">Wojciech Kry&#x15b;ci&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1\">Nazneen Rajani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1\">Divyansh Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of GraphSum's Attention Weights to Improve the Explainability of Multi-Document Summarization. (arXiv:2105.11908v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.11908","description":"<p>Modern multi-document summarization (MDS) methods are based on transformer\narchitectures. They generate state of the art summaries, but lack\nexplainability. We focus on graph-based transformer models for MDS as they\ngained recent popularity. We aim to improve the explainability of the\ngraph-based MDS by analyzing their attention weights. In a graph-based MDS such\nas GraphSum, vertices represent the textual units, while the edges form some\nsimilarity graph over the units. We compare GraphSum's performance utilizing\ndifferent textual units, i. e., sentences versus paragraphs, on two news\nbenchmark datasets, namely WikiSum and MultiNews. Our experiments show that\nparagraph-level representations provide the best summarization performance.\nThus, we subsequently focus oAnalysisn analyzing the paragraph-level attention\nweights of GraphSum's multi-heads and decoding layers in order to improve the\nexplainability of a transformer-based MDS model. As a reference metric, we\ncalculate the ROUGE scores between the input paragraphs and each sentence in\nthe generated summary, which indicate source origin information via text\nsimilarity. We observe a high correlation between the attention weights and\nthis reference metric, especially on the the later decoding layers of the\ntransformer architecture. Finally, we investigate if the generated summaries\nfollow a pattern of positional bias by extracting which paragraph provided the\nmost information for each generated summary. Our results show that there is a\nhigh correlation between the position in the summary and the source origin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hickmann_M/0/1/0/all/0/1\">M. Lautaro Hickmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wurzberger_F/0/1/0/all/0/1\">Fabian Wurzberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoxhalli_M/0/1/0/all/0/1\">Megi Hoxhalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lochner_A/0/1/0/all/0/1\">Arne Lochner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tollich_J/0/1/0/all/0/1\">Jessica T&#xf6;llich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1\">Ansgar Scherp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Defending Against Backdoor Attacks in Natural Language Generation. (arXiv:2106.01810v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.01810","description":"<p>The frustratingly fragile nature of neural network models make current\nnatural language generation (NLG) systems prone to backdoor attacks and\ngenerate malicious sequences that could be sexist or offensive. Unfortunately,\nlittle effort has been invested to how backdoor attacks can affect current NLG\nmodels and how to defend against these attacks. In this work, by giving a\nformal definition of backdoor attack and defense, we investigate this problem\non two important NLG tasks, machine translation and dialog generation. Tailored\nto the inherent nature of NLG models (e.g., producing a sequence of coherent\nwords given contexts), we design defending strategies against attacks. We find\nthat testing the backward probability of generating sources given targets\nyields effective defense performance against all different types of attacks,\nand is able to handle the {\\it one-to-many} issue in many NLG tasks such as\ndialog generation. We hope that this work can raise the awareness of backdoor\nrisks concealed in deep NLG systems and inspire more future work (both attack\nand defense) towards this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ao_X/0/1/0/all/0/1\">Xiang Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lingjuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Comparison of Pre-training Language Models. (arXiv:2106.11483v7 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.11483","description":"<p>Recently, the development of pre-trained language models has brought natural\nlanguage processing (NLP) tasks to the new state-of-the-art. In this paper we\nexplore the efficiency of various pre-trained language models. We pre-train a\nlist of transformer-based models with the same amount of text and the same\ntraining steps. The experimental results shows that the most improvement upon\nthe origin BERT is adding the RNN-layer to capture more contextual information\nfor short text understanding. But the conclusion is: There are no remarkable\nimprovement for short text understanding for similar BERT structures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DziriBERT: a Pre-trained Language Model for the Algerian Dialect. (arXiv:2109.12346v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12346","description":"<p>Pre-trained transformers are now the de facto models in Natural Language\nProcessing given their state-of-the-art results in many tasks and languages.\nHowever, most of the current models have been trained on languages for which\nlarge text resources are already available (such as English, French, Arabic,\netc.). Therefore, there are still a number of low-resource languages that need\nmore attention from the community. In this paper, we study the Algerian dialect\nwhich has several specificities that make the use of Arabic or multilingual\nmodels inappropriate. To address this issue, we collected more than one million\nAlgerian tweets, and pre-trained the first Algerian language model: DziriBERT.\nWhen compared with existing models, DziriBERT achieves better results,\nespecially when dealing with the Roman script. The obtained results show that\npre-training a dedicated model on a small dataset (150 MB) can outperform\nexisting models that have been trained on much more data (hundreds of GB).\nFinally, our model is publicly available to the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdaoui_A/0/1/0/all/0/1\">Amine Abdaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrimi_M/0/1/0/all/0/1\">Mohamed Berrimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oussalah_M/0/1/0/all/0/1\">Mourad Oussalah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moussaoui_A/0/1/0/all/0/1\">Abdelouahab Moussaoui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforcement Learning for Few-Shot Text Generation Adaptation. (arXiv:2111.11030v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.11030","description":"<p>Controlling the generative model to adapt a new domain with limited samples\nis a difficult challenge and it is receiving increasing attention. Recently,\nmethods based on meta-learning have shown promising results for few-shot domain\nadaptation. However, meta-learning-based methods usually suffer from the\nproblem of overfitting, which results in a lack of diversity in the generated\ntexts. To avoid this problem, in this study, a novel framework based on\nreinforcement learning (RL) is proposed. In this framework, to increase the\nsample utilization of RL and decrease its sample requirement, maximum\nlikelihood estimation learning is incorporated into the RL process. When there\nare only a few in-domain samples available, experimental results on five target\ndomains in two few-shot configurations show that this framework performs better\nthan baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pengsen Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jinqiao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiamiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiayong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_P/0/1/0/all/0/1\">Peng Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"General and Domain Adaptive Chinese Spelling Check with Error Consistent Pretraining. (arXiv:2203.10929v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10929","description":"<p>The lack of label data is one of the significant bottlenecks for Chinese\nSpelling Check (CSC). Existing researches use the method of automatic\ngeneration by exploiting unlabeled data to expand the supervised corpus.\nHowever, there is a big gap between the real input scenario and automatic\ngenerated corpus. Thus, we develop a competitive general speller ECSpell which\nadopts the Error Consistent masking strategy to create data for pretraining.\nThis error consistency masking strategy is used to specify the error types of\nautomatically generated sentences which is consistent with real scene. The\nexperimental result indicates our model outperforms previous state-of-the-art\nmodels on the general benchmark. Moreover, spellers often work within a\nparticular domain in real life. Due to lots of uncommon domain terms,\nexperiments on our built domain specific datasets show that general models\nperform terribly. Inspired by the common practice of input methods, we propose\nto add an alterable user dictionary to handle the zero-shot domain adaption\nproblem. Specifically, we attach a User Dictionary guided inference module (UD)\nto a general token classification based speller. Our experiments demonstrate\nthat ECSpell$^{UD}$, namely ECSpell combined with UD, surpasses all the other\nbaselines largely, even approaching the performance on the general benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lv_Q/0/1/0/all/0/1\">Qi Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Ziqiang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_L/0/1/0/all/0/1\">Lei Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_C/0/1/0/all/0/1\">Chunhui Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1\">Guohong Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jeopardy: An Invertible Functional Programming Language. (arXiv:2209.02422v3 [cs.PL] UPDATED)","link":"http://arxiv.org/abs/2209.02422","description":"<p>Algorithms are ways of mapping problems to solutions. An algorithm is\ninvertible precisely when this mapping is injective, such that the initial\nproblem can be uniquely inferred from its solution.\n</p>\n<p>While invertible algorithms can be described in general-purpose languages, no\nguarantees are generally made by such languages as regards invertibility, so\nensuring invertibility requires additional (and often non-trivial) proof. On\nthe other hand, while reversible programming languages guarantee that their\nprograms are invertible by restricting the permissible operations to those\nwhich are locally invertible, writing programs in the reversible style can be\ncumbersome, and may differ significantly from conventional implementations even\nwhen the implemented algorithm is, in fact, invertible.\n</p>\n<p>In this paper we introduce Jeopardy, a functional programming language that\nguarantees program invertibility without imposing local reversibility. In\nparticular, Jeopardy allows the limited use of uninvertible -- and even\nnondeterministic! -- operations, provided that they are used in a way that can\nbe statically determined to be invertible. To this end, we outline an\n\\emph{implicitly available arguments analysis} and three further approaches\nthat can give a partial static guarantee to the (generally difficult) problem\nof guaranteeing invertibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kristensen_J/0/1/0/all/0/1\">Joachim Tilsted Kristensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaarsgaard_R/0/1/0/all/0/1\">Robin Kaarsgaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomsen_M/0/1/0/all/0/1\">Michael Kirkedal Thomsen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning. (arXiv:2210.03112v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.03112","description":"<p>Recent studies in Vision-and-Language Navigation (VLN) train RL agents to\nexecute natural-language navigation instructions in photorealistic\nenvironments, as a step towards robots that can follow human instructions.\nHowever, given the scarcity of human instruction data and limited diversity in\nthe training environments, these agents still struggle with complex language\ngrounding and spatial language understanding. Pretraining on large text and\nimage-text datasets from the web has been extensively explored but the\nimprovements are limited. We investigate large-scale augmentation with\nsynthetic instructions. We take 500+ indoor environments captured in\ndensely-sampled 360 degree panoramas, construct navigation trajectories through\nthese panoramas, and generate a visually-grounded instruction for each\ntrajectory using Marky, a high-quality multilingual navigation instruction\ngenerator. We also synthesize image observations from novel viewpoints using an\nimage-to-image GAN. The resulting dataset of 4.2M instruction-trajectory pairs\nis two orders of magnitude larger than existing human-annotated datasets, and\ncontains a wider variety of environments and viewpoints. To efficiently\nleverage data at this scale, we train a simple transformer agent with imitation\nlearning. On the challenging RxR dataset, our approach outperforms all existing\nRL agents, improving the state-of-the-art NDTW from 71.1 to 79.1 in seen\nenvironments, and from 64.6 to 66.8 in unseen test environments. Our work\npoints to a new path to improving instruction-following agents, emphasizing\nlarge-scale imitation learning and the development of synthetic instruction\ngeneration capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamath_A/0/1/0/all/0/1\">Aishwarya Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_P/0/1/0/all/0/1\">Peter Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Su Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_J/0/1/0/all/0/1\">Jing Yu Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ku_A/0/1/0/all/0/1\">Alexander Ku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waters_A/0/1/0/all/0/1\">Austin Waters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_Z/0/1/0/all/0/1\">Zarana Parekh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What the DAAM: Interpreting Stable Diffusion Using Cross Attention. (arXiv:2210.04885v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.04885","description":"<p>Large-scale diffusion neural networks represent a substantial milestone in\ntext-to-image generation, but they remain poorly understood, lacking\ninterpretability analyses. In this paper, we perform a text-image attribution\nanalysis on Stable Diffusion, a recently open-sourced model. To produce\npixel-level attribution maps, we upscale and aggregate cross-attention\nword-pixel scores in the denoising subnetwork, naming our method DAAM. We\nevaluate its correctness by testing its semantic segmentation ability on nouns,\nas well as its generalized attribution quality on all parts of speech, rated by\nhumans. We then apply DAAM to study the role of syntax in the pixel space,\ncharacterizing head--dependent heat map interaction patterns for ten common\ndependency relations. Finally, we study several semantic phenomena using DAAM,\nwith a focus on feature entanglement, where we find that cohyponyms worsen\ngeneration quality and descriptive adjectives attend too broadly. To our\nknowledge, we are the first to interpret large diffusion models from a\nvisuolinguistic perspective, which enables future lines of research. Our code\nis at https://github.com/castorini/daam.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Raphael Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1\">Akshat Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhiying Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Gefei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_K/0/1/0/all/0/1\">Karun Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ture_F/0/1/0/all/0/1\">Ferhan Ture</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Hate Speech Varies by Target Identity: A Computational Analysis. (arXiv:2210.10839v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10839","description":"<p>This paper investigates how hate speech varies in systematic ways according\nto the identities it targets. Across multiple hate speech datasets annotated\nfor targeted identities, we find that classifiers trained on hate speech\ntargeting specific identity groups struggle to generalize to other targeted\nidentities. This provides empirical evidence for differences in hate speech by\ntarget identity; we then investigate which patterns structure this variation.\nWe find that the targeted demographic category (e.g. gender/sexuality or\nrace/ethnicity) appears to have a greater effect on the language of hate speech\nthan does the relative social power of the targeted identity group. We also\nfind that words associated with hate speech targeting specific identities often\nrelate to stereotypes, histories of oppression, current social movements, and\nother social contexts specific to identities. These experiments suggest the\nimportance of considering targeted identity, as well as the social contexts\nassociated with these identities, in automated hate speech classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoder_M/0/1/0/all/0/1\">Michael Miller Yoder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_L/0/1/0/all/0/1\">Lynnette Hui Xian Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1\">David West Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carley_K/0/1/0/all/0/1\">Kathleen M. Carley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling Reasoning Capabilities from Language Models with Compositional Reasoning Transformers. (arXiv:2210.11265v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.11265","description":"<p>This paper presents ReasonFormer, a unified reasoning framework for mirroring\nthe modular and compositional reasoning process of humans in complex\ndecision-making. Inspired by dual-process theory in cognitive science, the\nrepresentation module (automatic thinking) and reasoning modules (controlled\nthinking) are decoupled to capture different levels of cognition. Upon the top\nof the representation module, the pre-trained reasoning modules are modular and\nprofessional in specific and fundamental reasoning skills (e.g., logic, simple\nQA, etc). To mimic the controlled compositional thinking process, different\nreasoning modules are dynamically activated and composed in both parallel and\ncascaded manners to control what reasoning skills are activated and how deep\nthe reasoning process will be reached to solve the current problems. The\nunified reasoning framework solves multiple tasks with a single model, and is\ntrained and inferred in an end-to-end manner. Evaluated on 11 datasets\nrequiring different reasoning skills and complexity, ReasonFormer demonstrates\nsubstantial performance boosts, revealing the compositional reasoning ability.\nFew-shot experiments exhibit better generalization ability by learning to\ncompose pre-trained skills for new tasks with limited data, and decoupling the\nrepresentation module and the reasoning modules. Further analysis shows the\nmodularity of reasoning modules as different tasks activate distinct reasoning\nskills at different reasoning depths.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wanjun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tingting Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiahai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jian Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chin-Yew Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Instruction-Finetuned Language Models. (arXiv:2210.11416v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.11416","description":"<p>Finetuning language models on a collection of datasets phrased as\ninstructions has been shown to improve model performance and generalization to\nunseen tasks. In this paper we explore instruction finetuning with a particular\nfocus on (1) scaling the number of tasks, (2) scaling the model size, and (3)\nfinetuning on chain-of-thought data. We find that instruction finetuning with\nthe above aspects dramatically improves performance on a variety of model\nclasses (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and\nevaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For\ninstance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM\n540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves\nstate-of-the-art performance on several benchmarks, such as 75.2% on five-shot\nMMLU. We also publicly release Flan-T5 checkpoints, which achieve strong\nfew-shot performance even compared to much larger models, such as PaLM 62B.\nOverall, instruction finetuning is a general method for improving the\nperformance and usability of pretrained language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Le Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1\">Barret Zoph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedus_W/0/1/0/all/0/1\">William Fedus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahma_S/0/1/0/all/0/1\">Siddhartha Brahma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1\">Albert Webson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shixiang Shane Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zhuyun Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzgun_M/0/1/0/all/0/1\">Mirac Suzgun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhery_A/0/1/0/all/0/1\">Aakanksha Chowdhery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_Ros_A/0/1/0/all/0/1\">Alex Castro-Ros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pellat_M/0/1/0/all/0/1\">Marie Pellat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_K/0/1/0/all/0/1\">Kevin Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valter_D/0/1/0/all/0/1\">Dasha Valter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_G/0/1/0/all/0/1\">Gaurav Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Adams Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_V/0/1/0/all/0/1\">Vincent Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrov_S/0/1/0/all/0/1\">Slav Petrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed H. Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dean_J/0/1/0/all/0/1\">Jeff Dean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devlin_J/0/1/0/all/0/1\">Jacob Devlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AfroLID: A Neural Language Identification Tool for African Languages. (arXiv:2210.11744v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.11744","description":"<p>Language identification (LID) is a crucial precursor for NLP, especially for\nmining web data. Problematically, most of the world's 7000+ languages today are\nnot covered by LID technologies. We address this pressing issue for Africa by\nintroducing AfroLID, a neural LID toolkit for $517$ African languages and\nvarieties. AfroLID exploits a multi-domain web dataset manually curated from\nacross 14 language families utilizing five orthographic systems. When evaluated\non our blind Test set, AfroLID achieves 95.89 F_1-score. We also compare\nAfroLID to five existing LID tools that each cover a small number of African\nlanguages, finding it to outperform them on most languages. We further show the\nutility of AfroLID in the wild by testing it on the acutely under-served\nTwitter domain. Finally, we offer a number of controlled case studies and\nperform a linguistically-motivated error analysis that allow us to both\nshowcase AfroLID's powerful capabilities and limitations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adebara_I/0/1/0/all/0/1\">Ife Adebara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elmadany_A/0/1/0/all/0/1\">AbdelRahim Elmadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inciarte_A/0/1/0/all/0/1\">Alcides Alcoba Inciarte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CREATIVESUMM: Shared Task on Automatic Summarization for Creative Writing. (arXiv:2211.05886v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05886","description":"<p>This paper introduces the shared task of summarizing documents in several\ncreative domains, namely literary texts, movie scripts, and television scripts.\nSummarizing these creative documents requires making complex literary\ninterpretations, as well as understanding non-trivial temporal dependencies in\ntexts containing varied styles of plot development and narrative structure.\nThis poses unique challenges and is yet underexplored for text summarization\nsystems. In this shared task, we introduce four sub-tasks and their\ncorresponding datasets, focusing on summarizing books, movie scripts, primetime\ntelevision scripts, and daytime soap opera scripts. We detail the process of\ncurating these datasets for the task, as well as the metrics used for the\nevaluation of the submissions. As part of the CREATIVESUMM workshop at COLING\n2022, the shared task attracted 18 submissions in total. We discuss the\nsubmissions and the baselines for each sub-task in this paper, along with\ndirections for facilitating future work in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1\">Divyansh Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1\">Alexander R. Fabbri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Simeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryscinski_W/0/1/0/all/0/1\">Wojciech Kry&#x15b;ci&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ladhak_F/0/1/0/all/0/1\">Faisal Ladhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bryan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiseman_S/0/1/0/all/0/1\">Sam Wiseman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast DistilBERT on CPUs. (arXiv:2211.07715v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07715","description":"<p>Transformer-based language models have become the standard approach to\nsolving natural language processing tasks. However, industry adoption usually\nrequires the maximum throughput to comply with certain latency constraints that\nprevents Transformer models from being used in production. To address this gap,\nmodel compression techniques such as quantization and pruning may be used to\nimprove inference efficiency. However, these compression techniques require\nspecialized software to apply and deploy at scale. In this work, we propose a\nnew pipeline for creating and running Fast Transformer models on CPUs,\nutilizing hardware-aware pruning, knowledge distillation, quantization, and our\nown Transformer inference runtime engine with optimized kernels for sparse and\nquantized operators. We demonstrate the efficiency of our pipeline by creating\na Fast DistilBERT model showing minimal accuracy loss on the question-answering\nSQuADv1.1 benchmark, and throughput results under typical production\nconstraints and environments. Our results outperform existing state-of-the-art\nNeural Magic's DeepSparse runtime performance by up to 50% and up to 4.1x\nperformance speedup over ONNX Runtime. Source code is publicly available at\nhttps://github.com/intel/intel-extension-for-transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Haihao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafrir_O/0/1/0/all/0/1\">Ofir Zafrir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Hengyu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xinyu Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Hanwen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boudoukh_G/0/1/0/all/0/1\">Guy Boudoukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasserblat_M/0/1/0/all/0/1\">Moshe Wasserblat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Emotion-Aware Multi-Task Approach to Fake News and Rumour Detection using Transfer Learning. (arXiv:2211.12374v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.12374","description":"<p>Social networking sites, blogs, and online articles are instant sources of\nnews for internet users globally. However, in the absence of strict regulations\nmandating the genuineness of every text on social media, it is probable that\nsome of these texts are fake news or rumours. Their deceptive nature and\nability to propagate instantly can have an adverse effect on society. This\nnecessitates the need for more effective detection of fake news and rumours on\nthe web. In this work, we annotate four fake news detection and rumour\ndetection datasets with their emotion class labels using transfer learning. We\nshow the correlation between the legitimacy of a text with its intrinsic\nemotion for fake news and rumour detection, and prove that even within the same\nemotion class, fake and real news are often represented differently, which can\nbe used for improved feature extraction. Based on this, we propose a multi-task\nframework for fake news and rumour detection, predicting both the emotion and\nlegitimacy of the text. We train a variety of deep learning models in\nsingle-task and multi-task settings for a more comprehensive comparison. We\nfurther analyze the performance of our multi-task approach for fake news\ndetection in cross-domain settings to verify its efficacy for better\ngeneralization across datasets, and to verify that emotions act as a\ndomain-independent feature. Experimental results verify that our multi-task\nmodels consistently outperform their single-task counterparts in terms of\naccuracy, precision, recall, and F1 score, both for in-domain and cross-domain\nsettings. We also qualitatively analyze the difference in performance in\nsingle-task and multi-task learning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choudhry_A/0/1/0/all/0/1\">Arjun Choudhry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khatri_I/0/1/0/all/0/1\">Inder Khatri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1\">Minni Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vishwakarma_D/0/1/0/all/0/1\">Dinesh Kumar Vishwakarma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simplifying and Understanding State Space Models with Diagonal Linear RNNs. (arXiv:2212.00768v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2212.00768","description":"<p>Sequence models based on linear state spaces (SSMs) have recently emerged as\na promising choice of architecture for modeling long range dependencies across\nvarious modalities. However, they invariably rely on discretization of a\ncontinuous state space, which complicates their presentation and understanding.\nIn this work, we dispose of the discretization step, and propose a model based\non vanilla Diagonal Linear RNNs ($\\mathrm{DLR}$). We empirically show that\n$\\mathrm{DLR}$ is as performant as previously-proposed SSMs in the presence of\nstrong supervision, despite being conceptually much simpler. Moreover, we\ncharacterize the expressivity of SSMs (including $\\mathrm{DLR}$) and\nattention-based models via a suite of $13$ synthetic sequence-to-sequence tasks\ninvolving interactions over tens of thousands of tokens, ranging from simple\noperations, such as shifting an input sequence, to detecting co-dependent\nvisual features over long spatial ranges in flattened images. We find that\nwhile SSMs report near-perfect performance on tasks that can be modeled via\n$\\textit{few}$ convolutional kernels, they struggle on tasks requiring\n$\\textit{many}$ such kernels and especially when the desired sequence\nmanipulation is $\\textit{context-dependent}$. For example, $\\mathrm{DLR}$\nlearns to perfectly shift a $0.5M$-long input by an arbitrary number of\npositions but fails when the shift size depends on context. Despite these\nlimitations, $\\mathrm{DLR}$ reaches high performance on two higher-order\nreasoning tasks $\\mathrm{ListOpsSubTrees}$ and\n$\\mathrm{PathfinderSegmentation}\\text{-}\\mathrm{256}$ with input lengths $8K$\nand $65K$ respectively, and gives encouraging performance on\n$\\mathrm{PathfinderSegmentation}\\text{-}\\mathrm{512}$ with input length $262K$\nfor which attention is not a viable choice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ankit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_H/0/1/0/all/0/1\">Harsh Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards human-compatible autonomous car: A study of non-verbal Turing test in automated driving with affective transition modelling. (arXiv:2212.02908v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2212.02908","description":"<p>Autonomous cars are indispensable when humans go further down the hands-free\nroute. Although existing literature highlights that the acceptance of the\nautonomous car will increase if it drives in a human-like manner, sparse\nresearch offers the naturalistic experience from a passenger's seat perspective\nto examine the human likeness of current autonomous cars. The present study\ntested whether the AI driver could create a human-like ride experience for\npassengers based on 69 participants' feedback in a real-road scenario. We\ndesigned a ride experience-based version of the non-verbal Turing test for\nautomated driving. Participants rode in autonomous cars (driven by either human\nor AI drivers) as a passenger and judged whether the driver was human or AI.\nThe AI driver failed to pass our test because passengers detected the AI driver\nabove chance. In contrast, when the human driver drove the car, the passengers'\njudgement was around chance. We further investigated how human passengers\nascribe humanness in our test. Based on Lewin's field theory, we advanced a\ncomputational model combining signal detection theory with pre-trained language\nmodels to predict passengers' humanness rating behaviour. We employed affective\ntransition between pre-study baseline emotions and corresponding post-stage\nemotions as the signal strength of our model. Results showed that the\npassengers' ascription of humanness would increase with the greater affective\ntransition. Our study suggested an important role of affective transition in\npassengers' ascription of humanness, which might become a future direction for\nautonomous driving.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaoning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qiaoli Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhengming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Anqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haiyan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Miner Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ku_Y/0/1/0/all/0/1\">Yixuan Ku</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-12-07T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/"}}]}]}