{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-12-05T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Do androids dream of fictional references? A bibliographic dialogue with ChatGPT3.5. (arXiv:2312.00789v1 [cs.DL])","link":"http://arxiv.org/abs/2312.00789","description":"<p>This article focuses on bibliographic references generated by the ChatGPT3.5\ntool. Using this tool based on the trained GPT generation model ChatGPT3.5,\ndeveloped by the company OpenAI, we explored six different themes and analyzed\na sample of references generated by the model, in French and English. The\nresults revealed high percentages of fictitious references in several fields,\nunderlining the importance of carefully checking these references before using\nthem in research work. An improvement in results was nevertheless noted between\nMay and July with regard to English references for themes on which ChatGPR3.5\nhas been particularly trained, but the situation remains unsatisfactory in\nFrench, for example. It should also be pointed out that much of the text in\nthis article was generated by ChatGPT in a joint effort with the human author.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vergnas_O/0/1/0/all/0/1\">Olivier Las Vergnas</a> (AFA, CIREL)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic detection of problem-gambling signs from online texts using large language models. (arXiv:2312.00804v1 [cs.CL])","link":"http://arxiv.org/abs/2312.00804","description":"<p>Problem gambling is a major public health concern and is associated with\nprofound psychological distress and economic problems. There are numerous\ngambling communities on the internet where users exchange information about\ngames, gambling tactics, as well as gambling-related problems. Individuals\nexhibiting higher levels of problem gambling engage more in such communities.\nOnline gambling communities may provide insights into problem-gambling\nbehaviour. Using data scraped from a major German gambling discussion board, we\nfine-tuned a large language model, specifically a Bidirectional Encoder\nRepresentations from Transformers (BERT) model, to predict signs of\nproblem-gambling from forum posts. Training data were generated by manual\nannotation and by taking into account diagnostic criteria and gambling-related\ncognitive distortions. Using k-fold cross-validation, our models achieved a\nprecision of 0.95 and F1 score of 0.71, demonstrating that satisfactory\nclassification performance can be achieved by generating high-quality training\nmaterial through manual annotation based on diagnostic criteria. The current\nstudy confirms that a BERT-based model can be reliably used on small data sets\nand to detect signatures of problem gambling in online communication data. Such\ncomputational approaches may have potential for the detection of changes in\nproblem-gambling prevalence among online users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1\">Elke Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiter_N/0/1/0/all/0/1\">Nils Reiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1\">Jan Peters</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gender inference: can chatGPT outperform common commercial tools?. (arXiv:2312.00805v1 [cs.CL])","link":"http://arxiv.org/abs/2312.00805","description":"<p>An increasing number of studies use gender information to understand\nphenomena such as gender bias, inequity in access and participation, or the\nimpact of the Covid pandemic response. Unfortunately, most datasets do not\ninclude self-reported gender information, making it necessary for researchers\nto infer gender from other information, such as names or names and country\ninformation. An important limitation of these tools is that they fail to\nappropriately capture the fact that gender exists on a non-binary scale,\nhowever, it remains important to evaluate and compare how well these tools\nperform in a variety of contexts. In this paper, we compare the performance of\na generative Artificial Intelligence (AI) tool ChatGPT with three commercially\navailable list-based and machine learning-based gender inference tools (Namsor,\nGender-API, and genderize.io) on a unique dataset. Specifically, we use a large\nOlympic athlete dataset and report how variations in the input (e.g., first\nname and first and last name, with and without country information) impact the\naccuracy of their predictions. We report results for the full set, as well as\nfor the subsets: medal versus non-medal winners, athletes from the largest\nEnglish-speaking countries, and athletes from East Asia. On these sets, we find\nthat Namsor is the best traditional commercially available tool. However,\nChatGPT performs at least as well as Namsor and often outperforms it,\nespecially for the female sample when country and/or last name information is\navailable. All tools perform better on medalists versus non-medalists and on\nnames from English-speaking countries. Although not designed for this purpose,\nChatGPT may be a cost-effective tool for gender prediction. In the future, it\nmight even be possible for ChatGPT or other large scale language models to\nbetter identify self-reported gender rather than report gender on a binary\nscale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alexopoulos_M/0/1/0/all/0/1\">Michelle Alexopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyons_K/0/1/0/all/0/1\">Kelly Lyons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahetaji_K/0/1/0/all/0/1\">Kaushar Mahetaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_M/0/1/0/all/0/1\">Marcus Emmanuel Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutwillinger_R/0/1/0/all/0/1\">Rogan Gutwillinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models for Travel Behavior Prediction. (arXiv:2312.00819v1 [cs.LG])","link":"http://arxiv.org/abs/2312.00819","description":"<p>Travel behavior prediction is a fundamental task in transportation demand\nmanagement. The conventional methods for travel behavior prediction rely on\nnumerical data to construct mathematical models and calibrate model parameters\nto represent human preferences. Recent advancement in large language models\n(LLMs) has shown great reasoning abilities to solve complex problems. In this\nstudy, we propose to use LLMs to predict travel behavior with prompt\nengineering without data-based parameter learning. Specifically, we carefully\ndesign our prompts that include 1) task description, 2) travel characteristics,\n3) individual attributes, and 4) guides of thinking with domain knowledge, and\nask the LLMs to predict an individual's travel behavior and explain the\nresults. We select the travel mode choice task as a case study. Results show\nthat, though no training samples are provided, LLM-based predictions have\ncompetitive accuracy and F1-score as canonical supervised learning methods such\nas multinomial logit, random forest, and neural networks. LLMs can also output\nreasons that support their prediction. However, though in most of the cases,\nthe output explanations are reasonable, we still observe cases that violate\nlogic or with hallucinations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mo_B/0/1/0/all/0/1\">Baichuan Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hanyong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_D/0/1/0/all/0/1\">Dingyi Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Ruoyun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaotong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jinhua Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback. (arXiv:2312.00849v1 [cs.CL])","link":"http://arxiv.org/abs/2312.00849","description":"<p>Multimodal Large Language Models (MLLMs) have recently demonstrated\nimpressive capabilities in multimodal understanding, reasoning, and\ninteraction. However, existing MLLMs prevalently suffer from serious\nhallucination problems, generating text that is not factually grounded in\nassociated images. The problem makes existing MLLMs untrustworthy and thus\nimpractical in real-world (especially high-stakes) applications. To address the\nchallenge, we present RLHF-V, which enhances MLLM trustworthiness via behavior\nalignment from fine-grained correctional human feedback. Specifically, RLHF-V\ncollects human preference in the form of segment-level corrections on\nhallucinations, and performs dense direct preference optimization over the\nhuman feedback. Comprehensive experiments on five benchmarks in both automatic\nand human evaluation show that, RLHF-V can enable substantially more\ntrustworthy MLLM behaviors with promising data and computation efficiency.\nRemarkably, using 1.4k annotated data samples, RLHF-V significantly reduces the\nhallucination rate of the base MLLM by 34.8%, outperforming the concurrent\nLLaVA-RLHF trained on 10k annotated data. The final model achieves\nstate-of-the-art performance in trustworthiness among open-source MLLMs, and\nshows better robustness than GPT-4V in preventing hallucinations aroused from\nover-generalization. We open-source our code, model, and data at\nhttps://github.com/RLHF-V/RLHF-V.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianyu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoye Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Taiwen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yifeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1\">Ganqu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hi-ArG: Exploring the Integration of Hierarchical Argumentation Graphs in Language Pretraining. (arXiv:2312.00874v1 [cs.CL])","link":"http://arxiv.org/abs/2312.00874","description":"<p>The knowledge graph is a structure to store and represent knowledge, and\nrecent studies have discussed its capability to assist language models for\nvarious applications. Some variations of knowledge graphs aim to record\narguments and their relations for computational argumentation tasks. However,\nmany must simplify semantic types to fit specific schemas, thus losing\nflexibility and expression ability. In this paper, we propose the Hierarchical\nArgumentation Graph (Hi-ArG), a new structure to organize arguments. We also\nintroduce two approaches to exploit Hi-ArG, including a text-graph multi-modal\nmodel GreaseArG and a new pre-training framework augmented with graph\ninformation. Experiments on two argumentation tasks have shown that after\nfurther pre-training and fine-tuning, GreaseArG supersedes same-scale language\nmodels on these tasks, while incorporating graph information during further\npre-training can also improve the performance of vanilla language models. Code\nfor this paper is available at https://github.com/ljcleo/Hi-ArG .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jingcong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1\">Rong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Meng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_R/0/1/0/all/0/1\">Ruofei Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quick Back-Translation for Unsupervised Machine Translation. (arXiv:2312.00912v1 [cs.CL])","link":"http://arxiv.org/abs/2312.00912","description":"<p>The field of unsupervised machine translation has seen significant\nadvancement from the marriage of the Transformer and the back-translation\nalgorithm. The Transformer is a powerful generative model, and back-translation\nleverages Transformer's high-quality translations for iterative\nself-improvement. However, the Transformer is encumbered by the run-time of\nautoregressive inference during back-translation, and back-translation is\nlimited by a lack of synthetic data efficiency. We propose a two-for-one\nimprovement to Transformer back-translation: Quick Back-Translation (QBT). QBT\nre-purposes the encoder as a generative model, and uses encoder-generated\nsequences to train the decoder in conjunction with the original autoregressive\nback-translation step, improving data throughput and utilization. Experiments\non various WMT benchmarks demonstrate that a relatively small number of\nrefining steps of QBT improve current unsupervised machine translation models,\nand that QBT dramatically outperforms standard back-translation only method in\nterms of training efficiency for comparable translation qualities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brimacombe_B/0/1/0/all/0/1\">Benjamin Brimacombe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiawei Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperparameter Optimization for Large Language Model Instruction-Tuning. (arXiv:2312.00949v1 [cs.CL])","link":"http://arxiv.org/abs/2312.00949","description":"<p>The fine-tuning of Large Language Models (LLMs) has enabled them to recently\nachieve milestones in natural language processing applications. The emergence\nof ever larger LLMs has paved the way for more efficient fine-tuning methods.\nAmong these, the Low-Rank Adaptation (LoRA) method keeps most of the weights of\nthe pre-trained LLM frozen while introducing a low-rank decomposition of the\nweight matrix, enabling the tuning of only a very small proportion of the\nnetwork. The performance on downstream tasks of models fine-tuned with LoRA\nheavily relies on a set of hyperparameters including the rank of the\ndecomposition. In this work, we investigate the choice of these hyperparameters\nthrough two main blackbox optimization (BBO) techniques. We examine the whole\npipeline of performing fine-tuning and validation on a pre-trained LLM as a\nblackbox and efficiently explore the space of hyperparameters with the \\nomad\nalgorithm, achieving a boost in performance and human alignment of the tuned\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tribes_C/0/1/0/all/0/1\">Christophe Tribes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benarroch_Lelong_S/0/1/0/all/0/1\">Sacha Benarroch-Lelong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Peng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobyzev_I/0/1/0/all/0/1\">Ivan Kobyzev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models. (arXiv:2312.00960v1 [cs.CL])","link":"http://arxiv.org/abs/2312.00960","description":"<p>Compressing large language models (LLMs), often consisting of billions of\nparameters, provides faster inference, smaller memory footprints, and enables\nlocal deployment. Two standard compression techniques are pruning and\nquantization, with the former eliminating redundant connections in model layers\nand the latter representing model parameters with fewer bits. The key tradeoff\nis between the degree of compression and the impact on the quality of the\ncompressed model. Existing research on LLM compression primarily focuses on\nperformance in terms of general metrics like perplexity or downstream task\naccuracy. More fine-grained metrics, such as those measuring parametric\nknowledge, remain significantly underexplored. To help bridge this gap, we\npresent a comprehensive analysis across multiple model families (ENCODER,\nENCODER-DECODER, and DECODER) using the LAMA and LM-HARNESS benchmarks in order\nto systematically quantify the effect of commonly employed compression\ntechniques on model performance. A particular focus is on tradeoffs involving\nparametric knowledge, with the goal of providing practitioners with practical\ninsights to help make informed decisions on compression. We release our\ncodebase1 to enable further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Namburi_S/0/1/0/all/0/1\">Satya Sai Srinath Namburi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sreedhar_M/0/1/0/all/0/1\">Makesh Sreedhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Srinath Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1\">Frederic Sala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts. (arXiv:2312.00968v1 [cs.CV])","link":"http://arxiv.org/abs/2312.00968","description":"<p>Large multi-modal models (LMMs) exhibit remarkable performance across\nnumerous tasks. However, generalist LMMs often suffer from performance\ndegradation when tuned over a large collection of tasks. Recent research\nsuggests that Mixture of Experts (MoE) architectures are useful for instruction\ntuning, but for LMMs of parameter size around O(50-100B), the prohibitive cost\nof replicating and storing the expert models severely limits the number of\nexperts we can use. We propose Omni-SMoLA, an architecture that uses the Soft\nMoE approach to (softly) mix many multimodal low rank experts, and avoids\nintroducing a significant number of new parameters compared to conventional MoE\nmodels. The core intuition here is that the large model provides a foundational\nbackbone, while different lightweight experts residually learn specialized\nknowledge, either per-modality or multimodally. Extensive experiments\ndemonstrate that the SMoLA approach helps improve the generalist performance\nacross a broad range of generative vision-and-language tasks, achieving new\nSoTA generalist performance that often matches or outperforms single\nspecialized LMM baselines, as well as new SoTA specialist performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jialin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Teacher De-biasing Distillation Framework for Multi-domain Fake News Detection. (arXiv:2312.01006v1 [cs.CL])","link":"http://arxiv.org/abs/2312.01006","description":"<p>Multi-domain fake news detection aims to identify whether various news from\ndifferent domains is real or fake and has become urgent and important. However,\nexisting methods are dedicated to improving the overall performance of fake\nnews detection, ignoring the fact that unbalanced data leads to disparate\ntreatment for different domains, i.e., the domain bias problem. To solve this\nproblem, we propose the Dual-Teacher De-biasing Distillation framework (DTDBD)\nto mitigate bias across different domains. Following the knowledge distillation\nmethods, DTDBD adopts a teacher-student structure, where pre-trained large\nteachers instruct a student model. In particular, the DTDBD consists of an\nunbiased teacher and a clean teacher that jointly guide the student model in\nmitigating domain bias and maintaining performance. For the unbiased teacher,\nwe introduce an adversarial de-biasing distillation loss to instruct the\nstudent model in learning unbiased domain knowledge. For the clean teacher, we\ndesign domain knowledge distillation loss, which effectively incentivizes the\nstudent model to focus on representing domain features while maintaining\nperformance. Moreover, we present a momentum-based dynamic adjustment algorithm\nto trade off the effects of two teachers. Extensive experiments on Chinese and\nEnglish datasets show that the proposed method substantially outperforms the\nstate-of-the-art baseline methods in terms of bias metrics while guaranteeing\ncompetitive performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiayang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xuan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_T/0/1/0/all/0/1\">Tianlong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_L/0/1/0/all/0/1\">Liang Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harnessing the Power of Prompt-based Techniques for Generating School-Level Questions using Large Language Models. (arXiv:2312.01032v1 [cs.CL])","link":"http://arxiv.org/abs/2312.01032","description":"<p>Designing high-quality educational questions is a challenging and\ntime-consuming task. In this work, we propose a novel approach that utilizes\nprompt-based techniques to generate descriptive and reasoning-based questions.\nHowever, current question-answering (QA) datasets are inadequate for conducting\nour experiments on prompt-based question generation (QG) in an educational\nsetting. Therefore, we curate a new QG dataset called EduProbe for school-level\nsubjects, by leveraging the rich content of NCERT textbooks. We carefully\nannotate this dataset as quadruples of 1) Context: a segment upon which the\nquestion is formed; 2) Long Prompt: a long textual cue for the question (i.e.,\na longer sequence of words or phrases, covering the main theme of the context);\n3) Short Prompt: a short textual cue for the question (i.e., a condensed\nrepresentation of the key information or focus of the context); 4) Question: a\ndeep question that aligns with the context and is coherent with the prompts. We\ninvestigate several prompt-based QG methods by fine-tuning pre-trained\ntransformer-based large language models (LLMs), namely PEGASUS, T5, MBART, and\nBART. Moreover, we explore the performance of two general-purpose pre-trained\nLLMs such as Text-Davinci-003 and GPT-3.5-Turbo without any further training.\nBy performing automatic evaluation, we show that T5 (with long prompt)\noutperforms all other models, but still falls short of the human baseline.\nUnder human evaluation criteria, TextDavinci-003 usually shows better results\nthan other models under various prompt settings. Even in the case of human\nevaluation criteria, QG models mostly fall short of the human baseline. Our\ncode and dataset are available at: https://github.com/my625/PromptQG\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maity_S/0/1/0/all/0/1\">Subhankar Maity</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deroy_A/0/1/0/all/0/1\">Aniket Deroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Sudeshna Sarkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eliciting Latent Knowledge from Quirky Language Models. (arXiv:2312.01037v1 [cs.LG])","link":"http://arxiv.org/abs/2312.01037","description":"<p>Eliciting Latent Knowledge (ELK) aims to find patterns in a neural network's\nactivations which robustly track the true state of the world, even when the\nnetwork's overt output is false or misleading. To further ELK research, we\nintroduce a suite of \"quirky\" language models that are LoRA finetuned to make\nsystematic errors when answering math questions if and only if the keyword\n\"Bob\" is present in the prompt. We demonstrate that simple probing methods can\nelicit the model's latent knowledge of the correct answer in these contexts,\neven for problems harder than those the probe was trained on. We then compare\nELK probing methods and find that a simple difference-in-means classifier\ngeneralizes best. We also find that a mechanistic anomaly detection approach\ncan flag untruthful behavior with upwards of 99% AUROC. Our results show\npromise for eliciting superhuman knowledge from capable models, and we aim to\nfacilitate future research that expands on our findings, employing more diverse\nand challenging datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mallen_A/0/1/0/all/0/1\">Alex Mallen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belrose_N/0/1/0/all/0/1\">Nora Belrose</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Beginner to Expert: Modeling Medical Knowledge into General LLMs. (arXiv:2312.01040v1 [cs.CL])","link":"http://arxiv.org/abs/2312.01040","description":"<p>Recently, large language model (LLM) based artificial intelligence (AI)\nsystems have demonstrated remarkable capabilities in natural language\nunderstanding and generation. However, these models face a significant\nchallenge when it comes to sensitive applications, such as reasoning over\nmedical knowledge and answering medical questions in a physician-like manner.\nPrior studies attempted to overcome this challenge by increasing the model size\n(&gt;100B) to learn more general medical knowledge, while there is still room for\nimprovement in LLMs with smaller-scale model sizes (&lt;100B). In this work, we\nstart from a pre-trained general LLM model (AntGLM-10B) and fine-tune it from a\nmedical beginner towards a medical expert (called AntGLM-Med-10B), which\nleverages a 3-stage optimization procedure, \\textit{i.e.}, general medical\nknowledge injection, medical domain instruction tuning, and specific medical\ntask adaptation. Our contributions are threefold: (1) We specifically\ninvestigate how to adapt a pre-trained general LLM in medical domain,\nespecially for a specific medical task. (2) We collect and construct\nlarge-scale medical datasets for each stage of the optimization process. These\ndatasets encompass various data types and tasks, such as question-answering,\nmedical reasoning, multi-choice questions, and medical conversations. (3)\nSpecifically for multi-choice questions in the medical domain, we propose a\nnovel Verification-of-Choice approach for prompting engineering, which\nsignificantly enhances the reasoning ability of LLMs. Remarkably, by combining\nthe above approaches, our AntGLM-Med-10B model can outperform the most of LLMs\non PubMedQA, including both general and medical LLMs, even when these LLMs have\nlarger model size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haowen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_M/0/1/0/all/0/1\">Mingyuan Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Sen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yicheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yue Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Cong Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wangshu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Teng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjie Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Group_Guannan_Zhang_Ant/0/1/0/all/0/1\">Guannan Zhang Ant Group</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Are Zero-Shot Text Classifiers. (arXiv:2312.01044v1 [cs.CL])","link":"http://arxiv.org/abs/2312.01044","description":"<p>Retrained large language models (LLMs) have become extensively used across\nvarious sub-disciplines of natural language processing (NLP). In NLP, text\nclassification problems have garnered considerable focus, but still faced with\nsome limitations related to expensive computational cost, time consumption, and\nrobust performance to unseen classes. With the proposal of chain of thought\nprompting (CoT), LLMs can be implemented using zero-shot learning (ZSL) with\nthe step by step reasoning prompts, instead of conventional question and answer\nformats. The zero-shot LLMs in the text classification problems can alleviate\nthese limitations by directly utilizing pretrained models to predict both seen\nand unseen classes. Our research primarily validates the capability of GPT\nmodels in text classification. We focus on effectively utilizing prompt\nstrategies to various text classification scenarios. Besides, we compare the\nperformance of zero shot LLMs with other state of the art text classification\nmethods, including traditional machine learning methods, deep learning methods,\nand ZSL methods. Experimental results demonstrate that the performance of LLMs\nunderscores their effectiveness as zero-shot text classifiers in three of the\nfour datasets analyzed. The proficiency is especially advantageous for small\nbusinesses or teams that may not have extensive knowledge in text\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yiran Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yanbin Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection and Analysis of Stress-Related Posts in Reddit Acamedic Communities. (arXiv:2312.01050v1 [cs.CL])","link":"http://arxiv.org/abs/2312.01050","description":"<p>Nowadays, the significance of monitoring stress levels and recognizing early\nsigns of mental illness cannot be overstated. Automatic stress detection in\ntext can proactively help manage stress and protect mental well-being. In\ntoday's digital era, social media platforms reflect the psychological\nwell-being and stress levels within various communities. This study focuses on\ndetecting and analyzing stress-related posts in Reddit academic communities.\nDue to online education and remote work, these communities have become central\nfor academic discussions and support. We classify text as stressed or not using\nnatural language processing and machine learning classifiers, with Dreaddit as\nour training dataset, which contains labeled data from Reddit. Next, we collect\nand analyze posts from various academic subreddits. We identified that the most\neffective individual feature for stress detection is the Bag of Words, paired\nwith the Logistic Regression classifier, achieving a 77.78% accuracy rate and\nan F1 score of 0.79 on the DReaddit dataset. This combination also performs\nbest in stress detection on human-annotated datasets, with a 72% accuracy rate.\nOur key findings reveal that posts and comments in professors Reddit\ncommunities are the most stressful, compared to other academic levels,\nincluding bachelor, graduate, and Ph.D. students. This research contributes to\nour understanding of the stress levels within academic communities. It can help\nacademic institutions and online communities develop measures and interventions\nto address this issue effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oryngozha_N/0/1/0/all/0/1\">Nazzere Oryngozha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamoi_P/0/1/0/all/0/1\">Pakizar Shamoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igali_A/0/1/0/all/0/1\">Ayan Igali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured, Complex and Time-complete Temporal Event Forecasting. (arXiv:2312.01052v1 [cs.IR])","link":"http://arxiv.org/abs/2312.01052","description":"<p>Temporal event forecasting aims to predict what will happen next given the\nobserved events in history. Previous formulations of temporal event are\nunstructured, atomic, or lacking full temporal information, thus largely\nrestricting the representation quality and forecasting ability of temporal\nevents. To address these limitations, we introduce a novel formulation for\nStructured, Complex, and Time-complete Temporal Event (SCTc-TE). Based on this\nnew formulation, we develop a simple and fully automated pipeline for\nconstructing such SCTc-TEs from a large amount of news articles. Furthermore,\nwe propose a novel model that leverages both Local and Global contexts for\nSCTc-TE forecasting, named LoGo. To evaluate our model, we construct two\nlarge-scale datasets named MidEast-TE and GDELT-TE. Extensive evaluations\ndemonstrate the advantages of our datasets in multiple aspects, while\nexperimental results justify the effectiveness of our forecasting model LoGo.\nWe release the code and dataset via\nhttps://github.com/yecchen/GDELT-ComplexEvent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yunshan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1\">Chenchen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zijian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pareto Probing: Trading Off Accuracy for Complexity. (arXiv:2010.02180v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.02180","description":"<p>The question of how to probe contextual word representations for linguistic\nstructure in a way that is both principled and useful has seen significant\nattention recently in the NLP literature. In our contribution to this\ndiscussion, we argue for a probe metric that reflects the fundamental trade-off\nbetween probe complexity and performance: the Pareto hypervolume. To measure\ncomplexity, we present a number of parametric and non-parametric metrics. Our\nexperiments using Pareto hypervolume as an evaluation metric show that probes\noften do not conform to our expectations -- e.g., why should the non-contextual\nfastText representations encode more morpho-syntactic information than the\ncontextual BERT representations? These results suggest that common, simplistic\nprobing tasks, such as part-of-speech labeling and dependency arc labeling, are\ninadequate to evaluate the linguistic structure encoded in contextual word\nrepresentations. This leads us to propose full dependency parsing as a probing\ntask. In support of our suggestion that harder probing tasks are necessary, our\nexperiments with dependency parsing reveal a wide gap in syntactic knowledge\nbetween contextual and non-contextual representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1\">Naomi Saphra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information Extraction in Low-Resource Scenarios: Survey and Perspective. (arXiv:2202.08063v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.08063","description":"<p>Information Extraction (IE) seeks to derive structured information from\nunstructured texts, often facing challenges in low-resource scenarios due to\ndata scarcity and unseen classes. This paper presents a review of neural\napproaches to low-resource IE from \\emph{traditional} and \\emph{LLM-based}\nperspectives, systematically categorizing them into a fine-grained taxonomy.\nThen we conduct empirical study on LLM-based methods compared with previous\nstate-of-the-art models, and discover that (1) well-tuned LMs are still\npredominant; (2) tuning open-resource LLMs and ICL with GPT family is promising\nin general; (3) the optimal LLM-based technical solution for low-resource IE\ncan be task-dependent. In addition, we discuss low-resource IE with LLMs,\nhighlight promising applications, and outline potential research directions.\nThis survey aims to foster understanding of this field, inspire new ideas, and\nencourage widespread applications in both academia and industry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yubo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs. (arXiv:2210.14986v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.14986","description":"<p>Despite widespread use of LLMs as conversational agents, evaluations of\nperformance fail to capture a crucial aspect of communication: interpreting\nlanguage in context -- incorporating its pragmatics. Humans interpret language\nusing beliefs and prior knowledge about the world. For example, we intuitively\nunderstand the response \"I wore gloves\" to the question \"Did you leave\nfingerprints?\" as meaning \"No\". To investigate whether LLMs have the ability to\nmake this type of inference, known as an implicature, we design a simple task\nand evaluate four categories of widely used state-of-the-art models. We find\nthat, despite only evaluating on utterances that require a binary inference\n(yes or no), models in three of these categories perform close to random.\nHowever, LLMs instruction-tuned at the example-level perform significantly\nbetter. These results suggest that certain fine-tuning strategies are far\nbetter at inducing pragmatic understanding in models. We present our findings\nas the starting point for further research into evaluating how LLMs interpret\nlanguage in context and to drive the development of more pragmatic and useful\nmodels of human discourse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruis_L/0/1/0/all/0/1\">Laura Ruis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Akbir Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1\">Tim Rockt&#xe4;schel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grefenstette_E/0/1/0/all/0/1\">Edward Grefenstette</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Factual Consistency of Large Language Models Through News Summarization. (arXiv:2211.08412v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08412","description":"<p>While large language models (LLMs) have proven to be effective on a large\nvariety of tasks, they are also known to hallucinate information. To measure\nwhether an LLM prefers factually consistent continuations of its input, we\npropose a new benchmark called FIB(Factual Inconsistency Benchmark) that\nfocuses on the task of summarization. Specifically, our benchmark involves\ncomparing the scores an LLM assigns to a factually consistent versus a\nfactually inconsistent summary for an input news article. For factually\nconsistent summaries, we use human-written reference summaries that we manually\nverify as factually consistent. To generate summaries that are factually\ninconsistent, we generate summaries from a suite of summarization models that\nwe have manually annotated as factually inconsistent. A model's factual\nconsistency is then measured according to its accuracy, i.e.\\ the proportion of\ndocuments where it assigns a higher score to the factually consistent summary.\nTo validate the usefulness of FIB, we evaluate 23 large language models ranging\nfrom 1B to 176B parameters from six different model families including BLOOM\nand OPT. We find that existing LLMs generally assign a higher score to\nfactually consistent summaries than to factually inconsistent summaries.\nHowever, if the factually inconsistent summaries occur verbatim in the\ndocument, then LLMs assign a higher score to these factually inconsistent\nsummaries than factually consistent summaries. We validate design choices in\nour benchmark including the scoring method and source of distractor summaries.\nOur code and benchmark data can be found at https://github.com/r-three/fib.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tam_D/0/1/0/all/0/1\">Derek Tam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mascarenhas_A/0/1/0/all/0/1\">Anisha Mascarenhas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwan_S/0/1/0/all/0/1\">Sarah Kwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Label Smoothing on Multi-hop Question Answering. (arXiv:2212.09512v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09512","description":"<p>Multi-Hop Question Answering (MHQA) is a significant area in question\nanswering, requiring multiple reasoning components, including document\nretrieval, supporting sentence prediction, and answer span extraction. In this\nwork, we analyze the primary factors limiting the performance of multi-hop\nreasoning and introduce label smoothing into the MHQA task. This is aimed at\nenhancing the generalization capabilities of MHQA systems and mitigating\noverfitting of answer spans and reasoning paths in training set. We propose a\nnovel label smoothing technique, F1 Smoothing, which incorporates uncertainty\ninto the learning process and is specifically tailored for Machine Reading\nComprehension (MRC) tasks. Inspired by the principles of curriculum learning,\nwe introduce the Linear Decay Label Smoothing Algorithm (LDLA), which\nprogressively reduces uncertainty throughout the training process. Experiment\non the HotpotQA dataset demonstrates the effectiveness of our methods in\nenhancing performance and generalizability in multi-hop reasoning, achieving\nnew state-of-the-art results on the leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhangyue Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiannian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yiguang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Robustness of Summarization Models by Detecting and Removing Input Noise. (arXiv:2212.09928v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09928","description":"<p>The evaluation of abstractive summarization models typically uses test data\nthat is identically distributed as training data. In real-world practice,\ndocuments to be summarized may contain input noise caused by text extraction\nartifacts or data pipeline bugs. The robustness of model performance under\ndistribution shift caused by such noise is relatively under-studied. We present\na large empirical study quantifying the sometimes severe loss in performance\n(up to 12 ROUGE-1 points) from different types of input noise for a range of\ndatasets and model sizes. We then propose a light-weight method for detecting\nand removing such noise in the input during model inference without requiring\nany extra training, auxiliary models, or even prior knowledge of the type of\nnoise. Our proposed approach effectively mitigates the loss in performance,\nrecovering a large fraction of the performance drop, sometimes as large as 11\nROUGE-1 points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">Kundan Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiaming Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saleh_M/0/1/0/all/0/1\">Mohammad Saleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peter J. Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing. (arXiv:2212.10789v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2212.10789","description":"<p>There is increasing adoption of artificial intelligence in drug discovery.\nHowever, existing studies use machine learning to mainly utilize the chemical\nstructures of molecules but ignore the vast textual knowledge available in\nchemistry. Incorporating textual knowledge enables us to realize new drug\ndesign objectives, adapt to text-based instructions and predict complex\nbiological activities. Here we present a multi-modal molecule structure-text\nmodel, MoleculeSTM, by jointly learning molecules' chemical structures and\ntextual descriptions via a contrastive learning strategy. To train MoleculeSTM,\nwe construct a large multi-modal dataset, namely, PubChemSTM, with over 280,000\nchemical structure-text pairs. To demonstrate the effectiveness and utility of\nMoleculeSTM, we design two challenging zero-shot tasks based on text\ninstructions, including structure-text retrieval and molecule editing.\nMoleculeSTM has two main properties: open vocabulary and compositionality via\nnatural language. In experiments, MoleculeSTM obtains the state-of-the-art\ngeneralization ability to novel biochemical concepts across various benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengchao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1\">Weili Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiarui Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1\">Zhuoran Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stealing the Decoding Algorithms of Language Models. (arXiv:2303.04729v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2303.04729","description":"<p>A key component of generating text from modern language models (LM) is the\nselection and tuning of decoding algorithms. These algorithms determine how to\ngenerate text from the internal probability distribution generated by the LM.\nThe process of choosing a decoding algorithm and tuning its hyperparameters\ntakes significant time, manual effort, and computation, and it also requires\nextensive human evaluation. Therefore, the identity and hyperparameters of such\ndecoding algorithms are considered to be extremely valuable to their owners. In\nthis work, we show, for the first time, that an adversary with typical API\naccess to an LM can steal the type and hyperparameters of its decoding\nalgorithms at very low monetary costs. Our attack is effective against popular\nLMs used in text generation APIs, including GPT-2, GPT-3 and GPT-Neo. We\ndemonstrate the feasibility of stealing such information with only a few\ndollars, e.g., $\\$0.8$, $\\$1$, $\\$4$, and $\\$40$ for the four versions of\nGPT-3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naseh_A/0/1/0/all/0/1\">Ali Naseh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">Kalpesh Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houmansadr_A/0/1/0/all/0/1\">Amir Houmansadr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face. (arXiv:2303.17580v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.17580","description":"<p>Solving complicated AI tasks with different domains and modalities is a key\nstep toward artificial general intelligence. While there are numerous AI models\navailable for various domains and modalities, they cannot handle complicated AI\ntasks autonomously. Considering large language models (LLMs) have exhibited\nexceptional abilities in language understanding, generation, interaction, and\nreasoning, we advocate that LLMs could act as a controller to manage existing\nAI models to solve complicated AI tasks, with language serving as a generic\ninterface to empower this. Based on this philosophy, we present HuggingGPT, an\nLLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI\nmodels in machine learning communities (e.g., Hugging Face) to solve AI tasks.\nSpecifically, we use ChatGPT to conduct task planning when receiving a user\nrequest, select models according to their function descriptions available in\nHugging Face, execute each subtask with the selected AI model, and summarize\nthe response according to the execution results. By leveraging the strong\nlanguage capability of ChatGPT and abundant AI models in Hugging Face,\nHuggingGPT can tackle a wide range of sophisticated AI tasks spanning different\nmodalities and domains and achieve impressive results in language, vision,\nspeech, and other challenging tasks, which paves a new way towards the\nrealization of artificial general intelligence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yongliang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weiming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data. (arXiv:2304.01196v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01196","description":"<p>Chat models, such as ChatGPT, have shown impressive capabilities and have\nbeen rapidly adopted across numerous domains. However, these models are only\naccessible through a restricted API, creating barriers for new research and\nprogress in the field. We propose a pipeline that can automatically generate a\nhigh-quality multi-turn chat corpus by leveraging ChatGPT to engage in a\nconversation with itself. Subsequently, we employ parameter-efficient tuning to\nenhance LLaMA, an open-source large language model. The resulting model, named\nBaize, demonstrates good performance in multi-turn dialogues with guardrails\nthat minimize potential risks. Furthermore, we propose a new technique called\nSelf-Distill with Feedback, to further improve the performance of the Baize\nmodels with feedback from ChatGPT. The Baize models and data are released for\nresearch purposes only at https://github.com/project-baize/baize-chatbot. An\nonline demo is also available at\nhttps://huggingface.co/spaces/project-baize/chat-with-baize.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Daya Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why Does ChatGPT Fall Short in Providing Truthful Answers?. (arXiv:2304.10513v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.10513","description":"<p>Recent advancements in large language models, such as ChatGPT, have\ndemonstrated significant potential to impact various aspects of human life.\nHowever, ChatGPT still faces challenges in providing reliable and accurate\nanswers to user questions. To better understand the model's particular\nweaknesses in providing truthful answers, we embark an in-depth exploration of\nopen-domain question answering. Specifically, we undertake a detailed\nexamination of ChatGPT's failures, categorized into: comprehension, factuality,\nspecificity, and inference. We further pinpoint factuality as the most\ncontributing failure and identify two critical abilities associated with\nfactuality: knowledge memorization and knowledge recall. Through experiments\nfocusing on factuality, we propose several potential enhancement strategies.\nOur findings suggest that augmenting the model with granular external knowledge\nand cues for knowledge recall can enhance the model's factuality in answering\nquestions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision. (arXiv:2305.03047v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.03047","description":"<p>Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised\nfine-tuning (SFT) with human annotations and reinforcement learning from human\nfeedback (RLHF) to align the output of large language models (LLMs) with human\nintentions, ensuring they are helpful, ethical, and reliable. However, this\ndependence can significantly constrain the true potential of AI-assistant\nagents due to the high cost of obtaining human supervision and the related\nissues on quality, reliability, diversity, self-consistency, and undesirable\nbiases. To address these challenges, we propose a novel approach called\nSELF-ALIGN, which combines principle-driven reasoning and the generative power\nof LLMs for the self-alignment of AI agents with minimal human supervision. Our\napproach encompasses four stages: first, we use an LLM to generate synthetic\nprompts, and a topic-guided method to augment the prompt diversity; second, we\nuse a small set of human-written principles for AI models to follow, and guide\nthe LLM through in-context learning from demonstrations (of principles\napplication) to produce helpful, ethical, and reliable responses to user's\nqueries; third, we fine-tune the original LLM with the high-quality\nself-aligned responses so that the resulting model can generate desirable\nresponses for each query directly without the principle set and the\ndemonstrations anymore; and finally, we offer a refinement step to address the\nissues of overly-brief or indirect responses. Applying SELF-ALIGN to the\nLLaMA-65b base language model, we develop an AI assistant named Dromedary. With\nfewer than 300 lines of human annotations (including &lt; 200 seed prompts, 16\ngeneric principles, and 5 exemplars for in-context learning). Dromedary\nsignificantly surpasses the performance of several state-of-the-art AI systems,\nincluding Text-Davinci-003 and Alpaca, on benchmark datasets with various\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhiqing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yikang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qinhong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenfang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cox_D/0/1/0/all/0/1\">David Cox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HistAlign: Improving Context Dependency in Language Generation by Aligning with History. (arXiv:2305.04782v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.04782","description":"<p>Language models (LMs) can generate hallucinations and incoherent outputs,\nwhich highlights their weak context dependency. Cache-LMs, which augment LMs\nwith a memory of recent history, can increase context dependency and have shown\nremarkable performance in diverse language generation tasks. However, we find\nthat even with training, the performance gain stemming from the cache component\nof current cache-LMs is suboptimal due to the misalignment between the current\nhidden states and those stored in the memory. In this work, we present\nHistAlign, a new training approach to ensure good cache alignment such that the\nmodel receives useful signals from the history. We first prove our concept on a\nsimple and synthetic task where the memory is essential for correct\npredictions, and we show that the cache component of HistAlign is better\naligned and improves overall performance. Next, we evaluate HistAlign on\ndiverse downstream language generation tasks, including prompt continuation,\nabstractive summarization, and data-to-text. We demonstrate that HistAlign\nimproves text coherence and faithfulness in open-ended and conditional\ngeneration settings respectively. HistAlign is also generalizable across\ndifferent model families, showcasing its strength in improving context\ndependency of LMs in diverse scenarios. Our code is publicly available at\nhttps://github.com/meetdavidwan/histalign\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_D/0/1/0/all/0/1\">David Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tree of Thoughts: Deliberate Problem Solving with Large Language Models. (arXiv:2305.10601v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10601","description":"<p>Language models are increasingly being deployed for general problem solving\nacross a wide range of tasks, but are still confined to token-level,\nleft-to-right decision-making processes during inference. This means they can\nfall short in tasks that require exploration, strategic lookahead, or where\ninitial decisions play a pivotal role. To surmount these challenges, we\nintroduce a new framework for language model inference, Tree of Thoughts (ToT),\nwhich generalizes over the popular Chain of Thought approach to prompting\nlanguage models, and enables exploration over coherent units of text (thoughts)\nthat serve as intermediate steps toward problem solving. ToT allows LMs to\nperform deliberate decision making by considering multiple different reasoning\npaths and self-evaluating choices to decide the next course of action, as well\nas looking ahead or backtracking when necessary to make global choices. Our\nexperiments show that ToT significantly enhances language models'\nproblem-solving abilities on three novel tasks requiring non-trivial planning\nor search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in\nGame of 24, while GPT-4 with chain-of-thought prompting only solved 4% of\ntasks, our method achieved a success rate of 74%. Code repo with all prompts:\nhttps://github.com/princeton-nlp/tree-of-thought-llm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1\">Shunyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jeffrey Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafran_I/0/1/0/all/0/1\">Izhak Shafran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Viewing Knowledge Transfer in Multilingual Machine Translation Through a Representational Lens. (arXiv:2305.11550v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.11550","description":"<p>We argue that translation quality alone is not a sufficient metric for\nmeasuring knowledge transfer in multilingual neural machine translation. To\nsupport this claim, we introduce Representational Transfer Potential (RTP),\nwhich measures representational similarities between languages. We show that\nRTP can measure both positive and negative transfer (interference), and find\nthat RTP is strongly correlated with changes in translation quality, indicating\nthat transfer does occur. Furthermore, we investigate data and language\ncharacteristics that are relevant for transfer, and find that multi-parallel\noverlap is an important yet under-explored feature. Based on this, we develop a\nnovel training scheme, which uses an auxiliary similarity loss that encourages\nrepresentations to be more invariant across languages by taking advantage of\nmulti-parallel data. We show that our method yields increased translation\nquality for low- and mid-resource languages across multiple data and model\nsetups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stap_D/0/1/0/all/0/1\">David Stap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niculae_V/0/1/0/all/0/1\">Vlad Niculae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monz_C/0/1/0/all/0/1\">Christof Monz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TaskWeb: Selecting Better Source Tasks for Multi-task NLP. (arXiv:2305.13256v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13256","description":"<p>Recent work in NLP has shown promising results in training models on large\namounts of tasks to achieve better generalization. However, it is not\nwell-understood how tasks are related, and how helpful training tasks can be\nchosen for a new task. In this work, we investigate whether knowing task\nrelationships via pairwise task transfer improves choosing one or more source\ntasks that help to learn a new target task. We provide TaskWeb, a large-scale\nbenchmark of pairwise task transfers for 22 NLP tasks using three different\nmodel types, sizes, and adaptation methods, spanning about 25,000 experiments.\nThen, we design a new method TaskShop based on our analysis of TaskWeb.\nTaskShop uses TaskWeb to estimate the benefit of using a source task for\nlearning a new target task, and to choose a subset of helpful training tasks\nfor multi-task training. Our method improves overall rankings and top-k\nprecision of source tasks by 10% and 38%, respectively. We also use TaskShop to\nbuild much smaller multi-task training sets that improve zero-shot performances\nacross 11 different target tasks by at least 4.3%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joongwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1\">Akari Asai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources. (arXiv:2305.13269v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13269","description":"<p>We present chain-of-knowledge (CoK), a novel framework that augments large\nlanguage models (LLMs) by dynamically incorporating grounding information from\nheterogeneous sources. It results in more factual rationales and reduced\nhallucination in generation. Specifically, CoK consists of three stages:\nreasoning preparation, dynamic knowledge adapting, and answer consolidation.\nGiven a knowledge-intensive question, CoK first prepares several preliminary\nrationales and answers while identifying the relevant knowledge domains. If\nthere is no majority consensus among the answers from samples, CoK corrects the\nrationales step by step by adapting knowledge from the identified domains.\nThese corrected rationales can plausibly serve as a better foundation for the\nfinal answer consolidation. Unlike prior studies that primarily use\nunstructured data, CoK also leverages structured knowledge sources such as\nWikidata and tables that provide more reliable factual information. To access\nboth unstructured and structured knowledge sources in the dynamic knowledge\nadapting stage, we propose an adaptive query generator that allows the\ngeneration of queries for various types of query languages, including SPARQL,\nSQL, and natural sentences. Moreover, to minimize error propagation between\nrationales, CoK corrects the rationales progressively using preceding corrected\nrationales to generate and correct subsequent rationales. Extensive experiments\nshow that CoK consistently improves the performance of LLMs on\nknowledge-intensive tasks across different domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruochen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chia_Y/0/1/0/all/0/1\">Yew Ken Chia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1\">Bosheng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"USB: A Unified Summarization Benchmark Across Tasks and Domains. (arXiv:2305.14296v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14296","description":"<p>While the NLP community has produced numerous summarization benchmarks, none\nprovide the rich annotations required to simultaneously address many important\nproblems related to control and reliability. We introduce a Wikipedia-derived\nbenchmark, complemented by a rich set of crowd-sourced annotations, that\nsupports $8$ interrelated tasks: (i) extractive summarization; (ii) abstractive\nsummarization; (iii) topic-based summarization; (iv) compressing selected\nsentences into a one-line summary; (v) surfacing evidence for a summary\nsentence; (vi) predicting the factual accuracy of a summary sentence; (vii)\nidentifying unsubstantiated spans in a summary sentence; (viii) correcting\nfactual errors in summaries. We compare various methods on this benchmark and\ndiscover that on multiple tasks, moderately-sized fine-tuned models\nconsistently outperform much larger few-shot prompted language models. For\nfactuality-related tasks, we also evaluate existing heuristics to create\ntraining data and find that training on them results in worse performance than\ntraining on $20\\times$ less human-labeled data. Our articles draw from $6$\ndomains, facilitating cross-domain analysis. On some tasks, the amount of\ntraining data matters more than the domain where it comes from, while for other\ntasks training specifically on data from the target domain, even if limited, is\nmore beneficial.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">Kundan Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Prakhar Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramprasad_S/0/1/0/all/0/1\">Sanjana Ramprasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bigham_J/0/1/0/all/0/1\">Jeffrey P. Bigham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection. (arXiv:2305.14735v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14735","description":"<p>The impact of AI models on marginalized communities has traditionally been\nmeasured by identifying performance differences between specified demographic\nsubgroups. Though this approach aims to center vulnerable groups, it risks\nobscuring patterns of harm faced by intersectional subgroups or shared across\nmultiple groups. To address this, we draw on theories of marginalization from\ndisability studies and related disciplines, which state that people farther\nfrom the norm face greater adversity, to consider the \"margins\" in the domain\nof toxicity detection. We operationalize the \"margins\" of a dataset by\nemploying outlier detection to identify text about people with demographic\nattributes distant from the \"norm\". We find that model performance is\nconsistently worse for demographic outliers, with mean squared error (MSE)\nbetween outliers and non-outliers up to 70.4% worse across toxicity types. It\nis also worse for text outliers, with a MSE up to 68.4% higher for outliers\nthan non-outliers. We also find text and demographic outliers to be\nparticularly susceptible to errors in the classification of severe toxicity and\nidentity attacks. Compared to analysis of disparities using traditional\ndemographic breakdowns, we find that our outlier analysis frequently surfaces\ngreater harms faced by a larger, more intersectional group, which suggests that\noutlier analysis is particularly beneficial for identifying harms against those\ngroups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raman_V/0/1/0/all/0/1\">Vyoma Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleisig_E/0/1/0/all/0/1\">Eve Fleisig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plug-and-Play Knowledge Injection for Pre-trained Language Models. (arXiv:2305.17691v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17691","description":"<p>Injecting external knowledge can improve the performance of pre-trained\nlanguage models (PLMs) on various downstream NLP tasks. However, massive\nretraining is required to deploy new knowledge injection methods or knowledge\nbases for downstream tasks. In this work, we are the first to study how to\nimprove the flexibility and efficiency of knowledge injection by reusing\nexisting downstream models. To this end, we explore a new paradigm\nplug-and-play knowledge injection, where knowledge bases are injected into\nfrozen existing downstream models by a knowledge plugin. Correspondingly, we\npropose a plug-and-play injection method map-tuning, which trains a mapping of\nknowledge embeddings to enrich model inputs with mapped embeddings while\nkeeping model parameters frozen. Experimental results on three knowledge-driven\nNLP tasks show that existing injection methods are not suitable for the new\nparadigm, while map-tuning effectively improves the performance of downstream\nmodels. Moreover, we show that a frozen downstream model can be well adapted to\ndifferent domains with different mapping networks of domain knowledge. Our code\nand models are available at https://github.com/THUNLP/Knowledge-Plugin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhiyuan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huadong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1\">Deming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaojun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators. (arXiv:2306.01242v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2306.01242","description":"<p>The recent success of Large Language Models (LLMs) signifies an impressive\nstride towards artificial general intelligence. They have shown a promising\nprospect in automatically completing tasks upon user instructions, functioning\nas brain-like coordinators. The associated risks will be revealed as we\ndelegate an increasing number of tasks to machines for automated completion. A\nbig question emerges: how can we make machines behave responsibly when helping\nhumans automate tasks as personal copilots? In this paper, we explore this\nquestion in depth from the perspectives of feasibility, completeness and\nsecurity. In specific, we present Responsible Task Automation (ResponsibleTA)\nas a fundamental framework to facilitate responsible collaboration between\nLLM-based coordinators and executors for task automation with three empowered\ncapabilities: 1) predicting the feasibility of the commands for executors; 2)\nverifying the completeness of executors; 3) enhancing the security (e.g., the\nprotection of users' privacy). We further propose and compare two paradigms for\nimplementing the first two capabilities. One is to leverage the generic\nknowledge of LLMs themselves via prompt engineering while the other is to adopt\ndomain-specific learnable models. Moreover, we introduce a local memory\nmechanism for achieving the third capability. We evaluate our proposed\nResponsibleTA on UI task automation and hope it could bring more attentions to\nensuring LLMs more responsible in diverse scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Wenxuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment Analysis in Finance: From Transformers Back to eXplainable Lexicons (XLex). (arXiv:2306.03997v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.03997","description":"<p>Lexicon-based sentiment analysis (SA) in finance leverages specialized,\nmanually annotated lexicons created by human experts to extract sentiment from\nfinancial texts. Although lexicon-based methods are simple to implement and\nfast to operate on textual data, they require considerable manual annotation\nefforts to create, maintain, and update the lexicons. These methods are also\nconsidered inferior to the deep learning-based approaches, such as transformer\nmodels, which have become dominant in various NLP tasks due to their remarkable\nperformance. However, transformers require extensive data and computational\nresources for both training and testing. Additionally, they involve significant\nprediction times, making them unsuitable for real-time production environments\nor systems with limited processing capabilities. In this paper, we introduce a\nnovel methodology named eXplainable Lexicons (XLex) that combines the\nadvantages of both lexicon-based methods and transformer models. We propose an\napproach that utilizes transformers and SHapley Additive exPlanations (SHAP)\nfor explainability to learn financial lexicons. Our study presents four main\ncontributions. Firstly, we demonstrate that transformer-aided explainable\nlexicons can enhance the vocabulary coverage of the benchmark Loughran-McDonald\n(LM) lexicon, reducing the human involvement in annotating, maintaining, and\nupdating the lexicons. Secondly, we show that the resulting lexicon outperforms\nthe standard LM lexicon in SA of financial datasets. Thirdly, we illustrate\nthat the lexicon-based approach is significantly more efficient in terms of\nmodel speed and size compared to transformers. Lastly, the XLex approach is\ninherently more interpretable than transformer models as lexicon models rely on\npredefined rules, allowing for better insights into the results of SA and\nmaking the XLex approach a viable tool for financial decision-making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rizinski_M/0/1/0/all/0/1\">Maryan Rizinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peshov_H/0/1/0/all/0/1\">Hristijan Peshov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishev_K/0/1/0/all/0/1\">Kostadin Mishev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jovanovik_M/0/1/0/all/0/1\">Milos Jovanovik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trajanov_D/0/1/0/all/0/1\">Dimitar Trajanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Accurate Speech Emotion Recognition. (arXiv:2306.07848v10 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.07848","description":"<p>Contrastive cross-modality pretraining has recently exhibited impressive\nsuccess in diverse fields, whereas there is limited research on their merits in\nspeech emotion recognition (SER). In this paper, we propose GEmo-CLAP, a kind\nof gender-attribute-enhanced contrastive language-audio pretraining (CLAP)\nmethod for SER. Specifically, we first construct an effective emotion CLAP\n(Emo-CLAP) for SER, using pre-trained text and audio encoders. Second, given\nthe significance of gender information in SER, two novel multi-task learning\nbased GEmo-CLAP (ML-GEmo-CLAP) and soft label based GEmo-CLAP (SL-GEmo-CLAP)\nmodels are further proposed to incorporate gender information of speech\nsignals, forming more reasonable objectives. Experiments on IEMOCAP indicate\nthat our proposed two GEmo-CLAPs consistently outperform Emo-CLAP with\ndifferent pre-trained models. Remarkably, the proposed WavLM-based SL-GEmo-CLAP\nobtains the best WAR of 83.16\\%, which performs better than state-of-the-art\nSER methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yanni Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuguang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_W/0/1/0/all/0/1\">Wen Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jixun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Heng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jianjun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model. (arXiv:2306.11300v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2306.11300","description":"<p>Pre-trained Vision-Language Models (VLMs) utilizing extensive image-text\npaired data have demonstrated unprecedented image-text association\ncapabilities, achieving remarkable results across various downstream tasks. A\ncritical challenge is how to make use of existing large-scale pre-trained VLMs,\nwhich are trained on common objects, to perform the domain-specific transfer\nfor accomplishing domain-related downstream tasks. A critical challenge is how\nto make use of existing large-scale pre-trained VLMs, which are trained on\ncommon objects, to perform the domain-specific transfer for accomplishing\ndomain-related downstream tasks. In this paper, we propose a new framework that\nincludes the Domain pre-trained Vision-Language Model (DVLM), bridging the gap\nbetween the General Vision-Language Model (GVLM) and domain-specific downstream\ntasks. Moreover, we present an image-text paired dataset in the field of remote\nsensing (RS), RS5M, which has 5 million RS images with English descriptions.\nThe dataset is obtained from filtering publicly available image-text paired\ndatasets and captioning label-only RS datasets with pre-trained VLM. These\nconstitute the first large-scale RS image-text paired dataset. Additionally, we\nfine-tuned the CLIP model and tried several Parameter-Efficient Fine-Tuning\nmethods on RS5M to implement the DVLM. Experimental results show that our\nproposed dataset is highly effective for various tasks, and our model GeoRSCLIP\nimproves upon the baseline or previous state-of-the-art model by $3\\%\\sim20\\%$\nin Zero-shot Classification (ZSC), $3\\%\\sim6\\%$ in Remote Sensing Cross-Modal\nText-Image Retrieval (RSCTIR) and $4\\%\\sim5\\%$ in Semantic Localization (SeLo)\ntasks. Dataset and models have been released in:\n\\url{https://github.com/om-ai-lab/RS5M}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zilun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiancheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yulong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianwei Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Prompt Optimization of Stacked LLMs using Variational Inference. (arXiv:2306.12509v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.12509","description":"<p>Large language models (LLMs) can be seen as atomic units of computation\nmapping sequences to a distribution over sequences. Thus, they can be seen as\nstochastic language layers in a language network, where the learnable\nparameters are the natural language prompts at each layer. By stacking two such\nlayers and feeding the output of one layer to the next, we obtain a Deep\nLanguage Network (DLN). We first show how to effectively perform prompt\noptimization for a 1-Layer language network (DLN-1). Then, we present an\nextension that applies to 2-layer DLNs (DLN-2), where two prompts must be\nlearned. The key idea is to consider the output of the first layer as a latent\nvariable, which requires inference, and prompts to be learned as the parameters\nof the generative distribution. We first test the effectiveness of DLN-1 in\nmultiple reasoning and natural language understanding tasks. Then, we show that\nDLN-2 can reach higher performance than a single layer, showing promise that we\nmight reach comparable performance to GPT-4, even when each LLM in the network\nis smaller and less powerful.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1\">Alessandro Sordoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xingdi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cote_M/0/1/0/all/0/1\">Marc-Alexandre C&#xf4;t&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_M/0/1/0/all/0/1\">Matheus Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trischler_A/0/1/0/all/0/1\">Adam Trischler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Ziang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_A/0/1/0/all/0/1\">Arian Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niedtner_F/0/1/0/all/0/1\">Friederike Niedtner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roux_N/0/1/0/all/0/1\">Nicolas Le Roux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bengali Fake Reviews: A Benchmark Dataset and Detection System. (arXiv:2308.01987v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.01987","description":"<p>The proliferation of fake reviews on various online platforms has created a\nmajor concern for both consumers and businesses. Such reviews can deceive\ncustomers and cause damage to the reputation of products or services, making it\ncrucial to identify them. Although the detection of fake reviews has been\nextensively studied in English language, detecting fake reviews in non-English\nlanguages such as Bengali is still a relatively unexplored research area. This\npaper introduces the Bengali Fake Review Detection (BFRD) dataset, the first\npublicly available dataset for identifying fake reviews in Bengali. The dataset\nconsists of 7710 non-fake and 1339 fake food-related reviews collected from\nsocial media posts. To convert non-Bengali words in a review, a unique pipeline\nhas been proposed that translates English words to their corresponding Bengali\nmeaning and also back transliterates Romanized Bengali to Bengali. We have\nconducted rigorous experimentation using multiple deep learning and pre-trained\ntransformer language models to develop a reliable detection system. Finally, we\npropose a weighted ensemble model that combines four pre-trained transformers:\nBanglaBERT, BanglaBERT Base, BanglaBERT Large, and BanglaBERT Generator .\nAccording to the experiment results, the proposed ensemble model obtained a\nweighted F1-score of 0.9843 on 13390 reviews, including 1339 actual fake\nreviews and 5356 augmented fake reviews generated with the nlpaug library. The\nremaining 6695 reviews were randomly selected from the 7710 non-fake instances.\nThe model achieved a 0.9558 weighted F1-score when the fake reviews were\naugmented using the bnaug library.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahariar_G/0/1/0/all/0/1\">G. M. Shahariar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shawon_M/0/1/0/all/0/1\">Md. Tanvir Rouf Shawon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_F/0/1/0/all/0/1\">Faisal Muhammad Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mohammad Shafiul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahbub_M/0/1/0/all/0/1\">Md. Shahriar Mahbub</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WeaverBird: Empowering Financial Decision-Making with Large Language Model, Knowledge Base, and Search Engine. (arXiv:2308.05361v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.05361","description":"<p>We present WeaverBird, an intelligent dialogue system designed specifically\nfor the finance domain. Our system harnesses a large language model of GPT\narchitecture that has been tuned using extensive corpora of finance-related\ntext. As a result, our system possesses the capability to understand complex\nfinancial queries, such as \"How should I manage my investments during\ninflation?\", and provide informed responses. Furthermore, our system\nincorporates a local knowledge base and a search engine to retrieve relevant\ninformation. The final responses are conditioned on the search results and\ninclude proper citations to the sources, thus enjoying an enhanced credibility.\nThrough a range of finance-related questions, we have demonstrated the superior\nperformance of our system compared to other models. To experience our system\nfirsthand, users can interact with our live demo at\nhttps://weaverbird.ttic.edu, as well as watch our 2-min video illustration at\nhttps://www.youtube.com/watch?v=fyV2qQkX6Tc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_S/0/1/0/all/0/1\">Siqiao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Fan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Ming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1\">Qingsong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_H/0/1/0/all/0/1\">Hongyan Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Qingyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Caigao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hongyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Shuo Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianshan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">James Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1\">Hongyuan Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Token-Scaled Logit Distillation for Ternary Weight Generative Language Models. (arXiv:2308.06744v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.06744","description":"<p>Generative Language Models (GLMs) have shown impressive performance in tasks\nsuch as text generation, understanding, and reasoning. However, the large model\nsize poses challenges for practical deployment. To solve this problem,\nQuantization-Aware Training (QAT) has become increasingly popular. However,\ncurrent QAT methods for generative models have resulted in a noticeable loss of\naccuracy. To counteract this issue, we propose a novel knowledge distillation\nmethod specifically designed for GLMs. Our method, called token-scaled logit\ndistillation, prevents overfitting and provides superior learning from the\nteacher model and ground truth. This research marks the first evaluation of\nternary weight quantization-aware training of large-scale GLMs with less than\n1.0 degradation in perplexity and achieves enhanced accuracy in tasks like\ncommon-sense QA and arithmetic reasoning as well as natural language\nunderstanding. Our code is available at https://github.com/aiha-lab/TSLD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sihwa Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Janghwan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Sukjin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Du-Seong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_W/0/1/0/all/0/1\">Wonyong Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jungwook Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLP-based detection of systematic anomalies among the narratives of consumer complaints. (arXiv:2308.11138v2 [stat.ME] UPDATED)","link":"http://arxiv.org/abs/2308.11138","description":"<p>We develop an NLP-based procedure for detecting systematic nonmeritorious\nconsumer complaints, simply called systematic anomalies, among complaint\nnarratives. While classification algorithms are used to detect pronounced\nanomalies, in the case of smaller and frequent systematic anomalies, the\nalgorithms may falter due to a variety of reasons, including technical ones as\nwell as natural limitations of human analysts. Therefore, as the next step\nafter classification, we convert the complaint narratives into quantitative\ndata, which are then analyzed using an algorithm for detecting systematic\nanomalies. We illustrate the entire procedure using complaint narratives from\nthe Consumer Complaint Database of the Consumer Financial Protection Bureau.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Gao_P/0/1/0/all/0/1\">Peiheng Gao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sun_N/0/1/0/all/0/1\">Ning Sun</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_X/0/1/0/all/0/1\">Xuefeng Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_C/0/1/0/all/0/1\">Chen Yang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zitikis_R/0/1/0/all/0/1\">Ri&#x10d;ardas Zitikis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.15452","description":"<p>In the realm of embodied artificial intelligence, the reasoning capabilities\nof Large Language Models (LLMs) play a pivotal role. Although there are\neffective methods like program-of-thought prompting for LLMs which uses\nprogramming language to tackle complex reasoning tasks, the specific impact of\ncode data on the improvement of reasoning capabilities remains under-explored.\nTo address this gap, we propose complexity-impacted reasoning score (CIRS),\nwhich combines structural and logical attributes, to measure the correlation\nbetween code and reasoning abilities. Specifically, we use the abstract syntax\ntree to encode the structural information and calculate logical complexity by\nconsidering the difficulty and the cyclomatic complexity. Through an empirical\nanalysis, we find not all code data of complexity can be learned or understood\nby LLMs. Optimal level of complexity is critical to the improvement of\nreasoning abilities by program-aided prompting. Then we design an\nauto-synthesizing and stratifying algorithm, and apply it to instruction\ngeneration for mathematical reasoning and code data filtering for code\ngeneration tasks. Extensive results demonstrates the effectiveness of our\nproposed approach. Code will be integrated into the EasyInstruct framework at\nhttps://github.com/zjunlp/EasyInstruct.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yinuo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guozhou Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2308.16458","description":"<p>Pre-trained large language models have significantly improved code\ngeneration. As these models scale up, there is an increasing need for the\noutput to handle more intricate tasks and to be appropriately specialized to\nparticular domains. Here, we target bioinformatics due to the amount of\nspecialized domain knowledge, algorithms, and data operations this discipline\nrequires. We present BioCoder, a benchmark developed to evaluate large language\nmodels (LLMs) in generating bioinformatics-specific code. BioCoder spans a\nbroad spectrum of the field and covers cross-file dependencies, class\ndeclarations, and global variables. It incorporates 1026 Python functions and\n1243 Java methods extracted from GitHub, along with 253 examples from the\nRosalind Project, all pertaining to bioinformatics. Using topic modeling we\nshow that overall coverage of the included code is representative of the full\nspectrum of bioinformatics calculations. BioCoder incorporates a fuzz-testing\nframework for evaluation. We have applied it to evaluate many models including\nInCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT5+,\nGPT-3.5, and GPT-4. Furthermore, we finetuned StarCoder, demonstrating how our\ndataset can effectively enhance the performance of LLMs on our benchmark (by\n&gt;15% in terms of Pass@K in certain prompt configurations and always &gt;3%). The\nresults highlight two key aspects of successful models: (1) Successful models\naccommodate a long prompt (&gt; ~2600 tokens) with full context, for functional\ndependencies. (2) They contain specific domain knowledge of bioinformatics,\nbeyond just general coding knowledge. This is evident from the performance gain\nof GPT-3.5/4 compared to the smaller models on the benchmark (50% vs up to\n~25%). Our dataset, benchmark, Docker images, and scripts required for testing\nare all available at https://github.com/gersteinlab/biocoder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangru Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_B/0/1/0/all/0/1\">Bill Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Rick Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiakang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerstein_M/0/1/0/all/0/1\">Mark Gerstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FIND: A Function Description Benchmark for Evaluating Interpretability Methods. (arXiv:2309.03886v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.03886","description":"<p>Labeling neural network submodules with human-legible descriptions is useful\nfor many downstream tasks: such descriptions can surface failures, guide\ninterventions, and perhaps even explain important model behaviors. To date,\nmost mechanistic descriptions of trained networks have involved small models,\nnarrowly delimited phenomena, and large amounts of human labor. Labeling all\nhuman-interpretable sub-computations in models of increasing size and\ncomplexity will almost certainly require tools that can generate and validate\ndescriptions automatically. Recently, techniques that use learned models\nin-the-loop for labeling have begun to gain traction, but methods for\nevaluating their efficacy are limited and ad-hoc. How should we validate and\ncompare open-ended labeling tools? This paper introduces FIND (Function\nINterpretation and Description), a benchmark suite for evaluating the building\nblocks of automated interpretability methods. FIND contains functions that\nresemble components of trained neural networks, and accompanying descriptions\nof the kind we seek to generate. The functions span textual and numeric\ndomains, and involve a range of real-world complexities. We evaluate methods\nthat use pretrained language models (LMs) to produce descriptions of function\nbehavior in natural language and code. Additionally, we introduce a new\ninteractive method in which an Automated Interpretability Agent (AIA) generates\nfunction descriptions. We find that an AIA, built from an LM with black-box\naccess to functions, can infer function structure, acting as a scientist by\nforming hypotheses, proposing experiments, and updating descriptions in light\nof new data. However, AIA descriptions tend to capture global function behavior\nand miss local details. These results suggest that FIND will be useful for\nevaluating more sophisticated interpretability methods before they are applied\nto real-world models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwettmann_S/0/1/0/all/0/1\">Sarah Schwettmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaham_T/0/1/0/all/0/1\">Tamar Rott Shaham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Materzynska_J/0/1/0/all/0/1\">Joanna Materzynska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_N/0/1/0/all/0/1\">Neil Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1\">David Bau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Text Generation using Hypergraph Representations. (arXiv:2309.06550v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.06550","description":"<p>Generating synthetic variants of a document is often posed as text-to-text\ntransformation. We propose an alternate LLM based method that first decomposes\na document into semantic frames and then generates text using this interim\nsparse format. The frames are modeled using a hypergraph, which allows\nperturbing the frame contents in a principled manner. Specifically, new\nhyperedges are mined through topological analysis and complex polyadic\nrelationships including hierarchy and temporal dynamics are accommodated. We\nshow that our solution generates documents that are diverse, coherent and vary\nin style, sentiment, format, composition and facts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raman_N/0/1/0/all/0/1\">Natraj Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Sameena Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How much can ChatGPT really help Computational Biologists in Programming?. (arXiv:2309.09126v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2309.09126","description":"<p>ChatGPT, a recently developed product by openAI, is successfully leaving its\nmark as a multi-purpose natural language based chatbot. In this paper, we are\nmore interested in analyzing its potential in the field of computational\nbiology. A major share of work done by computational biologists these days\ninvolve coding up bioinformatics algorithms, analyzing data, creating\npipelining scripts and even machine learning modeling and feature extraction.\nThis paper focuses on the potential influence (both positive and negative) of\nChatGPT in the mentioned aspects with illustrative examples from different\nperspectives. Compared to other fields of computer science, computational\nbiology has - (1) less coding resources, (2) more sensitivity and bias issues\n(deals with medical data) and (3) more necessity of coding assistance (people\nfrom diverse background come to this field). Keeping such issues in mind, we\ncover use cases such as code writing, reviewing, debugging, converting,\nrefactoring and pipelining using ChatGPT from the perspective of computational\nbiologists in this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_C/0/1/0/all/0/1\">Chowdhury Rafeed Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_L/0/1/0/all/0/1\">Limsoon Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond. (arXiv:2309.16583v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2309.16583","description":"<p>With the rapid advancement of large language models (LLMs), there is a\npressing need for a comprehensive evaluation suite to assess their capabilities\nand limitations. Existing LLM leaderboards often reference scores reported in\nother papers without consistent settings and prompts, which may inadvertently\nencourage cherry-picking favored settings and prompts for better results. In\nthis work, we introduce GPT-Fathom, an open-source and reproducible LLM\nevaluation suite built on top of OpenAI Evals. We systematically evaluate 10+\nleading LLMs as well as OpenAI's legacy models on 20+ curated benchmarks across\n7 capability categories, all under aligned settings. Our retrospective study on\nOpenAI's earlier models offers valuable insights into the evolutionary path\nfrom GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3\nprogressively improves to GPT-4, including technical details like whether\nadding code data improves LLM's reasoning capability, which aspects of LLM\ncapability can be improved by SFT and RLHF, how much is the alignment tax, etc.\nOur analysis sheds light on many of these questions, aiming to improve the\ntransparency of advanced LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_C/0/1/0/all/0/1\">Chenguang Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Pengyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARN: A Comprehensive Framework and Benchmark for Analogical Reasoning on Narratives. (arXiv:2310.00996v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.00996","description":"<p>Analogical reasoning is one of the prime abilities of humans and is linked to\ncreativity and scientific discoveries. This ability has been studied\nextensively in natural language processing (NLP) and in cognitive psychology.\nNLP benchmarks often focus on proportional analogies, while the ones in\ncognitive psychology investigate longer pieces of text too. Yet, although\nstudies that focus on analogical reasoning in an involved setting utilize\nnarratives as their evaluation medium, analogical reasoning on narratives has\nnot been studied extensively. We create an extensive evaluation framework for\nanalogical reasoning on narratives that utilizes narrative elements to create\nlower-order and higher-order mappings that subsequently lead to the development\nof the Analogical Reasoning on Narratives (ARN) benchmark that covers four\ncategories of far(cross-domain)/near(within-domain) analogies and far/near\ndisanalogies, allowing us to study analogical reasoning in LLMs in distinct\nscenarios. Our results demonstrate that LLMs struggle to recognize higher-order\nmappings when they are not accompanied by lower-order mappings (far analogies)\nand show better performance when all mappings are formed simultaneously (near\nanalogies). We observe that in all the scenarios, the analogical reasoning\nabilities of LLMs can be easily impaired by lower-order mappings in near\ndisanalogies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sourati_Z/0/1/0/all/0/1\">Zhivar Sourati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sommerauer_P/0/1/0/all/0/1\">Pia Sommerauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifan Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors. (arXiv:2310.02980v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2310.02980","description":"<p>Modeling long-range dependencies across sequences is a longstanding goal in\nmachine learning and has led to architectures, such as state space models, that\ndramatically outperform Transformers on long sequences. However, these\nimpressive empirical gains have been by and large demonstrated on benchmarks\n(e.g. Long Range Arena), where models are randomly initialized and trained to\npredict a target label from an input sequence. In this work, we show that\nrandom initialization leads to gross overestimation of the differences between\narchitectures and that pretraining with standard denoising objectives, using\n$\\textit{only the downstream task data}$, leads to dramatic gains across\nmultiple architectures and to very small gaps between Transformers and state\nspace models (SSMs). In stark contrast to prior works, we find vanilla\nTransformers to match the performance of S4 on Long Range Arena when properly\npretrained, and we improve the best reported results of SSMs on the PathX-256\ntask by 20 absolute points. Subsequently, we analyze the utility of\npreviously-proposed structured parameterizations for SSMs and show they become\nmostly redundant in the presence of data-driven initialization obtained through\npretraining. Our work shows that, when evaluating different architectures on\nsupervised tasks, incorporation of data-driven priors via pretraining is\nessential for reliable performance estimation, and can be done efficiently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amos_I/0/1/0/all/0/1\">Ido Amos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ankit Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Kelly is a Warm Person, Joseph is a Role Model\": Gender Biases in LLM-Generated Reference Letters. (arXiv:2310.09219v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.09219","description":"<p>Large Language Models (LLMs) have recently emerged as an effective tool to\nassist individuals in writing various types of content, including professional\ndocuments such as recommendation letters. Though bringing convenience, this\napplication also introduces unprecedented fairness concerns. Model-generated\nreference letters might be directly used by users in professional scenarios. If\nunderlying biases exist in these model-constructed letters, using them without\nscrutinization could lead to direct societal harms, such as sabotaging\napplication success rates for female applicants. In light of this pressing\nissue, it is imminent and necessary to comprehensively study fairness issues\nand associated harms in this real-world use case. In this paper, we critically\nexamine gender biases in LLM-generated reference letters. Drawing inspiration\nfrom social science findings, we design evaluation methods to manifest biases\nthrough 2 dimensions: (1) biases in language style and (2) biases in lexical\ncontent. We further investigate the extent of bias propagation by analyzing the\nhallucination bias of models, a term that we define to be bias exacerbation in\nmodel-hallucinated contents. Through benchmarking evaluation on 2 popular LLMs-\nChatGPT and Alpaca, we reveal significant gender biases in LLM-generated\nrecommendation letters. Our findings not only warn against using LLMs for this\napplication without scrutinization, but also illuminate the importance of\nthoroughly studying hidden biases and harms in LLM-generated professional\ndocuments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yixin Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1\">George Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garimella_A/0/1/0/all/0/1\">Aparna Garimella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis. (arXiv:2310.09909v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2310.09909","description":"<p>Driven by the large foundation models, the development of artificial\nintelligence has witnessed tremendous progress lately, leading to a surge of\ngeneral interest from the public. In this study, we aim to assess the\nperformance of OpenAI's newest model, GPT-4V(ision), specifically in the realm\nof multimodal medical diagnosis. Our evaluation encompasses 17 human body\nsystems, including Central Nervous System, Head and Neck, Cardiac, Chest,\nHematology, Hepatobiliary, Gastrointestinal, Urogenital, Gynecology,\nObstetrics, Breast, Musculoskeletal, Spine, Vascular, Oncology, Trauma,\nPediatrics, with images taken from 8 modalities used in daily clinic routine,\ne.g., X-ray, Computed Tomography (CT), Magnetic Resonance Imaging (MRI),\nPositron Emission Tomography (PET), Digital Subtraction Angiography (DSA),\nMammography, Ultrasound, and Pathology. We probe the GPT-4V's ability on\nmultiple clinical tasks with or without patent history provided, including\nimaging modality and anatomy recognition, disease diagnosis, report generation,\ndisease localisation.\n</p>\n<p>Our observation shows that, while GPT-4V demonstrates proficiency in\ndistinguishing between medical image modalities and anatomy, it faces\nsignificant challenges in disease diagnosis and generating comprehensive\nreports. These findings underscore that while large multimodal models have made\nsignificant advancements in computer vision and natural language processing, it\nremains far from being used to effectively support real-world medical\napplications and clinical decision-making.\n</p>\n<p>All images used in this report can be found in\nhttps://github.com/chaoyi-wu/GPT-4V_Medical_Evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chaoyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jiayu Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qiaoyu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weike Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weixiong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoman Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bias in Emotion Recognition with ChatGPT. (arXiv:2310.11753v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2310.11753","description":"<p>This technical report explores the ability of ChatGPT in recognizing emotions\nfrom text, which can be the basis of various applications like interactive\nchatbots, data annotation, and mental health analysis. While prior research has\nshown ChatGPT's basic ability in sentiment analysis, its performance in more\nnuanced emotion recognition is not yet explored. Here, we conducted experiments\nto evaluate its performance of emotion recognition across different datasets\nand emotion labels. Our findings indicate a reasonable level of reproducibility\nin its performance, with noticeable improvement through fine-tuning. However,\nthe performance varies with different emotion labels and datasets, highlighting\nan inherent instability and possible bias. The choice of dataset and emotion\nlabels significantly impacts ChatGPT's emotion recognition performance. This\npaper sheds light on the importance of dataset and label selection, and the\npotential of fine-tuning in enhancing ChatGPT's emotion recognition\ncapabilities, providing a groundwork for better integration of emotion analysis\nin applications using ChatGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wake_N/0/1/0/all/0/1\">Naoki Wake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanehira_A/0/1/0/all/0/1\">Atsushi Kanehira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasabuchi_K/0/1/0/all/0/1\">Kazuhiro Sasabuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takamatsu_J/0/1/0/all/0/1\">Jun Takamatsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikeuchi_K/0/1/0/all/0/1\">Katsushi Ikeuchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Abstractiveness of Summarization Models through Calibrated Distillation. (arXiv:2310.13760v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.13760","description":"<p>Sequence-level knowledge distillation reduces the size of Seq2Seq models for\nmore efficient abstractive summarization. However, it often leads to a loss of\nabstractiveness in summarization. In this paper, we propose a novel approach\nnamed DisCal to enhance the level of abstractiveness (measured by n-gram\noverlap) without sacrificing the informativeness (measured by ROUGE) of\ngenerated summaries. DisCal exposes diverse pseudo summaries with two\nsupervision to the student model. Firstly, the best pseudo summary is\nidentified in terms of abstractiveness and informativeness and used for\nsequence-level distillation. Secondly, their ranks are used to ensure the\nstudent model to assign higher prediction scores to summaries with higher\nranks. Our experiments show that DisCal outperforms prior methods in\nabstractive summarization distillation, producing highly abstractive and\ninformative summaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Hwanjun Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalyminov_I/0/1/0/all/0/1\">Igor Shalyminov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Siffi Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1\">Kaisheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_S/0/1/0/all/0/1\">Saab Mansour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT. (arXiv:2310.15896v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.15896","description":"<p>Large language models (LLMs) have performed well in providing general and\nextensive health suggestions in single-turn conversations, exemplified by\nsystems such as ChatGPT, ChatGLM, ChatDoctor, DoctorGLM, and etc. However, the\nlimited information provided by users during single turn results in inadequate\npersonalization and targeting of the generated suggestions, which requires\nusers to independently select the useful part. It is mainly caused by the\nmissing ability to engage in multi-turn questioning. In real-world medical\nconsultations, doctors usually employ a series of iterative inquiries to\ncomprehend the patient's condition thoroughly, enabling them to provide\neffective and personalized suggestions subsequently, which can be defined as\nchain of questioning (CoQ) for LLMs. To improve the CoQ of LLMs, we propose\nBianQue, a ChatGLM-based LLM finetuned with the self-constructed health\nconversation dataset BianQueCorpus that is consist of multiple turns of\nquestioning and health suggestions polished by ChatGPT. Experimental results\ndemonstrate that the proposed BianQue can simultaneously balance the\ncapabilities of both questioning and health suggestions, which will help\npromote the research and application of LLMs in the field of proactive health.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yirong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1\">Xiaofen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+zheng_h/0/1/0/all/0/1\">huimin zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhipei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1\">Kai Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sihang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jieling Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiangmin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Social Contract AI: Aligning AI Assistants with Implicit Group Norms. (arXiv:2310.17769v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2310.17769","description":"<p>We explore the idea of aligning an AI assistant by inverting a model of\nusers' (unknown) preferences from observed interactions. To validate our\nproposal, we run proof-of-concept simulations in the economic ultimatum game,\nformalizing user preferences as policies that guide the actions of simulated\nplayers. We find that the AI assistant accurately aligns its behavior to match\nstandard policies from the economic literature (e.g., selfish, altruistic).\nHowever, the assistant's learned policies lack robustness and exhibit limited\ngeneralization in an out-of-distribution setting when confronted with a\ncurrency (e.g., grams of medicine) that was not included in the assistant's\ntraining distribution. Additionally, we find that when there is inconsistency\nin the relationship between language use and an unknown policy (e.g., an\naltruistic policy combined with rude language), the assistant's learning of the\npolicy is slowed. Overall, our preliminary results suggest that developing\nsimulation frameworks in which AI assistants need to infer preferences from\ndiverse users can provide a valuable approach for studying practical alignment\nquestions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Franken_J/0/1/0/all/0/1\">Jan-Philipp Fr&#xe4;nken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwok_S/0/1/0/all/0/1\">Sam Kwok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1\">Peixuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_K/0/1/0/all/0/1\">Kanishk Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arumugam_D/0/1/0/all/0/1\">Dilip Arumugam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1\">Jared Moore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamkin_A/0/1/0/all/0/1\">Alex Tamkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerstenberg_T/0/1/0/all/0/1\">Tobias Gerstenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah D. Goodman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChipNeMo: Domain-Adapted LLMs for Chip Design. (arXiv:2311.00176v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.00176","description":"<p>ChipNeMo aims to explore the applications of large language models (LLMs) for\nindustrial chip design. Instead of directly deploying off-the-shelf commercial\nor open-source LLMs, we instead adopt the following domain adaptation\ntechniques: custom tokenizers, domain-adaptive continued pretraining,\nsupervised fine-tuning (SFT) with domain-specific instructions, and\ndomain-adapted retrieval models. We evaluate these methods on three selected\nLLM applications for chip design: an engineering assistant chatbot, EDA script\ngeneration, and bug summarization and analysis. Our results show that these\ndomain adaptation techniques enable significant LLM performance improvements\nover general-purpose base models across the three evaluated applications,\nenabling up to 5x model size reduction with similar or better performance on a\nrange of design tasks. Our findings also indicate that there's still room for\nimprovement between our current results and ideal outcomes. We believe that\nfurther investigation of domain-adapted LLM approaches will help close this gap\nin the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ene_T/0/1/0/all/0/1\">Teodor-Dumitru Ene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirby_R/0/1/0/all/0/1\">Robert Kirby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Chris Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinckney_N/0/1/0/all/0/1\">Nathaniel Pinckney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1\">Rongjian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alben_J/0/1/0/all/0/1\">Jonah Alben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_H/0/1/0/all/0/1\">Himyanshu Anand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1\">Sanmitra Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayraktaroglu_I/0/1/0/all/0/1\">Ismet Bayraktaroglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhaskaran_B/0/1/0/all/0/1\">Bonita Bhaskaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_A/0/1/0/all/0/1\">Arjun Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clay_S/0/1/0/all/0/1\">Sharon Clay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dally_B/0/1/0/all/0/1\">Bill Dally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_L/0/1/0/all/0/1\">Laura Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_P/0/1/0/all/0/1\">Parikshit Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhodhi_S/0/1/0/all/0/1\">Siddhanth Dhodhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halepete_S/0/1/0/all/0/1\">Sameer Halepete</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_E/0/1/0/all/0/1\">Eric Hill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jiashang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sumit Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khailany_B/0/1/0/all/0/1\">Brucek Khailany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kokai_G/0/1/0/all/0/1\">George Kokai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunal_K/0/1/0/all/0/1\">Kishor Kunal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaowei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lind_C/0/1/0/all/0/1\">Charley Lind</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberman_S/0/1/0/all/0/1\">Stuart Oberman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omar_S/0/1/0/all/0/1\">Sujeet Omar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratty_S/0/1/0/all/0/1\">Sreedhar Pratty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raiman_J/0/1/0/all/0/1\">Jonathan Raiman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Ambar Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhengjiang Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hanfei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suthar_P/0/1/0/all/0/1\">Pratik P Suthar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tej_V/0/1/0/all/0/1\">Varun Tej</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_W/0/1/0/all/0/1\">Walker Turner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaizhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Haoxing Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crosslingual Retrieval Augmented In-context Learning for Bangla. (arXiv:2311.00587v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.00587","description":"<p>The promise of Large Language Models (LLMs) in Natural Language Processing\nhas often been overshadowed by their limited performance in low-resource\nlanguages such as Bangla. To address this, our paper presents a pioneering\napproach that utilizes cross-lingual retrieval augmented in-context learning.\nBy strategically sourcing semantically similar prompts from high-resource\nlanguage, we enable multilingual pretrained language models (MPLMs), especially\nthe generative model BLOOMZ, to successfully boost performance on Bangla tasks.\nOur extensive evaluation highlights that the cross-lingual retrieval augmented\nprompts bring steady improvements to MPLMs over the zero-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_E/0/1/0/all/0/1\">Ercong Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Sheng Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Classification to Generation: Insights into Crosslingual Retrieval Augmented ICL. (arXiv:2311.06595v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.06595","description":"<p>The remarkable ability of Large Language Models (LLMs) to understand and\nfollow instructions has sometimes been limited by their in-context learning\n(ICL) performance in low-resource languages. To address this, we introduce a\nnovel approach that leverages cross-lingual retrieval-augmented in-context\nlearning (CREA-ICL). By extracting semantically similar prompts from\nhigh-resource languages, we aim to improve the zero-shot performance of\nmultilingual pre-trained language models (MPLMs) across diverse tasks. Though\nour approach yields steady improvements in classification tasks, it faces\nchallenges in generation tasks. Our evaluation offers insights into the\nperformance dynamics of retrieval-augmented in-context learning across both\nclassification and generation domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_E/0/1/0/all/0/1\">Ercong Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Sheng Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Textual Normalization for Hate Speech Detection. (arXiv:2311.06851v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.06851","description":"<p>Social media data is a valuable resource for research, yet it contains a wide\nrange of non-standard words (NSW). These irregularities hinder the effective\noperation of NLP tools. Current state-of-the-art methods for the Vietnamese\nlanguage address this issue as a problem of lexical normalization, involving\nthe creation of manual rules or the implementation of multi-staged deep\nlearning frameworks, which necessitate extensive efforts to craft intricate\nrules. In contrast, our approach is straightforward, employing solely a\nsequence-to-sequence (Seq2Seq) model. In this research, we provide a dataset\nfor textual normalization, comprising 2,181 human-annotated comments with an\ninter-annotator agreement of 0.9014. By leveraging the Seq2Seq model for\ntextual normalization, our results reveal that the accuracy achieved falls\nslightly short of 70%. Nevertheless, textual normalization enhances the\naccuracy of the Hate Speech Detection (HSD) task by approximately 2%,\ndemonstrating its potential to improve the performance of complex NLP tasks.\nOur dataset is accessible for research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Thi-Hoang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dung Ha Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Nguyet Thi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_K/0/1/0/all/0/1\">Khanh Thanh-Duy Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiLoCo: Distributed Low-Communication Training of Language Models. (arXiv:2311.08105v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2311.08105","description":"<p>Large language models (LLM) have become a critical component in many\napplications of machine learning. However, standard approaches to training LLM\nrequire a large number of tightly interconnected accelerators, with devices\nexchanging gradients and other intermediate states at each optimization step.\nWhile it is difficult to build and maintain a single computing cluster hosting\nmany accelerators, it might be easier to find several computing clusters each\nhosting a smaller number of devices. In this work, we propose a distributed\noptimization algorithm, Distributed Low-Communication (DiLoCo), that enables\ntraining of language models on islands of devices that are poorly connected.\nThe approach is a variant of federated averaging, where the number of inner\nsteps is large, the inner optimizer is AdamW, and the outer optimizer is\nNesterov momentum. On the widely used C4 dataset, we show that DiLoCo on 8\nworkers performs as well as fully synchronous optimization while communicating\n500 times less. DiLoCo exhibits great robustness to the data distribution of\neach worker. It is also robust to resources becoming unavailable over time, and\nvice versa, it can seamlessly leverage resources that become available during\ntraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Douillard_A/0/1/0/all/0/1\">Arthur Douillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Qixuan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusu_A/0/1/0/all/0/1\">Andrei A. Rusu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhaparia_R/0/1/0/all/0/1\">Rachita Chhaparia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donchev_Y/0/1/0/all/0/1\">Yani Donchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuncoro_A/0/1/0/all/0/1\">Adhiguna Kuncoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranzato_M/0/1/0/all/0/1\">Marc&#x27;Aurelio Ranzato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiajun Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Systematic Review of Aspect-based Sentiment Analysis (ABSA): Domains, Methods, and Trends. (arXiv:2311.10777v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.10777","description":"<p>Aspect-based Sentiment Analysis (ABSA) is a type of fine-grained sentiment\nanalysis (SA) that identifies aspects and the associated opinions from a given\ntext. In the digital era, ABSA gained increasing popularity and applications in\nmining opinionated text data to obtain insights and support decisions. ABSA\nresearch employs linguistic, statistical, and machine-learning approaches and\nutilises resources such as labelled datasets, aspect and sentiment lexicons and\nontology. By its nature, ABSA is domain-dependent and can be sensitive to the\nimpact of misalignment between the resource and application domains. However,\nto our knowledge, this topic has not been explored by the existing ABSA\nliterature reviews. In this paper, we present a Systematic Literature Review\n(SLR) of ABSA studies with a focus on the research application domain, dataset\ndomain, and the research methods to examine their relationships and identify\ntrends over time. Our results suggest a number of potential systemic issues in\nthe ABSA research literature, including the predominance of the\n``product/service review'' dataset domain among the majority of studies that\ndid not have a specific research application domain, coupled with the\nprevalence of dataset-reliant methods such as supervised machine learning. This\nreview makes a number of unique contributions to the ABSA research field: 1) To\nour knowledge, it is the first SLR that links the research domain, dataset\ndomain, and research method through a systematic perspective; 2) it is one of\nthe largest scoped SLR on ABSA, with 519 eligible studies filtered from 4191\nsearch results without time constraint; and 3) our review methodology adopted\nan innovative automatic filtering process based on PDF-mining, which enhanced\nscreening quality and reliability. Suggestions and our review limitations are\nalso discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yan Cathy Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denny_P/0/1/0/all/0/1\">Paul Denny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taskova_K/0/1/0/all/0/1\">Katerina Taskova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wicker_J/0/1/0/all/0/1\">J&#xf6;rg Wicker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enabling On-Device Large Language Model Personalization with Self-Supervised Data Selection and Synthesis. (arXiv:2311.12275v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.12275","description":"<p>After a large language model (LLM) is deployed on edge devices, it is\ndesirable for these devices to learn from user-generated conversation data to\ngenerate user-specific and personalized responses in real-time. However,\nuser-generated data usually contains sensitive and private information, and\nuploading such data to the cloud for annotation is not preferred if not\nprohibited. While it is possible to obtain annotation locally by directly\nasking users to provide preferred responses, such annotations have to be sparse\nto not affect user experience. In addition, the storage of edge devices is\nusually too limited to enable large-scale fine-tuning with full user-generated\ndata. It remains an open question how to enable on-device LLM personalization,\nconsidering sparse annotation and limited on-device storage. In this paper, we\npropose a novel framework to select and store the most representative data\nonline in a self-supervised way. Such data has a small memory footprint and\nallows infrequent requests of user annotations for further fine-tuning. To\nenhance fine-tuning quality, multiple semantically similar pairs of question\ntexts and expected responses are generated using the LLM. Our experiments show\nthat the proposed framework achieves the best user-specific content-generating\ncapability (accuracy) and fine-tuning speed (performance) compared with vanilla\nbaselines. To the best of our knowledge, this is the very first on-device LLM\npersonalization framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Ruiyang Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Jun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhenge Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbasi_A/0/1/0/all/0/1\">Ahmed Abbasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Peipei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jingtong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yiyu Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibrated Language Models Must Hallucinate. (arXiv:2311.14648v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.14648","description":"<p>Recent language models generate false but plausible-sounding text with\nsurprising frequency. Such \"hallucinations\" are an obstacle to the usability of\nlanguage-based AI systems and can harm people who rely upon their outputs. This\nwork shows shows that there is an inherent statistical lower-bound on the rate\nthat pretrained language models hallucinate certain types of facts, having\nnothing to do with the transformer LM architecture or data quality. For\n\"arbitrary\" facts whose veracity cannot be determined from the training data,\nwe show that hallucinations must occur at a certain rate for language models\nthat satisfy a statistical calibration condition appropriate for generative\nlanguage models. Specifically, if the maximum probability of any fact is\nbounded, we show that the probability of generating a hallucination is close to\nthe fraction of facts that occur exactly once in the training data (a\n\"Good-Turing\" estimate), even assuming ideal training data without errors.\n</p>\n<p>One conclusion is that models pretrained to be sufficiently good predictors\n(i.e., calibrated) may require post-training to mitigate hallucinations on the\ntype of arbitrary facts that tend to appear once in the training set. However,\nour analysis also suggests that there is no statistical reason that pretraining\nwill lead to hallucination on facts that tend to appear more than once in the\ntraining data (like references to publications such as articles and books,\nwhose hallucinations have been particularly notable and problematic) or on\nsystematic facts (like arithmetic calculations). Therefore, different\narchitectures and learning algorithms may mitigate these latter types of\nhallucinations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalai_A/0/1/0/all/0/1\">Adam Tauman Kalai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vempala_S/0/1/0/all/0/1\">Santosh S. Vempala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Baseline Analysis of Reward Models' Ability To Accurately Analyze Foundation Models Under Distribution Shift. (arXiv:2311.14743v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.14743","description":"<p>Foundation models, specifically Large Language Models (LLM's), have lately\ngained wide-spread attention and adoption. Reinforcement Learning with Human\nFeedback (RLHF) involves training a reward model to capture desired behaviors,\nwhich is then used to align LLM's. These reward models are additionally used at\ninference-time to estimate LLM responses' adherence to those desired behaviors.\nHowever, there is little work measuring how robust these reward models are to\ndistribution shifts. In this work, we evaluate how reward model performance -\nmeasured via accuracy and calibration (i.e. alignment between accuracy and\nconfidence) - is affected by distribution shift. We show novel calibration\npatterns and accuracy drops due to OOD prompts and responses, and that the\nreward model is more sensitive to shifts in responses than prompts.\nAdditionally, we adapt an OOD detection technique commonly used in\nclassification to the reward model setting to detect these distribution shifts\nin prompts and responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pikus_B/0/1/0/all/0/1\">Ben Pikus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeVine_W/0/1/0/all/0/1\">Will LeVine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tony Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendryx_S/0/1/0/all/0/1\">Sean Hendryx</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Custom Data Augmentation for low resource ASR using Bark and Retrieval-Based Voice Conversion. (arXiv:2311.14836v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2311.14836","description":"<p>This paper proposes two innovative methodologies to construct customized\nCommon Voice datasets for low-resource languages like Hindi. The first\nmethodology leverages Bark, a transformer-based text-to-audio model developed\nby Suno, and incorporates Meta's enCodec and a pre-trained HuBert model to\nenhance Bark's performance. The second methodology employs Retrieval-Based\nVoice Conversion (RVC) and uses the Ozen toolkit for data preparation. Both\nmethodologies contribute to the advancement of ASR technology and offer\nvaluable insights into addressing the challenges of constructing customized\nCommon Voice datasets for under-resourced languages. Furthermore, they provide\na pathway to achieving high-quality, personalized voice generation for a range\nof applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamble_A/0/1/0/all/0/1\">Anand Kamble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tathe_A/0/1/0/all/0/1\">Aniket Tathe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumbharkar_S/0/1/0/all/0/1\">Suyash Kumbharkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandare_A/0/1/0/all/0/1\">Atharva Bhandare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1\">Anirban C. Mitra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Function-constrained Program Synthesis. (arXiv:2311.15500v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2311.15500","description":"<p>This work introduces (1) a technique that allows large language models (LLMs)\nto leverage user-provided code when solving programming tasks and (2) a method\nto iteratively generate modular sub-functions that can aid future code\ngeneration attempts when the initial code generated by the LLM is inadequate.\nGenerating computer programs in general-purpose programming languages like\nPython poses a challenge for LLMs when instructed to use code provided in the\nprompt. Code-specific LLMs (e.g., GitHub Copilot, CodeLlama2) can generate code\ncompletions in real-time by drawing on all code available in a development\nenvironment. However, restricting code-specific LLMs to use only in-context\ncode is not straightforward, as the model is not explicitly instructed to use\nthe user-provided code and users cannot highlight precisely which snippets of\ncode the model should incorporate into its context. Moreover, current systems\nlack effective recovery methods, forcing users to iteratively re-prompt the\nmodel with modified prompts until a sufficient solution is reached. Our method\ndiffers from traditional LLM-powered code-generation by constraining\ncode-generation to an explicit function set and enabling recovery from failed\nattempts through automatically generated sub-functions. When the LLM cannot\nproduce working code, we generate modular sub-functions to aid subsequent\nattempts at generating functional code. A by-product of our method is a library\nof reusable sub-functions that can solve related tasks, imitating a software\nteam where efficiency scales with experience. We also introduce a new\n\"half-shot\" evaluation paradigm that provides tighter estimates of LLMs' coding\nabilities compared to traditional zero-shot evaluation. Our proposed evaluation\nmethod encourages models to output solutions in a structured format, decreasing\nsyntax errors that can be mistaken for poor coding ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hajali_P/0/1/0/all/0/1\">Patrick Hajali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budvytis_I/0/1/0/all/0/1\">Ignas Budvytis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Efficacy of Hybrid Deep Learning Models in Distinguishing AI-Generated Text. (arXiv:2311.15565v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.15565","description":"<p>My research investigates the use of cutting-edge hybrid deep learning models\nto accurately differentiate between AI-generated text and human writing. I\napplied a robust methodology, utilising a carefully selected dataset comprising\nAI and human texts from various sources, each tagged with instructions.\nAdvanced natural language processing techniques facilitated the analysis of\ntextual features. Combining sophisticated neural networks, the custom model\nenabled it to detect nuanced differences between AI and human content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oketunji_F/0/1/0/all/0/1\">Finbarrs Oketunji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YUAN 2.0: A Large Language Model with Localized Filtering-based Attention. (arXiv:2311.15786v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.15786","description":"<p>In this work, we develop and release Yuan 2.0, a series of large language\nmodels with parameters ranging from 2.1 billion to 102.6 billion. The Localized\nFiltering-based Attention (LFA) is introduced to incorporate prior knowledge of\nlocal dependencies of natural language into Attention. A data filtering and\ngenerating system is presented to build pre-training and fine-tuning dataset in\nhigh quality. A distributed training method with non-uniform pipeline parallel,\ndata parallel, and optimizer parallel is proposed, which greatly reduces the\nbandwidth requirements of intra-node communication, and achieves good\nperformance in large-scale distributed training. Yuan 2.0 models display\nimpressive ability in code generation, math problem-solving, and chatting\ncompared with existing models. The latest version of YUAN 2.0, including model\nweights and source code, is accessible at Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shaohua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xudong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shenling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiangang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lingjun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongguo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Deep Speech Understanding at the Edge. (arXiv:2311.17065v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2311.17065","description":"<p>In contemporary speech understanding (SU), a sophisticated pipeline is\nemployed, encompassing the ingestion of streaming voice input. The pipeline\nexecutes beam search iteratively, invoking a deep neural network to generate\ntentative outputs (referred to as hypotheses) in an autoregressive manner.\nPeriodically, the pipeline assesses attention and Connectionist Temporal\nClassification (CTC) scores.\n</p>\n<p>This paper aims to enhance SU performance on edge devices with limited\nresources. Adopting a hybrid strategy, our approach focuses on accelerating\non-device execution and offloading inputs surpassing the device's capacity.\nWhile this approach is established, we tackle SU's distinctive challenges\nthrough innovative techniques: (1) Late Contextualization: This involves the\nparallel execution of a model's attentive encoder during input ingestion. (2)\nPilot Inference: Addressing temporal load imbalances in the SU pipeline, this\ntechnique aims to mitigate them effectively. (3) Autoregression Offramps:\nDecisions regarding offloading are made solely based on hypotheses, presenting\na novel approach.\n</p>\n<p>These techniques are designed to seamlessly integrate with existing speech\nmodels, pipelines, and frameworks, offering flexibility for independent or\ncombined application. Collectively, they form a hybrid solution for edge SU.\nOur prototype, named XYZ, has undergone testing on Arm platforms featuring 6 to\n8 cores, demonstrating state-of-the-art accuracy. Notably, it achieves a 2x\nreduction in end-to-end latency and a corresponding 2x decrease in offloading\nrequirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1\">Rongxiang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_F/0/1/0/all/0/1\">Felix Xiaozhu Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"General-Purpose vs. Domain-Adapted Large Language Models for Extraction of Data from Thoracic Radiology Reports. (arXiv:2311.17213v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.17213","description":"<p>Radiologists produce unstructured data that could be valuable for clinical\ncare when consumed by information systems. However, variability in style limits\nusage. Study compares performance of system using domain-adapted language model\n(RadLing) and general-purpose large language model (GPT-4) in extracting common\ndata elements (CDE) from thoracic radiology reports. Three radiologists\nannotated a retrospective dataset of 1300 thoracic reports (900 training, 400\ntest) and mapped to 21 pre-selected relevant CDEs. RadLing was used to generate\nembeddings for sentences and identify CDEs using cosine-similarity, which were\nmapped to values using light-weight mapper. GPT-4 system used OpenAI's\ngeneral-purpose embeddings to identify relevant CDEs and used GPT-4 to map to\nvalues. The output CDE:value pairs were compared to the reference standard; an\nidentical match was considered true positive. Precision (positive predictive\nvalue) was 96% (2700/2824) for RadLing and 99% (2034/2047) for GPT-4. Recall\n(sensitivity) was 94% (2700/2876) for RadLing and 70% (2034/2887) for GPT-4;\nthe difference was statistically significant (P&lt;.001). RadLing's domain-adapted\nembeddings were more sensitive in CDE identification (95% vs 71%) and its\nlight-weight mapper had comparable precision in value assignment (95.4% vs\n95.0%). RadLing system exhibited higher performance than GPT-4 system in\nextracting CDEs from radiology reports. RadLing system's domain-adapted\nembeddings outperform general-purpose embeddings from OpenAI in CDE\nidentification and its light-weight value mapper achieves comparable precision\nto large GPT-4. RadLing system offers operational advantages including local\ndeployment and reduced runtime costs. Domain-adapted RadLing system surpasses\nGPT-4 system in extracting common data elements from radiology reports, while\nproviding benefits of local deployment and lower costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhanaliwala_A/0/1/0/all/0/1\">Ali H. Dhanaliwala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1\">Rikhiya Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karn_S/0/1/0/all/0/1\">Sanjeev Kumar Karn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullaskrishnan_P/0/1/0/all/0/1\">Poikavila Ullaskrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farri_O/0/1/0/all/0/1\">Oladimeji Farri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comaniciu_D/0/1/0/all/0/1\">Dorin Comaniciu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahn_C/0/1/0/all/0/1\">Charles E. Kahn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does VLN Pretraining Work with Nonsensical or Irrelevant Instructions?. (arXiv:2311.17280v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.17280","description":"<p>Data augmentation via back-translation is common when pretraining\nVision-and-Language Navigation (VLN) models, even though the generated\ninstructions are noisy. But: does that noise matter? We find that nonsensical\nor irrelevant language instructions during pretraining can have little effect\non downstream performance for both HAMT and VLN-BERT on R2R, and is still\nbetter than only using clean, human data. To underscore these results, we\nconcoct an efficient augmentation method, Unigram + Object, which generates\nnonsensical instructions that nonetheless improve downstream performance. Our\nfindings suggest that what matters for VLN R2R pretraining is the quantity of\nvisual trajectories, not the quality of instructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1\">Ishika Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AlignBench: Benchmarking Chinese Alignment of Large Language Models. (arXiv:2311.18743v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.18743","description":"<p>Alignment has become a critical step for instruction-tuned Large Language\nModels (LLMs) to become helpful assistants. However, effective evaluation of\nalignment for emerging Chinese LLMs is still significantly lacking, calling for\nreal-scenario grounded, open-ended, challenging and automatic evaluations\ntailored for alignment. To fill in this gap, we introduce AlignBench, a\ncomprehensive multi-dimensional benchmark for evaluating LLMs' alignment in\nChinese. Equipped with a human-in-the-loop data curation pipeline, our\nbenchmark employs a rule-calibrated multi-dimensional LLM-as-Judge with\nChain-of-Thought to generate explanations and final ratings as evaluations,\nensuring high reliability and interpretability. Furthermore, we report\nAlignBench evaluated by CritiqueLLM, a dedicated Chinese evaluator LLM that\nrecovers 95% of GPT-4's evaluation ability. We will provide public APIs for\nevaluating AlignBench with CritiqueLLM to facilitate the evaluation of LLMs'\nChinese alignment. All evaluation codes, data, and LLM generations are\navailable at \\url{https://github.com/THUDM/AlignBench}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1\">Xuanyu Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhuoer Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bosi Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jiale Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1\">Pei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yifan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tam_W/0/1/0/all/0/1\">Weng Lam Tam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BioCLIP: A Vision Foundation Model for the Tree of Life. (arXiv:2311.18803v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2311.18803","description":"<p>Images of the natural world, collected by a variety of cameras, from drones\nto individual phones, are increasingly abundant sources of biological\ninformation. There is an explosion of computational methods and tools,\nparticularly computer vision, for extracting biologically relevant information\nfrom images for science and conservation. Yet most of these are bespoke\napproaches designed for a specific task and are not easily adaptable or\nextendable to new questions, contexts, and datasets. A vision model for general\norganismal biology questions on images is of timely need. To approach this, we\ncurate and release TreeOfLife-10M, the largest and most diverse ML-ready\ndataset of biology images. We then develop BioCLIP, a foundation model for the\ntree of life, leveraging the unique properties of biology captured by\nTreeOfLife-10M, namely the abundance and variety of images of plants, animals,\nand fungi, together with the availability of rich structured biological\nknowledge. We rigorously benchmark our approach on diverse fine-grained biology\nclassification tasks, and find that BioCLIP consistently and substantially\noutperforms existing baselines (by 17% to 20% absolute). Intrinsic evaluation\nreveals that BioCLIP has learned a hierarchical representation conforming to\nthe tree of life, shedding light on its strong generalizability. Our code,\nmodels and data will be made available at\nhttps://github.com/Imageomics/bioclip.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stevens_S/0/1/0/all/0/1\">Samuel Stevens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiaman Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_M/0/1/0/all/0/1\">Matthew J Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campolongo_E/0/1/0/all/0/1\">Elizabeth G Campolongo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chan Hee Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlyn_D/0/1/0/all/0/1\">David Edward Carlyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahdul_W/0/1/0/all/0/1\">Wasila M Dahdul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stewart_C/0/1/0/all/0/1\">Charles Stewart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berger_Wolf_T/0/1/0/all/0/1\">Tanya Berger-Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event-driven Real-time Retrieval in Web Search. (arXiv:2312.00372v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2312.00372","description":"<p>Information retrieval in real-time search presents unique challenges distinct\nfrom those encountered in classical web search. These challenges are\nparticularly pronounced due to the rapid change of user search intent, which is\ninfluenced by the occurrence and evolution of breaking news events, such as\nearthquakes, elections, and wars. Previous dense retrieval methods, which\nprimarily focused on static semantic representation, lack the capacity to\ncapture immediate search intent, leading to inferior performance in retrieving\nthe most recent event-related documents in time-sensitive scenarios. To address\nthis issue, this paper expands the query with event information that represents\nreal-time search intent. The Event information is then integrated with the\nquery through a cross-attention mechanism, resulting in a time-context query\nrepresentation. We further enhance the model's capacity for event\nrepresentation through multi-task training. Since publicly available datasets\nsuch as MS-MARCO do not contain any event information on the query side and\nhave few time-sensitive queries, we design an automatic data collection and\nannotation pipeline to address this issue, which includes ModelZoo-based Coarse\nAnnotation and LLM-driven Fine Annotation processes. In addition, we share the\ntraining tricks such as two-stage training and hard negative sampling. Finally,\nwe conduct a set of offline experiments on a million-scale production dataset\nto evaluate our approach and deploy an A/B testing in a real online system to\nverify the performance. Extensive experimental results demonstrate that our\nproposed approach significantly outperforms existing state-of-the-art baseline\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1\">Nan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shusen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yannan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiaoling Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Hualong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianhua Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jin Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-12-04T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/"}}]}]}