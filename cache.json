{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-06-19T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"MFAS: Emotion Recognition through Multiple Perspectives Fusion Architecture Search Emulating Human Cognition. (arXiv:2306.09361v1 [eess.AS])","link":"http://arxiv.org/abs/2306.09361","description":"<p>Speech emotion recognition aims to identify and analyze emotional states in\ntarget speech similar to humans. Perfect emotion recognition can greatly\nbenefit a wide range of human-machine interaction tasks. Inspired by the human\nprocess of understanding emotions, we demonstrate that compared to quantized\nmodeling, understanding speech content from a continuous perspective, akin to\nhuman-like comprehension, enables the model to capture more comprehensive\nemotional information. Additionally, considering that humans adjust their\nperception of emotional words in textual semantic based on certain cues present\nin speech, we design a novel search space and search for the optimal fusion\nstrategy for the two types of information. Experimental results further\nvalidate the significance of this perception adjustment. Building on these\nobservations, we propose a novel framework called Multiple perspectives Fusion\nArchitecture Search (MFAS). Specifically, we utilize continuous-based knowledge\nto capture speech semantic and quantization-based knowledge to learn textual\nsemantic. Then, we search for the optimal fusion strategy for them.\nExperimental results demonstrate that MFAS surpasses existing models in\ncomprehensively capturing speech emotion information and can automatically\nadjust fusion strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sun_H/0/1/0/all/0/1\">Haiyang Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Fulin Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lian_Z/0/1/0/all/0/1\">Zheng Lian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1\">Yingying Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shilei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations. (arXiv:2306.09390v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09390","description":"<p>This paper presents a novel framework for quantitatively evaluating the\ninteractive ChatGPT model in the context of suicidality assessment from social\nmedia posts, utilizing the University of Maryland Reddit suicidality dataset.\nWe conduct a technical evaluation of ChatGPT's performance on this task using\nZero-Shot and Few-Shot experiments and compare its results with those of two\nfine-tuned transformer-based models. Additionally, we investigate the impact of\ndifferent temperature parameters on ChatGPT's response generation and discuss\nthe optimal temperature based on the inconclusiveness rate of ChatGPT. Our\nresults indicate that while ChatGPT attains considerable accuracy in this task,\ntransformer-based models fine-tuned on human-annotated datasets exhibit\nsuperior performance. Moreover, our analysis sheds light on how adjusting the\nChatGPT's hyperparameters can improve its ability to assist mental health\nprofessionals in this critical task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghanadian_H/0/1/0/all/0/1\">Hamideh Ghanadian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nejadgholi_I/0/1/0/all/0/1\">Isar Nejadgholi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osman_H/0/1/0/all/0/1\">Hussein Al Osman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explore, Establish, Exploit: Red Teaming Language Models from Scratch. (arXiv:2306.09442v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09442","description":"<p>Deploying Large language models (LLMs) can pose hazards from harmful outputs\nsuch as toxic or dishonest speech. Prior work has introduced tools that elicit\nharmful outputs in order to identify and mitigate these risks. While this is a\nvaluable step toward securing language models, these approaches typically rely\non a pre-existing classifier for undesired outputs. This limits their\napplication to situations where the type of harmful behavior is known with\nprecision beforehand. However, this skips a central challenge of red teaming:\ndeveloping a contextual understanding of the behaviors that a model can\nexhibit. Furthermore, when such a classifier already exists, red teaming has\nlimited marginal value because the classifier could simply be used to filter\ntraining data or model outputs. In this work, we consider red teaming under the\nassumption that the adversary is working from a high-level, abstract\nspecification of undesired behavior. The red team is expected to refine/extend\nthis specification and identify methods to elicit this behavior from the model.\nOur red teaming framework consists of three steps: 1) Exploring the model's\nbehavior in the desired context; 2) Establishing a measurement of undesired\nbehavior (e.g., a classifier trained to reflect human evaluations); and 3)\nExploiting the model's flaws using this measure and an established red teaming\nmethodology. We apply this approach to red team GPT-2 and GPT-3 models to\nsystematically discover classes of prompts that elicit toxic and dishonest\nstatements. In doing so, we also construct and release the CommonClaim dataset\nof 20,000 statements that have been labeled by human subjects as\ncommon-knowledge-true, common-knowledge-false, or neither. Code is available at\nhttps://github.com/thestephencasper/explore_establish_exploit_llms. CommonClaim\nis available at https://github.com/thestephencasper/common_claim.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1\">Stephen Casper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jason Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_J/0/1/0/all/0/1\">Joe Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Culp_G/0/1/0/all/0/1\">Gatlen Culp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1\">Dylan Hadfield-Menell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inverse Scaling: When Bigger Isn't Better. (arXiv:2306.09479v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09479","description":"<p>Work on scaling laws has found that large language models (LMs) show\npredictable improvements to overall loss with increased scale (model size,\ntraining data, and compute). Here, we present evidence for the claim that LMs\nmay show inverse scaling, or worse task performance with increased scale, e.g.,\ndue to flaws in the training objective and data. We present empirical evidence\nof inverse scaling on 11 datasets collected by running a public contest, the\nInverse Scaling Prize, with a substantial prize pool. Through analysis of the\ndatasets, along with other examples found in the literature, we identify four\npotential causes of inverse scaling: (i) preference to repeat memorized\nsequences over following in-context instructions, (ii) imitation of undesirable\npatterns in the training data, (iii) tasks containing an easy distractor task\nwhich LMs could focus on, rather than the harder real task, and (iv) correct\nbut misleading few-shot demonstrations of the task. We release the winning\ndatasets at https://inversescaling.com/data to allow for further investigation\nof inverse scaling. Our tasks have helped drive the discovery of U-shaped and\ninverted-U scaling trends, where an initial trend reverses, suggesting that\nscaling trends are less reliable at predicting the behavior of larger-scale\nmodels than previously understood. Overall, our results suggest that there are\ntasks for which increased model scale alone may not lead to progress, and that\nmore careful thought needs to go into the data and objectives for training\nlanguage models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McKenzie_I/0/1/0/all/0/1\">Ian R. McKenzie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyzhov_A/0/1/0/all/0/1\">Alexander Lyzhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pieler_M/0/1/0/all/0/1\">Michael Pieler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parrish_A/0/1/0/all/0/1\">Alicia Parrish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_A/0/1/0/all/0/1\">Aaron Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhu_A/0/1/0/all/0/1\">Ameya Prabhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McLean_E/0/1/0/all/0/1\">Euan McLean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirtland_A/0/1/0/all/0/1\">Aaron Kirtland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1\">Alexis Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alisa Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritsevskiy_A/0/1/0/all/0/1\">Andrew Gritsevskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wurgaft_D/0/1/0/all/0/1\">Daniel Wurgaft</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kauffman_D/0/1/0/all/0/1\">Derik Kauffman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recchia_G/0/1/0/all/0/1\">Gabriel Recchia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiacheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavanagh_J/0/1/0/all/0/1\">Joe Cavanagh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_M/0/1/0/all/0/1\">Max Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sicong Huang</a>, The <a href=\"http://arxiv.org/find/cs/1/au:+Droid_F/0/1/0/all/0/1\">Floating Droid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_T/0/1/0/all/0/1\">Tom Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korbak_T/0/1/0/all/0/1\">Tomasz Korbak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xudong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhengping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Najoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wikibio: a Semantic Resource for the Intersectional Analysis of Biographical Events. (arXiv:2306.09505v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09505","description":"<p>Biographical event detection is a relevant task for the exploration and\ncomparison of the ways in which people's lives are told and represented. In\nthis sense, it may support several applications in digital humanities and in\nworks aimed at exploring bias about minoritized groups. Despite that, there are\nno corpora and models specifically designed for this task. In this paper we\nfill this gap by presenting a new corpus annotated for biographical event\ndetection. The corpus, which includes 20 Wikipedia biographies, was compared\nwith five existing corpora to train a model for the biographical event\ndetection task. The model was able to detect all mentions of the target-entity\nin a biography with an F-score of 0.808 and the entity-related events with an\nF-score of 0.859. Finally, the model was used for performing an analysis of\nbiases about women and non-Western people in Wikipedia biographies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stranisci_M/0/1/0/all/0/1\">Marco Antonio Stranisci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damiano_R/0/1/0/all/0/1\">Rossana Damiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensa_E/0/1/0/all/0/1\">Enrico Mensa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patti_V/0/1/0/all/0/1\">Viviana Patti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radicioni_D/0/1/0/all/0/1\">Daniele Radicioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caselli_T/0/1/0/all/0/1\">Tommaso Caselli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation-Aware Network with Attention-Based Loss for Few-Shot Knowledge Graph Completion. (arXiv:2306.09519v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09519","description":"<p>Few-shot knowledge graph completion (FKGC) task aims to predict unseen facts\nof a relation with few-shot reference entity pairs. Current approaches randomly\nselect one negative sample for each reference entity pair to minimize a\nmargin-based ranking loss, which easily leads to a zero-loss problem if the\nnegative sample is far away from the positive sample and then out of the\nmargin. Moreover, the entity should have a different representation under a\ndifferent context. To tackle these issues, we propose a novel Relation-Aware\nNetwork with Attention-Based Loss (RANA) framework. Specifically, to better\nutilize the plentiful negative samples and alleviate the zero-loss issue, we\nstrategically select relevant negative samples and design an attention-based\nloss function to further differentiate the importance of each negative sample.\nThe intuition is that negative samples more similar to positive samples will\ncontribute more to the model. Further, we design a dynamic relation-aware\nentity encoder for learning a context-dependent entity representation.\nExperiments demonstrate that RANA outperforms the state-of-the-art models on\ntwo benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Q/0/1/0/all/0/1\">Qiao Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuepei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Legal Concepts with Augmented Large Language Models (GPT-4). (arXiv:2306.09525v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09525","description":"<p>Interpreting the meaning of legal open-textured terms is a key task of legal\nprofessionals. An important source for this interpretation is how the term was\napplied in previous court cases. In this paper, we evaluate the performance of\nGPT-4 in generating factually accurate, clear and relevant explanations of\nterms in legislation. We compare the performance of a baseline setup, where\nGPT-4 is directly asked to explain a legal term, to an augmented approach,\nwhere a legal information retrieval module is used to provide relevant context\nto the model, in the form of sentences from case law. We found that the direct\napplication of GPT-4 yields explanations that appear to be of very high quality\non their surface. However, detailed analysis uncovered limitations in terms of\nthe factual accuracy of the explanations. Further, we found that the\naugmentation leads to improved quality, and appears to eliminate the issue of\nhallucination, where models invent incorrect statements. These findings open\nthe door to the building of systems that can autonomously retrieve relevant\nsentences from case law and condense them into a useful explanation for legal\nscholars, educators or practicing lawyers alike.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Savelka_J/0/1/0/all/0/1\">Jaromir Savelka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashley_K/0/1/0/all/0/1\">Kevin D. Ashley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gray_M/0/1/0/all/0/1\">Morgan A. Gray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Westermann_H/0/1/0/all/0/1\">Hannes Westermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huihui Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Block-State Transformer. (arXiv:2306.09539v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09539","description":"<p>State space models (SSMs) have shown impressive results on tasks that require\nmodeling long-range dependencies and efficiently scale to long sequences owing\nto their subquadratic runtime complexity. Originally designed for continuous\nsignals, SSMs have shown superior performance on a plethora of tasks, in vision\nand audio; however, SSMs still lag Transformer performance in Language Modeling\ntasks. In this work, we propose a hybrid layer named Block-State Transformer\n(BST), that internally combines an SSM sublayer for long-range\ncontextualization, and a Block Transformer sublayer for short-term\nrepresentation of sequences. We study three different, and completely\nparallelizable, variants that integrate SSMs and block-wise attention. We show\nthat our model outperforms similar Transformer-based architectures on language\nmodeling perplexity and generalizes to longer sequences. In addition, the\nBlock-State Transformer demonstrates more than tenfold increase in speed at the\nlayer level compared to the Block-Recurrent Transformer when model\nparallelization is employed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fathi_M/0/1/0/all/0/1\">Mahan Fathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilault_J/0/1/0/all/0/1\">Jonathan Pilault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacon_P/0/1/0/all/0/1\">Pierre-Luc Bacon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goroshin_R/0/1/0/all/0/1\">Ross Goroshin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building blocks for complex tasks: Robust generative event extraction for radiology reports under domain shifts. (arXiv:2306.09544v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09544","description":"<p>This paper explores methods for extracting information from radiology reports\nthat generalize across exam modalities to reduce requirements for annotated\ndata. We demonstrate that multi-pass T5-based text-to-text generative models\nexhibit better generalization across exam modalities compared to approaches\nthat employ BERT-based task-specific classification layers. We then develop\nmethods that reduce the inference cost of the model, making large-scale corpus\nprocessing more feasible for clinical applications. Specifically, we introduce\na generative technique that decomposes complex tasks into smaller subtask\nblocks, which improves a single-pass model when combined with multitask\ntraining. In addition, we leverage target-domain contexts during inference to\nenhance domain adaptation, enabling use of smaller models. Analyses offer\ninsights into the benefits of different cost reduction strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sitong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yetisgen_M/0/1/0/all/0/1\">Meliha Yetisgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1\">Mari Ostendorf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reproducibility in NLP: What Have We Learned from the Checklist?. (arXiv:2306.09562v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09562","description":"<p>Scientific progress in NLP rests on the reproducibility of researchers'\nclaims. The *CL conferences created the NLP Reproducibility Checklist in 2020\nto be completed by authors at submission to remind them of key information to\ninclude. We provide the first analysis of the Checklist by examining 10,405\nanonymous responses to it. First, we find evidence of an increase in reporting\nof information on efficiency, validation performance, summary statistics, and\nhyperparameters after the Checklist's introduction. Further, we show acceptance\nrate grows for submissions with more Yes responses. We find that the 44% of\nsubmissions that gather new data are 5% less likely to be accepted than those\nthat did not; the average reviewer-rated reproducibility of these submissions\nis also 2% lower relative to the rest. We find that only 46% of submissions\nclaim to open-source their code, though submissions that do have 8% higher\nreproducibility score relative to those that do not, the most for any item. We\ndiscuss what can be inferred about the state of reproducibility in NLP, and\nprovide a set of recommendations for future conferences, including: a) allowing\nsubmitting code and appendices one week after the deadline, and b) measuring\ndataset reproducibility by a checklist of data collection practices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Magnusson_I/0/1/0/all/0/1\">Ian Magnusson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodge_J/0/1/0/all/0/1\">Jesse Dodge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How do different tokenizers perform on downstream tasks in scriptio continua languages?: A case study in Japanese. (arXiv:2306.09572v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09572","description":"<p>This paper investigates the effect of tokenizers on the downstream\nperformance of pretrained language models (PLMs) in scriptio continua languages\nwhere no explicit spaces exist between words, using Japanese as a case study.\nThe tokenizer for such languages often consists of a morphological analyzer and\na subword tokenizer, requiring us to conduct a comprehensive study of all\npossible pairs. However, previous studies lack this comprehensiveness. We\ntherefore train extensive sets of tokenizers, build a PLM using each, and\nmeasure the downstream performance on a wide range of tasks. Our results\ndemonstrate that each downstream task has a different optimal morphological\nanalyzer, and that it is better to use Byte-Pair-Encoding or Unigram rather\nthan WordPiece as a subword tokenizer, regardless of the type of task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fujii_T/0/1/0/all/0/1\">Takuro Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shibata_K/0/1/0/all/0/1\">Koki Shibata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamaguchi_A/0/1/0/all/0/1\">Atsuki Yamaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morishita_T/0/1/0/all/0/1\">Terufumi Morishita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogawa_Y/0/1/0/all/0/1\">Yasuhiro Sogawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CMLM-CSE: Based on Conditional MLM Contrastive Learning for Sentence Embeddings. (arXiv:2306.09594v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09594","description":"<p>Traditional comparative learning sentence embedding directly uses the encoder\nto extract sentence features, and then passes in the comparative loss function\nfor learning. However, this method pays too much attention to the sentence body\nand ignores the influence of some words in the sentence on the sentence\nsemantics. To this end, we propose CMLM-CSE, an unsupervised contrastive\nlearning framework based on conditional MLM. On the basis of traditional\ncontrastive learning, an additional auxiliary network is added to integrate\nsentence embedding to perform MLM tasks, forcing sentence embedding to learn\nmore masked word information. Finally, when Bertbase was used as the\npretraining language model, we exceeded SimCSE by 0.55 percentage points on\naverage in textual similarity tasks, and when Robertabase was used as the\npretraining language model, we exceeded SimCSE by 0.3 percentage points on\naverage in textual similarity tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clickbait Detection via Large Language Models. (arXiv:2306.09597v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09597","description":"<p>Clickbait, which aims to induce users with some surprising and even thrilling\nheadlines for increasing click-through rates, permeates almost all online\ncontent publishers, such as news portals and social media. Recently, Large\nLanguage Models (LLMs) have emerged as a powerful instrument and achieved\ntremendous success in a serious of NLP downstream tasks. However, it is not yet\nknown whether LLMs can be served as a high-quality clickbait detection system.\nIn this paper, we analyze the performance of LLMs in the few-shot scenarios on\na number of English and Chinese benchmark datasets. Experimental results show\nthat LLMs cannot achieve the best results compared to the state-of-the-art deep\nand fine-tuning PLMs methods. Different from the human intuition, the\nexperiments demonstrated that LLMs cannot make satisfied clickbait detection\njust by the headlines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Han Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ye Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yunhao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiang_J/0/1/0/all/0/1\">Jipeng Qiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Listener Model for the PhotoBook Referential Game with CLIPScores as Implicit Reference Chain. (arXiv:2306.09607v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09607","description":"<p>PhotoBook is a collaborative dialogue game where two players receive private,\npartially-overlapping sets of images and resolve which images they have in\ncommon. It presents machines with a great challenge to learn how people build\ncommon ground around multimodal context to communicate effectively. Methods\ndeveloped in the literature, however, cannot be deployed to real gameplay since\nthey only tackle some subtasks of the game, and they require additional\nreference chains inputs, whose extraction process is imperfect. Therefore, we\npropose a reference chain-free listener model that directly addresses the\ngame's predictive task, i.e., deciding whether an image is shared with partner.\nOur DeBERTa-based listener model reads the full dialogue, and utilizes\nCLIPScore features to assess utterance-image relevance. We achieve &gt;77%\naccuracy on unseen sets of images/game themes, outperforming baseline by &gt;17\npoints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shih-Lun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1\">Yi-Hui Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangze Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AUGUST: an Automatic Generation Understudy for Synthesizing Conversational Recommendation Datasets. (arXiv:2306.09631v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09631","description":"<p>High-quality data is essential for conversational recommendation systems and\nserves as the cornerstone of the network architecture development and training\nstrategy design. Existing works contribute heavy human efforts to manually\nlabeling or designing and extending recommender dialogue templates. However,\nthey suffer from (i) the limited number of human annotators results in that\ndatasets can hardly capture rich and large-scale cases in the real world, (ii)\nthe limited experience and knowledge of annotators account for the\nuninformative corpus and inappropriate recommendations. In this paper, we\npropose a novel automatic dataset synthesis approach that can generate both\nlarge-scale and high-quality recommendation dialogues through a data2text\ngeneration process, where unstructured recommendation conversations are\ngenerated from structured graphs based on user-item information from the real\nworld. In doing so, we comprehensively exploit: (i) rich personalized user\nprofiles from traditional recommendation datasets, (ii) rich external knowledge\nfrom knowledge graphs, and (iii) the conversation ability contained in\nhuman-to-human conversational recommendation datasets. Extensive experiments\nvalidate the benefit brought by the automatically synthesized data under\nlow-resource scenarios and demonstrate the promising potential to facilitate\nthe development of a more effective conversational recommendation system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zichen Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Domain Toxic Spans Detection. (arXiv:2306.09642v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09642","description":"<p>Given the dynamic nature of toxic language use, automated methods for\ndetecting toxic spans are likely to encounter distributional shift. To explore\nthis phenomenon, we evaluate three approaches for detecting toxic spans under\ncross-domain conditions: lexicon-based, rationale extraction, and fine-tuned\nlanguage models. Our findings indicate that a simple method using off-the-shelf\nlexicons performs best in the cross-domain setup. The cross-domain error\nanalysis suggests that (1) rationale extraction methods are prone to false\nnegatives, while (2) language models, despite performing best for the in-domain\ncase, recall fewer explicitly toxic words than lexicons and are prone to\ncertain types of false positives. Our code is publicly available at:\nhttps://github.com/sfschouten/toxic-cross-domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schouten_S/0/1/0/all/0/1\">Stefan F. Schouten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbarestani_B/0/1/0/all/0/1\">Baran Barbarestani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tufa_W/0/1/0/all/0/1\">Wondimagegnhue Tufa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vossen_P/0/1/0/all/0/1\">Piek Vossen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markov_I/0/1/0/all/0/1\">Ilia Markov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReactGenie: An Object-Oriented State Abstraction for Complex Multimodal Interactions Using Large Language Models. (arXiv:2306.09649v1 [cs.HC])","link":"http://arxiv.org/abs/2306.09649","description":"<p>Multimodal interactions have been shown to be more flexible, efficient, and\nadaptable for diverse users and tasks than traditional graphical interfaces.\nHowever, existing multimodal development frameworks either do not handle the\ncomplexity and compositionality of multimodal commands well or require\ndevelopers to write a substantial amount of code to support these multimodal\ninteractions. In this paper, we present ReactGenie, a programming framework\nthat uses a shared object-oriented state abstraction to support building\ncomplex multimodal mobile applications. Having different modalities share the\nsame state abstraction allows developers using ReactGenie to seamlessly\nintegrate and compose these modalities to deliver multimodal interaction.\n</p>\n<p>ReactGenie is a natural extension to the existing workflow of building a\ngraphical app, like the workflow with React-Redux. Developers only have to add\na few annotations and examples to indicate how natural language is mapped to\nthe user-accessible functions in the program. ReactGenie automatically handles\nthe complex problem of understanding natural language by generating a parser\nthat leverages large language models.\n</p>\n<p>We evaluated the ReactGenie framework by using it to build three demo apps.\nWe evaluated the accuracy of the language parser using elicited commands from\ncrowd workers and evaluated the usability of the generated multimodal app with\n16 participants. Our results show that ReactGenie can be used to build\nversatile multimodal applications with highly accurate language parsers, and\nthe multimodal app can lower users' cognitive load and task completion time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jackie/0/1/0/all/0/1\">Jackie</a> (Junrui) <a href=\"http://arxiv.org/find/cs/1/au:+Yang/0/1/0/all/0/1\">Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Karina Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosli_D/0/1/0/all/0/1\">Daniel Wan Rosli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1\">Monica S. Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landay_J/0/1/0/all/0/1\">James A. Landay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Distillation for Pseudo-Relevance Feedback. (arXiv:2306.09657v1 [cs.IR])","link":"http://arxiv.org/abs/2306.09657","description":"<p>Model distillation has emerged as a prominent technique to improve neural\nsearch models. To date, distillation taken an offline approach, wherein a new\nneural model is trained to predict relevance scores between arbitrary queries\nand documents. In this paper, we explore a departure from this offline\ndistillation strategy by investigating whether a model for a specific query can\nbe effectively distilled from neural re-ranking results (i.e., distilling in an\nonline setting). Indeed, we find that a lexical model distilled online can\nreasonably replicate the re-ranking of a neural model. More importantly, these\nmodels can be used as queries that execute efficiently on indexes. This second\nretrieval stage can enrich the pool of documents for re-ranking by identifying\ndocuments that were missed in the first retrieval stage. Empirically, we show\nthat this approach performs favourably when compared with established pseudo\nrelevance feedback techniques, dense retrieval methods, and sparse-dense\nensemble \"hybrid\" approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+MacAvaney_S/0/1/0/all/0/1\">Sean MacAvaney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class-Adaptive Self-Training for Relation Extraction with Incompletely Annotated Training Data. (arXiv:2306.09697v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09697","description":"<p>Relation extraction (RE) aims to extract relations from sentences and\ndocuments. Existing relation extraction models typically rely on supervised\nmachine learning. However, recent studies showed that many RE datasets are\nincompletely annotated. This is known as the false negative problem in which\nvalid relations are falsely annotated as 'no_relation'. Models trained with\nsuch data inevitably make similar mistakes during the inference stage.\nSelf-training has been proven effective in alleviating the false negative\nproblem. However, traditional self-training is vulnerable to confirmation bias\nand exhibits poor performance in minority classes. To overcome this limitation,\nwe proposed a novel class-adaptive re-sampling self-training framework.\nSpecifically, we re-sampled the pseudo-labels for each class by precision and\nrecall scores. Our re-sampling strategy favored the pseudo-labels of classes\nwith high precision and low recall, which improved the overall recall without\nsignificantly compromising precision. We conducted experiments on\ndocument-level and biomedical relation extraction datasets, and the results\nshowed that our proposed self-training framework consistently outperforms\nexisting competitive methods on the Re-DocRED and ChemDisgene datasets when the\ntraining data are incompletely annotated. Our code is released at\nhttps://github.com/DAMO-NLP-SG/CAST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Q/0/1/0/all/0/1\">Qingyu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_H/0/1/0/all/0/1\">Hwee Tou Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-corpus Readability Compatibility Assessment for English Texts. (arXiv:2306.09704v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09704","description":"<p>Text readability assessment has gained significant attention from researchers\nin various domains. However, the lack of exploration into corpus compatibility\nposes a challenge as different research groups utilize different corpora. In\nthis study, we propose a novel evaluation framework, Cross-corpus text\nReadability Compatibility Assessment (CRCA), to address this issue. The\nframework encompasses three key components: (1) Corpus: CEFR, CLEC, CLOTH, NES,\nOSP, and RACE. Linguistic features, GloVe word vector representations, and\ntheir fusion features were extracted. (2) Classification models: Machine\nlearning methods (XGBoost, SVM) and deep learning methods (BiLSTM,\nAttention-BiLSTM) were employed. (3) Compatibility metrics: RJSD, RRNSS, and\nNDCG metrics. Our findings revealed: (1) Validated corpus compatibility, with\nOSP standing out as significantly different from other datasets. (2) An\nadaptation effect among corpora, feature representations, and classification\nmethods. (3) Consistent outcomes across the three metrics, validating the\nrobustness of the compatibility assessment framework. The outcomes of this\nstudy offer valuable insights into corpus selection, feature representation,\nand classification methods, and it can also serve as a beginning effort for\ncross-corpus transfer learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenzhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Han Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaohong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing Computational Costs in Sentiment Analysis: Tensorized Recurrent Networks vs. Recurrent Networks. (arXiv:2306.09705v1 [cs.LG])","link":"http://arxiv.org/abs/2306.09705","description":"<p>Anticipating audience reaction towards a certain text is integral to several\nfacets of society ranging from politics, research, and commercial industries.\nSentiment analysis (SA) is a useful natural language processing (NLP) technique\nthat utilizes lexical/statistical and deep learning methods to determine\nwhether different-sized texts exhibit positive, negative, or neutral emotions.\nRecurrent networks are widely used in machine-learning communities for problems\nwith sequential data. However, a drawback of models based on Long-Short Term\nMemory networks and Gated Recurrent Units is the significantly high number of\nparameters, and thus, such models are computationally expensive. This drawback\nis even more significant when the available data are limited. Also, such models\nrequire significant over-parameterization and regularization to achieve optimal\nperformance. Tensorized models represent a potential solution. In this paper,\nwe classify the sentiment of some social media posts. We compare traditional\nrecurrent models with their tensorized version, and we show that with the\ntensorized models, we reach comparable performances with respect to the\ntraditional models while using fewer resources for the training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lopez_G/0/1/0/all/0/1\">Gabriel Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anna Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaul_J/0/1/0/all/0/1\">Joe Kaul</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Offline Reinforcement Learning for Optimized Text Generation. (arXiv:2306.09712v1 [cs.LG])","link":"http://arxiv.org/abs/2306.09712","description":"<p>In reinforcement learning (RL), there are two major settings for interacting\nwith the environment: online and offline. Online methods explore the\nenvironment at significant time cost, and offline methods efficiently obtain\nreward signals by sacrificing exploration capability. We propose semi-offline\nRL, a novel paradigm that smoothly transits from offline to online settings,\nbalances exploration capability and training cost, and provides a theoretical\nfoundation for comparing different RL settings. Based on the semi-offline\nformulation, we present the RL setting that is optimal in terms of optimization\ncost, asymptotic error, and overfitting error bound. Extensive experiments show\nthat our semi-offline approach is efficient and yields comparable or often\nbetter performance compared with state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yiqiao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_V/0/1/0/all/0/1\">Victor Ye Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jie Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pushing the Limits of ChatGPT on NLP Tasks. (arXiv:2306.09719v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09719","description":"<p>Despite the success of ChatGPT, its performances on most NLP tasks are still\nwell below the supervised baselines. In this work, we looked into the causes,\nand discovered that its subpar performance was caused by the following factors:\n(1) token limit in the prompt does not allow for the full utilization of the\nsupervised datasets; (2) mismatch between the generation nature of ChatGPT and\nNLP tasks; (3) intrinsic pitfalls of LLMs models, e.g., hallucination, overly\nfocus on certain keywords, etc.\n</p>\n<p>In this work, we propose a collection of general modules to address these\nissues, in an attempt to push the limits of ChatGPT on NLP tasks. Our proposed\nmodules include (1) a one-input-multiple-prompts strategy that employs multiple\nprompts for one input to accommodate more demonstrations; (2) using fine-tuned\nmodels for better demonstration retrieval; (3) transforming tasks to formats\nthat are more tailored to the generation nature; (4) employing reasoning\nstrategies that are tailored to addressing the task-specific complexity; (5)\nthe self-verification strategy to address the hallucination issue of LLMs; (6)\nthe paraphrase strategy to improve the robustness of model predictions.\n</p>\n<p>We conduct experiments on 21 datasets of 10 representative NLP tasks,\nincluding question answering, commonsense reasoning, natural language\ninference, sentiment analysis, named entity recognition, entity-relation\nextraction, event extraction, dependency parsing, semantic role labeling, and\npart-of-speech tagging. Using the proposed assemble of techniques, we are able\nto significantly boost the performance of ChatGPT on the selected NLP tasks,\nachieving performances comparable to or better than supervised baselines, or\neven existing SOTA performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Linfeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zhen Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1\">Fei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lingjuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discourse Representation Structure Parsing for Chinese. (arXiv:2306.09725v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09725","description":"<p>Previous work has predominantly focused on monolingual English semantic\nparsing. We, instead, explore the feasibility of Chinese semantic parsing in\nthe absence of labeled data for Chinese meaning representations. We describe\nthe pipeline of automatically collecting the linearized Chinese meaning\nrepresentation data for sequential-to sequential neural networks. We further\npropose a test suite designed explicitly for Chinese semantic parsing, which\nprovides fine-grained evaluation for parsing performance, where we aim to study\nChinese parsing difficulties. Our experimental results show that the difficulty\nof Chinese semantic parsing is mainly caused by adverbs. Realizing Chinese\nparsing through machine translation and an English parser yields slightly lower\nperformance than training a model directly on Chinese data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunliu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bos_J/0/1/0/all/0/1\">Johan Bos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Natural Language Processing and Networks to Automate Structured Literature Reviews: An Application to Farmers Climate Change Adaptation. (arXiv:2306.09737v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09737","description":"<p>The fast-growing number of research articles makes it problematic for\nscholars to keep track of the new findings related to their areas of expertise.\nFurthermore, linking knowledge across disciplines in rapidly developing fields\nbecomes challenging for complex topics like climate change that demand\ninterdisciplinary solutions. At the same time, the rise of Black Box types of\ntext summarization makes it difficult to understand how text relationships are\nbuilt, let alone relate to existing theories conceptualizing cause-effect\nrelationships and permitting hypothesizing. This work aims to sensibly use\nNatural Language Processing by extracting variables relations and synthesizing\ntheir findings using networks while relating to key concepts dominant in\nrelevant disciplines. As an example, we apply our methodology to the analysis\nof farmers' adaptation to climate change. For this, we perform a Natural\nLanguage Processing analysis of publications returned by Scopus in August 2022.\nResults show that the use of Natural Language Processing together with networks\nin a descriptive manner offers a fast and interpretable way to synthesize\nliterature review findings as long as researchers back up results with theory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gil_Clavel_S/0/1/0/all/0/1\">Sofia Gil-Clavel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filatova_T/0/1/0/all/0/1\">Tatiana Filatova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Politeness Stereotypes and Attack Vectors: Gender Stereotypes in Japanese and Korean Language Models. (arXiv:2306.09752v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09752","description":"<p>In efforts to keep up with the rapid progress and use of large language\nmodels, gender bias research is becoming more prevalent in NLP. Non-English\nbias research, however, is still in its infancy with most work focusing on\nEnglish. In our work, we study how grammatical gender bias relating to\npoliteness levels manifests in Japanese and Korean language models. Linguistic\nstudies in these languages have identified a connection between gender bias and\npoliteness levels, however it is not yet known if language models reproduce\nthese biases. We analyze relative prediction probabilities of the male and\nfemale grammatical genders using templates and find that informal polite speech\nis most indicative of the female grammatical gender, while rude and formal\nspeech is most indicative of the male grammatical gender. Further, we find\npoliteness levels to be an attack vector for allocational gender bias in\ncyberbullying detection models. Cyberbullies can evade detection through simple\ntechniques abusing politeness levels. We introduce an attack dataset to (i)\nidentify representational gender bias across politeness levels, (ii)\ndemonstrate how gender biases can be abused to bypass cyberbullying detection\nmodels and (iii) show that allocational biases can be mitigated via training on\nour proposed dataset. Through our findings we highlight the importance of bias\nresearch moving beyond its current English-centrism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Steinborn_V/0/1/0/all/0/1\">Victor Steinborn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maronikolakis_A/0/1/0/all/0/1\">Antonis Maronikolakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Full Parameter Fine-tuning for Large Language Models with Limited Resources. (arXiv:2306.09782v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09782","description":"<p>Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP) but demand massive GPU resources for training. Lowering the threshold for\nLLMs training would encourage greater participation from researchers,\nbenefiting both academia and society. While existing approaches have focused on\nparameter-efficient fine-tuning, which tunes or adds a small number of\nparameters, few have addressed the challenge of tuning the full parameters of\nLLMs with limited resources. In this work, we propose a new optimizer,\nLOw-Memory Optimization (LOMO), which fuses the gradient computation and the\nparameter update in one step to reduce memory usage. By integrating LOMO with\nexisting memory saving techniques, we reduce memory usage to 10.8% compared to\nthe standard approach (DeepSpeed solution). Consequently, our approach enables\nthe full parameter fine-tuning of a 65B model on a single machine with 8 RTX\n3090, each with 24GB memory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lv_K/0/1/0/all/0/1\">Kai Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tengxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qinghui Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qipeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RED$^{\\rm FM}$: a Filtered and Multilingual Relation Extraction Dataset. (arXiv:2306.09802v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09802","description":"<p>Relation Extraction (RE) is a task that identifies relationships between\nentities in a text, enabling the acquisition of relational facts and bridging\nthe gap between natural language and structured knowledge. However, current RE\nmodels often rely on small datasets with low coverage of relation types,\nparticularly when working with languages other than English. In this paper, we\naddress the above issue and provide two new resources that enable the training\nand evaluation of multilingual RE systems. First, we present SRED$^{\\rm FM}$,\nan automatically annotated dataset covering 18 languages, 400 relation types,\n13 entity types, totaling more than 40 million triplet instances. Second, we\npropose RED$^{\\rm FM}$, a smaller, human-revised dataset for seven languages\nthat allows for the evaluation of multilingual RE systems. To demonstrate the\nutility of these novel datasets, we experiment with the first end-to-end\nmultilingual RE model, mREBEL, that extracts triplets, including entity types,\nin multiple languages. We release our resources and model checkpoints at\nhttps://www.github.com/babelscape/rebel\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cabot_P/0/1/0/all/0/1\">Pere-Llu&#xed;s Huguet Cabot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tedeschi_S/0/1/0/all/0/1\">Simone Tedeschi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1\">Axel-Cyrille Ngonga Ngomo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navigli_R/0/1/0/all/0/1\">Roberto Navigli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Utility of Surprisal from Large Language Models for Speech Synthesis Prosody. (arXiv:2306.09814v1 [eess.AS])","link":"http://arxiv.org/abs/2306.09814","description":"<p>This paper investigates the use of word surprisal, a measure of the\npredictability of a word in a given context, as a feature to aid speech\nsynthesis prosody. We explore how word surprisal extracted from large language\nmodels (LLMs) correlates with word prominence, a signal-based measure of the\nsalience of a word in a given discourse. We also examine how context length and\nLLM size affect the results, and how a speech synthesizer conditioned with\nsurprisal values compares with a baseline system. To evaluate these factors, we\nconducted experiments using a large corpus of English text and LLMs of varying\nsizes. Our results show that word surprisal and word prominence are moderately\ncorrelated, suggesting that they capture related but distinct aspects of\nlanguage use. We find that length of context and size of the LLM impact the\ncorrelations, but not in the direction anticipated, with longer contexts and\nlarger LLMs generally underpredicting prominent words in a nearly linear\nmanner. We demonstrate that, in line with these findings, a speech synthesizer\nconditioned with surprisal values provides a minimal improvement over the\nbaseline with the results suggesting a limited effect of using surprisal values\nfor eliciting appropriate prominence patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kakouros_S/0/1/0/all/0/1\">Sofoklis Kakouros</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Simko_J/0/1/0/all/0/1\">Juraj &#x160;imko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vainio_M/0/1/0/all/0/1\">Martti Vainio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suni_A/0/1/0/all/0/1\">Antti Suni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulator to Enhance Dialogue System. (arXiv:2306.09821v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09821","description":"<p>Dialogue systems and large language models (LLMs) have gained considerable\nattention. However, the direct utilization of LLMs as task-oriented dialogue\n(TOD) models has been found to underperform compared to smaller task-specific\nmodels. Nonetheless, it is crucial to acknowledge the significant potential of\nLLMs and explore improved approaches for leveraging their impressive abilities.\nMotivated by the goal of leveraging LLMs, we propose an alternative approach\ncalled User-Guided Response Optimization (UGRO) to combine it with a smaller\nTOD model. This approach uses LLM as annotation-free user simulator to assess\ndialogue responses, combining them with smaller fine-tuned end-to-end TOD\nmodels. By utilizing the satisfaction feedback generated by LLMs, UGRO further\noptimizes the supervised fine-tuned TOD model. Specifically, the TOD model\ntakes the dialogue history as input and, with the assistance of the user\nsimulator's feedback, generates high-satisfaction responses that meet the\nuser's requirements. Through empirical experiments on two TOD benchmarks, we\nvalidate the effectiveness of our method. The results demonstrate that our\napproach outperforms previous state-of-the-art (SOTA) results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yue Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1\">Anh Tuan Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1\">Aldo Lipani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Process Knowledge-infused Learning for Clinician-friendly Explanations. (arXiv:2306.09824v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09824","description":"<p>Language models have the potential to assess mental health using social media\ndata. By analyzing online posts and conversations, these models can detect\npatterns indicating mental health conditions like depression, anxiety, or\nsuicidal thoughts. They examine keywords, language markers, and sentiment to\ngain insights into an individual's mental well-being. This information is\ncrucial for early detection, intervention, and support, improving mental health\ncare and prevention strategies. However, using language models for mental\nhealth assessments from social media has two limitations: (1) They do not\ncompare posts against clinicians' diagnostic processes, and (2) It's\nchallenging to explain language model outputs using concepts that the clinician\ncan understand, i.e., clinician-friendly explanations. In this study, we\nintroduce Process Knowledge-infused Learning (PK-iL), a new learning paradigm\nthat layers clinical process knowledge structures on language model outputs,\nenabling clinician-friendly explanations of the underlying language model\npredictions. We rigorously test our methods on existing benchmark datasets,\naugmented with such clinical process knowledge, and release a new dataset for\nassessing suicidality. PK-iL performs competitively, achieving a 70% agreement\nwith users, while other XAI methods only achieve 47% agreement (average\ninter-rater agreement of 0.72). Our evaluations demonstrate that PK-iL\neffectively explains model predictions to clinicians.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zi_Y/0/1/0/all/0/1\">Yuxin Zi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_M/0/1/0/all/0/1\">Manas Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malekar_J/0/1/0/all/0/1\">Jinendra Malekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_V/0/1/0/all/0/1\">Vignesh Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sheffield's Submission to the AmericasNLP Shared Task on Machine Translation into Indigenous Languages. (arXiv:2306.09830v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09830","description":"<p>In this paper we describe the University of Sheffield's submission to the\nAmericasNLP 2023 Shared Task on Machine Translation into Indigenous Languages\nwhich comprises the translation from Spanish to eleven indigenous languages.\nOur approach consists of extending, training, and ensembling different\nvariations of NLLB-200. We use data provided by the organizers and data from\nvarious other sources such as constitutions, handbooks, news articles, and\nbacktranslations generated from monolingual data. On the dev set, our best\nsubmission outperforms the baseline by 11% average chrF across all languages,\nwith substantial improvements particularly for Aymara, Guarani and Quechua. On\nthe test set, we achieve the highest average chrF of all the submissions, we\nrank first in four of the eleven languages, and at least one of our submissions\nranks in the top 3 for all languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gow_Smith_E/0/1/0/all/0/1\">Edward Gow-Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_D/0/1/0/all/0/1\">Danae S&#xe1;nchez Villegas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views. (arXiv:2306.09841v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09841","description":"<p>Large Language Models (LLMs) have achieved great success in various natural\nlanguage tasks. It has aroused much interest in evaluating the specific\nreasoning capability of LLMs, such as multilingual reasoning and mathematical\nreasoning. However, as one of the key reasoning perspectives, logical reasoning\ncapability has not yet been thoroughly evaluated. In this work, we aim to\nbridge those gaps and provide comprehensive evaluations. Firstly, to offer\nsystematic evaluations, this paper selects fifteen typical logical reasoning\ndatasets and organizes them into deductive, inductive, abductive and mixed-form\nreasoning settings. Considering the comprehensiveness of evaluations, we\ninclude three representative LLMs (i.e., text-davinci-003, ChatGPT and BARD)\nand evaluate them on all selected datasets under zero-shot, one-shot and\nthree-shot settings. Secondly, different from previous evaluations relying only\non simple metrics (e.g., accuracy), we propose fine-level evaluations from\nobjective and subjective manners, covering both answers and explanations. Also,\nto uncover the logical flaws of LLMs, bad cases will be attributed to five\nerror types from two dimensions. Thirdly, to avoid the influences of knowledge\nbias and purely focus on benchmarking the logical reasoning capability of LLMs,\nwe propose a new dataset with neutral content. It contains 3K samples and\ncovers deductive, inductive and abductive reasoning settings. Based on the\nin-depth evaluations, this paper finally concludes the ability maps of logical\nreasoning capability from six dimensions (i.e., correct, rigorous, self-aware,\nactive, oriented and no hallucination). It reflects the pros and cons of LLMs\nand gives guiding directions for future works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fangzhi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qika Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianzhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models. (arXiv:2306.09869v1 [cs.CV])","link":"http://arxiv.org/abs/2306.09869","description":"<p>Despite the remarkable performance of text-to-image diffusion models in image\ngeneration tasks, recent studies have raised the issue that generated images\nsometimes cannot capture the intended semantic contents of the text prompts,\nwhich phenomenon is often called semantic misalignment. To address this, here\nwe present a novel energy-based model (EBM) framework. Specifically, we first\nformulate EBMs of latent image representations and text embeddings in each\ncross-attention layer of the denoising autoencoder. Then, we obtain the\ngradient of the log posterior of context vectors, which can be updated and\ntransferred to the subsequent cross-attention layer, thereby implicitly\nminimizing a nested hierarchy of energy functions. Our latent EBMs further\nallow zero-shot compositional generation as a linear combination of\ncross-attention outputs from different contexts. Using extensive experiments,\nwe demonstrate that the proposed method is highly effective in handling various\nimage generation tasks, including multi-concept generation, text-guided image\ninpainting, and real and synthetic image editing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1\">Geon Yeong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jeongsol Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Beomsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang Wan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revealing the impact of social circumstances on the selection of cancer therapy through natural language processing of social work notes. (arXiv:2306.09877v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09877","description":"<p>We aimed to investigate the impact of social circumstances on cancer therapy\nselection using natural language processing to derive insights from social\nworker documentation. We developed and employed a Bidirectional Encoder\nRepresentations from Transformers (BERT) based approach, using a hierarchical\nmulti-step BERT model (BERT-MS) to predict the prescription of targeted cancer\ntherapy to patients based solely on documentation by clinical social workers.\nOur corpus included free-text clinical social work notes, combined with\nmedication prescription information, for all patients treated for breast\ncancer. We conducted a feature importance analysis to pinpoint the specific\nsocial circumstances that impact cancer therapy selection. Using only social\nwork notes, we consistently predicted the administration of targeted therapies,\nsuggesting systematic differences in treatment selection exist due to\nnon-clinical factors. The UCSF-BERT model, pretrained on clinical text at UCSF,\noutperformed other publicly available language models with an AUROC of 0.675\nand a Macro F1 score of 0.599. The UCSF BERT-MS model, capable of leveraging\nmultiple pieces of notes, surpassed the UCSF-BERT model in both AUROC and\nMacro-F1. Our feature importance analysis identified several clinically\nintuitive social determinants of health (SDOH) that potentially contribute to\ndisparities in treatment. Our findings indicate that significant disparities\nexist among breast cancer patients receiving different types of therapies based\non social determinants of health. Social work reports play a crucial role in\nunderstanding these disparities in clinical decision-making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shenghuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zack_T/0/1/0/all/0/1\">Travis Zack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_C/0/1/0/all/0/1\">Christopher Y.K. Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butte_A/0/1/0/all/0/1\">Atul J. Butte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sushil_M/0/1/0/all/0/1\">Madhumita Sushil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Demystifying GPT Self-Repair for Code Generation. (arXiv:2306.09896v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09896","description":"<p>Large Language Models (LLMs) have shown remarkable aptitude in code\ngeneration but still struggle on challenging programming tasks. Self-repair --\nin which the model debugs and fixes mistakes in its own code -- has recently\nbecome a popular way to boost performance in these settings. However, only very\nlimited studies on how and when self-repair works effectively exist in the\nliterature, and one might wonder to what extent a model is really capable of\nproviding accurate feedback on why the code is wrong when that code was\ngenerated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's\nability to perform self-repair on APPS, a challenging dataset consisting of\ndiverse coding challenges. To do so, we first establish a new evaluation\nstrategy dubbed pass@t that measures the pass rate of the tasks against the\ntotal number of tokens sampled from the model, enabling a fair comparison to\npurely sampling-based approaches. With this evaluation strategy, we find that\nthe effectiveness of self-repair is only seen in GPT-4. We also observe that\nself-repair is bottlenecked by the feedback stage; using GPT-4 to give feedback\non the programs generated by GPT-3.5 and using expert human programmers to give\nfeedback on the programs generated by GPT-4, we unlock significant performance\ngains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Olausson_T/0/1/0/all/0/1\">Theo X. Olausson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inala_J/0/1/0/all/0/1\">Jeevana Priya Inala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenglong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solar_Lezama_A/0/1/0/all/0/1\">Armando Solar-Lezama</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No Strong Feelings One Way or Another: Re-operationalizing Neutrality in Natural Language Inference. (arXiv:2306.09918v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09918","description":"<p>Natural Language Inference (NLI) has been a cornerstone task in evaluating\nlanguage models' inferential reasoning capabilities. However, the standard\nthree-way classification scheme used in NLI has well-known shortcomings in\nevaluating models' ability to capture the nuances of natural human reasoning.\nIn this paper, we argue that the operationalization of the neutral label in\ncurrent NLI datasets has low validity, is interpreted inconsistently, and that\nat least one important sense of neutrality is often ignored. We uncover the\ndetrimental impact of these shortcomings, which in some cases leads to\nannotation datasets that actually decrease performance on downstream tasks. We\ncompare approaches of handling annotator disagreement and identify flaws in a\nrecent NLI dataset that designs an annotator study based on a problematic\noperationalization. Our findings highlight the need for a more refined\nevaluation framework for NLI, and we hope to spark further discussion and\naction in the NLP community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nighojkar_A/0/1/0/all/0/1\">Animesh Nighojkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laverghetta_A/0/1/0/all/0/1\">Antonio Laverghetta Jr.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Licato_J/0/1/0/all/0/1\">John Licato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Summarize and Answer Questions about a Virtual Robot's Past Actions. (arXiv:2306.09922v1 [cs.RO])","link":"http://arxiv.org/abs/2306.09922","description":"<p>When robots perform long action sequences, users will want to easily and\nreliably find out what they have done. We therefore demonstrate the task of\nlearning to summarize and answer questions about a robot agent's past actions\nusing natural language alone. A single system with a large language model at\nits core is trained to both summarize and answer questions about action\nsequences given ego-centric video frames of a virtual robot and a question\nprompt. To enable training of question answering, we develop a method to\nautomatically generate English-language questions and answers about objects,\nactions, and the temporal order in which actions occurred during episodes of\nrobot action in the virtual environment. Training one model to both summarize\nand answer questions enables zero-shot transfer of representations of objects\nlearned through question answering to improved action summarization. %\ninvolving objects not seen in training to summarize.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DeChant_C/0/1/0/all/0/1\">Chad DeChant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akinola_I/0/1/0/all/0/1\">Iretiayo Akinola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_D/0/1/0/all/0/1\">Daniel Bauer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trained Transformers Learn Linear Models In-Context. (arXiv:2306.09927v1 [stat.ML])","link":"http://arxiv.org/abs/2306.09927","description":"<p>Attention-based neural networks such as transformers have demonstrated a\nremarkable ability to exhibit in-context learning (ICL): Given a short prompt\nsequence of tokens from an unseen task, they can formulate relevant per-token\nand next-token predictions without any parameter updates. By embedding a\nsequence of labeled training data and unlabeled test data as a prompt, this\nallows for transformers to behave like supervised learning algorithms. Indeed,\nrecent work has shown that when training transformer architectures over random\ninstances of linear regression problems, these models' predictions mimic those\nof ordinary least squares.\n</p>\n<p>Towards understanding the mechanisms underlying this phenomenon, we\ninvestigate the dynamics of ICL in transformers with a single linear\nself-attention layer trained by gradient flow on linear regression tasks. We\nshow that despite non-convexity, gradient flow with a suitable random\ninitialization finds a global minimum of the objective function. At this global\nminimum, when given a test prompt of labeled examples from a new prediction\ntask, the transformer achieves prediction error competitive with the best\nlinear predictor over the test prompt distribution. We additionally\ncharacterize the robustness of the trained transformer to a variety of\ndistribution shifts and show that although a number of shifts are tolerated,\nshifts in the covariate distribution of the prompts are not. Motivated by this,\nwe consider a generalized ICL setting where the covariate distributions can\nvary across prompts. We show that although gradient flow succeeds at finding a\nglobal minimum in this setting, the trained transformer is still brittle under\nmild covariate shifts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_R/0/1/0/all/0/1\">Ruiqi Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Frei_S/0/1/0/all/0/1\">Spencer Frei</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bartlett_P/0/1/0/all/0/1\">Peter L. Bartlett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data and Comprehensive Evaluation. (arXiv:2306.09968v1 [cs.CL])","link":"http://arxiv.org/abs/2306.09968","description":"<p>Large language models have exhibited exceptional performance on various\nNatural Language Processing (NLP) tasks, leveraging techniques such as the\npre-training, and instruction fine-tuning. Despite these advances, their\neffectiveness in medical applications is limited, due to challenges such as\nfactual inaccuracies, reasoning abilities, and lack grounding in real-world\nexperience. In this study, we present ClinicalGPT, a language model explicitly\ndesigned and optimized for clinical scenarios. By incorporating extensive and\ndiverse real-world data, such as medical records, domain-specific knowledge,\nand multi-round dialogue consultations in the training process, ClinicalGPT is\nbetter prepared to handle multiple clinical task. Furthermore, we introduce a\ncomprehensive evaluation framework that includes medical knowledge\nquestion-answering, medical exams, patient consultations, and diagnostic\nanalysis of medical records. Our results demonstrate that ClinicalGPT\nsignificantly outperforms other models in these tasks, highlighting the\neffectiveness of our approach in adapting large language models to the critical\ndomain of healthcare.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guoxing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zongxin Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Longjun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaohu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering. (arXiv:2306.09996v1 [cs.CV])","link":"http://arxiv.org/abs/2306.09996","description":"<p>Visual question answering (VQA) is a challenging task that requires the\nability to comprehend and reason with visual information. While recent\nvision-language models have made strides, they continue to struggle with\nzero-shot VQA, particularly in handling complex compositional questions and\nadapting to new domains i.e. knowledge-based reasoning. This paper explores the\nuse of various prompting strategies, focusing on the BLIP2 model, to enhance\nzero-shot VQA performance. We conduct a comprehensive investigation across\nseveral VQA datasets, examining the effectiveness of different question\ntemplates, the role of few-shot exemplars, the impact of chain-of-thought (CoT)\nreasoning, and the benefits of incorporating image captions as additional\nvisual cues. Despite the varied outcomes, our findings demonstrate that\ncarefully designed question templates and the integration of additional visual\ncues, like image captions, can contribute to improved VQA performance,\nespecially when used in conjunction with few-shot examples. However, we also\nidentify a limitation in the use of chain-of-thought rationalization, which\nnegatively affects VQA accuracy. Our study thus provides critical insights into\nthe potential of prompting for improving zero-shot VQA performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Awal_R/0/1/0/all/0/1\">Rabiul Awal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Le Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Aishwarya Agrawal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing. (arXiv:2306.10012v1 [cs.CV])","link":"http://arxiv.org/abs/2306.10012","description":"<p>Text-guided image editing is widely needed in daily life, ranging from\npersonal use to professional applications such as Photoshop. However, existing\nmethods are either zero-shot or trained on an automatically synthesized\ndataset, which contains a high volume of noise. Thus, they still require lots\nof manual tuning to produce desirable outcomes in practice. To address this\nissue, we introduce MagicBrush (https://osu-nlp-group.github.io/MagicBrush/),\nthe first large-scale, manually annotated dataset for instruction-guided real\nimage editing that covers diverse scenarios: single-turn, multi-turn,\nmask-provided, and mask-free editing. MagicBrush comprises over 10K manually\nannotated triples (source image, instruction, target image), which supports\ntrainining large-scale text-guided image editing models. We fine-tune\nInstructPix2Pix on MagicBrush and show that the new model can produce much\nbetter images according to human evaluation. We further conduct extensive\nexperiments to evaluate current image editing baselines from multiple\ndimensions including quantitative, qualitative, and human evaluations. The\nresults reveal the challenging nature of our dataset and the gap between\ncurrent baselines and real-world editing needs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_L/0/1/0/all/0/1\">Lingbo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Just One Byte (per gradient): A Note on Low-Bandwidth Decentralized Language Model Finetuning Using Shared Randomness. (arXiv:2306.10015v1 [cs.LG])","link":"http://arxiv.org/abs/2306.10015","description":"<p>Language model training in distributed settings is limited by the\ncommunication cost of gradient exchanges. In this short note, we extend recent\nwork from Malladi et al. (2023), using shared randomness to perform distributed\nfine-tuning with low bandwidth. The method is a natural decentralized extension\nof memory-efficient Simultaneous Perturbation Stochastic Approximation (SPSA).\nEach iteration, each machine seeds a Random Number Generator (RNG) to perform\nlocal reproducible perturbations on model weights and calculate and exchange\nscalar projected gradients, which are then used to update each model. By using\na (machine, sample) identifier as the random seed, each model can regenerate\none another's perturbations. As machines only exchange single-byte projected\ngradients, this is highly communication efficient. There are also potential\nprivacy benefits, as projected gradients may be calculated on different\ntraining data, and models never access the other's data. Our approach not only\ndrastically reduces communication bandwidth requirements but also accommodates\ndynamic addition or removal of machines during the training process and retains\nthe memory-efficient and inference-only advantages of recent work. We perform\nproof-of-concept experiments to demonstrate the potential usefulness of this\nmethod, building off of rich literature on distributed optimization and\nmemory-efficient training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zelikman_E/0/1/0/all/0/1\">Eric Zelikman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haber_N/0/1/0/all/0/1\">Nick Haber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah D. Goodman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting DocRED -- Addressing the False Negative Problem in Relation Extraction. (arXiv:2205.12696v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12696","description":"<p>The DocRED dataset is one of the most popular and widely used benchmarks for\ndocument-level relation extraction (RE). It adopts a recommend-revise\nannotation scheme so as to have a large-scale annotated dataset. However, we\nfind that the annotation of DocRED is incomplete, i.e., false negative samples\nare prevalent. We analyze the causes and effects of the overwhelming false\nnegative problem in the DocRED dataset. To address the shortcoming, we\nre-annotate 4,053 documents in the DocRED dataset by adding the missed relation\ntriples back to the original DocRED. We name our revised DocRED dataset\nRe-DocRED. We conduct extensive experiments with state-of-the-art neural models\non both datasets, and the experimental results show that the models trained and\nevaluated on our Re-DocRED achieve performance improvements of around 13 F1\npoints. Moreover, we conduct a comprehensive analysis to identify the potential\nareas for further improvement. Our dataset is publicly available at\nhttps://github.com/tonytan48/Re-DocRED.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Q/0/1/0/all/0/1\">Qingyu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_H/0/1/0/all/0/1\">Hwee Tou Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aljunied_S/0/1/0/all/0/1\">Sharifah Mahani Aljunied</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resolving the Human Subjects Status of Machine Learning's Crowdworkers. (arXiv:2206.04039v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2206.04039","description":"<p>In recent years, machine learning (ML) has relied heavily on crowdworkers\nboth for building datasets and for addressing research questions requiring\nhuman interaction or judgment. The diverse tasks performed and uses of the data\nproduced render it difficult to determine when crowdworkers are best thought of\nas workers (versus human subjects). These difficulties are compounded by\nconflicting policies, with some institutions and researchers regarding all ML\ncrowdworkers as human subjects and others holding that they rarely constitute\nhuman subjects. Notably few ML papers involving crowdwork mention IRB\noversight, raising the prospect of non-compliance with ethical and regulatory\nrequirements. We investigate the appropriate designation of ML crowdsourcing\nstudies, focusing our inquiry on natural language processing to expose unique\nchallenges for research oversight. Crucially, under the U.S. Common Rule, these\njudgments hinge on determinations of aboutness, concerning both whom (or what)\nthe collected data is about and whom (or what) the analysis is about. We\nhighlight two challenges posed by ML: the same set of workers can serve\nmultiple roles and provide many sorts of information; and ML research tends to\nembrace a dynamic workflow, where research questions are seldom stated ex ante\nand data sharing opens the door for future studies to aim questions at\ndifferent targets. Our analysis exposes a potential loophole in the Common\nRule, where researchers can elude research ethics oversight by splitting data\ncollection and analysis into distinct studies. Finally, we offer several policy\nrecommendations to address these concerns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaushik_D/0/1/0/all/0/1\">Divyansh Kaushik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+London_A/0/1/0/all/0/1\">Alex John London</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fact-Saboteurs: A Taxonomy of Evidence Manipulation Attacks against Fact-Verification Systems. (arXiv:2209.03755v4 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2209.03755","description":"<p>Mis- and disinformation are a substantial global threat to our security and\nsafety. To cope with the scale of online misinformation, researchers have been\nworking on automating fact-checking by retrieving and verifying against\nrelevant evidence. However, despite many advances, a comprehensive evaluation\nof the possible attack vectors against such systems is still lacking.\nParticularly, the automated fact-verification process might be vulnerable to\nthe exact disinformation campaigns it is trying to combat. In this work, we\nassume an adversary that automatically tampers with the online evidence in\norder to disrupt the fact-checking model via camouflaging the relevant evidence\nor planting a misleading one. We first propose an exploratory taxonomy that\nspans these two targets and the different threat model dimensions. Guided by\nthis, we design and propose several potential attack methods. We show that it\nis possible to subtly modify claim-salient snippets in the evidence and\ngenerate diverse and claim-aligned evidence. Thus, we highly degrade the\nfact-checking performance under many different permutations of the taxonomy's\ndimensions. The attacks are also robust against post-hoc modifications of the\nclaim. Our analysis further hints at potential limitations in models' inference\nwhen faced with contradicting evidence. We emphasize that these attacks can\nhave harmful implications on the inspectable and human-in-the-loop usage\nscenarios of such models, and we conclude by discussing challenges and\ndirections for future defenses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelnabi_S/0/1/0/all/0/1\">Sahar Abdelnabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1\">Mario Fritz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding. (arXiv:2210.03347v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03347","description":"<p>Visually-situated language is ubiquitous -- sources range from textbooks with\ndiagrams to web pages with images and tables, to mobile apps with buttons and\nforms. Perhaps due to this diversity, previous work has typically relied on\ndomain-specific recipes with limited sharing of the underlying data, model\narchitectures, and objectives. We present Pix2Struct, a pretrained\nimage-to-text model for purely visual language understanding, which can be\nfinetuned on tasks containing visually-situated language. Pix2Struct is\npretrained by learning to parse masked screenshots of web pages into simplified\nHTML. The web, with its richness of visual elements cleanly reflected in the\nHTML structure, provides a large source of pretraining data well suited to the\ndiversity of downstream tasks. Intuitively, this objective subsumes common\npretraining signals such as OCR, language modeling, image captioning. In\naddition to the novel pretraining strategy, we introduce a variable-resolution\ninput representation and a more flexible integration of language and vision\ninputs, where language prompts such as questions are rendered directly on top\nof the input image. For the first time, we show that a single pretrained model\ncan achieve state-of-the-art results in six out of nine tasks across four\ndomains: documents, illustrations, user interfaces, and natural images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kenton Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1\">Mandar Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turc_I/0/1/0/all/0/1\">Iulia Turc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hexiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenschlos_J/0/1/0/all/0/1\">Julian Eisenschlos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_U/0/1/0/all/0/1\">Urvashi Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_P/0/1/0/all/0/1\">Peter Shaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toutanova_K/0/1/0/all/0/1\">Kristina Toutanova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SLUE Phase-2: A Benchmark Suite of Diverse Spoken Language Understanding Tasks. (arXiv:2212.10525v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10525","description":"<p>Spoken language understanding (SLU) tasks have been studied for many decades\nin the speech research community, but have not received as much attention as\nlower-level tasks like speech and speaker recognition. In particular, there are\nnot nearly as many SLU task benchmarks, and many of the existing ones use data\nthat is not freely available to all researchers. Recent work has begun to\nintroduce such benchmark datasets for several tasks. In this work, we introduce\nseveral new annotated SLU benchmark tasks based on freely available speech\ndata, which complement existing benchmarks and address gaps in the SLU\nevaluation landscape. We contribute four tasks: question answering and\nsummarization involve inference over longer speech sequences; named entity\nlocalization addresses the speech-specific task of locating the targeted\ncontent in the signal; dialog act classification identifies the function of a\ngiven speech utterance. We follow the blueprint of the Spoken Language\nUnderstanding Evaluation (SLUE) benchmark suite. In order to facilitate the\ndevelopment of SLU models that leverage the success of pre-trained speech\nrepresentations, we will be publishing for each task (i) annotations for a\nrelatively small fine-tuning set, (ii) annotated development and test sets, and\n(iii) baseline models for easy reproducibility and comparisons. In this work,\nwe present the details of data collection and annotation and the performance of\nthe baseline models. We also perform sensitivity analysis of pipeline models'\nperformance (speech recognizer + text model) to the speech recognition\naccuracy, using more than 20 state-of-the-art speech recognition models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shon_S/0/1/0/all/0/1\">Suwon Shon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Siddhant Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chyi-Jiunn Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasad_A/0/1/0/all/0/1\">Ankita Pasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Felix Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Roshan Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei-Lun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransFool: An Adversarial Attack against Neural Machine Translation Models. (arXiv:2302.00944v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.00944","description":"<p>Deep neural networks have been shown to be vulnerable to small perturbations\nof their inputs, known as adversarial attacks. In this paper, we investigate\nthe vulnerability of Neural Machine Translation (NMT) models to adversarial\nattacks and propose a new attack algorithm called TransFool. To fool NMT\nmodels, TransFool builds on a multi-term optimization problem and a gradient\nprojection step. By integrating the embedding representation of a language\nmodel, we generate fluent adversarial examples in the source language that\nmaintain a high level of semantic similarity with the clean samples.\nExperimental results demonstrate that, for different translation tasks and NMT\narchitectures, our white-box attack can severely degrade the translation\nquality while the semantic similarity between the original and the adversarial\nsentences stays high. Moreover, we show that TransFool is transferable to\nunknown target models. Finally, based on automatic and human evaluations,\nTransFool leads to improvement in terms of success rate, semantic similarity,\nand fluency compared to the existing attacks both in white-box and black-box\nsettings. Thus, TransFool permits us to better characterize the vulnerability\nof NMT models and outlines the necessity to design strong defense mechanisms\nand more robust NMT systems for real-life applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sadrizadeh_S/0/1/0/all/0/1\">Sahar Sadrizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolamic_L/0/1/0/all/0/1\">Ljiljana Dolamic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1\">Pascal Frossard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Activity Prediction Models in Drug Discovery with the Ability to Understand Human Language. (arXiv:2303.03363v2 [q-bio.BM] UPDATED)","link":"http://arxiv.org/abs/2303.03363","description":"<p>Activity and property prediction models are the central workhorses in drug\ndiscovery and materials sciences, but currently they have to be trained or\nfine-tuned for new tasks. Without training or fine-tuning, scientific language\nmodels could be used for such low-data tasks through their announced zero- and\nfew-shot capabilities. However, their predictive quality at activity prediction\nis lacking. In this work, we envision a novel type of activity prediction model\nthat is able to adapt to new prediction tasks at inference time, via\nunderstanding textual information describing the task. To this end, we propose\na new architecture with separate modules for chemical and natural language\ninputs, and a contrastive pre-training objective on data from large biochemical\ndatabases. In extensive experiments, we show that our method CLAMP yields\nimproved predictive performance on few-shot learning benchmarks and zero-shot\nproblems in drug discovery. We attribute the advances of our method to the\nmodularized architecture and to our pre-training objective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Seidl_P/0/1/0/all/0/1\">Philipp Seidl</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Vall_A/0/1/0/all/0/1\">Andreu Vall</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hochreiter_S/0/1/0/all/0/1\">Sepp Hochreiter</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Klambauer_G/0/1/0/all/0/1\">G&#xfc;nter Klambauer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Opinion Mining and Topic Classification of Course Reviews. (arXiv:2304.03394v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.03394","description":"<p>Student opinions for a course are important to educators and administrators,\nregardless of the type of the course or the institution. Reading and manually\nanalyzing open-ended feedback becomes infeasible for massive volumes of\ncomments at institution level or online forums. In this paper, we collected and\npre-processed a large number of course reviews publicly available online. We\napplied machine learning techniques with the goal to gain insight into student\nsentiments and topics. Specifically, we utilized current Natural Language\nProcessing (NLP) techniques, such as word embeddings and deep neural networks,\nand state-of-the-art BERT (Bidirectional Encoder Representations from\nTransformers), RoBERTa (Robustly optimized BERT approach) and XLNet\n(Generalized Auto-regression Pre-training). We performed extensive\nexperimentation to compare these techniques versus traditional approaches. This\ncomparative study demonstrates how to apply modern machine learning approaches\nfor sentiment polarity extraction and topic-based classification utilizing\ncourse feedback. For sentiment polarity, the top model was RoBERTa with 95.5%\naccuracy and 84.7% F1-macro, while for topic classification, an SVM (Support\nVector Machine) was the top classifier with 79.8% accuracy and 80.6% F1-macro.\nWe also provided an in-depth exploration of the effect of certain\nhyperparameters on the model performance and discussed our observations. These\nfindings can be used by institutions and course providers as a guide for\nanalyzing their own course feedback using NLP models towards self-evaluation\nand improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koufakou_A/0/1/0/all/0/1\">Anna Koufakou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Similarity-Aware Multimodal Prompt Learning for Fake News Detection. (arXiv:2304.04187v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.04187","description":"<p>The standard paradigm for fake news detection mainly utilizes text\ninformation to model the truthfulness of news. However, the discourse of online\nfake news is typically subtle and it requires expert knowledge to use textual\ninformation to debunk fake news. Recently, studies focusing on multimodal fake\nnews detection have outperformed text-only methods. Recent approaches utilizing\nthe pre-trained model to extract unimodal features, or fine-tuning the\npre-trained model directly, have become a new paradigm for detecting fake news.\nAgain, this paradigm either requires a large number of training instances, or\nupdates the entire set of pre-trained model parameters, making real-world fake\nnews detection impractical. Furthermore, traditional multimodal methods fuse\nthe cross-modal features directly without considering that the uncorrelated\nsemantic representation might inject noise into the multimodal features. This\npaper proposes a Similarity-Aware Multimodal Prompt Learning (SAMPLE)\nframework. First, we incorporate prompt learning into multimodal fake news\ndetection. Prompt learning, which only tunes prompts with a frozen language\nmodel, can reduce memory usage significantly and achieve comparable\nperformances, compared with fine-tuning. We analyse three prompt templates with\na soft verbalizer to detect fake news. In addition, we introduce the\nsimilarity-aware fusing method to adaptively fuse the intensity of multimodal\nrepresentation and mitigate the noise injection via uncorrelated cross-modal\nfeatures. For evaluation, SAMPLE surpasses the F1 and the accuracies of\nprevious works on two benchmark multimodal datasets, demonstrating the\neffectiveness of the proposed method in detecting fake news. In addition,\nSAMPLE also is superior to other approaches regardless of few-shot and\ndata-rich settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Ye Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaomin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yimin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaoman Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingyi Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maynard_D/0/1/0/all/0/1\">Diana Maynard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"When Words Fail, Emojis Prevail\": Generating Sarcastic Utterances with Emoji Using Valence Reversal and Semantic Incongruity. (arXiv:2305.04105v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.04105","description":"<p>Sarcasm is a form of figurative language that serves as a humorous tool for\nmockery and ridicule. We present a novel architecture for sarcasm generation\nwith emoji from a non-sarcastic input sentence in English. We divide the\ngeneration task into two sub tasks: one for generating textual sarcasm and\nanother for collecting emojis associated with those sarcastic sentences. Two\nkey elements of sarcasm are incorporated into the textual sarcasm generation\ntask: valence reversal and semantic incongruity with context, where the context\nmay involve shared commonsense or general knowledge between the speaker and\ntheir audience. The majority of existing sarcasm generation works have focused\non this textual form. However, in the real world, when written texts fall short\nof effectively capturing the emotional cues of spoken and face-to-face\ncommunication, people often opt for emojis to accurately express their\nemotions. Due to the wide range of applications of emojis, incorporating\nappropriate emojis to generate textual sarcastic sentences helps advance\nsarcasm generation. We conclude our study by evaluating the generated sarcastic\nsentences using human judgement. All the codes and data used in this study has\nbeen made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kader_F/0/1/0/all/0/1\">Faria Binte Kader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nujat_N/0/1/0/all/0/1\">Nafisa Hossain Nujat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogir_T/0/1/0/all/0/1\">Tasmia Binte Sogir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Mohsinul Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmud_H/0/1/0/all/0/1\">Hasan Mahmud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_K/0/1/0/all/0/1\">Kamrul Hasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating GPT-3 Generated Explanations for Hateful Content Moderation. (arXiv:2305.17680v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17680","description":"<p>Recent research has focused on using large language models (LLMs) to generate\nexplanations for hate speech through fine-tuning or prompting. Despite the\ngrowing interest in this area, these generated explanations' effectiveness and\npotential limitations remain poorly understood. A key concern is that these\nexplanations, generated by LLMs, may lead to erroneous judgments about the\nnature of flagged content by both users and content moderators. For instance,\nan LLM-generated explanation might inaccurately convince a content moderator\nthat a benign piece of content is hateful. In light of this, we propose an\nanalytical framework for examining hate speech explanations and conducted an\nextensive survey on evaluating such explanations. Specifically, we prompted\nGPT-3 to generate explanations for both hateful and non-hateful content, and a\nsurvey was conducted with 2,400 unique respondents to evaluate the generated\nexplanations. Our findings reveal that (1) human evaluators rated the\nGPT-generated explanations as high quality in terms of linguistic fluency,\ninformativeness, persuasiveness, and logical soundness, (2) the persuasive\nnature of these explanations, however, varied depending on the prompting\nstrategy employed, and (3) this persuasiveness may result in incorrect\njudgments about the hatefulness of the content. Our study underscores the need\nfor caution in applying LLM-generated explanations for content moderation. Code\nand results are available at https://github.com/Social-AI-Studio/GPT3-HateEval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Han Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hee_M/0/1/0/all/0/1\">Ming Shan Hee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awal_M/0/1/0/all/0/1\">Md Rabiul Awal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_K/0/1/0/all/0/1\">Kenny Tsu Wei Choo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Sometimes Generate Purely Negatively-Reinforced Text. (arXiv:2306.07567v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2306.07567","description":"<p>When using adversarial training, it is common practice to train against the\nmost egregious failures. However, this might imply using examples with\nsensitive information (such as leaked passwords or security vulnerabilities) as\ntraining data. One might assume that language models trained with gradient\ndescent never generate text snippets which were only present in examples\nassociated with the lowest possible reward. In this paper, we show that this\nassumption is wrong: in some situations, large language models do learn from\nsuch negatively-reinforced examples. We present a specific training setup that\nenables Pythia-160M to guess passwords 13% more often than it would by guessing\nrandomly, despite only showing it these passwords on examples where the model\nis incentivized to not output these passwords. Our code is available at\nwww.github.com/FabienRoger/Learning-From-Negative-Examples\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roger_F/0/1/0/all/0/1\">Fabien Roger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition. (arXiv:2306.07848v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.07848","description":"<p>Contrastive learning based pretraining methods have recently exhibited\nimpressive success in diverse fields. In this paper, we propose GEmo-CLAP, a\nkind of efficient gender-attribute-enhanced contrastive language-audio\npretraining (CLAP) model for speech emotion recognition. To be specific, we\nfirst build an effective emotion CLAP model Emo-CLAP for emotion recognition,\nutilizing various self-supervised learning based pre-trained models. Then,\nconsidering the importance of the gender attribute in speech emotion modeling,\ntwo GEmo-CLAP approaches are further proposed to integrate the emotion and\ngender information of speech signals, forming more reasonable objectives.\nExtensive experiments on the IEMOCAP corpus demonstrate that our proposed two\nGEmo-CLAP approaches consistently outperform the baseline Emo-CLAP with\ndifferent pre-trained models, while also achieving superior recognition\nperformance compared with other state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yanni Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuguang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jixun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_W/0/1/0/all/0/1\">Wen Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Heng Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"h2oGPT: Democratizing Large Language Models. (arXiv:2306.08161v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.08161","description":"<p>Applications built on top of Large Language Models (LLMs) such as GPT-4\nrepresent a revolution in AI due to their human-level capabilities in natural\nlanguage processing. However, they also pose many significant risks such as the\npresence of biased, private, or harmful text, and the unauthorized inclusion of\ncopyrighted material.\n</p>\n<p>We introduce h2oGPT, a suite of open-source code repositories for the\ncreation and use of LLMs based on Generative Pretrained Transformers (GPTs).\nThe goal of this project is to create the world's best truly open-source\nalternative to closed-source approaches. In collaboration with and as part of\nthe incredible and unstoppable open-source community, we open-source several\nfine-tuned h2oGPT models from 7 to 40 Billion parameters, ready for commercial\nuse under fully permissive Apache 2.0 licenses. Included in our release is\n100\\% private document search using natural language.\n</p>\n<p>Open-source language models help boost AI development and make it more\naccessible and trustworthy. They lower entry hurdles, allowing people and\ngroups to tailor these models to their needs. This openness increases\ninnovation, transparency, and fairness. An open-source strategy is needed to\nshare AI benefits fairly, and H2O.ai will continue to democratize AI and LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Candel_A/0/1/0/all/0/1\">Arno Candel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKinney_J/0/1/0/all/0/1\">Jon McKinney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singer_P/0/1/0/all/0/1\">Philipp Singer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_P/0/1/0/all/0/1\">Pascal Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeblick_M/0/1/0/all/0/1\">Maximilian Jeblick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhu_P/0/1/0/all/0/1\">Prithvi Prabhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gambera_J/0/1/0/all/0/1\">Jeff Gambera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landry_M/0/1/0/all/0/1\">Mark Landry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_S/0/1/0/all/0/1\">Shivam Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chesler_R/0/1/0/all/0/1\">Ryan Chesler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chun Ming Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1\">Marcos V. Conde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stetsenko_P/0/1/0/all/0/1\">Pasha Stetsenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grellier_O/0/1/0/all/0/1\">Olivier Grellier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambati_S/0/1/0/all/0/1\">SriSatish Ambati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-06-18T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}