{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-12-02T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Operationalizing Specifications, In Addition to Test Sets for Evaluating Constrained Generative Models. (arXiv:2212.00006v1 [cs.HC])","link":"http://arxiv.org/abs/2212.00006","description":"<p>In this work, we present some recommendations on the evaluation of\nstate-of-the-art generative models for constrained generation tasks. The\nprogress on generative models has been rapid in recent years. These large-scale\nmodels have had three impacts: firstly, the fluency of generation in both\nlanguage and vision modalities has rendered common average-case evaluation\nmetrics much less useful in diagnosing system errors. Secondly, the same\nsubstrate models now form the basis of a number of applications, driven both by\nthe utility of their representations as well as phenomena such as in-context\nlearning, which raise the abstraction level of interacting with such models.\nThirdly, the user expectations around these models and their feted public\nreleases have made the technical challenge of out of domain generalization much\nless excusable in practice. Subsequently, our evaluation methodologies haven't\nadapted to these changes. More concretely, while the associated utility and\nmethods of interacting with generative models have expanded, a similar\nexpansion has not been observed in their evaluation practices. In this paper,\nwe argue that the scale of generative models could be exploited to raise the\nabstraction level at which evaluation itself is conducted and provide\nrecommendations for the same. Our recommendations are based on leveraging\nspecifications as a powerful instrument to evaluate generation quality and are\nreadily applicable to a variety of tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raunak_V/0/1/0/all/0/1\">Vikas Raunak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Post_M/0/1/0/all/0/1\">Matt Post</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menezes_A/0/1/0/all/0/1\">Arul Menezes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Specific Embeddings for Ante-Hoc Explainable Text Classification. (arXiv:2212.00086v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00086","description":"<p>Current state-of-the-art approaches to text classification typically leverage\nBERT-style Transformer models with a softmax classifier, jointly fine-tuned to\npredict class labels of a target task. In this paper, we instead propose an\nalternative training objective in which we learn task-specific embeddings of\ntext: our proposed objective learns embeddings such that all texts that share\nthe same target class label should be close together in the embedding space,\nwhile all others should be far apart. This allows us to replace the softmax\nclassifier with a more interpretable k-nearest-neighbor classification\napproach. In a series of experiments, we show that this yields a number of\ninteresting benefits: (1) The resulting order induced by distances in the\nembedding space can be used to directly explain classification decisions. (2)\nThis facilitates qualitative inspection of the training data, helping us to\nbetter understand the problem space and identify labelling quality issues. (3)\nThe learned distances to some degree generalize to unseen classes, allowing us\nto incrementally add new classes without retraining the model. We present\nextensive experiments which show that the benefits of ante-hoc explainability\nand incremental learning come at no cost in overall classification accuracy,\nthus pointing to practical applicability of our proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Halder_K/0/1/0/all/0/1\">Kishaloy Halder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krapac_J/0/1/0/all/0/1\">Josip Krapac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbik_A/0/1/0/all/0/1\">Alan Akbik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brew_A/0/1/0/all/0/1\">Anthony Brew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyra_M/0/1/0/all/0/1\">Matti Lyra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Alignment in the Era of Deep Learning: A Tutorial. (arXiv:2212.00138v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00138","description":"<p>The word alignment task, despite its prominence in the era of statistical\nmachine translation (SMT), is niche and under-explored today. In this two-part\ntutorial, we argue for the continued relevance for word alignment. The first\npart provides a historical background to word alignment as a core component of\nthe traditional SMT pipeline. We zero-in on GIZA++, an unsupervised,\nstatistical word aligner with surprising longevity. Jumping forward to the era\nof neural machine translation (NMT), we show how insights from word alignment\ninspired the attention mechanism fundamental to present-day NMT. The second\npart shifts to a survey approach. We cover neural word aligners, showing the\nslow but steady progress towards surpassing GIZA++ performance. Finally, we\ncover the present-day applications of word alignment, from cross-lingual\nannotation projection, to improving translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bryan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Relation and Event Type Discovery with Type Abstraction. (arXiv:2212.00178v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00178","description":"<p>Conventional closed-world information extraction (IE) approaches rely on\nhuman ontologies to define the scope for extraction. As a result, such\napproaches fall short when applied to new domains. This calls for systems that\ncan automatically infer new types from given corpora, a task which we refer to\nas type discovery. To tackle this problem, we introduce the idea of type\nabstraction, where the model is prompted to generalize and name the type. Then\nwe use the similarity between inferred names to induce clusters. Observing that\nthis abstraction-based representation is often complementary to the\nentity/trigger token representation, we set up these two representations as two\nviews and design our model as a co-training framework. Our experiments on\nmultiple relation extraction and event extraction datasets consistently show\nthe advantage of our type abstraction approach. Code available at\nhttps://github.com/raspberryice/type-discovery-abs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AUG-FedPrompt: Practical Few-shot Federated NLP with Data-augmented Prompts. (arXiv:2212.00192v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00192","description":"<p>Transformer-based pre-trained models have become the de-facto solution for\nNLP tasks. Fine-tuning such pre-trained models for downstream tasks often\nrequires tremendous amount of data that is both private and labeled. However,\nin reality: 1) such private data cannot be collected and is distributed across\nmobile devices, and 2) well-curated labeled data is scarce. To tackle those\nissues, we first define a data generator for federated few-shot learning tasks,\nwhich encompasses the quantity and distribution of scarce labeled data in a\nrealistic setting. Then we propose AUG-FedPrompt, a prompt-based federated\nlearning algorithm that carefully annotates abundant unlabeled data for data\naugmentation. AUG-FedPrompt can perform on par with full-set fine-tuning with\nvery few initial labeled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Dongqi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yaozong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Haitao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shangguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Felix Xiaozhu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengwei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Multi-Step Reasoning Capabilities of Large Language Models into Smaller Models via Semantic Decompositions. (arXiv:2212.00193v1 [cs.LG])","link":"http://arxiv.org/abs/2212.00193","description":"<p>Step-by-step reasoning approaches like chain-of-thought (CoT) have proved to\nbe a very effective technique to induce reasoning capabilities in large\nlanguage models. However, the success of the CoT approach depends primarily on\nmodel size, and often billion parameter-scale models are needed to get CoT to\nwork. In this paper, we propose a knowledge distillation approach, that\nleverages the step-by-step CoT reasoning capabilities of larger models and\ndistils these reasoning abilities into smaller models. Our approach\nDecompositional Distillation learns a semantic decomposition of the original\nproblem into a sequence of subproblems and uses it to train two models: a) a\nproblem decomposer that learns to decompose the complex reasoning problem into\na sequence of simpler sub-problems and b) a problem solver that uses the\nintermediate subproblems to solve the overall problem. On a multi-step math\nword problem dataset (GSM8K), we boost the performance of GPT-2 variants up to\n35% when distilled with our approach compared to CoT. We show that using our\napproach, it is possible to train a GPT-2-large model (775M) that can\noutperform a 10X larger GPT-3 (6B) model trained using CoT reasoning. Finally,\nwe also demonstrate that our approach of problem decomposition can also be used\nas an alternative to CoT prompting, which boosts the GPT-3 performance by 40%\ncompared to CoT prompts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shridhar_K/0/1/0/all/0/1\">Kumar Shridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolfo_A/0/1/0/all/0/1\">Alessandro Stolfo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Efficient Finetuning Using Cross-Task Nearest Neighbors. (arXiv:2212.00196v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00196","description":"<p>Language models trained on massive prompted multitask datasets like T0 (Sanh\net al., 2021) or FLAN (Wei et al., 2021a) can generalize to tasks unseen during\ntraining. We show that training on a carefully chosen subset of instances can\noutperform training on all available data on a variety of datasets. We assume\naccess to a small number (250--1000) of unlabeled target task instances, select\ntheir nearest neighbors from a pool of multitask data, and use the retrieved\ndata to train target task-specific models. Our method is more data-efficient\nthan training a single multitask model, while still outperforming it by large\nmargins. We evaluate across a diverse set of tasks not in the multitask pool we\nretrieve from, including those used to evaluate T0 and additional complex tasks\nincluding legal and scientific document QA. We retrieve small subsets of P3\n(the collection of prompted datasets from which T0's training data was sampled)\nand finetune T5 models that outperform the 3-billion parameter variant of T0\n(T0-3B) by 3--30% on 12 out of 14 evaluation datasets while using at most 2% of\nthe data used to train T0-3B. These models also provide a better initialization\nthan T0-3B for few-shot finetuning on target-task data, as shown by a 2--23%\nrelative improvement over few-shot finetuned T0-3B models on 8 datasets. Our\ncode is available at https://github.com/allenai/data-efficient-finetuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivison_H/0/1/0/all/0/1\">Hamish Ivison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasigi_P/0/1/0/all/0/1\">Pradeep Dasigi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biomedical NER for the Enterprise with Distillated BERN2 and the Kazu Framework. (arXiv:2212.00223v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00223","description":"<p>In order to assist the drug discovery/development process, pharmaceutical\ncompanies often apply biomedical NER and linking techniques over internal and\npublic corpora. Decades of study of the field of BioNLP has produced a plethora\nof algorithms, systems and datasets. However, our experience has been that no\nsingle open source system meets all the requirements of a modern pharmaceutical\ncompany. In this work, we describe these requirements according to our\nexperience of the industry, and present Kazu, a highly extensible, scalable\nopen source framework designed to support BioNLP for the pharmaceutical sector.\nKazu is a built around a computationally efficient version of the BERN2 NER\nmodel (TinyBERN2), and subsequently wraps several other BioNLP technologies\ninto one coherent system. KAZU framework is open-sourced:\nhttps://github.com/AstraZeneca/KAZU\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_W/0/1/0/all/0/1\">Wonjin Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jackson_R/0/1/0/all/0/1\">Richard Jackson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ford_E/0/1/0/all/0/1\">Elliot Ford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poroshin_V/0/1/0/all/0/1\">Vladimir Poroshin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Complex Dialogue Mappings via Sentence Semantic Segmentation Guided Conditional Variational Auto-Encoder. (arXiv:2212.00231v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00231","description":"<p>Complex dialogue mappings (CDM), including one-to-many and many-to-one\nmappings, tend to make dialogue models generate incoherent or dull responses,\nand modeling these mappings remains a huge challenge for neural dialogue\nsystems. To alleviate these problems, methods like introducing external\ninformation, reconstructing the optimization function, and manipulating data\nsamples are proposed, while they primarily focus on avoiding training with CDM,\ninevitably weakening the model's ability of understanding CDM in human\nconversations and limiting further improvements in model performance. This\npaper proposes a Sentence Semantic \\textbf{Seg}mentation guided\n\\textbf{C}onditional \\textbf{V}ariational \\textbf{A}uto-\\textbf{E}ncoder\n(SegCVAE) method which can model and take advantages of the CDM data.\nSpecifically, to tackle the incoherent problem caused by one-to-many, SegCVAE\nuses response-related prominent semantics to constrained the latent variable.\nTo mitigate the non-diverse problem brought by many-to-one, SegCVAE segments\nmultiple prominent semantics to enrich the latent variables. Three novel\ncomponents, Internal Separation, External Guidance, and Semantic Norms, are\nproposed to achieve SegCVAE. On dialogue generation tasks, both the automatic\nand human evaluation results show that SegCVAE achieves new state-of-the-art\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shaoxiong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inference of Media Bias and Content Quality Using Natural-Language Processing. (arXiv:2212.00237v1 [physics.soc-ph])","link":"http://arxiv.org/abs/2212.00237","description":"<p>Media bias can significantly impact the formation and development of opinions\nand sentiments in a population. It is thus important to study the emergence and\ndevelopment of partisan media and political polarization. However, it is\nchallenging to quantitatively infer the ideological positions of media outlets.\nIn this paper, we present a quantitative framework to infer both political bias\nand content quality of media outlets from text, and we illustrate this\nframework with empirical experiments with real-world data. We apply a\nbidirectional long short-term memory (LSTM) neural network to a data set of\nmore than 1 million tweets to generate a two-dimensional ideological-bias and\ncontent-quality measurement for each tweet. We then infer a ``media-bias\nchart'' of (bias, quality) coordinates for the media outlets by integrating the\n(bias, quality) measurements of the tweets of the media outlets. We also apply\na variety of baseline machine-learning methods, such as a naive-Bayes method\nand a support-vector machine (SVM), to infer the bias and quality values for\neach tweet. All of these baseline approaches are based on a bag-of-words\napproach. We find that the LSTM-network approach has the best performance of\nthe examined methods. Our results illustrate the importance of leveraging word\norder into machine-learning methods in text analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Chao_Z/0/1/0/all/0/1\">Zehan Chao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Molitor_D/0/1/0/all/0/1\">Denali Molitor</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Needell_D/0/1/0/all/0/1\">Deanna Needell</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Porter_M/0/1/0/all/0/1\">Mason A. Porter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noisy Label Detection for Speaker Recognition. (arXiv:2212.00239v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00239","description":"<p>The success of deep neural networks requires both high annotation quality and\nmassive data. However, the size and the quality of a dataset are usually a\ntrade-off in practice, as data collection and cleaning are expensive and\ntime-consuming. Therefore, automatic noisy label detection (NLD) techniques are\ncritical to real-world applications, especially those using crowdsourcing\ndatasets. As this is an under-explored topic in automatic speaker verification\n(ASV), we present a simple but effective solution to the task. First, we\ncompare the effectiveness of various commonly used metric learning loss\nfunctions under different noise settings. Then, we propose two ranking-based\nNLD methods, inter-class inconsistency and intra-class inconsistency ranking.\nThey leverage the inconsistent nature of noisy labels and show high detection\nprecision even under a high level of noise. Our solution gives rise to both\nefficient and effective cleaning of large-scale speaker recognition datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1\">Ruibin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hanzhi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yifan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yushi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Super-CLEVR: A Virtual Benchmark to Diagnose Domain Robustness in Visual Reasoning. (arXiv:2212.00259v1 [cs.CV])","link":"http://arxiv.org/abs/2212.00259","description":"<p>Visual Question Answering (VQA) models often perform poorly on\nout-of-distribution data and struggle on domain generalization. Due to the\nmulti-modal nature of this task, multiple factors of variation are intertwined,\nmaking generalization difficult to analyze. This motivates us to introduce a\nvirtual benchmark, Super-CLEVR, where different factors in VQA domain shifts\ncan be isolated in order that their effects can be studied independently. Four\nfactors are considered: visual complexity, question redundancy, concept\ndistribution and concept compositionality. With controllably generated data,\nSuper-CLEVR enables us to test VQA methods in situations where the test data\ndiffers from the training data along each of these axes. We study four existing\nmethods, including two neural symbolic methods NSCL and NSVQA, and two\nnon-symbolic methods FiLM and mDETR; and our proposed method, probabilistic\nNSVQA (P-NSVQA), which extends NSVQA with uncertainty reasoning. P-NSVQA\noutperforms other methods on three of the four domain shift factors. Our\nresults suggest that disentangling reasoning and perception, combined with\nprobabilistic uncertainty, form a strong VQA model that is more robust to\ndomain shifts. The dataset and code are released at\nhttps://github.com/Lizw14/Super-CLEVR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuowan Li</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingrui Wang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Stengel_Eskin_E/0/1/0/all/0/1\">Elias Stengel-Eskin</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1\">Adam Kortylewski</a> (3 and 4), <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wufei Ma</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a> (1) ((1) Johns Hopkins University, (2) University of Southern California, (3) Max Planck Institute for Informatics, (4) University of Freiburg)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PIZZA: A new benchmark for complex end-to-end task-oriented parsing. (arXiv:2212.00265v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00265","description":"<p>Much recent work in task-oriented parsing has focused on finding a middle\nground between flat slots and intents, which are inexpressive but easy to\nannotate, and powerful representations such as the lambda calculus, which are\nexpressive but costly to annotate. This paper continues the exploration of\ntask-oriented parsing by introducing a new dataset for parsing pizza and drink\norders, whose semantics cannot be captured by flat slots and intents. We\nperform an extensive evaluation of deep-learning techniques for task-oriented\nparsing on this dataset, including different flavors of seq2seq systems and\nRNNGs. The dataset comes in two main versions, one in a recently introduced\nutterance-level hierarchical notation that we call TOP, and one whose targets\nare executable representations (EXR). We demonstrate empirically that training\nthe parser to directly generate EXR notation not only solves the problem of\nentity resolution in one fell swoop and overcomes a number of expressive\nlimitations of TOP notation, but also results in significantly greater parsing\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arkoudas_K/0/1/0/all/0/1\">Konstantine Arkoudas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mesnards_N/0/1/0/all/0/1\">Nicolas Guenon des Mesnards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubino_M/0/1/0/all/0/1\">Melanie Rubino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swamy_S/0/1/0/all/0/1\">Sandesh Swamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanna_S/0/1/0/all/0/1\">Saarthak Khanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haidar_K/0/1/0/all/0/1\">Khan Haidar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localization vs. Semantics: How Can Language Benefit Visual Representation Learning?. (arXiv:2212.00281v1 [cs.CV])","link":"http://arxiv.org/abs/2212.00281","description":"<p>Despite the superior performance brought by vision-and-language pretraining,\nit remains unclear whether learning with multi-modal data can help understand\neach individual modality. In this work, we investigate how language can help\nwith visual representation learning from a probing perspective. Specifically,\nwe compare vision-and-language and vision-only models by probing their visual\nrepresentations on a broad range of tasks, in order to assess the quality of\nthe learned representations in a fine-grained manner. Interestingly, our\nprobing results suggest that vision-and-language models are better at label\nprediction tasks like object and attribute prediction, while vision-only models\nare stronger at dense prediction tasks that require more localized information.\nWith further analysis using detailed metrics, our study suggests that language\nhelps vision models learn better semantics, but not localization. Code is\nreleased at https://github.com/Lizw14/visual_probing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuowan Li</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cihang Xie</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a> (1) ((1) Johns Hopkins University, (2) University of California, Santa Cruz)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Commonsense-Infused Language-Agnostic Learning Framework for Enhancing Prediction of Political Polarity in Multilingual News Headlines. (arXiv:2212.00298v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00298","description":"<p>Predicting the political polarity of news headlines is a challenging task\nthat becomes even more challenging in a multilingual setting with low-resource\nlanguages. To deal with this, we propose to utilise the Inferential Commonsense\nKnowledge via a Translate-Retrieve-Translate strategy to introduce a learning\nframework. To begin with, we use the method of translation and retrieval to\nacquire the inferential knowledge in the target language. We then employ an\nattention mechanism to emphasise important inferences. We finally integrate the\nattended inferences into a multilingual pre-trained language model for the task\nof bias prediction. To evaluate the effectiveness of our framework, we present\na dataset of over 62.6K multilingual news headlines in five European languages\nannotated with their respective political polarities. We evaluate several\nstate-of-the-art multilingual pre-trained language models since their\nperformance tends to vary across languages (low/high resource). Evaluation\nresults demonstrate that our proposed framework is effective regardless of the\nmodels employed. Overall, the best performing model trained with only headlines\nshow 0.90 accuracy and F1, and 0.83 jaccard score. With attended knowledge in\nour framework, the same model show an increase in 2.2% accuracy and F1, and\n3.6% jaccard score. Extending our experiments to individual languages reveals\nthat the models we analyze for Slovenian perform significantly worse than other\nlanguages in our dataset. To investigate this, we assess the effect of\ntranslation quality on prediction performance. It indicates that the disparity\nin performance is most likely due to poor translation quality. We release our\ndataset and scripts at: https://github.com/Swati17293/KG-Multi-Bias for future\nresearch. Our framework has the potential to benefit journalists, social\nscientists, news producers, and consumers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Swati_S/0/1/0/all/0/1\">Swati Swati</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Grobelnik_A/0/1/0/all/0/1\">Adrian Mladeni&#x107; Grobelnik</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Mladenic_D/0/1/0/all/0/1\">Dunja Mladeni&#x107;</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Grobelnik_M/0/1/0/all/0/1\">Marko Grobelnik</a> (1) ((1) Jo&#x17e;ef Stefan Institute - Ljubljana, (2) Jo&#x17e;ef Stefan International Postgraduate School - Ljubljana)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Select from Multiple Options. (arXiv:2212.00301v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00301","description":"<p>Many NLP tasks can be regarded as a selection problem from a set of options,\nsuch as classification tasks, multi-choice question answering, etc. Textual\nentailment (TE) has been shown as the state-of-the-art (SOTA) approach to\ndealing with those selection problems. TE treats input texts as premises (P),\noptions as hypotheses (H), then handles the selection problem by modeling (P,\nH) pairwise. Two limitations: first, the pairwise modeling is unaware of other\noptions, which is less intuitive since humans often determine the best options\nby comparing competing candidates; second, the inference process of pairwise TE\nis time-consuming, especially when the option space is large. To deal with the\ntwo issues, this work first proposes a contextualized TE model (Context-TE) by\nappending other k options as the context of the current (P, H) modeling.\nContext-TE is able to learn more reliable decision for the H since it considers\nvarious context. Second, we speed up Context-TE by coming up with Parallel-TE,\nwhich learns the decisions of multiple options simultaneously. Parallel-TE\nsignificantly improves the inference speed while keeping comparable performance\nwith Context-TE. Our methods are evaluated on three tasks (ultra-fine entity\ntyping, intent detection and multi-choice QA) that are typical selection\nproblems with different sizes of options. Experiments show our models set new\nSOTA performance; particularly, Parallel-TE is faster than the pairwise TE by k\ntimes in inference. Our code is publicly available at\nhttps://github.com/jiangshdd/LearningToSelect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jiangshu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenpeng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Congying Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anger Breeds Controversy: Analyzing Controversy and Emotions on Reddit. (arXiv:2212.00339v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00339","description":"<p>Emotions play an important role in interpersonal interactions and social\nconflict, yet their function in the development of controversy and disagreement\nin online conversations has not been explored. To address this gap, we study\ncontroversy on Reddit, a popular network of online discussion forums. We\ncollect discussions from a wide variety of topical forums and use emotion\ndetection to recognize a range of emotions from text, including anger, fear,\njoy, admiration, etc. Our study has three main findings. First, controversial\ncomments express more anger and less admiration, joy and optimism than\nnon-controversial comments. Second, controversial comments affect emotions of\ndownstream comments in a discussion, usually resulting in long-term increase in\nanger and a decrease in positive emotions, although the magnitude and direction\nof emotional change depends on the forum. Finally, we show that emotions help\nbetter predict which comments will become controversial. Understanding\nemotional dynamics of online discussions can help communities to better manage\nconversations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zihao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_R/0/1/0/all/0/1\">Rong-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerman_K/0/1/0/all/0/1\">Kristina Lerman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Model Pre-training on True Negatives. (arXiv:2212.00460v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00460","description":"<p>Discriminative pre-trained language models (PLMs) learn to predict original\ntexts from intentionally corrupted ones. Taking the former text as positive and\nthe latter as negative samples, the PLM can be trained effectively for\ncontextualized representation. However, the training of such a type of PLMs\nhighly relies on the quality of the automatically constructed samples. Existing\nPLMs simply treat all corrupted texts as equal negative without any\nexamination, which actually lets the resulting model inevitably suffer from the\nfalse negative issue where training is carried out on pseudo-negative data and\nleads to less efficiency and less robustness in the resulting PLMs. In this\nwork, on the basis of defining the false negative issue in discriminative PLMs\nthat has been ignored for a long time, we design enhanced pre-training methods\nto counteract false negative predictions and encourage pre-training language\nmodels on true negatives by correcting the harmful gradient updates subject to\nfalse negative predictions. Experimental results on GLUE and SQuAD benchmarks\nshow that our counter-false-negative pre-training methods indeed bring about\nbetter performance together with stronger robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utiyama_M/0/1/0/all/0/1\">Masao Utiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumita_E/0/1/0/all/0/1\">Eiichiro Sumita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CUNI Non-Autoregressive System for the WMT 22 Efficient Translation Shared Task. (arXiv:2212.00477v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00477","description":"<p>We present a non-autoregressive system submission to the WMT 22 Efficient\nTranslation Shared Task. Our system was used by Helcl et al. (2022) in an\nattempt to provide fair comparison between non-autoregressive and\nautoregressive models. This submission is an effort to establish solid\nbaselines along with sound evaluation methodology, particularly in terms of\nmeasuring the decoding speed. The model itself is a 12-layer Transformer model\ntrained with connectionist temporal classification on knowledge-distilled\ndataset by a strong autoregressive teacher model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Helcl_J/0/1/0/all/0/1\">Jind&#x159;ich Helcl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IRRGN: An Implicit Relational Reasoning Graph Network for Multi-turn Response Selection. (arXiv:2212.00482v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00482","description":"<p>The task of response selection in multi-turn dialogue is to find the best\noption from all candidates. In order to improve the reasoning ability of the\nmodel, previous studies pay more attention to using explicit algorithms to\nmodel the dependencies between utterances, which are deterministic, limited and\ninflexible. In addition, few studies consider differences between the options\nbefore and after reasoning. In this paper, we propose an Implicit Relational\nReasoning Graph Network to address these issues, which consists of the\nUtterance Relational Reasoner (URR) and the Option Dual Comparator (ODC). URR\naims to implicitly extract dependencies between utterances, as well as\nutterances and options, and make reasoning with relational graph convolutional\nnetworks. ODC focuses on perceiving the difference between the options through\ndual comparison, which can eliminate the interference of the noise options.\nExperimental results on two multi-turn dialogue reasoning benchmark datasets\nMuTual and MuTual+ show that our method significantly improves the baseline of\nfour pretrained language models and achieves state-of-the-art performance. The\nmodel surpasses human performance for the first time on the MuTual dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jingcheng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hengwei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xuewei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_Y/0/1/0/all/0/1\">Yuanchen Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CUNI Systems for the WMT22 Czech-Ukrainian Translation Task. (arXiv:2212.00486v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00486","description":"<p>We present Charles University submissions to the WMT22 General Translation\nShared Task on Czech-Ukrainian and Ukrainian-Czech machine translation. We\npresent two constrained submissions based on block back-translation and tagged\nback-translation and experiment with rule-based romanization of Ukrainian. Our\nresults show that the romanization only has a minor effect on the translation\nquality. Further, we describe Charles Translator, a system that was developed\nin March 2022 as a response to the migration from Ukraine to the Czech\nRepublic. Compared to our constrained systems, it did not use the romanization\nand used some proprietary data sources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Popel_M/0/1/0/all/0/1\">Martin Popel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Libovicky_J/0/1/0/all/0/1\">Jind&#x159;ich Libovick&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helcl_J/0/1/0/all/0/1\">Jind&#x159;ich Helcl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMSpeech: Multi-modal Multi-task Encoder-Decoder Pre-training for Speech Recognition. (arXiv:2212.00500v1 [cs.MM])","link":"http://arxiv.org/abs/2212.00500","description":"<p>In this paper, we propose a novel multi-modal multi-task encoder-decoder\npre-training framework (MMSpeech) for Mandarin automatic speech recognition\n(ASR), which employs both unlabeled speech and text data. The main difficulty\nin speech-text joint pre-training comes from the significant difference between\nspeech and text modalities, especially for Mandarin speech and text. Unlike\nEnglish and other languages with an alphabetic writing system, Mandarin uses an\nideographic writing system where character and sound are not tightly mapped to\none another. Therefore, we propose to introduce the phoneme modality into\npre-training, which can help capture modality-invariant information between\nMandarin speech and text. Specifically, we employ a multi-task learning\nframework including five self-supervised and supervised tasks with speech and\ntext data. For end-to-end pre-training, we introduce self-supervised\nspeech-to-pseudo-codes (S2C) and phoneme-to-text (P2T) tasks utilizing\nunlabeled speech and text data, where speech-pseudo-codes pairs and\nphoneme-text pairs are a supplement to the supervised speech-text pairs. To\ntrain the encoder to learn better speech representation, we introduce\nself-supervised masked speech prediction (MSP) and supervised phoneme\nprediction (PP) tasks to learn to map speech into phonemes. Besides, we\ndirectly add the downstream supervised speech-to-text (S2T) task into the\npre-training process, which can further improve the pre-training performance\nand achieve better recognition results even without fine-tuning. Experiments on\nAISHELL-1 show that our proposed method achieves state-of-the-art performance,\nwith a more than 40% relative improvement compared with other pre-training\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaohuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zeyu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhijie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CultureBERT: Fine-Tuning Transformer-Based Language Models for Corporate Culture. (arXiv:2212.00509v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00509","description":"<p>This paper introduces supervised machine learning to the literature measuring\ncorporate culture from text documents. We compile a unique data set of employee\nreviews that were labeled by human evaluators with respect to the information\nthe reviews reveal about the firms' corporate culture. Using this data set, we\nfine-tune state-of-the-art transformer-based language models to perform the\nsame classification task. In out-of-sample predictions, our language models\nclassify 16 to 28 percent points more of employee reviews in line with human\nevaluators than traditional approaches of text classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koch_S/0/1/0/all/0/1\">Sebastian Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasch_S/0/1/0/all/0/1\">Stefan Pasch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Research on the application of contrastive learning in multi-label text classification. (arXiv:2212.00552v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00552","description":"<p>The effective application of contrastive learning technology in natural\nlanguage processing tasks shows the superiority of contrastive learning in text\nanalysis tasks. How to construct positive and negative samples correctly and\nreasonably is the core challenge of contrastive learning. Since it is difficult\nto construct contrastive objects in multi-label multi-classification tasks,\nthere are few contrastive losses for multi-label multi-classification text\nclassification. In this paper, we propose five contrastive losses for\nmulti-label multi-classification tasks. They are Strict Contrastive Loss (SCL),\nIntra-label Contrastive Loss (ICL), Jaccard Similarity Contrastive Loss (JSCL),\nand Jaccard Similarity Probability Contrastive Loss (JSPCL) and Stepwise Label\nContrastive Loss (SLCL). We explore the effectiveness of contrastive learning\nfor multi-label multi-classification tasks under different strategies, and\nprovide a set of baseline methods for contrastive learning techniques on\nmulti-label classification tasks. We also perform an interpretability analysis\nof our approach to show how different contrastive learning methods play their\nroles. The experimental results in this paper demonstrate that our proposed\ncontrastive losses can bring some improvement for multi-label\nmulti-classification tasks. Our work reveal how to \"appropriately\" change the\ncontrastive way of contrastive learning is the key idea to improve the\nadaptability of contrastive learning in multi-label multi-classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_G/0/1/0/all/0/1\">Guanqiu Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jigang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Aimin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dong Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long-Document Cross-Lingual Summarization. (arXiv:2212.00586v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00586","description":"<p>Cross-Lingual Summarization (CLS) aims at generating summaries in one\nlanguage for the given documents in another language. CLS has attracted wide\nresearch attention due to its practical significance in the multi-lingual\nworld. Though great contributions have been made, existing CLS works typically\nfocus on short documents, such as news articles, short dialogues and guides.\nDifferent from these short texts, long documents such as academic articles and\nbusiness reports usually discuss complicated subjects and consist of thousands\nof words, making them non-trivial to process and summarize. To promote CLS\nresearch on long documents, we construct Perseus, the first long-document CLS\ndataset which collects about 94K Chinese scientific documents paired with\nEnglish summaries. The average length of documents in Perseus is more than two\nthousand tokens. As a preliminary study on long-document CLS, we build and\nevaluate various CLS baselines, including pipeline and end-to-end methods.\nExperimental results on Perseus show the superiority of the end-to-end\nbaseline, outperforming the strong pipeline models equipped with sophisticated\nmachine translation systems. Furthermore, to provide a deeper understanding, we\nmanually analyze the model outputs and discuss specific challenges faced by\ncurrent approaches. We hope that our work could benchmark long-document CLS and\nbenefit future studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shaohui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1\">Jianfeng Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">An Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embedding generation for text classification of Brazilian Portuguese user reviews: from bag-of-words to transformers. (arXiv:2212.00587v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00587","description":"<p>Text classification is a natural language processing (NLP) task relevant to\nmany commercial applications, like e-commerce and customer service. Naturally,\nclassifying such excerpts accurately often represents a challenge, due to\nintrinsic language aspects, like irony and nuance. To accomplish this task, one\nmust provide a robust numerical representation for documents, a process known\nas embedding. Embedding represents a key NLP field nowadays, having faced a\nsignificant advance in the last decade, especially after the introduction of\nthe word-to-vector concept and the popularization of Deep Learning models for\nsolving NLP tasks, including Convolutional Neural Networks (CNNs), Recurrent\nNeural Networks (RNNs), and Transformer-based Language Models (TLMs). Despite\nthe impressive achievements in this field, the literature coverage regarding\ngenerating embeddings for Brazilian Portuguese texts is scarce, especially when\nconsidering commercial user reviews. Therefore, this work aims to provide a\ncomprehensive experimental study of embedding approaches targeting a binary\nsentiment classification of user reviews in Brazilian Portuguese. This study\nincludes from classical (Bag-of-Words) to state-of-the-art (Transformer-based)\nNLP models. The methods are evaluated with five open-source databases with\npre-defined data partitions made available in an open digital repository to\nencourage reproducibility. The Fine-tuned TLMs achieved the best results for\nall cases, being followed by the Feature-based TLM, LSTM, and CNN, with\nalternate ranks, depending on the database under analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Souza_F/0/1/0/all/0/1\">Frederico Dias Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filho_J/0/1/0/all/0/1\">Jo&#xe3;o Baptista de Oliveira e Souza Filho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language models and brain alignment: beyond word-level semantics and prediction. (arXiv:2212.00596v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00596","description":"<p>Pretrained language models that have been trained to predict the next word\nover billions of text documents have been shown to also significantly predict\nbrain recordings of people comprehending language. Understanding the reasons\nbehind the observed similarities between language in machines and language in\nthe brain can lead to more insight into both systems. Recent works suggest that\nthe prediction of the next word is a key mechanism that contributes to the\nalignment between the two. What is not yet understood is whether prediction of\nthe next word is necessary for this observed alignment or simply sufficient,\nand whether there are other shared mechanisms or information that is similarly\nimportant. In this work, we take a first step towards a better understanding\nvia two simple perturbations in a popular pretrained language model. The first\nperturbation is to improve the model's ability to predict the next word in the\nspecific naturalistic stimulus text that the brain recordings correspond to. We\nshow that this indeed improves the alignment with the brain recordings.\nHowever, this improved alignment may also be due to any improved word-level or\nmulti-word level semantics for the specific world that is described by the\nstimulus narrative. We aim to disentangle the contribution of next word\nprediction and semantic knowledge via our second perturbation: scrambling the\nword order at inference time, which reduces the ability to predict the next\nword, but maintains any newly learned word-level semantics. By comparing the\nalignment with brain recordings of these differently perturbed models, we show\nthat improvements in alignment with brain recordings are due to more than\nimprovements in next word prediction and word-level semantics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Merlin_G/0/1/0/all/0/1\">Gabriele Merlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toneva_M/0/1/0/all/0/1\">Mariya Toneva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extensible Prompts for Language Models. (arXiv:2212.00616v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00616","description":"<p>We propose eXtensible Prompt (X-Prompt) for prompting a large language model\n(LLM) beyond natural language (NL). X-Prompt instructs an LLM with not only NL\nbut also an extensible vocabulary of imaginary words that are introduced to\nhelp represent what NL words hardly describe, allowing a prompt to be more\ndescriptive. Like NL prompts, X-Prompt is out-of-distribution (OOD) robust, for\nwhich we propose context-guided learning with prompt augmentation to learn its\nimaginary words for general usability, enabling them to use in different prompt\ncontexts for fine-grain specifications. The promising results of X-Prompt\ndemonstrate its potential of approaching advanced interaction between humans\nand LLMs to bridge their communication gap.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shaoguang Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si-Qing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing the State of Computer Science Research with the DBLP Discovery Dataset. (arXiv:2212.00629v1 [cs.DL])","link":"http://arxiv.org/abs/2212.00629","description":"<p>The number of scientific publications continues to rise exponentially,\nespecially in Computer Science (CS). However, current solutions to analyze\nthose publications restrict access behind a paywall, offer no features for\nvisual analysis, limit access to their data, only focus on niches or\nsub-fields, and/or are not flexible and modular enough to be transferred to\nother datasets. In this thesis, we conduct a scientometric analysis to uncover\nthe implicit patterns hidden in CS metadata and to determine the state of CS\nresearch. Specifically, we investigate trends of the quantity, impact, and\ntopics for authors, venues, document types (conferences vs. journals), and\nfields of study (compared to, e.g., medicine). To achieve this we introduce the\nCS-Insights system, an interactive web application to analyze CS publications\nwith various dashboards, filters, and visualizations. The data underlying this\nsystem is the DBLP Discovery Dataset (D3), which contains metadata from 5\nmillion CS publications. Both D3 and CS-Insights are open-access, and\nCS-Insights can be easily adapted to other datasets in the future. The most\ninteresting findings of our scientometric analysis include that i) there has\nbeen a stark increase in publications, authors, and venues in the last two\ndecades, ii) many authors only recently joined the field, iii) the most cited\nauthors and venues focus on computer vision and pattern recognition, while the\nmost productive prefer engineering-related topics, iv) the preference of\nresearchers to publish in conferences over journals dwindles, v) on average,\njournal articles receive twice as many citations compared to conference papers,\nbut the contrast is much smaller for the most cited conferences and journals,\nand vi) journals also get more citations in all other investigated fields of\nstudy, while only CS and engineering publish more in conferences than journals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kull_L/0/1/0/all/0/1\">Lennart K&#xfc;ll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapted Multimodal BERT with Layer-wise Fusion for Sentiment Analysis. (arXiv:2212.00678v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00678","description":"<p>Multimodal learning pipelines have benefited from the success of pretrained\nlanguage models. However, this comes at the cost of increased model parameters.\nIn this work, we propose Adapted Multimodal BERT (AMB), a BERT-based\narchitecture for multimodal tasks that uses a combination of adapter modules\nand intermediate fusion layers. The adapter adjusts the pretrained language\nmodel for the task at hand, while the fusion layers perform task-specific,\nlayer-wise fusion of audio-visual information with textual BERT\nrepresentations. During the adaptation process the pre-trained language model\nparameters remain frozen, allowing for fast, parameter-efficient training. In\nour ablations we see that this approach leads to efficient models, that can\noutperform their fine-tuned counterparts and are robust to input noise. Our\nexperiments on sentiment analysis with CMU-MOSEI show that AMB outperforms the\ncurrent state-of-the-art across metrics, with 3.4% relative reduction in the\nresulting error and 2.1% relative improvement in 7-class classification\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chlapanis_O/0/1/0/all/0/1\">Odysseas S. Chlapanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paraskevopoulos_G/0/1/0/all/0/1\">Georgios Paraskevopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potamianos_A/0/1/0/all/0/1\">Alexandros Potamianos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CliMedBERT: A Pre-trained Language Model for Climate and Health-related Text. (arXiv:2212.00689v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00689","description":"<p>Climate change is threatening human health in unprecedented orders and many\nways. These threats are expected to grow unless effective and evidence-based\npolicies are developed and acted upon to minimize or eliminate them. Attaining\nsuch a task requires the highest degree of the flow of knowledge from science\ninto policy. The multidisciplinary, location-specific, and vastness of\npublished science makes it challenging to keep track of novel work in this\narea, as well as making the traditional knowledge synthesis methods inefficient\nin infusing science into policy. To this end, we consider developing multiple\ndomain-specific language models (LMs) with different variations from Climate-\nand Health-related information, which can serve as a foundational step toward\ncapturing available knowledge to enable solving different tasks, such as\ndetecting similarities between climate- and health-related concepts,\nfact-checking, relation extraction, evidence of health effects to policy text\ngeneration, and more. To our knowledge, this is the first work that proposes\ndeveloping multiple domain-specific language models for the considered domains.\nWe will make the developed models, resources, and codebase available for the\nresearchers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fard_B/0/1/0/all/0/1\">B. Jalalzadeh Fard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1\">S. A. Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bell_J/0/1/0/all/0/1\">J. E. Bell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What do you MEME? Generating Explanations for Visual Semantic Role Labelling in Memes. (arXiv:2212.00715v1 [cs.CY])","link":"http://arxiv.org/abs/2212.00715","description":"<p>Memes are powerful means for effective communication on social media. Their\neffortless amalgamation of viral visuals and compelling messages can have\nfar-reaching implications with proper marketing. Previous research on memes has\nprimarily focused on characterizing their affective spectrum and detecting\nwhether the meme's message insinuates any intended harm, such as hate, offense,\nracism, etc. However, memes often use abstraction, which can be elusive. Here,\nwe introduce a novel task - EXCLAIM, generating explanations for visual\nsemantic role labeling in memes. To this end, we curate ExHVV, a novel dataset\nthat offers natural language explanations of connotative roles for three types\nof entities - heroes, villains, and victims, encompassing 4,680 entities\npresent in 3K memes. We also benchmark ExHVV with several strong unimodal and\nmultimodal baselines. Moreover, we posit LUMEN, a novel multimodal, multi-task\nlearning framework that endeavors to address EXCLAIM optimally by jointly\nlearning to predict the correct semantic roles and correspondingly to generate\nsuitable natural language explanations. LUMEN distinctly outperforms the best\nbaseline across 18 standard natural language generation evaluation metrics. Our\nsystematic evaluation and analyses demonstrate that characteristic multimodal\ncues required for adjudicating semantic roles are also helpful for generating\nsuitable explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shivam Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Siddhant Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_T/0/1/0/all/0/1\">Tharun Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md. Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charkraborty_T/0/1/0/all/0/1\">Tanmoy Charkraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving astroBERT using Semantic Textual Similarity. (arXiv:2212.00744v1 [cs.CL])","link":"http://arxiv.org/abs/2212.00744","description":"<p>The NASA Astrophysics Data System (ADS) is an essential tool for researchers\nthat allows them to explore the astronomy and astrophysics scientific\nliterature, but it has yet to exploit recent advances in natural language\nprocessing. At ADASS 2021, we introduced astroBERT, a machine learning language\nmodel tailored to the text used in astronomy papers in ADS. In this work we:\n</p>\n<p>- announce the first public release of the astroBERT language model;\n</p>\n<p>- show how astroBERT improves over existing public language models on\nastrophysics specific tasks;\n</p>\n<p>- and detail how ADS plans to harness the unique structure of scientific\npapers, the citation graph and citation context, to further improve astroBERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grezes_F/0/1/0/all/0/1\">Felix Grezes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allen_T/0/1/0/all/0/1\">Thomas Allen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanco_Cuaresma_S/0/1/0/all/0/1\">Sergi Blanco-Cuaresma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Accomazzi_A/0/1/0/all/0/1\">Alberto Accomazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurtz_M/0/1/0/all/0/1\">Michael J. Kurtz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shapurian_G/0/1/0/all/0/1\">Golnaz Shapurian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henneken_E/0/1/0/all/0/1\">Edwin Henneken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grant_C/0/1/0/all/0/1\">Carolyn S. Grant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_D/0/1/0/all/0/1\">Donna M. Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hostetler_T/0/1/0/all/0/1\">Timothy W. Hostetler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Templeton_M/0/1/0/all/0/1\">Matthew R. Templeton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lockhart_K/0/1/0/all/0/1\">Kelly E. Lockhart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shinyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_J/0/1/0/all/0/1\">Jennifer Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacovich_T/0/1/0/all/0/1\">Taylor Jacovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Protopapas_P/0/1/0/all/0/1\">Pavlos Protopapas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simplifying and Understanding State Space Models with Diagonal Linear RNNs. (arXiv:2212.00768v1 [cs.LG])","link":"http://arxiv.org/abs/2212.00768","description":"<p>Sequence models based on linear state spaces (SSMs) have recently emerged as\na promising choice of architecture for modeling long range dependencies across\nvarious modalities. However, they invariably rely on discretization of a\ncontinuous state space, which complicates their presentation and understanding.\nIn this work, we dispose of the discretization step, and propose a model based\non vanilla Diagonal Linear RNNs ($\\mathrm{DLR}$). We empirically show that\n$\\mathrm{DLR}$ is as performant as previously-proposed SSMs in the presence of\nstrong supervision, despite being conceptually much simpler. Moreover, we\ncharacterize the expressivity of SSMs (including $\\mathrm{DLR}$) and\nattention-based models via a suite of $13$ synthetic sequence-to-sequence tasks\ninvolving interactions over tens of thousands of tokens, ranging from simple\noperations, such as shifting an input sequence, to detecting co-dependent\nvisual features over long spatial ranges in flattened images. We find that\nwhile SSMs report near-perfect performance on tasks that can be modeled via\n$\\textit{few}$ convolutional kernels, they struggle on tasks requiring\n$\\textit{many}$ such kernels and especially when the desired sequence\nmanipulation is $\\textit{context-dependent}$. For example, $\\mathrm{DLR}$\nlearns to perfectly shift a $0.5M$-long input by an arbitrary number of\npositions but fails when the shift size depends on context. Despite these\nlimitations, $\\mathrm{DLR}$ reaches high performance on two higher-order\nreasoning tasks $\\mathrm{ListOpsSubTrees}$ and\n$\\mathrm{PathfinderSegmentation}\\text{-}\\mathrm{256}$ with input lengths $8K$\nand $65K$ respectively, and gives encouraging performance on\n$\\mathrm{PathfinderSegmentation}\\text{-}\\mathrm{512}$ with input length $262K$\nfor which attention is not a viable choice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ankit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_H/0/1/0/all/0/1\">Harsh Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ColBERT: Using BERT Sentence Embedding in Parallel Neural Networks for Computational Humor. (arXiv:2004.12765v7 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.12765","description":"<p>Automation of humor detection and rating has interesting use cases in modern\ntechnologies, such as humanoid robots, chatbots, and virtual assistants. In\nthis paper, we propose a novel approach for detecting and rating humor in short\ntexts based on a popular linguistic theory of humor. The proposed technical\nmethod initiates by separating sentences of the given text and utilizing the\nBERT model to generate embeddings for each one. The embeddings are fed to\nseparate lines of hidden layers in a neural network (one line for each\nsentence) to extract latent features. At last, the parallel lines are\nconcatenated to determine the congruity and other relationships between the\nsentences and predict the target value. We accompany the paper with a novel\ndataset for humor detection consisting of 200,000 formal short texts. In\naddition to evaluating our work on the novel dataset, we participated in a live\nmachine learning competition focused on rating humor in Spanish tweets. The\nproposed model obtained F1 scores of 0.982 and 0.869 in the humor detection\nexperiments which outperform general and state-of-the-art models. The\nevaluation performed on two contrasting settings confirm the strength and\nrobustness of the model and suggests two important factors in achieving high\naccuracy in the current task: 1) usage of sentence embeddings and 2) utilizing\nthe linguistic structure of humor in designing the proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Annamoradnejad_I/0/1/0/all/0/1\">Issa Annamoradnejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoghi_G/0/1/0/all/0/1\">Gohar Zoghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts. (arXiv:2205.11961v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11961","description":"<p>This work introduces a new multi-task, parameter-efficient language model\n(LM) tuning method that learns to transfer knowledge across different tasks via\na mixture of soft prompts-small prefix embedding vectors pre-trained for\ndifferent tasks. Our method, called ATTEMPT (ATTEntional Mixtures of Prompt\nTuning), obtains source prompts as encodings of large-scale source tasks into a\nsmall number of parameters and trains an attention module to interpolate the\nsource prompts and a newly initialized target prompt for every instance in the\ntarget task. During training, only the target task prompt and the attention\nweights, which are shared between tasks in multi-task training, are updated,\nwhile the original LM and source prompts are intact. ATTEMPT is highly\nparameter-efficient (e.g., updates 2,300 times fewer parameters than full\nfine-tuning) while achieving high task performance using knowledge from\nhigh-resource tasks. Moreover, it is modular using pre-trained soft prompts,\nand can flexibly add or remove source prompts for effective knowledge transfer.\nOur experimental results across 21 diverse NLP datasets show that ATTEMPT\nsignificantly outperforms prompt tuning and outperforms or matches fully\nfine-tuned or other parameter-efficient tuning approaches that use over ten\ntimes more parameters. Finally, ATTEMPT outperforms previous work in few-shot\nlearning settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1\">Akari Asai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1\">Mohammadreza Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1\">Matthew E. Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Approximate Conditional Coverage & Calibration via Neural Model Approximations. (arXiv:2205.14310v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.14310","description":"<p>A typical desideratum for quantifying the uncertainty from a classification\nmodel as a prediction set is class-conditional singleton set calibration. That\nis, such sets should map to the output of well-calibrated selective\nclassifiers, matching the observed frequencies of similar instances. Recent\nworks proposing adaptive and localized conformal p-values for deep networks do\nnot guarantee this behavior, nor do they achieve it empirically. Instead, we\nuse the strong signals for prediction reliability from KNN-based approximations\nof Transformer networks to construct data-driven partitions for Mondrian\nConformal Predictors, which are treated as weak selective classifiers that are\nthen calibrated via a new Inductive Venn Predictor, the Venn-ADMIT Predictor.\nThe resulting selective classifiers are well-calibrated, in a conservative but\npractically useful sense for a given threshold. They are inherently robust to\nchanges in the proportions of the data partitions, and straightforward\nconservative heuristics provide additional robustness to covariate shifts. We\ncompare and contrast to the quantities produced by recent Conformal Predictors\non several representative and challenging natural language processing\nclassification tasks, including class-imbalanced and distribution-shifted\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmaltz_A/0/1/0/all/0/1\">Allen Schmaltz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasooly_D/0/1/0/all/0/1\">Danielle Rasooly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Mandarin Speech Recogntion with Block-augmented Transformer. (arXiv:2207.11697v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.11697","description":"<p>Recently Convolution-augmented Transformer (Conformer) has shown promising\nresults in Automatic Speech Recognition (ASR), outperforming the previous best\npublished Transformer Transducer. In this work, we believe that the output\ninformation of each block in the encoder and decoder is not completely\ninclusive, in other words, their output information may be complementary. We\nstudy how to take advantage of the complementary information of each block in a\nparameter-efficient way, and it is expected that this may lead to more robust\nperformance. Therefore we propose the Block-augmented Transformer for speech\nrecognition, named Blockformer. We have implemented two block ensemble methods:\nthe base Weighted Sum of the Blocks Output (Base-WSBO), and the\nSqueeze-and-Excitation module to Weighted Sum of the Blocks Output (SE-WSBO).\nExperiments have proved that the Blockformer significantly outperforms the\nstate-of-the-art Conformer-based models on AISHELL-1, our model achieves a CER\nof 4.29\\% without using a language model and 4.05\\% with an external language\nmodel on the testset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaoming Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Huifeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Liuwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minghui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1\">Jie Hao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Encoder-Decoder Framework with Entity Memory. (arXiv:2210.03273v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03273","description":"<p>Entities, as important carriers of real-world knowledge, play a key role in\nmany NLP tasks. We focus on incorporating entity knowledge into an\nencoder-decoder framework for informative text generation. Existing approaches\ntried to index, retrieve, and read external documents as evidence, but they\nsuffered from a large computational overhead. In this work, we propose an\nencoder-decoder framework with an entity memory, namely EDMem. The entity\nknowledge is stored in the memory as latent representations, and the memory is\npre-trained on Wikipedia along with encoder-decoder parameters. To precisely\ngenerate entity names, we design three decoding methods to constrain entity\ngeneration by linking entities in the memory. EDMem is a unified framework that\ncan be used on various entity-intensive question answering and generation\ntasks. Extensive experimental results show that EDMem outperforms both\nmemory-based auto-encoder models and non-memory encoder-decoder models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Textual Entailment Recognition with Semantic Features from Empirical Text Representation. (arXiv:2210.09723v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.09723","description":"<p>Textual entailment recognition is one of the basic natural language\nunderstanding(NLU) tasks. Understanding the meaning of sentences is a\nprerequisite before applying any natural language processing(NLP) techniques to\nautomatically recognize the textual entailment. A text entails a hypothesis if\nand only if the true value of the hypothesis follows the text. Classical\napproaches generally utilize the feature value of each word from word embedding\nto represent the sentences. In this paper, we propose a novel approach to\nidentifying the textual entailment relationship between text and hypothesis,\nthereby introducing a new semantic feature focusing on empirical\nthreshold-based semantic text representation. We employ an element-wise\nManhattan distance vector-based feature that can identify the semantic\nentailment relationship between the text-hypothesis pair. We carried out\nseveral experiments on a benchmark entailment classification(SICK-RTE) dataset.\nWe train several machine learning(ML) algorithms applying both semantic and\nlexical features to classify the text-hypothesis pair as entailment, neutral,\nor contradiction. Our empirical sentence representation technique enriches the\nsemantic information of the texts and hypotheses found to be more efficient\nthan the classical ones. In the end, our approach significantly outperforms\nknown methods in understanding the meaning of the sentences for the textual\nentailment classification task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Atabuzzaman_M/0/1/0/all/0/1\">Md Atabuzzaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shajalal_M/0/1/0/all/0/1\">Md Shajalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baby_M/0/1/0/all/0/1\">Maksuda Bilkis Baby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Md Rezaul Karim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Knowledge Graph Construction: A Review. (arXiv:2210.12714v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12714","description":"<p>Generative Knowledge Graph Construction (KGC) refers to those methods that\nleverage the sequence-to-sequence framework for building knowledge graphs,\nwhich is flexible and can be adapted to widespread tasks. In this study, we\nsummarize the recent compelling progress in generative knowledge graph\nconstruction. We present the advantages and weaknesses of each paradigm in\nterms of different generation targets and provide theoretical insight and\nempirical analysis. Based on the review, we suggest promising research\ndirections for the future. Our contributions are threefold: (1) We present a\ndetailed, complete taxonomy for the generative KGC methods; (2) We provide a\ntheoretical and empirical analysis of the generative KGC methods; (3) We\npropose several research directions that can be developed in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tail Batch Sampling: Approximating Global Contrastive Losses as Optimization over Batch Assignments. (arXiv:2210.12874v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.12874","description":"<p>Contrastive Learning has recently achieved state-of-the-art performance in a\nwide range of tasks. Many contrastive learning approaches use mined hard\nnegatives to make batches more informative during training but these approaches\nare inefficient as they increase epoch length proportional to the number of\nmined negatives and require frequent updates of nearest neighbor indices or\nmining from recent batches. In this work, we provide an alternative to hard\nnegative mining in supervised contrastive learning, Tail Batch Sampling (TBS),\nan efficient approximation to the batch assignment problem that upper bounds\nthe gap between the global and training losses, $\\mathcal{L}^{Global} -\n\\mathcal{L}^{Train}$. TBS \\textbf{improves state-of-the-art performance} in\nsentence embedding (+0.37 Spearman) and code-search tasks (+2.2\\% MRR), is easy\nto implement - requiring only a few additional lines of code, does not maintain\nexternal data structures such as nearest neighbor indices, is more\ncomputationally efficient when compared to the most minimal hard negative\nmining approaches, and makes no changes to the model being trained.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sachidananda_V/0/1/0/all/0/1\">Vin Sachidananda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Maknuune: A Large Open Palestinian Arabic Lexicon. (arXiv:2210.12985v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12985","description":"<p>We present Maknuune, a large open lexicon for the Palestinian Arabic dialect.\nMaknuune has over 36K entries from 17K lemmas, and 3.7K roots. All entries\ninclude diacritized Arabic orthography, phonological transcription and English\nglosses. Some entries are enriched with additional information such as broken\nplurals and templatic feminine forms, associated phrases and collocations,\nStandard Arabic glosses, and examples or notes on grammar, usage, or location\nof collected entry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dibas_S/0/1/0/all/0/1\">Shahd Dibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khairallah_C/0/1/0/all/0/1\">Christian Khairallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadi_O/0/1/0/all/0/1\">Omar Fayez Sadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sairafy_T/0/1/0/all/0/1\">Tariq Sairafy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarabta_K/0/1/0/all/0/1\">Karmel Sarabta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardah_A/0/1/0/all/0/1\">Abrar Ardah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causality Detection using Multiple Annotation Decisions. (arXiv:2210.14852v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.14852","description":"<p>The paper describes the work that has been submitted to the 5th workshop on\nChallenges and Applications of Automated Extraction of socio-political events\nfrom text (CASE 2022). The work is associated with Subtask 1 of Shared Task 3\nthat aims to detect causality in protest news corpus. The authors used\ndifferent large language models with customized cross-entropy loss functions\nthat exploit annotation information. The experiments showed that\nbert-based-uncased with refined cross-entropy outperformed the others,\nachieving a F1 score of 0.8501 on the Causal News Corpus dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1\">Quynh Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1\">Arka Mitra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I Can't Believe There's No Images! Learning Visual Tasks Using only Language Data. (arXiv:2211.09778v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.09778","description":"<p>Many high-level skills that are required for computer vision tasks, such as\nparsing questions, comparing and contrasting semantics, and writing\ndescriptions, are also required in other domains such as natural language\nprocessing. In this paper, we ask whether this makes it possible to learn those\nskills from text data and then use them to complete vision tasks without ever\ntraining on visual training data. Key to our approach is exploiting the joint\nembedding space of contrastively trained vision and language encoders. In\npractice, there can be systematic differences between embedding spaces for\ndifferent modalities in contrastive models, and we analyze how these\ndifferences affect our approach and study a variety of strategies to mitigate\nthis concern. We produce models using only text training data on three tasks:\nimage captioning, visual entailment and visual question answering, and evaluate\nthem on standard benchmarks using images. We find that this kind of transfer is\npossible and results in only a small drop in performance relative to models\ntrained on images. We also showcase a variety of stylistic image captioning\nmodels that were trained using no image data and no human-curated language\ndata, but instead text data from books, the web, or language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Sophia Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_C/0/1/0/all/0/1\">Christopher Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1\">Aniruddha Kembhavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alignment-Enriched Tuning for Patch-Level Pre-trained Document Image Models. (arXiv:2211.14777v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.14777","description":"<p>Alignment between image and text has shown promising improvements on\npatch-level pre-trained document image models. However, investigating more\neffective or finer-grained alignment techniques during pre-training requires a\nlarge amount of computation cost and time. Thus, a question naturally arises:\nCould we fine-tune the pre-trained models adaptive to downstream tasks with\nalignment objectives and achieve comparable or better performance? In this\npaper, we propose a new model architecture with alignment-enriched tuning\n(dubbed AETNet) upon pre-trained document image models, to adapt downstream\ntasks with the joint task-specific supervised and alignment-aware contrastive\nobjective. Specifically, we introduce an extra visual transformer as the\nalignment-ware image encoder and an extra text transformer as the\nalignment-ware text encoder before multimodal fusion. We consider alignment in\nthe following three aspects: 1) document-level alignment by leveraging the\ncross-modal and intra-modal contrastive loss; 2) global-local alignment for\nmodeling localized and structural information in document images; and 3)\nlocal-level alignment for more accurate patch-level information. Experiments on\nvarious downstream tasks show that AETNet can achieve state-of-the-art\nperformance on various downstream tasks. Notably, AETNet consistently\noutperforms state-of-the-art pre-trained models, such as LayoutLMv3 with\nfine-tuning techniques, on three different downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiabang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detect-Localize-Repair: A Unified Framework for Learning to Debug with CodeT5. (arXiv:2211.14875v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2211.14875","description":"<p>Automated software debugging is a crucial task for improving the productivity\nof software developers. Many neural-based techniques have been proven effective\nfor debugging-related tasks such as bug localization and program repair (or bug\nfixing). However, these techniques often focus only on either one of them or\napproach them in a stage-wise manner, ignoring the mutual benefits between\nthem. In this work, we propose a novel unified \\emph{Detect-Localize-Repair}\nframework based on a pretrained programming language model CodeT5 to seamlessly\naddress these tasks, named CodeT5-DLR. Specifically, we propose three\nobjectives to adapt the generic CodeT5 for debugging: a bug detection objective\nto determine whether a given code snippet is buggy or not, a bug localization\nobjective to identify the buggy lines, and a program repair objective to\ntranslate the buggy code to its fixed version. We evaluate it on each of these\ntasks and their combined setting on two newly collected line-level debugging\ndatasets in Java and Python. Extensive results show that our model\nsignificantly outperforms existing baselines from both NLP and software\nengineering domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bui_N/0/1/0/all/0/1\">Nghi D. Q. Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EURO: ESPnet Unsupervised ASR Open-source Toolkit. (arXiv:2211.17196v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.17196","description":"<p>This paper describes the ESPnet Unsupervised ASR Open-source Toolkit (EURO),\nan end-to-end open-source toolkit for unsupervised automatic speech recognition\n(UASR). EURO adopts the state-of-the-art UASR learning method introduced by the\nWav2vec-U, originally implemented at FAIRSEQ, which leverages self-supervised\nspeech representations and adversarial training. In addition to wav2vec2, EURO\nextends the functionality and promotes reproducibility for UASR tasks by\nintegrating S3PRL and k2, resulting in flexible frontends from 27\nself-supervised models and various graph-based decoding strategies. EURO is\nimplemented in ESPnet and follows its unified pipeline to provide UASR recipes\nwith a complete setup. This improves the pipeline's efficiency and allows EURO\nto be easily applied to existing datasets in ESPnet. Extensive experiments on\nthree mainstream self-supervised models demonstrate the toolkit's effectiveness\nand achieve state-of-the-art UASR performance on TIMIT and LibriSpeech\ndatasets. EURO will be publicly available at https://github.com/espnet/espnet,\naiming to promote this exciting and emerging research area based on UASR\nthrough open-source activity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Dongji Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_S/0/1/0/all/0/1\">Shun-Po Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_L/0/1/0/all/0/1\">Leibny Paola Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khudanpur_S/0/1/0/all/0/1\">Sanjeev Khudanpur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models. (arXiv:2211.15029v2 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2211.15029","description":"<p>We present DiffusionBERT, a new generative masked language model based on\ndiscrete diffusion models. Diffusion models and many pre-trained language\nmodels have a shared training objective, i.e., denoising, making it possible to\ncombine the two powerful models and enjoy the best of both worlds. On the one\nhand, diffusion models offer a promising training strategy that helps improve\nthe generation quality. On the other hand, pre-trained denoising language\nmodels (e.g., BERT) can be used as a good initialization that accelerates\nconvergence. We explore training BERT to learn the reverse process of a\ndiscrete diffusion process with an absorbing state and elucidate several\ndesigns to improve it. First, we propose a new noise schedule for the forward\ndiffusion process that controls the degree of noise added at each step based on\nthe information of each token. Second, we investigate several designs of\nincorporating the time step into BERT. Experiments on unconditional text\ngeneration demonstrate that DiffusionBERT achieves significant improvement over\nexisting diffusion models for text (e.g., D3PM and Diffusion-LM) and previous\ngenerative masked language models in terms of perplexity and BLEU score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhengfu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kuanning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-12-01T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/"}}]}]}