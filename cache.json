{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-02-13T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Binarized Neural Machine Translation. (arXiv:2302.04907v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04907","description":"<p>The rapid scaling of language models is motivating research using\nlow-bitwidth quantization. In this work, we propose a novel binarization\ntechnique for Transformers applied to machine translation (BMT), the first of\nits kind. We identify and address the problem of inflated dot-product variance\nwhen using one-bit weights and activations. Specifically, BMT leverages\nadditional LayerNorms and residual connections to improve binarization quality.\nExperiments on the WMT dataset show that a one-bit weight-only Transformer can\nachieve the same quality as a float one, while being 16x smaller in size.\nOne-bit activations incur varying degrees of quality drop, but mitigated by the\nproposed architectural changes. We further conduct a scaling law study using\nproduction-scale translation datasets, which shows that one-bit weight\nTransformers scale and generalize well in both in-domain and out-of-domain\nsettings. Implementation in JAX/Flax will be open sourced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Ankush Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lew_L/0/1/0/all/0/1\">&#x141;ukasz Lew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghorbani_B/0/1/0/all/0/1\">Behrooz Ghorbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flexible, Model-Agnostic Method for Materials Data Extraction from Text Using General Purpose Language Models. (arXiv:2302.04914v1 [cond-mat.mtrl-sci])","link":"http://arxiv.org/abs/2302.04914","description":"<p>Accurate and comprehensive material databases extracted from research papers\nare critical for materials science and engineering but require significant\nhuman effort to develop. In this paper we present a simple method of extracting\nmaterials data from full texts of research papers suitable for quickly\ndeveloping modest-sized databases. The method requires minimal to no coding,\nprior knowledge about the extracted property, or model training, and provides\nhigh recall and almost perfect precision in the resultant database. The method\nis fully automated except for one human-assisted step, which typically requires\njust a few hours of human labor. The method builds on top of natural language\nprocessing and large general language models but can work with almost any such\nmodel. The language models GPT-3/3.5, bart and DeBERTaV3 are evaluated here for\ncomparison. We provide a detailed detailed analysis of the methods performance\nin extracting bulk modulus data, obtaining up to 90% precision at 96% recall,\ndepending on the amount of human effort involved. We then demonstrate the\nmethods broader effectiveness by developing a database of critical cooling\nrates for metallic glasses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cond-mat/1/au:+Polak_M/0/1/0/all/0/1\">Maciej P. Polak</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Modi_S/0/1/0/all/0/1\">Shrey Modi</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Latosinska_A/0/1/0/all/0/1\">Anna Latosinska</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Zhang_J/0/1/0/all/0/1\">Jinming Zhang</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Wang_C/0/1/0/all/0/1\">Ching-Wen Wang</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Wang_S/0/1/0/all/0/1\">Shanonan Wang</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Hazra_A/0/1/0/all/0/1\">Ayan Deep Hazra</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Morgan_D/0/1/0/all/0/1\">Dane Morgan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-Context Learning with Many Demonstration Examples. (arXiv:2302.04931v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04931","description":"<p>Large pre-training language models (PLMs) have shown promising in-context\nlearning abilities. However, due to the backbone transformer architecture,\nexisting PLMs are bottlenecked by the memory and computational cost when\nscaling up to a large context size, leaving instruction tuning and in-context\nlearning of many demonstration examples, as well as long-range language\nmodeling under-explored. In this study, we propose a long-range language model\nEVALM based on an efficient transformer mechanism. EVALM is trained with 8k\ntokens per batch line and can test up to 256k-lengthed contexts with\nextrapolation, 128 times to the limit of existing PLMs (e.g. GPT3). Based on\nEVALM, we scale up the size of examples efficiently in both instruction tuning\nand in-context learning to explore the boundary of the benefits from more\nannotated data. Experimental results on a diverse set of tasks show that EVALM\nachieves 4.1% higher accuracy on average, and the average length of achieving\nthe best accuracy score over tasks is around 12k. We find that in-context\nlearning can achieve higher performance with more demonstrations under\nmany-shot instruction tuning (8k), and further extending the length of\ninstructions (16k) can further improve the upper bound of scaling in-context\nlearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mukai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1\">Shansan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiangtao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging supplementary text data to kick-start automatic speech recognition system development with limited transcriptions. (arXiv:2302.04975v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04975","description":"<p>Recent research using pre-trained transformer models suggests that just 10\nminutes of transcribed speech may be enough to fine-tune such a model for\nautomatic speech recognition (ASR) -- at least if we can also leverage vast\namounts of text data (803 million tokens). But is that much text data\nnecessary? We study the use of different amounts of text data, both for\ncreating a lexicon that constrains ASR decoding to possible words (e.g. *dogz\nvs. dogs), and for training larger language models that bias the system toward\nprobable word sequences (e.g. too dogs vs. two dogs). We perform experiments\nusing 10 minutes of transcribed speech from English (for replicating prior\nwork) and two additional pairs of languages differing in the availability of\nsupplemental text data: Gronings and Frisian (~7.5M token corpora available),\nand Besemah and Nasal (only small lexica available). For all languages, we\nfound that using only a lexicon did not appreciably improve ASR performance.\nFor Gronings and Frisian, we found that lexica and language models derived from\n'novel-length' 80k token subcorpora reduced the word error rate (WER) to 39% on\naverage. Our findings suggest that where a text corpus in the upper tens of\nthousands of tokens or more is available, fine-tuning a transformer model with\njust tens of minutes of transcribed speech holds some promise towards obtaining\nhuman-correctable transcriptions near the 30% WER rule-of-thumb.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+San_N/0/1/0/all/0/1\">Nay San</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartelds_M/0/1/0/all/0/1\">Martijn Bartelds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Billings_B/0/1/0/all/0/1\">Blaine Billings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falco_E/0/1/0/all/0/1\">Ella de Falco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feriza_H/0/1/0/all/0/1\">Hendi Feriza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safri_J/0/1/0/all/0/1\">Johan Safri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahrozi_W/0/1/0/all/0/1\">Wawan Sahrozi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foley_B/0/1/0/all/0/1\">Ben Foley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonnell_B/0/1/0/all/0/1\">Bradley McDonnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoNMT: A Framework to Streamline the Research of Seq2Seq Models. (arXiv:2302.04981v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04981","description":"<p>We present AutoNMT, a framework to streamline the research of seq-to-seq\nmodels by automating the data pipeline (i.e., file management, data\npreprocessing, and exploratory analysis), automating experimentation in a\ntoolkit-agnostic manner, which allows users to use either their own models or\nexisting seq-to-seq toolkits such as Fairseq or OpenNMT, and finally,\nautomating the report generation (plots and summaries). Furthermore, this\nlibrary comes with its own seq-to-seq toolkit so that users can easily\ncustomize it for non-standard tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carrion_S/0/1/0/all/0/1\">Salvador Carri&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casacuberta_F/0/1/0/all/0/1\">Francisco Casacuberta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event Temporal Relation Extraction with Bayesian Translational Model. (arXiv:2302.04985v1 [cs.CL])","link":"http://arxiv.org/abs/2302.04985","description":"<p>Existing models to extract temporal relations between events lack a\nprincipled method to incorporate external knowledge. In this study, we\nintroduce Bayesian-Trans, a Bayesian learning-based method that models the\ntemporal relation representations as latent variables and infers their values\nvia Bayesian inference and translational functions. Compared to conventional\nneural approaches, instead of performing point estimation to find the best set\nparameters, the proposed model infers the parameters' posterior distribution\ndirectly, enhancing the model's capability to encode and express uncertainty\nabout the predictions. Experimental results on the three widely used datasets\nshow that Bayesian-Trans outperforms existing approaches for event temporal\nrelation extraction. We additionally present detailed analyses on uncertainty\nquantification, comparison of priors, and ablation studies, illustrating the\nbenefits of the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xingwei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pergola_G/0/1/0/all/0/1\">Gabriele Pergola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-Aware Multilingual Machine Translation with Self-Supervised Learning. (arXiv:2302.05008v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05008","description":"<p>Multilingual machine translation (MMT) benefits from cross-lingual transfer\nbut is a challenging multitask optimization problem. This is partly because\nthere is no clear framework to systematically learn language-specific\nparameters. Self-supervised learning (SSL) approaches that leverage large\nquantities of monolingual data (where parallel data is unavailable) have shown\npromise by improving translation performance as complementary tasks to the MMT\ntask. However, jointly optimizing SSL and MMT tasks is even more challenging.\nIn this work, we first investigate how to utilize intra-distillation to learn\nmore *language-specific* parameters and then show the importance of these\nlanguage-specific parameters. Next, we propose a novel but simple SSL task,\nconcurrent denoising, that co-trains with the MMT task by concurrently\ndenoising monolingual data on both the encoder and decoder. Finally, we apply\nintra-distillation to this co-training approach. Combining these two approaches\nsignificantly improves MMT performance, outperforming three state-of-the-art\nSSL methods by a large margin, e.g., 11.3\\% and 3.7\\% improvement on an\n8-language and a 15-language benchmark compared with MASS, respectively\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maillard_J/0/1/0/all/0/1\">Jean Maillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goswami_V/0/1/0/all/0/1\">Vedanuj Goswami</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is multi-modal vision supervision beneficial to language?. (arXiv:2302.05016v1 [cs.CV])","link":"http://arxiv.org/abs/2302.05016","description":"<p>Vision (image and video) - Language (VL) pre-training is the recent popular\nparadigm that achieved state-of-the-art results on multi-modal tasks like\nimage-retrieval, video-retrieval, visual question answering etc. These models\nare trained in an unsupervised way and greatly benefit from the complementary\nmodality supervision. In this paper, we explore if the language representations\ntrained using vision supervision perform better than vanilla language\nrepresentations on Natural Language Understanding and commonsense reasoning\nbenchmarks. We experiment with a diverse set of image-text models such as\nALBEF, BLIP, METER and video-text models like ALPRO, Frozen-in-Time (FiT),\nVIOLET. We compare the performance of language representations of stand-alone\ntext encoders of these models to the language representations of text encoders\nlearnt through vision supervision. Our experiments suggest that vanilla\nlanguage representations show superior performance on most of the tasks. These\nresults shed light on the current drawbacks of the vision-language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madasu_A/0/1/0/all/0/1\">Avinash Madasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1\">Vasudev Lal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Desirable Revisions of Evidence and Reasoning in Argumentative Writing. (arXiv:2302.05039v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05039","description":"<p>We develop models to classify desirable evidence and desirable reasoning\nrevisions in student argumentative writing. We explore two ways to improve\nclassifier performance - using the essay context of the revision, and using the\nfeedback students received before the revision. We perform both intrinsic and\nextrinsic evaluation for each of our models and report a qualitative analysis.\nOur results show that while a model using feedback information improves over a\nbaseline model, models utilizing context - either alone or with feedback - are\nthe most successful in identifying desirable revisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Afrin_T/0/1/0/all/0/1\">Tazin Afrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1\">Diane Litman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PATCorrect: Non-autoregressive Phoneme-augmented Transformer for ASR Error Correction. (arXiv:2302.05040v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05040","description":"<p>Speech-to-text errors made by automatic speech recognition (ASR) system\nnegatively impact downstream models relying on ASR transcriptions. Language\nerror correction models as a post-processing text editing approach have been\nrecently developed for refining the source sentences. However, efficient models\nfor correcting errors in ASR transcriptions that meet the low latency\nrequirements of industrial grade production systems have not been well studied.\nIn this work, we propose a novel non-autoregressive (NAR) error correction\napproach to improve the transcription quality by reducing word error rate (WER)\nand achieve robust performance across different upstream ASR systems. Our\napproach augments the text encoding of the Transformer model with a phoneme\nencoder that embeds pronunciation information. The representations from phoneme\nencoder and text encoder are combined via multi-modal fusion before feeding\ninto the length tagging predictor for predicting target sequence lengths. The\njoint encoders also provide inputs to the attention mechanism in the NAR\ndecoder. We experiment on 3 open-source ASR systems with varying speech-to-text\ntranscription quality and their erroneous transcriptions on 2 public English\ncorpus datasets. Results show that our PATCorrect (Phoneme Augmented\nTransformer for ASR error Correction) consistently outperforms state-of-the-art\nNAR error correction method on English corpus across different upstream ASR\nsystems. For example, PATCorrect achieves 11.62% WER reduction (WERR) averaged\non 3 ASR systems compared to 9.46% WERR achieved by other method using text\nonly modality and also achieves an inference latency comparable to other NAR\nmodels at tens of millisecond scale, especially on GPU hardware, while still\nbeing 4.2 - 6.7x times faster than autoregressive models on Common Voice and\nLibriSpeech datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhehui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamma_R/0/1/0/all/0/1\">Rajesh Kamma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eswaran_S/0/1/0/all/0/1\">Sharanya Eswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadagopan_N/0/1/0/all/0/1\">Narayanan Sadagopan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ControversialQA: Exploring Controversy in Question Answering. (arXiv:2302.05061v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05061","description":"<p>Controversy is widespread online. Previous studies mainly define controversy\nbased on vague assumptions of its relation to sentiment such as hate speech and\noffensive words. This paper introduces the first question-answering dataset\nthat defines content controversy by user perception, i.e., votes from plenty of\nusers. It contains nearly 10K questions, and each question has a best answer\nand a most controversial answer. Experimental results reveal that controversy\ndetection in question answering is essential and challenging, and there is no\nstrong correlation between controversy and sentiment tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Peide Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selective In-Context Data Augmentation for Intent Detection using Pointwise V-Information. (arXiv:2302.05096v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05096","description":"<p>This work focuses on in-context data augmentation for intent detection.\nHaving found that augmentation via in-context prompting of large pre-trained\nlanguage models (PLMs) alone does not improve performance, we introduce a novel\napproach based on PLMs and pointwise V-information (PVI), a metric that can\nmeasure the usefulness of a datapoint for training a model. Our method first\nfine-tunes a PLM on a small seed of training data and then synthesizes new\ndatapoints - utterances that correspond to given intents. It then employs\nintent-aware filtering, based on PVI, to remove datapoints that are not helpful\nto the downstream intent classifier. Our method is thus able to leverage the\nexpressive power of large language models to produce diverse training data.\nEmpirical results demonstrate that our method can produce synthetic training\ndata that achieve state-of-the-art performance on three challenging intent\ndetection datasets under few-shot settings (1.28% absolute improvement in\n5-shot and 1.18% absolute in 10-shot, on average) and perform on par with the\nstate-of-the-art in full-shot settings (within 0.01% absolute, on average).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papangelis_A/0/1/0/all/0/1\">Alexandros Papangelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungjin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1\">Devamanyu Hazarika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namazifar_M/0/1/0/all/0/1\">Mahdi Namazifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Corpora Spoken Language Identification with Domain Diversification and Generalization. (arXiv:2302.05110v1 [eess.AS])","link":"http://arxiv.org/abs/2302.05110","description":"<p>This work addresses the cross-corpora generalization issue for the\nlow-resourced spoken language identification (LID) problem. We have conducted\nthe experiments in the context of Indian LID and identified strikingly poor\ncross-corpora generalization due to corpora-dependent non-lingual biases. Our\ncontribution to this work is twofold. First, we propose domain diversification,\nwhich diversifies the limited training data using different audio data\naugmentation methods. We then propose the concept of maximally diversity-aware\ncascaded augmentations and optimize the augmentation fold-factor for effective\ndiversification of the training data. Second, we introduce the idea of domain\ngeneralization considering the augmentation methods as pseudo-domains. Towards\nthis, we investigate both domain-invariant and domain-aware approaches. Our LID\nsystem is based on the state-of-the-art emphasized channel attention,\npropagation, and aggregation based time delay neural network (ECAPA-TDNN)\narchitecture. We have conducted extensive experiments with three widely used\ncorpora for Indian LID research. In addition, we conduct a final blind\nevaluation of our proposed methods on the Indian subset of VoxLingua107 corpus\ncollected in the wild. Our experiments demonstrate that the proposed domain\ndiversification is more promising over commonly used simple augmentation\nmethods. The study also reveals that domain generalization is a more effective\nsolution than domain diversification. We also notice that domain-aware learning\nperforms better for same-corpora LID, whereas domain-invariant learning is more\nsuitable for cross-corpora generalization. Compared to basic ECAPA-TDNN, its\nproposed domain-invariant extensions improve the cross-corpora EER up to 5.23%.\nIn contrast, the proposed domain-aware extensions also improve performance for\nsame-corpora test scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dey_S/0/1/0/all/0/1\">Spandan Dey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sahidullah_M/0/1/0/all/0/1\">Md Sahidullah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saha_G/0/1/0/all/0/1\">Goutam Saha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Step by Step Loss Goes Very Far: Multi-Step Quantization for Adversarial Text Attacks. (arXiv:2302.05120v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05120","description":"<p>We propose a novel gradient-based attack against transformer-based language\nmodels that searches for an adversarial example in a continuous space of token\nprobabilities. Our algorithm mitigates the gap between adversarial loss for\ncontinuous and discrete text representations by performing multi-step\nquantization in a quantization-compensation loop. Experiments show that our\nmethod significantly outperforms other approaches on various natural language\nprocessing (NLP) tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gainski_P/0/1/0/all/0/1\">Piotr Gai&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balazy_K/0/1/0/all/0/1\">Klaudia Ba&#x142;azy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translating Natural Language to Planning Goals with Large-Language Models. (arXiv:2302.05128v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05128","description":"<p>Recent large language models (LLMs) have demonstrated remarkable performance\non a variety of natural language processing (NLP) tasks, leading to intense\nexcitement about their applicability across various domains. Unfortunately,\nrecent work has also shown that LLMs are unable to perform accurate reasoning\nnor solve planning problems, which may limit their usefulness for\nrobotics-related tasks. In this work, our central question is whether LLMs are\nable to translate goals specified in natural language to a structured planning\nlanguage. If so, LLM can act as a natural interface between the planner and\nhuman users; the translated goal can be handed to domain-independent AI\nplanners that are very effective at planning. Our empirical results on GPT 3.5\nvariants show that LLMs are much better suited towards translation rather than\nplanning. We find that LLMs are able to leverage commonsense knowledge and\nreasoning to furnish missing details from under-specified goals (as is often\nthe case in natural language). However, our experiments also reveal that LLMs\ncan fail to generate goals in tasks that involve numerical or physical (e.g.,\nspatial) reasoning, and that LLMs are sensitive to the prompts used. As such,\nthese models are promising for translation to structured planning languages,\nbut care should be taken in their use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yaqi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tongyao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jinbin Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Ze Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soh_H/0/1/0/all/0/1\">Harold Soh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Realistic Conversational Question Answering with Answer Selection based on Calibrated Confidence and Uncertainty Measurement. (arXiv:2302.05137v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05137","description":"<p>Conversational Question Answering (ConvQA) models aim at answering a question\nwith its relevant paragraph and previous question-answer pairs that occurred\nduring conversation multiple times. To apply such models to a real-world\nscenario, some existing work uses predicted answers, instead of unavailable\nground-truth answers, as the conversation history for inference. However, since\nthese models usually predict wrong answers, using all the predictions without\nfiltering significantly hampers the model performance. To address this problem,\nwe propose to filter out inaccurate answers in the conversation history based\non their estimated confidences and uncertainties from the ConvQA model, without\nmaking any architectural changes. Moreover, to make the confidence and\nuncertainty values more reliable, we propose to further calibrate them, thereby\nsmoothing the model predictions. We validate our models, Answer Selection-based\nrealistic Conversation Question Answering, on two standard ConvQA datasets, and\nthe results show that our models significantly outperform relevant baselines.\nCode is available at: https://github.com/starsuzi/AS-ConvQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1\">Soyeong Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1\">Jinheon Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jong C. Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plan-then-Seam: Towards Efficient Table-to-Text Generation. (arXiv:2302.05138v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05138","description":"<p>Table-to-text generation aims at automatically generating text to help people\nconveniently obtain salient information in tables. Recent works explicitly\ndecompose the generation process into content planning and surface generation\nstages, employing two autoregressive networks for them respectively. However,\nthey are computationally expensive due to the non-parallelizable nature of\nautoregressive decoding and the redundant parameters of two networks. In this\npaper, we propose the first totally non-autoregressive table-to-text model\n(Plan-then-Seam, PTS) that produces its outputs in parallel with one single\nnetwork. PTS firstly writes and calibrates one plan of the content to be\ngenerated with a novel rethinking pointer predictor, and then takes the plan as\nthe context for seaming to decode the description. These two steps share\nparameters and perform iteratively to capture token inter-dependency while\nkeeping parallel decoding. Experiments on two public benchmarks show that PTS\nachieves 3.0~5.6 times speedup for inference time, reducing 50% parameters,\nwhile maintaining as least comparable performance against strong two-stage\ntable-to-text competitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_R/0/1/0/all/0/1\">Ruiying Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Chengyang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Can Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Binhua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Wisdom of Hindsight Makes Language Models Better Instruction Followers. (arXiv:2302.05206v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05206","description":"<p>Reinforcement learning has seen wide success in finetuning large language\nmodels to better align with instructions via human feedback. The so-called\nalgorithm, Reinforcement Learning with Human Feedback (RLHF) demonstrates\nimpressive performance on the GPT series models. However, the underlying\nReinforcement Learning (RL) algorithm is complex and requires an additional\ntraining pipeline for reward and value networks. In this paper, we consider an\nalternative approach: converting feedback to instruction by relabeling the\noriginal one and training the model for better alignment in a supervised\nmanner. Such an algorithm doesn't require any additional parameters except for\nthe original language model and maximally reuses the pretraining pipeline. To\nachieve this, we formulate instruction alignment problem for language models as\na goal-reaching problem in decision making. We propose Hindsight Instruction\nRelabeling (HIR), a novel algorithm for aligning language models with\ninstructions. The resulting two-stage algorithm shed light to a family of\nreward-free approaches that utilize the hindsightly relabeled instructions\nbased on feedback. We evaluate the performance of HIR extensively on 12\nchallenging BigBench reasoning tasks and show that HIR outperforms the baseline\nalgorithms and is comparable to or even surpasses supervised finetuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Justin Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Song of Ice and Fire: Analyzing Textual Autotelic Agents in ScienceWorld. (arXiv:2302.05244v1 [cs.AI])","link":"http://arxiv.org/abs/2302.05244","description":"<p>Building open-ended agents that can autonomously discover a diversity of\nbehaviours is one of the long-standing goals of artificial intelligence. This\nchallenge can be studied in the framework of autotelic RL agents, i.e. agents\nthat learn by selecting and pursuing their own goals, self-organizing a\nlearning curriculum. Recent work identified language has a key dimension of\nautotelic learning, in particular because it enables abstract goal sampling and\nguidance from social peers for hindsight relabelling. Within this perspective,\nwe study the following open scientific questions: What is the impact of\nhindsight feedback from a social peer (e.g. selective vs. exhaustive)? How can\nthe agent learn from very rare language goal examples in its experience replay?\nHow can multiple forms of exploration be combined, and take advantage of easier\ngoals as stepping stones to reach harder ones? To address these questions, we\nuse ScienceWorld, a textual environment with rich abstract and combinatorial\nphysics. We show the importance of selectivity from the social peer's feedback;\nthat experience replay needs to over-sample examples of rare goals; and that\nfollowing self-generated goal sequences where the agent's competence is\nintermediate leads to significant improvements in final performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teodorescu_L/0/1/0/all/0/1\">Laetitia Teodorescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_E/0/1/0/all/0/1\">Eric Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cote_M/0/1/0/all/0/1\">Marc-Alexandre C&#xf4;t&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rumor Classification through a Multimodal Fusion Framework and Ensemble Learning. (arXiv:2302.05289v1 [cs.CV])","link":"http://arxiv.org/abs/2302.05289","description":"<p>The proliferation of rumors on social media has become a major concern due to\nits ability to create a devastating impact. Manually assessing the veracity of\nsocial media messages is a very time-consuming task that can be much helped by\nmachine learning. Most message veracity verification methods only exploit\ntextual contents and metadata. Very few take both textual and visual contents,\nand more particularly images, into account. Moreover, prior works have used\nmany classical machine learning models to detect rumors. However, although\nrecent studies have proven the effectiveness of ensemble machine learning\napproaches, such models have seldom been applied. Thus, in this paper, we\npropose a set of advanced image features that are inspired from the field of\nimage quality assessment, and introduce the Multimodal fusiON framework to\nassess message veracIty in social neTwORks (MONITOR), which exploits all\nmessage features by exploring various machine learning models. Moreover, we\ndemonstrate the effectiveness of ensemble learning algorithms for rumor\ndetection by using five metalearning models. Eventually, we conduct extensive\nexperiments on two real-world datasets. Results show that MONITOR outperforms\nstate-of-the-art machine learning baselines and that ensemble models\nsignificantly increase MONITOR's performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azri_A/0/1/0/all/0/1\">Abderrazek Azri</a> (ERIC), <a href=\"http://arxiv.org/find/cs/1/au:+Favre_C/0/1/0/all/0/1\">C&#xe9;cile Favre</a> (ERIC), <a href=\"http://arxiv.org/find/cs/1/au:+Harbi_N/0/1/0/all/0/1\">Nouria Harbi</a> (ERIC), <a href=\"http://arxiv.org/find/cs/1/au:+Darmont_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Darmont</a> (ERIC), <a href=\"http://arxiv.org/find/cs/1/au:+Nous_C/0/1/0/all/0/1\">Camille No&#xfb;s</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Span-based Named Entity Recognition by Generating and Compressing Information. (arXiv:2302.05392v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05392","description":"<p>The information bottleneck (IB) principle has been proven effective in\nvarious NLP applications. The existing work, however, only used either\ngenerative or information compression models to improve the performance of the\ntarget task. In this paper, we propose to combine the two types of IB models\ninto one system to enhance Named Entity Recognition (NER). For one type of IB\nmodel, we incorporate two unsupervised generative components, span\nreconstruction and synonym generation, into a span-based NER system. The span\nreconstruction ensures that the contextualised span representation keeps the\nspan information, while the synonym generation makes synonyms have similar\nrepresentations even in different contexts. For the other type of IB model, we\nadd a supervised IB layer that performs information compression into the system\nto preserve useful features for NER in the resulting span representations.\nExperiments on five different corpora indicate that jointly training both\ngenerative and information compression models can enhance the performance of\nthe baseline span-based NER system. Our source code is publicly available at\nhttps://github.com/nguyennth/joint-ib-models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Nhung T.H. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miwa_M/0/1/0/all/0/1\">Makoto Miwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1\">Sophia Ananiadou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Transformer Language Models for Contextual Commonsense Inference. (arXiv:2302.05406v1 [cs.CL])","link":"http://arxiv.org/abs/2302.05406","description":"<p>Contextualized or discourse aware commonsense inference is the task of\ngenerating coherent commonsense assertions (i.e., facts) from a given story,\nand a particular sentence from that story. Some problems with the task are:\nlack of controllability for topics of the inferred facts; lack of commonsense\nknowledge during training; and, possibly, hallucinated or false facts. In this\nwork, we utilize a transformer model for this task and develop techniques to\naddress the aforementioned problems in the task. We control the inference by\nintroducing a new technique we call \"hinting\". Hinting is a kind of language\nmodel prompting, that utilizes both hard prompts (specific words) and soft\nprompts (virtual learnable templates). This serves as a control signal to\nadvise the language model \"what to talk about\". Next, we establish a\nmethodology for performing joint inference with multiple commonsense knowledge\nbases. Joint inference of commonsense requires care, because it is imprecise\nand the level of generality is more flexible. You want to be sure that the\nresults \"still make sense\" for the context. To this end, we align the textual\nversion of assertions from three knowledge graphs (ConceptNet, ATOMIC2020, and\nGLUCOSE) with a story and a target sentence. This combination allows us to\ntrain a single model to perform joint inference with multiple knowledge graphs.\nWe show experimental results for the three knowledge graphs on joint inference.\nOur final contribution is exploring a GAN architecture that generates the\ncontextualized commonsense assertions and scores them as to their plausibility\nthrough a discriminator. The result is an integrated system for contextual\ncommonsense inference in stories, that can controllably generate plausible\ncommonsense assertions, and takes advantage of joint inference between multiple\ncommonsense knowledge bases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colon_Hernandez_P/0/1/0/all/0/1\">Pedro Colon-Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lieberman_H/0/1/0/all/0/1\">Henry Lieberman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_Y/0/1/0/all/0/1\">Yida Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Claire Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breazeal_C/0/1/0/all/0/1\">Cynthia Breazeal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_P/0/1/0/all/0/1\">Peter Chin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-based Language Model Fine-tuning Methods for COVID-19 Fake News Detection. (arXiv:2101.05509v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.05509","description":"<p>With the pandemic of COVID-19, relevant fake news is spreading all over the\nsky throughout the social media. Believing in them without discrimination can\ncause great trouble to people's life. However, universal language models may\nperform weakly in these fake news detection for lack of large-scale annotated\ndata and sufficient semantic understanding of domain-specific knowledge. While\nthe model trained on corresponding corpora is also mediocre for insufficient\nlearning. In this paper, we propose a novel transformer-based language model\nfine-tuning approach for these fake news detection. First, the token vocabulary\nof individual model is expanded for the actual semantics of professional\nphrases. Second, we adapt the heated-up softmax loss to distinguish the\nhard-mining samples, which are common for fake news because of the\ndisambiguation of short text. Then, we involve adversarial training to improve\nthe model's robustness. Last, the predicted features extracted by universal\nlanguage model RoBERTa and domain-specific model CT-BERT are fused by one\nmultiple layer perception to integrate fine-grained and high-level specific\nrepresentations. Quantitative experimental results evaluated on existing\nCOVID-19 fake news dataset show its superior performances compared to the\nstate-of-the-art methods among various evaluation metrics. Furthermore, the\nbest weighted average F1 score achieves 99.02%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Ben Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Dehong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qijin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_C/0/1/0/all/0/1\">Chengfu Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiaonan Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Weijun Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yang Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dive into Deep Learning. (arXiv:2106.11342v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.11342","description":"<p>This open-source book represents our attempt to make deep learning\napproachable, teaching readers the concepts, the context, and the code. The\nentire book is drafted in Jupyter notebooks, seamlessly integrating exposition\nfigures, math, and interactive examples with self-contained code. Our goal is\nto offer a resource that could (i) be freely available for everyone; (ii) offer\nsufficient technical depth to provide a starting point on the path to actually\nbecoming an applied machine learning scientist; (iii) include runnable code,\nshowing readers how to solve problems in practice; (iv) allow for rapid\nupdates, both by us and also by the community at large; (v) be complemented by\na forum for interactive discussion of technical details and to answer\nquestions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1\">Alexander J. Smola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Small-Text: Active Learning for Text Classification in Python. (arXiv:2107.10314v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.10314","description":"<p>We present small-text, an easy-to-use active learning library written in\nPython, which offers pool-based active learning for single- and multi-label\ntext classification in Python. It features many pre-implemented\nstate-of-the-art query strategies, including some that leverage the GPU.\nStandardized interfaces allow the combination of a variety of classifiers,\nquery strategies, and stopping criteria, facilitating a quick mix and match,\nand enabling a rapid development of both active learning experiments and\napplications. In order to make various classifiers and query strategies\naccessible for active learning, small-text integrates several well-known\nmachine learning libraries, namely scikit-learn, PyTorch, and Hugging Face\ntransformers. The latter integrations are optionally installable extensions, so\nGPUs can be used but are not required. The library is publicly available under\nthe MIT License at https://github.com/webis-de/small-text, in version 1.1.1 at\nthe time of writing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schroder_C/0/1/0/all/0/1\">Christopher Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_L/0/1/0/all/0/1\">Lydia M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekler_A/0/1/0/all/0/1\">Andreas Niekler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Development of an Extractive Clinical Question Answering Dataset with Multi-Answer and Multi-Focus Questions. (arXiv:2201.02517v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.02517","description":"<p>Background: Extractive question-answering (EQA) is a useful natural language\nprocessing (NLP) application for answering patient-specific questions by\nlocating answers in their clinical notes. Realistic clinical EQA can have\nmultiple answers to a single question and multiple focus points in one\nquestion, which are lacking in the existing datasets for development of\nartificial intelligence solutions. Objective: Create a dataset for developing\nand evaluating clinical EQA systems that can handle natural multi-answer and\nmulti-focus questions. Methods: We leveraged the annotated relations from the\n2018 National NLP Clinical Challenges (n2c2) corpus to generate an EQA dataset.\nSpecifically, the 1-to-N, M-to-1, and M-to-N drug-reason relations were\nincluded to form the multi-answer and multi-focus QA entries, which represent\nmore complex and natural challenges in addition to the basic\none-drug-one-reason cases. A baseline solution was developed and tested on the\ndataset. Results: The derived RxWhyQA dataset contains 96,939 QA entries. Among\nthe answerable questions, 25% require multiple answers, and 2% ask about\nmultiple drugs within one question. There are frequent cues observed around the\nanswers in the text, and 90% of the drug and reason terms occur within the same\nor an adjacent sentence. The baseline EQA solution achieved a best f1-measure\nof 0.72 on the entire dataset, and on specific subsets, it was: 0.93 on the\nunanswerable questions, 0.48 on single-drug questions versus 0.60 on multi-drug\nquestions, 0.54 on the single-answer questions versus 0.43 on multi-answer\nquestions. Discussion: The RxWhyQA dataset can be used to train and evaluate\nsystems that need to handle multi-answer and multi-focus questions.\nSpecifically, multi-answer EQA appears to be challenging and therefore warrants\nmore investment in research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Sungrim Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Huan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jungwei W. Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Should You Mask 15% in Masked Language Modeling?. (arXiv:2202.08005v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.08005","description":"<p>Masked language models (MLMs) conventionally mask 15% of tokens due to the\nbelief that more masking would leave insufficient context to learn good\nrepresentations; this masking rate has been widely used, regardless of model\nsizes or masking strategies. In this work, we revisit this important choice of\nMLM pre-training. We first establish that 15% is not universally optimal, and\nlarger models should adopt a higher masking rate. Specifically, we find that\nmasking 40% outperforms 15% for BERT-large size models on GLUE and SQuAD.\nInterestingly, an extremely high masking rate of 80% can still preserve 95%\nfine-tuning performance and most of the accuracy in linguistic probing,\nchallenging the conventional wisdom about the role of the masking rate. We then\nexamine the interplay between masking rates and masking strategies and find\nthat uniform masking requires a higher masking rate compared to sophisticated\nmasking strategies such as span or PMI masking. Finally, we argue that\nincreasing the masking rate has two distinct effects: it leads to more\ncorruption, which makes the prediction task more difficult; it also enables\nmore predictions, which benefits optimization. Using this framework, we revisit\nBERT's 80-10-10 corruption strategy. Together, our results contribute to a\nbetter understanding of MLM pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wettig_A/0/1/0/all/0/1\">Alexander Wettig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zexuan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation Extraction with Weighted Contrastive Pre-training on Distant Supervision. (arXiv:2205.08770v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.08770","description":"<p>Contrastive pre-training on distant supervision has shown remarkable\neffectiveness in improving supervised relation extraction tasks. However, the\nexisting methods ignore the intrinsic noise of distant supervision during the\npre-training stage. In this paper, we propose a weighted contrastive learning\nmethod by leveraging the supervised data to estimate the reliability of\npre-training instances and explicitly reduce the effect of noise. Experimental\nresults on three supervised datasets demonstrate the advantages of our proposed\nweighted contrastive learning approach compared to two state-of-the-art\nnon-weighted baselines.Our code and models are available at:\nhttps://github.com/YukinoWan/WCL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zhen Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1\">Fei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qianying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhuoyuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Haiyue Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurohashi_S/0/1/0/all/0/1\">Sadao Kurohashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Normalization of Temporal Expressions with Masked Language Models. (arXiv:2205.10399v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10399","description":"<p>The detection and normalization of temporal expressions is an important task\nand preprocessing step for many applications. However, prior work on\nnormalization is rule-based, which severely limits the applicability in\nreal-world multilingual settings, due to the costly creation of new rules. We\npropose a novel neural method for normalizing temporal expressions based on\nmasked language modeling. Our multilingual method outperforms prior rule-based\nsystems in many languages, and in particular, for low-resource languages with\nperformance improvements of up to 33 F1 on average compared to the state of the\nart.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lange_L/0/1/0/all/0/1\">Lukas Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strotgen_J/0/1/0/all/0/1\">Jannik Str&#xf6;tgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LingMess: Linguistically Informed Multi Expert Scorers for Coreference Resolution. (arXiv:2205.12644v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12644","description":"<p>While coreference resolution typically involves various linguistic\nchallenges, recent models are based on a single pairwise scorer for all types\nof pairs. We present LingMess, a new coreference model that defines different\ncategories of coreference cases and optimize multiple pairwise scorers, where\neach scorer learns a specific set of linguistic challenges. Our model\nsubstantially improves pairwise scores for most categories and outperforms\ncluster-level performance on Ontonotes and 5 additional datasets. Our model is\navailable in https://github.com/shon-otmazgin/lingmess-coref\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Otmazgin_S/0/1/0/all/0/1\">Shon Otmazgin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cattan_A/0/1/0/all/0/1\">Arie Cattan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PROD: Progressive Distillation for Dense Retrieval. (arXiv:2209.13335v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2209.13335","description":"<p>Knowledge distillation is an effective way to transfer knowledge from a\nstrong teacher to an efficient student model. Ideally, we expect the better the\nteacher is, the better the student. However, this expectation does not always\ncome true. It is common that a better teacher model results in a bad student\nvia distillation due to the nonnegligible gap between teacher and student. To\nbridge the gap, we propose PROD, a PROgressive Distillation method, for dense\nretrieval. PROD consists of a teacher progressive distillation and a data\nprogressive distillation to gradually improve the student. We conduct extensive\nexperiments on five widely-used benchmarks, MS MARCO Passage, TREC Passage 19,\nTREC Document 19, MS MARCO Document and Natural Questions, where PROD achieves\nthe state-of-the-art within the distillation methods for dense retrieval. The\ncode and models will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhenghao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_A/0/1/0/all/0/1\">Anlei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jian Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jingwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_R/0/1/0/all/0/1\">Rangan Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PLOT: Prompt Learning with Optimal Transport for Vision-Language Models. (arXiv:2210.01253v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.01253","description":"<p>With the increasing attention to large vision-language models such as CLIP,\nthere has been a significant amount of effort dedicated to building efficient\nprompts. Unlike conventional methods of only learning one single prompt, we\npropose to learn multiple comprehensive prompts to describe diverse\ncharacteristics of categories such as intrinsic attributes or extrinsic\ncontexts. However, directly matching each prompt to the same visual feature is\nproblematic, as it pushes the prompts to converge to one point. To solve this\nproblem, we propose to apply optimal transport to match the vision and text\nmodalities. Specifically, we first model images and the categories with visual\nand textual feature sets. Then, we apply a two-stage optimization strategy to\nlearn the prompts. In the inner loop, we optimize the optimal transport\ndistance to align visual features and prompts by the Sinkhorn algorithm, while\nin the outer loop, we learn the prompts by this distance from the supervised\ndata. Extensive experiments are conducted on the few-shot recognition task and\nthe improvement demonstrates the superiority of our method. The code is\navailable at https://github.com/CHENGY12/PLOT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Weiran Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiangchen Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyue Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Designing Robust Transformers using Robust Kernel Density Estimation. (arXiv:2210.05794v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.05794","description":"<p>Recent advances in Transformer architectures have empowered their empirical\nsuccess in a variety of tasks across different domains. However, existing works\nmainly focus on predictive accuracy and computational cost, without considering\nother practical issues, such as robustness to contaminated samples. Recent work\nby Nguyen et al., (2022) has shown that the self-attention mechanism, which is\nthe center of the Transformer architecture, can be viewed as a non-parametric\nestimator based on kernel density estimation (KDE). This motivates us to\nleverage a set of robust kernel density estimation methods for alleviating the\nissue of data contamination. Specifically, we introduce a series of\nself-attention mechanisms that can be incorporated into different Transformer\narchitectures and discuss the special properties of each method. We then\nperform extensive empirical studies on language modeling and image\nclassification tasks. Our methods demonstrate robust performance in multiple\nscenarios while maintaining competitive results on clean datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1\">Tongzheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tan Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_J/0/1/0/all/0/1\">Joydeep Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1\">Nhat Ho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Context into Subword Vocabularies. (arXiv:2210.07095v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07095","description":"<p>Most current popular subword tokenizers are trained based on word frequency\nstatistics over a corpus, without considering information about co-occurrence\nor context. Nevertheless, the resulting vocabularies are used in language\nmodels' highly contextualized settings. We present SaGe, a tokenizer that\ntailors subwords for their downstream use by baking in the contextualized\nsignal at the vocabulary creation phase. We show that SaGe does a better job\nthan current widespread tokenizers in keeping token contexts cohesive, while\nnot incurring a large price in terms of encoding efficiency or domain\nrobustness. SaGe improves performance on English GLUE classification tasks as\nwell as on NER, and on Inference and NER in Turkish, demonstrating its\nrobustness to language properties such as morphological exponence and\nagglutination.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yehezkel_S/0/1/0/all/0/1\">Shaked Yehezkel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinter_Y/0/1/0/all/0/1\">Yuval Pinter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Watermarking Pre-trained Language Models with Backdooring. (arXiv:2210.07543v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07543","description":"<p>Large pre-trained language models (PLMs) have proven to be a crucial\ncomponent of modern natural language processing systems. PLMs typically need to\nbe fine-tuned on task-specific downstream datasets, which makes it hard to\nclaim the ownership of PLMs and protect the developer's intellectual property\ndue to the catastrophic forgetting phenomenon. We show that PLMs can be\nwatermarked with a multi-task learning framework by embedding backdoors\ntriggered by specific inputs defined by the owners, and those watermarks are\nhard to remove even though the watermarked PLMs are fine-tuned on multiple\ndownstream tasks. In addition to using some rare words as triggers, we also\nshow that the combination of common words can be used as backdoor triggers to\navoid them being easily detected. Extensive experiments on multiple datasets\ndemonstrate that the embedded watermarks can be robustly extracted with a high\nsuccess rate and less influenced by the follow-up fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_C/0/1/0/all/0/1\">Chenxi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chengsong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training. (arXiv:2210.07688v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07688","description":"<p>Large-scale vision-language pre-trained (VLP) models are prone to hallucinate\nnon-existent visual objects when generating text based on visual information.\nIn this paper, we systematically study the object hallucination problem from\nthree aspects. First, we examine recent state-of-the-art VLP models, showing\nthat they still hallucinate frequently, and models achieving better scores on\nstandard metrics (e.g., CIDEr) could be more unfaithful. Second, we investigate\nhow different types of image encoding in VLP influence hallucination, including\nregion-based, grid-based, and patch-based. Surprisingly, we find that\npatch-based features perform the best and smaller patch resolution yields a\nnon-trivial reduction in object hallucination. Third, we decouple various VLP\nobjectives and demonstrate that token-level image-text alignment and controlled\ngeneration are crucial to reducing hallucination. Based on that, we propose a\nsimple yet effective VLP loss named ObjMLM to further mitigate object\nhallucination. Results show that it reduces object hallucination by up to 17.4%\nwhen tested on two benchmarks (COCO Caption for in-domain and NoCaps for\nout-of-domain evaluation).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Ziwei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From exemplar to copy: the scribal appropriation of a Hadewijch manuscript computationally explored. (arXiv:2210.14061v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.14061","description":"<p>This study is devoted to two of the oldest known manuscripts in which the\noeuvre of the medieval mystical author Hadewijch has been preserved: Brussels,\nKBR, 2879-2880 (ms. A) and Brussels, KBR, 2877-2878 (ms. B). On the basis of\ncodicological and contextual arguments, it is assumed that the scribe who\nproduced B used A as an exemplar. While the similarities in both layout and\ncontent between the two manuscripts are striking, the present article seeks to\nidentify the differences. After all, regardless of the intention to produce a\ncopy that closely follows the exemplar, subtle linguistic variation is\napparent. Divergences relate to spelling conventions, but also to the way in\nwhich words are abbreviated (and the extent to which abbreviations occur). The\npresent study investigates the spelling profiles of the scribes who produced\nmss. A and B in a computational way. In the first part of this study, we will\npresent both manuscripts in more detail, after which we will consider prior\nresearch carried out on scribal profiling. The current study both builds and\nexpands on Kestemont (2015). Next, we outline the methodology used to analyse\nand measure the degree of scribal appropriation that took place when ms. B was\ncopied off the exemplar ms. A. After this, we will discuss the results\nobtained, focusing on the scribal variation that can be found both at the level\nof individual words and n-grams. To this end, we use machine learning to\nidentify the most distinctive features that separate manuscript A from B.\nFinally, we look at possible diachronic trends in the appropriation by B's\nscribe of his exemplar. We argue that scribal takeovers in the exemplar impacts\nthe practice of the copying scribe, while transitions to a different content\nmatter cause little to no effect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haverals_W/0/1/0/all/0/1\">Wouter Haverals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kestemont_M/0/1/0/all/0/1\">Mike Kestemont</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards human-compatible autonomous car: A study of non-verbal Turing test in automated driving with affective transition modelling. (arXiv:2212.02908v4 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2212.02908","description":"<p>Autonomous cars are indispensable when humans go further down the hands-free\nroute. Although existing literature highlights that the acceptance of the\nautonomous car will increase if it drives in a human-like manner, sparse\nresearch offers the naturalistic experience from a passenger's seat perspective\nto examine the human likeness of current autonomous cars. The present study\ntested whether the AI driver could create a human-like ride experience for\npassengers based on 69 participants' feedback in a real-road scenario. We\ndesigned a ride experience-based version of the non-verbal Turing test for\nautomated driving. Participants rode in autonomous cars (driven by either human\nor AI drivers) as a passenger and judged whether the driver was human or AI.\nThe AI driver failed to pass our test because passengers detected the AI driver\nabove chance. In contrast, when the human driver drove the car, the passengers'\njudgement was around chance. We further investigated how human passengers\nascribe humanness in our test. Based on Lewin's field theory, we advanced a\ncomputational model combining signal detection theory with pre-trained language\nmodels to predict passengers' humanness rating behaviour. We employed affective\ntransition between pre-study baseline emotions and corresponding post-stage\nemotions as the signal strength of our model. Results showed that the\npassengers' ascription of humanness would increase with the greater affective\ntransition. Our study suggested an important role of affective transition in\npassengers' ascription of humanness, which might become a future direction for\nautonomous driving.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaoning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qiaoli Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhengming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Anqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haiyan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Miner Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ku_Y/0/1/0/all/0/1\">Yixuan Ku</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence Generation with Label Augmentation for Relation Extraction. (arXiv:2212.14266v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.14266","description":"<p>Sequence generation demonstrates promising performance in recent information\nextraction efforts, by incorporating large-scale pre-trained Seq2Seq models.\nThis paper investigates the merits of employing sequence generation in relation\nextraction, finding that with relation names or synonyms as generation targets,\ntheir textual semantics and the correlation (in terms of word sequence pattern)\namong them affect model performance. We then propose Relation Extraction with\nLabel Augmentation (RELA), a Seq2Seq model with automatic label augmentation\nfor RE. By saying label augmentation, we mean prod semantically synonyms for\neach relation name as the generation target. Besides, we present an in-depth\nanalysis of the Seq2Seq model's behavior when dealing with RE. Experimental\nresults show that RELA achieves competitive results compared with previous\nmethods on four RE datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dingyao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinglei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shikun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"tasksource: Structured Dataset Preprocessing Annotations for Frictionless Extreme Multi-Task Learning and Evaluation. (arXiv:2301.05948v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.05948","description":"<p>The HuggingFace Datasets Hub hosts thousands of datasets. This provides\nexciting opportunities for language model training and evaluation. However, the\ndatasets for a given type of task are stored with different schemas, and\nharmonization is harder than it seems (https://xkcd.com/927/). Multi-task\ntraining or evaluation requires manual work to fit data into task templates.\nVarious initiatives independently address this problem by releasing the\nharmonized datasets or harmonization codes to preprocess datasets to the same\nformat. We identify patterns across previous preprocessings, e.g. mapping of\ncolumn names, and extraction of a specific sub-field from structured data in a\ncolumn, and propose a structured annotation framework that makes our\nannotations fully exposed and not buried in unstructured code. We release a\ndataset annotation framework and dataset annotations for more than 400 English\ntasks (https://github.com/sileod/tasksource). These annotations provide\nmetadata, like the name of the columns that should be used as input or labels\nfor all datasets, and can save time for future dataset preprocessings, even if\nthey do not use our framework. We fine-tune a multi-task text encoder on all\ntasksource tasks, outperforming every publicly available text encoder of\ncomparable size on an external evaluation\nhttps://hf.co/sileod/deberta-v3-base-tasksource-nli.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sileo_D/0/1/0/all/0/1\">Damien Sileo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noisy Parallel Data Alignment. (arXiv:2301.09685v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.09685","description":"<p>An ongoing challenge in current natural language processing is how its major\nadvancements tend to disproportionately favor resource-rich languages, leaving\na significant number of under-resourced languages behind. Due to the lack of\nresources required to train and evaluate models, most modern language\ntechnologies are either nonexistent or unreliable to process endangered, local,\nand non-standardized languages. Optical character recognition (OCR) is often\nused to convert endangered language documents into machine-readable data.\nHowever, such OCR output is typically noisy, and most word alignment models are\nnot built to work under such noisy conditions. In this work, we study the\nexisting word-level alignment models under noisy settings and aim to make them\nmore robust to noisy data. Our noise simulation and structural biasing method,\ntested on multiple language pairs, manages to reduce the alignment error rate\non a state-of-the-art neural-based alignment model up to 59.6%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Ruoyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViDeBERTa: A powerful pre-trained language model for Vietnamese. (arXiv:2301.10439v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.10439","description":"<p>This paper presents ViDeBERTa, a new pre-trained monolingual language model\nfor Vietnamese, with three versions - ViDeBERTa_xsmall, ViDeBERTa_base, and\nViDeBERTa_large, which are pre-trained on a large-scale corpus of high-quality\nand diverse Vietnamese texts using DeBERTa architecture. Although many\nsuccessful pre-trained language models based on Transformer have been widely\nproposed for the English language, there are still few pre-trained models for\nVietnamese, a low-resource language, that perform good results on downstream\ntasks, especially Question answering. We fine-tune and evaluate our model on\nthree important natural language downstream tasks, Part-of-speech tagging,\nNamed-entity recognition, and Question answering. The empirical results\ndemonstrate that ViDeBERTa with far fewer parameters surpasses the previous\nstate-of-the-art models on multiple Vietnamese-specific natural language\nunderstanding tasks. Notably, ViDeBERTa_base with 86M parameters, which is only\nabout 23% of PhoBERT_large with 370M parameters, still performs the same or\nbetter results than the previous state-of-the-art model. Our ViDeBERTa models\nare available at: https://github.com/HySonLab/ViDeBERTa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_C/0/1/0/all/0/1\">Cong Dao Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1\">Nhut Huy Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hy_T/0/1/0/all/0/1\">Truong Son Hy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tu Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Clarifying Question Generation for Conversational Search. (arXiv:2301.12660v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2301.12660","description":"<p>A long-standing challenge for search and conversational assistants is query\nintention detection in ambiguous queries. Asking clarifying questions in\nconversational search has been widely studied and considered an effective\nsolution to resolve query ambiguity. Existing work have explored various\napproaches for clarifying question ranking and generation. However, due to the\nlack of real conversational search data, they have to use artificial datasets\nfor training, which limits their generalizability to real-world search\nscenarios. As a result, the industry has shown reluctance to implement them in\nreality, further suspending the availability of real conversational search\ninteraction data. The above dilemma can be formulated as a cold start problem\nof clarifying question generation and conversational search in general.\nFurthermore, even if we do have large-scale conversational logs, it is not\nrealistic to gather training data that can comprehensively cover all possible\nqueries and topics in open-domain search scenarios. The risk of fitting bias\nwhen training a clarifying question retrieval/generation model on\nincomprehensive dataset is thus another important challenge.\n</p>\n<p>In this work, we innovatively explore generating clarifying questions in a\nzero-shot setting to overcome the cold start problem and we propose a\nconstrained clarifying question generation system which uses both question\ntemplates and query facets to guide the effective and precise question\ngeneration. The experiment results show that our method outperforms existing\nstate-of-the-art zero-shot baselines by a large margin. Human annotations to\nour model outputs also indicate our method generates 25.2\\% more natural\nquestions, 18.1\\% more useful questions, 6.1\\% less unnatural and 4\\% less\nuseless questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenduo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1\">Yuancheng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosset_C/0/1/0/all/0/1\">Corby Rosset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Craswell_N/0/1/0/all/0/1\">Nick Craswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Ming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1\">Qingyao Ai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Learning of Language Models. (arXiv:2302.03241v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.03241","description":"<p>Language models (LMs) have been instrumental for the rapid advance of natural\nlanguage processing. This paper studies continual learning of LMs, in\nparticular, continual domain-adaptive pre-training (or continual DAP-training).\nExisting research has shown that further pre-training an LM using a domain\ncorpus to adapt the LM to the domain can improve the end-task performance in\nthe domain. This paper proposes a novel method to continually DAP-train an LM\nwith a sequence of unlabeled domain corpora to adapt the LM to these domains to\nimprove their end-task performances. The key novelty of our method is a\nsoft-masking mechanism that directly controls the update to the LM. A novel\nproxy is also proposed to preserve the general knowledge in the original LM.\nAdditionally, it contrasts the representations of the previously learned domain\nknowledge (including the general knowledge in the pre-trained LM) and the\nknowledge from the current full network to achieve knowledge integration. The\nmethod not only overcomes catastrophic forgetting, but also achieves knowledge\ntransfer to improve end-task performances. Empirical evaluation demonstrates\nthe effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_Z/0/1/0/all/0/1\">Zixuan Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yijia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haowei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konishi_T/0/1/0/all/0/1\">Tatsuya Konishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyuhak Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Categorical Archive of ChatGPT Failures. (arXiv:2302.03494v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.03494","description":"<p>Large language models have been demonstrated to be valuable in different\nfields. ChatGPT, developed by OpenAI, has been trained using massive amounts of\ndata and simulates human conversation by comprehending context and generating\nappropriate responses. It has garnered significant attention due to its ability\nto effectively answer a broad range of human inquiries, with fluent and\ncomprehensive answers surpassing prior public chatbots in both security and\nusefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,\nwhich is the focus of this study. Ten categories of failures, including\nreasoning, factual errors, math, coding, and bias, are presented and discussed.\nThe risks, limitations, and societal implications of ChatGPT are also\nhighlighted. The goal of this study is to assist researchers and developers in\nenhancing future language models and chatbots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1\">Ali Borji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reliable Natural Language Understanding with Large Language Models and Answer Set Programming. (arXiv:2302.03780v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.03780","description":"<p>Humans understand language by extracting information (meaning) from\nsentences, combining it with existing commonsense knowledge, and then\nperforming reasoning to draw conclusions. While large language models (LLMs)\nsuch as GPT-3 and ChatGPT are able to leverage patterns in the text to solve a\nvariety of NLP tasks, they fall short in problems that require reasoning. They\nalso cannot reliably explain the answers generated for a given question. In\norder to emulate humans better, we propose STAR, a framework that combines LLMs\nwith Answer Set Programming (ASP). We show how LLMs can be used to effectively\nextract knowledge -- represented as predicates -- from language. Goal-directed\nASP is then employed to reliably reason over this knowledge. We apply the STAR\nframework to three different NLU tasks requiring reasoning: qualitative\nreasoning, mathematical reasoning, and goal-directed conversation. Our\nexperiments reveal that STAR is able to bridge the gap of reasoning in NLU\ntasks, leading to significant performance improvements, especially for smaller\nLLMs, i.e., LLMs with a smaller number of parameters. NLU applications\ndeveloped using the STAR framework are also explainable: along with the\npredicates generated, a justification in the form of a proof tree can be\nproduced for a given output.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajasekharan_A/0/1/0/all/0/1\">Abhiramon Rajasekharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yankai Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padalkar_P/0/1/0/all/0/1\">Parth Padalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1\">Gopal Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Inferential Reproducibility of Machine Learning Research. (arXiv:2302.04054v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.04054","description":"<p>Reliability of machine learning evaluation -- the consistency of observed\nevaluation scores across replicated model training runs -- is affected by\nseveral sources of nondeterminism which can be regarded as measurement noise.\nCurrent tendencies to remove noise in order to enforce reproducibility of\nresearch results neglect inherent nondeterminism at the implementation level\nand disregard crucial interaction effects between algorithmic noise factors and\ndata properties. This limits the scope of conclusions that can be drawn from\nsuch experiments. Instead of removing noise, we propose to incorporate several\nsources of variance, including their interaction with data properties, into an\nanalysis of significance and reliability of machine learning evaluation, with\nthe aim to draw inferences beyond particular instances of trained models. We\nshow how to use linear mixed effects models (LMEMs) to analyze performance\nevaluation scores, and to conduct statistical inference with a generalized\nlikelihood ratio test (GLRT). This allows us to incorporate arbitrary sources\nof noise like meta-parameter variations into statistical significance testing,\nand to assess performance differences conditional on data properties.\nFurthermore, a variance component analysis (VCA) enables the analysis of the\ncontribution of noise sources to overall variance and the computation of a\nreliability coefficient by the ratio of substantial to total variance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagmann_M/0/1/0/all/0/1\">Michael Hagmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1\">Stefan Riezler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-02-12T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}}]}]}