{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-11-24T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Is the Elephant Flying? Resolving Ambiguities in Text-to-Image Generative Models. (arXiv:2211.12503v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12503","description":"<p>Natural language often contains ambiguities that can lead to\nmisinterpretation and miscommunication. While humans can handle ambiguities\neffectively by asking clarifying questions and/or relying on contextual cues\nand common-sense knowledge, resolving ambiguities can be notoriously hard for\nmachines. In this work, we study ambiguities that arise in text-to-image\ngenerative models. We curate a benchmark dataset covering different types of\nambiguities that occur in these systems. We then propose a framework to\nmitigate ambiguities in the prompts given to the systems by soliciting\nclarifications from the user. Through automatic and human evaluations, we show\nthe effectiveness of our framework in generating more faithful images aligned\nwith human intention in the presence of ambiguities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehrabi_N/0/1/0/all/0/1\">Ninareh Mehrabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Palash Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_A/0/1/0/all/0/1\">Apurv Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamala_J/0/1/0/all/0/1\">Jwala Dhamala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying gender bias in blockbuster movies through the lens of machine learning. (arXiv:2211.12504v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12504","description":"<p>The problem of gender bias is highly prevalent and well known. In this paper,\nwe have analysed the portrayal of gender roles in English movies, a medium that\neffectively influences society in shaping people's beliefs and opinions. First,\nwe gathered scripts of films from different genres and derived sentiments and\nemotions using natural language processing techniques. Afterwards, we converted\nthe scripts into embeddings, i.e. a way of representing text in the form of\nvectors. With a thorough investigation, we found specific patterns in male and\nfemale characters' personality traits in movies that align with societal\nstereotypes. Furthermore, we used mathematical and machine learning techniques\nand found some biases wherein men are shown to be more dominant and envious\nthan women, whereas women have more joyful roles in movies. In our work, we\nintroduce, to the best of our knowledge, a novel technique to convert dialogues\ninto an array of emotions by combining it with Plutchik's wheel of emotions.\nOur study aims to encourage reflections on gender equality in the domain of\nfilm and facilitate other researchers in analysing movies automatically instead\nof using manual approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haris_M/0/1/0/all/0/1\">Muhammad Junaid Haris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upreti_A/0/1/0/all/0/1\">Aanchal Upreti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurtaran_M/0/1/0/all/0/1\">Melih Kurtaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginter_F/0/1/0/all/0/1\">Filip Ginter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lafond_S/0/1/0/all/0/1\">Sebastien Lafond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azimi_S/0/1/0/all/0/1\">Sepinoud Azimi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time-Aware Datasets are Adaptive Knowledgebases for the New Normal. (arXiv:2211.12508v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12508","description":"<p>Recent advances in text classification and knowledge capture in language\nmodels have relied on availability of large-scale text datasets. However,\nlanguage models are trained on static snapshots of knowledge and are limited\nwhen that knowledge evolves. This is especially critical for misinformation\ndetection, where new types of misinformation continuously appear, replacing old\ncampaigns. We propose time-aware misinformation datasets to capture\ntime-critical phenomena. In this paper, we first present evidence of evolving\nmisinformation and show that incorporating even simple time-awareness\nsignificantly improves classifier accuracy. Second, we present COVID-TAD, a\nlarge-scale COVID-19 misinformation da-taset spanning 25 months. It is the\nfirst large-scale misinformation dataset that contains multiple snapshots of a\ndatastream and is orders of magnitude bigger than related misinformation\ndatasets. We describe the collection and labeling pro-cess, as well as\npreliminary experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suprem_A/0/1/0/all/0/1\">Abhijit Suprem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaidya_S/0/1/0/all/0/1\">Sanjyot Vaidya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_J/0/1/0/all/0/1\">Joao Eduardo Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_C/0/1/0/all/0/1\">Calton Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLP meets psychotherapy: Using predicted client emotions and self-reported client emotions to measure emotional coherence. (arXiv:2211.12512v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12512","description":"<p>Emotions are experienced and expressed through various response systems.\nCoherence between emotional experience and emotional expression is considered\nimportant to clients' well being. To date, emotional coherence (EC) has been\nstudied at a single time point using lab-based tasks with relatively small\ndatasets. No study has examined EC between the subjective experience of\nemotions and emotion expression in therapy or whether this coherence is\nassociated with clients' well being. Natural language Processing (NLP)\napproaches have been applied to identify emotions from psychotherapy dialogue,\nwhich can be implemented to study emotional processes on a larger scale.\nHowever, these methods have yet to be used to study coherence between emotional\nexperience and emotional expression over the course of therapy and whether it\nrelates to clients' well-being. This work presents an end-to-end approach where\nwe use emotion predictions from our transformer based emotion recognition model\nto study emotional coherence and its diagnostic potential in psychotherapy\nresearch. We first employ our transformer based approach on a Hebrew\npsychotherapy dataset to automatically label clients' emotions at utterance\nlevel in psychotherapy dialogues. We subsequently investigate the emotional\ncoherence between clients' self-reported emotional states and our model-based\nemotion predictions. We also examine the association between emotional\ncoherence and clients' well being. Our findings indicate a significant\ncorrelation between clients' self-reported emotions and positive and negative\nemotions expressed verbally during psychotherapy sessions. Coherence in\npositive emotions was also highly correlated with clients well-being. These\nresults illustrate how NLP can be applied to identify important emotional\nprocesses in psychotherapy to improve diagnosis and treatment for clients\nsuffering from mental-health problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Warikoo_N/0/1/0/all/0/1\">Neha Warikoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayer_T/0/1/0/all/0/1\">Tobias Mayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atzil_Slonim_D/0/1/0/all/0/1\">Dana Atzil-Slonim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eliassaf_A/0/1/0/all/0/1\">Amir Eliassaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haimovitz_S/0/1/0/all/0/1\">Shira Haimovitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smart Agriculture : A Novel Multilevel Approach for Agricultural Risk Assessment over Unstructured Data. (arXiv:2211.12515v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12515","description":"<p>Detecting opportunities and threats from massive text data is a challenging\ntask for most. Traditionally, companies would rely mainly on structured data to\ndetect and predict risks, losing a huge amount of information that could be\nextracted from unstructured text data. Fortunately, artificial intelligence\ncame to remedy this issue by innovating in data extraction and processing\ntechniques, allowing us to understand and make use of Natural Language data and\nturning it into structures that a machine can process and extract insight from.\nUncertainty refers to a state of not knowing what will happen in the future.\nThis paper aims to leverage natural language processing and machine learning\ntechniques to model uncertainties and evaluate the risk level in each\nuncertainty cluster using massive text data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Najmi_H/0/1/0/all/0/1\">Hasna Najmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikram_M/0/1/0/all/0/1\">Mounia Mikram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhanoui_M/0/1/0/all/0/1\">Maryem Rhanoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yousfi_S/0/1/0/all/0/1\">Siham Yousfi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval-Augmented Multimodal Language Modeling. (arXiv:2211.12561v1 [cs.CV])","link":"http://arxiv.org/abs/2211.12561","description":"<p>Recent multimodal models such as DALL-E and CM3 have achieved remarkable\nprogress in text-to-image and image-to-text generation. However, these models\nstore all learned knowledge (e.g., the appearance of the Eiffel Tower) in the\nmodel parameters, requiring increasingly larger models and training data to\ncapture more knowledge. To integrate knowledge in a more scalable and modular\nway, we propose a retrieval-augmented multimodal model, which enables a base\nmultimodal model (generator) to refer to relevant knowledge fetched by a\nretriever from external memory (e.g., multimodal documents on the web).\nSpecifically, we implement a retriever using the pretrained CLIP model and a\ngenerator using the CM3 Transformer architecture, and train this model using\nthe LAION dataset. Our resulting model, named Retrieval-Augmented CM3 (RA-CM3),\nis the first multimodal model that can retrieve and generate mixtures of text\nand images. We show that RA-CM3 significantly outperforms baseline multimodal\nmodels such as DALL-E and CM3 on both image and caption generation tasks (12\nFID and 17 CIDEr improvements on MS-COCO), while requiring much less compute\nfor training (&lt;30% of DALL-E). Moreover, we show that RA-CM3 exhibits novel\ncapabilities such as knowledge-intensive image generation and multimodal\nin-context learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weijia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_R/0/1/0/all/0/1\">Rich James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting the Type and Target of Offensive Social Media Posts in Marathi. (arXiv:2211.12570v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12570","description":"<p>The presence of offensive language on social media is very common motivating\nplatforms to invest in strategies to make communities safer. This includes\ndeveloping robust machine learning systems capable of recognizing offensive\ncontent online. Apart from a few notable exceptions, most research on automatic\noffensive language identification has dealt with English and a few other high\nresource languages such as French, German, and Spanish. In this paper we\naddress this gap by tackling offensive language identification in Marathi, a\nlow-resource Indo-Aryan language spoken in India. We introduce the Marathi\nOffensive Language Dataset v.2.0 or MOLD 2.0 and present multiple experiments\non this dataset. MOLD 2.0 is a much larger version of MOLD with expanded\nannotation to the levels B (type) and C (target) of the popular OLID taxonomy.\nMOLD 2.0 is the first hierarchical offensive language dataset compiled for\nMarathi, thus opening new avenues for research in low-resource Indo-Aryan\nlanguages. Finally, we also introduce SeMOLD, a larger dataset annotated\nfollowing the semi-supervised methods presented in SOLID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhari_M/0/1/0/all/0/1\">Mrinal Chaudhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaikwad_S/0/1/0/all/0/1\">Saurabh Gaikwad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_P/0/1/0/all/0/1\">Prajwal Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nene_M/0/1/0/all/0/1\">Mayuresh Nene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paygude_S/0/1/0/all/0/1\">Shrunali Paygude</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks. (arXiv:2211.12588v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12588","description":"<p>Recently, there has been significant progress in teaching language models to\nperform step-by-step reasoning to solve complex numerical reasoning tasks.\nChain-of-thoughts prompting (CoT) is by far the state-of-art method for these\ntasks. CoT uses language models to perform both reasoning and computation in\nthe multi-step `thought' process. To disentangle computation from reasoning, we\npropose `Program of Thoughts' (PoT), which uses language models (mainly Codex)\nto express the reasoning process as a program. The computation is relegated to\nan external computer, which executes the generated programs to derive the\nanswer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP,\nTabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA)\nfor both few-shot and zero-shot setups. Under both few-shot and zero-shot\nsettings, PoT can show an average performance gain over CoT by around 12\\%\nacross all the evaluated datasets. By combining PoT with self-consistency\ndecoding, we can achieve SoTA performance on all math problem datasets and\nnear-SoTA performance on financial datasets. All of our data and code are\nreleased in\nGithub\\footnote{\\url{https://github.com/wenhuchen/Program-of-Thoughts}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xueguang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoReply: Detecting Nonsense in Dialogue Introspectively with Discriminative Replies. (arXiv:2211.12615v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12615","description":"<p>Existing approaches built separate classifiers to detect nonsense in\ndialogues. In this paper, we show that without external classifiers, dialogue\nmodels can detect errors in their own messages introspectively, by calculating\nthe likelihood of replies that are indicative of poor messages. For example, if\nan agent believes its partner is likely to respond \"I don't understand\" to a\ncandidate message, that message may not make sense, so an alternative message\nshould be chosen. We evaluate our approach on a dataset from the game\nDiplomacy, which contains long dialogues richly grounded in the game state, on\nwhich existing models make many errors. We first show that hand-crafted replies\ncan be effective for the task of detecting nonsense in applications as complex\nas Diplomacy. We then design AutoReply, an algorithm to search for such\ndiscriminative replies automatically, given a small number of annotated\ndialogue examples. We find that AutoReply-generated replies outperform\nhandcrafted replies and perform on par with carefully fine-tuned large\nsupervised models. Results also show that one single reply without much\ncomputation overheads can also detect dialogue nonsense reasonably well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weiyan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinan_E/0/1/0/all/0/1\">Emily Dinan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renduchintala_A/0/1/0/all/0/1\">Adi Renduchintala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1\">Daniel Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacob_A/0/1/0/all/0/1\">Athul Paul Jacob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Data Recasting to Enhance Tabular Reasoning. (arXiv:2211.12641v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12641","description":"<p>Creating challenging tabular inference data is essential for learning complex\nreasoning. Prior work has mostly relied on two data generation strategies. The\nfirst is human annotation, which yields linguistically diverse data but is\ndifficult to scale. The second category for creation is synthetic generation,\nwhich is scalable and cost effective but lacks inventiveness. In this research,\nwe present a framework for semi-automatically recasting existing tabular data\nto make use of the benefits of both approaches. We utilize our framework to\nbuild tabular NLI instances from five datasets that were initially intended for\ntasks like table2text creation, tabular Q/A, and semantic parsing. We\ndemonstrate that recasted data could be used as evaluation benchmarks as well\nas augmentation data to enhance performance on tabular NLI tasks. Furthermore,\nwe investigate the effectiveness of models trained on recasted data in the\nzero-shot scenario, and analyse trends in performance across different recasted\ndatasets types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jena_A/0/1/0/all/0/1\">Aashna Jena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vivek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1\">Manish Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenschlos_J/0/1/0/all/0/1\">Julian Martin Eisenschlos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DyRRen: A Dynamic Retriever-Reranker-Generator Model for Numerical Reasoning over Tabular and Textual Data. (arXiv:2211.12668v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12668","description":"<p>Numerical reasoning over hybrid data containing tables and long texts has\nrecently received research attention from the AI community. To generate an\nexecutable reasoning program consisting of math and table operations to answer\na question, state-of-the-art methods use a retriever-generator pipeline.\nHowever, their retrieval results are static, while different generation steps\nmay rely on different sentences. To attend to the retrieved information that is\nrelevant to each generation step, in this paper, we propose DyRRen, an extended\nretriever-reranker-generator framework where each generation step is enhanced\nby a dynamic reranking of retrieved sentences. It outperforms existing\nbaselines on the FinQA dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sichen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_J/0/1/0/all/0/1\">Jiangzhou Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yuzhong Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gong Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word-Level Representation From Bytes For Language Modeling. (arXiv:2211.12677v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12677","description":"<p>Modern language models mostly take sub-words as input, a design that balances\nthe trade-off between vocabulary size, number of parameters, and performance.\nHowever, sub-word tokenization still has disadvantages like not being robust to\nnoise and difficult to generalize to new languages. Also, the current trend of\nscaling up models reveals that larger models require larger embeddings but that\nmakes parallelization hard. Previous work on image classification proves\nsplitting raw input into a sequence of chucks is a strong, model-agnostic\ninductive bias. Based on this observation, we rethink the existing\ncharacter-aware method that takes character-level inputs but makes word-level\nsequence modeling and prediction. We overhaul this method by introducing a\ncross-attention network that builds word-level representation directly from\nbytes, and a sub-word level prediction based on word-level hidden states to\navoid the time and space requirement of word-level prediction. With these two\nimprovements combined, we have a token free model with slim input embeddings\nfor downstream tasks. We name our method Byte2Word and perform evaluations on\nlanguage modeling and text classification. Experiments show that Byte2Word is\non par with the strong sub-word baseline BERT but only takes up 10\\% of\nembedding size. We further test our method on synthetic noise and cross-lingual\ntransfer and find it competitive to baseline methods on both settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chu-Tak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qipeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Learning of Natural Language Processing Tasks: A Survey. (arXiv:2211.12701v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12701","description":"<p>Continual learning (CL) is an emerging learning paradigm that aims to emulate\nthe human capability of learning and accumulating knowledge continually without\nforgetting the previously learned knowledge and also transferring the knowledge\nto new tasks to learn them better. This survey presents a comprehensive review\nof the recent progress of CL in the NLP field. It covers (1) all CL settings\nwith a taxonomy of existing techniques. Besides dealing with forgetting, it\nalso focuses on (2) knowledge transfer, which is of particular importance to\nNLP. Both (1) and (2) are not mentioned in the existing survey. Finally, a list\nof future directions is also discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_Z/0/1/0/all/0/1\">Zixuan Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Open-Domain QA Reader Utilize External Knowledge Efficiently like Humans?. (arXiv:2211.12707v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12707","description":"<p>Recent state-of-the-art open-domain QA models are typically based on a two\nstage retriever-reader approach in which the retriever first finds the relevant\nknowledge/passages and the reader then leverages that to predict the answer.\nPrior work has shown that the performance of the reader usually tends to\nimprove with the increase in the number of these passages. Thus,\nstate-of-the-art models use a large number of passages (e.g. 100) for\ninference. While the reader in this approach achieves high prediction\nperformance, its inference is computationally very expensive. We humans, on the\nother hand, use a more efficient strategy while answering: firstly, if we can\nconfidently answer the question using our already acquired knowledge then we do\nnot even use the external knowledge, and in the case when we do require\nexternal knowledge, we don't read the entire knowledge at once, instead, we\nonly read that much knowledge that is sufficient to find the answer. Motivated\nby this procedure, we ask a research question \"Can the open-domain QA reader\nutilize external knowledge efficiently like humans without sacrificing the\nprediction performance?\"\n</p>\n<p>Driven by this question, we explore an approach that utilizes both\n'closed-book' (leveraging knowledge already present in the model parameters)\nand 'open-book' inference (leveraging external knowledge). Furthermore, instead\nof using a large fixed number of passages for open-book inference, we\ndynamically read the external knowledge in multiple 'knowledge iterations'.\nThrough comprehensive experiments on NQ and TriviaQA datasets, we demonstrate\nthat this dynamic reading approach improves both the 'inference efficiency' and\nthe 'prediction accuracy' of the reader. Comparing with the FiD reader, this\napproach matches its accuracy by utilizing just 18.32% of its reader inference\ncost and also outperforms it by achieving up to 55.10% accuracy on NQ Open.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Man Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embedding Compression for Text Classification Using Dictionary Screening. (arXiv:2211.12715v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12715","description":"<p>In this paper, we propose a dictionary screening method for embedding\ncompression in text classification tasks. The key purpose of this method is to\nevaluate the importance of each keyword in the dictionary. To this end, we\nfirst train a pre-specified recurrent neural network-based model using a full\ndictionary. This leads to a benchmark model, which we then use to obtain the\npredicted class probabilities for each sample in a dataset. Next, to evaluate\nthe impact of each keyword in affecting the predicted class probabilities, we\ndevelop a novel method for assessing the importance of each keyword in a\ndictionary. Consequently, each keyword can be screened, and only the most\nimportant keywords are reserved. With these screened keywords, a new dictionary\nwith a considerably reduced size can be constructed. Accordingly, the original\ntext sequence can be substantially compressed. The proposed method leads to\nsignificant reductions in terms of parameters, average text sequence, and\ndictionary size. Meanwhile, the prediction power remains very competitive\ncompared to the benchmark model. Extensive numerical studies are presented to\ndemonstrate the empirical performance of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_X/0/1/0/all/0/1\">Xinru Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Muyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hansheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoentGen: Vision-Language Foundation Model for Chest X-ray Generation. (arXiv:2211.12737v1 [cs.CV])","link":"http://arxiv.org/abs/2211.12737","description":"<p>Multimodal models trained on large natural image-text pair datasets have\nexhibited astounding abilities in generating high-quality images. Medical\nimaging data is fundamentally different to natural images, and the language\nused to succinctly capture relevant details in medical data uses a different,\nnarrow but semantically rich, domain-specific vocabulary. Not surprisingly,\nmulti-modal models trained on natural image-text pairs do not tend to\ngeneralize well to the medical domain. Developing generative imaging models\nfaithfully representing medical concepts while providing compositional\ndiversity could mitigate the existing paucity of high-quality, annotated\nmedical imaging datasets. In this work, we develop a strategy to overcome the\nlarge natural-medical distributional shift by adapting a pre-trained latent\ndiffusion model on a corpus of publicly available chest x-rays (CXR) and their\ncorresponding radiology (text) reports. We investigate the model's ability to\ngenerate high-fidelity, diverse synthetic CXR conditioned on text prompts. We\nassess the model outputs quantitatively using image quality metrics, and\nevaluate image quality and text-image alignment by human domain experts. We\npresent evidence that the resulting model (RoentGen) is able to create visually\nconvincing, diverse synthetic CXR images, and that the output can be controlled\nto a new extent by using free-form text prompts including radiology-specific\nlanguage. Fine-tuning this model on a fixed training set and using it as a data\naugmentation method, we measure a 5% improvement of a classifier trained\njointly on synthetic and real images, and a 3% improvement when trained on a\nlarger but purely synthetic training set. Finally, we observe that this\nfine-tuning distills in-domain knowledge in the text-encoder and can improve\nits representation capabilities of certain diseases like pneumothorax by 25%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1\">Pierre Chambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bluethgen_C/0/1/0/all/0/1\">Christian Bluethgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delbrouck_J/0/1/0/all/0/1\">Jean-Benoit Delbrouck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sluijs_R/0/1/0/all/0/1\">Rogier Van der Sluijs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polacin_M/0/1/0/all/0/1\">Ma&#x142;gorzata Po&#x142;acin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaves_J/0/1/0/all/0/1\">Juan Manuel Zambrano Chaves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abraham_T/0/1/0/all/0/1\">Tanishq Mathew Abraham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purohit_S/0/1/0/all/0/1\">Shivanshu Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curtis P. Langlotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhari_A/0/1/0/all/0/1\">Akshay Chaudhari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Agent-Specific Deontic Modality Detection in Legal Language. (arXiv:2211.12752v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12752","description":"<p>Legal documents are typically long and written in legalese, which makes it\nparticularly difficult for laypeople to understand their rights and duties.\nWhile natural language understanding technologies can be valuable in supporting\nsuch understanding in the legal domain, the limited availability of datasets\nannotated for deontic modalities in the legal domain, due to the cost of hiring\nexperts and privacy issues, is a bottleneck. To this end, we introduce,\nLEXDEMOD, a corpus of English contracts annotated with deontic modality\nexpressed with respect to a contracting party or agent along with the modal\ntriggers. We benchmark this dataset on two tasks: (i) agent-specific\nmulti-label deontic modality classification, and (ii) agent-specific deontic\nmodality and trigger span detection using Transformer-based (Vaswani et al.,\n2017) language models. Transfer learning experiments show that the linguistic\ndiversity of modal expressions in LEXDEMOD generalizes reasonably from lease to\nemployment and rental agreements. A small case study indicates that a model\ntrained on LEXDEMOD can detect red flags with high recall. We believe our work\noffers a new research direction for deontic modality detection in the legal\ndomain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sancheti_A/0/1/0/all/0/1\">Abhilasha Sancheti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garimella_A/0/1/0/all/0/1\">Aparna Garimella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_B/0/1/0/all/0/1\">Balaji Vasan Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudinger_R/0/1/0/all/0/1\">Rachel Rudinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval. (arXiv:2211.12764v1 [cs.CV])","link":"http://arxiv.org/abs/2211.12764","description":"<p>Many recent studies leverage the pre-trained CLIP for text-video cross-modal\nretrieval by tuning the backbone with additional heavy modules, which not only\nbrings huge computational burdens with much more parameters, but also leads to\nthe knowledge forgetting from upstream models.In this work, we propose the VoP:\nText-Video Co-operative Prompt Tuning for efficient tuning on the text-video\nretrieval task. The proposed VoP is an end-to-end framework with both video &amp;\ntext prompts introducing, which can be regarded as a powerful baseline with\nonly 0.1% trainable parameters. Further, based on the spatio-temporal\ncharacteristics of videos, we develop three novel video prompt mechanisms to\nimprove the performance with different scales of trainable parameters. The\nbasic idea of the VoP enhancement is to model the frame position, frame\ncontext, and layer function with specific trainable prompts, respectively.\nExtensive experiments show that compared to full fine-tuning, the enhanced VoP\nachieves a 1.4% average R@1 gain across five text-video retrieval benchmarks\nwith 6x less parameter overhead. The code will be available at\nhttps://github.com/bighuang624/VoP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siteng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Biao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yulin Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jianwen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yiliang Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Donglin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breaking the Representation Bottleneck of Chinese Characters: Neural Machine Translation with Stroke Sequence Modeling. (arXiv:2211.12781v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12781","description":"<p>Existing research generally treats Chinese character as a minimum unit for\nrepresentation. However, such Chinese character representation will suffer two\nbottlenecks: 1) Learning bottleneck, the learning cannot benefit from its rich\ninternal features (e.g., radicals and strokes); and 2) Parameter bottleneck,\neach individual character has to be represented by a unique vector. In this\npaper, we introduce a novel representation method for Chinese characters to\nbreak the bottlenecks, namely StrokeNet, which represents a Chinese character\nby a Latinized stroke sequence (e.g., \"ao1 (concave)\" to \"ajaie\" and \"tu1\n(convex)\" to \"aeaqe\"). Specifically, StrokeNet maps each stroke to a specific\nLatin character, thus allowing similar Chinese characters to have similar Latin\nrepresentations. With the introduction of StrokeNet to neural machine\ntranslation (NMT), many powerful but not applicable techniques to non-Latin\nlanguages (e.g., shared subword vocabulary learning and ciphertext-based data\naugmentation) can now be perfectly implemented. Experiments on the widely-used\nNIST Chinese-English, WMT17 Chinese-English and IWSLT17 Japanese-English NMT\ntasks show that StrokeNet can provide a significant performance boost over the\nstrong baselines with fewer model parameters, achieving 26.5 BLEU on the WMT17\nChinese-English task which is better than any previously reported results\nwithout using monolingual data. Code and scripts are freely available at\nhttps://github.com/zjwang21/StrokeNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhijun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IMaSC -- ICFOSS Malayalam Speech Corpus. (arXiv:2211.12796v1 [cs.SD])","link":"http://arxiv.org/abs/2211.12796","description":"<p>Modern text-to-speech (TTS) systems use deep learning to synthesize speech\nincreasingly approaching human quality, but they require a database of high\nquality audio-text sentence pairs for training. Malayalam, the official\nlanguage of the Indian state of Kerala and spoken by 35+ million people, is a\nlow resource language in terms of available corpora for TTS systems. In this\npaper, we present IMaSC, a Malayalam text and speech corpora containing\napproximately 50 hours of recorded speech. With 8 speakers and a total of\n34,473 text-audio pairs, IMaSC is larger than every other publicly available\nalternative. We evaluated the database by using it to train TTS models for each\nspeaker based on a modern deep learning architecture. Via subjective\nevaluation, we show that our models perform significantly better in terms of\nnaturalness compared to previous studies and publicly available models, with an\naverage mean opinion score of 4.50, indicating that the synthesized speech is\nclose to human quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gopinath_D/0/1/0/all/0/1\">Deepa P Gopinath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_T/0/1/0/all/0/1\">Thennal D K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_V/0/1/0/all/0/1\">Vrinda V Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+S_S/0/1/0/all/0/1\">Swaraj K S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+G_S/0/1/0/all/0/1\">Sachin G</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tell Me What Happened: Unifying Text-guided Video Completion via Multimodal Masked Video Generation. (arXiv:2211.12824v1 [cs.CV])","link":"http://arxiv.org/abs/2211.12824","description":"<p>Generating a video given the first several static frames is challenging as it\nanticipates reasonable future frames with temporal coherence. Besides video\nprediction, the ability to rewind from the last frame or infilling between the\nhead and tail is also crucial, but they have rarely been explored for video\ncompletion. Since there could be different outcomes from the hints of just a\nfew frames, a system that can follow natural language to perform video\ncompletion may significantly improve controllability. Inspired by this, we\nintroduce a novel task, text-guided video completion (TVC), which requests the\nmodel to generate a video from partial frames guided by an instruction. We then\npropose Multimodal Masked Video Generation (MMVG) to address this TVC task.\nDuring training, MMVG discretizes the video frames into visual tokens and masks\nmost of them to perform video completion from any time point. At inference\ntime, a single MMVG model can address all 3 cases of TVC, including video\nprediction, rewind, and infilling, by applying corresponding masking\nconditions. We evaluate MMVG in various video scenarios, including egocentric,\nanimation, and gaming. Extensive experimental results indicate that MMVG is\neffective in generating high-quality visual appearances with text guidance for\nTVC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1\">Tsu-Jui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Licheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Cheng-Yang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jong-Chyi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bell_S/0/1/0/all/0/1\">Sean Bell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Generation of Socratic Subquestions for Teaching Math Word Problems. (arXiv:2211.12835v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12835","description":"<p>Socratic questioning is an educational method that allows students to\ndiscover answers to complex problems by asking them a series of thoughtful\nquestions. Generation of didactically sound questions is challenging, requiring\nunderstanding of the reasoning process involved in the problem. We hypothesize\nthat such questioning strategy can not only enhance the human performance, but\nalso assist the math word problem (MWP) solvers. In this work, we explore the\nability of large language models (LMs) in generating sequential questions for\nguiding math word problem-solving. We propose various guided question\ngeneration schemes based on input conditioning and reinforcement learning. On\nboth automatic and human quality evaluations, we find that LMs constrained with\ndesirable question properties generate superior questions and improve the\noverall performance of a math word problem solver. We conduct a preliminary\nuser study to examine the potential value of such question generation models in\nthe education domain. Results suggest that the difficulty level of problems\nplays an important role in determining whether questioning improves or hinders\nhuman performance. We discuss the future of using such questioning strategies\nin education.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shridhar_K/0/1/0/all/0/1\">Kumar Shridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macina_J/0/1/0/all/0/1\">Jakub Macina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Assady_M/0/1/0/all/0/1\">Mennatallah El-Assady</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_T/0/1/0/all/0/1\">Tanmay Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapur_M/0/1/0/all/0/1\">Manu Kapur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GraphWOZ: Dialogue Management with Conversational Knowledge Graphs. (arXiv:2211.12852v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12852","description":"<p>We present a new approach to dialogue management using conversational\nknowledge graphs as core representation of the dialogue state. To this end, we\nintroduce a new dataset, GraphWOZ, which comprises Wizard-of-Oz dialogues in\nwhich human participants interact with a robot acting as a receptionist. In\ncontrast to most existing work on dialogue management, GraphWOZ relies on a\ndialogue state explicitly represented as a dynamic knowledge graph instead of a\nfixed set of slots. This graph is composed of a varying number of entities\n(such as individuals, places, events, utterances and mentions) and relations\nbetween them (such as persons being part of a group or attending an event). The\ngraph is then regularly updated on the basis of new observations and system\nactions. GraphWOZ is released along with detailed manual annotations related to\nthe user intents, system responses, and reference relations occurring in both\nuser and system turns. Based on GraphWOZ, we present experimental results for\ntwo dialogue management tasks, namely conversational entity linking and\nresponse ranking. For conversational entity linking, we show how to connect\nutterance mentions to their corresponding entity in the knowledge graph with a\nneural model relying on a combination of both string and graph-based features.\nResponse ranking is then performed by summarizing the relevant content of the\ngraph into a text, which is concatenated with the dialogue history and employed\nas input to score possible responses to a given dialogue state.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Walker_N/0/1/0/all/0/1\">Nicholas Thomas Walker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ultes_S/0/1/0/all/0/1\">Stefan Ultes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lison_P/0/1/0/all/0/1\">Pierre Lison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Data Sparsity for Short Text Topic Modeling by Topic-Semantic Contrastive Learning. (arXiv:2211.12878v1 [cs.CL])","link":"http://arxiv.org/abs/2211.12878","description":"<p>To overcome the data sparsity issue in short text topic modeling, existing\nmethods commonly rely on data augmentation or the data characteristic of short\ntexts to introduce more word co-occurrence information. However, most of them\ndo not make full use of the augmented data or the data characteristic: they\ninsufficiently learn the relations among samples in data, leading to dissimilar\ntopic distributions of semantically similar text pairs. To better address data\nsparsity, in this paper we propose a novel short text topic modeling framework,\nTopic-Semantic Contrastive Topic Model (TSCTM). To sufficiently model the\nrelations among samples, we employ a new contrastive learning method with\nefficient positive and negative sampling strategies based on topic semantics.\nThis contrastive learning method refines the representations, enriches the\nlearning signals, and thus mitigates the sparsity issue. Extensive experimental\nresults show that our TSCTM outperforms state-of-the-art baselines regardless\nof the data augmentation availability, producing high-quality topics and topic\ndistributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaobao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1\">Anh Tuan Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xinshuai Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Conspiracy Theory Against COVID-19 Vaccines. (arXiv:2211.13003v1 [cs.CY])","link":"http://arxiv.org/abs/2211.13003","description":"<p>Since the beginning of the vaccination trial, social media has been flooded\nwith anti-vaccination comments and conspiracy beliefs. As the day passes, the\nnumber of COVID- 19 cases increases, and online platforms and a few news\nportals entertain sharing different conspiracy theories. The most popular\nconspiracy belief was the link between the 5G network spreading COVID-19 and\nthe Chinese government spreading the virus as a bioweapon, which initially\ncreated racial hatred. Although some disbelief has less impact on society,\nothers create massive destruction. For example, the 5G conspiracy led to the\nburn of the 5G Tower, and belief in the Chinese bioweapon story promoted an\nattack on the Asian-Americans. Another popular conspiracy belief was that Bill\nGates spread this Coronavirus disease (COVID-19) by launching a mass\nvaccination program to track everyone. This Conspiracy belief creates distrust\nissues among laypeople and creates vaccine hesitancy. This study aims to\ndiscover the conspiracy theory against the vaccine on social platforms. We\nperformed a sentiment analysis on the 598 unique sample comments related to\nCOVID-19 vaccines. We used two different models, BERT and Perspective API, to\nfind out the sentiment and toxicity of the sentence toward the COVID-19\nvaccine.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1\">Md Hasibul Amin</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Madanu_H/0/1/0/all/0/1\">Harika Madanu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lavu_S/0/1/0/all/0/1\">Sahithi Lavu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Mansourifar_H/0/1/0/all/0/1\">Hadi Mansourifar</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Alsagheer_D/0/1/0/all/0/1\">Dana Alsagheer</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weidong Shi</a> (1) ((1) University Of Houston)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sarcasm Detection Framework Using Emotion and Sentiment Features. (arXiv:2211.13014v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13014","description":"<p>Sarcasm detection is an essential task that can help identify the actual\nsentiment in user-generated data, such as discussion forums or tweets. Sarcasm\nis a sophisticated form of linguistic expression because its surface meaning\nusually contradicts its inner, deeper meaning. Such incongruity is the\nessential component of sarcasm, however, it makes sarcasm detection quite a\nchallenging task. In this paper, we propose a model which incorporates emotion\nand sentiment features to capture the incongruity intrinsic to sarcasm.\nMoreover, we use CNN and pre-trained Transformer to capture context features.\nOur approach achieved state-of-the-art results on four datasets from social\nnetworking platforms and online media.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vitman_O/0/1/0/all/0/1\">Oxana Vitman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kostiuk_Y/0/1/0/all/0/1\">Yevhen Kostiuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidorov_G/0/1/0/all/0/1\">Grigori Sidorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1\">Alexander Gelbukh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Lifelong Language Learning. (arXiv:2211.13050v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13050","description":"<p>Lifelong learning aims to accumulate knowledge and alleviate catastrophic\nforgetting when learning tasks sequentially. However, existing lifelong\nlanguage learning methods only focus on the supervised learning setting.\nUnlabeled data, which can be easily accessed in real-world scenarios, are\nunderexplored. In this paper, we explore a novel setting, semi-supervised\nlifelong language learning (SSLL), where a model learns sequentially arriving\nlanguage tasks with both labeled and unlabeled data. We propose an unlabeled\ndata enhanced lifelong learner to explore SSLL. Specially, we dedicate\ntask-specific modules to alleviate catastrophic forgetting and design two\nmodules to exploit unlabeled data: (1) a virtual supervision enhanced task\nsolver is constructed on a teacher-student framework to mine the underlying\nknowledge from unlabeled data; and (2) a backward augmented learner is built to\nencourage knowledge transfer from newly arrived unlabeled data to previous\ntasks. Experimental results on various language tasks demonstrate our model's\neffectiveness and superiority over competitive baselines under the new setting\nSSLL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yingxiu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bowen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhiliang Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongkyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Nevin L. Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Schr\\\"{o}dinger's Bat: Diffusion Models Sometimes Generate Polysemous Words in Superposition. (arXiv:2211.13095v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13095","description":"<p>Recent work has shown that despite their impressive capabilities,\ntext-to-image diffusion models such as DALL-E 2 (Ramesh et al., 2022) can\ndisplay strange behaviours when a prompt contains a word with multiple possible\nmeanings, often generating images containing both senses of the word (Rassin et\nal., 2022). In this work we seek to put forward a possible explanation of this\nphenomenon. Using the similar Stable Diffusion model (Rombach et al., 2022), we\nfirst show that when given an input that is the sum of encodings of two\ndistinct words, the model can produce an image containing both concepts\nrepresented in the sum. We then demonstrate that the CLIP encoder used to\nencode prompts (Radford et al., 2021) encodes polysemous words as a\nsuperposition of meanings, and that using linear algebraic techniques we can\nedit these representations to influence the senses represented in the generated\nimages. Combining these two findings, we suggest that the homonym duplication\nphenomenon described by Rassin et al. (2022) is caused by diffusion models\nproducing images representing both of the meanings that are present in\nsuperposition in the encoding of a polysemous word.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1\">Jennifer C. White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"This is the way: designing and compiling LEPISZCZE, a comprehensive NLP benchmark for Polish. (arXiv:2211.13112v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13112","description":"<p>The availability of compute and data to train larger and larger language\nmodels increases the demand for robust methods of benchmarking the true\nprogress of LM training. Recent years witnessed significant progress in\nstandardized benchmarking for English. Benchmarks such as GLUE, SuperGLUE, or\nKILT have become de facto standard tools to compare large language models.\nFollowing the trend to replicate GLUE for other languages, the KLEJ benchmark\nhas been released for Polish. In this paper, we evaluate the progress in\nbenchmarking for low-resourced languages. We note that only a handful of\nlanguages have such comprehensive benchmarks. We also note the gap in the\nnumber of tasks being evaluated by benchmarks for resource-rich English/Chinese\nand the rest of the world. In this paper, we introduce LEPISZCZE (the Polish\nword for glew, the Middle English predecessor of glue), a new, comprehensive\nbenchmark for Polish NLP with a large variety of tasks and high-quality\noperationalization of the benchmark. We design LEPISZCZE with flexibility in\nmind. Including new models, datasets, and tasks is as simple as possible while\nstill offering data versioning and model tracking. In the first run of the\nbenchmark, we test 13 experiments (task and dataset pairs) based on the five\nmost recent LMs for Polish. We use five datasets from the Polish benchmark and\nadd eight novel datasets. As the paper's main contribution, apart from\nLEPISZCZE, we provide insights and experiences learned while creating the\nbenchmark for Polish as the blueprint to design similar benchmarks for other\nlow-resourced languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Augustyniak_L/0/1/0/all/0/1\">&#x141;ukasz Augustyniak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagowski_K/0/1/0/all/0/1\">Kamil Tagowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawczyn_A/0/1/0/all/0/1\">Albert Sawczyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janiak_D/0/1/0/all/0/1\">Denis Janiak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartusiak_R/0/1/0/all/0/1\">Roman Bartusiak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szymczak_A/0/1/0/all/0/1\">Adrian Szymczak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watroba_M/0/1/0/all/0/1\">Marcin W&#x105;troba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janz_A/0/1/0/all/0/1\">Arkadiusz Janz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szymanski_P/0/1/0/all/0/1\">Piotr Szyma&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morzy_M/0/1/0/all/0/1\">Miko&#x142;aj Morzy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kajdanowicz_T/0/1/0/all/0/1\">Tomasz Kajdanowicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piasecki_M/0/1/0/all/0/1\">Maciej Piasecki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Number Theory Meets Linguistics: Modelling Noun Pluralisation Across 1497 Languages Using 2-adic Metrics. (arXiv:2211.13124v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13124","description":"<p>A simple machine learning model of pluralisation as a linear regression\nproblem minimising a p-adic metric substantially outperforms even the most\nrobust of Euclidean-space regressors on languages in the Indo-European,\nAustronesian, Trans New-Guinea, Sino-Tibetan, Nilo-Saharan, Oto-Meanguean and\nAtlantic-Congo language families. There is insufficient evidence to support\nmodelling distinct noun declensions as a p-adic neighbourhood even in\nIndo-European languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baker_G/0/1/0/all/0/1\">Gregory Baker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molla_Aliod_D/0/1/0/all/0/1\">Diego Molla-Aliod</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Average Token Delay: A Latency Metric for Simultaneous Translation. (arXiv:2211.13173v1 [cs.CL])","link":"http://arxiv.org/abs/2211.13173","description":"<p>Simultaneous translation is a task in which translation begins before the\nspeaker has finished speaking. In its evaluation, we have to consider the\nlatency of the translation in addition to the quality. The latency is\npreferably as small as possible for users to comprehend what the speaker says\nwith a small delay. Existing latency metrics focus on when the translation\nstarts but do not consider adequately when the translation ends. This means\nsuch metrics do not penalize the latency caused by a long translation output,\nwhich actually delays users' comprehension. In this work, we propose a novel\nlatency evaluation metric called Average Token Delay (ATD) that focuses on the\nend timings of partial translations in simultaneous translation. We discuss the\nadvantage of ATD using simulated examples and also investigate the differences\nbetween ATD and Average Lagging with simultaneous translation experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kano_Y/0/1/0/all/0/1\">Yasumasa Kano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudoh_K/0/1/0/all/0/1\">Katsuhito Sudoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1\">Satoshi Nakamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TorchScale: Transformers at Scale. (arXiv:2211.13184v1 [cs.LG])","link":"http://arxiv.org/abs/2211.13184","description":"<p>Large Transformers have achieved state-of-the-art performance across many\ntasks. Most open-source libraries on scaling Transformers focus on improving\ntraining or inference with better parallelization. In this work, we present\nTorchScale, an open-source toolkit that allows researchers and developers to\nscale up Transformers efficiently and effectively. TorchScale has the\nimplementation of several modeling techniques, which can improve modeling\ngenerality and capability, as well as training stability and efficiency.\nExperimental results on language modeling and neural machine translation\ndemonstrate that TorchScale can successfully scale Transformers to different\nsizes without tears. The library is available at https://aka.ms/torchscale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zewen Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benhaim_A/0/1/0/all/0/1\">Alon Benhaim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patra_B/0/1/0/all/0/1\">Barun Patra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1\">Vishrav Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeedBERT: Recovering Annotator Rating Distributions from an Aggregated Label. (arXiv:2211.13196v1 [cs.LG])","link":"http://arxiv.org/abs/2211.13196","description":"<p>Many machine learning tasks -- particularly those in affective computing --\nare inherently subjective. When asked to classify facial expressions or to rate\nan individual's attractiveness, humans may disagree with one another, and no\nsingle answer may be objectively correct. However, machine learning datasets\ncommonly have just one \"ground truth\" label for each sample, so models trained\non these labels may not perform well on tasks that are subjective in nature.\nThough allowing models to learn from the individual annotators' ratings may\nhelp, most datasets do not provide annotator-specific labels for each sample.\nTo address this issue, we propose SeedBERT, a method for recovering annotator\nrating distributions from a single label by inducing pre-trained models to\nattend to different portions of the input. Our human evaluations indicate that\nSeedBERT's attention mechanism is consistent with human sources of annotator\ndisagreement. Moreover, in our empirical evaluations using large language\nmodels, SeedBERT demonstrates substantial gains in performance on downstream\nsubjective tasks compared both to standard deep learning models and to other\ncurrent models that account explicitly for annotator disagreement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sampath_A/0/1/0/all/0/1\">Aneesha Sampath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_V/0/1/0/all/0/1\">Victoria Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Peekaboo: Text to Image Diffusion Models are Zero-Shot Segmentors. (arXiv:2211.13224v1 [cs.CV])","link":"http://arxiv.org/abs/2211.13224","description":"<p>Recent diffusion-based generative models combined with vision-language models\nare capable of creating realistic images from natural language prompts. While\nthese models are trained on large internet-scale datasets, such pre-trained\nmodels are not directly introduced to any semantic localization or grounding.\nMost current approaches for localization or grounding rely on human-annotated\nlocalization information in the form of bounding boxes or segmentation masks.\nThe exceptions are a few unsupervised methods that utilize architectures or\nloss functions geared towards localization, but they need to be trained\nseparately. In this work, we explore how off-the-shelf diffusion models,\ntrained with no exposure to such localization information, are capable of\ngrounding various semantic phrases with no segmentation-specific re-training.\nAn inference time optimization process is introduced, that is capable of\ngenerating segmentation masks conditioned on natural language. We evaluate our\nproposal Peekaboo for unsupervised semantic segmentation on the Pascal VOC\ndataset. In addition, we evaluate for referring segmentation on the RefCOCO\ndataset. In summary, we present a first zero-shot, open-vocabulary,\nunsupervised (no localization information), semantic grounding technique\nleveraging diffusion-based generative models with no re-training. Our code will\nbe released publicly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burgert_R/0/1/0/all/0/1\">Ryan Burgert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_K/0/1/0/all/0/1\">Kanchana Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1\">Michael S. Ryoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Executing Instructions in Situated Collaborative Interactions. (arXiv:1910.03655v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1910.03655","description":"<p>We study a collaborative scenario where a user not only instructs a system to\ncomplete tasks, but also acts alongside it. This allows the user to adapt to\nthe system abilities by changing their language or deciding to simply\naccomplish some tasks themselves, and requires the system to effectively\nrecover from errors as the user strategically assigns it new goals. We build a\ngame environment to study this scenario, and learn to map user instructions to\nsystem actions. We introduce a learning approach focused on recovery from\ncascading errors between instructions, and modeling methods to explicitly\nreason about instructions with multiple goals. We evaluate with a new\nevaluation protocol using recorded interactions and online games with human\nusers, and observe how users adapt to the system abilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suhr_A/0/1/0/all/0/1\">Alane Suhr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Claudia Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluger_C/0/1/0/all/0/1\">Charlotte Schluger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stanley Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khader_H/0/1/0/all/0/1\">Hadi Khader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouallem_M/0/1/0/all/0/1\">Marwa Mouallem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_I/0/1/0/all/0/1\">Iris Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CANDLE: Decomposing Conditional and Conjunctive Queries for Task-Oriented Dialogue Systems. (arXiv:2107.03884v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.03884","description":"<p>Domain-specific dialogue systems generally determine user intents by relying\non sentence level classifiers that mainly focus on single action sentences.\nSuch classifiers are not designed to effectively handle complex queries\ncomposed of conditional and sequential clauses that represent multiple actions.\nWe attempt to decompose such queries into smaller single action subqueries that\nare reasonable for intent classifiers to understand in a dialogue pipeline. We\nrelease, CANDLE(Conditional &amp; AND type Expressions), a dataset consisting of\n4282 utterances manually tagged with conditional and sequential labels, and\ndemonstrates this decomposition by training two baseline taggers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aadesh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhole_K/0/1/0/all/0/1\">Kaustubh D.Dhole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarway_R/0/1/0/all/0/1\">Rahul Tarway</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakar_S/0/1/0/all/0/1\">Swetha Prabhakar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Ashish Shrivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Template-free Prompt Tuning for Few-shot NER. (arXiv:2109.13532v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13532","description":"<p>Prompt-based methods have been successfully applied in sentence-level\nfew-shot learning tasks, mostly owing to the sophisticated design of templates\nand label words. However, when applied to token-level labeling tasks such as\nNER, it would be time-consuming to enumerate the template queries over all\npotential entity spans. In this work, we propose a more elegant method to\nreformulate NER tasks as LM problems without any templates. Specifically, we\ndiscard the template construction process while maintaining the word prediction\nparadigm of pre-training models to predict a class-related pivot word (or label\nword) at the entity position. Meanwhile, we also explore principled ways to\nautomatically search for appropriate label words that the pre-trained models\ncan easily adapt to. While avoiding complicated template-based process, the\nproposed LM objective also reduces the gap between different objectives used in\npre-training and fine-tuning, thus it can better benefit the few-shot\nperformance. Experimental results demonstrate the effectiveness of the proposed\nmethod over bert-tagger and template-based method under few-shot setting.\nMoreover, the decoding speed of the proposed method is up to 1930.12 times\nfaster than the template-based method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Ruotian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yiding Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech-enhanced and Noise-aware Networks for Robust Speech Recognition. (arXiv:2203.13696v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.13696","description":"<p>Compensation for channel mismatch and noise interference is essential for\nrobust automatic speech recognition. Enhanced speech has been introduced into\nthe multi-condition training of acoustic models to improve their generalization\nability. In this paper, a noise-aware training framework based on two cascaded\nneural structures is proposed to jointly optimize speech enhancement and speech\nrecognition. The feature enhancement module is composed of a multi-task\nautoencoder, where noisy speech is decomposed into clean speech and noise. By\nconcatenating its enhanced, noise-aware, and noisy features for each frame, the\nacoustic-modeling module maps each feature-augmented frame into a triphone\nstate by optimizing the lattice-free maximum mutual information and cross\nentropy between the predicted and actual state sequences. On top of the\nfactorized time delay neural network (TDNN-F) and its convolutional variant\n(CNN-TDNNF), both with SpecAug, the two proposed systems achieve word error\nrate (WER) of 3.90% and 3.55%, respectively, on the Aurora-4 task. Compared\nwith the best existing systems that use bigram and trigram language models for\ndecoding, the proposed CNN-TDNNF-based system achieves a relative WER reduction\nof 15.20% and 33.53%, respectively. In addition, the proposed CNN-TDNNF-based\nsystem also outperforms the baseline CNN-TDNNF system on the AMI task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Shin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yao-Fei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Dynamic Semantics into Pre-Trained Language Model for Aspect-based Sentiment Analysis. (arXiv:2203.16369v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16369","description":"<p>Aspect-based sentiment analysis (ABSA) predicts sentiment polarity towards a\nspecific aspect in the given sentence. While pre-trained language models such\nas BERT have achieved great success, incorporating dynamic semantic changes\ninto ABSA remains challenging. To this end, in this paper, we propose to\naddress this problem by Dynamic Re-weighting BERT (DR-BERT), a novel method\ndesigned to learn dynamic aspect-oriented semantics for ABSA. Specifically, we\nfirst take the Stack-BERT layers as a primary encoder to grasp the overall\nsemantic of the sentence and then fine-tune it by incorporating a lightweight\nDynamic Re-weighting Adapter (DRA). Note that the DRA can pay close attention\nto a small region of the sentences at each step and re-weigh the vitally\nimportant words for better aspect-aware sentiment understanding. Finally,\nexperimental results on three benchmark datasets demonstrate the effectiveness\nand the rationality of our proposed model and provide good interpretable\ninsights for future semantic modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengdi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hongke Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Spoken Dialogue Language Modeling. (arXiv:2203.16502v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16502","description":"<p>We introduce dGSLM, the first \"textless\" model able to generate audio samples\nof naturalistic spoken dialogues. It uses recent work on unsupervised spoken\nunit discovery coupled with a dual-tower transformer architecture with\ncross-attention trained on 2000 hours of two-channel raw conversational audio\n(Fisher dataset) without any text or labels. We show that our model is able to\ngenerate speech, laughter and other paralinguistic signals in the two channels\nsimultaneously and reproduces more naturalistic and fluid turn-taking compared\nto a text-based cascaded model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1\">Jade Copet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elkahky_A/0/1/0/all/0/1\">Ali Elkahky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomasello_P/0/1/0/all/0/1\">Paden Tomasello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Algayres_R/0/1/0/all/0/1\">Robin Algayres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Benoit Sagot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phylogeny-Inspired Adaptation of Multilingual Models to New Languages. (arXiv:2205.09634v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09634","description":"<p>Large pretrained multilingual models, trained on dozens of languages, have\ndelivered promising results due to cross-lingual learning capabilities on\nvariety of language tasks. Further adapting these models to specific languages,\nespecially ones unseen during pre-training, is an important goal towards\nexpanding the coverage of language technologies. In this study, we show how we\ncan use language phylogenetic information to improve cross-lingual transfer\nleveraging closely related languages in a structured, linguistically-informed\nmanner. We perform adapter-based training on languages from diverse language\nfamilies (Germanic, Uralic, Tupian, Uto-Aztecan) and evaluate on both syntactic\nand semantic tasks, obtaining more than 20% relative performance improvements\nover strong commonly used baselines, especially on languages unseen during\npre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faisal_F/0/1/0/all/0/1\">Fahim Faisal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeT: Code Generation with Generated Tests. (arXiv:2207.10397v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.10397","description":"<p>The task of generating code solutions for a given programming problem can\nbenefit from the use of pre-trained language models such as Codex, which can\nproduce multiple diverse samples. However, a major challenge for this task is\nto select the most appropriate solution from the multiple samples generated by\nthe pre-trained language models. A natural way to evaluate the quality and\ncorrectness of a code solution is to run it against a set of test cases, but\nthe manual creation of such test cases is often costly and time-consuming. In\nthis paper, we propose a novel method, CodeT, that leverages the same\npre-trained language models to automatically generate test cases for the code\nsamples, thus reducing the human effort and increasing the coverage of the test\nscenarios. CodeT then executes the code samples using the generated test cases,\nand performs a dual execution agreement, which considers both the consistency\nof the outputs against the generated test cases and the agreement of the\noutputs with other code samples. We conduct comprehensive experiments on four\nbenchmarks, HumanEval, MBPP, APPS and CodeContests, using five different\npre-trained language models with varying sizes and capabilities. Our results\nshow that CodeT can significantly improve the performance of code solution\nselection over previous methods, achieving remarkable and consistent gains\nacross different models and benchmarks. For instance, CodeT improves the pass@1\nmetric on HumanEval to 65.8%, which represents an absolute improvement of 18.8%\nover the code-davinci-002 model, and an absolute improvement of more than 20%\nover the previous state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fengji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zan_D/0/1/0/all/0/1\">Daoguang Zan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zeqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Coherent Narratives by Learning Dynamic and Discrete Entity States with a Contrastive Framework. (arXiv:2208.03985v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.03985","description":"<p>Despite advances in generating fluent texts, existing pretraining models tend\nto attach incoherent event sequences to involved entities when generating\nnarratives such as stories and news. We conjecture that such issues result from\nrepresenting entities as static embeddings of superficial words, while\nneglecting to model their ever-changing states, i.e., the information they\ncarry, as the text unfolds. Therefore, we extend the Transformer model to\ndynamically conduct entity state updates and sentence realization for narrative\ngeneration. We propose a contrastive framework to learn the state\nrepresentations in a discrete space, and insert additional attention layers\ninto the decoder to better exploit these states. Experiments on two narrative\ndatasets show that our model can generate more coherent and diverse narratives\nthan strong baselines with the guidance of meaningful entity states.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1\">Jian Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongsheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned. (arXiv:2209.07858v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.07858","description":"<p>We describe our early efforts to red team language models in order to\nsimultaneously discover, measure, and attempt to reduce their potentially\nharmful outputs. We make three main contributions. First, we investigate\nscaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B\nparameters) and 4 model types: a plain language model (LM); an LM prompted to\nbe helpful, honest, and harmless; an LM with rejection sampling; and a model\ntrained to be helpful and harmless using reinforcement learning from human\nfeedback (RLHF). We find that the RLHF models are increasingly difficult to red\nteam as they scale, and we find a flat trend with scale for the other model\ntypes. Second, we release our dataset of 38,961 red team attacks for others to\nanalyze and learn from. We provide our own analysis of the data and find a\nvariety of harmful outputs, which range from offensive language to more subtly\nharmful non-violent unethical outputs. Third, we exhaustively describe our\ninstructions, processes, statistical methodologies, and uncertainty about red\nteaming. We hope that this transparency accelerates our ability to work\ntogether as a community in order to develop shared norms, practices, and\ntechnical standards for how to red team language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_D/0/1/0/all/0/1\">Deep Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovitt_L/0/1/0/all/0/1\">Liane Lovitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kernion_J/0/1/0/all/0/1\">Jackson Kernion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askell_A/0/1/0/all/0/1\">Amanda Askell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yuntao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1\">Saurav Kadavath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mann_B/0/1/0/all/0/1\">Ben Mann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiefer_N/0/1/0/all/0/1\">Nicholas Schiefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ndousse_K/0/1/0/all/0/1\">Kamal Ndousse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_A/0/1/0/all/0/1\">Andy Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Sam Bowman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anna Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conerly_T/0/1/0/all/0/1\">Tom Conerly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DasSarma_N/0/1/0/all/0/1\">Nova DasSarma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1\">Dawn Drain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhage_N/0/1/0/all/0/1\">Nelson Elhage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Showk_S/0/1/0/all/0/1\">Sheer El-Showk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1\">Stanislav Fort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hatfield_Dodds_Z/0/1/0/all/0/1\">Zac Hatfield-Dodds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henighan_T/0/1/0/all/0/1\">Tom Henighan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_D/0/1/0/all/0/1\">Danny Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hume_T/0/1/0/all/0/1\">Tristan Hume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobson_J/0/1/0/all/0/1\">Josh Jacobson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnston_S/0/1/0/all/0/1\">Scott Johnston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kravec_S/0/1/0/all/0/1\">Shauna Kravec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsson_C/0/1/0/all/0/1\">Catherine Olsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ringer_S/0/1/0/all/0/1\">Sam Ringer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Johnson_E/0/1/0/all/0/1\">Eli Tran-Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amodei_D/0/1/0/all/0/1\">Dario Amodei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_T/0/1/0/all/0/1\">Tom Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_N/0/1/0/all/0/1\">Nicholas Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCandlish_S/0/1/0/all/0/1\">Sam McCandlish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olah_C/0/1/0/all/0/1\">Chris Olah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1\">Jared Kaplan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jack Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Instruction-Finetuned Language Models. (arXiv:2210.11416v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.11416","description":"<p>Finetuning language models on a collection of datasets phrased as\ninstructions has been shown to improve model performance and generalization to\nunseen tasks. In this paper we explore instruction finetuning with a particular\nfocus on (1) scaling the number of tasks, (2) scaling the model size, and (3)\nfinetuning on chain-of-thought data. We find that instruction finetuning with\nthe above aspects dramatically improves performance on a variety of model\nclasses (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and\nevaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For\ninstance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM\n540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves\nstate-of-the-art performance on several benchmarks, such as 75.2% on five-shot\nMMLU. We also publicly release Flan-T5 checkpoints, which achieve strong\nfew-shot performance even compared to much larger models, such as PaLM 62B.\nOverall, instruction finetuning is a general method for improving the\nperformance and usability of pretrained language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Le Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1\">Barret Zoph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedus_W/0/1/0/all/0/1\">William Fedus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahma_S/0/1/0/all/0/1\">Siddhartha Brahma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1\">Albert Webson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shixiang Shane Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zhuyun Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzgun_M/0/1/0/all/0/1\">Mirac Suzgun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhery_A/0/1/0/all/0/1\">Aakanksha Chowdhery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_Ros_A/0/1/0/all/0/1\">Alex Castro-Ros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pellat_M/0/1/0/all/0/1\">Marie Pellat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_K/0/1/0/all/0/1\">Kevin Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valter_D/0/1/0/all/0/1\">Dasha Valter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_G/0/1/0/all/0/1\">Gaurav Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Adams Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_V/0/1/0/all/0/1\">Vincent Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrov_S/0/1/0/all/0/1\">Slav Petrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed H. Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dean_J/0/1/0/all/0/1\">Jeff Dean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devlin_J/0/1/0/all/0/1\">Jacob Devlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrence Boosts Diversity! Revisiting Recurrent Latent Variable in Transformer-Based Variational AutoEncoder for Diverse Text Generation. (arXiv:2210.12409v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12409","description":"<p>Variational Auto-Encoder (VAE) has been widely adopted in text generation.\nAmong many variants, recurrent VAE learns token-wise latent variables with each\nconditioned on the preceding ones, which captures sequential variability better\nin the era of RNN. However, it is unclear how to incorporate such recurrent\ndynamics into the recently dominant Transformer due to its parallelism. In this\nwork, we propose TRACE, a Transformer-based recurrent VAE structure. TRACE\nimposes recurrence on segment-wise latent variables with arbitrarily separated\ntext segments and constructs the posterior distribution with residual\nparameterization. Besides, we design an acceleration method by approximating\nidempotent matrices, which allows parallelism while maintaining the conditional\ndependence of latent variables. We demonstrate that TRACE could enhance the\nentanglement of each segment and preceding latent variables and deduce a\nnon-zero lower bound of the KL term, providing a theoretical guarantee of\ngeneration diversity. Experiments on two unconditional and one conditional\ngeneration tasks show that TRACE achieves significantly improved diversity\nwhile maintaining satisfactory generation quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1\">Xiaoyuan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Teacher-student Framework for Unsupervised Speech Enhancement Using Noise Remixing Training and Two-stage Inference. (arXiv:2210.15368v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2210.15368","description":"<p>The lack of clean speech is a practical challenge to the development of\nspeech enhancement systems, which means that the training of neural network\nmodels must be done in an unsupervised manner, and there is an inevitable\nmismatch between their training criterion and evaluation metric. In response to\nthis unfavorable situation, we propose a teacher-student training strategy that\ndoes not require any subjective/objective speech quality metrics as learning\nreference by improving the previously proposed noisy-target training (NyTT).\nBecause homogeneity between in-domain noise and extraneous noise is the key to\nthe effectiveness of NyTT, we train various student models by remixing the\nteacher model's estimated speech and noise for clean-target training or raw\nnoisy speech and the teacher model's estimated noise for noisy-target training.\nWe use the NyTT model as the initial teacher model. Experimental results show\nthat our proposed method outperforms several baselines, especially with\ntwo-stage inference, where clean speech is derived successively through the\nbootstrap model and the final student model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yao-Fei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Shin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic extraction of materials and properties from superconductors scientific literature. (arXiv:2210.15600v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.15600","description":"<p>The automatic extraction of materials and related properties from the\nscientific literature is gaining attention in data-driven materials science\n(Materials Informatics). In this paper, we discuss Grobid-superconductors, our\nsolution for automatically extracting superconductor material names and\nrespective properties from text. Built as a Grobid module, it combines machine\nlearning and heuristic approaches in a multi-step architecture that supports\ninput data as raw text or PDF documents. Using Grobid-superconductors, we built\nSuperCon2, a database of 40324 materials and properties records from 37700\npapers. The material (or sample) information is represented by name, chemical\nformula, and material class, and is characterized by shape, doping,\nsubstitution variables for components, and substrate as adjoined information.\nThe properties include the Tc superconducting critical temperature and, when\navailable, applied pressure with the Tc measurement method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Foppiano_L/0/1/0/all/0/1\">Luca Foppiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1\">Pedro Baptista de Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suarez_P/0/1/0/all/0/1\">Pedro Ortiz Suarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terashima_K/0/1/0/all/0/1\">Kensei Terashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takano_Y/0/1/0/all/0/1\">Yoshihiko Takano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_M/0/1/0/all/0/1\">Masashi Ishii</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Prompting: Making Pre-trained Language Models Better Zero-shot Learners by Clustering Representations. (arXiv:2210.16637v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.16637","description":"<p>Recent work has demonstrated that pre-trained language models (PLMs) are\nzero-shot learners. However, most existing zero-shot methods involve heavy\nhuman engineering or complicated self-training pipelines, hindering their\napplication to new situations. In this work, we show that zero-shot text\nclassification can be improved simply by clustering texts in the embedding\nspaces of PLMs. Specifically, we fit the unlabeled texts with a Bayesian\nGaussian Mixture Model after initializing cluster positions and shapes using\nclass names. Despite its simplicity, this approach achieves superior or\ncomparable performance on both topic and sentiment classification datasets and\noutperforms prior works significantly on unbalanced datasets. We further\nexplore the applicability of our clustering approach by evaluating it on 14\ndatasets with more diverse topics, text lengths, and numbers of classes. Our\napproach achieves an average of 20% absolute improvement over prompt-based\nzero-shot learning. Finally, we compare different PLM embedding spaces and find\nthat texts are well-clustered by topics even if the PLM is not explicitly\npre-trained to generate meaningful sentence embeddings. This work indicates\nthat PLM embeddings can categorize texts without task-specific fine-tuning,\nthus providing a new way to analyze and utilize their knowledge and zero-shot\nlearning ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fei_Y/0/1/0/all/0/1\">Yu Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_P/0/1/0/all/0/1\">Ping Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1\">Roger Wattenhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-11-23T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}