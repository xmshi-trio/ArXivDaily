{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-11-10T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Discover, Explanation, Improvement: Automatic Slice Detection Framework for Natural Language Processing. (arXiv:2211.04476v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04476","description":"<p>Current natural language processing (NLP) models such as BERT and RoBERTa\nhave achieved high overall performance, but they often make systematic errors\ndue to bias or certain difficult features to learn. Thus research on slice\ndetection models (SDM) which automatically identifies underperforming groups of\ndatapoints has gradually caught more attention, which aims at both\nunderstanding model behaviors and providing insights for future model training\nand designing. However, there is little systematic research on SDM and\nquantitative evaluation of its assessment for NLP models. Our paper fills this\ngap by proposing \"Discover, Explanation, Improvement\" framework that discovers\ncoherent and underperforming groups of datapoints and unites datapoints of each\nslice under human-understandable concepts; it also provides comprehensive\nevaluation tasks and the corresponding quantitative metrics, which enable\nconvenient comparison for future works. Results show that our framework can\naccurately select error-prone datapoints with informative semantic features\nthat summarize error patterns, based on which it directly boosts model\nperformance by an average of 2.85 points based on trained models without tuning\nany parameters across multiple datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wenyue Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lifeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linfeng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1\">Haitao Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Example Selection for In-Context Learning. (arXiv:2211.04486v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04486","description":"<p>With a handful of demonstration examples, large-scale language models show\nstrong capability to perform various tasks by in-context learning from these\nexamples, without any fine-tuning. We demonstrate that in-context learning\nperformance can be highly unstable across samples of examples, indicating the\nidiosyncrasies of how language models acquire information. We formulate example\nselection for in-context learning as a sequential decision problem, and propose\na reinforcement learning algorithm for identifying generalizable policies to\nselect demonstration examples. For GPT-2, our learned policies demonstrate\nstrong abilities of generalizing to unseen tasks in training, with a $5.8\\%$\nimprovement on average. Examples selected from our learned policies can even\nachieve a small improvement on GPT-3 Ada. However, the improvement diminishes\non larger GPT-3 models, suggesting emerging capabilities of large language\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpeechMatrix: A Large-Scale Mined Corpus of Multilingual Speech-to-Speech Translations. (arXiv:2211.04508v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04508","description":"<p>We present SpeechMatrix, a large-scale multilingual corpus of\nspeech-to-speech translations mined from real speech of European Parliament\nrecordings. It contains speech alignments in 136 language pairs with a total of\n418 thousand hours of speech. To evaluate the quality of this parallel speech,\nwe train bilingual speech-to-speech translation models on mined data only and\nestablish extensive baseline results on EuroParl-ST, VoxPopuli and FLEURS test\nsets. Enabled by the multilinguality of SpeechMatrix, we also explore\nmultilingual speech-to-speech translation, a topic which was addressed by few\nother works. We also demonstrate that model pre-training and sparse scaling\nusing Mixture-of-Experts bring large gains to translation performance. The\nmined data and models are freely available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duquenne_P/0/1/0/all/0/1\">Paul-Ambroise Duquenne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1\">Ning Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jingfei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goswani_V/0/1/0/all/0/1\">Vedanuj Goswani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Beno&#xee;t Sagot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwenk_H/0/1/0/all/0/1\">Holger Schwenk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Going for GOAL: A Resource for Grounded Football Commentaries. (arXiv:2211.04534v1 [cs.CV])","link":"http://arxiv.org/abs/2211.04534","description":"<p>Recent video+language datasets cover domains where the interaction is highly\nstructured, such as instructional videos, or where the interaction is scripted,\nsuch as TV shows. Both of these properties can lead to spurious cues to be\nexploited by models rather than learning to ground language. In this paper, we\npresent GrOunded footbAlL commentaries (GOAL), a novel dataset of football (or\n`soccer') highlights videos with transcribed live commentaries in English. As\nthe course of a game is unpredictable, so are commentaries, which makes them a\nunique resource to investigate dynamic language grounding. We also provide\nstate-of-the-art baselines for the following tasks: frame reordering, moment\nretrieval, live commentary retrieval and play-by-play live commentary\ngeneration. Results show that SOTA models perform reasonably well in most\ntasks. We discuss the implications of these results and suggest new tasks for\nwhich GOAL can be used. Our codebase is available at:\nhttps://gitlab.com/grounded-sport-convai/goal-baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suglia_A/0/1/0/all/0/1\">Alessandro Suglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopes_J/0/1/0/all/0/1\">Jos&#xe9; Lopes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastianelli_E/0/1/0/all/0/1\">Emanuele Bastianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanzo_A/0/1/0/all/0/1\">Andrea Vanzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Shubham Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikandrou_M/0/1/0/all/0/1\">Malvina Nikandrou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konstas_I/0/1/0/all/0/1\">Ioannis Konstas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1\">Verena Rieser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward a Neural Semantic Parsing System for EHR Question Answering. (arXiv:2211.04569v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04569","description":"<p>Clinical semantic parsing (SP) is an important step toward identifying the\nexact information need (as a machine-understandable logical form) from a\nnatural language query aimed at retrieving information from electronic health\nrecords (EHRs). Current approaches to clinical SP are largely based on\ntraditional machine learning and require hand-building a lexicon. The recent\nadvancements in neural SP show a promise for building a robust and flexible\nsemantic parser without much human effort. Thus, in this paper, we aim to\nsystematically assess the performance of two such neural SP models for EHR\nquestion answering (QA). We found that the performance of these advanced neural\nmodels on two clinical SP datasets is promising given their ease of application\nand generalizability. Our error analysis surfaces the common types of errors\nmade by these models and has the potential to inform future research into\nimproving the performance of neural SP models for EHR QA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soni_S/0/1/0/all/0/1\">Sarvesh Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_K/0/1/0/all/0/1\">Kirk Roberts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Euphemisms with Literal Descriptions and Visual Imagery. (arXiv:2211.04576v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04576","description":"<p>This paper describes our two-stage system for the Euphemism Detection shared\ntask hosted by the 3rd Workshop on Figurative Language Processing in\nconjunction with EMNLP 2022. Euphemisms tone down expressions about sensitive\nor unpleasant issues like addiction and death. The ambiguous nature of\neuphemistic words or expressions makes it challenging to detect their actual\nmeaning within a context. In the first stage, we seek to mitigate this\nambiguity by incorporating literal descriptions into input text prompts to our\nbaseline model. It turns out that this kind of direct supervision yields\nremarkable performance improvement. In the second stage, we integrate visual\nsupervision into our system using visual imageries, two sets of images\ngenerated by a text-to-image model by taking terms and descriptions as input.\nOur experiments demonstrate that visual supervision also gives a statistically\nsignificant performance boost. Our system achieved the second place with an F1\nscore of 87.2%, only about 0.9% worse than the best submission.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kesen_I/0/1/0/all/0/1\">&#x130;lker Kesen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_A/0/1/0/all/0/1\">Aykut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_E/0/1/0/all/0/1\">Erkut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calixto_I/0/1/0/all/0/1\">Iacer Calixto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Follow Instructions in Text-Based Games. (arXiv:2211.04591v1 [cs.LG])","link":"http://arxiv.org/abs/2211.04591","description":"<p>Text-based games present a unique class of sequential decision making problem\nin which agents interact with a partially observable, simulated environment via\nactions and observations conveyed through natural language. Such observations\ntypically include instructions that, in a reinforcement learning (RL) setting,\ncan directly or indirectly guide a player towards completing reward-worthy\ntasks. In this work, we study the ability of RL agents to follow such\ninstructions. We conduct experiments that show that the performance of\nstate-of-the-art text-based game agents is largely unaffected by the presence\nor absence of such instructions, and that these agents are typically unable to\nexecute tasks to completion. To further study and address the task of\ninstruction following, we equip RL agents with an internal structured\nrepresentation of natural language instructions in the form of Linear Temporal\nLogic (LTL), a formal language that is increasingly used for temporally\nextended reward specification in RL. Our framework both supports and highlights\nthe benefit of understanding the temporal semantics of instructions and in\nmeasuring progress towards achievement of such a temporally extended behaviour.\nExperiments with 500+ games in TextWorld demonstrate the superior performance\nof our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tuli_M/0/1/0/all/0/1\">Mathieu Tuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Andrew C. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaezipoor_P/0/1/0/all/0/1\">Pashootan Vaezipoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klassen_T/0/1/0/all/0/1\">Toryn Q. Klassen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanner_S/0/1/0/all/0/1\">Scott Sanner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McIlraith_S/0/1/0/all/0/1\">Sheila A. McIlraith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StructDiffusion: Object-Centric Diffusion for Semantic Rearrangement of Novel Objects. (arXiv:2211.04604v1 [cs.RO])","link":"http://arxiv.org/abs/2211.04604","description":"<p>Robots operating in human environments must be able to rearrange objects into\nsemantically-meaningful configurations, even if these objects are previously\nunseen. In this work, we focus on the problem of building physically-valid\nstructures without step-by-step instructions. We propose StructDiffusion, which\ncombines a diffusion model and an object-centric transformer to construct\nstructures out of a single RGB-D image based on high-level language goals, such\nas \"set the table.\" Our method shows how diffusion models can be used for\ncomplex multi-step 3D planning tasks. StructDiffusion improves success rate on\nassembling physically-valid structures out of unseen objects by on average 16%\nover an existing multi-modal transformer model, while allowing us to use one\nmulti-task model to produce a wider range of different structures. We show\nexperiments on held-out objects in both simulation and on real-world\nrearrangement tasks. For videos and additional results, check out our website:\n<a href=\"http://weiyuliu.com/StructDiffusion/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermans_T/0/1/0/all/0/1\">Tucker Hermans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chernova_S/0/1/0/all/0/1\">Sonia Chernova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepE: a deep neural network for knowledge graph embedding. (arXiv:2211.04620v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04620","description":"<p>Recently, neural network based methods have shown their power in learning\nmore expressive features on the task of knowledge graph embedding (KGE).\nHowever, the performance of deep methods often falls behind the shallow ones on\nsimple graphs. One possible reason is that deep models are difficult to train,\nwhile shallow models might suffice for accurately representing the structure of\nthe simple KGs.\n</p>\n<p>In this paper, we propose a neural network based model, named DeepE, to\naddress the problem, which stacks multiple building blocks to predict the tail\nentity based on the head entity and the relation. Each building block is an\naddition of a linear and a non-linear function. The stacked building blocks are\nequivalent to a group of learning functions with different non-linear depth.\nHence, DeepE allows deep functions to learn deep features, and shallow\nfunctions to learn shallow features. Through extensive experiments, we find\nDeepE outperforms other state-of-the-art baseline methods. A major advantage of\nDeepE is the robustness. DeepE achieves a Mean Rank (MR) score that is 6%, 30%,\n65% lower than the best baseline methods on FB15k-237, WN18RR and YAGO3-10. Our\ndesign makes it possible to train much deeper networks on KGE, e.g. 40 layers\non FB15k-237, and without scarifying precision on simple relations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Danhao_Z/0/1/0/all/0/1\">Zhu Danhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1\">Shen Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shujian_H/0/1/0/all/0/1\">Huang Shujian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziqi_D/0/1/0/all/0/1\">Ding Ziqi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Method to Judge the Style of Classical Poetry Based on Pre-trained Model. (arXiv:2211.04657v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04657","description":"<p>One of the important topics in the research field of Chinese classical poetry\nis to analyze the poetic style. By examining the relevant works of previous\ndynasties, researchers judge a poetic style mostly by their subjective\nfeelings, and refer to the previous evaluations that have become a certain\nconclusion. Although this judgment method is often effective, there may be some\nerrors. This paper builds the most perfect data set of Chinese classical poetry\nat present, trains a BART-poem pre -trained model on this data set, and puts\nforward a generally applicable poetry style judgment method based on this\nBART-poem model, innovatively introduces in-depth learning into the field of\ncomputational stylistics, and provides a new research method for the study of\nclassical poetry. This paper attempts to use this method to solve the problem\nof poetry style identification in the Tang and Song Dynasties, and takes the\npoetry schools that are considered to have a relatively clear and consistent\npoetic style, such as the Hongzheng Qizi and Jiajing Qizi, Jiangxi poetic\nschool and Tongguang poetic school, as the research object, and takes the poems\nof their representative poets for testing. Experiments show that the judgment\nresults of the tested poetry work made by the model are basically consistent\nwith the conclusions given by critics of previous dynasties, verify some\navant-garde judgments of Mr. Qian Zhongshu, and better solve the task of poetry\nstyle recognition in the Tang and Song dynasties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiandong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Label Prompt Selection. (arXiv:2211.04668v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04668","description":"<p>Natural language prompts have been shown to facilitate cross-task\ngeneralization for large language models. However, with no or limited labeled\nexamples, the cross-task performance is highly sensitive to the choice of\nprompts, while selecting a high-performing prompt is challenging given the\nscarcity of labels. To address the issue, we propose a Zero-Label Prompt\nSelection (ZPS) method that selects prompts without any labeled data or\ngradient update. Specifically, given the candidate human-written prompts for a\ntask, ZPS labels a set of unlabeled data with a prompt ensemble and uses the\npseudo-labels for prompt selection. Experiments show that ZPS improves over\nprior methods by a sizeable margin in zero-label performance. We also extend\nZPS to a few-shot setting and show its advantages over strong baselines such as\nprompt tuning and model tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1\">Chonghua Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yanan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhilin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Character Understanding in Movies as an Assessment to Meta-Learning of Theory-of-Mind. (arXiv:2211.04684v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04684","description":"<p>When reading a story, humans can rapidly understand new fictional characters\nwith a few observations, mainly by drawing analogy to fictional and real people\nthey met before in their lives. This reflects the few-shot and meta-learning\nessence of humans' inference of characters' mental states, i.e., humans'\ntheory-of-mind (ToM), which is largely ignored in existing research. We fill\nthis gap with a novel NLP benchmark, TOM-IN-AMC, the first assessment of\nmodels' ability of meta-learning of ToM in a realistic narrative understanding\nscenario. Our benchmark consists of $\\sim$1,000 parsed movie scripts for this\npurpose, each corresponding to a few-shot character understanding task; and\nrequires models to mimic humans' ability of fast digesting characters with a\nfew starting scenes in a new movie. Our human study verified that humans can\nsolve our problem by inferring characters' mental states based on their\npreviously seen movies; while the state-of-the-art metric-learning and\nmeta-learning approaches adapted to our task lags 30% behind.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_Y/0/1/0/all/0/1\">Yisi Sang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_K/0/1/0/all/0/1\">Kangsheng Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zekai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Han Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Extractive Summarization with Heterogeneous Graph Embeddings for Chinese Document. (arXiv:2211.04698v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04698","description":"<p>In the scenario of unsupervised extractive summarization, learning\nhigh-quality sentence representations is essential to select salient sentences\nfrom the input document. Previous studies focus more on employing statistical\napproaches or pre-trained language models (PLMs) to extract sentence\nembeddings, while ignoring the rich information inherent in the heterogeneous\ntypes of interaction between words and sentences. In this paper, we are the\nfirst to propose an unsupervised extractive summarizaiton method with\nheterogeneous graph embeddings (HGEs) for Chinese document. A heterogeneous\ntext graph is constructed to capture different granularities of interactions by\nincorporating graph structural information. Moreover, our proposed graph is\ngeneral and flexible where additional nodes such as keywords can be easily\nintegrated. Experimental results demonstrate that our method consistently\noutperforms the strong baseline in three summarization datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1\">Siyu An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Di Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FF2: A Feature Fusion Two-Stream Framework for Punctuation Restoration. (arXiv:2211.04699v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04699","description":"<p>To accomplish punctuation restoration, most existing methods focus on\nintroducing extra information (e.g., part-of-speech) or addressing the class\nimbalance problem. Recently, large-scale transformer-based pre-trained language\nmodels (PLMS) have been utilized widely and obtained remarkable success.\nHowever, the PLMS are trained on the large dataset with marks, which may not\nfit well with the small dataset without marks, causing the convergence to be\nnot ideal. In this study, we propose a Feature Fusion two-stream framework\n(FF2) to bridge the gap. Specifically, one stream leverages a pre-trained\nlanguage model to capture the semantic feature, while another auxiliary module\ncaptures the feature at hand. We also modify the computation of multi-head\nattention to encourage communication among heads. Then, two features with\ndifferent perspectives are aggregated to fuse information and enhance context\nawareness. Without additional data, the experimental results on the popular\nbenchmark IWSLT demonstrate that FF2 achieves new SOTA performance, which\nverifies that our approach is effective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yangjun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1\">Kebin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1\">Lifeng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengqi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Noisy Student Training on Non-target Domain Data for Automatic Speech Recognition. (arXiv:2211.04717v1 [cs.SD])","link":"http://arxiv.org/abs/2211.04717","description":"<p>Noisy Student Training (NST) has recently demonstrated extremely strong\nperformance in Automatic Speech Recognition (ASR). In this paper, we propose a\ndata selection strategy named LM Filter to improve the performances of NST on\nnon-target domain data in ASR tasks. Hypothesis with and without Language Model\nare generated and CER differences between them are utilized as a filter\nthreshold. Results reveal that significant improvements of 10.4% compared with\nno data filtering baselines. We can achieve 3.31% CER in AISHELL-1 test set,\nwhich is best result from our knowledge without any other supervised data. We\nalso perform evaluations on supervised 1000 hour AISHELL-2 dataset and\ncompetitive results of 4.72% CER can be achieved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wen Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1\">Junjie Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nested Named Entity Recognition from Medical Texts: An Adaptive Shared Network Architecture with Attentive CRF. (arXiv:2211.04759v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04759","description":"<p>Recognizing useful named entities plays a vital role in medical information\nprocessing, which helps drive the development of medical area research. Deep\nlearning methods have achieved good results in medical named entity recognition\n(NER). However, we find that existing methods face great challenges when\ndealing with the nested named entities. In this work, we propose a novel\nmethod, referred to as ASAC, to solve the dilemma caused by the nested\nphenomenon, in which the core idea is to model the dependency between different\ncategories of entity recognition. The proposed method contains two key modules:\nthe adaptive shared (AS) part and the attentive conditional random field (ACRF)\nmodule. The former part automatically assigns adaptive weights across each task\nto achieve optimal recognition accuracy in the multi-layer network. The latter\nmodule employs the attention operation to model the dependency between\ndifferent entities. In this way, our model could learn better entity\nrepresentations by capturing the implicit distinctions and relationships\nbetween different categories of entities. Extensive experiments on public\ndatasets verify the effectiveness of our method. Besides, we also perform\nablation analyses to deeply understand our methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junzhe Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Mingyue Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distribution-based Emotion Recognition in Conversation. (arXiv:2211.04834v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04834","description":"<p>Automatic emotion recognition in conversation (ERC) is crucial for\nemotion-aware conversational artificial intelligence. This paper proposes a\ndistribution-based framework that formulates ERC as a sequence-to-sequence\nproblem for emotion distribution estimation. The inherent ambiguity of emotions\nand the subjectivity of human perception lead to disagreements in emotion\nlabels, which is handled naturally in our framework from the perspective of\nuncertainty estimation in emotion distributions. A Bayesian training loss is\nintroduced to improve the uncertainty estimation by conditioning each emotional\nstate on an utterance-specific Dirichlet prior distribution. Experimental\nresults on the IEMOCAP dataset show that ERC outperformed the\nsingle-utterance-based system, and the proposed distribution-based ERC methods\nhave not only better classification accuracy, but also show improved\nuncertainty estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mask More and Mask Later: Efficient Pre-training of Masked Language Models by Disentangling the [MASK] Token. (arXiv:2211.04898v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04898","description":"<p>The pre-training of masked language models (MLMs) consumes massive\ncomputation to achieve good results on downstream NLP tasks, resulting in a\nlarge carbon footprint. In the vanilla MLM, the virtual tokens, [MASK]s, act as\nplaceholders and gather the contextualized information from unmasked tokens to\nrestore the corrupted information. It raises the question of whether we can\nappend [MASK]s at a later layer, to reduce the sequence length for earlier\nlayers and make the pre-training more efficient. We show: (1) [MASK]s can\nindeed be appended at a later layer, being disentangled from the word\nembedding; (2) The gathering of contextualized information from unmasked tokens\ncan be conducted with a few layers. By further increasing the masking rate from\n15% to 50%, we can pre-train RoBERTa-base and RoBERTa-large from scratch with\nonly 78% and 68% of the original computational budget without any degradation\non the GLUE benchmark. When pre-training with the original budget, our method\noutperforms RoBERTa for 6 out of 8 GLUE tasks, on average by 0.4%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1\">Baohao Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thulke_D/0/1/0/all/0/1\">David Thulke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hewavitharana_S/0/1/0/all/0/1\">Sanjika Hewavitharana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monz_C/0/1/0/all/0/1\">Christof Monz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Novel Chapter Abstractive Summarization using Spinal Tree Aware Sub-Sentential Content Selection. (arXiv:2211.04903v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04903","description":"<p>Summarizing novel chapters is a difficult task due to the input length and\nthe fact that sentences that appear in the desired summaries draw content from\nmultiple places throughout the chapter. We present a pipelined\nextractive-abstractive approach where the extractive step filters the content\nthat is passed to the abstractive component. Extremely lengthy input also\nresults in a highly skewed dataset towards negative instances for extractive\nsummarization; we thus adopt a margin ranking loss for extraction to encourage\nseparation between positive and negative examples. Our extraction component\noperates at the constituent level; our approach to this problem enriches the\ntext with spinal tree information which provides syntactic context (in the form\nof constituents) to the extraction model. We show an improvement of 3.71\nRouge-1 points over best results reported in prior work on an existing novel\nchapter dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hardy_H/0/1/0/all/0/1\">Hardy Hardy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballesteros_M/0/1/0/all/0/1\">Miguel Ballesteros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ladhak_F/0/1/0/all/0/1\">Faisal Ladhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalifa_M/0/1/0/all/0/1\">Muhammad Khalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castelli_V/0/1/0/all/0/1\">Vittorio Castelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"miCSE: Mutual Information Contrastive Learning for Low-shot Sentence Embeddings. (arXiv:2211.04928v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04928","description":"<p>This paper presents miCSE, a mutual information-based Contrastive learning\nframework that significantly advances the state-of-the-art in few-shot sentence\nembedding. The proposed approach imposes alignment between the attention\npattern of different views during contrastive learning. Learning sentence\nembeddings with miCSE entails enforcing the syntactic consistency across\naugmented views for every single sentence, making contrastive self-supervised\nlearning more sample efficient. As a result, the proposed approach shows strong\nperformance in the few-shot learning domain. While it achieves superior results\ncompared to state-of-the-art methods on multiple benchmarks in few-shot\nlearning, it is comparable in the full-shot scenario. The proposed approach is\nconceptually simple, easy to implement and optimize, yet empirically powerful.\nThis study opens up avenues for efficient self-supervised learning methods that\nare more robust than current contrastive methods for sentence embedding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klein_T/0/1/0/all/0/1\">Tassilo Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1\">Moin Nabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DoSA : A System to Accelerate Annotations on Business Documents with Human-in-the-Loop. (arXiv:2211.04934v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04934","description":"<p>Business documents come in a variety of structures, formats and information\nneeds which makes information extraction a challenging task. Due to these\nvariations, having a document generic model which can work well across all\ntypes of documents and for all the use cases seems far-fetched. For\ndocument-specific models, we would need customized document-specific labels. We\nintroduce DoSA (Document Specific Automated Annotations), which helps\nannotators in generating initial annotations automatically using our novel\nbootstrap approach by leveraging document generic datasets and models. These\ninitial annotations can further be reviewed by a human for correctness. An\ninitial document-specific model can be trained and its inference can be used as\nfeedback for generating more automated annotations. These automated annotations\ncan be reviewed by human-in-the-loop for the correctness and a new improved\nmodel can be trained using the current model as pre-trained model before going\nfor the next iteration. In this paper, our scope is limited to Form like\ndocuments due to limited availability of generic annotated datasets, but this\nidea can be extended to a variety of other documents as more datasets are\nbuilt. An open-source ready-to-use implementation is made available on GitHub\nhttps://github.com/neeleshkshukla/DoSA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shukla_N/0/1/0/all/0/1\">Neelesh K Shukla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raja_M/0/1/0/all/0/1\">Msp Raja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katikeri_R/0/1/0/all/0/1\">Raghu Katikeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaid_A/0/1/0/all/0/1\">Amit Vaid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Speech Translation with Pre-trained Models. (arXiv:2211.04939v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04939","description":"<p>When building state-of-the-art speech translation models, the need for large\ncomputational resources is a significant obstacle due to the large training\ndata size and complex models. The availability of pre-trained models is a\npromising opportunity to build strong speech translation systems efficiently.\nIn a first step, we investigate efficient strategies to build cascaded and\nend-to-end speech translation systems based on pre-trained models. Using this\nstrategy, we can train and apply the models on a single GPU. While the\nend-to-end models show superior translation performance to cascaded ones, the\napplication of this technology has a limitation on the need for additional\nend-to-end training data. In a second step, we proposed an additional\nsimilarity loss to encourage the model to generate similar hidden\nrepresentations for speech and transcript. Using this technique, we can\nincrease the data efficiency and improve the translation quality by 6 BLEU\npoints in scenarios with limited end-to-end training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaolin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niehues_J/0/1/0/all/0/1\">Jan Niehues</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating and Improving Context Attention Distribution on Multi-Turn Response Generation using Self-Contained Distractions. (arXiv:2211.04943v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04943","description":"<p>Despite the rapid progress of open-domain generation-based conversational\nagents, most deployed systems treat dialogue contexts as single-turns, while\nsystems dealing with multi-turn contexts are less studied. There is a lack of a\nreliable metric for evaluating multi-turn modelling, as well as an effective\nsolution for improving it. In this paper, we focus on an essential component of\nmulti-turn generation-based conversational agents: context attention\ndistribution, i.e. how systems distribute their attention on dialogue's\ncontext. For evaluation of this component, We introduce a novel\nattention-mechanism-based metric: DAS ratio. To improve performance on this\ncomponent, we propose an optimization strategy that employs self-contained\ndistractions. Our experiments on the Ubuntu chatlogs dataset show that models\nwith comparable perplexity can be distinguished by their ability on context\nattention distribution. Our proposed optimization strategy improves both\nnon-hierarchical and hierarchical models on the proposed metric by about 10%\nfrom baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1\">Yujie Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulla_J/0/1/0/all/0/1\">Jon Atle Gulla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accountable and Explainable Methods for Complex Reasoning over Text. (arXiv:2211.04946v1 [cs.LG])","link":"http://arxiv.org/abs/2211.04946","description":"<p>A major concern of Machine Learning (ML) models is their opacity. They are\ndeployed in an increasing number of applications where they often operate as\nblack boxes that do not provide explanations for their predictions. Among\nothers, the potential harms associated with the lack of understanding of the\nmodels' rationales include privacy violations, adversarial manipulations, and\nunfair discrimination. As a result, the accountability and transparency of ML\nmodels have been posed as critical desiderata by works in policy and law,\nphilosophy, and computer science.\n</p>\n<p>In computer science, the decision-making process of ML models has been\nstudied by developing accountability and transparency methods. Accountability\nmethods, such as adversarial attacks and diagnostic datasets, expose\nvulnerabilities of ML models that could lead to malicious manipulations or\nsystematic faults in their predictions. Transparency methods explain the\nrationales behind models' predictions gaining the trust of relevant\nstakeholders and potentially uncovering mistakes and unfairness in models'\ndecisions. To this end, transparency methods have to meet accountability\nrequirements as well, e.g., being robust and faithful to the underlying\nrationales of a model.\n</p>\n<p>This thesis presents my research that expands our collective knowledge in the\nareas of accountability and transparency of ML models developed for complex\nreasoning tasks over text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Atanasova_P/0/1/0/all/0/1\">Pepa Atanasova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Cross-modal Interactions in V&L Models that Generate Scene Descriptions. (arXiv:2211.04971v1 [cs.CL])","link":"http://arxiv.org/abs/2211.04971","description":"<p>Image captioning models tend to describe images in an object-centric way,\nemphasising visible objects. But image descriptions can also abstract away from\nobjects and describe the type of scene depicted. In this paper, we explore the\npotential of a state-of-the-art Vision and Language model, VinVL, to caption\nimages at the scene level using (1) a novel dataset which pairs images with\nboth object-centric and scene descriptions. Through (2) an in-depth analysis of\nthe effect of the fine-tuning, we show (3) that a small amount of curated data\nsuffices to generate scene descriptions without losing the capability to\nidentify object-level concepts in the scene; the model acquires a more holistic\nview of the image compared to when object-centric descriptions are generated.\nWe discuss the parallels between these results and insights from computational\nand cognitive science research on scene perception.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cafagna_M/0/1/0/all/0/1\">Michele Cafagna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1\">Albert Gatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deemter_K/0/1/0/all/0/1\">Kees van Deemter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discord Questions: A Computational Approach To Diversity Analysis in News Coverage. (arXiv:2211.05007v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05007","description":"<p>There are many potential benefits to news readers accessing diverse sources.\nModern news aggregators do the hard work of organizing the news, offering\nreaders a plethora of source options, but choosing which source to read remains\nchallenging. We propose a new framework to assist readers in identifying source\ndifferences and gaining an understanding of news coverage diversity. The\nframework is based on the generation of Discord Questions: questions with a\ndiverse answer pool, explicitly illustrating source differences. To assemble a\nprototype of the framework, we focus on two components: (1) discord question\ngeneration, the task of generating questions answered differently by sources,\nfor which we propose an automatic scoring method, and create a model that\nimproves performance from current question generation (QG) methods by 5%, (2)\nanswer consolidation, the task of grouping answers to a question that are\nsemantically similar, for which we collect data and repurpose a method that\nachieves 81% balanced accuracy on our realistic test set. We illustrate the\nframework's feasibility through a prototype interface. Even though model\nperformance at discord QG still lags human performance by more than 15%,\ngenerated questions are judged to be more interesting than factoid questions\nand can reveal differences in the level of detail, sentiment, and reasoning of\nsources in news coverage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1\">Philippe Laban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murakhovska_L/0/1/0/all/0/1\">Lidiya Murakhovs&#x27;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang &#x27;Anthony&#x27; Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Languages Unintelligible to Multilingual Models through Local Structure Probes. (arXiv:2211.05015v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05015","description":"<p>Providing better language tools for low-resource and endangered languages is\nimperative for equitable growth. Recent progress with massively multilingual\npretrained models has proven surprisingly effective at performing zero-shot\ntransfer to a wide variety of languages. However, this transfer is not\nuniversal, with many languages not currently understood by multilingual\napproaches. It is estimated that only 72 languages possess a \"small set of\nlabeled datasets\" on which we could test a model's performance, the vast\nmajority of languages not having the resources available to simply evaluate\nperformances on. In this work, we attempt to clarify which languages do and do\nnot currently benefit from such transfer. To that end, we develop a general\napproach that requires only unlabelled text to detect which languages are not\nwell understood by a cross-lingual model. Our approach is derived from the\nhypothesis that if a model's understanding is insensitive to perturbations to\ntext in a language, it is likely to have a limited understanding of that\nlanguage. We construct a cross-lingual sentence similarity task to evaluate our\napproach empirically on 350, primarily low-resource, languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clouatre_L/0/1/0/all/0/1\">Louis Clou&#xe2;tre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathi_P/0/1/0/all/0/1\">Prasanna Parthasarathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zouaq_A/0/1/0/all/0/1\">Amal Zouaq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Structure Matters Most in Most Languages. (arXiv:2211.05025v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05025","description":"<p>Many recent perturbation studies have found unintuitive results on what does\nand does not matter when performing Natural Language Understanding (NLU) tasks\nin English. Coding properties, such as the order of words, can often be removed\nthrough shuffling without impacting downstream performances. Such insight may\nbe used to direct future research into English NLP models. As many improvements\nin multilingual settings consist of wholesale adaptation of English approaches,\nit is important to verify whether those studies replicate or not in\nmultilingual settings. In this work, we replicate a study on the importance of\nlocal structure, and the relative unimportance of global structure, in a\nmultilingual setting. We find that the phenomenon observed on the English\nlanguage broadly translates to over 120 languages, with a few caveats.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clouatre_L/0/1/0/all/0/1\">Louis Clou&#xe2;tre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathi_P/0/1/0/all/0/1\">Prasanna Parthasarathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zouaq_A/0/1/0/all/0/1\">Amal Zouaq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Creative Writing with an AI-Powered Writing Assistant: Perspectives from Professional Writers. (arXiv:2211.05030v1 [cs.HC])","link":"http://arxiv.org/abs/2211.05030","description":"<p>Recent developments in natural language generation (NLG) using neural\nlanguage models have brought us closer than ever to the goal of building\nAI-powered creative writing tools. However, most prior work on human-AI\ncollaboration in the creative writing domain has evaluated new systems with\namateur writers, typically in contrived user studies of limited scope. In this\nwork, we commissioned 13 professional, published writers from a diverse set of\ncreative writing backgrounds to craft stories using Wordcraft, a text editor\nwith built-in AI-powered writing assistance tools. Using interviews and\nparticipant journals, we discuss the potential of NLG to have significant\nimpact in the creative writing domain--especially with respect to\nbrainstorming, generation of story details, world-building, and research\nassistance. Experienced writers, more so than amateurs, typically have\nwell-developed systems and methodologies for writing, as well as distinctive\nvoices and target audiences. Our work highlights the challenges in building for\nthese writers; NLG technologies struggle to preserve style and authorial voice,\nand they lack deep understanding of story contents. In order for AI-powered\nwriting assistants to realize their full potential, it is essential that they\ntake into account the diverse goals and expertise of human writers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_A/0/1/0/all/0/1\">Ann Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coenen_A/0/1/0/all/0/1\">Andy Coenen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnam_S/0/1/0/all/0/1\">Sehmon Burnam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Performance of Automatic Keyword Extraction (AKE) Methods Using PoS-Tagging and Enhanced Semantic-Awareness. (arXiv:2211.05031v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05031","description":"<p>Automatic keyword extraction (AKE) has gained more importance with the\nincreasing amount of digital textual data that modern computing systems\nprocess. It has various applications in information retrieval (IR) and natural\nlanguage processing (NLP), including text summarisation, topic analysis and\ndocument indexing. This paper proposes a simple but effective\npost-processing-based universal approach to improve the performance of any AKE\nmethods, via an enhanced level of semantic-awareness supported by PoS-tagging.\nTo demonstrate the performance of the proposed approach, we considered word\ntypes retrieved from a PoS-tagging step and two representative sources of\nsemantic information -- specialised terms defined in one or more\ncontext-dependent thesauri, and named entities in Wikipedia. The above three\nsteps can be simply added to the end of any AKE methods as part of a\npost-processor, which simply re-evaluate all candidate keywords following some\ncontext-specific and semantic-aware criteria. For five state-of-the-art (SOTA)\nAKE methods, our experimental results with 17 selected datasets showed that the\nproposed approach improved their performances both consistently (up to 100\\% in\nterms of improved cases) and significantly (between 10.2\\% and 53.8\\%, with an\naverage of 25.8\\%, in terms of F1-score and across all five methods),\nespecially when all the three enhancement steps are used. Our results have\nprofound implications considering the ease to apply our proposed approach to\nany AKE methods and to further extend it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Altuncu_E/0/1/0/all/0/1\">Enes Altuncu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nurse_J/0/1/0/all/0/1\">Jason R.C. Nurse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jie Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shujun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Contrastive Learning and Knowledge Graph Embeddings to develop medical word embeddings for the Italian language. (arXiv:2211.05035v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05035","description":"<p>Word embeddings play a significant role in today's Natural Language\nProcessing tasks and applications. While pre-trained models may be directly\nemployed and integrated into existing pipelines, they are often fine-tuned to\nbetter fit with specific languages or domains. In this paper, we attempt to\nimprove available embeddings in the uncovered niche of the Italian medical\ndomain through the combination of Contrastive Learning (CL) and Knowledge Graph\nEmbedding (KGE). The main objective is to improve the accuracy of semantic\nsimilarity between medical terms, which is also used as an evaluation task.\nSince the Italian language lacks medical texts and controlled vocabularies, we\nhave developed a specific solution by combining preexisting CL methods\n(multi-similarity loss, contextualization, dynamic sampling) and the\nintegration of KGEs, creating a new variant of the loss. Although without\nhaving outperformed the state-of-the-art, represented by multilingual models,\nthe obtained results are encouraging, providing a significant leap in\nperformance compared to the starting model, while using a significantly lower\namount of data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bondarenko_D/0/1/0/all/0/1\">Denys Amore Bondarenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrod_R/0/1/0/all/0/1\">Roger Ferrod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caro_L/0/1/0/all/0/1\">Luigi Di Caro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MACSum: Controllable Summarization with Mixed Attributes. (arXiv:2211.05041v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05041","description":"<p>Controllable summarization allows users to generate customized summaries with\nspecified attributes. However, due to the lack of designated annotations of\ncontrolled summaries, existing works have to craft pseudo datasets by adapting\ngeneric summarization benchmarks. Furthermore, most research focuses on\ncontrolling single attributes individually (e.g., a short summary or a highly\nabstractive summary) rather than controlling a mix of attributes together\n(e.g., a short and highly abstractive summary). In this paper, we propose\nMACSum, the first human-annotated summarization dataset for controlling mixed\nattributes. It contains source texts from two domains, news articles and\ndialogues, with human-annotated summaries controlled by five designed\nattributes (Length, Extractiveness, Specificity, Topic, and Speaker). We\npropose two simple and effective parameter-efficient approaches for the new\ntask of mixed controllable summarization based on hard prompt tuning and soft\nprefix tuning. Results and analysis demonstrate that hard prompt models yield\nthe best performance on all metrics and human evaluations. However,\nmixed-attribute control is still challenging for summarization tasks. Our\ndataset and code are available at https://github.com/psunlpgroup/MACSum.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yusen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuwei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What is Wrong with Language Models that Can Not Tell a Story?. (arXiv:2211.05044v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05044","description":"<p>This paper argues that a deeper understanding of narrative and the successful\ngeneration of longer subjectively interesting texts is a vital bottleneck that\nhinders the progress in modern Natural Language Processing (NLP) and may even\nbe in the whole field of Artificial Intelligence. We demonstrate that there are\nno adequate datasets, evaluation methods, and even operational concepts that\ncould be used to start working on narrative processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1\">Ivan P. Yamshchikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Transfer Learning for Check-worthy Claim Identification over Twitter. (arXiv:2211.05087v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05087","description":"<p>Misinformation spread over social media has become an undeniable infodemic.\nHowever, not all spreading claims are made equal. If propagated, some claims\ncan be destructive, not only on the individual level, but to organizations and\neven countries. Detecting claims that should be prioritized for fact-checking\nis considered the first step to fight against spread of fake news. With\ntraining data limited to a handful of languages, developing supervised models\nto tackle the problem over lower-resource languages is currently infeasible.\nTherefore, our work aims to investigate whether we can use existing datasets to\ntrain models for predicting worthiness of verification of claims in tweets in\nother languages. We present a systematic comparative study of six approaches\nfor cross-lingual check-worthiness estimation across pairs of five diverse\nlanguages with the help of Multilingual BERT (mBERT) model. We run our\nexperiments using a state-of-the-art multilingual Twitter dataset. Our results\nshow that for some language pairs, zero-shot cross-lingual transfer is possible\nand can perform as good as monolingual models that are trained on the target\nlanguage. We also show that in some languages, this approach outperforms (or at\nleast is comparable to) state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hasanain_M/0/1/0/all/0/1\">Maram Hasanain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsayed_T/0/1/0/all/0/1\">Tamer Elsayed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. (arXiv:2211.05100v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05100","description":"<p>Large language models (LLMs) have been shown to be able to perform new tasks\nbased on a few demonstrations or natural language instructions. While these\ncapabilities have led to widespread adoption, most LLMs are developed by\nresource-rich organizations and are frequently kept from the public. As a step\ntowards democratizing this powerful technology, we present BLOOM, a\n176B-parameter open-access language model designed and built thanks to a\ncollaboration of hundreds of researchers. BLOOM is a decoder-only Transformer\nlanguage model that was trained on the ROOTS corpus, a dataset comprising\nhundreds of sources in 46 natural and 13 programming languages (59 in total).\nWe find that BLOOM achieves competitive performance on a wide variety of\nbenchmarks, with stronger results after undergoing multitask prompted\nfinetuning. To facilitate future research and applications using LLMs, we\npublicly release our models and code under the Responsible AI License.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scao_T/0/1/0/all/0/1\">Teven Le Scao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1\">Angela Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akiki_C/0/1/0/all/0/1\">Christopher Akiki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilic_S/0/1/0/all/0/1\">Suzana Ili&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hesslow_D/0/1/0/all/0/1\">Daniel Hesslow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castagne_R/0/1/0/all/0/1\">Roman Castagn&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luccioni_A/0/1/0/all/0/1\">Alexandra Sasha Luccioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yvon_F/0/1/0/all/0/1\">Fran&#xe7;ois Yvon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galle_M/0/1/0/all/0/1\">Matthias Gall&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tow_J/0/1/0/all/0/1\">Jonathan Tow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1\">Albert Webson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammanamanchi_P/0/1/0/all/0/1\">Pawan Sasanka Ammanamanchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Thomas Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Beno&#xee;t Sagot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1\">Niklas Muennighoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moral_A/0/1/0/all/0/1\">Albert Villanova del Moral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruwase_O/0/1/0/all/0/1\">Olatunji Ruwase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bawden_R/0/1/0/all/0/1\">Rachel Bawden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekman_S/0/1/0/all/0/1\">Stas Bekman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMillan_Major_A/0/1/0/all/0/1\">Angelina McMillan-Major</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saulnier_L/0/1/0/all/0/1\">Lucile Saulnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Samson Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suarez_P/0/1/0/all/0/1\">Pedro Ortiz Suarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanh_V/0/1/0/all/0/1\">Victor Sanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laurencon_H/0/1/0/all/0/1\">Hugo Lauren&#xe7;on</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1\">Yacine Jernite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Launay_J/0/1/0/all/0/1\">Julien Launay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1\">Margaret Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokaslan_A/0/1/0/all/0/1\">Aaron Gokaslan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simhi_A/0/1/0/all/0/1\">Adi Simhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soroa_A/0/1/0/all/0/1\">Aitor Soroa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfassy_A/0/1/0/all/0/1\">Amit Alfassy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_A/0/1/0/all/0/1\">Anna Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nitzav_A/0/1/0/all/0/1\">Ariel Kreisberg Nitzav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_C/0/1/0/all/0/1\">Chenghao Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1\">Chris Emezue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klamm_C/0/1/0/all/0/1\">Christopher Klamm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leong_C/0/1/0/all/0/1\">Colin Leong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strien_D/0/1/0/all/0/1\">Daniel van Strien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, et al. (342 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficiently Scaling Transformer Inference. (arXiv:2211.05102v1 [cs.LG])","link":"http://arxiv.org/abs/2211.05102","description":"<p>We study the problem of efficient generative inference for Transformer\nmodels, in one of its most challenging settings: large deep models, with tight\nlatency targets and long sequence lengths. Better understanding of the\nengineering tradeoffs for inference for large Transformer-based models is\nimportant as use cases of these models are growing rapidly throughout\napplication areas. We develop a simple analytical model for inference\nefficiency to select the best multi-dimensional partitioning techniques\noptimized for TPU v4 slices based on the application requirements. We combine\nthese with a suite of low-level optimizations to achieve a new Pareto frontier\non the latency and model FLOPS utilization (MFU) tradeoffs on 500B+ parameter\nmodels that outperforms the FasterTransformer suite of benchmarks. We further\nshow that with appropriate partitioning, the lower memory requirements of\nmultiquery attention (i.e. multiple query heads share single key/value head)\nenables scaling up to 32x larger context lengths. Finally, we achieve a\nlow-batch-size latency of 29ms per token during generation (using int8 weight\nquantization) and a 76% MFU during large-batch-size processing of input tokens,\nwhile supporting a long 2048-token context length on the PaLM 540B parameter\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pope_R/0/1/0/all/0/1\">Reiner Pope</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Douglas_S/0/1/0/all/0/1\">Sholto Douglas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhery_A/0/1/0/all/0/1\">Aakanksha Chowdhery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devlin_J/0/1/0/all/0/1\">Jacob Devlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradbury_J/0/1/0/all/0/1\">James Bradbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levskaya_A/0/1/0/all/0/1\">Anselm Levskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heek_J/0/1/0/all/0/1\">Jonathan Heek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1\">Kefan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Shivani Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dean_J/0/1/0/all/0/1\">Jeff Dean</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accidental Learners: Spoken Language Identification in Multilingual Self-Supervised Models. (arXiv:2211.05103v1 [eess.AS])","link":"http://arxiv.org/abs/2211.05103","description":"<p>In this paper, we extend previous self-supervised approaches for language\nidentification by experimenting with Conformer based architecture in a\nmultilingual pre-training paradigm. We find that pre-trained speech models\noptimally encode language discriminatory information in lower layers. Further,\nwe demonstrate that the embeddings obtained from these layers are significantly\nrobust to classify unseen languages and different acoustic environments without\nadditional training. After fine-tuning a pre-trained Conformer model on the\nVoxLingua107 dataset, we achieve results similar to current state-of-the-art\nsystems for language identification. More, our model accomplishes this with 5x\nless parameters. We open-source the model through the NVIDIA NeMo toolkit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bartley_T/0/1/0/all/0/1\">Travis M. Bartley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_F/0/1/0/all/0/1\">Fei Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Puvvada_K/0/1/0/all/0/1\">Krishna C. Puvvada</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kriman_S/0/1/0/all/0/1\">Samuel Kriman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ginsburg_B/0/1/0/all/0/1\">Boris Ginsburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models with Controllable Working Memory. (arXiv:2211.05110v1 [cs.CL])","link":"http://arxiv.org/abs/2211.05110","description":"<p>Large language models (LLMs) have led to a series of breakthroughs in natural\nlanguage processing (NLP), owing to their excellent understanding and\ngeneration abilities. Remarkably, what further sets these models apart is the\nmassive amounts of world knowledge they internalize during pretraining. While\nmany downstream applications provide the model with an informational context to\naid its performance on the underlying task, how the model's world knowledge\ninteracts with the factual information presented in the context remains under\nexplored. As a desirable behavior, an LLM should give precedence to the context\nwhenever it contains task-relevant information that conflicts with the model's\nmemorized knowledge. This enables model predictions to be grounded in the\ncontext, which can then be used to update or correct specific model predictions\nwithout frequent retraining. By contrast, when the context is irrelevant to the\ntask, the model should ignore it and fall back on its internal knowledge. In\nthis paper, we undertake a first joint study of the aforementioned two\nproperties, namely controllability and robustness, in the context of LLMs. We\ndemonstrate that state-of-the-art T5 and PaLM (both pretrained and finetuned)\ncould exhibit poor controllability and robustness, which do not scale with\nincreasing model size. As a solution, we propose a novel method - Knowledge\nAware FineTuning (KAFT) - to strengthen both controllability and robustness by\nincorporating counterfactual and irrelevant contexts to standard supervised\ndatasets. Our comprehensive evaluation showcases the utility of KAFT across\nmodel architectures and sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Daliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_A/0/1/0/all/0/1\">Ankit Singh Rawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Manzil Zaheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasik_M/0/1/0/all/0/1\">Michal Lukasik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1\">Andreas Veit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Felix Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The challenges of temporal alignment on Twitter during crises. (arXiv:2104.08535v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08535","description":"<p>Language use changes over time, and this impacts the effectiveness of NLP\nsystems. This phenomenon is even more prevalent in social media data during\ncrisis events where meaning and frequency of word usage may change over the\ncourse of days. Contextual language models fail to adapt temporally,\nemphasizing the need for temporal adaptation in models which need to be\ndeployed over an extended period of time. While existing approaches consider\ndata spanning large periods of time (from years to decades), shorter time spans\nare critical for crisis data. We quantify temporal degradation for this\nscenario and propose methods to cope with performance loss by leveraging\ntechniques from domain adaptation. To the best of our knowledge, this is the\nfirst effort to explore effects of rapid language change driven by adversarial\nadaptations, particularly during natural and human-induced disasters. Through\nextensive experimentation on diverse crisis datasets, we analyze under what\nconditions our approaches outperform strong baselines while highlighting the\ncurrent limitations of temporal adaptation methods in scenarios where access to\nunlabeled data is scarce.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pramanick_A/0/1/0/all/0/1\">Aniket Pramanick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beck_T/0/1/0/all/0/1\">Tilman Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stowe_K/0/1/0/all/0/1\">Kevin Stowe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering the Unknown Knowns: Turning Implicit Knowledge in the Dataset into Explicit Training Examples for Visual Question Answering. (arXiv:2109.06122v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06122","description":"<p>Visual question answering (VQA) is challenging not only because the model has\nto handle multi-modal information, but also because it is just so hard to\ncollect sufficient training examples -- there are too many questions one can\nask about an image. As a result, a VQA model trained solely on human-annotated\nexamples could easily over-fit specific question styles or image contents that\nare being asked, leaving the model largely ignorant about the sheer diversity\nof questions. Existing methods address this issue primarily by introducing an\nauxiliary task such as visual grounding, cycle consistency, or debiasing. In\nthis paper, we take a drastically different approach. We found that many of the\n\"unknowns\" to the learned VQA model are indeed \"known\" in the dataset\nimplicitly. For instance, questions asking about the same object in different\nimages are likely paraphrases; the number of detected or annotated objects in\nan image already provides the answer to the \"how many\" question, even if the\nquestion has not been annotated for that image. Building upon these insights,\nwe present a simple data augmentation pipeline SimpleAug to turn this \"known\"\nknowledge into training examples for VQA. We show that these augmented examples\ncan notably improve the learned VQA models' performance, not only on the VQA-CP\ndataset with language prior shifts but also on the VQA v2 dataset without such\nshifts. Our method further opens up the door to leverage weakly-labeled or\nunlabeled images in a principled way to enhance VQA models. Our code and data\nare publicly available at https://github.com/heendung/simpleAUG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kil_J/0/1/0/all/0/1\">Jihyung Kil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_D/0/1/0/all/0/1\">Dong Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Will You Find These Shortcuts?\" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification. (arXiv:2111.07367v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.07367","description":"<p>Feature attribution a.k.a. input salience methods which assign an importance\nscore to a feature are abundant but may produce surprisingly different results\nfor the same model on the same input. While differences are expected if\ndisparate definitions of importance are assumed, most methods claim to provide\nfaithful attributions and point at the features most relevant for a model's\nprediction. Existing work on faithfulness evaluation is not conclusive and does\nnot provide a clear answer as to how different methods are to be compared.\nFocusing on text classification and the model debugging scenario, our main\ncontribution is a protocol for faithfulness evaluation that makes use of\npartially synthetic data to obtain ground truth for feature importance ranking.\nFollowing the protocol, we do an in-depth analysis of four standard salience\nmethod classes on a range of datasets and shortcuts for BERT and LSTM models\nand demonstrate that some of the most popular method configurations provide\npoor results even for simplest shortcuts. We recommend following the protocol\nfor each new task and model combination to find the best method for identifying\nshortcuts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bastings_J/0/1/0/all/0/1\">Jasmijn Bastings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebert_S/0/1/0/all/0/1\">Sebastian Ebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zablotskaia_P/0/1/0/all/0/1\">Polina Zablotskaia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandholm_A/0/1/0/all/0/1\">Anders Sandholm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filippova_K/0/1/0/all/0/1\">Katja Filippova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M-MELD: A Multilingual Multi-Party Dataset for Emotion Recognition in Conversations. (arXiv:2203.16799v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16799","description":"<p>Expression of emotions is a crucial part of daily human communication.\nEmotion recognition in conversations (ERC) is an emerging field of study, where\nthe primary task is to identify the emotion behind each utterance in a\nconversation. Though a lot of work has been done on ERC in the past, these\nworks only focus on ERC in the English language, thereby ignoring any other\nlanguages. In this paper, we present Multilingual MELD (M-MELD), where we\nextend the Multimodal EmotionLines Dataset (MELD) \\cite{poria2018meld} to 4\nother languages beyond English, namely Greek, Polish, French, and Spanish.\nBeyond just establishing strong baselines for all of these 4 languages, we also\npropose a novel architecture, DiscLSTM, that uses both sequential and\nconversational discourse context in a conversational dialogue for ERC. Our\nproposed approach is computationally efficient, can transfer across languages\nusing just a cross-lingual encoder, and achieves better performance than most\nuni-modal text approaches in the literature on both MELD and M-MELD. We make\nour data and code publicly on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramaneswaran_S/0/1/0/all/0/1\">S Ramaneswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_U/0/1/0/all/0/1\">Utkarsh Tyagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_H/0/1/0/all/0/1\">Harshvardhan Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepcha_S/0/1/0/all/0/1\">Samden Lepcha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakshi_S/0/1/0/all/0/1\">S Sakshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Near-Negative Distinction: Giving a Second Life to Human Evaluation Datasets. (arXiv:2205.06871v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.06871","description":"<p>Precisely assessing the progress in natural language generation (NLG) tasks\nis challenging, and human evaluation to establish a preference in a model's\noutput over another is often necessary. However, human evaluation is usually\ncostly, difficult to reproduce, and non-reusable. In this paper, we propose a\nnew and simple automatic evaluation method for NLG called Near-Negative\nDistinction (NND) that repurposes prior human annotations into NND tests. In an\nNND test, an NLG model must place a higher likelihood on a high-quality output\ncandidate than on a near-negative candidate with a known error. Model\nperformance is established by the number of NND tests a model passes, as well\nas the distribution over task-specific errors the model fails on. Through\nexperiments on three NLG tasks (question generation, question answering, and\nsummarization), we show that NND achieves a higher correlation with human\njudgments than standard NLG evaluation metrics. We then illustrate NND\nevaluation in four practical scenarios, for example performing fine-grain model\nanalysis, or studying model training dynamics. Our findings suggest that NND\ncan give a second life to human annotations and provide low-cost NLG\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1\">Philippe Laban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empathetic Conversational Systems: A Review of Current Advances, Gaps, and Opportunities. (arXiv:2206.05017v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.05017","description":"<p>Empathy is a vital factor that contributes to mutual understanding, and joint\nproblem-solving. In recent years, a growing number of studies have recognized\nthe benefits of empathy and started to incorporate empathy in conversational\nsystems. We refer to this topic as empathetic conversational systems. To\nidentify the critical gaps and future opportunities in this topic, this paper\nexamines this rapidly growing field using five review dimensions: (i)\nconceptual empathy models and frameworks, (ii) adopted empathy-related\nconcepts, (iii) datasets and algorithmic techniques developed, (iv) evaluation\nstrategies, and (v) state-of-the-art approaches. The findings show that most\nstudies have centered on the use of the EMPATHETICDIALOGUES dataset, and the\ntext-based modality dominates research in this field. Studies mainly focused on\nextracting features from the messages of the users and the conversational\nsystems, with minimal emphasis on user modeling and profiling. Notably, studies\nthat have incorporated emotion causes, external knowledge, and affect matching\nin the response generation models, have obtained significantly better results.\nFor implementation in diverse real-world settings, we recommend that future\nstudies should address key gaps in areas of detecting and authenticating\nemotions at the entity level, handling multimodal inputs, displaying more\nnuanced empathetic behaviors, and encompassing additional dialogue system\nfeatures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raamkumar_A/0/1/0/all/0/1\">Aravind Sesagiri Raamkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinping Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Automatic Evaluation of the WMT22 General Machine Translation Task. (arXiv:2209.14172v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.14172","description":"<p>This report presents an automatic evaluation of the general machine\ntranslation task of the Seventh Conference on Machine Translation (WMT22). It\nevaluates a total of 185 systems for 21 translation directions including\nhigh-resource to low-resource language pairs and from closely related to\ndistant languages. This large-scale automatic evaluation highlights some of the\ncurrent limits of state-of-the-art machine translation systems. It also shows\nhow automatic metrics, namely chrF, BLEU, and COMET, can complement themselves\nto mitigate their own limits in terms of interpretability and accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marie_B/0/1/0/all/0/1\">Benjamin Marie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code-Switching without Switching: Language Agnostic End-to-End Speech Translation. (arXiv:2210.01512v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.01512","description":"<p>We propose a) a Language Agnostic end-to-end Speech Translation model (LAST),\nand b) a data augmentation strategy to increase code-switching (CS)\nperformance. With increasing globalization, multiple languages are increasingly\nused interchangeably during fluent speech. Such CS complicates traditional\nspeech recognition and translation, as we must recognize which language was\nspoken first and then apply a language-dependent recognizer and subsequent\ntranslation component to generate the desired target language output. Such a\npipeline introduces latency and errors. In this paper, we eliminate the need\nfor that, by treating speech recognition and translation as one unified\nend-to-end speech translation problem. By training LAST with both input\nlanguages, we decode speech into one target language, regardless of the input\nlanguage. LAST delivers comparable recognition and speech translation accuracy\nin monolingual usage, while reducing latency and error rate considerably when\nCS is observed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huber_C/0/1/0/all/0/1\">Christian Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ugan_E/0/1/0/all/0/1\">Enes Yavuz Ugan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alexander Waibel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable Causal Analysis of Mental Health on Social Media Data. (arXiv:2210.08430v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08430","description":"<p>With recent developments in Social Computing, Natural Language Processing and\nClinical Psychology, the social NLP research community addresses the challenge\nof automation in mental illness on social media. A recent extension to the\nproblem of multi-class classification of mental health issues is to identify\nthe cause behind the user's intention. However, multi-class causal\ncategorization for mental health issues on social media has a major challenge\nof wrong prediction due to the overlapping problem of causal explanations.\nThere are two possible mitigation techniques to solve this problem: (i)\nInconsistency among causal explanations/ inappropriate human-annotated\ninferences in the dataset, (ii) in-depth analysis of arguments and stances in\nself-reported text using discourse analysis. In this research work, we\nhypothesise that if there exists the inconsistency among F1 scores of different\nclasses, there must be inconsistency among corresponding causal explanations as\nwell. In this task, we fine tune the classifiers and find explanations for\nmulti-class causal categorization of mental illness on social media with LIME\nand Integrated Gradient (IG) methods. We test our methods with CAMS dataset and\nvalidate with annotated interpretations. A key contribution of this research\nwork is to find the reason behind inconsistency in accuracy of multi-class\ncausal categorization. The effectiveness of our methods is evident with the\nresults obtained having category-wise average scores of $81.29 \\%$ and $0.906$\nusing cosine similarity and word mover's distance, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saxena_C/0/1/0/all/0/1\">Chandni Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_M/0/1/0/all/0/1\">Muskan Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ansari_G/0/1/0/all/0/1\">Gunjan Ansari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Too Brittle To Touch: Comparing the Stability of Quantization and Distillation Towards Developing Lightweight Low-Resource MT Models. (arXiv:2210.15184v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.15184","description":"<p>Leveraging shared learning through Massively Multilingual Models,\nstate-of-the-art machine translation models are often able to adapt to the\npaucity of data for low-resource languages. However, this performance comes at\nthe cost of significantly bloated models which are not practically deployable.\nKnowledge Distillation is one popular technique to develop competitive,\nlightweight models: In this work, we first evaluate its use to compress MT\nmodels focusing on languages with extremely limited training data. Through our\nanalysis across 8 languages, we find that the variance in the performance of\nthe distilled models due to their dependence on priors including the amount of\nsynthetic data used for distillation, the student architecture, training\nhyperparameters and confidence of the teacher models, makes distillation a\nbrittle compression mechanism. To mitigate this, we explore the use of\npost-training quantization for the compression of these models. Here, we find\nthat while distillation provides gains across some low-resource languages,\nquantization provides more consistent performance trends for the entire range\nof languages, especially the lowest-resource languages in our target set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diddee_H/0/1/0/all/0/1\">Harshita Diddee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dandapat_S/0/1/0/all/0/1\">Sandipan Dandapat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganu_T/0/1/0/all/0/1\">Tanuja Ganu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bali_K/0/1/0/all/0/1\">Kalika Bali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DORE: Document Ordered Relation Extraction based on Generative Framework. (arXiv:2210.16064v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.16064","description":"<p>In recent years, there is a surge of generation-based information extraction\nwork, which allows a more direct use of pre-trained language models and\nefficiently captures output dependencies. However, previous generative methods\nusing lexical representation do not naturally fit document-level relation\nextraction (DocRE) where there are multiple entities and relational facts. In\nthis paper, we investigate the root cause of the underwhelming performance of\nthe existing generative DocRE models and discover that the culprit is the\ninadequacy of the training paradigm, instead of the capacities of the models.\nWe propose to generate a symbolic and ordered sequence from the relation matrix\nwhich is deterministic and easier for model to learn. Moreover, we design a\nparallel row generation method to process overlong target sequences. Besides,\nwe introduce several negative sampling strategies to improve the performance\nwith balanced signals. Experimental results on four datasets show that our\nproposed method can improve the performance of the generative DocRE models. We\nhave released our code at https://github.com/ayyyq/DORE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qipeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"User-Entity Differential Privacy in Learning Natural Language Models. (arXiv:2211.01141v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2211.01141","description":"<p>In this paper, we introduce a novel concept of user-entity differential\nprivacy (UeDP) to provide formal privacy protection simultaneously to both\nsensitive entities in textual data and data owners in learning natural language\nmodels (NLMs). To preserve UeDP, we developed a novel algorithm, called\nUeDP-Alg, optimizing the trade-off between privacy loss and model utility with\na tight sensitivity bound derived from seamlessly combining user and sensitive\nentity sampling processes. An extensive theoretical analysis and evaluation\nshow that our UeDP-Alg outperforms baseline approaches in model utility under\nthe same privacy budget consumption on several NLM tasks, using benchmark\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_P/0/1/0/all/0/1\">Phung Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_N/0/1/0/all/0/1\">NhatHai Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Rajiv Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiuxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barmpalios_N/0/1/0/all/0/1\">Nikolaos Barmpalios</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Zero-Shot Code-Switched Speech Recognition. (arXiv:2211.01458v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.01458","description":"<p>In this work, we seek to build effective code-switched (CS) automatic speech\nrecognition systems (ASR) under the zero-shot setting where no transcribed CS\nspeech data is available for training. Previously proposed frameworks which\nconditionally factorize the bilingual task into its constituent monolingual\nparts are a promising starting point for leveraging monolingual data\nefficiently. However, these methods require the monolingual modules to perform\nlanguage segmentation. That is, each monolingual module has to simultaneously\ndetect CS points and transcribe speech segments of one language while ignoring\nthose of other languages -- not a trivial task. We propose to simplify each\nmonolingual module by allowing them to transcribe all speech segments\nindiscriminately with a monolingual script (i.e. transliteration). This simple\nmodification passes the responsibility of CS point detection to subsequent\nbilingual modules which determine the final output by considering multiple\nmonolingual transliterations along with external language model information. We\napply this transliteration-based approach in an end-to-end differentiable\nneural network and demonstrate its efficacy for zero-shot CS ASR on\nMandarin-English SEAME test sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiesner_M/0/1/0/all/0/1\">Matthew Wiesner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klejch_O/0/1/0/all/0/1\">Ondrej Klejch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1\">Preethi Jyothi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid-SD (H_SD): A new hybrid evaluation metric for automatic speech recognition tasks. (arXiv:2211.01722v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.01722","description":"<p>Many studies have examined the shortcomings of word error rate (WER) as an\nevaluation metric for automatic speech recognition (ASR) systems, particularly\nwhen used for spoken language understanding tasks such as intent recognition\nand dialogue systems. In this paper, we propose Hybrid-SD (H_SD), a new hybrid\nevaluation metric for ASR systems that takes into account both semantic\ncorrectness and error rate. To generate sentence dissimilarity scores (SD), we\nbuilt a fast and lightweight SNanoBERT model using distillation techniques. Our\nexperiments show that the SNanoBERT model is 25.9x smaller and 38.8x faster\nthan SRoBERTa while achieving comparable results on well-known benchmarks.\nHence, making it suitable for deploying with ASR models on edge devices. We\nalso show that H_SD correlates more strongly with downstream tasks such as\nintent recognition and named-entity recognition (NER).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sasindran_Z/0/1/0/all/0/1\">Zitha Sasindran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yelchuri_H/0/1/0/all/0/1\">Harsha Yelchuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1\">Supreeth Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakar_T/0/1/0/all/0/1\">T. V. Prabhakar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-11-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}