{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-12-14T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Understanding (Un)Intended Memorization in Text-to-Image Generative Models. (arXiv:2312.07550v1 [cs.CV])","link":"http://arxiv.org/abs/2312.07550","description":"<p>Multimodal machine learning, especially text-to-image models like Stable\nDiffusion and DALL-E 3, has gained significance for transforming text into\ndetailed images.\n</p>\n<p>Despite their growing use and remarkable generative capabilities, there is a\npressing need for a detailed examination of these models' behavior,\nparticularly with respect to memorization. Historically, memorization in\nmachine learning has been context-dependent, with diverse definitions emerging\nfrom classification tasks to complex models like Large Language Models (LLMs)\nand Diffusion models. Yet, a definitive concept of memorization that aligns\nwith the intricacies of text-to-image synthesis remains elusive. This\nunderstanding is vital as memorization poses privacy risks yet is essential for\nmeeting user expectations, especially when generating representations of\nunderrepresented entities. In this paper, we introduce a specialized definition\nof memorization tailored to text-to-image models, categorizing it into three\ndistinct types according to user expectations. We closely examine the subtle\ndistinctions between intended and unintended memorization, emphasizing the\nimportance of balancing user privacy with the generative quality of the model\noutputs. Using the Stable Diffusion model, we offer examples to validate our\nmemorization definitions and clarify their application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naseh_A/0/1/0/all/0/1\">Ali Naseh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roh_J/0/1/0/all/0/1\">Jaechul Roh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houmansadr_A/0/1/0/all/0/1\">Amir Houmansadr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Model Alignment with Elastic Reset. (arXiv:2312.07551v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07551","description":"<p>Finetuning language models with reinforcement learning (RL), e.g. from human\nfeedback (HF), is a prominent method for alignment. But optimizing against a\nreward model can improve on reward while degrading performance in other areas,\na phenomenon known as reward hacking, alignment tax, or language drift. First,\nwe argue that commonly-used test metrics are insufficient and instead measure\nhow different algorithms tradeoff between reward and drift. The standard method\nmodified the reward with a Kullback-Lieber (KL) penalty between the online and\ninitial model. We propose Elastic Reset, a new algorithm that achieves higher\nreward with less drift without explicitly modifying the training objective. We\nperiodically reset the online model to an exponentially moving average (EMA) of\nitself, then reset the EMA model to the initial model. Through the use of an\nEMA, our model recovers quickly after resets and achieves higher reward with\nless drift in the same number of steps. We demonstrate that fine-tuning\nlanguage models with Elastic Reset leads to state-of-the-art performance on a\nsmall scale pivot-translation benchmark, outperforms all baselines in a\nmedium-scale RLHF-like IMDB mock sentiment task and leads to a more performant\nand more aligned technical QA chatbot with LLaMA-7B. Code available at\ngithub.com/mnoukhov/elastic-reset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Noukhovitch_M/0/1/0/all/0/1\">Michael Noukhovitch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavoie_S/0/1/0/all/0/1\">Samuel Lavoie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strub_F/0/1/0/all/0/1\">Florian Strub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models for Intent-Driven Session Recommendations. (arXiv:2312.07552v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07552","description":"<p>Intent-aware session recommendation (ISR) is pivotal in discerning user\nintents within sessions for precise predictions. Traditional approaches,\nhowever, face limitations due to their presumption of a uniform number of\nintents across all sessions. This assumption overlooks the dynamic nature of\nuser sessions, where the number and type of intentions can significantly vary.\nIn addition, these methods typically operate in latent spaces, thus hinder the\nmodel's transparency.Addressing these challenges, we introduce a novel ISR\napproach, utilizing the advanced reasoning capabilities of large language\nmodels (LLMs). First, this approach begins by generating an initial prompt that\nguides LLMs to predict the next item in a session, based on the varied intents\nmanifested in user sessions. Then, to refine this process, we introduce an\ninnovative prompt optimization mechanism that iteratively self-reflects and\nadjusts prompts. Furthermore, our prompt selection module, built upon the LLMs'\nbroad adaptability, swiftly selects the most optimized prompts across diverse\ndomains. This new paradigm empowers LLMs to discern diverse user intents at a\nsemantic level, leading to more accurate and interpretable session\nrecommendations. Our extensive experiments on three real-world datasets\ndemonstrate the effectiveness of our method, marking a significant advancement\nin ISR systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xinghua Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1\">Kaidong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_Y/0/1/0/all/0/1\">Yew-Soon Ong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hijacking Context in Large Multi-modal Models. (arXiv:2312.07553v1 [cs.AI])","link":"http://arxiv.org/abs/2312.07553","description":"<p>Recently, Large Multi-modal Models (LMMs) have demonstrated their ability to\nunderstand the visual contents of images given the instructions regarding the\nimages. Built upon the Large Language Models (LLMs), LMMs also inherit their\nabilities and characteristics such as in-context learning where a coherent\nsequence of images and texts are given as the input prompt. However, we\nidentify a new limitation of off-the-shelf LMMs where a small fraction of\nincoherent images or text descriptions mislead LMMs to only generate biased\noutput about the hijacked context, not the originally intended context. To\naddress this, we propose a pre-filtering method that removes irrelevant\ncontexts via GPT-4V, based on its robustness towards distribution shift within\nthe contexts. We further investigate whether replacing the hijacked visual and\ntextual contexts with the correlated ones via GPT-4V and text-to-image models\ncan help yield coherent responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Joonhyun Jeong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Learning for Short Text Clustering. (arXiv:2312.07556v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07556","description":"<p>Short text clustering has been popularly studied for its significance in\nmining valuable insights from many short texts. In this paper, we focus on the\nfederated short text clustering (FSTC) problem, i.e., clustering short texts\nthat are distributed in different clients, which is a realistic problem under\nprivacy requirements. Compared with the centralized short text clustering\nproblem that short texts are stored on a central server, the FSTC problem has\nnot been explored yet. To fill this gap, we propose a Federated Robust Short\nText Clustering (FSTC) framework. FSTC includes two main modules, i.e., robust\nshort text clustering module and federated cluster center aggregation module.\nThe robust short text clustering module aims to train an effective short text\nclustering model with local data in each client. We innovatively combine\noptimal transport to generate pseudo-labels with Gaussian-uniform mixture model\nto ensure the reliability of the pseudo-supervised data. The federated cluster\ncenter aggregation module aims to exchange knowledge across clients without\nsharing local raw data in an efficient way. The server aggregates the local\ncluster centers from different clients and then sends the global centers back\nto all clients in each communication round. Our empirical studies on three\nshort text clustering datasets demonstrate that FSTC significantly outperforms\nthe federated short text clustering baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Mengling Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaochao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1\">Xinting Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaolin Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PaperQA: Retrieval-Augmented Generative Agent for Scientific Research. (arXiv:2312.07559v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07559","description":"<p>Large Language Models (LLMs) generalize well across language tasks, but\nsuffer from hallucinations and uninterpretability, making it difficult to\nassess their accuracy without ground-truth. Retrieval-Augmented Generation\n(RAG) models have been proposed to reduce hallucinations and provide provenance\nfor how an answer was generated. Applying such models to the scientific\nliterature may enable large-scale, systematic processing of scientific\nknowledge. We present PaperQA, a RAG agent for answering questions over the\nscientific literature. PaperQA is an agent that performs information retrieval\nacross full-text scientific articles, assesses the relevance of sources and\npassages, and uses RAG to provide answers. Viewing this agent as a question\nanswering model, we find it exceeds performance of existing LLMs and LLM agents\non current science QA benchmarks. To push the field closer to how humans\nperform research on scientific literature, we also introduce LitQA, a more\ncomplex benchmark that requires retrieval and synthesis of information from\nfull-text scientific papers across the literature. Finally, we demonstrate\nPaperQA's matches expert human researchers on LitQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lala_J/0/1/0/all/0/1\">Jakub L&#xe1;la</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODonoghue_O/0/1/0/all/0/1\">Odhran O&#x27;Donoghue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shtedritski_A/0/1/0/all/0/1\">Aleksandar Shtedritski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cox_S/0/1/0/all/0/1\">Sam Cox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriques_S/0/1/0/all/0/1\">Samuel G. Rodriques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1\">Andrew D. White</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arabic Handwritten Text Line Dataset. (arXiv:2312.07573v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07573","description":"<p>Segmentation of Arabic manuscripts into lines of text and words is an\nimportant step to make recognition systems more efficient and accurate. The\nproblem of segmentation into text lines is solved since there are carefully\nannotated dataset dedicated to this task. However, To the best of our\nknowledge, there are no dataset annotating the word position of Arabic texts.\nIn this paper, we present a new dataset specifically designed for historical\nArabic script in which we annotate position in word level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bouchal_H/0/1/0/all/0/1\">Hakim Bouchal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belaid_A/0/1/0/all/0/1\">Ahror Belaid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConvD: Attention Enhanced Dynamic Convolutional Embeddings for Knowledge Graph Completion. (arXiv:2312.07589v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07589","description":"<p>Knowledge graphs generally suffer from incompleteness, which can be\nalleviated by completing the missing information. Deep knowledge convolutional\nembedding models based on neural networks are currently popular methods for\nknowledge graph completion. However, most existing methods use external\nconvolution kernels and traditional plain convolution processes, which limits\nthe feature interaction capability of the model. In this paper, we propose a\nnovel dynamic convolutional embedding model ConvD for knowledge graph\ncompletion, which directly reshapes the relation embeddings into multiple\ninternal convolution kernels to improve the external convolution kernels of the\ntraditional convolutional embedding model. The internal convolution kernels can\neffectively augment the feature interaction between the relation embeddings and\nentity embeddings, thus enhancing the model embedding performance. Moreover, we\ndesign a priori knowledge-optimized attention mechanism, which can assign\ndifferent contribution weight coefficients to multiple relation convolution\nkernels for dynamic convolution to improve the expressiveness of the model\nfurther. Extensive experiments on various datasets show that our proposed model\nconsistently outperforms the state-of-the-art baseline methods, with average\nimprovements ranging from 11.30\\% to 16.92\\% across all model evaluation\nmetrics. Ablation experiments verify the effectiveness of each component module\nof the ConvD model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wenbin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zirui Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating ChatGPT as a Question Answering System: A Comprehensive Analysis and Comparison with Existing Models. (arXiv:2312.07592v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07592","description":"<p>In the current era, a multitude of language models has emerged to cater to\nuser inquiries. Notably, the GPT-3.5 Turbo language model has gained\nsubstantial attention as the underlying technology for ChatGPT. Leveraging\nextensive parameters, this model adeptly responds to a wide range of questions.\nHowever, due to its reliance on internal knowledge, the accuracy of responses\nmay not be absolute. This article scrutinizes ChatGPT as a Question Answering\nSystem (QAS), comparing its performance to other existing QASs. The primary\nfocus is on evaluating ChatGPT's proficiency in extracting responses from\nprovided paragraphs, a core QAS capability. Additionally, performance\ncomparisons are made in scenarios without a surrounding passage. Multiple\nexperiments, exploring response hallucination and considering question\ncomplexity, were conducted on ChatGPT. Evaluation employed well-known Question\nAnswering (QA) datasets, including SQuAD, NewsQA, and PersianQuAD, across\nEnglish and Persian languages. Metrics such as F-score, exact match, and\naccuracy were employed in the assessment. The study reveals that, while ChatGPT\ndemonstrates competence as a generative model, it is less effective in question\nanswering compared to task-specific models. Providing context improves its\nperformance, and prompt engineering enhances precision, particularly for\nquestions lacking explicit answers in provided paragraphs. ChatGPT excels at\nsimpler factual questions compared to \"how\" and \"why\" question types. The\nevaluation highlights occurrences of hallucinations, where ChatGPT provides\nresponses to questions without available answers in the provided context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bahak_H/0/1/0/all/0/1\">Hossein Bahak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taheri_F/0/1/0/all/0/1\">Farzaneh Taheri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zojaji_Z/0/1/0/all/0/1\">Zahra Zojaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazemi_A/0/1/0/all/0/1\">Arefeh Kazemi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive News and Social Media Linking using BERT for Articles and Tweets across Dual Platforms. (arXiv:2312.07599v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07599","description":"<p>X (formerly Twitter) has evolved into a contemporary agora, offering a\nplatform for individuals to express opinions and viewpoints on current events.\nThe majority of the topics discussed on Twitter are directly related to ongoing\nevents, making it an important source for monitoring public discourse. However,\nlinking tweets to specific news presents a significant challenge due to their\nconcise and informal nature. Previous approaches, including topic models,\ngraph-based models, and supervised classifiers, have fallen short in\neffectively capturing the unique characteristics of tweets and articles.\n</p>\n<p>Inspired by the success of the CLIP model in computer vision, which employs\ncontrastive learning to model similarities between images and captions, this\npaper introduces a contrastive learning approach for training a representation\nspace where linked articles and tweets exhibit proximity. We present our\ncontrastive learning approach, CATBERT (Contrastive Articles Tweets BERT),\nleveraging pre-trained BERT models. The model is trained and tested on a\ndataset containing manually labeled English and Polish tweets and articles\nrelated to the Russian-Ukrainian war. We evaluate CATBERT's performance against\ntraditional approaches like LDA, and the novel method based on OpenAI\nembeddings, which has not been previously applied to this task. Our findings\nindicate that CATBERT demonstrates superior performance in associating tweets\nwith relevant news articles. Furthermore, we demonstrate the performance of the\nmodels when applied to finding the main topic -- represented by an article --\nof the whole cascade of tweets. In this new task, we report the performance of\nthe different models in dependence on the cascade size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Piotrowski_J/0/1/0/all/0/1\">Jan Piotrowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachnicki_M/0/1/0/all/0/1\">Marek Wachnicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perlik_M/0/1/0/all/0/1\">Mateusz Perlik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Podolak_J/0/1/0/all/0/1\">Jakub Podolak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rucki_G/0/1/0/all/0/1\">Grzegorz Rucki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brzozowski_M/0/1/0/all/0/1\">Micha&#x142; Brzozowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olejnik_P/0/1/0/all/0/1\">Pawe&#x142; Olejnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozlowski_J/0/1/0/all/0/1\">Julian Koz&#x142;owski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nocon_T/0/1/0/all/0/1\">Tomasz Noco&#x144;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koziel_J/0/1/0/all/0/1\">Jakub Kozie&#x142;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gizinski_S/0/1/0/all/0/1\">Stanis&#x142;aw Gizi&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankowski_P/0/1/0/all/0/1\">Piotr Sankowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mathematical Language Models: A Survey. (arXiv:2312.07622v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07622","description":"<p>In recent years, there has been remarkable progress in leveraging Language\nModels (LMs), encompassing Pre-trained Language Models (PLMs) and Large-scale\nLanguage Models (LLMs), within the domain of mathematics. This paper conducts a\ncomprehensive survey of mathematical LMs, systematically categorizing pivotal\nresearch endeavors from two distinct perspectives: tasks and methodologies. The\nlandscape reveals a large number of proposed mathematical LLMs, which are\nfurther delineated into instruction learning, tool-based methods, fundamental\nCoT techniques, and advanced CoT methodologies. In addition, our survey entails\nthe compilation of over 60 mathematical datasets, including training datasets,\nbenchmark datasets, and augmented datasets. Addressing the primary challenges\nand delineating future trajectories within the field of mathematical LMs, this\nsurvey is positioned as a valuable resource, poised to facilitate and inspire\nfuture innovation among researchers invested in advancing this domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hanglei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuyang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junsong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jiayi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mengliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aimin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Liang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor. (arXiv:2312.07661v1 [cs.CV])","link":"http://arxiv.org/abs/2312.07661","description":"<p>Existing open-vocabulary image segmentation methods require a fine-tuning\nstep on mask annotations and/or image-text datasets. Mask labels are\nlabor-intensive, which limits the number of categories in segmentation\ndatasets. As a result, the open-vocabulary capacity of pre-trained VLMs is\nseverely reduced after fine-tuning. However, without fine-tuning, VLMs trained\nunder weak image-text supervision tend to make suboptimal mask predictions when\nthere are text queries referring to non-existing concepts in the image. To\nalleviate these issues, we introduce a novel recurrent framework that\nprogressively filters out irrelevant texts and enhances mask quality without\ntraining efforts. The recurrent unit is a two-stage segmenter built upon a VLM\nwith frozen weights. Thus, our model retains the VLM's broad vocabulary space\nand strengthens its segmentation capability. Experimental results show that our\nmethod outperforms not only the training-free counterparts, but also those\nfine-tuned with millions of additional data samples, and sets new\nstate-of-the-art records for both zero-shot semantic and referring image\nsegmentation tasks. Specifically, we improve the current record by 28.8, 16.0,\nand 6.9 mIoU on Pascal VOC, COCO Object, and Pascal Context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shuyang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Runjia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiuye Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FULL-W2V: Fully Exploiting Data Reuse for W2V on GPU-Accelerated Systems. (arXiv:2312.07743v1 [cs.LG])","link":"http://arxiv.org/abs/2312.07743","description":"<p>Word2Vec remains one of the highly-impactful innovations in the field of\nNatural Language Processing (NLP) that represents latent grammatical and\nsyntactical information in human text with dense vectors in a low dimension.\nWord2Vec has high computational cost due to the algorithm's inherent\nsequentiality, intensive memory accesses, and the large vocabularies it\nrepresents. While prior studies have investigated technologies to explore\nparallelism and improve memory system performance, they struggle to effectively\ngain throughput on powerful GPUs.\n</p>\n<p>We identify memory data access and latency as the primary bottleneck in prior\nworks on GPUs, which prevents highly optimized kernels from attaining the\narchitecture's peak performance. We present a novel algorithm, FULL-W2V, which\nmaximally exploits the opportunities for data reuse in the W2V algorithm and\nleverages GPU architecture and resources to reduce access to low memory levels\nand improve temporal locality. FULL-W2V is capable of reducing accesses to GPU\nglobal memory significantly, e.g., by more than 89\\%, compared to prior\nstate-of-the-art GPU implementations, resulting in significant performance\nimprovement that scales across successive hardware generations. Our prototype\nimplementation achieves 2.97X speedup when ported from Nvidia Pascal P100 to\nVolta V100 cards, and outperforms the state-of-the-art by 5.72X on V100 cards\nwith the same embedding quality. In-depth analysis indicates that the reduction\nof memory accesses through register and shared memory caching and\nhigh-throughput shared memory reduction leads to a significantly improved\narithmetic intensity. FULL-W2V can potentially benefit many applications in NLP\nand other domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Randall_T/0/1/0/all/0/1\">Thomas Randall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allen_T/0/1/0/all/0/1\">Tyler Allen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1\">Rong Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Human Language Models: A Need and the Challenges. (arXiv:2312.07751v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07751","description":"<p>As research in human-centered NLP advances, there is a growing recognition of\nthe importance of incorporating human and social factors into NLP models. At\nthe same time, our NLP systems have become heavily reliant on LLMs, most of\nwhich do not model authors. To build NLP systems that can truly understand\nhuman language, we must better integrate human contexts into LLMs. This brings\nto the fore a range of design considerations and challenges in terms of what\nhuman aspects to capture, how to represent them, and what modeling strategies\nto pursue. To address these, we advocate for three positions toward creating\nlarge human language models (LHLMs) using concepts from psychological and\nbehavioral sciences: First, LM training should include the human context.\nSecond, LHLMs should recognize that people are more than their group(s). Third,\nLHLMs should be able to account for the dynamic and temporally-dependent nature\nof the human context. We refer to relevant advances and present open challenges\nthat need to be addressed and their possible solutions in realizing these\ngoals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soni_N/0/1/0/all/0/1\">Nikita Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_H/0/1/0/all/0/1\">H. Andrew Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can LLM find the green circle? Investigation and Human-guided tool manipulation for compositional generalization. (arXiv:2312.07763v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07763","description":"<p>The meaning of complex phrases in natural language is composed of their\nindividual components. The task of compositional generalization evaluates a\nmodel's ability to understand new combinations of components. Previous studies\ntrained smaller, task-specific models, which exhibited poor generalization.\nWhile large language models (LLMs) exhibit impressive generalization abilities\non many tasks through in-context learning (ICL), their potential for\ncompositional generalization remains unexplored. In this paper, we first\nempirically investigate prevailing ICL methods in compositional generalization.\nWe find that they struggle with complex compositional questions due to\ncumulative errors in long reasoning steps and intricate logic required for\ntool-making. Consequently, we propose a human-guided tool manipulation\nframework (HTM) that generates tools for sub-questions and integrates multiple\ntools. Our method enhances the effectiveness of tool creation and usage with\nminimal human effort. Experiments show that our method achieves\nstate-of-the-art performance on two compositional generalization benchmarks and\noutperforms existing methods on the most challenging test split by 70%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Shuo Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_M/0/1/0/all/0/1\">Murong Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Linhang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chang-Tien Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harnessing Retrieval-Augmented Generation (RAG) for Uncovering Knowledge Gaps. (arXiv:2312.07796v1 [cs.IR])","link":"http://arxiv.org/abs/2312.07796","description":"<p>The paper presents a methodology for uncovering knowledge gaps on the\ninternet using the Retrieval Augmented Generation (RAG) model. By simulating\nuser search behaviour, the RAG system identifies and addresses gaps in\ninformation retrieval systems. The study demonstrates the effectiveness of the\nRAG system in generating relevant suggestions with a consistent accuracy of\n93%. The methodology can be applied in various fields such as scientific\ndiscovery, educational enhancement, research development, market analysis,\nsearch engine optimisation, and content development. The results highlight the\nvalue of identifying and understanding knowledge gaps to guide future\nendeavours.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hurtado_J/0/1/0/all/0/1\">Joan Figuerola Hurtado</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment analysis in Tourism: Fine-tuning BERT or sentence embeddings concatenation?. (arXiv:2312.07797v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07797","description":"<p>Undoubtedly that the Bidirectional Encoder representations from Transformers\nis the most powerful technique in making Natural Language Processing tasks such\nas Named Entity Recognition, Question &amp; Answers or Sentiment Analysis, however,\nthe use of traditional techniques remains a major potential for the improvement\nof recent models, in particular word tokenization techniques and embeddings,\nbut also the improvement of neural network architectures which are now the core\nof each architecture. recent. In this paper, we conduct a comparative study\nbetween Fine-Tuning the Bidirectional Encoder Representations from Transformers\nand a method of concatenating two embeddings to boost the performance of a\nstacked Bidirectional Long Short-Term Memory-Bidirectional Gated Recurrent\nUnits model; these two approaches are applied in the context of sentiment\nanalysis of shopping places in Morocco. A search for the best learning rate was\nmade at the level of the two approaches, and a comparison of the best\noptimizers was made for each sentence embedding combination with regard to the\nsecond approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bouabdallaoui_I/0/1/0/all/0/1\">Ibrahim Bouabdallaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerouate_F/0/1/0/all/0/1\">Fatima Guerouate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouhaddour_S/0/1/0/all/0/1\">Samya Bouhaddour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saadi_C/0/1/0/all/0/1\">Chaimae Saadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sbihi_M/0/1/0/all/0/1\">Mohammed Sbihi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Native Language Identification with Large Language Models. (arXiv:2312.07819v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07819","description":"<p>We present the first experiments on Native Language Identification (NLI)\nusing LLMs such as GPT-4. NLI is the task of predicting a writer's first\nlanguage by analyzing their writings in a second language, and is used in\nsecond language acquisition and forensic linguistics. Our results show that GPT\nmodels are proficient at NLI classification, with GPT-4 setting a new\nperformance record of 91.7% on the benchmark TOEFL11 test set in a zero-shot\nsetting. We also show that unlike previous fully-supervised settings, LLMs can\nperform NLI without being limited to a set of known classes, which has\npractical implications for real-world applications. Finally, we also show that\nLLMs can provide justification for their choices, providing reasoning based on\nspelling errors, syntactic patterns, and usage of directly translated\nlinguistic patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salle_A/0/1/0/all/0/1\">Alexandre Salle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Learning-Based System for Automatic Case Summarization. (arXiv:2312.07824v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07824","description":"<p>This paper presents a deep learning-based system for efficient automatic case\nsummarization. Leveraging state-of-the-art natural language processing\ntechniques, the system offers both supervised and unsupervised methods to\ngenerate concise and relevant summaries of lengthy legal case documents. The\nuser-friendly interface allows users to browse the system's database of legal\ncase documents, select their desired case, and choose their preferred\nsummarization method. The system generates comprehensive summaries for each\nsubsection of the legal text as well as an overall summary. This demo\nstreamlines legal case document analysis, potentially benefiting legal\nprofessionals by reducing workload and increasing efficiency. Future work will\nfocus on refining summarization techniques and exploring the application of our\nmethods to other types of legal texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duong_M/0/1/0/all/0/1\">Minh Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Long Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuong_Y/0/1/0/all/0/1\">Yen Vuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Trong Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha-Thanh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Abusive Span Detection for Vietnamese Narrative Texts. (arXiv:2312.07831v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07831","description":"<p>Abuse in its various forms, including physical, psychological, verbal,\nsexual, financial, and cultural, has a negative impact on mental health.\nHowever, there are limited studies on applying natural language processing\n(NLP) in this field in Vietnam. Therefore, we aim to contribute by building a\nhuman-annotated Vietnamese dataset for detecting abusive content in Vietnamese\nnarrative texts. We sourced these texts from VnExpress, Vietnam's popular\nonline newspaper, where readers often share stories containing abusive content.\nIdentifying and categorizing abusive spans in these texts posed significant\nchallenges during dataset creation, but it also motivated our research. We\nexperimented with lightweight baseline models by freezing PhoBERT and\nXLM-RoBERTa and using their hidden states in a BiLSTM to assess the complexity\nof the dataset. According to our experimental results, PhoBERT outperforms\nother models in both labeled and unlabeled abusive span detection tasks. These\nresults indicate that it has the potential for future improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Nhu-Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_K/0/1/0/all/0/1\">Khoa Thi-Kim Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duc-Vu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finetuning an LLM on Contextual Knowledge of Classics for Q&A. (arXiv:2312.07848v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07848","description":"<p>The open-source publishing of large language models (LLMs) has created many\npossibilities for how anyone who understands language and has access to a\ncomputer can interact with significant tools of artificial intelligence,\nparticularly in the context of learning and knowledge dissemination. However,\nthe utility of these models in specialized fields like Classics is still\nlargely unexplored. This project is an attempt to merge the knowledge of\nClassics with the capabilities of artificial intelligence by finetuning an LLM\nto cater to the specific needs of learners and professionals. The goal of this\nproject is to develop an LLM that not only reproduces contextual knowledge\naccurately but also exhibits a consistent \"personality\" - and, indeed, has\nconsistent propriety - to appeal to a diverse audience who possess differing\nlevels of knowledge. A significant portion of this project was dedicated to\nrefining the dataset, following the principle of \"garbage in, garbage out,\" to\nensure the model generates relevant, useful, and creative responses when given\na prompt (a statement, question, or single word). After training and\nevaluation, my model's ability to handle a vast array of different types of\ninputs and prompting exceeded expectations for a 355M parameter model, though\nits occasional hallucinations (especially when set with a high temperature),\nparticularly in its assertions about historical events or its own identity,\nmake it seem somewhat capricious and more work in the form of continuous\nfinetuning will be undertaken.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Strachan_S/0/1/0/all/0/1\">Shane Storm Strachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BESTMVQA: A Benchmark Evaluation System for Medical Visual Question Answering. (arXiv:2312.07867v1 [cs.AI])","link":"http://arxiv.org/abs/2312.07867","description":"<p>Medical Visual Question Answering (Med-VQA) is a very important task in\nhealthcare industry, which answers a natural language question with a medical\nimage. Existing VQA techniques in information systems can be directly applied\nto solving the task. However, they often suffer from (i) the data insufficient\nproblem, which makes it difficult to train the state of the arts (SOTAs) for\nthe domain-specific task, and (ii) the reproducibility problem, that many\nexisting models have not been thoroughly evaluated in a unified experimental\nsetup. To address these issues, this paper develops a Benchmark Evaluation\nSysTem for Medical Visual Question Answering, denoted by BESTMVQA. Given\nself-collected clinical data, our system provides a useful tool for users to\nautomatically build Med-VQA datasets, which helps overcoming the data\ninsufficient problem. Users also can conveniently select a wide spectrum of\nSOTA models from our model library to perform a comprehensive empirical study.\nWith simple configurations, our system automatically trains and evaluates the\nselected models over a benchmark dataset, and reports the comprehensive results\nfor users to develop new techniques or perform medical practice. Limitations of\nexisting work are overcome (i) by the data generation tool, which automatically\nconstructs new datasets from unstructured clinical data, and (ii) by evaluating\nSOTAs on benchmark datasets in a unified experimental setup. The demonstration\nvideo of our system can be found at https://youtu.be/QkEeFlu1x4A. Our code and\ndata will be available soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1\">Xiaojie Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zixin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feiyan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph vs. Sequence: An Empirical Study on Knowledge Forms for Knowledge-Grounded Dialogue. (arXiv:2312.07868v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07868","description":"<p>Knowledge-grounded dialogue is a task of generating an informative response\nbased on both the dialogue history and external knowledge source. In general,\nthere are two forms of knowledge: manually annotated knowledge graphs and\nknowledge text from website. From various evaluation viewpoints, each type of\nknowledge has advantages and downsides. To further distinguish the principles\nand determinants from the intricate factors, we conduct a thorough experiment\nand study on the task to answer three essential questions. The questions\ninvolve the choice of appropriate knowledge form, the degree of mutual effects\nbetween knowledge and the model selection, and the few-shot performance of\nknowledge. Supported by statistical shreds of evidence, we offer conclusive\nsolutions and sensible suggestions for directions and standards of future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yizhe Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yihang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modality Plug-and-Play: Elastic Modality Adaptation in Multimodal LLMs for Embodied AI. (arXiv:2312.07886v1 [cs.AI])","link":"http://arxiv.org/abs/2312.07886","description":"<p>Large Language Models (LLMs) are capable of reasoning over diverse input data\nmodalities through pre-trained encoders. However, the growing diversity of\ninput data modalities prevents incorporating all modalities into LLMs,\nespecially when LLMs are deployed on resource-constrained edge devices for\nembodied AI applications. Instead, a better option is to adaptively involve\nonly the useful modalities at runtime, depending on the current environmental\ncontexts and task requirements. For such modality adaptation, existing work\nadopts fixed connections between encoders and the LLM's input layer, leading to\nhigh training cost at runtime and ineffective cross-modal interaction. In this\npaper, we address these limitations by presenting mPnP-LLM, a new technique\nthat allows fully elastic, automated and prompt runtime modality adaptation, by\nconnecting unimodal encoders to a flexible set of last LLM blocks and making\nsuch latent connections fully trainable at runtime. Experiments over the\nnuScenes-QA dataset show that mPnP-LLM can achieve up to 3.7x FLOPs reduction\nand 30% GPU memory usage reduction, while retaining on-par accuracy with the\nexisting schemes. Under the same compute budget, mPnP-LLM improves the task\naccuracy by up to 4% compared to the best existing scheme.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Boyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models. (arXiv:2312.07887v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07887","description":"<p>Incremental Learning (IL) has been a long-standing problem in both vision and\nNatural Language Processing (NLP) communities. In recent years, as Pre-trained\nLanguage Models (PLMs) have achieved remarkable progress in various NLP\ndownstream tasks, utilizing PLMs as backbones has become a common practice in\nrecent research of IL in NLP. Most assume that catastrophic forgetting is the\nbiggest obstacle to achieving superior IL performance and propose various\ntechniques to overcome this issue. However, we find that this assumption is\nproblematic. Specifically, we revisit more than 20 methods on four\nclassification tasks (Text Classification, Intent Classification, Relation\nExtraction, and Named Entity Recognition) under the two most popular IL\nsettings (Class-Incremental and Task-Incremental) and reveal that most of them\nseverely underestimate the inherent anti-forgetting ability of PLMs. Based on\nthe observation, we propose a frustratingly easy method called SEQ* for IL with\nPLMs. The results show that SEQ* has competitive or superior performance\ncompared to state-of-the-art (SOTA) IL methods and requires considerably less\ntrainable parameters and training time. These findings urge us to revisit the\nIL with PLMs and encourage future studies to have a fundamental understanding\nof the catastrophic forgetting in PLMs. The data, code and scripts are publicly\navailable at\nhttps://github.com/zzz47zzz/pretrained-lm-for-incremental-learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Junhao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Shengjie Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qianli Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptBench: A Unified Library for Evaluation of Large Language Models. (arXiv:2312.07910v1 [cs.AI])","link":"http://arxiv.org/abs/2312.07910","description":"<p>The evaluation of large language models (LLMs) is crucial to assess their\nperformance and mitigate potential security risks. In this paper, we introduce\nPromptBench, a unified library to evaluate LLMs. It consists of several key\ncomponents that are easily used and extended by researchers: prompt\nconstruction, prompt engineering, dataset and model loading, adversarial prompt\nattack, dynamic evaluation protocols, and analysis tools. PromptBench is\ndesigned to be an open, general, and flexible codebase for research purposes\nthat can facilitate original study in creating new benchmarks, deploying\ndownstream applications, and designing new evaluation protocols. The code is\navailable at: https://github.com/microsoft/promptbench and will be continuously\nsupported.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kaijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qinlin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Text Watermarking in the Era of Large Language Models. (arXiv:2312.07913v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07913","description":"<p>In recent years, significant advancements have been made in the text\ngeneration capabilities of Large Language Models (LLMs), demonstrating\nexceptional performance in downstream tasks such as abstract summarization,\ndialogue generation, and data-to-text conversion. However, their generative\nabilities also pose risks such as the rapid spread of fake news, infringement\nof datasets/LLM copyrights, and challenges to academic integrity. Text\nwatermarking technology emerges as a potential solution. By embedding invisible\nyet detectable patterns in generated texts, it helps in tracking and verifying\ntext origins, thus preventing misuse and piracy.\n</p>\n<p>This survey aims to comprehensively summarize current text watermarking\ntechnologies, covering three main aspects: (1) an overview and comparison of\ndifferent text watermarking techniques; (2) evaluation methods for text\nwatermarking algorithms, including their success rate, impact on text quality,\nrobustness, and unforgeability; (3) potential applications of text watermarking\ntechnologys. This survey aims to help researchers thoroughly understanding the\ntext watermarking technologies, thereby fostering further development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Leyi Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Optimal Statistical Watermarking. (arXiv:2312.07930v1 [cs.LG])","link":"http://arxiv.org/abs/2312.07930","description":"<p>We study statistical watermarking by formulating it as a hypothesis testing\nproblem, a general framework which subsumes all previous statistical\nwatermarking methods. Key to our formulation is a coupling of the output tokens\nand the rejection region, realized by pseudo-random generators in practice,\nthat allows non-trivial trade-off between the Type I error and Type II error.\nWe characterize the Uniformly Most Powerful (UMP) watermark in this context. In\nthe most common scenario where the output is a sequence of $n$ tokens, we\nestablish matching upper and lower bounds on the number of i.i.d. tokens\nrequired to guarantee small Type I and Type II errors. Our rate scales as\n$\\Theta(h^{-1} \\log (1/h))$ with respect to the average entropy per token $h$\nand thus greatly improves the $O(h^{-2})$ rate in the previous works. For\nscenarios where the detector lacks knowledge of the model's distribution, we\nintroduce the concept of model-agnostic watermarking and establish the minimax\nbounds for the resultant increase in Type II error. Moreover, we formulate the\nrobust watermarking problem where user is allowed to perform a class of\nperturbation on the generated texts, and characterize the optimal type II error\nof robust UMP tests via a linear programming problem. To the best of our\nknowledge, this is the first systematic statistical treatment on the\nwatermarking problem with near-optimal rates in the i.i.d. setting, and might\nbe of interest for future works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Baihe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Banghua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hanlin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jason D. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jiantao Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CBQ: Cross-Block Quantization for Large Language Models. (arXiv:2312.07950v1 [cs.LG])","link":"http://arxiv.org/abs/2312.07950","description":"<p>Post-training quantization (PTQ) has driven attention to producing efficient\nlarge language models (LLMs) with ultra-low costs. Since hand-craft\nquantization parameters lead to low performance in low-bit quantization, recent\nmethods optimize the quantization parameters through block-wise reconstruction\nbetween the floating-point and quantized models. However, these methods suffer\nfrom two challenges: accumulated errors from independent one-by-one block\nquantization and reconstruction difficulties from extreme weight and activation\noutliers. To address these two challenges, we propose CBQ, a cross-block\nreconstruction-based PTQ method for LLMs. To reduce error accumulation, we\nintroduce a cross-block dependency with the aid of a homologous reconstruction\nscheme to build the long-range dependency between adjacent multi-blocks with\noverlapping. To reduce reconstruction difficulty, we design a coarse-to-fine\npre-processing (CFP) to truncate weight outliers and dynamically scale\nactivation outliers before optimization, and an adaptive rounding scheme,\ncalled LoRA-Rounding, with two low-rank learnable matrixes to further rectify\nweight quantization errors. Extensive experiments demonstrate that: (1) CBQ\npushes both activation and weight quantization to low-bit settings W4A4, W4A8,\nand W2A16. (2) CBQ achieves better performance than the existing\nstate-of-the-art methods on various LLMs and benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhijun Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhiwei Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Baoqun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Few-Shot Named Entity Recognition with Boundary Discrimination and Correlation Purification. (arXiv:2312.07961v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07961","description":"<p>Few-shot named entity recognition (NER) aims to recognize novel named\nentities in low-resource domains utilizing existing knowledge. However, the\npresent few-shot NER models assume that the labeled data are all clean without\nnoise or outliers, and there are few works focusing on the robustness of the\ncross-domain transfer learning ability to textual adversarial attacks in\nFew-shot NER. In this work, we comprehensively explore and assess the\nrobustness of few-shot NER models under textual adversarial attack scenario,\nand found the vulnerability of existing few-shot NER models. Furthermore, we\npropose a robust two-stage few-shot NER method with Boundary Discrimination and\nCorrelation Purification (BDCP). Specifically, in the span detection stage, the\nentity boundary discriminative module is introduced to provide a highly\ndistinguishing boundary representation space to detect entity spans. In the\nentity typing stage, the correlations between entities and contexts are\npurified by minimizing the interference information and facilitating\ncorrelation generalization to alleviate the perturbations caused by textual\nadversarial attacks. In addition, we construct adversarial examples for\nfew-shot NER based on public datasets Few-NERD and Cross-Dataset. Comprehensive\nevaluations on those two groups of few-shot NER datasets containing adversarial\nexamples demonstrate the robustness and superiority of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiaojun Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chunxia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianxiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhendong Niu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SLJP: Semantic Extraction based Legal Judgment Prediction. (arXiv:2312.07979v1 [cs.CL])","link":"http://arxiv.org/abs/2312.07979","description":"<p>Legal Judgment Prediction (LJP) is a judicial assistance system that\nrecommends the legal components such as applicable statues, prison term and\npenalty term by analyzing the given input case document. Indian legal system is\nin the need of technical assistance such as artificial intelligence to solve\nthe crores of pending cases in various courts for years and its being increased\nday to day. Most of the existing Indian models did not adequately concentrate\non the semantics embedded in the fact description (FD) that impacts the\ndecision. The proposed semantic extraction based LJP (SLJP) model provides the\nadvantages of pretrained transformers for complex unstructured legal case\ndocument understanding and to generate embeddings. The model draws the in-depth\nsemantics of the given FD at multiple levels i.e., chunk and case document\nlevel by following the divide and conquer approach. It creates the concise view\nof the given fact description using the extracted semantics as per the original\ncourt case document structure and predicts judgment using attention mechanism.\nWe tested the model performance on two available Indian datasets Indian Legal\nDocuments corpus (ILDC) and Indian Legal Statue Identification (ILSI) and got\npromising results. Also shown the highest performance and less performance\ndegradation for increased epochs than base models on ILDC dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madambakam_P/0/1/0/all/0/1\">Prameela Madambakam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajmohan_S/0/1/0/all/0/1\">Shathanaa Rajmohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_H/0/1/0/all/0/1\">Himangshu Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1\">Tummepalli Anka Chandrahas Purushotham Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention. (arXiv:2312.07987v1 [cs.LG])","link":"http://arxiv.org/abs/2312.07987","description":"<p>The costly self-attention layers in modern Transformers require memory and\ncompute quadratic in sequence length. Existing approximation methods usually\nunderperform and fail to obtain significant speedups in practice. Here we\npresent SwitchHead - a novel method that reduces both compute and memory\nrequirements and achieves wall-clock speedup, while matching the language\nmodeling performance of baseline Transformers with the same parameter budget.\nSwitchHead uses Mixture-of-Experts (MoE) layers for the value and output\nprojections and requires 4 to 8 times fewer attention matrices than standard\nTransformers. Our novel attention can also be combined with MoE MLP layers,\nresulting in an efficient fully-MoE \"SwitchHead\" Transformer model. Our code is\npublic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Csordas_R/0/1/0/all/0/1\">R&#xf3;bert Csord&#xe1;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piekos_P/0/1/0/all/0/1\">Piotr Pi&#x119;kos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irie_K/0/1/0/all/0/1\">Kazuki Irie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Helping Language Models Learn More: Multi-dimensional Task Prompt for Few-shot Tuning. (arXiv:2312.08027v1 [cs.CL])","link":"http://arxiv.org/abs/2312.08027","description":"<p>Large language models (LLMs) can be used as accessible and intelligent\nchatbots by constructing natural language queries and directly inputting the\nprompt into the large language model. However, different prompt' constructions\noften lead to uncertainty in the answers and thus make it hard to utilize the\nspecific knowledge of LLMs (like ChatGPT). To alleviate this, we use an\ninterpretable structure to explain the prompt learning principle in LLMs, which\ncertificates that the effectiveness of language models is determined by\nposition changes of the task's related tokens. Therefore, we propose MTPrompt,\na multi-dimensional task prompt learning method consisting based on\ntask-related object, summary, and task description information. By\nautomatically building and searching for appropriate prompts, our proposed\nMTPrompt achieves the best results on few-shot samples setting and five\ndifferent datasets. In addition, we demonstrate the effectiveness and stability\nof our method in different experimental settings and ablation experiments. In\ninteraction with large language models, embedding more task-related information\ninto prompts will make it easier to stimulate knowledge embedded in large\nlanguage models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weng_J/0/1/0/all/0/1\">Jinta Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiarui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fa_D/0/1/0/all/0/1\">Daidong Fa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuand_X/0/1/0/all/0/1\">Xiaofeng Xuand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoRTEx: Contrastive Learning for Representing Terms via Explanations with Applications on Constructing Biomedical Knowledge Graphs. (arXiv:2312.08036v1 [cs.CL])","link":"http://arxiv.org/abs/2312.08036","description":"<p>Objective: Biomedical Knowledge Graphs play a pivotal role in various\nbiomedical research domains. Concurrently, term clustering emerges as a crucial\nstep in constructing these knowledge graphs, aiming to identify synonymous\nterms. Due to a lack of knowledge, previous contrastive learning models trained\nwith Unified Medical Language System (UMLS) synonyms struggle at clustering\ndifficult terms and do not generalize well beyond UMLS terms. In this work, we\nleverage the world knowledge from Large Language Models (LLMs) and propose\nContrastive Learning for Representing Terms via Explanations (CoRTEx) to\nenhance term representation and significantly improves term clustering.\nMaterials and Methods: The model training involves generating explanations for\na cleaned subset of UMLS terms using ChatGPT. We employ contrastive learning,\nconsidering term and explanation embeddings simultaneously, and progressively\nintroduce hard negative samples. Additionally, a ChatGPT-assisted BIRCH\nalgorithm is designed for efficient clustering of a new ontology. Results: We\nestablished a clustering test set and a hard negative test set, where our model\nconsistently achieves the highest F1 score. With CoRTEx embeddings and the\nmodified BIRCH algorithm, we grouped 35,580,932 terms from the Biomedical\nInformatics Ontology System (BIOS) into 22,104,559 clusters with O(N) queries\nto ChatGPT. Case studies highlight the model's efficacy in handling challenging\nsamples, aided by information from explanations. Conclusion: By aligning terms\nto their explanations, CoRTEx demonstrates superior accuracy over benchmark\nmodels and robustness beyond its training set, and it is suitable for\nclustering terms for large-scale biomedical ontologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ying_H/0/1/0/all/0/1\">Huaiyuan Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhengyun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1\">Sihang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sheng Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimation of Concept Explanations Should be Uncertainty Aware. (arXiv:2312.08063v1 [cs.LG])","link":"http://arxiv.org/abs/2312.08063","description":"<p>Model explanations are very valuable for interpreting and debugging\nprediction models. We study a specific kind of global explanations called\nConcept Explanations, where the goal is to interpret a model using\nhuman-understandable concepts. Recent advances in multi-modal learning\nrekindled interest in concept explanations and led to several label-efficient\nproposals for estimation. However, existing estimation methods are unstable to\nthe choice of concepts or dataset that is used for computing explanations. We\nobserve that instability in explanations is due to high variance in point\nestimation of importance scores. We propose an uncertainty aware Bayesian\nestimation method, which readily improved reliability of the concept\nexplanations. We demonstrate with theoretical analysis and empirical evaluation\nthat explanations computed by our method are more reliable while also being\nlabel-efficient and faithful.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Piratla_V/0/1/0/all/0/1\">Vihari Piratla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_J/0/1/0/all/0/1\">Juyeon Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sukriti Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic Image-Report Generation. (arXiv:2312.08078v1 [cs.CV])","link":"http://arxiv.org/abs/2312.08078","description":"<p>To address these issues, we propose a novel Adaptive patch-word Matching\n(AdaMatch) model to correlate chest X-ray (CXR) image regions with words in\nmedical reports and apply it to CXR-report generation to provide explainability\nfor the generation process. AdaMatch exploits the fine-grained relation between\nadaptive patches and words to provide explanations of specific image regions\nwith corresponding words. To capture the abnormal regions of varying sizes and\npositions, we introduce the Adaptive Patch extraction (AdaPatch) module to\nacquire the adaptive patches for these regions adaptively. In order to provide\nexplicit explainability for CXR-report generation task, we propose an\nAdaMatch-based bidirectional large language model for Cyclic CXR-report\ngeneration (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords\nfor CXR images and `keypatches' for medical reports as hints to guide\nCXR-report generation. Extensive experiments on two publicly available CXR\ndatasets prove the effectiveness of our method and its superior performance to\nexisting methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yixuan Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extending Whisper with prompt tuning to target-speaker ASR. (arXiv:2312.08079v1 [cs.CL])","link":"http://arxiv.org/abs/2312.08079","description":"<p>Target-speaker automatic speech recognition (ASR) aims to transcribe the\ndesired speech of a target speaker from multi-talker overlapped utterances.\nMost of the existing target-speaker ASR (TS-ASR) methods involve either\ntraining from scratch or fully fine-tuning a pre-trained model, leading to\nsignificant training costs and becoming inapplicable to large foundation\nmodels. This work leverages prompt tuning, a parameter-efficient fine-tuning\napproach, to extend Whisper, a large-scale single-talker ASR model, to TS-ASR.\nExperimental results show that prompt tuning can achieve performance comparable\nto state-of-the-art full fine-tuning approaches while only requiring about 1%\nof task-specific model parameters. Notably, the original Whisper's features,\nsuch as inverse text normalization and timestamp prediction, are retained in\ntarget-speaker ASR, keeping the generated transcriptions natural and\ninformative.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhiyuan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1\">Mingjie Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ju Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Model-Based Data Acquisition for Subjective Multi-Task NLP Problems. (arXiv:2312.08198v1 [cs.CL])","link":"http://arxiv.org/abs/2312.08198","description":"<p>Data annotated by humans is a source of knowledge by describing the\npeculiarities of the problem and therefore fueling the decision process of the\ntrained model. Unfortunately, the annotation process for subjective natural\nlanguage processing (NLP) problems like offensiveness or emotion detection is\noften very expensive and time-consuming. One of the inevitable risks is to\nspend some of the funds and annotator effort on annotations that do not provide\nany additional knowledge about the specific task. To minimize these costs, we\npropose a new model-based approach that allows the selection of tasks annotated\nindividually for each text in a multi-task scenario. The experiments carried\nout on three datasets, dozens of NLP tasks, and thousands of annotations show\nthat our method allows up to 40% reduction in the number of annotations with\nnegligible loss of knowledge. The results also emphasize the need to collect a\ndiverse amount of data required to efficiently train a model, depending on the\nsubjectivity of the annotation task. We also focused on measuring the relation\nbetween subjective tasks by evaluating the model in single-task and multi-task\nscenarios. Moreover, for some datasets, training only on the labels predicted\nby our model improved the efficiency of task selection as a self-supervised\nlearning regularization technique.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanclerz_K/0/1/0/all/0/1\">Kamil Kanclerz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielaniewicz_J/0/1/0/all/0/1\">Julita Bielaniewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gruza_M/0/1/0/all/0/1\">Marcin Gruza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocon_J/0/1/0/all/0/1\">Jan Kocon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wozniak_S/0/1/0/all/0/1\">Stanis&#x142;aw Wo&#x17a;niak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazienko_P/0/1/0/all/0/1\">Przemys&#x142;aw Kazienko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-throughput Biomedical Relation Extraction for Semi-Structured Web Articles Empowered by Large Language Models. (arXiv:2312.08274v1 [cs.CL])","link":"http://arxiv.org/abs/2312.08274","description":"<p>Objective: To develop a high-throughput biomedical relation extraction system\nthat takes advantage of the large language models' (LLMs) reading comprehension\nability and biomedical world knowledge in a scalable and evidential manner.\nMethods: We formulate the relation extraction task as a simple binary\nclassification problem for large language models such as ChatGPT. Specifically,\nLLMs make the decision based on the external corpus and its world knowledge,\ngiving the reason for the judgment to factual verification. This method is\ntailored for semi-structured web articles, wherein we designate the main title\nas the tail entity and explicitly incorporate it into the context, and the\npotential head entities are matched based on a biomedical thesaurus. Moreover,\nlengthy contents are sliced into text chunks, embedded, and retrieved with\nadditional embedding models, ensuring compatibility with the context window\nsize constraints of available open-source LLMs. Results: Using an open-source\nLLM, we extracted 304315 relation triplets of three distinct relation types\nfrom four reputable biomedical websites. To assess the efficacy of the basic\npipeline employed for biomedical relation extraction, we curated a benchmark\ndataset annotated by a medical expert. Evaluation results indicate that the\npipeline exhibits performance comparable to that of GPT-4. Case studies further\nilluminate challenges faced by contemporary LLMs in the context of biomedical\nrelation extraction for semi-structured web articles. Conclusion: The proposed\nmethod has demonstrated its effectiveness in leveraging the strengths of LLMs\nfor high-throughput biomedical relation extraction. Its adaptability is\nevident, as it can be seamlessly extended to diverse semi-structured biomedical\nwebsites, facilitating the extraction of various types of biomedical relations\nwith ease.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Songchi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sheng Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting LLMs with content plans to enhance the summarization of scientific articles. (arXiv:2312.08282v1 [cs.CL])","link":"http://arxiv.org/abs/2312.08282","description":"<p>This paper presents novel prompting techniques to improve the performance of\nautomatic summarization systems for scientific articles. Scientific article\nsummarization is highly challenging due to the length and complexity of these\ndocuments. We conceive, implement, and evaluate prompting techniques that\nprovide additional contextual information to guide summarization systems.\nSpecifically, we feed summarizers with lists of key terms extracted from\narticles, such as author keywords or automatically generated keywords. Our\ntechniques are tested with various summarization models and input texts.\nResults show performance gains, especially for smaller models summarizing\nsections separately. This evidences that prompting is a promising approach to\novercoming the limitations of less powerful systems. Our findings introduce a\nnew research direction of using prompts to aid smaller models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Creo_A/0/1/0/all/0/1\">Aldan Creo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lama_M/0/1/0/all/0/1\">Manuel Lama</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_J/0/1/0/all/0/1\">Juan C. Vidal</a> (1) ((1) Singular Research Center on Intelligent Technologies (CiTIUS), Universidade de Santiago de Compostela, Santiago de Compostela, Spain)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conceptualizing Suicidal Behavior: Utilizing Explanations of Predicted Outcomes to Analyze Longitudinal Social Media Data. (arXiv:2312.08299v1 [cs.CL])","link":"http://arxiv.org/abs/2312.08299","description":"<p>The COVID-19 pandemic has escalated mental health crises worldwide, with\nsocial isolation and economic instability contributing to a rise in suicidal\nbehavior. Suicide can result from social factors such as shame, abuse,\nabandonment, and mental health conditions like depression, Post-Traumatic\nStress Disorder (PTSD), Attention-Deficit/Hyperactivity Disorder (ADHD),\nanxiety disorders, and bipolar disorders. As these conditions develop, signs of\nsuicidal ideation may manifest in social media interactions. Analyzing social\nmedia data using artificial intelligence (AI) techniques can help identify\npatterns of suicidal behavior, providing invaluable insights for suicide\nprevention agencies, professionals, and broader community awareness\ninitiatives. Machine learning algorithms for this purpose require large volumes\nof accurately labeled data. Previous research has not fully explored the\npotential of incorporating explanations in analyzing and labeling longitudinal\nsocial media data. In this study, we employed a model explanation method, Layer\nIntegrated Gradients, on top of a fine-tuned state-of-the-art language model,\nto assign each token from Reddit users' posts an attribution score for\npredicting suicidal ideation. By extracting and analyzing attributions of\ntokens from the data, we propose a methodology for preliminary screening of\nsocial media posts for suicidal ideation without using large language models\nduring inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Van Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nur_N/0/1/0/all/0/1\">Nasheen Nur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stern_W/0/1/0/all/0/1\">William Stern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mercer_T/0/1/0/all/0/1\">Thomas Mercer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_C/0/1/0/all/0/1\">Chiradeep Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_S/0/1/0/all/0/1\">Siddhartha Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tumbiolo_V/0/1/0/all/0/1\">Victor Tumbiolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_S/0/1/0/all/0/1\">Seng Jhing Goh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models. (arXiv:2312.08303v1 [cs.CL])","link":"http://arxiv.org/abs/2312.08303","description":"<p>Toxic content detection is crucial for online services to remove\ninappropriate content that violates community standards. To automate the\ndetection process, prior works have proposed varieties of machine learning (ML)\napproaches to train Language Models (LMs) for toxic content detection. However,\nboth their accuracy and transferability across datasets are limited. Recently,\nLarge Language Models (LLMs) have shown promise in toxic content detection due\nto their superior zero-shot and few-shot in-context learning ability as well as\nbroad transferability on ML tasks. However, efficiently designing prompts for\nLLMs remains challenging. Moreover, the high run-time cost of LLMs may hinder\ntheir deployments in production. To address these challenges, in this work, we\npropose BD-LLM, a novel and efficient approach to Bootstrapping and Distilling\nLLMs for toxic content detection. Specifically, we design a novel prompting\nmethod named Decision-Tree-of-Thought (DToT) to bootstrap LLMs' detection\nperformance and extract high-quality rationales. DToT can automatically select\nmore fine-grained context to re-prompt LLMs when their responses lack\nconfidence. Additionally, we use the rationales extracted via DToT to fine-tune\nstudent LMs. Our experimental results on various datasets demonstrate that DToT\ncan improve the accuracy of LLMs by up to 4.6%. Furthermore, student LMs\nfine-tuned with rationales extracted via DToT outperform baselines on all\ndatasets with up to 16.9\\% accuracy improvement, while being more than 60x\nsmaller than conventional LLMs. Finally, we observe that student LMs fine-tuned\nwith rationales exhibit better cross-dataset transferability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Cheng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zheng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Psounis_K/0/1/0/all/0/1\">Konstantinos Psounis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cem Mil Podcasts: A Spoken Portuguese Document Corpus For Multi-modal, Multi-lingual and Multi-Dialect Information Access Research. (arXiv:2209.11871v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.11871","description":"<p>In this paper we describe the Portuguese-language podcast dataset we have\nreleased for academic research purposes. We give an overview of how the data\nwas sampled, descriptive statistics over the collection, as well as information\nabout the distribution over Brazilian and Portuguese dialects. We give results\nfrom experiments on multi-lingual summarization, showing that summarizing\npodcast transcripts can be performed well by a system supporting both English\nand Portuguese. We also show experiments on Portuguese podcast genre\nclassification using text metadata. Combining this collection with previously\nreleased English-language collection opens up the potential for multi-modal,\nmulti-lingual and multi-dialect podcast information access research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garmash_E/0/1/0/all/0/1\">Ekaterina Garmash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_E/0/1/0/all/0/1\">Edgar Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifton_A/0/1/0/all/0/1\">Ann Clifton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correia_J/0/1/0/all/0/1\">Joana Correia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jat_S/0/1/0/all/0/1\">Sharmistha Jat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Winstead Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">Rosie Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlgren_J/0/1/0/all/0/1\">Jussi Karlgren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConsPrompt: Exploiting Contrastive Samples for Fewshot Prompt Learning. (arXiv:2211.04118v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.04118","description":"<p>Prompt recently have become an effective linguistic tool on utilizing the\npre-trained language models. However, in few-shot scenarios, subtle changes of\nprompt's design always make the result widely different, and the prompt design\nis also easy to overfit the current limited samples. To alleviate this, we\nexplore how to utilize suitable contrastive samples and multiple contrastive\nlearning methods to realize a more robust prompt's representation. Therefore,\nthe contrastive prompt model ConsPrompt combining with prompt encoding network,\ncontrastive sampling modules, and contrastive scoring modules are introduced to\nrealize differential contrastive learning. Our results exhibit the\nstate-of-the-art performance in different few-shot settings, and the ablation\nexperiments also certificate the effectiveness in utilizing multi-degree\ncontrastive learning in prompt-based fine-tuning process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weng_J/0/1/0/all/0/1\">Jinta Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yifan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_d/0/1/0/all/0/1\">d Donghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1\">Hao You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Label Smoothing on Multi-hop Question Answering. (arXiv:2212.09512v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09512","description":"<p>Multi-Hop Question Answering (MHQA) is a significant area in question\nanswering, requiring multiple reasoning components, including document\nretrieval, supporting sentence prediction, and answer span extraction. In this\nwork, we analyze the primary factors limiting the performance of multi-hop\nreasoning and introduce label smoothing into the MHQA task. This is aimed at\nenhancing the generalization capabilities of MHQA systems and mitigating\noverfitting of answer spans and reasoning paths in training set. We propose a\nnovel label smoothing technique, F1 Smoothing, which incorporates uncertainty\ninto the learning process and is specifically tailored for Machine Reading\nComprehension (MRC) tasks. Inspired by the principles of curriculum learning,\nwe introduce the Linear Decay Label Smoothing Algorithm (LDLA), which\nprogressively reduces uncertainty throughout the training process. Experiment\non the HotpotQA dataset demonstrates the effectiveness of our methods in\nenhancing performance and generalizability in multi-hop reasoning, achieving\nnew state-of-the-art results on the leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhangyue Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiannian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yiguang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conformers are All You Need for Visual Speech Recognition. (arXiv:2302.10915v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.10915","description":"<p>Visual speech recognition models extract visual features in a hierarchical\nmanner. At the lower level, there is a visual front-end with a limited temporal\nreceptive field that processes the raw pixels depicting the lips or faces. At\nthe higher level, there is an encoder that attends to the embeddings produced\nby the front-end over a large temporal receptive field. Previous work has\nfocused on improving the visual front-end of the model to extract more useful\nfeatures for speech recognition. Surprisingly, our work shows that complex\nvisual front-ends are not necessary. Instead of allocating resources to a\nsophisticated visual front-end, we find that a linear visual front-end paired\nwith a larger Conformer encoder results in lower latency, more efficient memory\nusage, and improved WER performance. We achieve a new state-of-the-art of 12.8%\nWER for visual speech recognition on the TED LRS3 dataset, which rivals the\nperformance of audio-only models from just four years ago.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_O/0/1/0/all/0/1\">Oscar Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1\">Hank Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serdyuk_D/0/1/0/all/0/1\">Dmitriy Serdyuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Ankit Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siohan_O/0/1/0/all/0/1\">Olivier Siohan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of GPT and BERT-based models on identifying protein-protein interactions in biomedical text. (arXiv:2303.17728v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.17728","description":"<p>Detecting protein-protein interactions (PPIs) is crucial for understanding\ngenetic mechanisms, disease pathogenesis, and drug design. However, with the\nfast-paced growth of biomedical literature, there is a growing need for\nautomated and accurate extraction of PPIs to facilitate scientific knowledge\ndiscovery. Pre-trained language models, such as generative pre-trained\ntransformers (GPT) and bidirectional encoder representations from transformers\n(BERT), have shown promising results in natural language processing (NLP)\ntasks. We evaluated the performance of PPI identification of multiple GPT and\nBERT models using three manually curated gold-standard corpora: Learning\nLanguage in Logic (LLL) with 164 PPIs in 77 sentences, Human Protein Reference\nDatabase with 163 PPIs in 145 sentences, and Interaction Extraction Performance\nAssessment with 335 PPIs in 486 sentences. BERT-based models achieved the best\noverall performance, with BioBERT achieving the highest recall (91.95%) and\nF1-score (86.84%) and PubMedBERT achieving the highest precision (85.25%).\nInterestingly, despite not being explicitly trained for biomedical texts, GPT-4\nachieved commendable performance, comparable to the top-performing BERT models.\nIt achieved a precision of 88.37%, a recall of 85.14%, and an F1-score of\n86.49% on the LLL dataset. These results suggest that GPT models can\neffectively detect PPIs from text data, offering promising avenues for\napplication in biomedical literature mining. Further research could explore how\nthese models might be fine-tuned for even more specialized tasks within the\nbiomedical domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rehana_H/0/1/0/all/0/1\">Hasin Rehana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cam_N/0/1/0/all/0/1\">Nur Bengisu &#xc7;am</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basmaci_M/0/1/0/all/0/1\">Mert Basmaci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jemiyo_C/0/1/0/all/0/1\">Christianah Jemiyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yongqun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozgur_A/0/1/0/all/0/1\">Arzucan &#xd6;zg&#xfc;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hur_J/0/1/0/all/0/1\">Junguk Hur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Clustering Framework for Unsupervised and Semi-supervised New Intent Discovery. (arXiv:2304.07699v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.07699","description":"<p>New intent discovery is of great value to natural language processing,\nallowing for a better understanding of user needs and providing friendly\nservices. However, most existing methods struggle to capture the complicated\nsemantics of discrete text representations when limited or no prior knowledge\nof labeled data is available. To tackle this problem, we propose a novel\nclustering framework, USNID, for unsupervised and semi-supervised new intent\ndiscovery, which has three key technologies. First, it fully utilizes\nunsupervised or semi-supervised data to mine shallow semantic similarity\nrelations and provide well-initialized representations for clustering. Second,\nit designs a centroid-guided clustering mechanism to address the issue of\ncluster allocation inconsistency and provide high-quality self-supervised\ntargets for representation learning. Third, it captures high-level semantics in\nunsupervised or semi-supervised data to discover fine-grained intent-wise\nclusters by optimizing both cluster-level and instance-level objectives. We\nalso propose an effective method for estimating the cluster number in\nopen-world scenarios without knowing the number of new intents beforehand.\nUSNID performs exceptionally well on several benchmark intent datasets,\nachieving new state-of-the-art results in unsupervised and semi-supervised new\nintent discovery and demonstrating robust performance with different cluster\nnumbers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_F/0/1/0/all/0/1\">Fei Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1\">Kai Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Instruction Tuning. (arXiv:2304.08485v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2304.08485","description":"<p>Instruction tuning large language models (LLMs) using machine-generated\ninstruction-following data has improved zero-shot capabilities on new tasks,\nbut the idea is less explored in the multimodal field. In this paper, we\npresent the first attempt to use language-only GPT-4 to generate multimodal\nlanguage-image instruction-following data. By instruction tuning on such\ngenerated data, we introduce LLaVA: Large Language and Vision Assistant, an\nend-to-end trained large multimodal model that connects a vision encoder and\nLLM for general-purpose visual and language understanding.Our early experiments\nshow that LLaVA demonstrates impressive multimodel chat abilities, sometimes\nexhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and\nyields a 85.1% relative score compared with GPT-4 on a synthetic multimodal\ninstruction-following dataset. When fine-tuned on Science QA, the synergy of\nLLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make\nGPT-4 generated visual instruction tuning data, our model and code base\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haotian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qingyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yong Jae Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Plan with Natural Language. (arXiv:2304.10464v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.10464","description":"<p>Large Language Models (LLMs) have shown remarkable performance in various\nbasic natural language tasks. For completing the complex task, we still need a\nplan for the task to guide LLMs to generate the specific solutions step by\nstep. LLMs can directly generate task plans, but these plans may still contain\nfactual errors or are incomplete. A high-quality task plan contains correct\nstep-by-step solutions for solving all situations and behavioral instructions\nfor avoiding mistakes. To obtain it, we propose the Learning to Plan method,\nwhich involves two phases: (1) In the first learning task plan phase, it\niteratively updates the task plan with new step-by-step solutions and\nbehavioral instructions, which are obtained by prompting LLMs to derive from\ntraining error feedback. (2) In the subsequent test phase, the LLM uses the\nlearned task plan to guide the inference of LLM on the test set. We demonstrate\nthe effectiveness of our method on the five different reasoning type tasks (8\ndatasets). Further, our analysis experiment shows that the task plan learned by\none LLM can directly guide another LLM to improve its performance, which\nreveals a new transfer learning paradigm. We release the code at\n\\url{https://github.com/Eureka6174/LearnNLPlan}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yiduo Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yaobo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenshan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Aware Semantic Similarity Measurement for Unsupervised Word Sense Disambiguation. (arXiv:2305.03520v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03520","description":"<p>The issue of word sense ambiguity poses a significant challenge in natural\nlanguage processing due to the scarcity of annotated data to feed machine\nlearning models to face the challenge. Therefore, unsupervised word sense\ndisambiguation methods have been developed to overcome that challenge without\nrelying on annotated data. This research proposes a new context-aware approach\nto unsupervised word sense disambiguation, which provides a flexible mechanism\nfor incorporating contextual information into the similarity measurement\nprocess. We experiment with a popular benchmark dataset to evaluate the\nproposed strategy and compare its performance with state-of-the-art\nunsupervised word sense disambiguation techniques. The experimental results\nindicate that our approach substantially enhances disambiguation accuracy and\nsurpasses the performance of several existing techniques. Our findings\nunderscore the significance of integrating contextual information in semantic\nsimilarity measurements to manage word sense ambiguity in unsupervised\nscenarios effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Gil_J/0/1/0/all/0/1\">Jorge Martinez-Gil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure-CLIP: Towards Scene Graph Knowledge to Enhance Multi-modal Structured Representations. (arXiv:2305.06152v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.06152","description":"<p>Large-scale vision-language pre-training has achieved significant performance\nin multi-modal understanding and generation tasks. However, existing methods\noften perform poorly on image-text matching tasks that require structured\nrepresentations, i.e., representations of objects, attributes, and relations.\nAs illustrated in Fig.~reffig:case (a), the models cannot make a distinction\nbetween ``An astronaut rides a horse\" and ``A horse rides an astronaut\". This\nis because they fail to fully leverage structured knowledge when learning\nrepresentations in multi-modal scenarios. In this paper, we present an\nend-to-end framework Structure-CLIP, which integrates Scene Graph Knowledge\n(SGK) to enhance multi-modal structured representations. Firstly, we use scene\ngraphs to guide the construction of semantic negative examples, which results\nin an increased emphasis on learning structured representations. Moreover, a\nKnowledge-Enhance Encoder (KEE) is proposed to leverage SGK as input to further\nenhance structured representations. To verify the effectiveness of the proposed\nframework, we pre-train our model with the aforementioned approaches and\nconduct experiments on downstream tasks. Experimental results demonstrate that\nStructure-CLIP achieves state-of-the-art (SOTA) performance on VG-Attribution\nand VG-Relation datasets, with 12.5% and 4.1% ahead of the multi-modal SOTA\nmodel respectively. Meanwhile, the results on MSCOCO indicate that\nStructure-CLIP significantly enhances the structured representations while\nmaintaining the ability of general representations. Our code is available at\nhttps://github.com/zjukg/Structure-CLIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiji Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongsheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weijie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tangjie Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StarCoder: may the source be with you!. (arXiv:2305.06161v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.06161","description":"<p>The BigCode community, an open-scientific collaboration working on the\nresponsible development of Large Language Models for Code (Code LLMs),\nintroduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context\nlength, infilling capabilities and fast large-batch inference enabled by\nmulti-query attention. StarCoderBase is trained on 1 trillion tokens sourced\nfrom The Stack, a large collection of permissively licensed GitHub repositories\nwith inspection tools and an opt-out process. We fine-tuned StarCoderBase on\n35B Python tokens, resulting in the creation of StarCoder. We perform the most\ncomprehensive evaluation of Code LLMs to date and show that StarCoderBase\noutperforms every open Code LLM that supports multiple programming languages\nand matches or outperforms the OpenAI code-cushman-001 model. Furthermore,\nStarCoder outperforms every model that is fine-tuned on Python, can be prompted\nto achieve 40\\% pass@1 on HumanEval, and still retains its performance on other\nprogramming languages. We take several important steps towards a safe\nopen-access model release, including an improved PII redaction pipeline and a\nnovel attribution tracing tool, and make the StarCoder models publicly\navailable under a more commercially viable version of the Open Responsible AI\nModel license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Raymond Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allal_L/0/1/0/all/0/1\">Loubna Ben Allal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zi_Y/0/1/0/all/0/1\">Yangtian Zi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1\">Niklas Muennighoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocetkov_D/0/1/0/all/0/1\">Denis Kocetkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_C/0/1/0/all/0/1\">Chenghao Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marone_M/0/1/0/all/0/1\">Marc Marone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akiki_C/0/1/0/all/0/1\">Christopher Akiki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chim_J/0/1/0/all/0/1\">Jenny Chim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheltonozhskii_E/0/1/0/all/0/1\">Evgenii Zheltonozhskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1\">Terry Yue Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Thomas Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehaene_O/0/1/0/all/0/1\">Olivier Dehaene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davaadorj_M/0/1/0/all/0/1\">Mishig Davaadorj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamy_Poirier_J/0/1/0/all/0/1\">Joel Lamy-Poirier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monteiro_J/0/1/0/all/0/1\">Jo&#xe3;o Monteiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shliazhko_O/0/1/0/all/0/1\">Oleh Shliazhko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gontier_N/0/1/0/all/0/1\">Nicolas Gontier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meade_N/0/1/0/all/0/1\">Nicholas Meade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zebaze_A/0/1/0/all/0/1\">Armel Zebaze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yee_M/0/1/0/all/0/1\">Ming-Ho Yee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umapathi_L/0/1/0/all/0/1\">Logesh Kumar Umapathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipkin_B/0/1/0/all/0/1\">Benjamin Lipkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oblokulov_M/0/1/0/all/0/1\">Muhtasham Oblokulov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiruo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murthy_R/0/1/0/all/0/1\">Rudra Murthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stillerman_J/0/1/0/all/0/1\">Jason Stillerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1\">Siva Sankalp Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abulkhanov_D/0/1/0/all/0/1\">Dmitry Abulkhanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zocca_M/0/1/0/all/0/1\">Marco Zocca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_M/0/1/0/all/0/1\">Manan Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fahmy_N/0/1/0/all/0/1\">Nour Fahmy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_U/0/1/0/all/0/1\">Urvashi Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Swayam Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luccioni_S/0/1/0/all/0/1\">Sasha Luccioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_P/0/1/0/all/0/1\">Paulo Villegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunakov_M/0/1/0/all/0/1\">Maxim Kunakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhdanov_F/0/1/0/all/0/1\">Fedor Zhdanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_M/0/1/0/all/0/1\">Manuel Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tony Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timor_N/0/1/0/all/0/1\">Nadav Timor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jennifer Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlesinger_C/0/1/0/all/0/1\">Claire Schlesinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoelkopf_H/0/1/0/all/0/1\">Hailey Schoelkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebert_J/0/1/0/all/0/1\">Jan Ebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dao_T/0/1/0/all/0/1\">Tri Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_M/0/1/0/all/0/1\">Mayank Mishra</a>, et al. (15 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation. (arXiv:2305.09515v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.09515","description":"<p>Diffusion models have gained significant attention in the realm of image\ngeneration due to their exceptional performance. Their success has been\nrecently expanded to text generation via generating all tokens within a\nsequence concurrently. However, natural language exhibits a far more pronounced\nsequential dependency in comparison to images, and the majority of existing\nlanguage models are trained with a left-to-right auto-regressive approach. To\naccount for the inherent sequential characteristic of natural language, we\nintroduce Auto-Regressive Diffusion (AR-Diffusion). AR-Diffusion ensures that\nthe generation of tokens on the right depends on the generated ones on the\nleft, a mechanism achieved through employing a dynamic number of denoising\nsteps that vary based on token position. This results in tokens on the left\nundergoing fewer denoising steps than those on the right, thereby enabling them\nto generate earlier and subsequently influence the generation of tokens on the\nright. In a series of experiments on various text generation tasks, including\ntext summarization, machine translation, and common sense generation,\nAR-Diffusion clearly demonstrated its superiority over existing diffusion\nlanguage models and that it can be $100\\times\\sim600\\times$ faster when\nachieving comparable results. Our code is available at\nhttps://github.com/microsoft/ProphetNet/tree/master/AR-diffusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhihao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jian Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jian Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model. (arXiv:2305.18290v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.18290","description":"<p>While large-scale unsupervised language models (LMs) learn broad world\nknowledge and some reasoning skills, achieving precise control of their\nbehavior is difficult due to the completely unsupervised nature of their\ntraining. Existing methods for gaining such steerability collect human labels\nof the relative quality of model generations and fine-tune the unsupervised LM\nto align with these preferences, often with reinforcement learning from human\nfeedback (RLHF). However, RLHF is a complex and often unstable procedure, first\nfitting a reward model that reflects the human preferences, and then\nfine-tuning the large unsupervised LM using reinforcement learning to maximize\nthis estimated reward without drifting too far from the original model. In this\npaper we introduce a new parameterization of the reward model in RLHF that\nenables extraction of the corresponding optimal policy in closed form, allowing\nus to solve the standard RLHF problem with only a simple classification loss.\nThe resulting algorithm, which we call Direct Preference Optimization (DPO), is\nstable, performant, and computationally lightweight, eliminating the need for\nsampling from the LM during fine-tuning or performing significant\nhyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align\nwith human preferences as well as or better than existing methods. Notably,\nfine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of\ngenerations, and matches or improves response quality in summarization and\nsingle-turn dialogue while being substantially simpler to implement and train.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rafailov_R/0/1/0/all/0/1\">Rafael Rafailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Archit Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_E/0/1/0/all/0/1\">Eric Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Voronoi Sampling. (arXiv:2306.03061v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.03061","description":"<p>Gradient-based sampling algorithms have demonstrated their effectiveness in\ntext generation, especially in the context of controlled text generation.\nHowever, there exists a lack of theoretically grounded and principled\napproaches for this task. In this paper, we take an important step toward\nbuilding a principled approach for sampling from language models with\ngradient-based methods. We use discrete distributions given by language models\nto define densities and develop an algorithm based on Hamiltonian Monte Carlo\nto sample from them. We name our gradient-based technique Structured Voronoi\nSampling (SVS). In an experimental setup where the reference distribution is\nknown, we show that the empirical distribution of SVS samples is closer to the\nreference distribution compared to alternative sampling schemes. Furthermore,\nin a controlled generation task, SVS is able to generate fluent and diverse\nsamples while following the control targets significantly better than other\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Afra Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Li Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2308.08742","description":"<p>Model editing techniques modify a minor proportion of knowledge in Large\nLanguage Models (LLMs) at a relatively low cost, which have demonstrated\nnotable success. Existing methods assume Transformer Layer (TL) hidden states\nare values of key-value memories of the Feed-Forward Network (FFN). They\nusually optimize the TL hidden states to memorize target knowledge and use it\nto update the weights of the FFN in LLMs. However, the information flow of TL\nhidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN,\nand residual connections. Existing methods neglect the fact that the TL hidden\nstates contains information not specifically required for FFN. Consequently,\nthe performance of model editing decreases. To achieve more precise model\nediting, we analyze hidden states of MHSA and FFN, finding that MHSA encodes\ncertain general knowledge extraction patterns. This implies that MHSA weights\ndo not require updating when new knowledge is introduced. Based on above\nfindings, we introduce PMET, which simultaneously optimizes Transformer\nComponent (TC, namely MHSA and FFN) hidden states, while only using the\noptimized TC hidden states of FFN to precisely update FFN weights. Our\nexperiments demonstrate that PMET exhibits state-of-the-art performance on both\nthe COUNTERFACT and zsRE datasets. Our ablation experiments substantiate the\neffectiveness of our enhancements, further reinforcing the finding that the\nMHSA encodes certain general knowledge extraction patterns and indicating its\nstorage of a small amount of factual knowledge. Our code is available at\nhttps://github.com/xpq-tech/PMET.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaopeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shasha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shezheng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jie Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Norm Tweaking: High-performance Low-bit Quantization of Large Language Models. (arXiv:2309.02784v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2309.02784","description":"<p>As the size of large language models (LLMs) continues to grow, model\ncompression without sacrificing accuracy has become a crucial challenge for\ndeployment. While some quantization methods, such as GPTQ, have made progress\nin achieving acceptable 4-bit weight-only quantization, attempts at lower-bit\nquantization often result in severe performance degradation. In this paper, we\nintroduce a technique called norm tweaking, which can be used as a plugin in\ncurrent PTQ methods to achieve high precision while being cost-efficient. Our\napproach is inspired by the observation that rectifying the quantized\nactivation distribution to match its float counterpart can readily restore\naccuracy for LLMs. To achieve this, we carefully design a tweaking strategy\nthat includes calibration data generation and channel-wise distance constraint\nto update the weights of normalization layers for better generalization. We\nconduct extensive experiments on various datasets using several open-sourced\nLLMs. Our method demonstrates significant improvements in both weight-only\nquantization and joint quantization of weights and activations, surpassing\nexisting PTQ methods. On GLM-130B and OPT-66B, our method even achieves the\nsame level of accuracy at 2-bit quantization as their float ones. Our simple\nand effective approach makes it more practical for real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiangxiang Chu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ScaLearn: Simple and Highly Parameter-Efficient Task Transfer by Learning to Scale. (arXiv:2310.01217v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2310.01217","description":"<p>Multi-task learning (MTL) has shown considerable practical benefits,\nparticularly when using pre-trained language models (PLMs). While this is\ncommonly achieved by simultaneously learning $n$ tasks under a joint\noptimization procedure, recent methods such as AdapterFusion structure the\nproblem into two distinct stages: (i) task learning, where knowledge specific\nto a task is encapsulated within sets of parameters (e.g., adapters), and (ii)\ntransfer, where this already learned knowledge is leveraged for a target task.\nThis separation of concerns provides numerous benefits, such as promoting\nreusability, and addressing cases involving data privacy and societal concerns;\non the flip side, current two-stage MTL methods come with the cost of\nintroducing a substantial number of additional parameters. In this work, we\naddress this issue by leveraging the usefulness of linearly scaling the output\nrepresentations of source adapters for transfer learning. We introduce\nScaLearn, a simple and highly parameter-efficient two-stage MTL method that\ncapitalizes on the knowledge of the source tasks by learning a minimal set of\nscaling parameters that enable effective knowledge transfer to a target task.\nOur experiments on three benchmarks (GLUE, SuperGLUE, and HumSet) show that our\nScaLearn, in addition to facilitating the benefits of two-stage MTL,\nconsistently outperforms strong baselines with only a small number of transfer\nparameters - roughly 0.35% of those of AdapterFusion. Remarkably, we observe\nthat ScaLearn maintains its strong abilities even when further reducing\nparameters through uniform scaling and layer-sharing, achieving similarly\ncompetitive results with only $8$ transfer parameters for each target task. Our\nproposed approach thus demonstrates the power of simple scaling as a promise\nfor more efficient task transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Frohmann_M/0/1/0/all/0/1\">Markus Frohmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holtermann_C/0/1/0/all/0/1\">Carolin Holtermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masoudian_S/0/1/0/all/0/1\">Shahed Masoudian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekabsaz_N/0/1/0/all/0/1\">Navid Rekabsaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Defending Our Privacy With Backdoors. (arXiv:2310.08320v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2310.08320","description":"<p>The proliferation of large AI models trained on uncurated, often sensitive\nweb-scraped data has raised significant privacy concerns. One of the concerns\nis that adversaries can extract information about the training data using\nprivacy attacks. Unfortunately, the task of removing specific information from\nthe models without sacrificing performance is not straightforward and has\nproven to be challenging. We propose a rather easy yet effective defense based\non backdoor attacks to remove private information such as names of individuals\nfrom models, and focus in this work on text encoders. Specifically, through\nstrategic insertion of backdoors, we align the embeddings of sensitive phrases\nwith those of neutral terms-\"a person\" instead of the person's name. Our\nempirical results demonstrate the effectiveness of our backdoor-based defense\non CLIP by assessing its performance using a specialized privacy attack for\nzero-shot classifiers. Our approach provides not only a new \"dual-use\"\nperspective on backdoor attacks, but also presents a promising avenue to\nenhance the privacy of individuals within models trained on uncurated\nweb-scraped data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hintersdorf_D/0/1/0/all/0/1\">Dominik Hintersdorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Struppek_L/0/1/0/all/0/1\">Lukas Struppek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neider_D/0/1/0/all/0/1\">Daniel Neider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motion2Language, unsupervised learning of synchronized semantic motion segmentation. (arXiv:2310.10594v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2310.10594","description":"<p>In this paper, we investigate building a sequence to sequence architecture\nfor motion to language translation and synchronization. The aim is to translate\nmotion capture inputs into English natural-language descriptions, such that the\ndescriptions are generated synchronously with the actions performed, enabling\nsemantic segmentation as a byproduct, but without requiring synchronized\ntraining data. We propose a new recurrent formulation of local attention that\nis suited for synchronous/live text generation, as well as an improved motion\nencoder architecture better suited to smaller data and for synchronous\ngeneration. We evaluate both contributions in individual experiments, using the\nstandard BLEU4 metric, as well as a simple semantic equivalence measure, on the\nKIT motion language dataset. In a follow-up experiment, we assess the quality\nof the synchronization of generated text in our proposed approaches through\nmultiple evaluation metrics. We find that both contributions to the attention\nmechanism and the encoder architecture additively improve the quality of\ngenerated text (BLEU and semantic equivalence), but also of synchronization.\nOur code is available at\nhttps://github.com/rd20karim/M2T-Segmentation/tree/main\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Radouane_K/0/1/0/all/0/1\">Karim Radouane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchechmedjiev_A/0/1/0/all/0/1\">Andon Tchechmedjiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lagarde_J/0/1/0/all/0/1\">Julien Lagarde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranwez_S/0/1/0/all/0/1\">Sylvie Ranwez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts. (arXiv:2311.05608v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2311.05608","description":"<p>Ensuring the safety of artificial intelligence-generated content (AIGC) is a\nlongstanding topic in the artificial intelligence (AI) community, and the\nsafety concerns associated with Large Language Models (LLMs) have been widely\ninvestigated. Recently, large vision-language models (VLMs) represent an\nunprecedented revolution, as they are built upon LLMs but can incorporate\nadditional modalities (e.g., images). However, the safety of VLMs lacks\nsystematic evaluation, and there may be an overconfidence in the safety\nguarantees provided by their underlying LLMs. In this paper, to demonstrate\nthat introducing additional modality modules leads to unforeseen AI safety\nissues, we propose FigStep, a straightforward yet effective jailbreaking\nalgorithm against VLMs. Instead of feeding textual harmful instructions\ndirectly, FigStep converts the harmful content into images through typography\nto bypass the safety alignment within the textual module of the VLMs, inducing\nVLMs to output unsafe responses that violate common AI safety policies. In our\nevaluation, we manually review 46,500 model responses generated by 3 families\nof the promising open-source VLMs, i.e., LLaVA, MiniGPT4, and CogVLM (a total\nof 6 VLMs). The experimental results show that FigStep can achieve an average\nattack success rate of 82.50% on 500 harmful queries in 10 topics. Moreover, we\ndemonstrate that the methodology of FigStep can even jailbreak GPT-4V, which\nalready leverages an OCR detector to filter harmful queries. Above all, our\nwork reveals that VLMs are vulnerable to jailbreaking attacks, which highlights\nthe necessity of novel safety alignments between visual and textual modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yichen Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ran_D/0/1/0/all/0/1\">Delong Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Conglei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_T/0/1/0/all/0/1\">Tianshuo Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Anyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1\">Sisi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Gender Bias in the Translation of Gender-Neutral Languages into English. (arXiv:2311.08836v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2311.08836","description":"<p>Machine Translation (MT) continues to improve in quality and adoption, yet\nthe inadvertent perpetuation of gender bias remains a significant concern.\nDespite numerous studies into gender bias in translations from gender-neutral\nlanguages such as Turkish into more strongly gendered languages like English,\nthere are no benchmarks for evaluating this phenomenon or for assessing\nmitigation strategies. To address this gap, we introduce GATE X-E, an extension\nto the GATE (Rarrick et al., 2023) corpus, that consists of human translations\nfrom Turkish, Hungarian, Finnish, and Persian into English. Each translation is\naccompanied by feminine, masculine, and neutral variants for each possible\ngender interpretation. The dataset, which contains between 1250 and 1850\ninstances for each of the four language pairs, features natural sentences with\na wide range of sentence lengths and domains, challenging translation rewriters\non various linguistic phenomena. Additionally, we present an English gender\nrewriting solution built on GPT-3.5 Turbo and use GATE X-E to evaluate it. We\nopen source our contributions to encourage further research on gender\ndebiasing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rarrick_S/0/1/0/all/0/1\">Spencer Rarrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_R/0/1/0/all/0/1\">Ranjita Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poudel_S/0/1/0/all/0/1\">Sundar Poudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhary_V/0/1/0/all/0/1\">Vishal Chowdhary</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-12-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}