{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-05-19T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Toxicity Inspector: A Framework to Evaluate Ground Truth in Toxicity Detection Through Feedback. (arXiv:2305.10433v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10433","description":"<p>Toxic language is difficult to define, as it is not monolithic and has many\nvariations in perceptions of toxicity. This challenge of detecting toxic\nlanguage is increased by the highly contextual and subjectivity of its\ninterpretation, which can degrade the reliability of datasets and negatively\naffect detection model performance. To fill this void, this paper introduces a\ntoxicity inspector framework that incorporates a human-in-the-loop pipeline\nwith the aim of enhancing the reliability of toxicity benchmark datasets by\ncentering the evaluator's values through an iterative feedback cycle. The\ncenterpiece of this framework is the iterative feedback process, which is\nguided by two metric types (hard and soft) that provide evaluators and dataset\ncreators with insightful examination to balance the tradeoff between\nperformance gains and toxicity avoidance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Althunayan_H/0/1/0/all/0/1\">Huriyyah Althunayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahlas_R/0/1/0/all/0/1\">Rahaf Bahlas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alharbi_M/0/1/0/all/0/1\">Manar Alharbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alsuwailem_L/0/1/0/all/0/1\">Lena Alsuwailem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldayel_A/0/1/0/all/0/1\">Abeer Aldayel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ALahmadi_R/0/1/0/all/0/1\">Rehab ALahmadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning the Visualness of Text Using Large Vision-Language Models. (arXiv:2305.10434v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10434","description":"<p>Visual text evokes an image in a person's mind, while non-visual text fails\nto do so. A method to automatically detect visualness in text will unlock the\nability to augment text with relevant images, as neural text-to-image\ngeneration and retrieval models operate on the implicit assumption that the\ninput text is visual in nature. We curate a dataset of 3,620 English sentences\nand their visualness scores provided by multiple human annotators.\nAdditionally, we use documents that contain text and visual assets to create a\ndistantly supervised corpus of document text and associated images. We also\npropose a fine-tuning strategy that adapts large vision-language models like\nCLIP that assume a one-to-one correspondence between text and image to the task\nof scoring text visualness from text input alone. Our strategy involves\nmodifying the model's contrastive learning objective to map text identified as\nnon-visual to a common NULL image while matching visual text to their\ncorresponding images in the document. We evaluate the proposed approach on its\nability to (i) classify visual and non-visual text accurately, and (ii) attend\nover words that are identified as visual in psycholinguistic studies. Empirical\nevaluation indicates that our approach performs better than several heuristics\nand baseline models for the proposed task. Furthermore, to highlight the\nimportance of modeling the visualness of text, we conduct qualitative analyses\nof text-to-image generation systems like DALL-E.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Verma_G/0/1/0/all/0/1\">Gaurav Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1\">Ryan A. Rossi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tensmeyer_C/0/1/0/all/0/1\">Christopher Tensmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiuxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenkova_A/0/1/0/all/0/1\">Ani Nenkova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions. (arXiv:2305.10435v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10435","description":"<p>The Generative Pre-trained Transformer models represent a notable\nbreakthrough in the domain of natural language processing, which is propelling\nus toward the development of machines that can understand and communicate using\nlanguage in a manner that closely resembles that of humans. Generative\nPre-trained Transformer models are based on the transformer architecture, a\ndeep neural network designed for natural language processing tasks. Due to\ntheir impressive performance on natural language processing tasks and ability\nto effectively converse, Generative Pre-trained Transformer models have gained\nsignificant popularity among researchers and industrial communities, making\nthem one of the most widely used and effective models in natural language\nprocessing and related fields, which motivated to conduct this review. This\nreview provides a detailed overview of the Generative Pre-trained Transformer,\nincluding its architecture, working process, training procedures, enabling\ntechnologies, and its impact on various applications. In this review, we also\nexplored the potential challenges and limitations of a Generative Pre-trained\nTransformer. Furthermore, we discuss potential solutions and future directions.\nOverall, this paper aims to provide a comprehensive understanding of Generative\nPre-trained Transformers, enabling technologies, their impact on various\napplications, emerging challenges, and potential solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yenduri_G/0/1/0/all/0/1\">Gokul Yenduri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+M_R/0/1/0/all/0/1\">Ramalingam M</a>, <a href=\"http://arxiv.org/find/cs/1/au:+G_C/0/1/0/all/0/1\">Chemmalar Selvi G</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Y_S/0/1/0/all/0/1\">Supriya Y</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_G/0/1/0/all/0/1\">Gautam Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maddikunta_P/0/1/0/all/0/1\">Praveen Kumar Reddy Maddikunta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+G_D/0/1/0/all/0/1\">Deepti Raj G</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jhaveri_R/0/1/0/all/0/1\">Rutvij H Jhaveri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+B_P/0/1/0/all/0/1\">Prabadevi B</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weizheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasilakos_A/0/1/0/all/0/1\">Athanasios V. Vasilakos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadekallu_T/0/1/0/all/0/1\">Thippa Reddy Gadekallu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SmartPhone: Exploring Keyword Mnemonic with Auto-generated Verbal and Visual Cues. (arXiv:2305.10436v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10436","description":"<p>In second language vocabulary learning, existing works have primarily focused\non either the learning interface or scheduling personalized retrieval practices\nto maximize memory retention. However, the learning content, i.e., the\ninformation presented on flashcards, has mostly remained constant. Keyword\nmnemonic is a notable learning strategy that relates new vocabulary to existing\nknowledge by building an acoustic and imagery link using a keyword that sounds\nalike. Beyond that, producing verbal and visual cues associated with the\nkeyword to facilitate building these links requires a manual process and is not\nscalable. In this paper, we explore an opportunity to use large language models\nto automatically generate verbal and visual cues for keyword mnemonics. Our\napproach, an end-to-end pipeline for auto-generating verbal and visual cues,\ncan automatically generate highly memorable cues. We investigate the\neffectiveness of our approach via a human participant experiment by comparing\nit with manually generated cues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaewook Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_A/0/1/0/all/0/1\">Andrew Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IMAGINATOR: Pre-Trained Image+Text Joint Embeddings using Word-Level Grounding of Images. (arXiv:2305.10438v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10438","description":"<p>Word embeddings, i.e., semantically meaningful vector representation of\nwords, are largely influenced by the distributional hypothesis \"You shall know\na word by the company it keeps\" (Harris, 1954), whereas modern prediction-based\nneural network embeddings rely on design choices and hyperparameter\noptimization. Word embeddings like Word2Vec, GloVe etc. well capture the\ncontextuality and real-world analogies but contemporary convolution-based image\nembeddings such as VGGNet, AlexNet, etc. do not capture contextual knowledge.\nThe popular king-queen analogy does not hold true for most commonly used vision\nembeddings.\n</p>\n<p>In this paper, we introduce a pre-trained joint embedding (JE), named\nIMAGINATOR, trained on 21K distinct image objects level from 1M image+text\npairs. JE is a way to encode multimodal data into a vector space where the text\nmodality serves as the ground-ing key, which the complementary modality (in\nthis case, the image) is anchored with. IMAGINATOR encapsulates three\nindividual representations: (i) object-object co-location, (ii) word-object\nco-location, and (iii) word-object correlation. These three ways capture\ncomplementary aspects of the two modalities which are further combined to\nobtain the final JEs.\n</p>\n<p>Generated JEs are intrinsically evaluated to assess how well they capture the\ncontextuality and real-world analogies. We also evaluate pre-trained IMAGINATOR\nJEs on three downstream tasks: (i) image captioning, (ii) Image2Tweet, and\n(iii) text-based image retrieval. IMAGINATOR establishes a new standard on the\naforementioned down-stream tasks by outperforming the current SoTA on all the\nselected tasks. IMAGINATOR will be made publicly available. The codes are\navailable at https://github.com/varunakk/IMAGINATOR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishna_V/0/1/0/all/0/1\">Varuna Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suryavardan_S/0/1/0/all/0/1\">S Suryavardan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shreyash Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamoorthy_S/0/1/0/all/0/1\">Sathyanarayanan Ramamoorthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwa_P/0/1/0/all/0/1\">Parth Patwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_M/0/1/0/all/0/1\">Megha Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Amitava Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memorization for Good: Encryption with Autoregressive Language Models. (arXiv:2305.10445v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10445","description":"<p>Over-parameterized neural language models (LMs) can memorize and recite long\nsequences of training data. While such memorization is normally associated with\nundesired properties such as overfitting and information leaking, our work\ncasts memorization as an unexplored capability of LMs. We propose the first\nsymmetric encryption algorithm with autoregressive language models (SELM). We\nshow that autoregressive LMs can encode arbitrary data into a compact\nreal-valued vector (i.e., encryption) and then losslessly decode the vector to\nthe original message (i.e., decryption) via random subspace optimization and\ngreedy decoding. While SELM is not amenable to conventional cryptanalysis, we\ninvestigate its security through a novel empirical variant of the classic\nIND-CPA (indistinguishability under chosen-plaintext attack) game. Our code and\ndatasets are available at https://github.com/OSU-NLP-Group/SELM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stevens_S/0/1/0/all/0/1\">Samuel Stevens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion Recognition based on Psychological Components in Guided Narratives for Emotion Regulation. (arXiv:2305.10446v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10446","description":"<p>Emotion regulation is a crucial element in dealing with emotional events and\nhas positive effects on mental health. This paper aims to provide a more\ncomprehensive understanding of emotional events by introducing a new French\ncorpus of emotional narratives collected using a questionnaire for emotion\nregulation. We follow the theoretical framework of the Component Process Model\nwhich considers emotions as dynamic processes composed of four interrelated\ncomponents (behavior, feeling, thinking and territory). Each narrative is\nrelated to a discrete emotion and is structured based on all emotion components\nby the writers. We study the interaction of components and their impact on\nemotion classification with machine learning methods and pre-trained language\nmodels. Our results show that each component improves prediction performance,\nand that the best results are achieved by jointly considering all components.\nOur results also show the effectiveness of pre-trained language models in\npredicting discrete emotion from certain components, which reveal differences\nin how emotion components are expressed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cortal_G/0/1/0/all/0/1\">Gustave Cortal</a> (LMF, LISN), <a href=\"http://arxiv.org/find/cs/1/au:+Finkel_A/0/1/0/all/0/1\">Alain Finkel</a> (LMF, IUF), <a href=\"http://arxiv.org/find/cs/1/au:+Paroubek_P/0/1/0/all/0/1\">Patrick Paroubek</a> (LISN), <a href=\"http://arxiv.org/find/cs/1/au:+Ye_L/0/1/0/all/0/1\">Lina Ye</a> (LMF)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Effectiveness of a Dynamic Loss Function in Neural Network Based Automated Essay Scoring. (arXiv:2305.10447v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10447","description":"<p>Neural networks and in particular the attention mechanism have brought\nsignificant advances to the field of Automated Essay Scoring. Many of these\nsystems use a regression-based model which may be prone to underfitting when\nthe model only predicts the mean of the training data. In this paper, we\npresent a dynamic loss function that creates an incentive for the model to\npredict with the correct distribution, as well as predicting the correct\nvalues. Our loss function achieves this goal without sacrificing any\nperformance achieving a Quadratic Weighted Kappa score of 0.752 on the\nAutomated Student Assessment Prize Automated Essay Scoring dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Morris_O/0/1/0/all/0/1\">Oscar Morris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence-to-Sequence Pre-training with Unified Modality Masking for Visual Document Understanding. (arXiv:2305.10448v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10448","description":"<p>This paper presents GenDoc, a general sequence-to-sequence document\nunderstanding model pre-trained with unified masking across three modalities:\ntext, image, and layout. The proposed model utilizes an encoder-decoder\narchitecture, which allows for increased adaptability to a wide range of\ndownstream tasks with diverse output formats, in contrast to the encoder-only\nmodels commonly employed in document understanding. In addition to the\ntraditional text infilling task used in previous encoder-decoder models, our\npre-training extends to include tasks of masked image token prediction and\nmasked layout prediction. We also design modality-specific instruction and\nadopt both disentangled attention and the mixture-of-modality-experts strategy\nto effectively capture the information leveraged by each modality. Evaluation\nof the proposed model through extensive experiments on several downstream tasks\nin document understanding demonstrates its ability to achieve superior or\ncompetitive performance compared to state-of-the-art approaches. Our analysis\nfurther suggests that GenDoc is more robust than the encoder-only models in\nscenarios where the OCR quality is imperfect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shuwei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_T/0/1/0/all/0/1\">Tianyang Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1\">Zhanming Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luong_T/0/1/0/all/0/1\">Trung Quoc Luong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xiaoran Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding of Normal and Abnormal Hearts by Phase Space Analysis and Convolutional Neural Networks. (arXiv:2305.10450v1 [eess.IV])","link":"http://arxiv.org/abs/2305.10450","description":"<p>Cardiac diseases are one of the leading mortality factors in modern,\nindustrialized societies, which cause high expenses in public health systems.\nDue to high costs, developing analytical methods to improve cardiac diagnostics\nis essential. The heart's electric activity was first modeled using a set of\nnonlinear differential equations. Following this, variations of cardiac spectra\noriginating from deterministic dynamics are investigated. Analyzing a normal\nhuman heart's power spectra offers His-Purkinje network, which possesses a\nfractal-like structure. Phase space trajectories are extracted from the time\nseries electrocardiogram (ECG) graph with third-order derivate Taylor Series.\nHere in this study, phase space analysis and Convolutional Neural Networks\n(CNNs) method are applied to 44 records via the MIT-BIH database recorded with\nMLII. In order to increase accuracy, a straight line is drawn between the\nhighest Q-R distance in the phase space images of the records. Binary CNN\nclassification is used to determine healthy or unhealthy hearts. With a 90.90%\naccuracy rate, this model could classify records according to their heart\nstatus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Koc_B/0/1/0/all/0/1\">Bekir Yavuz Koc</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arsan_T/0/1/0/all/0/1\">Taner Arsan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pekcan_O/0/1/0/all/0/1\">Onder Pekcan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Attribution Importance for Improving Faithfulness Metrics. (arXiv:2305.10496v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10496","description":"<p>Feature attribution methods (FAs) are popular approaches for providing\ninsights into the model reasoning process of making predictions. The more\nfaithful a FA is, the more accurately it reflects which parts of the input are\nmore important for the prediction. Widely used faithfulness metrics, such as\nsufficiency and comprehensiveness use a hard erasure criterion, i.e. entirely\nremoving or retaining the top most important tokens ranked by a given FA and\nobserving the changes in predictive likelihood. However, this hard criterion\nignores the importance of each individual token, treating them all equally for\ncomputing sufficiency and comprehensiveness. In this paper, we propose a simple\nyet effective soft erasure criterion. Instead of entirely removing or retaining\ntokens from the input, we randomly mask parts of the token vector\nrepresentations proportionately to their FA importance. Extensive experiments\nacross various natural language processing tasks and different FAs show that\nour soft-sufficiency and soft-comprehensiveness metrics consistently prefer\nmore faithful explanations compared to hard sufficiency and comprehensiveness.\nOur code: https://github.com/casszhao/SoftFaith\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhixue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT Perpetuates Gender Bias in Machine Translation and Ignores Non-Gendered Pronouns: Findings across Bengali and Five other Low-Resource Languages. (arXiv:2305.10510v1 [cs.CY])","link":"http://arxiv.org/abs/2305.10510","description":"<p>In this multicultural age, language translation is one of the most performed\ntasks, and it is becoming increasingly AI-moderated and automated. As a novel\nAI system, ChatGPT claims to be proficient in such translation tasks and in\nthis paper, we put that claim to the test. Specifically, we examine ChatGPT's\naccuracy in translating between English and languages that exclusively use\ngender-neutral pronouns. We center this study around Bengali, the 7$^{th}$ most\nspoken language globally, but also generalize our findings across five other\nlanguages: Farsi, Malay, Tagalog, Thai, and Turkish. We find that ChatGPT\nperpetuates gender defaults and stereotypes assigned to certain occupations\n(e.g. man = doctor, woman = nurse) or actions (e.g. woman = cook, man = go to\nwork), as it converts gender-neutral pronouns in languages to `he' or `she'. We\nalso observe ChatGPT completely failing to translate the English gender-neutral\npronoun `they' into equivalent gender-neutral pronouns in other languages, as\nit produces translations that are incoherent and incorrect. While it does\nrespect and provide appropriately gender-marked versions of Bengali words when\nprompted with gender information in English, ChatGPT appears to confer a higher\nrespect to men than to women in the same occupation. We conclude that ChatGPT\nexhibits the same gender biases which have been demonstrated for tools like\nGoogle Translate or MS Translator, as we provide recommendations for a human\ncentered approach for future designers of AIs that perform language translation\nto better accommodate such low-resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sourojit Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caliskan_A/0/1/0/all/0/1\">Aylin Caliskan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IMAD: IMage-Augmented multi-modal Dialogue. (arXiv:2305.10512v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10512","description":"<p>Currently, dialogue systems have achieved high performance in processing\ntext-based communication. However, they have not yet effectively incorporated\nvisual information, which poses a significant challenge. Furthermore, existing\nmodels that incorporate images in dialogue generation focus on discussing the\nimage itself. Our proposed approach presents a novel perspective on multi-modal\ndialogue systems, which interprets the image in the context of the dialogue. By\ndoing so, we aim to expand the capabilities of current dialogue systems and\ntransition them from single modality (text) to multi-modality. However, there\nis a lack of validated English datasets that contain both images and dialogue\ncontexts for this task. Thus, we propose a two-stage approach to automatically\nconstruct a multi-modal dialogue dataset. In the first stage, we utilize\ntext-to-image similarity and sentence similarity to identify which utterances\ncould be replaced with an image. In the second stage, we replace those\nutterances by selecting a subset of relevant images and filtering them with a\nvisual question answering model. We used this approach, along with additional\nlabeling, to create the IMage Augmented multi-modal Dialogue dataset (IMAD),\nwhich can serve as a validated dataset for this task. Furthermore, we propose a\nbaseline model trained on this dataset, which outperforms model trained on the\nsame data without images and BlenderBot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Viktor_M/0/1/0/all/0/1\">Moskvoretskii Viktor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anton_F/0/1/0/all/0/1\">Frolov Anton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denis_K/0/1/0/all/0/1\">Kuznetsov Denis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Statistical Knowledge Assessment for Generative Language Models. (arXiv:2305.10519v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10519","description":"<p>Generative Language Models (GLMs) have demonstrated capabilities to store\nfactual knowledge and answer queries efficiently. Given varying prompts, does a\nGLM consistently generate factually correct answers? In this paper, we\nintroduce a statistical knowledge assessment framework guided by latent\nvariables and the KaRR metric, which quantifies a model's knowledge by\ncomputing its continuous probability across diverse text forms. We conduct a\ncomprehensive comparison of knowledge across 14 GLMs using our framework,\nincluding LLaMA, Alpaca, OPT, and others. Our statistical knowledge assessment\nencompasses 600 relation types and exhibits a strong correlation (0.43\nKendall's $\\tau$) with human evaluation. Our findings reveal that the knowledge\nin GLMs with the same backbone architecture adheres to the scaling law, and\nthat tuning on instruction-following data may compromise the model's ability to\ngenerate factually correct text consistently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qingxiu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable and Safe Remediation of Defective Actions in Self-Learning Conversational Systems. (arXiv:2305.10528v1 [cs.AI])","link":"http://arxiv.org/abs/2305.10528","description":"<p>Off-Policy reinforcement learning has been a driving force for the\nstate-of-the-art conversational AIs leading to more natural humanagent\ninteractions and improving the user satisfaction for goal-oriented agents.\nHowever, in large-scale commercial settings, it is often challenging to balance\nbetween policy improvements and experience continuity on the broad spectrum of\napplications handled by such system. In the literature, off-policy evaluation\nand guard-railing on aggregate statistics has been commonly used to address\nthis problem. In this paper, we propose a method for curating and leveraging\nhigh-precision samples sourced from historical regression incident reports to\nvalidate, safe-guard, and improve policies prior to the online deployment. We\nconducted extensive experiments using data from a real-world conversational\nsystem and actual regression incidents. The proposed method is currently\ndeployed in our production system to protect customers against broken\nexperiences and enable long-term policy improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_S/0/1/0/all/0/1\">Sarthak Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kachuee_M/0/1/0/all/0/1\">Mohammad Kachuee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheikholeslami_F/0/1/0/all/0/1\">Fateme Sheikholeslami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_J/0/1/0/all/0/1\">Jaeyoung Do</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bring More Attention to Syntactic Symmetry for Automatic Postediting of High-Quality Machine Translations. (arXiv:2305.10557v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10557","description":"<p>Automatic postediting (APE) is an automated process to refine a given machine\ntranslation (MT). Recent findings present that existing APE systems are not\ngood at handling high-quality MTs even for a language pair with abundant data\nresources, English$\\unicode{x2013}$German: the better the given MT is, the\nharder it is to decide what parts to edit and how to fix these errors. One\npossible solution to this problem is to instill deeper knowledge about the\ntarget language into the model. Thus, we propose a linguistically motivated\nmethod of regularization that is expected to enhance APE models' understanding\nof the target language: a loss function that encourages symmetric\nself-attention on the given MT. Our analysis of experimental results\ndemonstrates that the proposed method helps improving the state-of-the-art\narchitecture's APE quality for high-quality MTs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_B/0/1/0/all/0/1\">Baikjin Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Myungji Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jong-Hyeok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yunsu Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Massively Multi-Lingual Event Understanding: Extraction, Visualization, and Search. (arXiv:2305.10561v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10561","description":"<p>In this paper, we present ISI-Clear, a state-of-the-art, cross-lingual,\nzero-shot event extraction system and accompanying user interface for event\nvisualization &amp; search. Using only English training data, ISI-Clear makes\nglobal events available on-demand, processing user-supplied text in 100\nlanguages ranging from Afrikaans to Yiddish. We provide multiple event-centric\nviews of extracted events, including both a graphical representation and a\ndocument-level summary. We also integrate existing cross-lingual search\nalgorithms with event extraction capabilities to provide cross-lingual\nevent-centric search, allowing English-speaking users to search over events\nautomatically extracted from a corpus of non-English documents, using either\nEnglish natural language queries (e.g. cholera outbreaks in Iran) or structured\nqueries (e.g. find all events of type Disease-Outbreak with agent cholera and\nlocation Iran).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jenkins_C/0/1/0/all/0/1\">Chris Jenkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Shantanu Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barry_J/0/1/0/all/0/1\">Joel Barry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fincke_S/0/1/0/all/0/1\">Steven Fincke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boschee_E/0/1/0/all/0/1\">Elizabeth Boschee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Effect of Hard Negative Sample Distribution on Contrastive Knowledge Graph Embedding. (arXiv:2305.10563v1 [cs.AI])","link":"http://arxiv.org/abs/2305.10563","description":"<p>The success of the knowledge graph completion task heavily depends on the\nquality of the knowledge graph embeddings (KGEs), which relies on\nself-supervised learning and augmenting the dataset with negative triples.\nThere is a gap in literature between the theoretical analysis of negative\nsamples on contrastive loss and heuristic generation of quality (i.e., hard)\nnegative triples. In this paper, we modify the InfoNCE loss to explicitly\naccount for the negative sample distribution. We show minimizing InfoNCE loss\nwith hard negatives maximizes the KL-divergence between the given and negative\ntriple embedding. However, we also show that hard negatives can lead to false\nnegatives (i.e., accidentally factual triples) and reduce downstream task\nperformance. To address this issue, we propose a novel negative sample\ndistribution that uses the graph structure of the knowledge graph to remove the\nfalse negative triples. We call our algorithm Hardness and Structure-aware\n(\\textbf{HaSa}) contrastive KGE. Experiments show that our method outperforms\nstate-of-the-art KGE methods in several metrics for WN18RR and FB15k-237\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honggen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">June Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From chocolate bunny to chocolate crocodile: Do Language Models Understand Noun Compounds?. (arXiv:2305.10568v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10568","description":"<p>Noun compound interpretation is the task of expressing a noun compound (e.g.\nchocolate bunny) in a free-text paraphrase that makes the relationship between\nthe constituent nouns explicit (e.g. bunny-shaped chocolate). We propose\nmodifications to the data and evaluation setup of the standard task (Hendrickx\net al., 2013), and show that GPT-3 solves it almost perfectly. We then\ninvestigate the task of noun compound conceptualization, i.e. paraphrasing a\nnovel or rare noun compound. E.g., chocolate crocodile is a crocodile-shaped\nchocolate. This task requires creativity, commonsense, and the ability to\ngeneralize knowledge about similar concepts. While GPT-3's performance is not\nperfect, it is better than that of humans -- likely thanks to its access to\nvast amounts of knowledge, and because conceptual processing is effortful for\npeople (Connell and Lynott, 2012). Finally, we estimate the extent to which\nGPT-3 is reasoning about the world vs. parroting its training data. We find\nthat the outputs from GPT-3 often have significant overlap with a large web\ncorpus, but that the parroting strategy is less beneficial for novel noun\ncompounds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coil_J/0/1/0/all/0/1\">Jordan Coil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1\">Vered Shwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Better Way to Do Masked Language Model Scoring. (arXiv:2305.10588v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10588","description":"<p>Estimating the log-likelihood of a given sentence under an autoregressive\nlanguage model is straightforward: one can simply apply the chain rule and sum\nthe log-likelihood values for each successive token. However, for masked\nlanguage models, there is no direct way to estimate the log-likelihood of a\nsentence. To address this issue, Salazar et al. (2020) propose to estimate\nsentence pseudo-log-likelihood (PLL) scores, computed by successively masking\neach sentence token, retrieving its score using the rest of the sentence as\ncontext, and summing the resulting values. Here, we demonstrate that the\noriginal PLL method yields inflated scores for out-of-vocabulary words and\npropose an adapted metric, in which we mask not only the target token, but also\nall within-word tokens to the right of the target. We show that our adapted\nmetric (PLL-word-l2r) outperforms both the original PLL metric and a PLL metric\nin which all within-word tokens are masked. In particular, it better satisfies\ntheoretical desiderata and better correlates with scores from autoregressive\nmodels. Finally, we show that the choice of metric affects even tightly\ncontrolled, minimal pair evaluation benchmarks (such as BLiMP), underscoring\nthe importance of selecting an appropriate scoring metric for evaluating MLM\nproperties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kauf_C/0/1/0/all/0/1\">Carina Kauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivanova_A/0/1/0/all/0/1\">Anna Ivanova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tree of Thoughts: Deliberate Problem Solving with Large Language Models. (arXiv:2305.10601v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10601","description":"<p>Language models are increasingly being deployed for general problem solving\nacross a wide range of tasks, but are still confined to token-level,\nleft-to-right decision-making processes during inference. This means they can\nfall short in tasks that require exploration, strategic lookahead, or where\ninitial decisions play a pivotal role. To surmount these challenges, we\nintroduce a new framework for language model inference, Tree of Thoughts (ToT),\nwhich generalizes over the popular Chain of Thought approach to prompting\nlanguage models, and enables exploration over coherent units of text (thoughts)\nthat serve as intermediate steps toward problem solving. ToT allows LMs to\nperform deliberate decision making by considering multiple different reasoning\npaths and self-evaluating choices to decide the next course of action, as well\nas looking ahead or backtracking when necessary to make global choices. Our\nexperiments show that ToT significantly enhances language models'\nproblem-solving abilities on three novel tasks requiring non-trivial planning\nor search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in\nGame of 24, while GPT-4 with chain-of-thought prompting only solved 4% of\ntasks, our method achieved a success rate of 74%. Code repo with all prompts:\nhttps://github.com/ysymyth/tree-of-thought-llm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1\">Shunyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jeffrey Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafran_I/0/1/0/all/0/1\">Izhak Shafran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solving Cosine Similarity Underestimation between High Frequency Words by L2 Norm Discounting. (arXiv:2305.10610v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10610","description":"<p>Cosine similarity between two words, computed using their contextualised\ntoken embeddings obtained from masked language models (MLMs) such as BERT has\nshown to underestimate the actual similarity between those words (Zhou et al.,\n2022). This similarity underestimation problem is particularly severe for\nhighly frequent words. Although this problem has been noted in prior work, no\nsolution has been proposed thus far. We observe that the L2 norm of\ncontextualised embeddings of a word correlates with its log-frequency in the\npretraining corpus. Consequently, the larger L2 norms associated with the\nhighly frequent words reduce the cosine similarity values measured between\nthem, thus underestimating the similarity scores. To solve this issue, we\npropose a method to discount the L2 norm of a contextualised word embedding by\nthe frequency of that word in a corpus when measuring the cosine similarities\nbetween words. We show that the so called stop words behave differently from\nthe rest of the words, which require special consideration during their\ndiscounting process. Experimental results on a contextualised word similarity\ndataset show that our proposed discounting method accurately solves the\nsimilarity underestimation problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wannasuphoprasit_S/0/1/0/all/0/1\">Saeth Wannasuphoprasit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning. (arXiv:2305.10613v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10613","description":"<p>Temporal knowledge graph (TKG) forecasting benchmarks challenge models to\npredict future facts using knowledge of past facts. In this paper, we apply\nlarge language models (LLMs) to these benchmarks using in-context learning\n(ICL). We investigate whether and to what extent LLMs can be used for TKG\nforecasting, especially without any fine-tuning or explicit modules for\ncapturing structural and temporal information. For our experiments, we present\na framework that converts relevant historical facts into prompts and generates\nranked predictions using token probabilities. Surprisingly, we observe that\nLLMs, out-of-the-box, perform on par with state-of-the-art TKG models carefully\ndesigned and trained for TKG forecasting. Our extensive evaluation presents\nperformances across several models and datasets with different characteristics,\ncompares alternative heuristics for preparing contextual information, and\ncontrasts to prominent TKG methods and simple frequency and recency baselines.\nWe also discover that using numerical indices instead of entity/relation names,\ni.e., hiding semantic information, does not significantly affect the\nperformance ($\\pm$0.4\\% Hit@1). This shows that prior semantic knowledge is\nunnecessary; instead, LLMs can leverage the existing patterns in the context to\nachieve such performance. Our analysis also reveals that ICL enables LLMs to\nlearn irregular patterns from the historical context, going beyond simple\npredictions based on common or recent information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong-Ho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahrabian_K/0/1/0/all/0/1\">Kian Ahrabian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Woojeong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morstatter_F/0/1/0/all/0/1\">Fred Morstatter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Token-wise Decomposition of Autoregressive Language Model Hidden States for Analyzing Model Predictions. (arXiv:2305.10614v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10614","description":"<p>While there is much recent interest in studying why Transformer-based large\nlanguage models make predictions the way they do, the complex computations\nperformed within each layer have traditionally posed a strong bottleneck. To\nmitigate this shortcoming, this work presents a linear decomposition of final\nhidden states from autoregressive language models based on each initial input\ntoken, which is exact for virtually all contemporary Transformer architectures.\nThis decomposition allows the definition of probability distributions that\nablate the contribution of specific input tokens, which can be used to analyze\ntheir influence on model probabilities over a sequence of upcoming words with\nonly one forward pass from the model. Using the change in next-word probability\nas a measure of importance, this work first examines which context words make\nthe biggest contribution to language model predictions. Regression experiments\nsuggest that Transformer-based language models rely primarily on collocational\nassociations, followed by linguistic factors such as syntactic dependencies and\ncoreference relationships in making next-word predictions. Additionally,\nanalyses using these measures to predict syntactic dependencies and coreferent\nmention spans show that collocational association and repetitions of the same\ntoken respectively, largely explain the language model's predictions on the\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oh_B/0/1/0/all/0/1\">Byung-Doh Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuler_W/0/1/0/all/0/1\">William Schuler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ML-SUPERB: Multilingual Speech Universal PERformance Benchmark. (arXiv:2305.10615v1 [cs.SD])","link":"http://arxiv.org/abs/2305.10615","description":"<p>Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard\nto benchmark the performance of Self-Supervised Learning (SSL) models on\nvarious speech processing tasks. However, SUPERB largely considers English\nspeech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB),\ncovering 143 languages (ranging from high-resource to endangered), and\nconsidering both automatic speech recognition and language identification.\nFollowing the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and\nemploys a simple framework for multilingual tasks by learning a shallow\ndownstream model. Similar to the SUPERB benchmark, we find speech SSL models\ncan significantly improve performance compared to FBANK features. Furthermore,\nwe find that multilingual models do not always perform better than their\nmonolingual counterparts. We will release ML-SUPERB as a challenge with\norganized datasets and reproducible training scripts for future multilingual\nrepresentation research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrebbi_D/0/1/0/all/0/1\">Dan Berrebbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">William Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Ho-Lam Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1\">En-Pei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Ping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models Meet World Models: Embodied Experiences Enhance Language Models. (arXiv:2305.10626v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10626","description":"<p>While large language models (LMs) have shown remarkable capabilities across\nnumerous tasks, they often struggle with simple reasoning and planning in\nphysical environments, such as understanding object permanence or planning\nhousehold activities. The limitation arises from the fact that LMs are trained\nonly on written text and miss essential embodied knowledge and skills. In this\npaper, we propose a new paradigm of enhancing LMs by finetuning them with world\nmodels, to gain diverse embodied knowledge while retaining their general\nlanguage capabilities. Our approach deploys an embodied agent in a world model,\nparticularly a simulator of the physical world (VirtualHome), and acquires a\ndiverse set of embodied experiences through both goal-oriented planning and\nrandom exploration. These experiences are then used to finetune LMs to teach\ndiverse abilities of reasoning and acting in the physical world, e.g., planning\nand completing goals, object permanence and tracking, etc. Moreover, it is\ndesirable to preserve the generality of LMs during finetuning, which\nfacilitates generalizing the embodied knowledge across tasks rather than being\ntied to specific simulations. We thus further introduce the classical elastic\nweight consolidation (EWC) for selective weight updates, combined with low-rank\nadapters (LoRA) for training efficiency. Extensive experiments show our\napproach substantially improves base LMs on 18 downstream tasks by 64.28% on\naverage. In particular, the small LMs (1.3B and 6B) enhanced by our approach\nmatch or even outperform much larger LMs (e.g., ChatGPT).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1\">Jiannan Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_T/0/1/0/all/0/1\">Tianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1\">Tianmin Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Large Language Models Fit For Guided Reading?. (arXiv:2305.10645v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10645","description":"<p>This paper looks at the ability of large language models to participate in\neducational guided reading. We specifically, evaluate their ability to generate\nmeaningful questions from the input text, generate diverse questions both in\nterms of content coverage and difficulty of the questions and evaluate their\nability to recommend part of the text that a student should re-read based on\nthe student's responses to the questions. Based on our evaluation of ChatGPT\nand Bard, we report that,\n</p>\n<p>1) Large language models are able to generate high quality meaningful\nquestions that have high correlation with the input text, 2) They generate\ndiverse question that cover most topics in the input text even though this\nability is significantly degraded as the input text increases, 3)The large\nlanguage models are able to generate both low and high cognitive questions even\nthough they are significantly biased toward low cognitive question, 4) They are\nable to effectively summarize responses and extract a portion of text that\nshould be re-read.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ochieng_P/0/1/0/all/0/1\">Peter Ochieng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BioAug: Conditional Generation based Data Augmentation for Low-Resource Biomedical NER. (arXiv:2305.10647v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10647","description":"<p>Biomedical Named Entity Recognition (BioNER) is the fundamental task of\nidentifying named entities from biomedical text. However, BioNER suffers from\nsevere data scarcity and lacks high-quality labeled data due to the highly\nspecialized and expert knowledge required for annotation. Though data\naugmentation has shown to be highly effective for low-resource NER in general,\nexisting data augmentation techniques fail to produce factual and diverse\naugmentations for BioNER. In this paper, we present BioAug, a novel data\naugmentation framework for low-resource BioNER. BioAug, built on BART, is\ntrained to solve a novel text reconstruction task based on selective masking\nand knowledge augmentation. Post training, we perform conditional generation\nand generate diverse augmentations conditioning BioAug on selectively corrupted\ntext similar to the training stage. We demonstrate the effectiveness of BioAug\non 5 benchmark BioNER datasets and show that BioAug outperforms all our\nbaselines by a significant margin (1.5%-21.5% absolute improvement) and is able\nto generate augmentations that are both more factual and diverse. Code:\nhttps://github.com/Sreyan88/BioAug.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_U/0/1/0/all/0/1\">Utkarsh Tyagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sonal Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs. (arXiv:2305.10649v1 [cs.SD])","link":"http://arxiv.org/abs/2305.10649","description":"<p>In this paper, we present ZeroPrompt (Figure 1-(a)) and the corresponding\nPrompt-and-Refine strategy (Figure 3), two simple but effective\n\\textbf{training-free} methods to decrease the Token Display Time (TDT) of\nstreaming ASR models \\textbf{without any accuracy loss}. The core idea of\nZeroPrompt is to append zeroed content to each chunk during inference, which\nacts like a prompt to encourage the model to predict future tokens even before\nthey were spoken. We argue that streaming acoustic encoders naturally have the\nmodeling ability of Masked Language Models and our experiments demonstrate that\nZeroPrompt is engineering cheap and can be applied to streaming acoustic\nencoders on any dataset without any accuracy loss. Specifically, compared with\nour baseline models, we achieve 350 $\\sim$ 700ms reduction on First Token\nDisplay Time (TDT-F) and 100 $\\sim$ 400ms reduction on Last Token Display Time\n(TDT-L), with theoretically and experimentally equal WER on both Aishell-1 and\nLibrispeech datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingchen Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhendong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_B/0/1/0/all/0/1\">Bo Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Fuping Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Separation based on Contrastive Learning and Deep Modularization. (arXiv:2305.10652v1 [cs.SD])","link":"http://arxiv.org/abs/2305.10652","description":"<p>The current monaural state of the art tools for speech separation relies on\nsupervised learning. This means that they must deal with permutation problem,\nthey are impacted by the mismatch on the number of speakers used in training\nand inference. Moreover, their performance heavily relies on the presence of\nhigh-quality labelled data. These problems can be effectively addressed by\nemploying a fully unsupervised technique for speech separation. In this paper,\nwe use contrastive learning to establish the representations of frames then use\nthe learned representations in the downstream deep modularization task.\nConcretely, we demonstrate experimentally that in speech separation, different\nframes of a speaker can be viewed as augmentations of a given hidden standard\nframe of that speaker. The frames of a speaker contain enough prosodic\ninformation overlap which is key in speech separation. Based on this, we\nimplement a self-supervised learning to learn to minimize the distance between\nframes belonging to a given speaker. The learned representations are used in a\ndownstream deep modularization task to cluster frames based on speaker\nidentity. Evaluation of the developed technique on WSJ0-2mix and WSJ0-3mix\nshows that the technique attains SI-SNRi and SDRi of 20.8 and 21.0 respectively\nin WSJ0-2mix. In WSJ0-3mix, it attains SI-SNRi and SDRi of 20.7 and 20.7\nrespectively in WSJ0-2mix. Its greatest strength being that as the number of\nspeakers increase, its performance does not degrade significantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ochieng_P/0/1/0/all/0/1\">Peter Ochieng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"a unified front-end framework for english text-to-speech synthesis. (arXiv:2305.10666v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10666","description":"<p>The front-end is a critical component of English text-to-speech (TTS)\nsystems, responsible for extracting linguistic features that are essential for\na text-to-speech model to synthesize speech, such as prosodies and phonemes.\nThe English TTS front-end typically consists of a text normalization (TN)\nmodule, a prosody word prosody phrase (PWPP) module, and a grapheme-to-phoneme\n(G2P) module. However, current research on the English TTS front-end focuses\nsolely on individual modules, neglecting the interdependence between them and\nresulting in sub-optimal performance for each module. Therefore, this paper\nproposes a unified front-end framework that captures the dependencies among the\nEnglish TTS front-end modules. Extensive experiments have demonstrated that the\nproposed method achieves state-of-the-art (SOTA) performance in all modules.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ying_Z/0/1/0/all/0/1\">Zelin Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1\">Qiuqiang Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">YuanYuan Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Think Outside the Code: Brainstorming Boosts Large Language Models in Code Generation. (arXiv:2305.10679v1 [cs.AI])","link":"http://arxiv.org/abs/2305.10679","description":"<p>Code generation aims to automatically generate source code from high-level\ntask specifications, which can significantly increase productivity of software\nengineering. Recently, approaches based on large language models (LLMs) have\nshown remarkable code generation abilities on simple tasks. However, generate\ncode for more complex tasks, such as competition-level problems, remains\nchallenging. In this paper, we introduce Brainstorm framework for code\ngeneration. It leverages a brainstorming step that generates and selects\ndiverse thoughts on the problem to facilitate algorithmic reasoning, where the\nthoughts are possible blueprint of solving the problem. We demonstrate that\nBrainstorm significantly enhances the ability of LLMs to solve\ncompetition-level programming problems, resulting in a more than 50% increase\nin the pass@$k$ metrics for ChatGPT on the CodeContests benchmark, achieving\nstate-of-the-art performance. Furthermore, our experiments conducted on\nLeetCode contests show that our framework boosts the ability of ChatGPT to a\nlevel comparable to that of human programmers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin-Ye Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jiang-Tian Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Ming Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate and Reliable Confidence Estimation Based on Non-Autoregressive End-to-End Speech Recognition System. (arXiv:2305.10680v1 [cs.SD])","link":"http://arxiv.org/abs/2305.10680","description":"<p>Estimating confidence scores for recognition results is a classic task in ASR\nfield and of vital importance for kinds of downstream tasks and training\nstrategies. Previous end-to-end~(E2E) based confidence estimation models (CEM)\npredict score sequences of equal length with input transcriptions, leading to\nunreliable estimation when deletion and insertion errors occur. In this paper\nwe proposed CIF-Aligned confidence estimation model (CA-CEM) to achieve\naccurate and reliable confidence estimation based on novel non-autoregressive\nE2E ASR model - Paraformer. CA-CEM utilizes the modeling character of\ncontinuous integrate-and-fire (CIF) mechanism to generate token-synchronous\nacoustic embedding, which solves the estimation failure issue above. We measure\nthe quality of estimation with AUC and RMSE in token level and ECE-U - a\nproposed metrics in utterance level. CA-CEM gains 24% and 19% relative\nreduction on ECE-U and also better AUC and RMSE on two test sets. Furthermore,\nwe conduct analysis to explore the potential of CEM for different ASR related\nusage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haoneng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhifu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhijie Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paxion: Patching Action Knowledge in Video-Language Foundation Models. (arXiv:2305.10683v1 [cs.CV])","link":"http://arxiv.org/abs/2305.10683","description":"<p>Action knowledge involves the understanding of textual, visual, and temporal\naspects of actions. We introduce the Action Dynamics Benchmark (ActionBench)\ncontaining two carefully designed probing tasks: Action Antonym and Video\nReversal, which targets multimodal alignment capabilities and temporal\nunderstanding skills of the model, respectively. Despite recent video-language\nmodels' (VidLM) impressive performance on various benchmark tasks, our\ndiagnostic tasks reveal their surprising deficiency (near-random performance)\nin action knowledge, suggesting that current models rely on object recognition\nabilities as a shortcut for action understanding. To remedy this, we propose a\nnovel framework, Paxion, along with a new Discriminative Video Dynamics\nModeling (DVDM) objective. The Paxion framework utilizes a Knowledge Patcher\nnetwork to encode new action knowledge and a Knowledge Fuser component to\nintegrate the Patcher into frozen VidLMs without compromising their existing\ncapabilities. Due to limitations of the widely-used Video-Text Contrastive\n(VTC) loss for learning action knowledge, we introduce the DVDM objective to\ntrain the Knowledge Patcher. DVDM forces the model to encode the correlation\nbetween the action text and the correct ordering of video frames. Our extensive\nanalyses show that Paxion and DVDM together effectively fill the gap in action\nknowledge understanding (~50% to 80%), while maintaining or improving\nperformance on a wide spectrum of both object- and action-centric downstream\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhailong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blume_A/0/1/0/all/0/1\">Ansel Blume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Genglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zineng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RMSSinger: Realistic-Music-Score based Singing Voice Synthesis. (arXiv:2305.10686v1 [cs.SD])","link":"http://arxiv.org/abs/2305.10686","description":"<p>We are interested in a challenging task, Realistic-Music-Score based Singing\nVoice Synthesis (RMS-SVS). RMS-SVS aims to generate high-quality singing voices\ngiven realistic music scores with different note types (grace, slur, rest,\netc.). Though significant progress has been achieved, recent singing voice\nsynthesis (SVS) methods are limited to fine-grained music scores, which require\na complicated data collection pipeline with time-consuming manual annotation to\nalign music notes with phonemes. Furthermore, these manual annotation destroys\nthe regularity of note durations in music scores, making fine-grained music\nscores inconvenient for composing. To tackle these challenges, we propose\nRMSSinger, the first RMS-SVS method, which takes realistic music scores as\ninput, eliminating most of the tedious manual annotation and avoiding the\naforementioned inconvenience. Note that music scores are based on words rather\nthan phonemes, in RMSSinger, we introduce word-level modeling to avoid the\ntime-consuming phoneme duration annotation and the complicated phoneme-level\nmel-note alignment. Furthermore, we propose the first diffusion-based pitch\nmodeling method, which ameliorates the naturalness of existing pitch-modeling\nmethods. To achieve these, we collect a new dataset containing realistic music\nscores and singing voices according to these realistic music scores from\nprofessional singers. Extensive experiments on the dataset demonstrate the\neffectiveness of our methods. Audio samples are available at\nhttps://rmssinger.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jinzheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zhenhui Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rongjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Chenye Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huadai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MolXPT: Wrapping Molecules with Text for Generative Pre-training. (arXiv:2305.10688v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10688","description":"<p>Generative pre-trained Transformer (GPT) has demonstrates its great success\nin natural language processing and related techniques have been adapted into\nmolecular modeling. Considering that text is the most important record for\nscientific discovery, in this paper, we propose MolXPT, a unified language\nmodel of text and molecules pre-trained on SMILES (a sequence representation of\nmolecules) wrapped by text. Briefly, we detect the molecule names in each\nsequence and replace them to the corresponding SMILES. In this way, the SMILES\ncould leverage the information from surrounding text, and vice versa. The above\nwrapped sequences, text sequences from PubMed and SMILES sequences from PubChem\nare all fed into a language model for pre-training. Experimental results\ndemonstrate that MolXPT outperforms strong baselines of molecular property\nprediction on MoleculeNet, performs comparably to the best model in\ntext-molecule translation while using less than half of its parameters, and\nenables zero-shot molecular generation without finetuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zequn Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yingce Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lijun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Shufang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval. (arXiv:2305.10703v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10703","description":"<p>With the development of large language models (LLMs), zero-shot learning has\nattracted much attention for various NLP tasks. Different from prior works that\ngenerate training data with billion-scale natural language generation (NLG)\nmodels, we propose a retrieval-enhanced framework to create training data from\na general-domain unlabeled corpus. To realize this, we first conduct\ncontrastive pretraining to learn an unsupervised dense retriever for extracting\nthe most relevant documents using class-descriptive verbalizers. We then\nfurther propose two simple strategies, namely Verbalizer Augmentation with\nDemonstrations and Self-consistency Guided Filtering to improve the topic\ncoverage of the dataset while removing noisy examples. Experiments on nine\ndatasets demonstrate that REGEN achieves 4.3% gain over the strongest baselines\nand saves around 70% of the time compared to baselines using large NLG models.\nBesides, REGEN can be naturally integrated with recently proposed large\nlanguage models to boost performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yuchen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiaming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NoisywikiHow: A Benchmark for Learning with Real-world Noisy Labels in Natural Language Processing. (arXiv:2305.10709v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10709","description":"<p>Large-scale datasets in the real world inevitably involve label noise. Deep\nmodels can gradually overfit noisy labels and thus degrade model\ngeneralization. To mitigate the effects of label noise, learning with noisy\nlabels (LNL) methods are designed to achieve better generalization performance.\nDue to the lack of suitable datasets, previous studies have frequently employed\nsynthetic label noise to mimic real-world label noise. However, synthetic noise\nis not instance-dependent, making this approximation not always effective in\npractice. Recent research has proposed benchmarks for learning with real-world\nnoisy labels. However, the noise sources within may be single or fuzzy, making\nbenchmarks different from data with heterogeneous label noises in the real\nworld. To tackle these issues, we contribute NoisywikiHow, the largest NLP\nbenchmark built with minimal supervision. Specifically, inspired by human\ncognition, we explicitly construct multiple sources of label noise to imitate\nhuman errors throughout the annotation, replicating real-world noise, whose\ncorruption is affected by both ground-truth labels and instances. Moreover, we\nprovide a variety of noise levels to support controlled experiments on noisy\ndata, enabling us to evaluate LNL methods systematically and comprehensively.\nAfter that, we conduct extensive multi-dimensional experiments on a broad range\nof LNL methods, obtaining new and intriguing findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tingting Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Minji Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency. (arXiv:2305.10713v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10713","description":"<p>With growing capabilities of large language models, prompting them has become\nthe dominant way to access them. This has motivated the development of\nstrategies for automatically selecting effective language prompts. In this\npaper, we introduce prompt flatness, a new metric to quantify the expected\nutility of a language prompt. This metric is inspired by flatness\nregularization in statistical learning that quantifies the robustness of the\nmodel towards its parameter perturbations. We provide theoretical foundations\nfor this metric and its relationship with other prompt selection metrics,\nproviding a comprehensive understanding of existing methods. Empirically, we\nshow that combining prompt flatness with existing metrics improves both\nperformance and sample efficiency. Our metric outperforms the previous prompt\nselection metrics with an average increase of 5% in accuracy and 10% in Pearson\ncorrelation across 6 classification benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lingfeng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Weiting Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Boyuan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Norm Violations in Live-Stream Chat. (arXiv:2305.10731v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10731","description":"<p>Toxic language, such as hate speech, can deter users from participating in\nonline communities and enjoying popular platforms. Previous approaches to\ndetecting toxic language and norm violations have been primarily concerned with\nconversations from online forums and social media, such as Reddit and Twitter.\nThese approaches are less effective when applied to conversations on\nlive-streaming platforms, such as Twitch and YouTube Live, as each comment is\nonly visible for a limited time and lacks a thread structure that establishes\nits relationship with other comments. In this work, we share the first NLP\nstudy dedicated to detecting norm violations in conversations on live-streaming\nplatforms. We define norm violation categories in live-stream chats and\nannotate 4,583 moderated comments from Twitch. We articulate several facets of\nlive-stream data that differ from other forums, and demonstrate that existing\nmodels perform poorly in this setting. By conducting a user study, we identify\nthe informational context humans use in live-stream moderation, and train\nmodels leveraging context to identify norm violations. Our results show that\nappropriate contextual information can boost moderation performance by 35\\%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1\">Jihyung Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong-Ho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyundong Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Woojeong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chan Young Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungjoon Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffusion-Based Speech Enhancement with Joint Generative and Predictive Decoders. (arXiv:2305.10734v1 [cs.SD])","link":"http://arxiv.org/abs/2305.10734","description":"<p>Diffusion-based speech enhancement (SE) has been investigated recently, but\nits decoding is very time-consuming. One solution is to initialize the decoding\nprocess with the enhanced feature estimated by a predictive SE system. However,\nthis two-stage method ignores the complementarity between predictive and\ndiffusion SE. In this paper, we propose a unified system that integrates these\ntwo SE modules. The system encodes both generative and predictive information,\nand then applies both generative and predictive decoders, whose outputs are\nfused. Specifically, the two SE modules are fused in the first and final\ndiffusion steps: the first step fusion initializes the diffusion process with\nthe predictive SE for improving the convergence, and the final step fusion\ncombines the two complementary SE outputs to improve the SE performance.\nExperiments on the Voice-Bank dataset show that the diffusion score estimation\ncan benefit from the predictive information and speed up the decoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimada_K/0/1/0/all/0/1\">Kazuki Shimada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirano_M/0/1/0/all/0/1\">Masato Hirano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shibuya_T/0/1/0/all/0/1\">Takashi Shibuya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyama_Y/0/1/0/all/0/1\">Yuichiro Koyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takahashi_S/0/1/0/all/0/1\">Shusuke Takahashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawahara_T/0/1/0/all/0/1\">Tatsuya Kawahara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitsufuji_Y/0/1/0/all/0/1\">Yuki Mitsufuji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual Debiasing for Generating Factually Consistent Text Summaries. (arXiv:2305.10736v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10736","description":"<p>Despite substantial progress in abstractive text summarization to generate\nfluent and informative texts, the factual inconsistency in the generated\nsummaries remains an important yet challenging problem to be solved. In this\npaper, we construct causal graphs for abstractive text summarization and\nidentify the intrinsic causes of the factual inconsistency, i.e., the language\nbias and irrelevancy bias, and further propose a debiasing framework, named\nCoFactSum, to alleviate the causal effects of these biases by counterfactual\nestimation. Specifically, the proposed CoFactSum provides two counterfactual\nestimation strategies, i.e., Explicit Counterfactual Masking with an explicit\ndynamic masking strategy, and Implicit Counterfactual Training with an implicit\ndiscriminative cross-attention mechanism. Meanwhile, we design a Debiasing\nDegree Adjustment mechanism to dynamically adapt the debiasing degree at each\ndecoding step. Extensive experiments on two widely-used summarization datasets\ndemonstrate the effectiveness of CoFactSum in enhancing the factual consistency\nof generated summaries compared with several baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chenhe Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuexiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings. (arXiv:2305.10786v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10786","description":"<p>Prior studies diagnose the anisotropy problem in sentence representations\nfrom pre-trained language models, e.g., BERT, without fine-tuning. Our analysis\nreveals that the sentence embeddings from BERT suffer from a bias towards\nuninformative words, limiting the performance in semantic textual similarity\n(STS) tasks. To address this bias, we propose a simple and efficient\nunsupervised approach, Diagonal Attention Pooling (Ditto), which weights words\nwith model-based importance estimations and computes the weighted average of\nword representations from pre-trained models as sentence embeddings. Ditto can\nbe easily applied to any pre-trained language model as a postprocessing\noperation. Compared to prior sentence embedding approaches, Ditto does not add\nparameters nor requires any learning. Empirical evaluations demonstrate that\nour proposed Ditto can alleviate the anisotropy problem and improve various\npre-trained models on STS tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Siqi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yukun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Whisper-KDQ: A Lightweight Whisper via Guided Knowledge Distillation and Quantization for Efficient ASR. (arXiv:2305.10788v1 [cs.SD])","link":"http://arxiv.org/abs/2305.10788","description":"<p>Due to the rapid development of computing hardware resources and the dramatic\ngrowth of data, pre-trained models in speech recognition, such as Whisper, have\nsignificantly improved the performance of speech recognition tasks. However,\nthese models usually have a high computational overhead, making it difficult to\nexecute effectively on resource-constrained devices. To speed up inference and\nreduce model size while maintaining performance, we propose a novel guided\nknowledge distillation and quantization for large pre-trained model Whisper.\nThe student model selects distillation and quantization layers based on\nquantization loss and distillation loss, respectively. We compressed\n$\\text{Whisper}_\\text{small}$ to $\\text{Whisper}_\\text{base}$ and\n$\\text{Whisper}_\\text{tiny}$ levels, making $\\text{Whisper}_\\text{small}$\n5.18x/10.48x smaller, respectively. Moreover, compared to the original\n$\\text{Whisper}_\\text{base}$ and $\\text{Whisper}_\\text{tiny}$, there is also a\nrelative character error rate~(CER) reduction of 11.3% and 14.0% for the new\ncompressed model respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1\">Hang Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yanmin Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Democratized Diffusion Language Model. (arXiv:2305.10818v1 [cs.LG])","link":"http://arxiv.org/abs/2305.10818","description":"<p>Despite the potential benefits of Diffusion Models for NLP applications,\npublicly available implementations, trained models, or reproducible training\nprocedures currently need to be publicly available. We present the Democratized\nDiffusion Language Model (DDLM), based on the Continuous Diffusion for\nCategorical Data (CDCD) framework, to address these challenges. We propose a\nsimplified training procedure for DDLM using the C4 dataset and perform an\nin-depth analysis of the trained model's behavior. Furthermore, we introduce a\nnovel early-exiting strategy for faster sampling with models trained with score\ninterpolation. Since no previous works aimed at solving downstream tasks with\npre-trained Diffusion LM (e.g., classification tasks), we experimented with\nGLUE Benchmark to study the ability of DDLM to transfer knowledge. With this\npaper, we propose available training and evaluation pipelines to other\nresearchers and pre-trained DDLM models, which could be used in future research\nwith Diffusion LMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balagansky_N/0/1/0/all/0/1\">Nikita Balagansky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavrilov_D/0/1/0/all/0/1\">Daniil Gavrilov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLEME: Debiasing Multi-reference Evaluation for Grammatical Error Correction. (arXiv:2305.10819v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10819","description":"<p>It is intractable to evaluate the performance of Grammatical Error Correction\n(GEC) systems since GEC is a highly subjective task. Designing an evaluation\nmetric that is as objective as possible is crucial to the development of GEC\ntask. Previous mainstream evaluation metrics, i.e., reference-based metrics,\nintroduce bias into the multi-reference evaluation because they extract edits\nwithout considering the presence of multiple references. To overcome the\nproblem, we propose Chunk-LEvel Multi-reference Evaluation (CLEME) designed to\nevaluate GEC systems in multi-reference settings. First, CLEME builds chunk\nsequences with consistent boundaries for the source, the hypothesis and all the\nreferences, thus eliminating the bias caused by inconsistent edit boundaries.\nThen, based on the discovery that there exist boundaries between different\ngrammatical errors, we automatically determine the grammatical error boundaries\nand compute F$_{0.5}$ scores in a novel way. Our proposed CLEME approach\nconsistently and substantially outperforms existing reference-based GEC metrics\non multiple reference sets in both corpus-level and sentence-level settings.\nExtensive experiments and detailed analyses demonstrate the correctness of our\ndiscovery and the effectiveness of our designed evaluation metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jingheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shirong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Methods for Extracting Metaphorical Names of Flowers and Plants. (arXiv:2305.10833v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10833","description":"<p>The domain of Botany is rich with metaphorical terms. Those terms play an\nimportant role in the description and identification of flowers and plants.\nHowever, the identification of such terms in discourse is an arduous task. This\nleads in some cases to committing errors during translation processes and\nlexicographic tasks. The process is even more challenging when it comes to\nmachine translation, both in the cases of single-word terms and multi-word\nterms. One of the recent concerns of Natural Language Processing (NLP)\napplications and Machine Translation (MT) technologies is the automatic\nidentification of metaphor-based words in discourse through Deep Learning (DL).\nIn this study, we seek to fill this gap through the use of thirteen popular\ntransformer based models, as well as ChatGPT, and we show that discriminative\nmodels perform better than GPT-3.5 model with our best performer reporting\n92.2349% F1 score in metaphoric flower and plant names identification task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haddad_A/0/1/0/all/0/1\">Amal Haddad Haddad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Premasiri_D/0/1/0/all/0/1\">Damith Premasiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitkov_R/0/1/0/all/0/1\">Ruslan Mitkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AIwriting: Relations Between Image Generation and Digital Writing. (arXiv:2305.10834v1 [cs.AI])","link":"http://arxiv.org/abs/2305.10834","description":"<p>During 2022, both transformer-based AI text generation sys-tems such as GPT-3\nand AI text-to-image generation systems such as DALL-E 2 and Stable Diffusion\nmade exponential leaps forward and are unquestionably altering the fields of\ndigital art and electronic literature. In this panel a group of electronic\nliterature authors and theorists consider new oppor-tunities for human\ncreativity presented by these systems and present new works have produced\nduring the past year that specifically address these systems as environments\nfor literary expressions that are translated through iterative interlocutive\nprocesses into visual representations. The premise that binds these\npresentations is that these systems and the works gener-ated must be considered\nfrom a literary perspective, as they originate in human writing. In works\nranging from a visual memoir of the personal experience of a health crisis, to\ninterac-tive web comics, to architectures based on abstract poetic language, to\npolitical satire, four artists explore the capabili-ties of these writing\nenvironments for new genres of literary artist practice, while a digital\nculture theorist considers the origins and effects of the particular training\ndatasets of human language and images on which these new hybrid forms are\nbased.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rettberg_S/0/1/0/all/0/1\">Scott Rettberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Memmott_T/0/1/0/all/0/1\">Talan Memmott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rettberg_J/0/1/0/all/0/1\">Jill Walker Rettberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nelson_J/0/1/0/all/0/1\">Jason Nelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lichty_P/0/1/0/all/0/1\">Patrick Lichty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ahead-of-Time P-Tuning. (arXiv:2305.10835v1 [cs.LG])","link":"http://arxiv.org/abs/2305.10835","description":"<p>In this paper, we propose Ahead-of-Time (AoT) P-Tuning, a novel\nparameter-efficient fine-tuning method for pre-trained Language Models (LMs)\nthat adds input-dependent bias before each Transformer layer. We evaluate AoT\nP-Tuning on GLUE and SuperGLUE benchmarking datasets using RoBERTa and DeBERTa\nmodels, showing that it outperforms BitFit and is comparable or better than\nother baseline methods for efficient fine-tuning. Additionally, we assess the\ninference overhead of AoT P-Tuning and demonstrate that it introduces\nnegligible overhead compared to established baseline methods. Our method\nenables multi-task inference with a single backbone LM, making it a practical\nsolution for real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gavrilov_D/0/1/0/all/0/1\">Daniil Gavrilov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balagansky_N/0/1/0/all/0/1\">Nikita Balagansky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Lexical-aware Non-autoregressive Transformer-based ASR Model. (arXiv:2305.10839v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10839","description":"<p>Non-autoregressive automatic speech recognition (ASR) has become a mainstream\nof ASR modeling because of its fast decoding speed and satisfactory result. To\nfurther boost the performance, relaxing the conditional independence assumption\nand cascading large-scaled pre-trained models are two active research\ndirections. In addition to these strategies, we propose a lexical-aware\nnon-autoregressive Transformer-based (LA-NAT) ASR framework, which consists of\nan acoustic encoder, a speech-text shared encoder, and a speech-text shared\ndecoder. The acoustic encoder is used to process the input speech features as\nusual, and the speech-text shared encoder and decoder are designed to train\nspeech and text data simultaneously. By doing so, LA-NAT aims to make the ASR\nmodel aware of lexical information, so the resulting model is expected to\nachieve better results by leveraging the learned linguistic knowledge. A series\nof experiments are conducted on the AISHELL-1, CSJ, and TEDLIUM 2 datasets.\nAccording to the experiments, the proposed LA-NAT can provide superior results\nthan other recently proposed non-autoregressive ASR models. In addition, LA-NAT\nis a relatively compact model than most non-autoregressive ASR models, and it\nis about 58 times faster than the classic autoregressive model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chong-En Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kuan-Yu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TAPIR: Learning Adaptive Revision for Incremental Natural Language Understanding with a Two-Pass Model. (arXiv:2305.10845v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10845","description":"<p>Language is by its very nature incremental in how it is produced and\nprocessed. This property can be exploited by NLP systems to produce fast\nresponses, which has been shown to be beneficial for real-time interactive\napplications. Recent neural network-based approaches for incremental processing\nmainly use RNNs or Transformers. RNNs are fast but monotonic (cannot correct\nearlier output, which can be necessary in incremental processing).\nTransformers, on the other hand, consume whole sequences, and hence are by\nnature non-incremental. A restart-incremental interface that repeatedly passes\nlonger input prefixes can be used to obtain partial outputs, while providing\nthe ability to revise. However, this method becomes costly as the sentence\ngrows longer. In this work, we propose the Two-pass model for AdaPtIve Revision\n(TAPIR) and introduce a method to obtain an incremental supervision signal for\nlearning an adaptive revision policy. Experimental results on sequence\nlabelling show that our model has better incremental performance and faster\ninference speed compared to restart-incremental Transformers, while showing\nlittle degradation on full sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kahardipraja_P/0/1/0/all/0/1\">Patrick Kahardipraja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madureira_B/0/1/0/all/0/1\">Brielen Madureira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlangen_D/0/1/0/all/0/1\">David Schlangen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models can be Guided to Evade AI-Generated Text Detection. (arXiv:2305.10847v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10847","description":"<p>Large Language Models (LLMs) have demonstrated exceptional performance in a\nvariety of tasks, including essay writing and question answering. However, it\nis crucial to address the potential misuse of these models, which can lead to\ndetrimental outcomes such as plagiarism and spamming. Recently, several\ndetectors have been proposed, including fine-tuned classifiers and various\nstatistical methods. In this study, we reveal that with the aid of carefully\ncrafted prompts, LLMs can effectively evade these detection systems. We propose\na novel Substitution-based In-Context example Optimization method (SICO) to\nautomatically generate such prompts. On three real-world tasks where LLMs can\nbe misused, SICO successfully enables ChatGPT to evade six existing detectors,\ncausing a significant 0.54 AUC drop on average. Surprisingly, in most cases\nthese detectors perform even worse than random classifiers. These results\nfirmly reveal the vulnerability of existing detectors. Finally, the strong\nperformance of SICO suggests itself as a reliable evaluation protocol for any\nnew detector in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_N/0/1/0/all/0/1\">Ning Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengcai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Rui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advancing Full-Text Search Lemmatization Techniques with Paradigm Retrieval from OpenCorpora. (arXiv:2305.10848v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10848","description":"<p>In this paper, we unveil a groundbreaking method to amplify full-text search\nlemmatization, utilizing the OpenCorpora dataset and a bespoke paradigm\nretrieval algorithm. Our primary aim is to streamline the extraction of a\nword's primary form or lemma - a crucial factor in full-text search.\nAdditionally, we propose a compact dictionary storage strategy, significantly\nboosting the speed and precision of lemma retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalugin_Balashov_D/0/1/0/all/0/1\">Dmitriy Kalugin-Balashov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantically Aligned Task Decomposition in Multi-Agent Reinforcement Learning. (arXiv:2305.10865v1 [cs.LG])","link":"http://arxiv.org/abs/2305.10865","description":"<p>The difficulty of appropriately assigning credit is particularly heightened\nin cooperative MARL with sparse reward, due to the concurrent time and\nstructural scales involved. Automatic subgoal generation (ASG) has recently\nemerged as a viable MARL approach inspired by utilizing subgoals in\nintrinsically motivated reinforcement learning. However, end-to-end learning of\ncomplex task planning from sparse rewards without prior knowledge, undoubtedly\nrequires massive training samples. Moreover, the diversity-promoting nature of\nexisting ASG methods can lead to the \"over-representation\" of subgoals,\ngenerating numerous spurious subgoals of limited relevance to the actual task\nreward and thus decreasing the sample efficiency of the algorithm. To address\nthis problem and inspired by the disentangled representation learning, we\npropose a novel \"disentangled\" decision-making method, Semantically Aligned\ntask decomposition in MARL (SAMA), that prompts pretrained language models with\nchain-of-thought that can suggest potential goals, provide suitable goal\ndecomposition and subgoal allocation as well as self-reflection-based\nreplanning. Additionally, SAMA incorporates language-grounded RL to train each\nagent's subgoal-conditioned policy. SAMA demonstrates considerable advantages\nin sample efficiency compared to state-of-the-art ASG methods, as evidenced by\nits performance on two challenging sparse-reward tasks, Overcooked and MiniRTS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_D/0/1/0/all/0/1\">Dan Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Baoxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiangfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1\">Bo Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1\">Hongyuan Zha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TEPrompt: Task Enlightenment Prompt Learning for Implicit Discourse Relation Recognition. (arXiv:2305.10866v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10866","description":"<p>Implicit Discourse Relation Recognition (IDRR) aims at classifying the\nrelation sense between two arguments without an explicit connective. Recently,\nthe ConnPrompt~\\cite{Wei.X:et.al:2022:COLING} has leveraged the powerful prompt\nlearning for IDRR based on the fusion of multi-prompt decisions from three\ndifferent yet much similar connective prediction templates. Instead of\nmulti-prompt ensembling, we propose to design auxiliary tasks with enlightened\nprompt learning for the IDRR task. Although an auxiliary task is not used to\ndirectly output final prediction, we argue that during the joint training some\nof its learned features can be useful to boost the main task. In light of such\nmotivations, we propose a task enlightenment prompt learning model, called\nTEPrompt, to fuse learned features from three related tasks for IDRR. In\nparticular, the TEPrompt contains three tasks, viz., Discourse Relation\nRecognition (DRR), Sense Semantics Classification (SSC) and Annotated\nConnective Prediction (ACP), each with a unique prompt template and an answer\nspace. In the training phase, we jointly train three prompt learning tasks with\nshared argument representation. In the testing phase, we only take the DRR\noutput with fused features as the final IDRR decision. Experiments with the\nsame conditions have shown that the proposed TEPrompt outperforms the\nConnPrompt. This can be attributed to the promoted decision features and\nlanguage models benefited from joint-training of auxiliary tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Wei Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EventNet-ITA: Italian Frame Parsing for Events. (arXiv:2305.10892v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10892","description":"<p>This paper introduces EventNet-ITA, a large, multi-domain corpus annotated\nwith event frames for Italian, and presents an efficient approach for\nmulti-label Frame Parsing. The approach is then evaluated on the dataset.\nCovering a wide range of individual, social and historical phenomena, the main\ncontribution of EventNet-ITA is to provide the research community with a\nresource for textual event mining and a novel and extensive tool for Frame\nParsing in Italian.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rovera_M/0/1/0/all/0/1\">Marco Rovera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Take a Break in the Middle: Investigating Subgoals towards Hierarchical Script Generation. (arXiv:2305.10907v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10907","description":"<p>Goal-oriented Script Generation is a new task of generating a list of steps\nthat can fulfill the given goal. In this paper, we propose to extend the task\nfrom the perspective of cognitive theory. Instead of a simple flat structure,\nthe steps are typically organized hierarchically - Human often decompose a\ncomplex task into subgoals, where each subgoal can be further decomposed into\nsteps. To establish the benchmark, we contribute a new dataset, propose several\nbaseline methods, and set up evaluation metrics. Both automatic and human\nevaluation verify the high-quality of dataset, as well as the effectiveness of\nincorporating subgoals into hierarchical script generation. Furthermore, We\nalso design and evaluate the model to discover subgoal, and find that it is a\nbit more difficult to decompose the goals than summarizing from segmented\nsteps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-Supervised Visual-Textual Grounding with Semantic Prior Refinement. (arXiv:2305.10913v1 [cs.CV])","link":"http://arxiv.org/abs/2305.10913","description":"<p>Using only image-sentence pairs, weakly-supervised visual-textual grounding\naims to learn region-phrase correspondences of the respective entity mentions.\nCompared to the supervised approach, learning is more difficult since bounding\nboxes and textual phrases correspondences are unavailable. In light of this, we\npropose the Semantic Prior Refinement Model (SPRM), whose predictions are\nobtained by combining the output of two main modules. The first untrained\nmodule aims to return a rough alignment between textual phrases and bounding\nboxes. The second trained module is composed of two sub-components that refine\nthe rough alignment to improve the accuracy of the final phrase-bounding box\nalignments. The model is trained to maximize the multimodal similarity between\nan image and a sentence, while minimizing the multimodal similarity of the same\nsentence and a new unrelated image, carefully selected to help the most during\ntraining. Our approach shows state-of-the-art results on two popular datasets,\nFlickr30k Entities and ReferIt, shining especially on ReferIt with a 9.6%\nabsolute improvement. Moreover, thanks to the untrained component, it reaches\ncompetitive performances just using a small fraction of training examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rigoni_D/0/1/0/all/0/1\">Davide Rigoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parolari_L/0/1/0/all/0/1\">Luca Parolari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serafini_L/0/1/0/all/0/1\">Luciano Serafini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sperduti_A/0/1/0/all/0/1\">Alessandro Sperduti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballan_L/0/1/0/all/0/1\">Lamberto Ballan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent Communication with Attention. (arXiv:2305.10920v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10920","description":"<p>To develop computational agents that better communicate using their own\nemergent language, we endow the agents with an ability to focus their attention\non particular concepts in the environment. Humans often understand an object or\nscene as a composite of concepts and those concepts are further mapped onto\nwords. We implement this intuition as cross-modal attention mechanisms in\nSpeaker and Listener agents in a referential game and show attention leads to\nmore compositional and interpretable emergent language. We also demonstrate how\nattention aids in understanding the learned communication protocol by\ninvestigating the attention weights associated with each message symbol and the\nalignment of attention weights between Speaker and Listener agents. Overall,\nour results suggest that attention is a promising mechanism for developing more\nhuman-like emergent language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ri_R/0/1/0/all/0/1\">Ryokan Ri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ueda_R/0/1/0/all/0/1\">Ryo Ueda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naradowsky_J/0/1/0/all/0/1\">Jason Naradowsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query Performance Prediction: From Ad-hoc to Conversational Search. (arXiv:2305.10923v1 [cs.IR])","link":"http://arxiv.org/abs/2305.10923","description":"<p>Query performance prediction (QPP) is a core task in information retrieval.\nThe QPP task is to predict the retrieval quality of a search system for a query\nwithout relevance judgments. Research has shown the effectiveness and\nusefulness of QPP for ad-hoc search. Recent years have witnessed considerable\nprogress in conversational search (CS). Effective QPP could help a CS system to\ndecide an appropriate action to be taken at the next turn. Despite its\npotential, QPP for CS has been little studied. We address this research gap by\nreproducing and studying the effectiveness of existing QPP methods in the\ncontext of CS. While the task of passage retrieval remains the same in the two\nsettings, a user query in CS depends on the conversational history, introducing\nnovel QPP challenges. In particular, we seek to explore to what extent findings\nfrom QPP methods for ad-hoc search generalize to three CS settings: (i)\nestimating the retrieval quality of different query rewriting-based retrieval\nmethods, (ii) estimating the retrieval quality of a conversational dense\nretrieval method, and (iii) estimating the retrieval quality for top ranks vs.\ndeeper-ranked lists. Our findings can be summarized as follows: (i) supervised\nQPP methods distinctly outperform unsupervised counterparts only when a\nlarge-scale training set is available; (ii) point-wise supervised QPP methods\noutperform their list-wise counterparts in most cases; and (iii) retrieval\nscore-based unsupervised QPP methods show high effectiveness in assessing the\nconversational dense retrieval method, ConvDR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1\">Chuan Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arabzadeh_N/0/1/0/all/0/1\">Negar Arabzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aliannejadi_M/0/1/0/all/0/1\">Mohammad Aliannejadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Document-Grounded Dialogue Pre-training. (arXiv:2305.10927v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10927","description":"<p>The goal of document-grounded dialogue (DocGD) is to generate a response by\ngrounding the evidence in a supporting document in accordance with the dialogue\ncontext. This process involves four variables that are causally connected.\nRecently, task-specific pre-training has greatly boosted performances on many\ndownstream tasks. Existing DocGD methods, however, continue to rely on general\npre-trained language models without a specifically tailored pre-training\napproach that explicitly captures the causal relationships. To tackle this\nissue, we are the first to present a causally-complete dataset construction\nstrategy for building million-level DocGD pre-training corpora. To better\ncapture causality, we further propose a causally-perturbed pre-training\nstrategy, which introduces causal perturbations on the variables and optimizes\nthe overall causal effect. Experiments on three benchmark datasets demonstrate\nthat our causal pre-training achieves considerable and consistent improvements\nunder fully-supervised, low-resource, few-shot, and zero-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yingxiu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bowen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Nevin L. Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Event Extraction from Historical Newspaper Adverts. (arXiv:2305.10928v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10928","description":"<p>NLP methods can aid historians in analyzing textual materials in greater\nvolumes than manually feasible. Developing such methods poses substantial\nchallenges though. First, acquiring large, annotated historical datasets is\ndifficult, as only domain experts can reliably label them. Second, most\navailable off-the-shelf NLP models are trained on modern language texts,\nrendering them significantly less effective when applied to historical corpora.\nThis is particularly problematic for less well studied tasks, and for languages\nother than English. This paper addresses these challenges while focusing on the\nunder-explored task of event extraction from a novel domain of historical\ntexts. We introduce a new multilingual dataset in English, French, and Dutch\ncomposed of newspaper ads from the early modern colonial period reporting on\nenslaved people who liberated themselves from enslavement. We find that: 1)\neven with scarce annotated data, it is possible to achieve surprisingly good\nresults by formulating the problem as an extractive QA task and leveraging\nexisting datasets and models for modern languages; and 2) cross-lingual\nlow-resource learning for historical languages is highly challenging, and\nmachine translation of the historical datasets to the considered target\nlanguages is, in practice, often the best-performing solution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borenstein_N/0/1/0/all/0/1\">Nadav Borenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_N/0/1/0/all/0/1\">Natalia da Silva Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Off-Target Problem of Zero-Shot Multilingual Neural Machine Translation. (arXiv:2305.10930v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10930","description":"<p>While multilingual neural machine translation has achieved great success, it\nsuffers from the off-target issue, where the translation is in the wrong\nlanguage. This problem is more pronounced on zero-shot translation tasks. In\nthis work, we find that failing in encoding discriminative target language\nsignal will lead to off-target and a closer lexical distance (i.e.,\nKL-divergence) between two languages' vocabularies is related with a higher\noff-target rate. We also find that solely isolating the vocab of different\nlanguages in the decoder can alleviate the problem. Motivated by the findings,\nwe propose Language Aware Vocabulary Sharing (LAVS), a simple and effective\nalgorithm to construct the multilingual vocabulary, that greatly alleviates the\noff-target problem of the translation model by increasing the KL-divergence\nbetween languages. We conduct experiments on a multilingual machine translation\nbenchmark in 11 languages. Experiments show that the off-target rate for 90\ntranslation tasks is reduced from 29\\% to 8\\%, while the overall BLEU score is\nimproved by an average of 1.9 points without extra training cost or sacrificing\nthe supervised directions' performance. We release the code at\n\\href{https://github.com/chenllliang/Off-Target-MNMT}{https://github.com/chenllliang/Off-Target-MNMT}\nfor reproduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making More of Little Data: Improving Low-Resource Automatic Speech Recognition Using Data Augmentation. (arXiv:2305.10951v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10951","description":"<p>The performance of automatic speech recognition (ASR) systems has advanced\nsubstantially in recent years, particularly for languages for which a large\namount of transcribed speech is available. Unfortunately, for low-resource\nlanguages, such as minority languages, regional languages or dialects, ASR\nperformance generally remains much lower. In this study, we investigate whether\ndata augmentation techniques could help improve low-resource ASR performance,\nfocusing on four typologically diverse minority languages or language variants\n(West Germanic: Gronings, West-Frisian; Malayo-Polynesian: Besemah, Nasal). For\nall four languages, we examine the use of self-training, where an ASR system\ntrained with the available human-transcribed data is used to generate\ntranscriptions, which are then combined with the original data to train a new\nASR system. For Gronings, for which there was a pre-existing text-to-speech\n(TTS) system available, we also examined the use of TTS to generate ASR\ntraining data from text-only sources. We find that using a self-training\napproach consistently yields improved performance (a relative WER reduction up\nto 20.5% compared to using an ASR system trained on 24 minutes of manually\ntranscribed speech). The performance gain from TTS augmentation for Gronings\nwas even stronger (up to 25.5% relative reduction in WER compared to a system\nbased on 24 minutes of manually transcribed speech). In sum, our results show\nthe benefit of using self-training or (if possible) TTS-generated data as an\nefficient solution to overcome the limitations of data availability for\nresource-scarce languages in order to improve ASR performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartelds_M/0/1/0/all/0/1\">Martijn Bartelds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+San_N/0/1/0/all/0/1\">Nay San</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonnell_B/0/1/0/all/0/1\">Bradley McDonnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieling_M/0/1/0/all/0/1\">Martijn Wieling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NollySenti: Leveraging Transfer Learning and Machine Translation for Nigerian Movie Sentiment Classification. (arXiv:2305.10971v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10971","description":"<p>Africa has over 2000 indigenous languages but they are under-represented in\nNLP research due to lack of datasets. In recent years, there have been progress\nin developing labeled corpora for African languages. However, they are often\navailable in a single domain and may not generalize to other domains. In this\npaper, we focus on the task of sentiment classification for cross domain\nadaptation. We create a new dataset, NollySenti - based on the Nollywood movie\nreviews for five languages widely spoken in Nigeria (English, Hausa, Igbo,\nNigerian-Pidgin, and Yoruba. We provide an extensive empirical evaluation using\nclassical machine learning methods and pre-trained language models. Leveraging\ntransfer learning, we compare the performance of cross-domain adaptation from\nTwitter domain, and cross-lingual adaptation from English language. Our\nevaluation shows that transfer from English in the same target domain leads to\nmore than 5% improvement in accuracy compared to transfer from Twitter in the\nsame language. To further mitigate the domain difference, we leverage machine\ntranslation (MT) from English to other Nigerian languages, which leads to a\nfurther improvement of 7% over cross-lingual evaluation. While MT to\nlow-resource languages are often of low quality, through human evaluation, we\nshow that most of the translated sentences preserve the sentiment of the\noriginal English reviews.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shode_I/0/1/0/all/0/1\">Iyanuoluwa Shode</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jing Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_A/0/1/0/all/0/1\">Anna Feldman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-CrossRE A Multi-Lingual Multi-Domain Dataset for Relation Extraction. (arXiv:2305.10985v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10985","description":"<p>Most research in Relation Extraction (RE) involves the English language,\nmainly due to the lack of multi-lingual resources. We propose Multi-CrossRE,\nthe broadest multi-lingual dataset for RE, including 26 languages in addition\nto English, and covering six text domains. Multi-CrossRE is a machine\ntranslated version of CrossRE (Bassignana and Plank, 2022), with a sub-portion\nincluding more than 200 sentences in seven diverse languages checked by native\nspeakers. We run a baseline model over the 26 new datasets and--as sanity\ncheck--over the 26 back-translations to English. Results on the back-translated\ndata are consistent with the ones on the original English CrossRE, indicating\nhigh quality of the translation and the resulting dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bassignana_E/0/1/0/all/0/1\">Elisa Bassignana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginter_F/0/1/0/all/0/1\">Filip Ginter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyysalo_S/0/1/0/all/0/1\">Sampo Pyysalo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goot_R/0/1/0/all/0/1\">Rob van der Goot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less is More! A slim architecture for optimal language translation. (arXiv:2305.10991v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10991","description":"<p>The softmax attention mechanism has emerged as a noteworthy development in\nthe field of Artificial Intelligence research, building on the successes of\nTransformer-based architectures. However, their ever increasing sizes\nnecessitate ever increasing computational memory, that limits their usage. We\npropose KgV, a sigmoid gating mechanism that, in conjunction with softmax\nattention, significantly boosts performance without increasing architecture\nsize. To amend the size requirements, we leverage Tensor Chains to identify and\nprune the excess parameters. We find that such excess resides primarily within\nthe embedding layer, and not in the output linear layer. To further improve\nembedding and significantly reduce parameters, we introduce H-SoftPOS, a\nhierarchical embedding layer which simultaneously enhances performance.\nRemarkably, on the WMT14 English-German validation set, our approach yields a\nthreefold reduction in perplexity, surpassing the current state-of-the-art,\nwhile reducing parameter counts also by a factor of 3. When we further reduce\nthe number of parameters up to sevenfold, we can still achieve a 21\\% decrease\nin perplexity with respect to the baseline Transformer. To understand\ngeneralization capabilities, we conduct experiments on the 7 language pairs of\nthe WMT17 dataset. Our method outperforms existing techniques in terms of test\nloss while simultaneously halving the number of parameters. Moreover, we\nobserve a 70 times reduction in variance with respect to the prior\nstate-of-the-art. In conclusion, our proposed method yields significant\nimprovements in performance and much lower memory cost. We call the resulting\narchitecture Anthe.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Herranz_Celotti_L/0/1/0/all/0/1\">Luca Herranz-Celotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rrapaj_E/0/1/0/all/0/1\">Ermal Rrapaj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How does the task complexity of masked pretraining objectives affect downstream performance?. (arXiv:2305.10992v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10992","description":"<p>Masked language modeling (MLM) is a widely used self-supervised pretraining\nobjective, where a model needs to predict an original token that is replaced\nwith a mask given contexts. Although simpler and computationally efficient\npretraining objectives, e.g., predicting the first character of a masked token,\nhave recently shown comparable results to MLM, no objectives with a masking\nscheme actually outperform it in downstream tasks. Motivated by the assumption\nthat their lack of complexity plays a vital role in the degradation, we\nvalidate whether more complex masked objectives can achieve better results and\ninvestigate how much complexity they should have to perform comparably to MLM.\nOur results using GLUE, SQuAD, and Universal Dependencies benchmarks\ndemonstrate that more complicated objectives tend to show better downstream\nresults with at least half of the MLM complexity needed to perform comparably\nto MLM. Finally, we discuss how we should pretrain a model using a masked\nobjective from the task complexity perspective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamaguchi_A/0/1/0/all/0/1\">Atsuki Yamaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozaki_H/0/1/0/all/0/1\">Hiroaki Ozaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morishita_T/0/1/0/all/0/1\">Terufumi Morishita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morio_G/0/1/0/all/0/1\">Gaku Morio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogawa_Y/0/1/0/all/0/1\">Yasuhiro Sogawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Web Can Be Your Oyster for Improving Large Language Models. (arXiv:2305.10998v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10998","description":"<p>Large language models (LLMs) encode a large amount of world knowledge.\nHowever, as such knowledge is frozen at the time of model training, the models\nbecome static and limited by the training data at that time. In order to\nfurther improve the capacity of LLMs for knowledge-intensive tasks, we consider\naugmenting LLMs with the large-scale web using search engine. Unlike previous\naugmentation sources (e.g., Wikipedia data dump), the web provides broader,\nmore comprehensive and constantly updated information. In this paper, we\npresent a web-augmented LLM UNIWEB, which is trained over 16\nknowledge-intensive tasks in a unified text-to-text format. Instead of simply\nusing the retrieved contents from web, our approach has made two major\nimprovements. Firstly, we propose an adaptive search engine assisted learning\nmethod that can self-evaluate the confidence level of LLM's predictions, and\nadaptively determine when to refer to the web for more data, which can avoid\nuseless or noisy augmentation from web. Secondly, we design a pretraining task,\ni.e., continual knowledge learning, based on salient spans prediction, to\nreduce the discrepancy between the encoded and retrieved knowledge. Experiments\non a wide range of knowledge-intensive tasks show that our model significantly\noutperforms previous retrieval-augmented methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities. (arXiv:2305.11000v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11000","description":"<p>Multi-modal large language models are regarded as a crucial step towards\nArtificial General Intelligence (AGI) and have garnered significant interest\nwith the emergence of ChatGPT. However, current speech-language models\ntypically adopt the cascade paradigm, preventing inter-modal knowledge\ntransfer. In this paper, we propose SpeechGPT, a large language model with\nintrinsic cross-modal conversational abilities, capable of perceiving and\ngenerating multi-model content. With discrete speech representations, we first\nconstruct SpeechInstruct, a large-scale cross-modal speech instruction dataset.\nAdditionally, we employ a three-stage training strategy that includes\nmodality-adaptation pre-training, cross-modal instruction fine-tuning, and\nchain-of-modality instruction fine-tuning. The experimental results demonstrate\nthat SpeechGPT has an impressive capacity to follow multi-modal human\ninstructions and highlight the potential of handling multiple modalities with\none model. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shimin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1\">Jun Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yaqian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Taxonomy Completion with Probabilistic Scorer via Box Embedding. (arXiv:2305.11004v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11004","description":"<p>Taxonomy completion, a task aimed at automatically enriching an existing\ntaxonomy with new concepts, has gained significant interest in recent years.\nPrevious works have introduced complex modules, external information, and\npseudo-leaves to enrich the representation and unify the matching process of\nattachment and insertion. While they have achieved good performance, these\nintroductions may have brought noise and unfairness during training and\nscoring. In this paper, we present TaxBox, a novel framework for taxonomy\ncompletion that maps taxonomy concepts to box embeddings and employs two\nprobabilistic scorers for concept attachment and insertion, avoiding the need\nfor pseudo-leaves. Specifically, TaxBox consists of three components: (1) a\ngraph aggregation module to leverage the structural information of the taxonomy\nand two lightweight decoders that map features to box embedding and capture\ncomplex relationships between concepts; (2) two probabilistic scorers that\ncorrespond to attachment and insertion operations and ensure the avoidance of\npseudo-leaves; and (3) three learning objectives that assist the model in\nmapping concepts more granularly onto the box embedding space. Experimental\nresults on four real-world datasets suggest that TaxBox outperforms baseline\nmethods by a considerable margin and surpasses previous state-of-art methods to\na certain extent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1\">Wei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yongliang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Wenqi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jietian Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Siliang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weiming Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FunASR: A Fundamental End-to-End Speech Recognition Toolkit. (arXiv:2305.11013v1 [cs.SD])","link":"http://arxiv.org/abs/2305.11013","description":"<p>This paper introduces FunASR, an open-source speech recognition toolkit\ndesigned to bridge the gap between academic research and industrial\napplications. FunASR offers models trained on large-scale industrial corpora\nand the ability to deploy them in applications. The toolkit's flagship model,\nParaformer, is a non-autoregressive end-to-end speech recognition model that\nhas been trained on a manually annotated Mandarin speech recognition dataset\nthat contains 60,000 hours of speech. To improve the performance of Paraformer,\nwe have added timestamp prediction and hotword customization capabilities to\nthe standard Paraformer backbone. In addition, to facilitate model deployment,\nwe have open-sourced a voice activity detection model based on the Feedforward\nSequential Memory Network (FSMN-VAD) and a text post-processing punctuation\nmodel based on the controllable time-delay Transformer (CT-Transformer), both\nof which were trained on industrial corpora. These functional modules provide a\nsolid foundation for building high-precision long audio speech recognition\nservices. Compared to other models trained on open datasets, Paraformer\ndemonstrates superior performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhifu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zerui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haoneng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mengzhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yabin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_L/0/1/0/all/0/1\">Lingyun Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zhihao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhangyu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Silver Syntax Pre-training for Cross-Domain Relation Extraction. (arXiv:2305.11016v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11016","description":"<p>Relation Extraction (RE) remains a challenging task, especially when\nconsidering realistic out-of-domain evaluations. One of the main reasons for\nthis is the limited training size of current RE datasets: obtaining\nhigh-quality (manually annotated) data is extremely expensive and cannot\nrealistically be repeated for each new domain. An intermediate training step on\ndata from related tasks has shown to be beneficial across many NLP\ntasks.However, this setup still requires supplementary annotated data, which is\noften not available. In this paper, we investigate intermediate pre-training\nspecifically for RE. We exploit the affinity between syntactic structure and\nsemantic RE, and identify the syntactic relations which are closely related to\nRE by being on the shortest dependency path between two entities. We then take\nadvantage of the high accuracy of current syntactic parsers in order to\nautomatically obtain large amounts of low-cost pre-training data. By\npre-training our RE model on the relevant syntactic relations, we are able to\noutperform the baseline in five out of six cross-domain setups, without any\nadditional annotated data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bassignana_E/0/1/0/all/0/1\">Elisa Bassignana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginter_F/0/1/0/all/0/1\">Filip Ginter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyysalo_S/0/1/0/all/0/1\">Sampo Pyysalo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goot_R/0/1/0/all/0/1\">Rob van der Goot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Multiple Intent Conditioned Slot Filling. (arXiv:2305.11023v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11023","description":"<p>Natural language understanding includes the tasks of intent detection\n(identifying a user's objectives) and slot filling (extracting the entities\nrelevant to those objectives). Prior slot filling methods assume that each\nintent type cannot occur more than once within a message, however this is often\nnot a valid assumption for real-world settings. In this work, we generalize\nslot filling by removing the constraint of unique intents in a message. We cast\nthis as a JSON generation task and approach it using a language model. We\ncreate a pre-training dataset by combining DBpedia and existing slot filling\ndatasets that we convert for JSON generation. We also generate an in-domain\ndataset using GPT-3. We train T5 models for this task (with and without\nexemplars in the prompt) and find that both training datasets improve\nperformance, and that the model is able to generalize to intent types not seen\nduring training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_H/0/1/0/all/0/1\">Harshil Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilcke_A/0/1/0/all/0/1\">Arthur Wilcke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobzarenco_M/0/1/0/all/0/1\">Marius Cobzarenco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobzarenco_C/0/1/0/all/0/1\">Cristi Cobzarenco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Challis_E/0/1/0/all/0/1\">Edward Challis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barber_D/0/1/0/all/0/1\">David Barber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction. (arXiv:2305.11029v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11029","description":"<p>Document-level relation extraction (DocRE) aims to infer complex semantic\nrelations among entities in a document. Distant supervision (DS) is able to\ngenerate massive auto-labeled data, which can improve DocRE performance. Recent\nworks leverage pseudo labels generated by the pre-denoising model to reduce\nnoise in DS data. However, unreliable pseudo labels bring new noise, e.g.,\nadding false pseudo labels and losing correct DS labels. Therefore, how to\nselect effective pseudo labels to denoise DS data is still a challenge in\ndocument-level distant relation extraction. To tackle this issue, we introduce\nuncertainty estimation technology to determine whether pseudo labels can be\ntrusted. In this work, we propose a Document-level distant Relation Extraction\nframework with Uncertainty Guided label denoising, UGDRE. Specifically, we\npropose a novel instance-level uncertainty estimation method, which measures\nthe reliability of the pseudo labels with overlapping relations. By further\nconsidering the long-tail problem, we design dynamic uncertainty thresholds for\ndifferent types of relations to filter high-uncertainty pseudo labels. We\nconduct experiments on two public datasets. Our framework outperforms strong\nbaselines by 1.91 F1 and 2.28 Ign F1 on the RE-DocRED dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaocui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1\">Pengfei Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trading Syntax Trees for Wordpieces: Target-oriented Opinion Words Extraction with Wordpieces and Aspect Enhancement. (arXiv:2305.11034v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11034","description":"<p>State-of-the-art target-oriented opinion word extraction (TOWE) models\ntypically use BERT-based text encoders that operate on the word level, along\nwith graph convolutional networks (GCNs) that incorporate syntactic information\nextracted from syntax trees. These methods achieve limited gains with GCNs and\nhave difficulty using BERT wordpieces. Meanwhile, BERT wordpieces are known to\nbe effective at representing rare words or words with insufficient context\ninformation. To address this issue, this work trades syntax trees for BERT\nwordpieces by entirely removing the GCN component from the methods'\narchitectures. To enhance TOWE performance, we tackle the issue of aspect\nrepresentation loss during encoding. Instead of solely utilizing a sentence as\nthe input, we use a sentence-aspect pair. Our relatively simple approach\nachieves state-of-the-art results on benchmark datasets and should serve as a\nstrong baseline for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mensah_S/0/1/0/all/0/1\">Samuel Mensah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Kai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning In-context Learning for Named Entity Recognition. (arXiv:2305.11038v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11038","description":"<p>Named entity recognition in real-world applications suffers from the\ndiversity of entity types, the emergence of new entity types, and the lack of\nhigh-quality annotations. To address the above problems, this paper proposes an\nin-context learning-based NER approach, which can effectively inject in-context\nNER ability into PLMs and recognize entities of novel types on-the-fly using\nonly a few demonstrative instances. Specifically, we model PLMs as a\nmeta-function $\\mathcal{ \\lambda_ {\\text{instruction, demonstrations, text}}.\nM}$, and a new entity extractor can be implicitly constructed by applying new\ninstruction and demonstrations to PLMs, i.e., $\\mathcal{ (\\lambda . M)\n}$(instruction, demonstrations) $\\to$ $\\mathcal{F}$ where $\\mathcal{F}$ will be\na new entity extractor, i.e., $\\mathcal{F}$: text $\\to$ entities. To inject the\nabove in-context NER ability into PLMs, we propose a meta-function pre-training\nalgorithm, which pre-trains PLMs by comparing the (instruction,\ndemonstration)-initialized extractor with a surrogate golden extractor.\nExperimental results on 4 few-shot NER datasets show that our method can\neffectively inject in-context NER ability into PLMs and significantly\noutperforms the PLMs+fine-tuning counterparts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yaojie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jie Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Wei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1\">Boxi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERM: Training the Balanced and Extractable Representation for Matching to Improve Generalization Ability of Dense Retrieval. (arXiv:2305.11052v1 [cs.IR])","link":"http://arxiv.org/abs/2305.11052","description":"<p>Dense retrieval has shown promise in the first-stage retrieval process when\ntrained on in-domain labeled datasets. However, previous studies have found\nthat dense retrieval is hard to generalize to unseen domains due to its weak\nmodeling of domain-invariant and interpretable feature (i.e., matching signal\nbetween two texts, which is the essence of information retrieval). In this\npaper, we propose a novel method to improve the generalization of dense\nretrieval via capturing matching signal called BERM. Fully fine-grained\nexpression and query-oriented saliency are two properties of the matching\nsignal. Thus, in BERM, a single passage is segmented into multiple units and\ntwo unit-level requirements are proposed for representation as the constraint\nin training to obtain the effective matching signal. One is semantic unit\nbalance and the other is essential matching unit extractability. Unit-level\nview and balanced semantics make representation express the text in a\nfine-grained manner. Essential matching unit extractability makes passage\nrepresentation sensitive to the given query to extract the pure matching\ninformation from the passage containing complex context. Experiments on BEIR\nshow that our method can be effectively combined with different dense retrieval\ntraining methods (vanilla, hard negatives mining and knowledge distillation) to\nimprove its generalization ability without any additional inference overhead\nand target domain data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shicheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPSQL: Step-by-step Parsing Based Framework for Text-to-SQL Generation. (arXiv:2305.11061v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11061","description":"<p>Converting text into the structured query language (Text2SQL) is a research\nhotspot in the field of natural language processing (NLP), which has broad\napplication prospects. In the era of big data, the use of databases has\npenetrated all walks of life, in which the collected data is large in scale,\ndiverse in variety, and wide in scope, making the data query cumbersome and\ninefficient, and putting forward higher requirements for the Text2SQL model. In\npractical applications, the current mainstream end-to-end Text2SQL model is not\nonly difficult to build due to its complex structure and high requirements for\ntraining data, but also difficult to adjust due to massive parameters. In\naddition, the accuracy of the model is hard to achieve the desired result.\nBased on this, this paper proposes a pipelined Text2SQL method: SPSQL. This\nmethod disassembles the Text2SQL task into four subtasks--table selection,\ncolumn selection, SQL generation, and value filling, which can be converted\ninto a text classification problem, a sequence labeling problem, and two text\ngeneration problems, respectively. Then, we construct data formats of different\nsubtasks based on existing data and improve the accuracy of the overall model\nby improving the accuracy of each submodel. We also use the named entity\nrecognition module and data augmentation to optimize the overall model. We\nconstruct the dataset based on the marketing business data of the State Grid\nCorporation of China. Experiments demonstrate our proposed method achieves the\nbest performance compared with the end-to-end method and other pipeline\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_R/0/1/0/all/0/1\">Ran Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Gang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Liangfeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Han Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bits of Grass: Does GPT already know how to write like Whitman?. (arXiv:2305.11064v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11064","description":"<p>This study examines the ability of GPT-3.5, GPT-3.5-turbo (ChatGPT) and GPT-4\nmodels to generate poems in the style of specific authors using zero-shot and\nmany-shot prompts (which use the maximum context length of 8192 tokens). We\nassess the performance of models that are not fine-tuned for generating poetry\nin the style of specific authors, via automated evaluation. Our findings\nindicate that without fine-tuning, even when provided with the maximum number\nof 17 poem examples (8192 tokens) in the prompt, these models do not generate\npoetry in the desired style.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sawicki_P/0/1/0/all/0/1\">Piotr Sawicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzes_M/0/1/0/all/0/1\">Marek Grzes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goes_F/0/1/0/all/0/1\">Fabricio Goes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1\">Dan Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peeperkorn_M/0/1/0/all/0/1\">Max Peeperkorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khatun_A/0/1/0/all/0/1\">Aisha Khatun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ORKG-Leaderboards: A Systematic Workflow for Mining Leaderboards as a Knowledge Graph. (arXiv:2305.11068v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11068","description":"<p>The purpose of this work is to describe the Orkg-Leaderboard software\ndesigned to extract leaderboards defined as Task-Dataset-Metric tuples\nautomatically from large collections of empirical research papers in Artificial\nIntelligence (AI). The software can support both the main workflows of\nscholarly publishing, viz. as LaTeX files or as PDF files. Furthermore, the\nsystem is integrated with the Open Research Knowledge Graph (ORKG) platform,\nwhich fosters the machine-actionable publishing of scholarly findings. Thus the\nsystem output, when integrated within the ORKG's supported Semantic Web\ninfrastructure of representing machine-actionable 'resources' on the Web,\nenables: 1) broadly, the integration of empirical results of researchers across\nthe world, thus enabling transparency in empirical research with the potential\nto also being complete contingent on the underlying data source(s) of\npublications; and 2) specifically, enables researchers to track the progress in\nAI with an overview of the state-of-the-art (SOTA) across the most common AI\ntasks and their corresponding datasets via dynamic ORKG frontend views\nleveraging tables and visualization charts over the machine-actionable data.\nOur best model achieves performances above 90% F1 on the \\textit{leaderboard}\nextraction task, thus proving Orkg-Leaderboards a practically viable tool for\nreal-world usage. Going forward, in a sense, Orkg-Leaderboards transforms the\nleaderboard extraction task to an automated digitalization task, which has\nbeen, for a long time in the community, a crowdsourced endeavor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kabongo_S/0/1/0/all/0/1\">Salomon Kabongo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DSouza_J/0/1/0/all/0/1\">Jennifer D&#x27;Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auer_S/0/1/0/all/0/1\">S&#xf6;ren Auer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enriching language models with graph-based context information to better understand textual data. (arXiv:2305.11070v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11070","description":"<p>A considerable number of texts encountered daily are somehow connected with\neach other. For example, Wikipedia articles refer to other articles via\nhyperlinks, scientific papers relate to others via citations or (co)authors,\nwhile tweets relate via users that follow each other or reshare content. Hence,\na graph-like structure can represent existing connections and be seen as\ncapturing the \"context\" of the texts. The question thus arises if extracting\nand integrating such context information into a language model might help\nfacilitate a better automated understanding of the text. In this study, we\nexperimentally demonstrate that incorporating graph-based contextualization\ninto BERT model enhances its performance on an example of a classification\ntask. Specifically, on Pubmed dataset, we observed a reduction in error from\n8.51% to 7.96%, while increasing the number of parameters just by 1.6%.\n</p>\n<p>Our source code: https://github.com/tryptofanik/gc-bert\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roethel_A/0/1/0/all/0/1\">Albert Roethel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganzha_M/0/1/0/all/0/1\">Maria Ganzha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wroblewska_A/0/1/0/all/0/1\">Anna Wr&#xf3;blewska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Fine-tuning for Improved Content Representations by Speaker-invariant Clustering. (arXiv:2305.11072v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11072","description":"<p>Self-supervised speech representation models have succeeded in various tasks,\nbut improving them for content-related problems using unlabeled data is\nchallenging. We propose speaker-invariant clustering (Spin), a novel\nself-supervised learning method that clusters speech representations and\nperforms swapped prediction between the original and speaker-perturbed\nutterances. Spin disentangles speaker information and preserves content\nrepresentations with just 45 minutes of fine-tuning on a single GPU. Spin\nimproves pre-trained networks and outperforms prior methods in speech\nrecognition and acoustic unit discovery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alexander H. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks. (arXiv:2305.11073v1 [cs.CL])","link":"http://arxiv.org/abs/2305.11073","description":"<p>Conformer, a convolution-augmented Transformer variant, has become the de\nfacto encoder architecture for speech processing due to its superior\nperformance in various tasks, including automatic speech recognition (ASR),\nspeech translation (ST) and spoken language understanding (SLU). Recently, a\nnew encoder called E-Branchformer has outperformed Conformer in the LibriSpeech\nASR benchmark, making it promising for more general speech applications. This\nwork compares E-Branchformer and Conformer through extensive experiments using\ndifferent types of end-to-end sequence-to-sequence models. Results demonstrate\nthat E-Branchformer achieves comparable or better performance than Conformer in\nalmost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while\nbeing more stable during training. We will release our training configurations\nand pre-trained models for reproducibility, which can benefit the speech\ncommunity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kwangyoun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Felix Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Siddhant Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">William Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiyang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shon_S/0/1/0/all/0/1\">Suwon Shon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_P/0/1/0/all/0/1\">Prashant Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inspecting the Geographical Representativeness of Images from Text-to-Image Models. (arXiv:2305.11080v1 [cs.CV])","link":"http://arxiv.org/abs/2305.11080","description":"<p>Recent progress in generative models has resulted in models that produce both\nrealistic as well as relevant images for most textual inputs. These models are\nbeing used to generate millions of images everyday, and hold the potential to\ndrastically impact areas such as generative art, digital marketing and data\naugmentation. Given their outsized impact, it is important to ensure that the\ngenerated content reflects the artifacts and surroundings across the globe,\nrather than over-representing certain parts of the world. In this paper, we\nmeasure the geographical representativeness of common nouns (e.g., a house)\ngenerated through DALL.E 2 and Stable Diffusion models using a crowdsourced\nstudy comprising 540 participants across 27 countries. For deliberately\nunderspecified inputs without country names, the generated images most reflect\nthe surroundings of the United States followed by India, and the top\ngenerations rarely reflect surroundings from all other countries (average score\nless than 3 out of 5). Specifying the country names in the input increases the\nrepresentativeness by 1.44 points on average for DALL.E 2 and 0.75 for Stable\nDiffusion, however, the overall scores for many countries still remain low,\nhighlighting the need for future models to be more geographically inclusive.\nLastly, we examine the feasibility of quantifying the geographical\nrepresentativeness of generated images without conducting user studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1\">Abhipsa Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_R/0/1/0/all/0/1\">R. Venkatesh Babu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruthi_D/0/1/0/all/0/1\">Danish Pruthi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization. (arXiv:2305.11095v1 [eess.AS])","link":"http://arxiv.org/abs/2305.11095","description":"<p>We investigate the emergent abilities of the recently proposed web-scale\nspeech model Whisper, by adapting it to unseen tasks with prompt engineering.\nWe selected three tasks: audio-visual speech recognition (AVSR), code-switched\nspeech recognition (CS-ASR), and speech translation (ST) on unseen language\npairs. We design task-specific prompts, by either leveraging another\nlarge-scale model, or simply manipulating the special tokens in the default\nprompts. Experiments show that compared to the default prompts, our proposed\nprompts improve performance by 10% to 45% on the three zero-shot tasks, and\neven outperform SotA supervised models on some datasets. In addition, our\nexperiments reveal many interesting properties of Whisper, including its\nrobustness to prompts, bias on accents, and the multilingual understanding in\nits latent space. Code is available at\nhttps://github.com/jasonppy/PromptingWhisper\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_P/0/1/0/all/0/1\">Puyuan Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoTriggER: Label-Efficient and Robust Named Entity Recognition with Auxiliary Trigger Extraction. (arXiv:2109.04726v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04726","description":"<p>Deep neural models for named entity recognition (NER) have shown impressive\nresults in overcoming label scarcity and generalizing to unseen entities by\nleveraging distant supervision and auxiliary information such as explanations.\nHowever, the costs of acquiring such additional information are generally\nprohibitive. In this paper, we present a novel two-stage framework\n(AutoTriggER) to improve NER performance by automatically generating and\nleveraging ``entity triggers'' which are human-readable cues in the text that\nhelp guide the model to make better decisions. Our framework leverages post-hoc\nexplanation to generate rationales and strengthens a model's prior knowledge\nusing an embedding interpolation technique. This approach allows models to\nexploit triggers to infer entity boundaries and types instead of solely\nmemorizing the entity words themselves. Through experiments on three\nwell-studied NER datasets, AutoTriggER shows strong label-efficiency, is\ncapable of generalizing to unseen entities, and outperforms the RoBERTa-CRF\nbaseline by nearly 0.5 F1 points on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong-Ho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Selvam_R/0/1/0/all/0/1\">Ravi Kiran Selvam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarwar_S/0/1/0/all/0/1\">Sheikh Muhammad Sarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morstatter_F/0/1/0/all/0/1\">Fred Morstatter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boschee_E/0/1/0/all/0/1\">Elizabeth Boschee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allan_J/0/1/0/all/0/1\">James Allan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation. (arXiv:2202.13047v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13047","description":"<p>Crowdsourced dialogue corpora are usually limited in scale and topic coverage\ndue to the expensive cost of data curation. This would hinder the\ngeneralization of downstream dialogue models to open-domain topics. In this\nwork, we leverage large language models for dialogue augmentation in the task\nof emotional support conversation (ESC). By treating dialogue augmentation as a\ndialogue completion task, we prompt a fine-tuned language model to complete\nfull dialogues from available dialogue posts of various topics, which are then\npostprocessed based on heuristics. Applying this approach, we construct AugESC,\nan augmented dataset for the ESC task, which largely extends the scale and\ntopic coverage of the crowdsourced ESConv corpus. Through comprehensive human\nevaluation, we demonstrate that our approach is superior to strong baselines of\ndialogue augmentation and that AugESC has comparable dialogue quality to the\ncrowdsourced corpus. We also conduct human interactive evaluation and prove\nthat post-training on AugESC improves downstream dialogue models'\ngeneralization ability to open-domain topics. These results suggest the utility\nof AugESC and highlight the potential of large language models in improving\ndata-scarce dialogue generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chujie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabour_S/0/1/0/all/0/1\">Sahand Sabour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jiaxin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing the factors affecting usefulness of Self-Supervised Pre-trained Representations for Speech Recognition. (arXiv:2203.16973v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16973","description":"<p>Self-supervised learning (SSL) to learn high-level speech representations has\nbeen a popular approach to building Automatic Speech Recognition (ASR) systems\nin low-resource settings. However, the common assumption made in literature is\nthat a considerable amount of unlabeled data is available for the same domain\nor language that can be leveraged for SSL pre-training, which we acknowledge is\nnot feasible in a real-world setting. In this paper, as part of the Interspeech\nGram Vaani ASR challenge, we try to study the effect of domain, language,\ndataset size, and other aspects of our upstream pre-training SSL data on the\nfinal performance low-resource downstream ASR task. We also build on the\ncontinued pre-training paradigm to study the effect of prior knowledge\npossessed by models trained using SSL. Extensive experiments and studies reveal\nthat the performance of ASR systems is susceptible to the data used for SSL\npre-training. Their performance improves with an increase in similarity and\nvolume of pre-training data. We believe our work will be helpful to the speech\ncommunity in building better ASR systems in low-resource settings and steer\nresearch towards improving generalization in SSL-based pre-training for speech\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seth_A/0/1/0/all/0/1\">Ashish Seth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_L/0/1/0/all/0/1\">Lodagala V S V Durga Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PALBERT: Teaching ALBERT to Ponder. (arXiv:2204.03276v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.03276","description":"<p>Currently, pre-trained models can be considered the default choice for a wide\nrange of NLP tasks. Despite their SoTA results, there is practical evidence\nthat these models may require a different number of computing layers for\ndifferent input sequences, since evaluating all layers leads to overconfidence\nin wrong predictions (namely overthinking). This problem can potentially be\nsolved by implementing adaptive computation time approaches, which were first\ndesigned to improve inference speed. Recently proposed PonderNet may be a\npromising solution for performing an early exit by treating the exit layer's\nindex as a latent variable. However, the originally proposed exit criterion,\nrelying on sampling from trained posterior distribution on the probability of\nexiting from the $i$-th layer, introduces major variance in exit layer indices,\nsignificantly reducing the resulting model's performance. In this paper, we\npropose improving PonderNet with a novel deterministic Q-exit criterion and a\nrevisited model architecture. We adapted the proposed mechanism to ALBERT and\nRoBERTa and compared it with recent methods for performing an early exit. We\nobserved that the proposed changes can be considered significant improvements\non the original PonderNet architecture and outperform PABEE on a wide range of\nGLUE tasks. In addition, we also performed an in-depth ablation study of the\nproposed architecture to further understand Lambda layers and their\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balagansky_N/0/1/0/all/0/1\">Nikita Balagansky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavrilov_D/0/1/0/all/0/1\">Daniil Gavrilov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation. (arXiv:2204.06674v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06674","description":"<p>Recent improvements in KG-to-text generation are due to additional auxiliary\npre-training tasks designed to give the fine-tune task a boost in performance.\nThese tasks require extensive computational resources while only suggesting\nmarginal improvements. Here, we demonstrate that by fusing graph-aware elements\ninto existing pre-trained language models, we are able to outperform\nstate-of-the-art models and close the gap imposed by additional pre-training\ntasks. We do so by proposing a mask structure to capture neighborhood\ninformation and a novel type encoder that adds a bias to the graph-attention\nweights depending on the connection type. Experiments on two KG-to-text\nbenchmark datasets show our models are competitive while involving fewer\nparameters and no additional pre-training tasks. By formulating the problem as\na framework, we can interchange the various proposed components and begin\ninterpreting KG-to-text generative models based on the topological and type\ninformation found in a graph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colas_A/0/1/0/all/0/1\">Anthony Colas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvandipour_M/0/1/0/all/0/1\">Mehrdad Alvandipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daisy Zhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Study on Transformer Configuration and Training Objective. (arXiv:2205.10505v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.10505","description":"<p>Transformer-based models have delivered impressive results on many tasks,\nparticularly vision and language tasks. In many model training situations,\nconventional configurations are typically adopted. For example, we often set\nthe base model with hidden dimensions (i.e. model width) to be 768 and the\nnumber of transformer layers (i.e. model depth) to be 12. In this paper, we\nrevisit these conventional configurations. Through theoretical analysis and\nexperimental evaluation, we show that the masked autoencoder is effective in\nalleviating the over-smoothing issue in deep transformer training. Based on\nthis finding, we propose Bamboo, an idea of using deeper and narrower\ntransformer configurations, for masked autoencoder training. On ImageNet, with\nsuch a simple change in configuration, re-designed model achieves 87.1% top-1\naccuracy and outperforms SoTA models like MAE and BEiT. On language tasks,\nre-designed model outperforms BERT with default setting by 1.1 points on\naverage, on GLUE datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianghai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zangwei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaoxin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yongming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ranking-Enhanced Unsupervised Sentence Representation Learning. (arXiv:2209.04333v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.04333","description":"<p>Unsupervised sentence representation learning has progressed through\ncontrastive learning and data augmentation methods such as dropout masking.\nDespite this progress, sentence encoders are still limited to using only an\ninput sentence when predicting its semantic vector. In this work, we show that\nthe semantic meaning of a sentence is also determined by nearest-neighbor\nsentences that are similar to the input sentence. Based on this finding, we\npropose a novel unsupervised sentence encoder, RankEncoder. RankEncoder\npredicts the semantic vector of an input sentence by leveraging its\nrelationship with other sentences in an external corpus, as well as the input\nsentence itself. We evaluate RankEncoder on semantic textual benchmark\ndatasets. From the experimental results, we verify that 1) RankEncoder achieves\n80.07% Spearman's correlation, a 1.1% absolute improvement compared to the\nprevious state-of-the-art performance, 2) RankEncoder is universally applicable\nto existing unsupervised sentence embedding methods, and 3) RankEncoder is\nspecifically effective for predicting the similarity scores of similar sentence\npairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seonwoo_Y/0/1/0/all/0/1\">Yeon Seonwoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_C/0/1/0/all/0/1\">Changmin Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1\">Sajal Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Puyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Intersection of Context-Free and Regular Languages. (arXiv:2209.06809v2 [cs.FL] UPDATED)","link":"http://arxiv.org/abs/2209.06809","description":"<p>The Bar-Hillel construction is a classic result in formal language theory. It\nshows, by a simple construction, that the intersection of a context-free\nlanguage and a regular language is itself context-free. In the construction,\nthe regular language is specified by a finite-state automaton. However, neither\nthe original construction (Bar-Hillel et al., 1961) nor its weighted extension\n(Nederhof and Satta, 2003) can handle finite-state automata with\n$\\varepsilon$-arcs. While it is possible to remove $\\varepsilon$-arcs from a\nfinite-state automaton efficiently without modifying the language, such an\noperation modifies the automaton's set of paths. We give a construction that\ngeneralizes the Bar-Hillel in the case where the desired automaton has\n$\\varepsilon$-arcs, and further prove that our generalized construction leads\nto a grammar that encodes the structure of both the input automaton and grammar\nwhile retaining the asymptotic size of the original construction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pasti_C/0/1/0/all/0/1\">Clemente Pasti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Opedal_A/0/1/0/all/0/1\">Andreas Opedal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vieira_T/0/1/0/all/0/1\">Tim Vieira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisner_J/0/1/0/all/0/1\">Jason Eisner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Generation of Grounded Logical Explanations in a Neuro-Symbolic Expert System. (arXiv:2209.07662v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.07662","description":"<p>We propose an approach for systematic reasoning that produces human\ninterpretable proof trees grounded in a factbase. Our approach evokes classic\nProlog-based inference engines, where we replace handcrafted rules by combining\nneural language modeling, guided generation, and semiparametric dense\nretrieval. We demonstrate this approach through a novel system, NELLIE, which\ndynamically instantiates interpretable inference rules that capture and score\nentailment (de)compositions over natural language statements. This leads to\nstrong performance, as shown in the scientific reasoning domain, while also\nproducing reasoning traces showing how answers derive logically from the\ncomposition of human-verified facts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weir_N/0/1/0/all/0/1\">Nathaniel Weir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REV: Information-Theoretic Evaluation of Free-Text Rationales. (arXiv:2210.04982v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04982","description":"<p>Generating free-text rationales is a promising step towards explainable NLP,\nyet evaluating such rationales remains a challenge. Existing metrics have\nmostly focused on measuring the association between the rationale and a given\nlabel. We argue that an ideal metric should focus on the new information\nuniquely provided in the rationale that is otherwise not provided in the input\nor the label. We investigate this research problem from an\ninformation-theoretic perspective using conditional V-information (Hewitt et\nal., 2021). More concretely, we propose a metric called REV (Rationale\nEvaluation with conditional V-information), to quantify the amount of new,\nlabel-relevant information in a rationale beyond the information already\navailable in the input or the label. Experiments across four benchmarks with\nreasoning tasks, including chain-of-thought, demonstrate the effectiveness of\nREV in evaluating rationale-label pairs, compared to existing metrics. We\nfurther demonstrate REV is consistent with human judgments on rationale\nevaluations and provides more sensitive measurements of new information in\nfree-text rationales. When used alongside traditional performance metrics, REV\nprovides deeper insights into models' reasoning and prediction processes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PeerDA: Data Augmentation via Modeling Peer Relation for Span Identification Tasks. (arXiv:2210.08855v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08855","description":"<p>Span identification aims at identifying specific text spans from text input\nand classifying them into pre-defined categories. Different from previous works\nthat merely leverage the Subordinate (SUB) relation (i.e. if a span is an\ninstance of a certain category) to train models, this paper for the first time\nexplores the Peer (PR) relation, which indicates that two spans are instances\nof the same category and share similar features. Specifically, a novel Peer\nData Augmentation (PeerDA) approach is proposed which employs span pairs with\nthe PR relation as the augmentation data for training. PeerDA has two unique\nadvantages: (1) There are a large number of PR span pairs for augmenting the\ntraining data. (2) The augmented data can prevent the trained model from\nover-fitting the superficial span-category mapping by pushing the model to\nleverage the span semantics. Experimental results on ten datasets over four\ndiverse tasks across seven domains demonstrate the effectiveness of PeerDA.\nNotably, PeerDA achieves state-of-the-art results on six of them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpeechBlender: Speech Augmentation Framework for Mispronunciation Data Generation. (arXiv:2211.00923v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2211.00923","description":"<p>The lack of labeled second language (L2) speech data is a major challenge in\ndesigning mispronunciation detection models. We introduce SpeechBlender - a\nfine-grained data augmentation pipeline for generating mispronunciation errors\nto overcome such data scarcity. The SpeechBlender utilizes varieties of masks\nto target different regions of phonetic units, and use the mixing factors to\nlinearly interpolate raw speech signals while augmenting pronunciation. The\nmasks facilitate smooth blending of the signals, generating more effective\nsamples than the `Cut/Paste' method. Our proposed technique achieves\nstate-of-the-art results, with Speechocean762, on ASR dependent\nmispronunciation detection models at phoneme level, with a 2.0% gain in Pearson\nCorrelation Coefficient (PCC) compared to the previous state-of-the-art [1].\nAdditionally, we demonstrate a 5.0% improvement at the phoneme level compared\nto our baseline. We also observed a 4.6% increase in F1-score with Arabic\nAraVoiceL2 testset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kheir_Y/0/1/0/all/0/1\">Yassine El Kheir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afzal_S/0/1/0/all/0/1\">Shazia Afzal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAST: Multiscale Audio Spectrogram Transformers. (arXiv:2211.01515v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2211.01515","description":"<p>We present Multiscale Audio Spectrogram Transformer (MAST) for audio\nclassification, which brings the concept of multiscale feature hierarchies to\nthe Audio Spectrogram Transformer (AST). Given an input audio spectrogram, we\nfirst patchify and project it into an initial temporal resolution and embedding\ndimension, post which the multiple stages in MAST progressively expand the\nembedding dimension while reducing the temporal resolution of the input. We use\na pyramid structure that allows early layers of MAST operating at a high\ntemporal resolution but low embedding space to model simple low-level acoustic\ninformation and deeper temporally coarse layers to model high-level acoustic\ninformation with high-dimensional embeddings. We also extend our approach to\npresent a new Self-Supervised Learning (SSL) method called SS-MAST, which\ncalculates a symmetric contrastive loss between latent representations from a\nstudent and a teacher encoder, leveraging patch-drop, a novel audio\naugmentation approach that we introduce. In practice, MAST significantly\noutperforms AST by an average accuracy of 3.4% across 8 speech and non-speech\ntasks from the LAPE Benchmark, achieving state-of-the-art results on keyword\nspotting in Speech Commands. Additionally, our proposed SS-MAST achieves an\nabsolute average improvement of 2.6% over the previously proposed SSAST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seth_A/0/1/0/all/0/1\">Ashish Seth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SLICER: Learning universal audio representations using low-resource self-supervised pre-training. (arXiv:2211.01519v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2211.01519","description":"<p>We present a new Self-Supervised Learning (SSL) approach to pre-train\nencoders on unlabeled audio data that reduces the need for large amounts of\nlabeled data for audio and speech classification. Our primary aim is to learn\naudio representations that can generalize across a large variety of speech and\nnon-speech tasks in a low-resource un-labeled audio pre-training setting.\nInspired by the recent success of clustering and contrasting learning paradigms\nfor SSL-based speech representation learning, we propose SLICER (Symmetrical\nLearning of Instance and Cluster-level Efficient Representations), which brings\ntogether the best of both clustering and contrasting learning paradigms. We use\na symmetric loss between latent representations from student and teacher\nencoders and simultaneously solve instance and cluster-level contrastive\nlearning tasks. We obtain cluster representations online by just projecting the\ninput spectrogram into an output subspace with dimensions equal to the number\nof clusters. In addition, we propose a novel mel-spectrogram augmentation\nprocedure, k-mix, based on mixup, which does not require labels and aids\nunsupervised representation learning for audio. Overall, SLICER achieves\nstate-of-the-art results on the LAPE Benchmark \\cite{9868132}, significantly\noutperforming DeLoRes-M and other prior approaches, which are pre-trained on\n$10\\times$ larger of unsupervised data. We will make all our codes available on\nGitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Seth_A/0/1/0/all/0/1\">Ashish Seth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deanthropomorphising NLP: Can a Language Model Be Conscious?. (arXiv:2211.11483v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.11483","description":"<p>This work is intended as a voice in the discussion over the recent claims\nthat LaMDA, a pretrained language model based on the Transformer model\narchitecture, is sentient. This claim, if confirmed, would have serious\nramifications in the Natural Language Processing (NLP) community due to\nwide-spread use of similar models. However, here we take the position that such\na language model cannot be sentient, or conscious, and that LaMDA in particular\nexhibits no advances over other similar models that would qualify it. We\njustify this by analysing the Transformer architecture through Integrated\nInformation Theory. We see the claims of consciousness as part of a wider\ntendency to use anthropomorphic language in NLP reporting. Regardless of the\nveracity of the claims, we consider this an opportune moment to take stock of\nprogress in language modelling and consider the ethical implications of the\ntask. In order to make this work helpful for readers outside the NLP community,\nwe also present the necessary background in language modelling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shardlow_M/0/1/0/all/0/1\">Matthew Shardlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Przybyla_P/0/1/0/all/0/1\">Piotr Przyby&#x142;a</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What learning algorithm is in-context learning? Investigations with linear models. (arXiv:2211.15661v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2211.15661","description":"<p>Neural sequence models, especially transformers, exhibit a remarkable\ncapacity for in-context learning. They can construct new predictors from\nsequences of labeled examples $(x, f(x))$ presented in the input without\nfurther parameter updates. We investigate the hypothesis that transformer-based\nin-context learners implement standard learning algorithms implicitly, by\nencoding smaller models in their activations, and updating these implicit\nmodels as new examples appear in the context. Using linear regression as a\nprototypical problem, we offer three sources of evidence for this hypothesis.\nFirst, we prove by construction that transformers can implement learning\nalgorithms for linear models based on gradient descent and closed-form ridge\nregression. Second, we show that trained in-context learners closely match the\npredictors computed by gradient descent, ridge regression, and exact\nleast-squares regression, transitioning between different predictors as\ntransformer depth and dataset noise vary, and converging to Bayesian estimators\nfor large widths and depths. Third, we present preliminary evidence that\nin-context learners share algorithmic features with these predictors: learners'\nlate layers non-linearly encode weight vectors and moment matrices. These\nresults suggest that in-context learning is understandable in algorithmic\nterms, and that (at least in the linear case) learners may rediscover standard\nestimation algorithms. Code and reference implementations are released at\nhttps://github.com/ekinakyurek/google-research/blob/master/incontext.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akyurek_E/0/1/0/all/0/1\">Ekin Aky&#xfc;rek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Reasoning Capabilities into Smaller Language Models. (arXiv:2212.00193v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2212.00193","description":"<p>Step-by-step reasoning approaches like chain of thought (CoT) have proved to\nbe very effective in inducing reasoning capabilities in large language models.\nHowever, the success of the CoT approach is fundamentally tied to the model\nsize, and billion parameter-scale models are often needed to get CoT to work.\nIn this paper, we propose a knowledge distillation approach that leverages the\nstep-by-step CoT reasoning capabilities of larger models and distills these\nabilities into smaller models.\n</p>\n<p>In this work, we propose an alternative reasoning scheme, Socratic CoT, that\nlearns a decomposition of the original problem into a sequence of subproblems\nand uses it to guide the intermediate reasoning steps. We use Socratic CoT to\ntrain a combination of two small distilled models: a problem decomposer and a\nsubproblem solver. In practice, given a new problem, the two distilled models\nwork in sync to decompose and solve complex problems. On multiple reasoning\ndatasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies\nboosts the performance of smaller models over 70% compared to the baselines.\nFinally, we investigate when Socratic CoT is an effective alternative to CoT,\ndemonstrating cases where a much smaller model (GPT-2 large) can outperform a\n10X larger model (GPT-3 6B). Our code is available here:\nhttps://github.com/kumar-shridhar/Distiiling-LM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shridhar_K/0/1/0/all/0/1\">Kumar Shridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolfo_A/0/1/0/all/0/1\">Alessandro Stolfo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DC-MBR: Distributional Cooling for Minimum Bayesian Risk Decoding. (arXiv:2212.04205v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.04205","description":"<p>Minimum Bayesian Risk Decoding (MBR) emerges as a promising decoding\nalgorithm in Neural Machine Translation. However, MBR performs poorly with\nlabel smoothing, which is surprising as label smoothing provides decent\nimprovement with beam search and improves generality in various tasks. In this\nwork, we show that the issue arises from the un-consistency of label smoothing\non the token-level and sequence-level distributions. We demonstrate that even\nthough label smoothing only causes a slight change in the token-level, the\nsequence-level distribution is highly skewed. We coin the issue\n\\emph{autoregressive over-smoothness}. To address this issue, we propose a\nsimple and effective method, Distributional Cooling MBR (DC-MBR), which\nmanipulates the entropy of output distributions by tuning down the Softmax\ntemperature. We theoretically prove the equivalence between pre-tuning label\nsmoothing factor and distributional cooling. Extensive experiments on NMT\nbenchmarks validate that distributional cooling improves MBR in various\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jianhao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Clozing to Comprehending: Retrofitting Pre-trained Masked Language Model to Pre-trained Machine Reader. (arXiv:2212.04755v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.04755","description":"<p>We present Pre-trained Machine Reader (PMR), a novel method for retrofitting\npre-trained masked language models (MLMs) to pre-trained machine reading\ncomprehension (MRC) models without acquiring labeled data. PMR can resolve the\ndiscrepancy between model pre-training and downstream fine-tuning of existing\nMLMs. To build the proposed PMR, we constructed a large volume of\ngeneral-purpose and high-quality MRC-style training data by using Wikipedia\nhyperlinks and designed a Wiki Anchor Extraction task to guide the MRC-style\npre-training. Apart from its simplicity, PMR effectively solves extraction\ntasks, such as Extractive Question Answering and Named Entity Recognition. PMR\nshows tremendous improvements over existing approaches, especially in\nlow-resource scenarios. When applied to the sequence classification task in the\nMRC formulation, PMR enables the extraction of high-quality rationales to\nexplain the classification process, thereby providing greater prediction\nexplainability. PMR also has the potential to serve as a unified model for\ntackling various extraction and classification tasks in the MRC formulation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Meng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gradient-based Intra-attention Pruning on Pre-trained Language Models. (arXiv:2212.07634v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.07634","description":"<p>Pre-trained language models achieve superior performance but are\ncomputationally expensive. Techniques such as pruning and knowledge\ndistillation have been developed to reduce their sizes and latencies. In this\nwork, we propose a structured pruning method GRAIN (Gradient-based\nIntra-attention pruning), which performs task-specific pruning with knowledge\ndistillation and yields highly effective models. Different from common\napproaches that prune each attention head as a whole, GRAIN inspects and prunes\nintra-attention structures, which greatly expands the structure search space\nand enables more flexible models. We also propose a gradient separation\nstrategy that reduces the interference of distillation on pruning for a better\ncombination of the two approaches. Experiments on GLUE, SQuAD, and CoNLL 2003\nshow that GRAIN notably outperforms other methods, especially in the high\nsparsity regime, and achieves $6\\sim7\\times$ speedups while maintaining\n$93\\%\\sim99\\%$ performance. Under extreme compression where only $3\\%$\ntransformer weights remain, the pruned model is still competitive compared to\nlarger models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yiming Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shijin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClarifyDelphi: Reinforced Clarification Questions with Defeasibility Rewards for Social and Moral Situations. (arXiv:2212.10409v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10409","description":"<p>Context is everything, even in commonsense moral reasoning. Changing contexts\ncan flip the moral judgment of an action; \"Lying to a friend\" is wrong in\ngeneral, but may be morally acceptable if it is intended to protect their life.\n</p>\n<p>We present ClarifyDelphi, an interactive system that learns to ask\nclarification questions (e.g., why did you lie to your friend?) in order to\nelicit additional salient contexts of a social or moral situation. We posit\nthat questions whose potential answers lead to diverging moral judgments are\nthe most informative. Thus, we propose a reinforcement learning framework with\na defeasibility reward that aims to maximize the divergence between moral\njudgments of hypothetical answers to a question. Human evaluation demonstrates\nthat our system generates more relevant, informative and defeasible questions\ncompared to competitive baselines. Our work is ultimately inspired by studies\nin cognitive science that have investigated the flexibility in moral cognition\n(i.e., the diverse contexts in which moral rules can be bent), and we hope that\nresearch in this direction can assist both cognitive and computational\ninvestigations of moral judgments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pyatkin_V/0/1/0/all/0/1\">Valentina Pyatkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-Agnostic Molecular Generation with Self-feedback. (arXiv:2301.11259v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2301.11259","description":"<p>The generation of molecules with desired properties has gained tremendous\npopularity, revolutionizing the way scientists design molecular structures and\nproviding valuable support for chemical and drug design. However, despite the\npotential of language models in molecule generation, they face numerous\nchallenges such as the generation of syntactically or chemically flawed\nmolecules, narrow domain focus, and limitations in creating diverse and\ndirectionally feasible molecules due to a dearth of annotated data or external\nmolecular databases. To this end, we introduce MolGen, a pre-trained molecular\nlanguage model tailored specifically for molecule generation. MolGen acquires\nintrinsic structural and grammatical insights by reconstructing over 100\nmillion molecular SELFIES, while facilitating knowledge transfer between\ndifferent domains through domain-agnostic molecular prefix tuning. Moreover, we\npresent a self-feedback paradigm that inspires the pre-trained model to align\nwith the ultimate goal of producing molecules with desirable properties.\nExtensive experiments demonstrate that MolGen achieves superior performance on\nwell-known molecule generation benchmarks. Further analysis shows that MolGen\ncan accurately capture molecule distributions, implicitly learn their\nstructural characteristics, and efficiently explore chemical space. The\npre-trained model, codes, and datasets are publicly available for future\nresearch at https://github.com/zjunlp/MolGen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiaohui Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Case-Based Reasoning with Language Models for Classification of Logical Fallacies. (arXiv:2301.11879v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2301.11879","description":"<p>The ease and speed of spreading misinformation and propaganda on the Web\nmotivate the need to develop trustworthy technology for detecting fallacies in\nnatural language arguments. However, state-of-the-art language modeling methods\nexhibit a lack of robustness on tasks like logical fallacy classification that\nrequire complex reasoning. In this paper, we propose a Case-Based Reasoning\nmethod that classifies new cases of logical fallacy by language-modeling-driven\nretrieval and adaptation of historical cases. We design four complementary\nstrategies to enrich input representation for our model, based on external\ninformation about goals, explanations, counterarguments, and argument\nstructure. Our experiments in in-domain and out-of-domain settings indicate\nthat Case-Based Reasoning improves the accuracy and generalizability of\nlanguage models. Our ablation studies suggest that representations of similar\ncases have a strong impact on the model performance, that models perform well\nwith fewer retrieved cases, and that the size of the case database has a\nnegligible effect on the performance. Finally, we dive deeper into the\nrelationship between the properties of the retrieved cases and the model\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sourati_Z/0/1/0/all/0/1\">Zhivar Sourati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandlin_H/0/1/0/all/0/1\">H&#xf4;ng-&#xc2;n Sandlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mermoud_A/0/1/0/all/0/1\">Alain Mermoud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unifying Molecular and Textual Representations via Multi-task Language Modelling. (arXiv:2301.12586v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2301.12586","description":"<p>The recent advances in neural language models have also been successfully\napplied to the field of chemistry, offering generative solutions for classical\nproblems in molecular design and synthesis planning. These new methods have the\npotential to fuel a new era of data-driven automation in scientific discovery.\nHowever, specialized models are still typically required for each task, leading\nto the need for problem-specific fine-tuning and neglecting task\ninterrelations. The main obstacle in this field is the lack of a unified\nrepresentation between natural language and chemical representations,\ncomplicating and limiting human-machine interaction. Here, we propose the first\nmulti-domain, multi-task language model that can solve a wide range of tasks in\nboth the chemical and natural language domains. Our model can handle chemical\nand natural language concurrently, without requiring expensive pre-training on\nsingle domains or task-specific models. Interestingly, sharing weights across\ndomains remarkably improves our model when benchmarked against state-of-the-art\nbaselines on single-domain and cross-domain tasks. In particular, sharing\ninformation across domains and tasks gives rise to large improvements in\ncross-domain tasks, the magnitude of which increase with scale, as measured by\nmore than a dozen of relevant metrics. Our work suggests that such models can\nrobustly and efficiently accelerate discovery in physical sciences by\nsuperseding problem-specific fine-tuning and enhancing human-model\ninteractions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Christofidellis_D/0/1/0/all/0/1\">Dimitrios Christofidellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannone_G/0/1/0/all/0/1\">Giorgio Giannone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Born_J/0/1/0/all/0/1\">Jannis Born</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1\">Ole Winther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laino_T/0/1/0/all/0/1\">Teodoro Laino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manica_M/0/1/0/all/0/1\">Matteo Manica</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Role of Semantic Parsing in Understanding Procedural Text. (arXiv:2302.06829v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.06829","description":"<p>In this paper, we investigate whether symbolic semantic representations,\nextracted from deep semantic parsers, can help reasoning over the states of\ninvolved entities in a procedural text. We consider a deep semantic\nparser~(TRIPS) and semantic role labeling as two sources of semantic parsing\nknowledge. First, we propose PROPOLIS, a symbolic parsing-based procedural\nreasoning framework. Second, we integrate semantic parsing information into\nstate-of-the-art neural models to conduct procedural reasoning. Our experiments\nindicate that explicitly incorporating such semantic knowledge improves\nprocedural understanding. This paper presents new metrics for evaluating\nprocedural reasoning tasks that clarify the challenges and identify differences\namong neural, symbolic, and integrated models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faghihi_H/0/1/0/all/0/1\">Hossein Rajaby Faghihi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kordjamshidi_P/0/1/0/all/0/1\">Parisa Kordjamshidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_C/0/1/0/all/0/1\">Choh Man Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allen_J/0/1/0/all/0/1\">James Allen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks. (arXiv:2304.01665v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01665","description":"<p>Language models (LMs) proficiency in handling deterministic symbolic\nreasoning and rule-based tasks remains limited due to their dependency implicit\nlearning on textual data. To enable fully rule comprehension ability, we\nexplore how to incorporate compiled neural networks (CoNNs) which weight is\nspecially designed into the architecture of LMs, to achieve high accuracy and\nrobust performance. CoNNs are transformer-based neural networks that execute\nrules through artificially generated attention weights. Our method, which call\n\"Neural Comprehension\", by incorporating CoNN modules into the LM, the\nframework effectively tackles rule-intensive challenges. Our experiments on\nsymbolic reasoning tasks and real-world arithmetic reasoning tasks demonstrate\nthe superior performance of our method compared to existing techniques.\nFurthermore, our LM achieves flawless execution on symbolic operations tasks,\nhighlighting the potential of our method in enabling LMs to possess true\nsymbolic comprehension capabilities. Our code is publicly available at:\nhttps://github.com/WENGSYX/Neural-Comprehension.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Minjun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shizhu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01890","description":"<p>We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for\nthe countries of Brazil, Germany, India and Kenya, to aid training and\ninterpretability of models. We demonstrate how our lexicon can be used to\ninterpret model predictions, showing that models developed to classify extreme\nspeech rely heavily on target words when making predictions. Further, we\npropose a method to aid shot selection for training in low-resource settings\nvia HATELEXICON. In few-shot learning, the selection of shots is of paramount\nimportance to model performance. In our work, we simulate a few-shot setting\nfor German and Hindi, using HASOC data for training and the Multilingual\nHateCheck (MHC) as a benchmark. We show that selecting shots based on our\nlexicon leads to models performing better on MHC than models trained on shots\nsampled randomly. Thus, when given only a few training examples, using our\nlexicon to select shots containing more sociocultural information leads to\nbetter few-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maronikolakis_A/0/1/0/all/0/1\">Antonis Maronikolakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koksal_A/0/1/0/all/0/1\">Abdullatif K&#xf6;ksal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-step Jailbreaking Privacy Attacks on ChatGPT. (arXiv:2304.05197v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.05197","description":"<p>With the rapid progress of large language models (LLMs), many downstream NLP\ntasks can be well solved given appropriate prompts. Though model developers and\nresearchers work hard on dialog safety to avoid generating harmful content from\nLLMs, it is still challenging to steer AI-generated content (AIGC) for the\nhuman good. As powerful LLMs are devouring existing text data from various\ndomains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether\nthe private information is included in the training data and what privacy\nthreats can these LLMs and their downstream applications bring. In this paper,\nwe study the privacy threats from OpenAI's ChatGPT and the New Bing enhanced by\nChatGPT and show that application-integrated LLMs may cause new privacy\nthreats. To this end, we conduct extensive experiments to support our claims\nand discuss LLMs' privacy implications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Dadi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingshi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fanpu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading. (arXiv:2304.10784v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.10784","description":"<p>Eye movements during reading offer insights into both the reader's cognitive\nprocesses and the characteristics of the text that is being read. Hence, the\nanalysis of scanpaths in reading have attracted increasing attention across\nfields, ranging from cognitive science over linguistics to computer science. In\nparticular, eye-tracking-while-reading data has been argued to bear the\npotential to make machine-learning-based language models exhibit a more\nhuman-like linguistic behavior. However, one of the main challenges in modeling\nhuman scanpaths in reading is their dual-sequence nature: the words are ordered\nfollowing the grammatical rules of the language, whereas the fixations are\nchronologically ordered. As humans do not strictly read from left-to-right, but\nrather skip or refixate words and regress to previous words, the alignment of\nthe linguistic and the temporal sequence is non-trivial. In this paper, we\ndevelop Eyettention, the first dual-sequence model that simultaneously\nprocesses the sequence of words and the chronological sequence of fixations.\nThe alignment of the two sequences is achieved by a cross-sequence attention\nmechanism. We show that Eyettention outperforms state-of-the-art models in\npredicting scanpaths. We provide an extensive within- and across-data set\nevaluation on different languages. An ablation study and qualitative analysis\nsupport an in-depth understanding of the model's behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shuwen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reich_D/0/1/0/all/0/1\">David R. Reich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasse_P/0/1/0/all/0/1\">Paul Prasse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haller_P/0/1/0/all/0/1\">Patrick Haller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheffer_T/0/1/0/all/0/1\">Tobias Scheffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jager_L/0/1/0/all/0/1\">Lena A. J&#xe4;ger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nondeterministic Stacks in Neural Networks. (arXiv:2304.12955v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.12955","description":"<p>Human language is full of compositional syntactic structures, and although\nneural networks have contributed to groundbreaking improvements in computer\nsystems that process language, widely-used neural network architectures still\nexhibit limitations in their ability to process syntax. To address this issue,\nprior work has proposed adding stack data structures to neural networks,\ndrawing inspiration from theoretical connections between syntax and stacks.\nHowever, these methods employ deterministic stacks that are designed to track\none parse at a time, whereas syntactic ambiguity, which requires a\nnondeterministic stack to parse, is extremely common in language. In this\ndissertation, we remedy this discrepancy by proposing a method of incorporating\nnondeterministic stacks into neural networks. We develop a differentiable data\nstructure that efficiently simulates a nondeterministic pushdown automaton,\nrepresenting an exponential number of computations with a dynamic programming\nalgorithm. We incorporate this module into two predominant architectures:\nrecurrent neural networks (RNNs) and transformers. We show that this raises\ntheir formal recognition power to arbitrary context-free languages, and also\naids training, even on deterministic context-free languages. Empirically,\nneural networks with nondeterministic stacks learn context-free languages much\nmore effectively than prior stack-augmented models, including a language with\ntheoretically maximal parsing difficulty. We also show that an RNN augmented\nwith a nondeterministic stack is capable of surprisingly powerful behavior,\nsuch as learning cross-serial dependencies, a well-known non-context-free\npattern. We demonstrate improvements on natural language modeling and provide\nanalysis on a syntactic generalization benchmark. This work represents an\nimportant step toward building systems that learn to use syntax in more\nhuman-like fashion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DuSell_B/0/1/0/all/0/1\">Brian DuSell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unlimiformer: Long-Range Transformers with Unlimited Length Input. (arXiv:2305.01625v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01625","description":"<p>Since the proposal of transformers, these models have been limited to bounded\ninput lengths, because of their need to attend to every token in the input. In\nthis work, we propose Unlimiformer: a general approach that wraps any existing\npretrained encoder-decoder transformer, and offloads the cross-attention\ncomputation to a single k-nearest-neighbor (kNN) index, while the returned kNN\ndistances are the attention dot-product scores. This kNN index can be kept on\neither the GPU or CPU memory and queried in sub-linear time; this way, we can\nindex practically unlimited input sequences, while every attention head in\nevery decoder layer retrieves its top-k keys, instead of attending to every\nkey. We evaluate Unlimiformer on several long-document and book-summarization\nbenchmarks, showing that it can process even 500k token-long inputs from the\nBookSum dataset, without any input truncation at test time. We demonstrate that\nUnlimiformer improves pretrained models such as BART and Longformer by\nextending them to unlimited inputs without additional learned weights and\nwithout modifying their code. We make our code and models publicly available at\nhttps://github.com/abertsch72/unlimiformer .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bertsch_A/0/1/0/all/0/1\">Amanda Bertsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1\">Uri Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gormley_M/0/1/0/all/0/1\">Matthew R. Gormley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Model Learning for Various Neural Machine Translation. (arXiv:2305.02777v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.02777","description":"<p>Existing neural machine translation (NMT) studies mainly focus on developing\ndataset-specific models based on data from different tasks (e.g., document\ntranslation and chat translation). Although the dataset-specific models have\nachieved impressive performance, it is cumbersome as each dataset demands a\nmodel to be designed, trained, and stored. In this work, we aim to unify these\ntranslation tasks into a more general setting. Specifically, we propose a\n``versatile'' model, i.e., the Unified Model Learning for NMT (UMLNMT) that\nworks with data from different tasks, and can translate well in multiple\nsettings simultaneously, and theoretically it can be as many as possible.\nThrough unified learning, UMLNMT is able to jointly train across multiple\ntasks, implementing intelligent on-demand translation. On seven widely-used\ntranslation tasks, including sentence translation, document translation, and\nchat translation, our UMLNMT results in substantial improvements over\ndataset-specific models with significantly reduced model deployment costs.\nFurthermore, UMLNMT can achieve competitive or better performance than\nstate-of-the-art dataset-specific methods. Human evaluation and in-depth\nanalysis also demonstrate the superiority of our approach on generating diverse\nand high-quality translations. Additionally, we provide a new genre translation\ndataset about famous aphorisms with 186k Chinese-&gt;English sentence pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmented Large Language Models with Parametric Knowledge Guiding. (arXiv:2305.04757v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.04757","description":"<p>Large Language Models (LLMs) have significantly advanced natural language\nprocessing (NLP) with their impressive language understanding and generation\ncapabilities. However, their performance may be suboptimal for domain-specific\ntasks that require specialized knowledge due to limited exposure to the related\ndata. Additionally, the lack of transparency of most state-of-the-art (SOTA)\nLLMs, which can only be accessed via APIs, impedes further fine-tuning with\ndomain custom data. Moreover, providing private data to the LLMs' owner leads\nto data privacy problems. To address these challenges, we propose the novel\nParametric Knowledge Guiding (PKG) framework, which equips LLMs with a\nknowledge-guiding module to access relevant knowledge without altering the\nLLMs' parameters. Our PKG is based on open-source \"white-box\" language models,\nallowing offline memory of any knowledge that LLMs require. We demonstrate that\nour PKG framework can enhance the performance of \"black-box\" LLMs on a range of\ndomain knowledge-intensive tasks that require factual (+7.9%), tabular\n(+11.9%), medical (+3.0%), and multimodal (+8.1%) knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Ziyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Pu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qingwei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Script Knowledge from Large Language Models for Constrained Language Planning. (arXiv:2305.05252v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.05252","description":"<p>In everyday life, humans often plan their actions by following step-by-step\ninstructions in the form of goal-oriented scripts. Previous work has exploited\nlanguage models (LMs) to plan for abstract goals of stereotypical activities\n(e.g., \"make a cake\"), but leaves more specific goals with multi-facet\nconstraints understudied (e.g., \"make a cake for diabetics\"). In this paper, we\ndefine the task of constrained language planning for the first time. We propose\nan overgenerate-then-filter approach to improve large language models (LLMs) on\nthis task, and use it to distill a novel constrained language planning dataset,\nCoScript, which consists of 55,000 scripts. Empirical results demonstrate that\nour method significantly improves the constrained language planning ability of\nLLMs, especially on constraint faithfulness. Furthermore, CoScript is\ndemonstrated to be quite effective in endowing smaller LMs with constrained\nlanguage planning ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Siyu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiangjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Ziquan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1\">Xuyang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Soham Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jankowski_C/0/1/0/all/0/1\">Charles Robert Jankowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Deqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Framework for Designing Foundation Model based Systems. (arXiv:2305.05352v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2305.05352","description":"<p>The recent release of large language model (LLM) based chatbots, such as\nChatGPT, has attracted significant attention on foundation models. It is widely\nbelieved that foundation models will serve as the fundamental building blocks\nfor future AI systems. As foundation models are in their early stages, the\ndesign of foundation model based systems has not yet been systematically\nexplored. There is little understanding about the impact of introducing\nfoundation models in software architecture. Therefore, in this paper, we\npropose a taxonomy of foundation model based systems, which classifies and\ncompares the characteristics of foundation models and design options of\nfoundation model based systems. Our taxonomy comprises three categories:\nfoundation model pretraining and fine-tuning, architecture design of foundation\nmodel based systems, and responsible-AI-by-design. This taxonomy provides\nconcrete guidance for making major design decisions when designing foundation\nmodel based systems and highlights trade-offs arising from design decisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qinghua Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Liming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1\">Zhenchang Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whittle_J/0/1/0/all/0/1\">Jon Whittle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation. (arXiv:2305.07375v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.07375","description":"<p>Causal reasoning ability is crucial for numerous NLP applications. Despite\nthe impressive emerging ability of ChatGPT in various NLP tasks, it is unclear\nhow well ChatGPT performs in causal reasoning. In this paper, we conduct the\nfirst comprehensive evaluation of the ChatGPT's causal reasoning capabilities.\nExperiments show that ChatGPT is not a good causal reasoner, but a good causal\ninterpreter. Besides, ChatGPT has a serious hallucination on causal reasoning,\npossibly due to the reporting biases between causal and non-causal\nrelationships in natural language, as well as ChatGPT's upgrading processes,\nsuch as RLHF. The In-Context Learning (ICL) and Chain-of-Though (COT)\ntechniques can further exacerbate such causal hallucination. Additionally, the\ncausal reasoning ability of ChatGPT is sensitive to the words used to express\nthe causal concept in prompts, and close-ended prompts perform better than\nopen-ended prompts. For events in sentences, ChatGPT excels at capturing\nexplicit causality rather than implicit causality, and performs better in\nsentences with lower event density and smaller lexical distance between events.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jinglong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance Smoothed Contrastive Learning for Unsupervised Sentence Embedding. (arXiv:2305.07424v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.07424","description":"<p>Contrastive learning-based methods, such as unsup-SimCSE, have achieved\nstate-of-the-art (SOTA) performances in learning unsupervised sentence\nembeddings. However, in previous studies, each embedding used for contrastive\nlearning only derived from one sentence instance, and we call these embeddings\ninstance-level embeddings. In other words, each embedding is regarded as a\nunique class of its own, whichmay hurt the generalization performance. In this\nstudy, we propose IS-CSE (instance smoothing contrastive sentence embedding) to\nsmooth the boundaries of embeddings in the feature space. Specifically, we\nretrieve embeddings from a dynamic memory buffer according to the semantic\nsimilarity to get a positive embedding group. Then embeddings in the group are\naggregated by a self-attention operation to produce a smoothed instance\nembedding for further analysis. We evaluate our method on standard semantic\ntext similarity (STS) tasks and achieve an average of 78.30%, 79.47%, 77.73%,\nand 79.42% Spearman's correlation on the base of BERT-base, BERT-large,\nRoBERTa-base, and RoBERTa-large respectively, a 2.05%, 1.06%, 1.16% and 0.52%\nimprovement compared to unsup-SimCSE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junlei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1\">Zhenzhong Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dr. LLaMA: Improving Small Language Models on PubMedQA via Generative Data Augmentation. (arXiv:2305.07804v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.07804","description":"<p>Large Language Models (LLMs) have made remarkable strides in natural language\nprocessing, but their expanding size poses challenges in terms of computational\nexpense and inefficiency. Conversely, Small Language Models (SLMs) are known\nfor their efficiency but often encounter difficulties in tasks with limited\ncapacity and training data, particularly in domain-specific scenarios. In this\npaper, we introduce Dr. LLaMA, a method that improves SLMs in the medical\ndomain through generative data augmentation utilizing LLMs. The objective is to\ndevelop more efficient and capable models tailored for specialized\napplications. Our preliminary results on the PubMedQA dataset demonstrate that\nLLMs effectively refine and diversify existing question-answer pairs, leading\nto improved performance of a significantly smaller model after fine-tuning. The\nbest SLM surpasses few-shot GPT-4 with under 1.6 billion parameters on the\nPubMedQA. Our code and generated data are publicly available to facilitate\nfurther explorations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shangdi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content. (arXiv:2305.07969v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.07969","description":"<p>This paper presents a novel approach for detecting ChatGPT-generated vs.\nhuman-written text using language models. To this end, we first collected and\nreleased a pre-processed dataset named OpenGPTText, which consists of rephrased\ncontent generated using ChatGPT. We then designed, implemented, and trained two\ndifferent models for text classification, using Robustly Optimized BERT\nPretraining Approach (RoBERTa) and Text-to-Text Transfer Transformer (T5),\nrespectively. Our models achieved remarkable results, with an accuracy of over\n97% on the test dataset, as evaluated through various metrics. Furthermore, we\nconducted an interpretability study to showcase our model's ability to extract\nand differentiate key features between human-written and ChatGPT-generated\ntext. Our findings provide important insights into the effective use of\nlanguage models to detect generated text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yutian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hao Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_V/0/1/0/all/0/1\">Vivian Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rita Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Neural Factor Analysis for Disentangling Utterance-level Speech Representations. (arXiv:2305.08099v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2305.08099","description":"<p>Self-supervised learning (SSL) speech models such as wav2vec and HuBERT have\ndemonstrated state-of-the-art performance on automatic speech recognition (ASR)\nand proved to be extremely useful in low label-resource settings. However, the\nsuccess of SSL models has yet to transfer to utterance-level tasks such as\nspeaker, emotion, and language recognition, which still require supervised\nfine-tuning of the SSL models to obtain good performance. We argue that the\nproblem is caused by the lack of disentangled representations and an\nutterance-level learning objective for these tasks. Inspired by how HuBERT uses\nclustering to discover hidden acoustic units, we formulate a factor analysis\n(FA) model that uses the discovered hidden acoustic units to align the SSL\nfeatures. The underlying utterance-level representations are disentangled from\nthe content of speech using probabilistic inference on the aligned features.\nFurthermore, the variational lower bound derived from the FA model provides an\nutterance-level objective, allowing error gradients to be backpropagated to the\nTransformer layers to learn highly discriminative acoustic units. When used in\nconjunction with HuBERT's masked prediction training, our models outperform the\ncurrent best model, WavLM, on all utterance-level non-semantic tasks on the\nSUPERB benchmark with only 20% of labeled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiwei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chenhang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mak_M/0/1/0/all/0/1\">Man-Wai Mak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1\">Youzhi Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Generalize for Cross-domain QA. (arXiv:2305.08208v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08208","description":"<p>There have been growing concerns regarding the out-of-domain generalization\nability of natural language processing (NLP) models, particularly in\nquestion-answering (QA) tasks. Current synthesized data augmentation methods\nfor QA are hampered by increased training costs. To address this issue, we\npropose a novel approach that combines prompting methods and linear probing\nthen fine-tuning strategy, which does not entail additional cost. Our method\nhas been theoretically and empirically shown to be effective in enhancing the\ngeneralization ability of both generative and discriminative models. Our\napproach outperforms state-of-the-art baselines, with an average increase in F1\nscore of 4.5%-7.9%. Furthermore, our method can be easily integrated into any\npre-trained models and offers a promising solution to the under-explored\ncross-domain QA task. We release our source code at GitHub*.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yingjie Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Ruihai Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Fine-Tuning with Layer Pruning on Free-Text Sequence-to-Sequence modeling. (arXiv:2305.08285v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08285","description":"<p>The increasing size of language models raises great research interests in\nparameter-efficient fine-tuning such as LoRA that freezes the pre-trained\nmodel, and injects small-scale trainable parameters for multiple downstream\ntasks (e.g., summarization, question answering and translation). To further\nenhance the efficiency of fine-tuning, we propose a framework that integrates\nLoRA and structured layer pruning. The integrated framework is validated on two\ncreated deidentified medical report summarization datasets based on\nMIMIC-IV-Note and two public medical dialogue datasets. By tuning 0.6%\nparameters of the original model and pruning over 30% Transformer-layers, our\nframework can reduce 50% of GPU memory usage and speed up 100% of the training\nphase, while preserving over 92% generation qualities on free-text\nsequence-to-sequence tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yunqi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuebing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuanyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wensheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization. (arXiv:2305.08503v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08503","description":"<p>Pre-trained language models (PLMs) have accomplished impressive achievements\nin abstractive single-document summarization (SDS). However, such benefits may\nnot be readily extended to muti-document summarization (MDS), where the\ninteractions among documents are more complex. Previous works either design new\narchitectures or new pre-training objectives for MDS, or apply PLMs to MDS\nwithout considering the complex document interactions. While the former does\nnot make full use of previous pre-training efforts and may not generalize well\nacross multiple domains, the latter cannot fully attend to the intricate\nrelationships unique to MDS tasks. In this paper, we enforce hierarchy on both\nthe encoder and decoder and seek to make better use of a PLM to facilitate\nmulti-document interactions for the MDS task. We test our design on 10 MDS\ndatasets across a wide range of domains. Extensive experiments show that our\nproposed method can achieve consistent improvements on all these datasets,\noutperforming the previous best models, and even achieving better or\ncompetitive results as compared to some models with additional MDS pre-training\nor larger model parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chenhui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Liying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_X/0/1/0/all/0/1\">Xuan-Phi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLG Evaluation Metrics Beyond Correlation Analysis: An Empirical Metric Preference Checklist. (arXiv:2305.08566v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08566","description":"<p>In this study, we analyze NLG automatic metrics based on whether human\nevaluation aspect is used as context or objective to compute the metrics: (i)\nTask-agnostic and (ii) Human-aligned. Task-agnostic metrics, such as\nPerplexity, BLEU, BERTScore, are cost-effective and highly adaptable to diverse\nNLG tasks, yet they have a weak correlation with human. Human-aligned metrics\n(CTC, CtrlEval, UniEval) improves correlation level by incorporating desirable\nhuman-like qualities as training objective. However, their effectiveness at\ndiscerning system-level performance and quality of system outputs remain\nunclear.\n</p>\n<p>We present metric preference checklist as a framework to assess the\ndiscriminative power of automatic metrics in three NLG tasks: Text\nSummarization, Dialogue Response Generation, and Controlled Generation. We show\nthat multi-aspect human-aligned metric (UniEval) is not necessarily dominant\nover single-aspect human-aligned metrics (CTC, CtrlEval) and task-agnostic\nmetrics (BLEU, BERTScore), particularly when a disagreement between human\nevaluation aspects is present. We also show particular use cases in which\nautomatic metrics provide a better guidance than human on discriminating\nsystem-level performance. Our proposed framework provides access: (i) for\nverifying whether automatic metrics are faithful to human preference,\nregardless their correlation level to human; and (ii) for scrutinizing the\nstrengths and limitations of NLG systems, which are often obscured by a\nstandard averaging method of evaluation scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nimah_I/0/1/0/all/0/1\">Iftitahu Ni&#x27;mah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1\">Vlado Menkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DarkBERT: A Language Model for the Dark Side of the Internet. (arXiv:2305.08596v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08596","description":"<p>Recent research has suggested that there are clear differences in the\nlanguage used in the Dark Web compared to that of the Surface Web. As studies\non the Dark Web commonly require textual analysis of the domain, language\nmodels specific to the Dark Web may provide valuable insights to researchers.\nIn this work, we introduce DarkBERT, a language model pretrained on Dark Web\ndata. We describe the steps taken to filter and compile the text data used to\ntrain DarkBERT to combat the extreme lexical and structural diversity of the\nDark Web that may be detrimental to building a proper representation of the\ndomain. We evaluate DarkBERT and its vanilla counterpart along with other\nwidely used language models to validate the benefits that a Dark Web domain\nspecific model offers in various use cases. Our evaluations show that DarkBERT\noutperforms current language models and may serve as a valuable resource for\nfuture research on the Dark Web.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Youngjin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_E/0/1/0/all/0/1\">Eugene Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jian Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Jin-Woo Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yongjae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Seungwon Shin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Epsilon Sampling Rocks: Investigating Sampling Strategies for Minimum Bayes Risk Decoding for Machine Translation. (arXiv:2305.09860v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.09860","description":"<p>Recent advances in machine translation (MT) have shown that Minimum Bayes\nRisk (MBR) decoding can be a powerful alternative to beam search decoding,\nespecially when combined with neural-based utility functions. However, the\nperformance of MBR decoding depends heavily on how and how many candidates are\nsampled from the model. In this paper, we explore how different sampling\napproaches for generating candidate lists for MBR decoding affect performance.\nWe evaluate popular sampling approaches, such as ancestral, nucleus, and top-k\nsampling. Based on our insights into their limitations, we experiment with the\nrecently proposed epsilon-sampling approach, which prunes away all tokens with\na probability smaller than epsilon, ensuring that each token in a sample\nreceives a fair probability mass. Through extensive human evaluations, we\ndemonstrate that MBR decoding based on epsilon-sampling significantly\noutperforms not only beam search decoding, but also MBR decoding with all other\ntested sampling methods across four language pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1\">Markus Freitag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghorbani_B/0/1/0/all/0/1\">Behrooz Ghorbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1\">Patrick Fernandes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"I'm fully who I am\": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.09941","description":"<p>Transgender and non-binary (TGNB) individuals disproportionately experience\ndiscrimination and exclusion from daily life. Given the recent popularity and\nadoption of language generation technologies, the potential to further\nmarginalize this population only grows. Although a multitude of NLP fairness\nliterature focuses on illuminating and addressing gender biases, assessing\ngender harms for TGNB identities requires understanding how such identities\nuniquely interact with societal gender norms and how they differ from gender\nbinary-centric perspectives. Such measurement frameworks inherently require\ncentering TGNB voices to help guide the alignment between gender-inclusive NLP\nand whom they are intended to serve. Towards this goal, we ground our work in\nthe TGNB community and existing interdisciplinary literature to assess how the\nsocial reality surrounding experienced marginalization by TGNB persons\ncontributes to and persists within Open Language Generation (OLG). By first\nunderstanding their marginalization stressors, we evaluate (1) misgendering and\n(2) harmful responses to gender disclosure. To do this, we introduce the TANGO\ndataset, comprising of template-based text curated from real-world text within\na TGNB-oriented community. We discover a dominance of binary gender norms\nwithin the models; LLMs least misgendered subjects in generated text when\ntriggered by prompts whose subjects used binary pronouns. Meanwhile,\nmisgendering was most prevalent when triggering generation with singular they\nand neopronouns. When prompted with gender disclosures, LLM text contained\nstigmatizing language and scored most toxic when triggered by TGNB gender\ndisclosure. Our findings warrant further research on how TGNB harms manifest in\nLLMs and serve as a broader case study toward concretely grounding the design\nof gender-inclusive AI in community voices and interdisciplinary literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ovalle_A/0/1/0/all/0/1\">Anaelia Ovalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Palash Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamala_J/0/1/0/all/0/1\">Jwala Dhamala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggers_Z/0/1/0/all/0/1\">Zachary Jaggers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EfficientSCI: Densely Connected Network with Space-time Factorization for Large-scale Video Snapshot Compressive Imaging. (arXiv:2305.10006v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.10006","description":"<p>Video snapshot compressive imaging (SCI) uses a two-dimensional detector to\ncapture consecutive video frames during a single exposure time. Following this,\nan efficient reconstruction algorithm needs to be designed to reconstruct the\ndesired video frames. Although recent deep learning-based state-of-the-art\n(SOTA) reconstruction algorithms have achieved good results in most tasks, they\nstill face the following challenges due to excessive model complexity and GPU\nmemory limitations: 1) these models need high computational cost, and 2) they\nare usually unable to reconstruct large-scale video frames at high compression\nratios. To address these issues, we develop an efficient network for video SCI\nby using dense connections and space-time factorization mechanism within a\nsingle residual block, dubbed EfficientSCI. The EfficientSCI network can well\nestablish spatial-temporal correlation by using convolution in the spatial\ndomain and Transformer in the temporal domain, respectively. We are the first\ntime to show that an UHD color video with high compression ratio can be\nreconstructed from a snapshot 2D measurement using a single end-to-end deep\nlearning model with PSNR above 32 dB. Extensive results on both simulation and\nreal data show that our method significantly outperforms all previous SOTA\nalgorithms with better real-time performance. The code is at\nhttps://github.com/ucaswangls/EfficientSCI.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lishun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Miao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MemoryBank: Enhancing Large Language Models with Long-Term Memory. (arXiv:2305.10250v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10250","description":"<p>Revolutionary advancements in Large Language Models have drastically reshaped\nour interactions with artificial intelligence systems. Despite this, a notable\nhindrance remains-the deficiency of a long-term memory mechanism within these\nmodels. This shortfall becomes increasingly evident in situations demanding\nsustained interaction, such as personal companion systems and psychological\ncounseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored\nfor LLMs. MemoryBank enables the models to summon relevant memories,\ncontinually evolve through continuous memory updates, comprehend, and adapt to\na user personality by synthesizing information from past interactions. To mimic\nanthropomorphic behaviors and selectively preserve memory, MemoryBank\nincorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting\nCurve theory, which permits the AI to forget and reinforce memory based on time\nelapsed and the relative significance of the memory, thereby offering a\nhuman-like memory mechanism. MemoryBank is versatile in accommodating both\nclosed-source models like ChatGPT and open-source models like ChatGLM. We\nexemplify application of MemoryBank through the creation of an LLM-based\nchatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned\nwith psychological dialogs, SiliconFriend displays heightened empathy in its\ninteractions. Experiment involves both qualitative analysis with real-world\nuser dialogs and quantitative analysis with simulated dialogs. In the latter,\nChatGPT acts as users with diverse characteristics and generates long-term\ndialog contexts covering a wide array of topics. The results of our analysis\nreveal that SiliconFriend, equipped with MemoryBank, exhibits a strong\ncapability for long-term companionship as it can provide emphatic response,\nrecall relevant memories and understand user personality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wanjun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Lianghong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanlin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy. (arXiv:2305.10307v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10307","description":"<p>Measuring the distance between machine-produced and human language is a\ncritical open problem. Inspired by empirical findings from psycholinguistics on\nthe periodicity of entropy in language, we propose FACE, a set of metrics based\non Fourier Analysis of the estimated Cross-Entropy of language, for measuring\nthe similarity between model-generated and human-written languages. Based on an\nopen-ended generation task and the experimental data from previous studies, we\nfind that FACE can effectively identify the human-model gap, scales with model\nsize, reflects the outcomes of different sampling methods for decoding,\ncorrelates well with other evaluation metrics and with human judgment scores.\nFACE is computationally efficient and provides intuitive interpretations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zuhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yingfang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_S/0/1/0/all/0/1\">Shuo Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Huajun Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kefan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-Scale Text Analysis Using Generative Language Models: A Case Study in Discovering Public Value Expressions in AI Patents. (arXiv:2305.10383v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10383","description":"<p>Labeling data is essential for training text classifiers but is often\ndifficult to accomplish accurately, especially for complex and abstract\nconcepts. Seeking an improved method, this paper employs a novel approach using\na generative language model (GPT-4) to produce labels and rationales for\nlarge-scale text analysis. We apply this approach to the task of discovering\npublic value expressions in US AI patents. We collect a database comprising\n154,934 patent documents using an advanced Boolean query submitted to\nInnovationQ+. The results are merged with full patent text from the USPTO,\nresulting in 5.4 million sentences. We design a framework for identifying and\nlabeling public value expressions in these AI patent sentences. A prompt for\nGPT-4 is developed which includes definitions, guidelines, examples, and\nrationales for text classification. We evaluate the quality of the labels and\nrationales produced by GPT-4 using BLEU scores and topic modeling and find that\nthey are accurate, diverse, and faithful. These rationales also serve as a\nchain-of-thought for the model, a transparent mechanism for human verification,\nand support for human annotators to overcome cognitive limitations. We conclude\nthat GPT-4 achieved a high-level of recognition of public value theory from our\nframework, which it also uses to discover unseen public value expressions. We\nuse the labels produced by GPT-4 to train BERT-based classifiers and predict\nsentences on the entire database, achieving high F1 scores for the 3-class\n(0.85) and 2-class classification (0.91) tasks. We discuss the implications of\nour approach for conducting large-scale text analyses with complex and abstract\nconcepts and suggest that, with careful framework design and interactive human\noversight, generative language models can offer significant advantages in\nquality and in reduced time and costs for producing labels and rationales.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pelaez_S/0/1/0/all/0/1\">Sergio Pelaez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_G/0/1/0/all/0/1\">Gaurav Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_B/0/1/0/all/0/1\">Barbara Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shapira_P/0/1/0/all/0/1\">Philip Shapira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-05-18T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}