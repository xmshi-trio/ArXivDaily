{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-06-05T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Weakly-supervised forced alignment of disfluent speech using phoneme-level modeling. (arXiv:2306.00996v1 [eess.AS])","link":"http://arxiv.org/abs/2306.00996","description":"<p>The study of speech disorders can benefit greatly from time-aligned data.\nHowever, audio-text mismatches in disfluent speech cause rapid performance\ndegradation for modern speech aligners, hindering the use of automatic\napproaches. In this work, we propose a simple and effective modification of\nalignment graph construction of CTC-based models using Weighted Finite State\nTransducers. The proposed weakly-supervised approach alleviates the need for\nverbatim transcription of speech disfluencies for forced alignment. During the\ngraph construction, we allow the modeling of common speech disfluencies, i.e.\nrepetitions and omissions. Further, we show that by assessing the degree of\naudio-text mismatch through the use of Oracle Error Rate, our method can be\neffectively used in the wild. Our evaluation on a corrupted version of the\nTIMIT test set and the UCLASS dataset shows significant improvements,\nparticularly for recall, achieving a 23-25% relative improvement over our\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kouzelis_T/0/1/0/all/0/1\">Theodoros Kouzelis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paraskevopoulos_G/0/1/0/all/0/1\">Georgios Paraskevopoulos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Katsamanis_A/0/1/0/all/0/1\">Athanasios Katsamanis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Katsouros_V/0/1/0/all/0/1\">Vassilis Katsouros</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Selection of Text-to-speech Data to Augment ASR Training. (arXiv:2306.00998v1 [eess.AS])","link":"http://arxiv.org/abs/2306.00998","description":"<p>This paper presents a method for selecting appropriate synthetic speech\nsamples from a given large text-to-speech (TTS) dataset as supplementary\ntraining data for an automatic speech recognition (ASR) model. We trained a\nneural network, which can be optimised using cross-entropy loss or Arcface\nloss, to measure the similarity of a synthetic data to real speech. We found\nthat incorporating synthetic samples with considerable dissimilarity to real\nspeech, owing in part to lexical differences, into ASR training is crucial for\nboosting recognition performance. Experimental results on Librispeech test sets\nindicate that, in order to maintain the same speech recognition accuracy as\nwhen using all TTS data, our proposed solution can reduce the size of the TTS\ndata down below its $30\\,\\%$, which is superior to several baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Shuo Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sari_L/0/1/0/all/0/1\">Leda Sar&#x131;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_C/0/1/0/all/0/1\">Chunyang Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keren_G/0/1/0/all/0/1\">Gil Keren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shangguan_Y/0/1/0/all/0/1\">Yuan Shangguan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahadeokar_J/0/1/0/all/0/1\">Jay Mahadeokar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AoM: Detecting Aspect-oriented Information for Multimodal Aspect-Based Sentiment Analysis. (arXiv:2306.01004v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01004","description":"<p>Multimodal aspect-based sentiment analysis (MABSA) aims to extract aspects\nfrom text-image pairs and recognize their sentiments. Existing methods make\ngreat efforts to align the whole image to corresponding aspects. However,\ndifferent regions of the image may relate to different aspects in the same\nsentence, and coarsely establishing image-aspect alignment will introduce noise\nto aspect-based sentiment analysis (i.e., visual noise). Besides, the sentiment\nof a specific aspect can also be interfered by descriptions of other aspects\n(i.e., textual noise). Considering the aforementioned noises, this paper\nproposes an Aspect-oriented Method (AoM) to detect aspect-relevant semantic and\nsentiment information. Specifically, an aspect-aware attention module is\ndesigned to simultaneously select textual tokens and image blocks that are\nsemantically related to the aspects. To accurately aggregate sentiment\ninformation, we explicitly introduce sentiment embedding into AoM, and use a\ngraph convolutional network to model the vision-text and text-text interaction.\nExtensive experiments demonstrate the superiority of AoM to existing methods.\nThe source code is publicly released at https://github.com/SilyRab/AoM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Ru Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wenya Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xumeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shenglong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaojie Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Evidence-based Instructional Design Expertise through Large Language Models. (arXiv:2306.01006v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01006","description":"<p>This paper presents a comprehensive exploration of leveraging Large Language\nModels (LLMs), specifically GPT-4, in the field of instructional design. With a\nfocus on scaling evidence-based instructional design expertise, our research\naims to bridge the gap between theoretical educational studies and practical\nimplementation. We discuss the benefits and limitations of AI-driven content\ngeneration, emphasizing the necessity of human oversight in ensuring the\nquality of educational materials. This work is elucidated through two detailed\ncase studies where we applied GPT-4 in creating complex higher-order\nassessments and active learning components for different courses. From our\nexperiences, we provide best practices for effectively using LLMs in\ninstructional design tasks, such as utilizing templates, fine-tuning, handling\nunexpected output, implementing LLM chains, citing references, evaluating\noutput, creating rubrics, grading, and generating distractors. We also share\nour vision of a future recommendation system, where a customized GPT-4 extracts\ninstructional design principles from educational studies and creates\npersonalized, evidence-supported strategies for users' unique educational\ncontexts. Our research contributes to understanding and optimally harnessing\nthe potential of AI-driven language models in enhancing educational outcomes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_G/0/1/0/all/0/1\">Gautam Yadav</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Examining the Emergence of Deductive Reasoning in Generative Language Models. (arXiv:2306.01009v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01009","description":"<p>We conduct a preliminary inquiry into the ability of generative transformer\nmodels to deductively reason from premises provided. We observe notable\ndifferences in the performance of models coming from different training setups\nand find that the deductive reasoning ability increases with scale. Further, we\ndiscover that the performance generally does not decrease with the length of\nthe deductive chain needed to reach the conclusion, with the exception of\nOpenAI GPT-3 and GPT-3.5 models. Our study considers a wide variety of\ntransformer-decoder models, ranging from 117 million to 175 billion parameters\nin size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belcak_P/0/1/0/all/0/1\">Peter Belcak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanzendorfer_L/0/1/0/all/0/1\">Luca A. Lanzend&#xf6;rfer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1\">Roger Wattenhofer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Estimate Model Transferability of Pre-Trained Speech Models?. (arXiv:2306.01015v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01015","description":"<p>In this work, we introduce a ``score-based assessment'' framework for\nestimating the transferability of pre-trained speech models (PSMs) for\nfine-tuning target tasks. We leverage upon two representation theories,\nBayesian likelihood estimation and optimal transport, to generate rank scores\nfor the PSM candidates using the extracted representations. Our framework\nefficiently computes transferability scores without actual fine-tuning of\ncandidate models or layers by making a temporal independent hypothesis. We\nevaluate some popular supervised speech models (e.g., Conformer RNN-Transducer)\nand self-supervised speech models (e.g., HuBERT) in cross-layer and cross-model\nsettings using public data. Experimental results show a high Spearman's rank\ncorrelation and low $p$-value between our estimation framework and fine-tuning\nground truth. Our proposed transferability framework requires less\ncomputational time and resources, making it a resource-saving and\ntime-efficient approach for tuning speech foundation models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zih-Ching Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nanxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shou-Yiin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhavalkar_R/0/1/0/all/0/1\">Rohit Prabhavalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PV2TEA: Patching Visual Modality to Textual-Established Information Extraction. (arXiv:2306.01016v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01016","description":"<p>Information extraction, e.g., attribute value extraction, has been\nextensively studied and formulated based only on text. However, many attributes\ncan benefit from image-based extraction, like color, shape, pattern, among\nothers. The visual modality has long been underutilized, mainly due to\nmultimodal annotation difficulty. In this paper, we aim to patch the visual\nmodality to the textual-established attribute information extractor. The\ncross-modality integration faces several unique challenges: (C1) images and\ntextual descriptions are loosely paired intra-sample and inter-samples; (C2)\nimages usually contain rich backgrounds that can mislead the prediction; (C3)\nweakly supervised labels from textual-established extractors are biased for\nmultimodal training. We present PV2TEA, an encoder-decoder architecture\nequipped with three bias reduction schemes: (S1) Augmented label-smoothed\ncontrast to improve the cross-modality alignment for loosely-paired image and\ntext; (S2) Attention-pruning that adaptively distinguishes the visual\nforeground; (S3) Two-level neighborhood regularization that mitigates the label\ntextual bias via reliability estimation. Empirical results on real-world\ne-Commerce datasets demonstrate up to 11.74% absolute (20.97% relatively) F1\nincrease over unimodal baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1\">Rongmei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zalmout_N/0/1/0/all/0/1\">Nasser Zalmout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bypass Temporal Classification: Weakly Supervised Automatic Speech Recognition with Imperfect Transcripts. (arXiv:2306.01031v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01031","description":"<p>This paper presents a novel algorithm for building an automatic speech\nrecognition (ASR) model with imperfect training data. Imperfectly transcribed\nspeech is a prevalent issue in human-annotated speech corpora, which degrades\nthe performance of ASR models. To address this problem, we propose Bypass\nTemporal Classification (BTC) as an expansion of the Connectionist Temporal\nClassification (CTC) criterion. BTC explicitly encodes the uncertainties\nassociated with transcripts during training. This is accomplished by enhancing\nthe flexibility of the training graph, which is implemented as a weighted\nfinite-state transducer (WFST) composition. The proposed algorithm improves the\nrobustness and accuracy of ASR systems, particularly when working with\nimprecisely transcribed speech corpora. Our implementation will be\nopen-sourced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Dongji Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiesner_M/0/1/0/all/0/1\">Matthew Wiesner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hainan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_L/0/1/0/all/0/1\">Leibny Paola Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Povey_D/0/1/0/all/0/1\">Daniel Povey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khudanpur_S/0/1/0/all/0/1\">Sanjeev Khudanpur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Layout-Infused Language Models Robust to Layout Distribution Shifts? A Case Study with Scientific Documents. (arXiv:2306.01058v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01058","description":"<p>Recent work has shown that infusing layout features into language models\n(LMs) improves processing of visually-rich documents such as scientific papers.\nLayout-infused LMs are often evaluated on documents with familiar layout\nfeatures (e.g., papers from the same publisher), but in practice models\nencounter documents with unfamiliar distributions of layout features, such as\nnew combinations of text sizes and styles, or new spatial configurations of\ntextual elements. In this work we test whether layout-infused LMs are robust to\nlayout distribution shifts. As a case study we use the task of scientific\ndocument structure recovery, segmenting a scientific paper into its structural\ncategories (e.g., \"title\", \"caption\", \"reference\"). To emulate distribution\nshifts that occur in practice we re-partition the GROTOAP2 dataset. We find\nthat under layout distribution shifts model performance degrades by up to 20\nF1. Simple training strategies, such as increasing training diversity, can\nreduce this degradation by over 35% relative F1; however, models fail to reach\nin-distribution performance in any tested out-of-distribution conditions. This\nwork highlights the need to consider layout distribution shifts during model\nevaluation, and presents a methodology for conducting such evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Catherine Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zejiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reimagining Retrieval Augmented Language Models for Answering Queries. (arXiv:2306.01061v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01061","description":"<p>We present a reality check on large language models and inspect the promise\nof retrieval augmented language models in comparison. Such language models are\nsemi-parametric, where models integrate model parameters and knowledge from\nexternal data sources to make their predictions, as opposed to the parametric\nnature of vanilla large language models. We give initial experimental findings\nthat semi-parametric architectures can be enhanced with views, a query\nanalyzer/planner, and provenance to make a significantly more powerful system\nfor question answering in terms of accuracy and efficiency, and potentially for\nother NLP tasks\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wang-Chiew Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1\">Pedro Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_R/0/1/0/all/0/1\">Richard James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xi Victoria Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halevy_A/0/1/0/all/0/1\">Alon Halevy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_S/0/1/0/all/0/1\">Scott Yih</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TimelineQA: A Benchmark for Question Answering over Timelines. (arXiv:2306.01069v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01069","description":"<p>Lifelogs are descriptions of experiences that a person had during their life.\nLifelogs are created by fusing data from the multitude of digital services,\nsuch as online photos, maps, shopping and content streaming services. Question\nanswering over lifelogs can offer personal assistants a critical resource when\nthey try to provide advice in context. However, obtaining answers to questions\nover lifelogs is beyond the current state of the art of question answering\ntechniques for a variety of reasons, the most pronounced of which is that\nlifelogs combine free text with some degree of structure such as temporal and\ngeographical information.\n</p>\n<p>We create and publicly release TimelineQA1, a benchmark for accelerating\nprogress on querying lifelogs. TimelineQA generates lifelogs of imaginary\npeople. The episodes in the lifelog range from major life episodes such as high\nschool graduation to those that occur on a daily basis such as going for a run.\nWe describe a set of experiments on TimelineQA with several state-of-the-art QA\nmodels. Our experiments reveal that for atomic queries, an extractive QA system\nsignificantly out-performs a state-of-the-art retrieval-augmented QA system.\nFor multi-hop queries involving aggregates, we show that the best result is\nobtained with a state-of-the-art table QA technique, assuming the ground truth\nset of episodes for deriving the answer is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wang-Chiew Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwivedi_Yu_J/0/1/0/all/0/1\">Jane Dwivedi-Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathias_L/0/1/0/all/0/1\">Lambert Mathias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeidi_M/0/1/0/all/0/1\">Marzieh Saeidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jing Nathan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halevy_A/0/1/0/all/0/1\">Alon Y. Halevy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantization-Aware and Tensor-Compressed Training of Transformers for Natural Language Understanding. (arXiv:2306.01076v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01076","description":"<p>Fine-tuned transformer models have shown superior performances in many\nnatural language tasks. However, the large model size prohibits deploying\nhigh-performance transformer models on resource-constrained devices. This paper\nproposes a quantization-aware tensor-compressed training approach to reduce the\nmodel size, arithmetic operations, and ultimately runtime latency of\ntransformer-based models. We compress the embedding and linear layers of\ntransformers into small low-rank tensor cores, which significantly reduces\nmodel parameters. A quantization-aware training with learnable scale factors is\nused to further obtain low-precision representations of the tensor-compressed\nmodels. The developed approach can be used for both end-to-end training and\ndistillation-based training. To improve the convergence, a layer-by-layer\ndistillation is applied to distill a quantized and tensor-compressed student\nmodel from a pre-trained transformer. The performance is demonstrated in two\nnatural language understanding tasks, showing up to $63\\times$ compression\nratio, little accuracy loss and remarkable inference and training speedup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1\">Samridhi Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunzmann_S/0/1/0/all/0/1\">Siegfried Kunzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Robustness of Summarization Systems with Dual Augmentation. (arXiv:2306.01090v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01090","description":"<p>A robust summarization system should be able to capture the gist of the\ndocument, regardless of the specific word choices or noise in the input. In\nthis work, we first explore the summarization models' robustness against\nperturbations including word-level synonym substitution and noise. To create\nsemantic-consistent substitutes, we propose a SummAttacker, which is an\nefficient approach to generating adversarial samples based on language models.\nExperimental results show that state-of-the-art summarization models have a\nsignificant decrease in performance on adversarial and noisy test sets. Next,\nwe analyze the vulnerability of the summarization systems and explore improving\nthe robustness by data augmentation. Specifically, the first brittleness factor\nwe found is the poor understanding of infrequent words in the input.\nCorrespondingly, we feed the encoder with more diverse cases created by\nSummAttacker in the input space. The other factor is in the latent space, where\nthe attacked inputs bring more variations to the hidden states. Hence, we\nconstruct adversarial decoder input and devise manifold softmixing operation in\nhidden space to introduce more diversity. Experimental results on Gigaword and\nCNN/DM datasets demonstrate that our approach achieves significant improvements\nover strong baselines and exhibits higher robustness on noisy, attacked, and\nclean datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangliang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UCAS-IIE-NLP at SemEval-2023 Task 12: Enhancing Generalization of Multilingual BERT for Low-resource Sentiment Analysis. (arXiv:2306.01093v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01093","description":"<p>This paper describes our system designed for SemEval-2023 Task 12: Sentiment\nanalysis for African languages. The challenge faced by this task is the\nscarcity of labeled data and linguistic resources in low-resource settings. To\nalleviate these, we propose a generalized multilingual system SACL-XLMR for\nsentiment analysis on low-resource languages. Specifically, we design a\nlexicon-based multilingual BERT to facilitate language adaptation and\nsentiment-aware representation learning. Besides, we apply a supervised\nadversarial contrastive learning technique to learn sentiment-spread structured\nrepresentations and enhance model generalization. Our system achieved\ncompetitive results, largely outperforming baselines on both multilingual and\nzero-shot sentiment classification subtasks. Notably, the system obtained the\n1st rank on the zero-shot classification subtask in the official ranking.\nExtensive experiments demonstrate the effectiveness of our system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Dou Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Lingwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yaxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songlin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization. (arXiv:2306.01102v1 [cs.NE])","link":"http://arxiv.org/abs/2306.01102","description":"<p>Large Language Models (LLMs) have emerged as powerful tools capable of\naccomplishing a broad spectrum of tasks. Their abilities span numerous areas,\nand one area where they have made a significant impact is in the domain of code\ngeneration. In this context, we view LLMs as mutation and crossover tools.\nMeanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and\nrobust solutions. By merging the code-generating abilities of LLMs with the\ndiversity and robustness of QD solutions, we introduce LLMatic, a Neural\nArchitecture Search (NAS) algorithm. While LLMs struggle to conduct NAS\ndirectly through prompts, LLMatic uses a procedural approach, leveraging QD for\nprompts and network architecture to create diverse and highly performant\nnetworks. We test LLMatic on the CIFAR-10 image classification benchmark,\ndemonstrating that it can produce competitive networks with just $2,000$\nsearches, even without prior knowledge of the benchmark domain or exposure to\nany previous top-performing models for the benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nasir_M/0/1/0/all/0/1\">Muhammad U. Nasir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Earle_S/0/1/0/all/0/1\">Sam Earle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1\">Julian Togelius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Steven James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cleghorn_C/0/1/0/all/0/1\">Christopher Cleghorn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Hate Speech Benchmarks: From Data Curation to System Deployment. (arXiv:2306.01105v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01105","description":"<p>Social media is awash with hateful content, much of which is often veiled\nwith linguistic and topical diversity. The benchmark datasets used for hate\nspeech detection do not account for such divagation as they are predominantly\ncompiled using hate lexicons. However, capturing hate signals becomes\nchallenging in neutrally-seeded malicious content. Thus, designing models and\ndatasets that mimic the real-world variability of hate warrants further\ninvestigation.\n</p>\n<p>To this end, we present GOTHate, a large-scale code-mixed crowdsourced\ndataset of around 51k posts for hate speech detection from Twitter. GOTHate is\nneutrally seeded, encompassing different languages and topics. We conduct\ndetailed comparisons of GOTHate with the existing hate speech datasets,\nhighlighting its novelty. We benchmark it with 10 recent baselines. Our\nextensive empirical and benchmarking experiments suggest that GOTHate is hard\nto classify in a text-only setup. Thus, we investigate how adding endogenous\nsignals enhances the hate speech detection task. We augment GOTHate with the\nuser's timeline information and ego network, bringing the overall data source\ncloser to the real-world setup for understanding hateful content. Our proposed\nsolution HEN-mBERT is a modular, multilingual, mixture-of-experts model that\nenriches the linguistic subspace with latent endogenous signals from history,\ntopology, and exemplars. HEN-mBERT transcends the best baseline by 2.5% and 5%\nin overall macro-F1 and hate class F1, respectively. Inspired by our\nexperiments, in partnership with Wipro AI, we are developing a semi-automated\npipeline to detect hateful content as a part of their mission to tackle online\nharm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1\">Atharva Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masud_S/0/1/0/all/0/1\">Sarah Masud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1\">Vikram Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only. (arXiv:2306.01116v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01116","description":"<p>Large language models are commonly trained on a mixture of filtered web data\nand curated high-quality corpora, such as social media conversations, books, or\ntechnical papers. This curation process is believed to be necessary to produce\nperformant models with broad zero-shot generalization abilities. However, as\nlarger models requiring pretraining on trillions of tokens are considered, it\nis unclear how scalable is curation and whether we will run out of unique\nhigh-quality data soon. At variance with previous beliefs, we show that\nproperly filtered and deduplicated web data alone can lead to powerful models;\neven significantly outperforming models from the state-of-the-art trained on\nThe Pile. Despite extensive filtering, the high-quality data we extract from\nthe web is still plentiful, and we are able to obtain five trillion tokens from\nCommonCrawl. We publicly release an extract of 600 billion tokens from our\nRefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Penedo_G/0/1/0/all/0/1\">Guilherme Penedo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malartic_Q/0/1/0/all/0/1\">Quentin Malartic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hesslow_D/0/1/0/all/0/1\">Daniel Hesslow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cojocaru_R/0/1/0/all/0/1\">Ruxandra Cojocaru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cappelli_A/0/1/0/all/0/1\">Alessandro Cappelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alobeidli_H/0/1/0/all/0/1\">Hamza Alobeidli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pannier_B/0/1/0/all/0/1\">Baptiste Pannier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almazrouei_E/0/1/0/all/0/1\">Ebtesam Almazrouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Launay_J/0/1/0/all/0/1\">Julien Launay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Examining the Causal Effect of First Names on Language Models: The Case of Social Commonsense Reasoning. (arXiv:2306.01117v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01117","description":"<p>As language models continue to be integrated into applications of personal\nand societal relevance, ensuring these models' trustworthiness is crucial,\nparticularly with respect to producing consistent outputs regardless of\nsensitive attributes. Given that first names may serve as proxies for\n(intersectional) socio-demographic representations, it is imperative to examine\nthe impact of first names on commonsense reasoning capabilities. In this paper,\nwe study whether a model's reasoning given a specific input differs based on\nthe first names provided. Our underlying assumption is that the reasoning about\nAlice should not differ from the reasoning about James. We propose and\nimplement a controlled experimental framework to measure the causal effect of\nfirst names on commonsense reasoning, enabling us to distinguish between model\npredictions due to chance and caused by actual factors of interest. Our results\nindicate that the frequency of first names has a direct effect on model\nprediction, with less frequent names yielding divergent predictions compared to\nmore frequent names. To gain insights into the internal mechanisms of models\nthat are contributing to these behaviors, we also conduct an in-depth\nexplainable analysis. Overall, our findings suggest that to ensure model\nrobustness, it is essential to augment datasets with more diverse first names\nduring the configuration stage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeoung_S/0/1/0/all/0/1\">Sullam Jeoung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diesner_J/0/1/0/all/0/1\">Jana Diesner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilicoglu_H/0/1/0/all/0/1\">Halil Kilicoglu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Transformer Programs. (arXiv:2306.01128v1 [cs.LG])","link":"http://arxiv.org/abs/2306.01128","description":"<p>Recent research in mechanistic interpretability has attempted to\nreverse-engineer Transformer models by carefully inspecting network weights and\nactivations. However, these approaches require considerable manual effort and\nstill fall short of providing complete, faithful descriptions of the underlying\nalgorithms. In this work, we introduce a procedure for training Transformers\nthat are mechanistically interpretable by design. We build on RASP [Weiss et\nal., 2021], a programming language that can be compiled into Transformer\nweights. Instead of compiling human-written programs into Transformers, we\ndesign a modified Transformer that can be trained using gradient-based\noptimization and then be automatically converted into a discrete,\nhuman-readable program. We refer to these models as Transformer Programs. To\nvalidate our approach, we learn Transformer Programs for a variety of problems,\nincluding an in-context learning task, a suite of algorithmic problems (e.g.\nsorting, recognizing Dyck-languages), and NLP tasks including named entity\nrecognition and text classification. The Transformer Programs can automatically\nfind reasonable solutions, performing on par with standard Transformers of\ncomparable size; and, more importantly, they are easy to interpret. To\ndemonstrate these advantages, we convert Transformers into Python programs and\nuse off-the-shelf code analysis tools to debug model errors and identify the\n``circuits'' used to solve different sub-problems. We hope that Transformer\nPrograms open a new path toward the goal of intrinsically interpretable machine\nlearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Friedman_D/0/1/0/all/0/1\">Dan Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wettig_A/0/1/0/all/0/1\">Alexander Wettig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Capabilities of Multi-modal Reasoning Models with Synthetic Task Data. (arXiv:2306.01144v1 [cs.LG])","link":"http://arxiv.org/abs/2306.01144","description":"<p>The impressive advances and applications of large language and joint\nlanguage-and-visual understanding models has led to an increased need for\nmethods of probing their potential reasoning capabilities. However, the\ndifficulty of gather naturally-occurring data for complex multi-modal reasoning\ntasks bottlenecks the evaluation of AI methods on tasks which are not already\ncovered by an academic dataset. In this work, we leverage recent advances in\nhigh resolution text-to-image generation to develop a framework for generating\nevaluation data for multi-modal reasoning tasks. We apply this framework to\ngenerate context-dependent anomaly data, creating a synthetic dataset on a\nchallenging task which is not well covered by existing datasets. We benchmark\nthe performance of a state-of-the-art visual question answering (VQA) model\nagainst data generated with this method, and demonstrate that while the task is\ntractable, the model performs significantly worse on the context-dependent\nanomaly detection task than on standard VQA tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vaska_N/0/1/0/all/0/1\">Nathan Vaska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helus_V/0/1/0/all/0/1\">Victoria Helus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning. (arXiv:2306.01150v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01150","description":"<p>Large language models (LLMs) have shown impressive performance in following\nnatural language instructions to solve unseen tasks. However, it remains\nunclear whether models truly understand task definitions and whether the\nhuman-written definitions are optimal. In this paper, we systematically study\nthe role of task definitions in instruction learning. We first conduct an\nablation analysis informed by human annotations to understand which parts of a\ntask definition are most important, and find that model performance only drops\nsubstantially when removing contents describing the task output, in particular\nlabel information. Next, we propose an automatic algorithm to compress task\ndefinitions to a minimal supporting set of tokens, and find that 60\\% of tokens\ncan be removed while maintaining or even improving model performance. Based on\nthese results, we propose two strategies to help models better leverage task\ninstructions: (1) providing only key information for tasks in a common\nstructured format, and (2) adding a meta-tuning stage to help the model better\nunderstand the definitions. With these two strategies, we achieve a 4.2 Rouge-L\nimprovement over 119 unseen test tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1\">Fan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vig_J/0/1/0/all/0/1\">Jesse Vig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1\">Philippe Laban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Jason Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse and Faithful Knowledge-Grounded Dialogue Generation via Sequential Posterior Inference. (arXiv:2306.01153v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01153","description":"<p>The capability to generate responses with diversity and faithfulness using\nfactual knowledge is paramount for creating a human-like, trustworthy dialogue\nsystem. Common strategies either adopt a two-step paradigm, which optimizes\nknowledge selection and response generation separately, and may overlook the\ninherent correlation between these two tasks, or leverage conditional\nvariational method to jointly optimize knowledge selection and response\ngeneration by employing an inference network. In this paper, we present an\nend-to-end learning framework, termed Sequential Posterior Inference (SPI),\ncapable of selecting knowledge and generating dialogues by approximately\nsampling from the posterior distribution. Unlike other methods, SPI does not\nrequire the inference network or assume a simple geometry of the posterior\ndistribution. This straightforward and intuitive inference procedure of SPI\ndirectly queries the response generation model, allowing for accurate knowledge\nselection and generation of faithful responses. In addition to modeling\ncontributions, our experimental results on two common dialogue datasets (Wizard\nof Wikipedia and Holl-E) demonstrate that SPI outperforms previous strong\nbaselines according to both automatic and human evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Deqian Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dehong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Ziwei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faster Causal Attention Over Large Sequences Through Sparse Flash Attention. (arXiv:2306.01160v1 [cs.LG])","link":"http://arxiv.org/abs/2306.01160","description":"<p>Transformer-based language models have found many diverse applications\nrequiring them to process sequences of increasing length. For these\napplications, the causal self-attention -- which is the only component scaling\nquadratically w.r.t. the sequence length -- becomes a central concern. While\nmany works have proposed schemes to sparsify the attention patterns and reduce\nthe computational overhead of self-attention, those are often limited by\nimplementations concerns and end up imposing a simple and static structure over\nthe attention matrix. Conversely, implementing more dynamic sparse attentions\noften results in runtimes significantly slower than computing the full\nattention using the Flash implementation from Dao et al. (2022). We extend\nFlashAttention to accommodate a large class of attention sparsity patterns\nthat, in particular, encompass key/query dropping and hashing-based attention.\nThis leads to implementations with no computational complexity overhead and a\nmulti-fold runtime speedup on top of FlashAttention. Even with relatively low\ndegrees of sparsity, our method improves visibly upon FlashAttention as the\nsequence length increases. Without sacrificing perplexity, we increase the\ntraining speed of a transformer language model by $2.0\\times$ and $3.3\\times$\nfor sequences of respectively $8k$ and $16k$ tokens.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pagliardini_M/0/1/0/all/0/1\">Matteo Pagliardini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paliotta_D/0/1/0/all/0/1\">Daniele Paliotta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleuret_F/0/1/0/all/0/1\">Fran&#xe7;ois Fleuret</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Natural Language Processing For Public Health Screening On YouTube: A COVID-19 Case Study. (arXiv:2306.01164v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01164","description":"<p>Background: Social media platforms have become a viable source of medical\ninformation, with patients and healthcare professionals using them to share\nhealth-related information and track diseases. Similarly, YouTube, the largest\nvideo-sharing platform in the world contains vlogs where individuals talk about\ntheir illnesses. The aim of our study was to investigate the use of Natural\nLanguage Processing (NLP) to identify the spoken content of YouTube vlogs\nrelated to the diagnosis of Coronavirus disease of 2019 (COVID-19) for public\nhealth screening. Methods: COVID-19 videos on YouTube were searched using\nrelevant keywords. A total of 1000 videos being spoken in English were\ndownloaded out of which 791 were classified as vlogs, 192 were non-vlogs, and\n17 were deleted by the channel. The videos were converted into a textual format\nusing Microsoft Streams. The textual data was preprocessed using basic and\nadvanced preprocessing methods. A lexicon of 200 words was created which\ncontained words related to COVID-19. The data was analyzed using topic\nmodeling, word clouds, and lexicon matching. Results: The word cloud results\nrevealed discussions about COVID-19 symptoms like \"fever\", along with generic\nterms such as \"mask\" and \"isolation\". Lexical analysis demonstrated that in\n96.46% of videos, patients discussed generic terms, and in 95.45% of videos,\npeople talked about COVID-19 symptoms. LDA Topic Modeling results also\ngenerated topics that successfully captured key themes and content related to\nour investigation of COVID-19 diagnoses in YouTube vlogs. Conclusion: By\nleveraging NLP techniques on YouTube vlogs public health practitioners can\nenhance their ability to mitigate the effects of pandemics and effectively\nrespond to public health challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aslam_A/0/1/0/all/0/1\">Ahrar Bin Aslam</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Syed_Z/0/1/0/all/0/1\">Zafi Sherhan Syed</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Muhammad Faiz Khan</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Baloch_A/0/1/0/all/0/1\">Asghar Baloch</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Syed_M/0/1/0/all/0/1\">Muhammad Shehram Shah Syed</a> (1) ((1) Mehran University of Engineering and Technology)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Long Document Summarization using C2F-FAR and ChatGPT: A Practical Study. (arXiv:2306.01169v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01169","description":"<p>Text summarization is a downstream natural language processing (NLP) task\nthat challenges the understanding and generation capabilities of language\nmodels. Considerable progress has been made in automatically summarizing short\ntexts, such as news articles, often leading to satisfactory results. However,\nsummarizing long documents remains a major challenge. This is due to the\ncomplex contextual information in the text and the lack of open-source\nbenchmarking datasets and evaluation frameworks that can be used to develop and\ntest model performance. In this work, we use ChatGPT, the latest breakthrough\nin the field of large language models (LLMs), together with the extractive\nsummarization model C2F-FAR (Coarse-to-Fine Facet-Aware Ranking) to propose a\nhybrid extraction and summarization pipeline for long documents such as\nbusiness articles and books. We work with the world-renowned company\ngetAbstract AG and leverage their expertise and experience in professional book\nsummarization. A practical study has shown that machine-generated summaries can\nperform at least as well as human-written summaries when evaluated using\ncurrent automated evaluation metrics. However, a closer examination of the\ntexts generated by ChatGPT through human evaluations has shown that there are\nstill critical issues in terms of text coherence, faithfulness, and style.\nOverall, our results show that the use of ChatGPT is a very promising but not\nyet mature approach for summarizing long documents and can at best serve as an\ninspiration for human editors. We anticipate that our work will inform NLP\nresearchers about the extent to which ChatGPT's capabilities for summarizing\nlong documents overlap with practitioners' needs. Further work is needed to\ntest the proposed hybrid summarization pipeline, in particular involving GPT-4,\nand to propose a new evaluation framework tailored to the task of summarizing\nlong documents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larcher_S/0/1/0/all/0/1\">Sylvia B. Larcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Tu Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation. (arXiv:2306.01183v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01183","description":"<p>Very large language models (LLMs) perform extremely well on a spectrum of NLP\ntasks in a zero-shot setting. However, little is known about their performance\non human-level NLP problems which rely on understanding psychological concepts,\nsuch as assessing personality traits. In this work, we investigate the\nzero-shot ability of GPT-3 to estimate the Big 5 personality traits from users'\nsocial media posts. Through a set of systematic experiments, we find that\nzero-shot GPT-3 performance is somewhat close to an existing pre-trained SotA\nfor broad classification upon injecting knowledge about the trait in the\nprompts. However, when prompted to provide fine-grained classification, its\nperformance drops to close to a simple most frequent class (MFC) baseline. We\nfurther analyze where GPT-3 performs better, as well as worse, than a\npretrained lexical model, illustrating systematic errors that suggest ways to\nimprove LLMs on human-level NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ganesan_A/0/1/0/all/0/1\">Adithya V Ganesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_Y/0/1/0/all/0/1\">Yash Kumar Lal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nilsson_A/0/1/0/all/0/1\">August H&#xe5;kan Nilsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_H/0/1/0/all/0/1\">H. Andrew Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Dimensional Evaluation of Text Summarization with In-Context Learning. (arXiv:2306.01200v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01200","description":"<p>Evaluation of natural language generation (NLG) is complex and\nmulti-dimensional. Generated text can be evaluated for fluency, coherence,\nfactuality, or any other dimensions of interest. Most frameworks that perform\nsuch multi-dimensional evaluation require training on large manually or\nsynthetically generated datasets. In this paper, we study the efficacy of large\nlanguage models as multi-dimensional evaluators using in-context learning,\nobviating the need for large training datasets. Our experiments show that\nin-context learning-based evaluators are competitive with learned evaluation\nframeworks for the task of text summarization, establishing state-of-the-art on\ndimensions such as relevance and factual consistency. We then analyze the\neffects of factors such as the selection and number of in-context examples on\nperformance. Finally, we study the efficacy of in-context learning based\nevaluators in evaluating zero-shot summaries written by large language models\nsuch as GPT-3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sameer Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keshava_V/0/1/0/all/0/1\">Vaishakh Keshava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sathyendra_S/0/1/0/all/0/1\">Swarnashree Mysore Sathyendra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1\">Patrick Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chunting Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning When to Speak: Latency and Quality Trade-offs for Simultaneous Speech-to-Speech Translation with Offline Models. (arXiv:2306.01201v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01201","description":"<p>Recent work in speech-to-speech translation (S2ST) has focused primarily on\noffline settings, where the full input utterance is available before any output\nis given. This, however, is not reasonable in many real-world scenarios. In\nlatency-sensitive applications, rather than waiting for the full utterance,\ntranslations should be spoken as soon as the information in the input is\npresent. In this work, we introduce a system for simultaneous S2ST targeting\nreal-world use cases. Our system supports translation from 57 languages to\nEnglish with tunable parameters for dynamically adjusting the latency of the\noutput -- including four policies for determining when to speak an output\nsequence. We show that these policies achieve offline-level accuracy with\nminimal increases in latency over a Greedy (wait-$k$) baseline. We open-source\nour evaluation code and interactive test script to aid future SimulS2ST\nresearch and application development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dugan_L/0/1/0/all/0/1\">Liam Dugan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadhawan_A/0/1/0/all/0/1\">Anshul Wadhawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spence_K/0/1/0/all/0/1\">Kyle Spence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGuire_M/0/1/0/all/0/1\">Morgan McGuire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zordan_V/0/1/0/all/0/1\">Victor Zordan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating Semantic Similarity between In-Domain and Out-of-Domain Samples. (arXiv:2306.01206v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01206","description":"<p>Prior work typically describes out-of-domain (OOD) or out-of-distribution\n(OODist) samples as those that originate from dataset(s) or source(s) different\nfrom the training set but for the same task. When compared to in-domain (ID)\nsamples, the models have been known to usually perform poorer on OOD samples,\nalthough this observation is not consistent. Another thread of research has\nfocused on OOD detection, albeit mostly using supervised approaches. In this\nwork, we first consolidate and present a systematic analysis of multiple\ndefinitions of OOD and OODist as discussed in prior literature. Then, we\nanalyze the performance of a model under ID and OOD/OODist settings in a\nprincipled way. Finally, we seek to identify an unsupervised method for\nreliably identifying OOD/OODist samples without using a trained model. The\nresults of our extensive evaluation using 12 datasets from 4 different tasks\nsuggest the promising potential of unsupervised metrics in this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pokharel_R/0/1/0/all/0/1\">Rhitabrat Pokharel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Ameeta Agrawal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting an Unadaptable ASR System. (arXiv:2306.01208v1 [eess.AS])","link":"http://arxiv.org/abs/2306.01208","description":"<p>As speech recognition model sizes and training data requirements grow, it is\nincreasingly common for systems to only be available via APIs from online\nservice providers rather than having direct access to models themselves. In\nthis scenario it is challenging to adapt systems to a specific target domain.\nTo address this problem we consider the recently released OpenAI Whisper ASR as\nan example of a large-scale ASR system to assess adaptation methods. An error\ncorrection based approach is adopted, as this does not require access to the\nmodel, but can be trained from either 1-best or N-best outputs that are\nnormally available via the ASR API. LibriSpeech is used as the primary target\ndomain for adaptation. The generalization ability of the system in two distinct\ndimensions are then evaluated. First, whether the form of correction model is\nportable to other speech recognition domains, and secondly whether it can be\nused for ASR models having a different architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ma_R/0/1/0/all/0/1\">Rao Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_M/0/1/0/all/0/1\">Mengjie Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gales_M/0/1/0/all/0/1\">Mark J. F. Gales</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Knill_K/0/1/0/all/0/1\">Kate M. Knill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators. (arXiv:2306.01242v1 [cs.AI])","link":"http://arxiv.org/abs/2306.01242","description":"<p>The recent success of Large Language Models (LLMs) signifies an impressive\nstride towards artificial general intelligence. They have shown a promising\nprospect in automatically completing tasks upon user instructions, functioning\nas brain-like coordinators. The associated risks will be revealed as we\ndelegate an increasing number of tasks to machines for automated completion. A\nbig question emerges: how can we make machines behave responsibly when helping\nhumans automate tasks as personal copilots? In this paper, we explore this\nquestion in depth from the perspectives of feasibility, completeness and\nsecurity. In specific, we present Responsible Task Automation (ResponsibleTA)\nas a fundamental framework to facilitate responsible collaboration between\nLLM-based coordinators and executors for task automation with three empowered\ncapabilities: 1) predicting the feasibility of the commands for executors; 2)\nverifying the completeness of executors; 3) enhancing the security (e.g., the\nprotection of users' privacy). We further propose and compare two paradigms for\nimplementing the first two capabilities. One is to leverage the generic\nknowledge of LLMs themselves via prompt engineering while the other is to adopt\ndomain-specific learnable models. Moreover, we introduce a local memory\nmechanism for achieving the third capability. We evaluate our proposed\nResponsibleTA on UI task automation and hope it could bring more attentions to\nensuring LLMs more responsible in diverse scenarios. The research project\nhomepage is at\nhttps://task-automation-research.github.io/responsible_task_automation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Wenxuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"THiFLY Research at SemEval-2023 Task 7: A Multi-granularity System for CTR-based Textual Entailment and Evidence Retrieval. (arXiv:2306.01245v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01245","description":"<p>The NLI4CT task aims to entail hypotheses based on Clinical Trial Reports\n(CTRs) and retrieve the corresponding evidence supporting the justification.\nThis task poses a significant challenge, as verifying hypotheses in the NLI4CT\ntask requires the integration of multiple pieces of evidence from one or two\nCTR(s) and the application of diverse levels of reasoning, including textual\nand numerical. To address these problems, we present a multi-granularity system\nfor CTR-based textual entailment and evidence retrieval in this paper.\nSpecifically, we construct a Multi-granularity Inference Network (MGNet) that\nexploits sentence-level and token-level encoding to handle both textual\nentailment and evidence retrieval tasks. Moreover, we enhance the numerical\ninference capability of the system by leveraging a T5-based model, SciFive,\nwhich is pre-trained on the medical corpus. Model ensembling and a joint\ninference method are further utilized in the system to increase the stability\nand consistency of inference. The system achieves f1-scores of 0.856 and 0.853\non textual entailment and evidence retrieval tasks, resulting in the best\nperformance on both subtasks. The experimental results corroborate the\neffectiveness of our proposed method. Our code is publicly available at\nhttps://github.com/THUMLP/NLI4CT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Ziyu Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Meiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Miao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xien Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1\">Xinxin You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Ji Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?. (arXiv:2306.01248v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01248","description":"<p>Automatic summarization of legal case judgements has traditionally been\nattempted by using extractive summarization methods. However, in recent years,\nabstractive summarization models are gaining popularity since they can generate\nmore natural and coherent summaries. Legal domain-specific pre-trained\nabstractive summarization models are now available. Moreover, general-domain\npre-trained Large Language Models (LLMs), such as ChatGPT, are known to\ngenerate high-quality text and have the capacity for text summarization. Hence\nit is natural to ask if these models are ready for off-the-shelf application to\nautomatically generate abstractive summaries for case judgements. To explore\nthis question, we apply several state-of-the-art domain-specific abstractive\nsummarization models and general-domain LLMs on Indian court case judgements,\nand check the quality of the generated summaries. In addition to standard\nmetrics for summary quality, we check for inconsistencies and hallucinations in\nthe summaries. We see that abstractive summarization models generally achieve\nslightly higher scores than extractive models in terms of standard summary\nevaluation metrics such as ROUGE and BLEU. However, we often find inconsistent\nor hallucinated information in the generated abstractive summaries. Overall,\nour investigation indicates that the pre-trained abstractive summarization\nmodels and LLMs are not yet ready for fully automatic deployment for case\njudgement summarization; rather a human-in-the-loop approach including manual\nchecks for inconsistencies is more suitable at present.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deroy_A/0/1/0/all/0/1\">Aniket Deroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_K/0/1/0/all/0/1\">Kripabandhu Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Saptarshi Ghosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Translation of Hate Speech to Non-hate Speech in Social Media Texts. (arXiv:2306.01261v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01261","description":"<p>In this paper, we investigate the issue of hate speech by presenting a novel\ntask of translating hate speech into non-hate speech text while preserving its\nmeaning. As a case study, we use Spanish texts. We provide a dataset and\nseveral baselines as a starting point for further research in the task. We\nevaluated our baseline results using multiple metrics, including BLEU scores.\nThe aim of this study is to contribute to the development of more effective\nmethods for reducing the spread of hate speech in online communities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kostiuk_Y/0/1/0/all/0/1\">Yevhen Kostiuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonja_A/0/1/0/all/0/1\">Atnafu Lambebo Tonja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidorov_G/0/1/0/all/0/1\">Grigori Sidorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolesnikova_O/0/1/0/all/0/1\">Olga Kolesnikova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VoteTRANS: Detecting Adversarial Text without Training by Voting on Hard Labels of Transformations. (arXiv:2306.01273v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01273","description":"<p>Adversarial attacks reveal serious flaws in deep learning models. More\ndangerously, these attacks preserve the original meaning and escape human\nrecognition. Existing methods for detecting these attacks need to be trained\nusing original/adversarial data. In this paper, we propose detection without\ntraining by voting on hard labels from predictions of transformations, namely,\nVoteTRANS. Specifically, VoteTRANS detects adversarial text by comparing the\nhard labels of input text and its transformation. The evaluation demonstrates\nthat VoteTRANS effectively detects adversarial text across various\nstate-of-the-art attacks, models, and datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Son_H/0/1/0/all/0/1\">Hoang-Quoc Nguyen-Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hidano_S/0/1/0/all/0/1\">Seira Hidano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukushima_K/0/1/0/all/0/1\">Kazuhide Fukushima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiyomoto_S/0/1/0/all/0/1\">Shinsaku Kiyomoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Echizen_I/0/1/0/all/0/1\">Isao Echizen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KL-Divergence Guided Temperature Sampling. (arXiv:2306.01286v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01286","description":"<p>Temperature sampling is a conventional approach to diversify large language\nmodel predictions. As temperature increases, the prediction becomes diverse but\nalso vulnerable to hallucinations -- generating tokens that are sensible but\nnot factual. One common approach to mitigate hallucinations is to provide\nsource/grounding documents and the model is trained to produce predictions that\nbind to and are attributable to the provided source. It appears that there is a\ntrade-off between diversity and attribution. To mitigate any such trade-off, we\npropose to relax the constraint of having a fixed temperature over decoding\nsteps, and a mechanism to guide the dynamic temperature according to its\nrelevance to the source through KL-divergence. Our experiments justifies the\ntrade-off, and shows that our sampling algorithm outperforms the conventional\ntop-k and top-p algorithms in conversational question-answering and\nsummarization tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chung-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reitter_D/0/1/0/all/0/1\">David Reitter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aksitov_R/0/1/0/all/0/1\">Renat Aksitov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yun-Hsuan Sung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Training for End-to-End Streaming Automatic Speech Recognition Model with Punctuation. (arXiv:2306.01296v1 [eess.AS])","link":"http://arxiv.org/abs/2306.01296","description":"<p>Punctuated text prediction is crucial for automatic speech recognition as it\nenhances readability and impacts downstream natural language processing tasks.\nIn streaming scenarios, the ability to predict punctuation in real-time is\nparticularly desirable but presents a difficult technical challenge. In this\nwork, we propose a method for predicting punctuated text from input speech\nusing a chunk-based Transformer encoder trained with Connectionist Temporal\nClassification (CTC) loss. The acoustic model trained with long sequences by\nconcatenating the input and target sequences can learn punctuation marks\nattached to the end of sentences more effectively. Additionally, by combining\nCTC losses on the chunks and utterances, we achieved both the improved F1 score\nof punctuation prediction and Word Error Rate (WER).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1\">Hanbyul Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seo_S/0/1/0/all/0/1\">Seunghyun Seo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_L/0/1/0/all/0/1\">Lukas Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baek_S/0/1/0/all/0/1\">Seolki Baek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DistilXLSR: A Light Weight Cross-Lingual Speech Representation Model. (arXiv:2306.01303v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01303","description":"<p>Multilingual self-supervised speech representation models have greatly\nenhanced the speech recognition performance for low-resource languages, and the\ncompression of these huge models has also become a crucial prerequisite for\ntheir industrial application. In this paper, we propose DistilXLSR, a distilled\ncross-lingual speech representation model. By randomly shuffling the phonemes\nof existing speech, we reduce the linguistic information and distill\ncross-lingual models using only English data. We also design a layer-jumping\ninitialization method to fully leverage the teacher's pre-trained weights.\nExperiments on 2 kinds of teacher models and 15 low-resource languages show\nthat our method can reduce the parameters by 50% while maintaining\ncross-lingual representation ability. Our method is proven to be generalizable\nto various languages/teacher models and has the potential to improve the\ncross-lingual performance of the English pre-trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei-Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jinfeng Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaVL: Transferring In-Context Learning Ability From Language Models to Vision-Language Models. (arXiv:2306.01311v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01311","description":"<p>Large-scale language models have shown the ability to adapt to a new task via\nconditioning on a few demonstrations (i.e., in-context learning). However, in\nthe vision-language domain, most large-scale pre-trained vision-language (VL)\nmodels do not possess the ability to conduct in-context learning. How can we\nenable in-context learning for VL models? In this paper, we study an\ninteresting hypothesis: can we transfer the in-context learning ability from\nthe language domain to VL domain? Specifically, we first meta-trains a language\nmodel to perform in-context learning on NLP tasks (as in MetaICL); then we\ntransfer this model to perform VL tasks by attaching a visual encoder. Our\nexperiments suggest that indeed in-context learning ability can be transferred\ncross modalities: our model considerably improves the in-context learning\ncapability on VL tasks and can even compensate for the size of the model\nsignificantly. On VQA, OK-VQA, and GQA, our method could outperform the\nbaseline model while having 20 times fewer parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Monajatipoor_M/0/1/0/all/0/1\">Masoud Monajatipoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouhsedaghat_M/0/1/0/all/0/1\">Mozhdeh Rouhsedaghat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lin F. Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syntax-aware Hybrid prompt model for Few-shot multi-modal sentiment analysis. (arXiv:2306.01312v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01312","description":"<p>Multimodal Sentiment Analysis (MSA) has been a popular topic in natural\nlanguage processing nowadays, at both sentence and aspect level. However, the\nexisting approaches almost require large-size labeled datasets, which bring\nabout large consumption of time and resources. Therefore, it is practical to\nexplore the method for few-shot sentiment analysis in cross-modalities.\nPrevious works generally execute on textual modality, using the prompt-based\nmethods, mainly two types: hand-crafted prompts and learnable prompts. The\nexisting approach in few-shot multi-modality sentiment analysis task has\nutilized both methods, separately. We further design a hybrid pattern that can\ncombine one or more fixed hand-crafted prompts and learnable prompts and\nutilize the attention mechanisms to optimize the prompt encoder. The\nexperiments on both sentence-level and aspect-level datasets prove that we get\na significant outperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zikai Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Style Transfer Back-Translation. (arXiv:2306.01318v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01318","description":"<p>Back Translation (BT) is widely used in the field of machine translation, as\nit has been proved effective for enhancing translation quality. However, BT\nmainly improves the translation of inputs that share a similar style (to be\nmore specific, translation-like inputs), since the source side of BT data is\nmachine-translated. For natural inputs, BT brings only slight improvements and\nsometimes even adverse effects. To address this issue, we propose Text Style\nTransfer Back Translation (TST BT), which uses a style transfer model to modify\nthe source side of BT data. By making the style of source-side text more\nnatural, we aim to improve the translation of natural inputs. Our experiments\non various language pairs, including both high-resource and low-resource ones,\ndemonstrate that TST BT significantly improves translation performance against\npopular BT benchmarks. In addition, TST BT is proved to be effective in domain\nadaptation so this strategy can be regarded as a general data augmentation\nmethod. Our training code and text style transfer model are open-sourced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Daimeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhanglin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_H/0/1/0/all/0/1\">Hengchao Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zongyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Minghan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaxin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhengzhe Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LyricSIM: A novel Dataset and Benchmark for Similarity Detection in Spanish Song LyricS. (arXiv:2306.01325v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01325","description":"<p>In this paper, we present a new dataset and benchmark tailored to the task of\nsemantic similarity in song lyrics. Our dataset, originally consisting of 2775\npairs of Spanish songs, was annotated in a collective annotation experiment by\n63 native annotators. After collecting and refining the data to ensure a high\ndegree of consensus and data integrity, we obtained 676 high-quality annotated\npairs that were used to evaluate the performance of various state-of-the-art\nmonolingual and multilingual language models. Consequently, we established\nbaseline results that we hope will be useful to the community in all future\nacademic and industrial applications conducted in this context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benito_Santos_A/0/1/0/all/0/1\">Alejandro Benito-Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghajari_A/0/1/0/all/0/1\">Adri&#xe1;n Ghajari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_P/0/1/0/all/0/1\">Pedro Hern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fresno_V/0/1/0/all/0/1\">V&#xed;ctor Fresno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ros_S/0/1/0/all/0/1\">Salvador Ros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Blanco_E/0/1/0/all/0/1\">Elena Gonz&#xe1;lez-Blanco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Translation with Foundation Models and Optimal Transport: UPC at IWSLT23. (arXiv:2306.01327v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01327","description":"<p>This paper describes the submission of the UPC Machine Translation group to\nthe IWSLT 2023 Offline Speech Translation task. Our Speech Translation systems\nutilize foundation models for speech (wav2vec 2.0) and text (mBART50). We\nincorporate a Siamese pretraining step of the speech and text encoders with CTC\nand Optimal Transport, to adapt the speech representations to the space of the\ntext model, thus maximizing transfer learning from MT. After this pretraining,\nwe fine-tune our system end-to-end on ST, with Cross Entropy and Knowledge\nDistillation. Apart from the available ST corpora, we create synthetic data\nwith SegAugment to better adapt our models to the custom segmentations of the\nIWSLT test sets. Our best single model obtains 31.2 BLEU points on MuST-C\ntst-COMMON, 29.8 points on IWLST.tst2020 and 33.4 points on the newly released\nIWSLT.ACLdev2023.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsiamas_I/0/1/0/all/0/1\">Ioannis Tsiamas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Gerard I. G&#xe1;llego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonollosa_J/0/1/0/all/0/1\">Jos&#xe9; A. R. Fonollosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study on Challenging Math Problem Solving with GPT-4. (arXiv:2306.01337v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01337","description":"<p>Employing Large Language Models (LLMs) to address mathematical problems is an\nintriguing research endeavor, considering the abundance of math problems\nexpressed in natural language across numerous science and engineering fields.\nWhile several prior works have investigated solving elementary mathematics\nusing LLMs, this work explores the frontier of using GPT-4 for solving more\ncomplex and challenging math problems. We evaluate various ways of using GPT-4.\nSome of them are adapted from existing work, and one is \\MathChat, a\nconversational problem-solving framework newly proposed in this work. We\nperform the evaluation on difficult high school competition problems from the\nMATH dataset, which shows the advantage of the proposed conversational\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yiran Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_F/0/1/0/all/0/1\">Feiran Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaokun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qingyun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hangyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1\">Erkang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yin Tat Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1\">Richard Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Auxiliary Domain Parallel Data in Intermediate Task Fine-tuning for Low-resource Translation. (arXiv:2306.01382v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01382","description":"<p>NMT systems trained on Pre-trained Multilingual Sequence-Sequence (PMSS)\nmodels flounder when sufficient amounts of parallel data is not available for\nfine-tuning. This specifically holds for languages missing/under-represented in\nthese models. The problem gets aggravated when the data comes from different\ndomains. In this paper, we show that intermediate-task fine-tuning (ITFT) of\nPMSS models is extremely beneficial for domain-specific NMT, especially when\ntarget domain data is limited/unavailable and the considered languages are\nmissing or under-represented in the PMSS model. We quantify the domain-specific\nresults variations using a domain-divergence test, and show that ITFT can\nmitigate the impact of domain divergence to some extent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_S/0/1/0/all/0/1\">Shravan Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranathunga_S/0/1/0/all/0/1\">Surangika Ranathunga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thillainathan_S/0/1/0/all/0/1\">Sarubi Thillainathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_R/0/1/0/all/0/1\">Rikki Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rinaldi_A/0/1/0/all/0/1\">Anthony Rinaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yining Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mackey_J/0/1/0/all/0/1\">Jonah Mackey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_A/0/1/0/all/0/1\">Andrew Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1\">En-Shiun Annie Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Agnostic Structured Pruning of Speech Representation Models. (arXiv:2306.01385v1 [eess.AS])","link":"http://arxiv.org/abs/2306.01385","description":"<p>Self-supervised pre-trained models such as Wav2vec2, Hubert, and WavLM have\nbeen shown to significantly improve many speech tasks. However, their large\nmemory and strong computational requirements hinder their industrial\napplicability. Structured pruning is a hardware-friendly model compression\ntechnique but usually results in a larger loss of accuracy. In this paper, we\npropose a fine-grained attention head pruning method to compensate for the\nperformance degradation. In addition, we also introduce the straight through\nestimator into the L0 regularization to further accelerate the pruned model.\nExperiments on the SUPERB benchmark show that our model can achieve comparable\nperformance to the dense model in multiple tasks and outperforms the Wav2vec\n2.0 base model on average, with 72% fewer parameters and 2 times faster\ninference speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Siyuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Wei-Qiang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suo_H/0/1/0/all/0/1\">Hongbin Suo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wan_Y/0/1/0/all/0/1\">Yulong Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT for Zero-shot Dialogue State Tracking: A Solution or an Opportunity?. (arXiv:2306.01386v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01386","description":"<p>Recent research on dialogue state tracking (DST) focuses on methods that\nallow few- and zero-shot transfer to new domains or schemas. However,\nperformance gains heavily depend on aggressive data augmentation and\nfine-tuning of ever larger language model based architectures. In contrast,\ngeneral purpose language models, trained on large amounts of diverse data, hold\nthe promise of solving any kind of task without task-specific training. We\npresent preliminary experimental results on the ChatGPT research preview,\nshowing that ChatGPT achieves state-of-the-art performance in zero-shot DST.\nDespite our findings, we argue that properties inherent to general purpose\nmodels limit their ability to replace specialized systems. We further theorize\nthat the in-context learning capabilities of such models will likely become\npowerful tools to support the development of dedicated and dynamic dialogue\nstate trackers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heck_M/0/1/0/all/0/1\">Michael Heck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lubis_N/0/1/0/all/0/1\">Nurul Lubis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruppik_B/0/1/0/all/0/1\">Benjamin Ruppik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vukovic_R/0/1/0/all/0/1\">Renato Vukovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geishauser_C/0/1/0/all/0/1\">Christian Geishauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hsien-Chin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekerk_C/0/1/0/all/0/1\">Carel van Niekerk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasic_M/0/1/0/all/0/1\">Milica Ga&#x161;i&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing the Importance of Frequency versus Compositionality for Subword-based Tokenization in NMT. (arXiv:2306.01393v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01393","description":"<p>Subword tokenization is the de facto standard for tokenization in neural\nlanguage models and machine translation systems. Three advantages are\nfrequently cited in favor of subwords: shorter encoding of frequent tokens,\ncompositionality of subwords, and ability to deal with unknown words. As their\nrelative importance is not entirely clear yet, we propose a tokenization\napproach that enables us to separate frequency (the first advantage) from\ncompositionality. The approach uses Huffman coding to tokenize words, by order\nof frequency, using a fixed amount of symbols. Experiments with CS-DE, EN-FR\nand EN-DE NMT show that frequency alone accounts for 90%-95% of the scores\nreached by BPE, hence compositionality has less importance than previously\nthought.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolleb_B/0/1/0/all/0/1\">Benoist Wolleb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silvestri_R/0/1/0/all/0/1\">Romain Silvestri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vernikos_G/0/1/0/all/0/1\">Giorgos Vernikos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_Belis_L/0/1/0/all/0/1\">Ljiljana Dolamic Andrei Popescu-Belis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph Reasoning over Entities and Numerical Values. (arXiv:2306.01399v1 [cs.AI])","link":"http://arxiv.org/abs/2306.01399","description":"<p>A complex logic query in a knowledge graph refers to a query expressed in\nlogic form that conveys a complex meaning, such as where did the Canadian\nTuring award winner graduate from? Knowledge graph reasoning-based\napplications, such as dialogue systems and interactive search engines, rely on\nthe ability to answer complex logic queries as a fundamental task. In most\nknowledge graphs, edges are typically used to either describe the relationships\nbetween entities or their associated attribute values. An attribute value can\nbe in categorical or numerical format, such as dates, years, sizes, etc.\nHowever, existing complex query answering (CQA) methods simply treat numerical\nvalues in the same way as they treat entities. This can lead to difficulties in\nanswering certain queries, such as which Australian Pulitzer award winner is\nborn before 1927, and which drug is a pain reliever and has fewer side effects\nthan Paracetamol. In this work, inspired by the recent advances in numerical\nencoding and knowledge graph reasoning, we propose numerical complex query\nanswering. In this task, we introduce new numerical variables and operations to\ndescribe queries involving numerical attribute values. To address the\ndifference between entities and numerical values, we also propose the framework\nof Number Reasoning Network (NRN) for alternatively encoding entities and\nnumerical values into separate encoding structures. During the numerical\nencoding process, NRN employs a parameterized density function to encode the\ndistribution of numerical values. During the entity encoding process, NRN uses\nestablished query encoding methods for the original CQA problem. Experimental\nresults show that NRN consistently improves various query encoding methods on\nthree different knowledge graphs and achieves state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiaxin Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1\">Qingyu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable and Explainable Logical Policies via Neurally Guided Symbolic Abstraction. (arXiv:2306.01439v1 [cs.LG])","link":"http://arxiv.org/abs/2306.01439","description":"<p>The limited priors required by neural networks make them the dominating\nchoice to encode and learn policies using reinforcement learning (RL). However,\nthey are also black-boxes, making it hard to understand the agent's behaviour,\nespecially when working on the image level. Therefore, neuro-symbolic RL aims\nat creating policies that are interpretable in the first place. Unfortunately,\ninterpretability is not explainability. To achieve both, we introduce Neurally\ngUided Differentiable loGic policiEs (NUDGE). NUDGE exploits trained neural\nnetwork-based agents to guide the search of candidate-weighted logic rules,\nthen uses differentiable logic to train the logic agents. Our experimental\nevaluation demonstrates that NUDGE agents can induce interpretable and\nexplainable policies while outperforming purely neural ones and showing good\nflexibility to environments of different initial states and problem sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Delfosse_Q/0/1/0/all/0/1\">Quentin Delfosse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shindo_H/0/1/0/all/0/1\">Hikaru Shindo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhami_D/0/1/0/all/0/1\">Devendra Dhami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust FastSpeech 2 by Modelling Residual Multimodality. (arXiv:2306.01442v1 [cs.SD])","link":"http://arxiv.org/abs/2306.01442","description":"<p>State-of-the-art non-autoregressive text-to-speech (TTS) models based on\nFastSpeech 2 can efficiently synthesise high-fidelity and natural speech. For\nexpressive speech datasets however, we observe characteristic audio\ndistortions. We demonstrate that such artefacts are introduced to the vocoder\nreconstruction by over-smooth mel-spectrogram predictions, which are induced by\nthe choice of mean-squared-error (MSE) loss for training the mel-spectrogram\ndecoder. With MSE loss FastSpeech 2 is limited to learn conditional averages of\nthe training distribution, which might not lie close to a natural sample if the\ndistribution still appears multimodal after all conditioning signals. To\nalleviate this problem, we introduce TVC-GMM, a mixture model of\nTrivariate-Chain Gaussian distributions, to model the residual multimodality.\nTVC-GMM reduces spectrogram smoothness and improves perceptual audio quality in\nparticular for expressive datasets as shown by both objective and subjective\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kogel_F/0/1/0/all/0/1\">Fabian K&#xf6;gel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Bac Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardinaux_F/0/1/0/all/0/1\">Fabien Cardinaux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Paraphrasing of Multiword Expressions. (arXiv:2306.01443v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01443","description":"<p>We propose an unsupervised approach to paraphrasing multiword expressions\n(MWEs) in context. Our model employs only monolingual corpus data and\npre-trained language models (without fine-tuning), and does not make use of any\nexternal resources such as dictionaries. We evaluate our method on the SemEval\n2022 idiomatic semantic text similarity task, and show that it outperforms all\nunsupervised systems and rivals supervised systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wada_T/0/1/0/all/0/1\">Takashi Wada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsumoto_Y/0/1/0/all/0/1\">Yuji Matsumoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Extractive Summarization of Emotion Triggers. (arXiv:2306.01444v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01444","description":"<p>Understanding what leads to emotions during large-scale crises is important\nas it can provide groundings for expressed emotions and subsequently improve\nthe understanding of ongoing disasters. Recent approaches trained supervised\nmodels to both detect emotions and explain emotion triggers (events and\nappraisals) via abstractive summarization. However, obtaining timely and\nqualitative abstractive summaries is expensive and extremely time-consuming,\nrequiring highly-trained expert annotators. In time-sensitive, high-stake\ncontexts, this can block necessary responses. We instead pursue unsupervised\nsystems that extract triggers from text. First, we introduce CovidET-EXT,\naugmenting (Zhan et al. 2022)'s abstractive dataset (in the context of the\nCOVID-19 crisis) with extractive triggers. Second, we develop new unsupervised\nlearning models that can jointly detect emotions and summarize their triggers.\nOur best approach, entitled Emotion-Aware Pagerank, incorporates emotion\ninformation from external sources combined with a language understanding\nmodule, and outperforms strong baselines. We release our data and code at\nhttps://github.com/tsosea2/CovidET-EXT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sosea_T/0/1/0/all/0/1\">Tiberiu Sosea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1\">Hongli Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1\">Cornelia Caragea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Driving Context into Text-to-Text Privatization. (arXiv:2306.01457v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01457","description":"<p>\\textit{Metric Differential Privacy} enables text-to-text privatization by\nadding calibrated noise to the vector of a word derived from an embedding space\nand projecting this noisy vector back to a discrete vocabulary using a nearest\nneighbor search. Since words are substituted without context, this mechanism is\nexpected to fall short at finding substitutes for words with ambiguous\nmeanings, such as \\textit{'bank'}. To account for these ambiguous words, we\nleverage a sense embedding and incorporate a sense disambiguation step prior to\nnoise injection. We encompass our modification to the privatization mechanism\nwith an estimation of privacy and utility. For word sense disambiguation on the\n\\textit{Words in Context} dataset, we demonstrate a substantial increase in\nclassification accuracy by $6.05\\%$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arnold_S/0/1/0/all/0/1\">Stefan Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yesilbas_D/0/1/0/all/0/1\">Dilara Yesilbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinzierl_S/0/1/0/all/0/1\">Sven Weinzierl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Light Coreference Resolution for Russian with Hierarchical Discourse Features. (arXiv:2306.01465v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01465","description":"<p>Coreference resolution is the task of identifying and grouping mentions\nreferring to the same real-world entity. Previous neural models have mainly\nfocused on learning span representations and pairwise scores for coreference\ndecisions. However, current methods do not explicitly capture the referential\nchoice in the hierarchical discourse, an important factor in coreference\nresolution. In this study, we propose a new approach that incorporates\nrhetorical information into neural coreference resolution models. We collect\nrhetorical features from automated discourse parses and examine their impact.\nAs a base model, we implement an end-to-end span-based coreference resolver\nusing a partially fine-tuned multilingual entity-aware language model LUKE. We\nevaluate our method on the RuCoCo-23 Shared Task for coreference resolution in\nRussian. Our best model employing rhetorical distance between mentions has\nranked 1st on the development set (74.6% F1) and 2nd on the test set (73.3% F1)\nof the Shared Task. We hope that our work will inspire further research on\nincorporating discourse information in neural coreference resolution models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chistova_E/0/1/0/all/0/1\">Elena Chistova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smirnov_I/0/1/0/all/0/1\">Ivan Smirnov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guiding Text-to-Text Privatization by Syntax. (arXiv:2306.01471v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01471","description":"<p>Metric Differential Privacy is a generalization of differential privacy\ntailored to address the unique challenges of text-to-text privatization. By\nadding noise to the representation of words in the geometric space of\nembeddings, words are replaced with words located in the proximity of the noisy\nrepresentation. Since embeddings are trained based on word co-occurrences, this\nmechanism ensures that substitutions stem from a common semantic context.\nWithout considering the grammatical category of words, however, this mechanism\ncannot guarantee that substitutions play similar syntactic roles. We analyze\nthe capability of text-to-text privatization to preserve the grammatical\ncategory of words after substitution and find that surrogate texts consist\nalmost exclusively of nouns. Lacking the capability to produce surrogate texts\nthat correlate with the structure of the sensitive texts, we encompass our\nanalysis by transforming the privatization step into a candidate selection\nproblem in which substitutions are directed to words with matching grammatical\nproperties. We demonstrate a substantial improvement in the performance of\ndownstream tasks by up to $4.66\\%$ while retaining comparative privacy\nguarantees.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arnold_S/0/1/0/all/0/1\">Stefan Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yesilbas_D/0/1/0/all/0/1\">Dilara Yesilbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinzierl_S/0/1/0/all/0/1\">Sven Weinzierl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAIA Search: Hugging Face and Pyserini Interoperability for NLP Training Data Exploration. (arXiv:2306.01481v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01481","description":"<p>Noticing the urgent need to provide tools for fast and user-friendly\nqualitative analysis of large-scale textual corpora of the modern NLP, we\npropose to turn to the mature and well-tested methods from the domain of\nInformation Retrieval (IR) - a research field with a long history of tackling\nTB-scale document collections. We discuss how Pyserini - a widely used toolkit\nfor reproducible IR research can be integrated with the Hugging Face ecosystem\nof open-source AI libraries and artifacts. We leverage the existing\nfunctionalities of both platforms while proposing novel features further\nfacilitating their integration. Our goal is to give NLP researchers tools that\nwill allow them to develop retrieval-based instrumentation for their data\nanalytics needs with ease and agility. We include a Jupyter Notebook-based walk\nthrough the core interoperability features, available on GitHub at\nhttps://github.com/huggingface/gaia. We then demonstrate how the ideas we\npresent can be operationalized to create a powerful tool for qualitative data\nanalysis in NLP. We present GAIA Search - a search engine built following\npreviously laid out principles, giving access to four popular large-scale text\ncollections. GAIA serves a dual purpose of illustrating the potential of\nmethodologies we discuss but also as a standalone qualitative analysis tool\nthat can be leveraged by NLP researchers aiming to understand datasets prior to\nusing them in training. GAIA is hosted live on Hugging Face Spaces -\nhttps://huggingface.co/spaces/spacerini/gaia.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Piktus_A/0/1/0/all/0/1\">Aleksandra Piktus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogundepo_O/0/1/0/all/0/1\">Odunayo Ogundepo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akiki_C/0/1/0/all/0/1\">Christopher Akiki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oladipo_A/0/1/0/all/0/1\">Akintunde Oladipo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoelkopf_H/0/1/0/all/0/1\">Hailey Schoelkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Efficient French Language Modeling with CamemBERTa. (arXiv:2306.01497v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01497","description":"<p>Recent advances in NLP have significantly improved the performance of\nlanguage models on a variety of tasks. While these advances are largely driven\nby the availability of large amounts of data and computational power, they also\nbenefit from the development of better training methods and architectures. In\nthis paper, we introduce CamemBERTa, a French DeBERTa model that builds upon\nthe DeBERTaV3 architecture and training objective. We evaluate our model's\nperformance on a variety of French downstream tasks and datasets, including\nquestion answering, part-of-speech tagging, dependency parsing, named entity\nrecognition, and the FLUE benchmark, and compare against CamemBERT, the\nstate-of-the-art monolingual model for French. Our results show that, given the\nsame amount of training tokens, our model outperforms BERT-based models trained\nwith MLM on most tasks. Furthermore, our new model reaches similar or superior\nperformance on downstream tasks compared to CamemBERT, despite being trained on\nonly 30% of its total number of input tokens. In addition to our experimental\nresults, we also publicly release the weights and code implementation of\nCamemBERTa, making it the first publicly available DeBERTaV3 model outside of\nthe original paper and the first openly available implementation of a DeBERTaV3\ntraining objective. https://gitlab.inria.fr/almanach/CamemBERTa\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antoun_W/0/1/0/all/0/1\">Wissam Antoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Beno&#xee;t Sagot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seddah_D/0/1/0/all/0/1\">Djam&#xe9; Seddah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can LLMs like GPT-4 outperform traditional AI tools in dementia diagnosis? Maybe, but not today. (arXiv:2306.01499v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01499","description":"<p>Recent investigations show that large language models (LLMs), specifically\nGPT-4, not only have remarkable capabilities in common Natural Language\nProcessing (NLP) tasks but also exhibit human-level performance on various\nprofessional and academic benchmarks. However, whether GPT-4 can be directly\nused in practical applications and replace traditional artificial intelligence\n(AI) tools in specialized domains requires further experimental validation. In\nthis paper, we explore the potential of LLMs such as GPT-4 to outperform\ntraditional AI tools in dementia diagnosis. Comprehensive comparisons between\nGPT-4 and traditional AI tools are conducted to examine their diagnostic\naccuracy in a clinical setting. Experimental results on two real clinical\ndatasets show that, although LLMs like GPT-4 demonstrate potential for future\nadvancements in dementia diagnosis, they currently do not surpass the\nperformance of traditional AI tools. The interpretability and faithfulness of\nGPT-4 are also evaluated by comparison with real doctors. We discuss the\nlimitations of GPT-4 in its current state and propose future research\ndirections to enhance GPT-4 in dementia diagnosis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rongzhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bowen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiuxing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1\">Chenhui Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Liling Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations. (arXiv:2306.01505v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01505","description":"<p>Extracting generalized and robust representations is a major challenge in\nemotion recognition in conversations (ERC). To address this, we propose a\nsupervised adversarial contrastive learning (SACL) framework for learning\nclass-spread structured representations. The framework applies contrast-aware\nadversarial training to generate worst-case samples and uses a joint\nclass-spread contrastive learning objective on both original and adversarial\nsamples. It can effectively utilize label-level feature consistency and retain\nfine-grained intra-class features. To avoid the negative impact of adversarial\nperturbations on context-dependent data, we design a contextual adversarial\ntraining strategy to learn more diverse features from context and enhance the\nmodel's context robustness. We develop a sequence-based method SACL-LSTM under\nthis framework, to learn label-consistent and context-robust emotional features\nfor ERC. Experiments on three datasets demonstrate that SACL-LSTM achieves\nstate-of-the-art performance on ERC. Extended experiments prove the\neffectiveness of the SACL framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Dou Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yinan Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Lingwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songlin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models. (arXiv:2306.01506v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01506","description":"<p>Self-supervised techniques for learning speech representations have been\nshown to develop linguistic competence from exposure to speech without the need\nfor human labels. In order to fully realize the potential of these approaches\nand further our understanding of how infants learn language, simulations must\nclosely emulate real-life situations by training on developmentally plausible\ncorpora and benchmarking against appropriate test sets. To this end, we propose\na language-acquisition-friendly benchmark to probe spoken language models at\nthe lexical and syntactic levels, both of which are compatible with the\nvocabulary typical of children's language experiences. This paper introduces\nthe benchmark and summarizes a range of experiments showing its usefulness. In\naddition, we highlight two exciting challenges that need to be addressed for\nfurther progress: bridging the gap between text and speech and between clean\nspeech and in-the-wild speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lavechin_M/0/1/0/all/0/1\">Marvin Lavechin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sy_Y/0/1/0/all/0/1\">Yaya Sy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titeux_H/0/1/0/all/0/1\">Hadrien Titeux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blandon_M/0/1/0/all/0/1\">Mar&#xed;a Andrea Cruz Bland&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1\">Okko R&#xe4;s&#xe4;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bredin_H/0/1/0/all/0/1\">Herv&#xe9; Bredin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristia_A/0/1/0/all/0/1\">Alejandrina Cristia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PassGPT: Password Modeling and (Guided) Generation with Large Language Models. (arXiv:2306.01545v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01545","description":"<p>Large language models (LLMs) successfully model natural language from vast\namounts of text without the need for explicit supervision. In this paper, we\ninvestigate the efficacy of LLMs in modeling passwords. We present PassGPT, a\nLLM trained on password leaks for password generation. PassGPT outperforms\nexisting methods based on generative adversarial networks (GAN) by guessing\ntwice as many previously unseen passwords. Furthermore, we introduce the\nconcept of guided password generation, where we leverage PassGPT sampling\nprocedure to generate passwords matching arbitrary constraints, a feat lacking\nin current GAN-based strategies. Lastly, we conduct an in-depth analysis of the\nentropy and probability distribution that PassGPT defines over passwords and\ndiscuss their use in enhancing existing password strength estimators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rando_J/0/1/0/all/0/1\">Javier Rando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Cruz_F/0/1/0/all/0/1\">Fernando Perez-Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hitaj_B/0/1/0/all/0/1\">Briland Hitaj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Machine Translation Quality with Conformal Predictive Distributions. (arXiv:2306.01549v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01549","description":"<p>This paper presents a new approach for assessing uncertainty in machine\ntranslation by simultaneously evaluating translation quality and providing a\nreliable confidence score. Our approach utilizes conformal predictive\ndistributions to produce prediction intervals with guaranteed coverage, meaning\nthat for any given significance level $\\epsilon$, we can expect the true\nquality score of a translation to fall out of the interval at a rate of\n$1-\\epsilon$. In this paper, we demonstrate how our method outperforms a\nsimple, but effective baseline on six different language pairs in terms of\ncoverage and sharpness. Furthermore, we validate that our approach requires the\ndata exchangeability assumption to hold for optimal performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giovannotti_P/0/1/0/all/0/1\">Patrizio Giovannotti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing a composite model versus chained models to locate a nearest visual object. (arXiv:2306.01551v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01551","description":"<p>Extracting information from geographic images and text is crucial for\nautonomous vehicles to determine in advance the best cell stations to connect\nto along their future path. Multiple artificial neural network models can\naddress this challenge; however, there is no definitive guidance on the\nselection of an appropriate model for such use cases. Therefore, we\nexperimented two architectures to solve such a task: a first architecture with\nchained models where each model in the chain addresses a sub-task of the task;\nand a second architecture with a single model that addresses the whole task.\nOur results showed that these two architectures achieved the same level\nperformance with a root mean square error (RMSE) of 0.055 and 0.056; The\nfindings further revealed that when the task can be decomposed into sub-tasks,\nthe chain architecture exhibits a twelve-fold increase in training speed\ncompared to the composite model. Nevertheless, the composite model\nsignificantly alleviates the burden of data labeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borgne_A/0/1/0/all/0/1\">Antoine Le Borgne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marjou_X/0/1/0/all/0/1\">Xavier Marjou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parzysz_F/0/1/0/all/0/1\">Fanny Parzysz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lemlouma_T/0/1/0/all/0/1\">Tayeb Lemlouma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EmoUS: Simulating User Emotions in Task-Oriented Dialogues. (arXiv:2306.01579v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01579","description":"<p>Existing user simulators (USs) for task-oriented dialogue systems only model\nuser behaviour on semantic and natural language levels without considering the\nuser persona and emotions. Optimising dialogue systems with generic user\npolicies, which cannot model diverse user behaviour driven by different\nemotional states, may result in a high drop-off rate when deployed in the real\nworld. Thus, we present EmoUS, a user simulator that learns to simulate user\nemotions alongside user behaviour. EmoUS generates user emotions, semantic\nactions, and natural language responses based on the user goal, the dialogue\nhistory, and the user persona. By analysing what kind of system behaviour\nelicits what kind of user emotions, we show that EmoUS can be used as a probe\nto evaluate a variety of dialogue systems and in particular their effect on the\nuser's emotional state. Developing such methods is important in the age of\nlarge language model chat-bots and rising ethical concerns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hsien-Chin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geishauser_C/0/1/0/all/0/1\">Christian Geishauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lubis_N/0/1/0/all/0/1\">Nurul Lubis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekerk_C/0/1/0/all/0/1\">Carel van Niekerk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heck_M/0/1/0/all/0/1\">Michael Heck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruppik_B/0/1/0/all/0/1\">Benjamin Ruppik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vukovic_R/0/1/0/all/0/1\">Renato Vukovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasic_M/0/1/0/all/0/1\">Milica Ga&#x161;i&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Partially Annotated Data: Example-aware Creation of Gap-filling Exercises for Language Learning. (arXiv:2306.01584v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01584","description":"<p>Since performing exercises (including, e.g., practice tests) forms a crucial\ncomponent of learning, and creating such exercises requires non-trivial effort\nfrom the teacher. There is a great value in automatic exercise generation in\ndigital tools in education. In this paper, we particularly focus on automatic\ncreation of gapfilling exercises for language learning, specifically grammar\nexercises. Since providing any annotation in this domain requires human expert\neffort, we aim to avoid it entirely and explore the task of converting existing\ntexts into new gap-filling exercises, purely based on an example exercise,\nwithout explicit instruction or detailed annotation of the intended grammar\ntopics. We contribute (i) a novel neural network architecture specifically\ndesigned for aforementioned gap-filling exercise generation task, and (ii) a\nreal-world benchmark dataset for French grammar. We show that our model for\nthis French grammar gap-filling exercise generation outperforms a competitive\nbaseline classifier by 8% in F1 percentage points, achieving an average F1\nscore of 82%. Our model implementation and the dataset are made publicly\navailable to foster future research, thus offering a standardized evaluation\nand baseline solution of the proposed partially annotated data prediction task\nin grammar exercise creation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bitew_S/0/1/0/all/0/1\">Semere Kiros Bitew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deleu_J/0/1/0/all/0/1\">Johannes Deleu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dogruoz_A/0/1/0/all/0/1\">A. Seza Dogru&#xf6;z</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Develder_C/0/1/0/all/0/1\">Chris Develder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1\">Thomas Demeester</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation. (arXiv:2306.01657v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01657","description":"<p>Empathy is a crucial factor in open-domain conversations, which naturally\nshows one's caring and understanding to others. Though several methods have\nbeen proposed to generate empathetic responses, existing works often lead to\nmonotonous empathy that refers to generic and safe expressions. In this paper,\nwe propose to use explicit control to guide the empathy expression and design a\nframework DiffusEmp based on conditional diffusion language model to unify the\nutilization of dialogue context and attribute-oriented control signals.\nSpecifically, communication mechanism, intent, and semantic frame are imported\nas multi-grained signals that control the empathy realization from coarse to\nfine levels. We then design a specific masking strategy to reflect the\nrelationship between multi-grained signals and response tokens, and integrate\nit into the diffusion model to influence the generative process. Experimental\nresults on a benchmark dataset EmpatheticDialogue show that our framework\noutperforms competitive baselines in terms of controllability, informativeness,\nand diversity without the loss of context-relatedness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bi_G/0/1/0/all/0/1\">Guanqun Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yanan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Meng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuqiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Grained Human Feedback Gives Better Rewards for Language Model Training. (arXiv:2306.01693v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01693","description":"<p>Language models (LMs) often exhibit undesirable text generation behaviors,\nincluding generating false, toxic, or irrelevant outputs. Reinforcement\nlearning from human feedback (RLHF) - where human preference judgments on LM\noutputs are transformed into a learning signal - has recently shown promise in\naddressing these issues. However, such holistic feedback conveys limited\ninformation on long text outputs; it does not indicate which aspects of the\noutputs influenced user preference; e.g., which parts contain what type(s) of\nerrors. In this paper, we use fine-grained human feedback (e.g., which sentence\nis false, which sub-sentence is irrelevant) as an explicit training signal. We\nintroduce Fine-Grained RLHF, a framework that enables training and learning\nfrom reward functions that are fine-grained in two respects: (1) density,\nproviding a reward after every segment (e.g., a sentence) is generated; and (2)\nincorporating multiple reward models associated with different feedback types\n(e.g., factual incorrectness, irrelevance, and information incompleteness). We\nconduct experiments on detoxification and long-form question answering to\nillustrate how learning with such reward functions leads to improved\nperformance, supported by both automatic and human evaluation. Additionally, we\nshow that LM behaviors can be customized using different combinations of\nfine-grained reward models. We release all data, collected human feedback, and\ncodes at https://FineGrainedRLHF.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zeqiu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yushi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weijia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziri_N/0/1/0/all/0/1\">Nouha Dziri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhr_A/0/1/0/all/0/1\">Alane Suhr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1\">Mari Ostendorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Multi-step Reasoning from Arithmetic Task. (arXiv:2306.01707v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01707","description":"<p>Mathematical reasoning is regarded as a necessary ability for Language Models\n(LMs). Recent works demonstrate large LMs' impressive performance in solving\nmath problems. The success is attributed to their Chain-of-Thought (CoT)\nreasoning abilities, i.e., the ability to decompose complex questions into\nstep-by-step reasoning chains, but such ability seems only to emerge from\nmodels with abundant parameters. This work investigates how to incorporate\nrelatively small LMs with the capabilities of multi-step reasoning. We propose\nto inject such abilities by continually pre-training LMs on a synthetic dataset\nMsAT, which stands for Multi-step Arithmetic Task. Our experiments on four math\nword problem datasets show the effectiveness of the proposed method in\nenhancing LMs' math reasoning abilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianduo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resolving Interference When Merging Models. (arXiv:2306.01708v1 [cs.LG])","link":"http://arxiv.org/abs/2306.01708","description":"<p>Transfer learning - i.e., further fine-tuning a pre-trained model on a\ndownstream task - can confer significant advantages, including improved\ndownstream performance, faster convergence, and better sample efficiency. These\nadvantages have led to a proliferation of task-specific fine-tuned models,\nwhich typically can only perform a single task and do not benefit from one\nanother. Recently, model merging techniques have emerged as a solution to\ncombine multiple task-specific models into a single multitask model without\nperforming additional training. However, existing merging methods often ignore\nthe interference between parameters of different models, resulting in large\nperformance drops when merging multiple models. In this paper, we demonstrate\nthat prior merging techniques inadvertently lose valuable information due to\ntwo major sources of interference: (a) interference due to redundant parameter\nvalues and (b) disagreement on the sign of a given parameter's values across\nmodels. To address this, we propose our method, TrIm, Elect Sign &amp; Merge\n(TIES-Merging), which introduces three novel steps when merging models: (1)\nresetting parameters that only changed a small amount during fine-tuning, (2)\nresolving sign conflicts, and (3) merging only the parameters that are in\nalignment with the final agreed-upon sign. We find that TIES-Merging\noutperforms several existing methods in diverse settings covering a range of\nmodalities, domains, number of tasks, model sizes, architectures, and\nfine-tuning settings. We further analyze the impact of different types of\ninterference on model parameters, highlight the importance of resolving sign\ninterference. Our code is available at\nhttps://github.com/prateeky2806/ties-merging\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1\">Prateek Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tam_D/0/1/0/all/0/1\">Derek Tam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Efficient Language-Specific Models for Cross-Lingual Transfer. (arXiv:2306.01709v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01709","description":"<p>Massively multilingual Transformers (MMTs), such as mBERT and XLM-R, are\nwidely used for cross-lingual transfer learning. While these are pretrained to\nrepresent hundreds of languages, end users of NLP systems are often interested\nonly in individual languages. For such purposes, the MMTs' language coverage\nmakes them unnecessarily expensive to deploy in terms of model size, inference\ntime, energy, and hardware cost. We thus propose to extract compressed,\nlanguage-specific models from MMTs which retain the capacity of the original\nMMTs for cross-lingual transfer. This is achieved by distilling the MMT\nbilingually, i.e., using data from only the source and target language of\ninterest. Specifically, we use a two-phase distillation approach, termed\nBiStil: (i) the first phase distils a general bilingual model from the MMT,\nwhile (ii) the second, task-specific phase sparsely fine-tunes the bilingual\n\"student\" model using a task-tuned variant of the original MMT as its\n\"teacher\". We evaluate this distillation technique in zero-shot cross-lingual\ntransfer across a number of standard cross-lingual benchmarks. The key results\nindicate that the distilled models exhibit minimal degradation in target\nlanguage performance relative to the base MMT despite being significantly\nsmaller and faster. Furthermore, we find that they outperform multilingually\ndistilled models such as DistilmBERT and MiniLMv2 while having a very modest\ntraining budget in comparison, even on a per-language basis. We also show that\nbilingual models distilled from MMTs greatly outperform bilingual models\ntrained from scratch. Our code and models are available at\nhttps://github.com/AlanAnsell/bistil.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ansell_A/0/1/0/all/0/1\">Alan Ansell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo Maria Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Generalization in Task-oriented Dialogues with Workflows and Action Plans. (arXiv:2306.01729v1 [cs.CL])","link":"http://arxiv.org/abs/2306.01729","description":"<p>Task-oriented dialogue is difficult in part because it involves understanding\nuser intent, collecting information from the user, executing API calls, and\ngenerating helpful and fluent responses. However, for complex tasks one must\nalso correctly do all of these things over multiple steps, and in a specific\norder. While large pre-trained language models can be fine-tuned end-to-end to\ncreate multi-step task-oriented dialogue agents that generate fluent text, our\nexperiments confirm that this approach alone cannot reliably perform new\nmulti-step tasks that are unseen during training. To address these limitations,\nwe augment the dialogue contexts given to \\textmd{text2text} transformers with\nknown \\textit{valid workflow names} and \\textit{action plans}. Action plans\nconsist of sequences of actions required to accomplish a task, and are encoded\nas simple sequences of keywords (e.g. verify-identity, pull-up-account,\nreset-password, etc.). We perform extensive experiments on the Action-Based\nConversations Dataset (ABCD) with T5-small, base and large models, and show\nthat such models: a) are able to more readily generalize to unseen workflows by\nfollowing the provided plan, and b) are able to generalize to executing unseen\nactions if they are provided in the plan. In contrast, models are unable to\nfully accomplish new multi-step tasks when they are not provided action plan\ninformation, even when given new valid workflow names.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raimondo_S/0/1/0/all/0/1\">Stefania Raimondo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaotian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazquez_D/0/1/0/all/0/1\">David Vazquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palacios_H/0/1/0/all/0/1\">Hector Palacios</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lessons on Parameter Sharing across Layers in Transformers. (arXiv:2104.06022v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06022","description":"<p>We propose a parameter sharing method for Transformers (Vaswani et al.,\n2017). The proposed approach relaxes a widely used technique, which shares\nparameters for one layer with all layers such as Universal Transformers\n(Dehghani et al., 2019), to increase the efficiency in the computational time.\nWe propose three strategies: Sequence, Cycle, and Cycle (rev) to assign\nparameters to each layer. Experimental results show that the proposed\nstrategies are efficient in the parameter size and computational time.\nMoreover, we indicate that the proposed strategies are also effective in the\nconfiguration where we use many training data such as the recent WMT\ncompetition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Takase_S/0/1/0/all/0/1\">Sho Takase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiyono_S/0/1/0/all/0/1\">Shun Kiyono</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syntax-Aware Graph-to-Graph Transformer for Semantic Role Labelling. (arXiv:2104.07704v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07704","description":"<p>Recent models have shown that incorporating syntactic knowledge into the\nsemantic role labelling (SRL) task leads to a significant improvement. In this\npaper, we propose Syntax-aware Graph-to-Graph Transformer (SynG2G-Tr) model,\nwhich encodes the syntactic structure using a novel way to input graph\nrelations as embeddings, directly into the self-attention mechanism of\nTransformer. This approach adds a soft bias towards attention patterns that\nfollow the syntactic structure but also allows the model to use this\ninformation to learn alternative patterns. We evaluate our model on both\nspan-based and dependency-based SRL datasets, and outperform previous\nalternative methods in both in-domain and out-of-domain settings, on CoNLL 2005\nand CoNLL 2009 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadshahi_A/0/1/0/all/0/1\">Alireza Mohammadshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PhysNLU: A Language Resource for Evaluating Natural Language Understanding and Explanation Coherence in Physics. (arXiv:2201.04275v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.04275","description":"<p>In order for language models to aid physics research, they must first encode\nrepresentations of mathematical and natural language discourse which lead to\ncoherent explanations, with correct ordering and relevance of statements. We\npresent a collection of datasets developed to evaluate the performance of\nlanguage models in this regard, which measure capabilities with respect to\nsentence ordering, position, section prediction, and discourse coherence.\nAnalysis of the data reveals equations and sub-disciplines which are most\ncommon in physics discourse, as well as the sentence-level frequency of\nequations and expressions. We present baselines that demonstrate how\ncontemporary language models are challenged by coherence related tasks in\nphysics, even when trained on mathematical natural language objectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meadows_J/0/1/0/all/0/1\">Jordan Meadows</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zili Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andre Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BertNet: Harvesting Knowledge Graphs with Arbitrary Relations from Pretrained Language Models. (arXiv:2206.14268v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.14268","description":"<p>It is crucial to automatically construct knowledge graphs (KGs) of diverse\nnew relations to support knowledge discovery and broad applications. Previous\nKG construction methods, based on either crowdsourcing or text mining, are\noften limited to a small predefined set of relations due to manual cost or\nrestrictions in text corpus. Recent research proposed to use pretrained\nlanguage models (LMs) as implicit knowledge bases that accept knowledge queries\nwith prompts. Yet, the implicit knowledge lacks many desirable properties of a\nfull-scale symbolic KG, such as easy access, navigation, editing, and quality\nassurance. In this paper, we propose a new approach of harvesting massive KGs\nof arbitrary relations from pretrained LMs. With minimal input of a relation\ndefinition (a prompt and a few shot of example entity pairs), the approach\nefficiently searches in the vast entity pair space to extract diverse accurate\nknowledge of the desired relation. We develop an effective search-and-rescore\nmechanism for improved efficiency and accuracy. We deploy the approach to\nharvest KGs of over 400 new relations from different LMs. Extensive human and\nautomatic evaluations show our approach manages to extract diverse accurate\nknowledge, including tuples of complex relations (e.g., \"A is capable of but\nnot good at B\"). The resulting KGs as a symbolic interpretation of the source\nLMs also reveal new insights into the LMs' knowledge capacities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1\">Shibo Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1\">Bowen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Kaiwen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bin Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_X/0/1/0/all/0/1\">Xiyan Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hengzhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ThinkSum: Probabilistic reasoning over sets using large language models. (arXiv:2210.01293v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.01293","description":"<p>Large language models (LLMs) have a substantial capacity for high-level\nanalogical reasoning: reproducing patterns in linear text that occur in their\ntraining data (zero-shot evaluation) or in the provided context (few-shot\nin-context learning). However, recent studies show that even the more advanced\nLLMs fail in scenarios that require reasoning over multiple objects or facts\nand making sequences of logical deductions. We propose a two-stage\nprobabilistic inference paradigm, ThinkSum, which reasons over sets of objects\nor facts in a structured manner. In the first stage (Think - retrieval of\nassociations), a LLM is queried in parallel over a set of phrases extracted\nfrom the prompt or an auxiliary model call. In the second stage (Sum -\nprobabilistic inference or reasoning), the results of these queries are\naggregated to make the final prediction. We demonstrate the possibilities and\nadvantages of ThinkSum on the BIG-bench suite of LLM evaluation tasks,\nachieving improvements over the state of the art using GPT-family models on\nthirteen difficult tasks, often with far smaller model variants. We also\ncompare and contrast ThinkSum with other proposed modifications to direct\nprompting of LLMs, such as variants of chain-of-thought prompting. Our results\nsuggest that because the probabilistic inference in ThinkSum is performed\noutside of calls to the LLM, ThinkSum is less sensitive to prompt design,\nyields more interpretable predictions, and can be flexibly combined with latent\nvariable models to extract structured knowledge from LLMs. Overall, our\nproposed paradigm represents a promising approach for enhancing the reasoning\ncapabilities of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ozturkler_B/0/1/0/all/0/1\">Batu Ozturkler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malkin_N/0/1/0/all/0/1\">Nikolay Malkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jojic_N/0/1/0/all/0/1\">Nebojsa Jojic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Detoxifying and Debiasing in Language Generation via Inference-time Adaptive Optimization. (arXiv:2210.04492v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04492","description":"<p>Warning: this paper contains model outputs exhibiting offensiveness and\nbiases. Recently pre-trained language models (PLMs) have prospered in various\nnatural language generation (NLG) tasks due to their ability to generate fairly\nfluent text. Nevertheless, these models are observed to capture and reproduce\nharmful contents in training corpora, typically toxic language and social\nbiases, raising severe moral issues. Prior works on ethical NLG tackle\ndetoxifying and debiasing separately, which is problematic since we find\ndebiased models still exhibit toxicity while detoxified ones even exacerbate\nsocial biases. To address such a challenge, we propose the first unified\nframework of detoxifying and debiasing called UDDIA, which jointly formalizes\nthese two problems as rectifying the output space. We theoretically interpret\nour framework as learning a text distribution mixing weighted attributes.\nBesides, UDDIA conducts adaptive optimization of only a few parameters during\ndecoding based on a parameter-efficient tuning schema without any training\ndata. This leads to minimal generation quality loss and improved rectification\nperformance with acceptable computational cost. Experimental results\ndemonstrate that compared to several strong baselines, UDDIA achieves debiasing\nand detoxifying simultaneously and better balances efficiency and\neffectiveness, taking a further step towards practical ethical NLG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zonghan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1\">Xiaoyuan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REV: Information-Theoretic Evaluation of Free-Text Rationales. (arXiv:2210.04982v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04982","description":"<p>Generating free-text rationales is a promising step towards explainable NLP,\nyet evaluating such rationales remains a challenge. Existing metrics have\nmostly focused on measuring the association between the rationale and a given\nlabel. We argue that an ideal metric should focus on the new information\nuniquely provided in the rationale that is otherwise not provided in the input\nor the label. We investigate this research problem from an\ninformation-theoretic perspective using conditional V-information (Hewitt et\nal., 2021). More concretely, we propose a metric called REV (Rationale\nEvaluation with conditional V-information), to quantify the amount of new,\nlabel-relevant information in a rationale beyond the information already\navailable in the input or the label. Experiments across four benchmarks with\nreasoning tasks, including chain-of-thought, demonstrate the effectiveness of\nREV in evaluating rationale-label pairs, compared to existing metrics. We\nfurther demonstrate REV is consistent with human judgments on rationale\nevaluations and provides more sensitive measurements of new information in\nfree-text rationales. When used alongside traditional performance metrics, REV\nprovides deeper insights into models' reasoning and prediction processes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Easy Guided Decoding in Providing Suggestions for Interactive Machine Translation. (arXiv:2211.07093v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07093","description":"<p>Machine translation technology has made great progress in recent years, but\nit cannot guarantee error free results. Human translators perform post editing\non machine translations to correct errors in the scene of computer aided\ntranslation. In favor of expediting the post editing process, many works have\ninvestigated machine translation in interactive modes, in which machines can\nautomatically refine the rest of translations constrained by human's edits.\nTranslation Suggestion (TS), as an interactive mode to assist human\ntranslators, requires machines to generate alternatives for specific incorrect\nwords or phrases selected by human translators. In this paper, we utilize the\nparameterized objective function of neural machine translation (NMT) and\npropose a novel constrained decoding algorithm, namely Prefix Suffix Guided\nDecoding (PSGD), to deal with the TS problem without additional training.\nCompared to the state of the art lexically constrained decoding method, PSGD\nimproves translation quality by an average of $10.87$ BLEU and $8.62$ BLEU on\nthe WeTS and the WMT 2022 Translation Suggestion datasets, respectively, and\nreduces decoding time overhead by an average of 63.4% tested on the WMT\ntranslation datasets. Furthermore, on both of the TS benchmark datasets, it is\nsuperior to other supervised learning systems trained with TS annotated data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Ke Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1\">Xin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiayi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuqi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why Did the Chicken Cross the Road? Rephrasing and Analyzing Ambiguous Questions in VQA. (arXiv:2211.07516v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.07516","description":"<p>Natural language is ambiguous. Resolving ambiguous questions is key to\nsuccessfully answering them. Focusing on questions about images, we create a\ndataset of ambiguous examples. We annotate these, grouping answers by the\nunderlying question they address and rephrasing the question for each group to\nreduce ambiguity. Our analysis reveals a linguistically-aligned ontology of\nreasons for ambiguity in visual questions. We then develop an English\nquestion-generation model which we demonstrate via automatic and human\nevaluation produces less ambiguous questions. We further show that the question\ngeneration objective we use allows the model to integrate answer group\ninformation without any direct supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stengel_Eskin_E/0/1/0/all/0/1\">Elias Stengel-Eskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guallar_Blasco_J/0/1/0/all/0/1\">Jimena Guallar-Blasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text. (arXiv:2211.11300v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.11300","description":"<p>Self-supervised representation learning has proved to be a valuable component\nfor out-of-distribution (OoD) detection with only the texts of in-distribution\n(ID) examples. These approaches either train a language model from scratch or\nfine-tune a pre-trained language model using ID examples, and then take the\nperplexity output by the language model as OoD scores. In this paper, we\nanalyze the complementary characteristics of both OoD detection methods and\npropose a multi-level knowledge distillation approach that integrates their\nstrengths while mitigating their limitations. Specifically, we use a fine-tuned\nmodel as the teacher to teach a randomly initialized student model on the ID\nexamples. Besides the prediction layer distillation, we present a\nsimilarity-based intermediate layer distillation method to thoroughly explore\nthe representation space of the teacher model. In this way, the learned student\ncan better represent the ID data manifold while gaining a stronger ability to\nmap OoD examples outside the ID data manifold with the regularization inherited\nfrom pre-training. Besides, the student model sees only ID examples during\nparameter learning, further promoting more distinguishable features for OoD\ndetection. We conduct extensive experiments over multiple benchmark datasets,\ni.e., CLINC150, SST, ROSTD, 20 NewsGroups, and AG News; showing that the\nproposed method yields new state-of-the-art performance. We also explore its\napplication as an AIGC detector to distinguish between answers generated by\nChatGPT and human experts. It is observed that our model exceeds human\nevaluators in the pair-expert task on the Human ChatGPT Comparison Corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qianhui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Huiqiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Haonan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_B/0/1/0/all/0/1\">B&#xf6;rje F. Karlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chin-Yew Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Federated Learning Meets Pre-trained Language Models' Parameter-Efficient Tuning Methods. (arXiv:2212.10025v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2212.10025","description":"<p>With increasing privacy concerns on data, recent studies have made\nsignificant progress using federated learning (FL) on privacy-sensitive natural\nlanguage processing (NLP) tasks. Much literature suggests fully fine-tuning\npre-trained language models (PLMs) in the FL paradigm can mitigate the data\nheterogeneity problem and close the performance gap with centralized training.\nHowever, large PLMs bring the curse of prohibitive communication overhead and\nlocal model adaptation costs for the FL system. To this end, we introduce\nvarious parameter-efficient tuning (PETuning) methods into federated learning.\nSpecifically, we provide a holistic empirical study of representative PLMs\ntuning methods in FL. The experimental results cover the analysis of data\nheterogeneity levels, data scales, and different FL scenarios. Overall\ncommunication overhead can be significantly reduced by locally tuning and\nglobally aggregating lightweight model parameters while maintaining acceptable\nperformance in various FL settings. To facilitate the research of PETuning in\nFL, we also develop a federated tuning framework FedPETuning, which allows\npractitioners to exploit different PETuning methods under the FL training\nparadigm conveniently. The source code is available at\n\\url{https://github.com/iezhuozhuo/FedETuning/tree/deltaTuning}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuanhang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lizhen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4. (arXiv:2212.10114v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10114","description":"<p>Large language models (LLMs) have demonstrated solid zero-shot reasoning\ncapabilities, which is reflected in their performance on the current test\ntasks. This calls for a more challenging benchmark requiring highly advanced\nreasoning ability to be solved. In this paper, we introduce such a benchmark,\nconsisting of 191 long-form (1200 words on average) mystery narratives\nconstructed as detective puzzles. Puzzles are sourced from the \"5 Minute\nMystery\" platform and include a multiple-choice question for evaluation. Only\n47% of humans solve a puzzle successfully on average, while the best human\nsolvers achieve over 80% success rate. We show that GPT-3 models barely\noutperform random on this benchmark (with 28% accuracy) while state-of-the-art\nGPT-4 solves only 38% of puzzles. This indicates that there is still a\nsignificant gap in the deep reasoning abilities of LLMs and humans and\nhighlights the need for further research in this area. Our work introduces a\nchallenging benchmark for future studies on reasoning in language models and\ncontributes to a better understanding of the limits of LLMs' abilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Del_M/0/1/0/all/0/1\">Maksym Del</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fishel_M/0/1/0/all/0/1\">Mark Fishel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shades of Iteration: from Elgot to Kleene. (arXiv:2301.06202v2 [cs.LO] UPDATED)","link":"http://arxiv.org/abs/2301.06202","description":"<p>Notions of iteration range from the arguably most general Elgot iteration to\na very specific Kleene iteration. The fundamental nature of Elgot iteration has\nbeen extensively explored by Bloom and Esik in the form of iteration theories,\nwhile Kleene iteration became extremely popular as an integral part of\n(untyped) formalisms, such as automata theory, regular expressions and Kleene\nalgebra. Here, we establish a formal connection between Elgot iteration and\nKleene iteration in the form of Elgot monads and Kleene monads, respectively.\nWe also introduce a novel class of while-monads, which like Kleene monads admit\na relatively simple description in algebraic terms. Like Elgot monads,\nwhile-monads cover a large variety of models that meaningfully support\nwhile-loops, but may fail the Kleene algebra laws, or even fail to support a\nKleen iteration operator altogether.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goncharov_S/0/1/0/all/0/1\">Sergey Goncharov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT: Jack of all trades, master of none. (arXiv:2302.10724v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.10724","description":"<p>OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and\nrevolutionized the approach in artificial intelligence to human-model\ninteraction. Several publications on ChatGPT evaluation test its effectiveness\non well-known natural language processing (NLP) tasks. However, the existing\nstudies are mostly non-automated and tested on a very limited scale. In this\nwork, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks,\nmost of them subjective even to humans, such as sentiment analysis, emotion\nrecognition, offensiveness, and stance detection. In contrast, the other tasks\nrequire more objective reasoning like word sense disambiguation, linguistic\nacceptability, and question answering. We also evaluated GPT-4 model on five\nselected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process\nand analyzed more than 49k responses. Our comparison of its results with\navailable State-of-the-Art (SOTA) solutions showed that the average loss in\nquality of the ChatGPT model was about 25% for zero-shot and few-shot\nevaluation. For GPT-4 model, a loss for semantic tasks is significantly lower\nthan for ChatGPT. We showed that the more difficult the task (lower SOTA\nperformance), the higher the ChatGPT loss. It especially refers to pragmatic\nNLP problems like emotion recognition. We also tested the ability to\npersonalize ChatGPT responses for selected subjective tasks via Random\nContextual Few-Shot Personalization, and we obtained significantly better\nuser-based predictions. Additional qualitative analysis revealed a ChatGPT\nbias, most likely due to the rules imposed on human trainers by OpenAI. Our\nresults provide the basis for a fundamental discussion of whether the high\nquality of recent predictive NLP models can indicate a tool's usefulness to\nsociety and how the learning and validation procedures for such systems should\nbe established.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kocon_J/0/1/0/all/0/1\">Jan Koco&#x144;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cichecki_I/0/1/0/all/0/1\">Igor Cichecki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaszyca_O/0/1/0/all/0/1\">Oliwier Kaszyca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochanek_M/0/1/0/all/0/1\">Mateusz Kochanek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szydlo_D/0/1/0/all/0/1\">Dominika Szyd&#x142;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baran_J/0/1/0/all/0/1\">Joanna Baran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielaniewicz_J/0/1/0/all/0/1\">Julita Bielaniewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gruza_M/0/1/0/all/0/1\">Marcin Gruza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janz_A/0/1/0/all/0/1\">Arkadiusz Janz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanclerz_K/0/1/0/all/0/1\">Kamil Kanclerz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocon_A/0/1/0/all/0/1\">Anna Koco&#x144;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koptyra_B/0/1/0/all/0/1\">Bart&#x142;omiej Koptyra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mieleszczenko_Kowszewicz_W/0/1/0/all/0/1\">Wiktoria Mieleszczenko-Kowszewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milkowski_P/0/1/0/all/0/1\">Piotr Mi&#x142;kowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oleksy_M/0/1/0/all/0/1\">Marcin Oleksy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piasecki_M/0/1/0/all/0/1\">Maciej Piasecki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radlinski_L/0/1/0/all/0/1\">&#x141;ukasz Radli&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wojtasik_K/0/1/0/all/0/1\">Konrad Wojtasik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wozniak_S/0/1/0/all/0/1\">Stanis&#x142;aw Wo&#x17a;niak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazienko_P/0/1/0/all/0/1\">Przemys&#x142;aw Kazienko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fluid Transformers and Creative Analogies: Exploring Large Language Models' Capacity for Augmenting Cross-Domain Analogical Creativity. (arXiv:2302.12832v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.12832","description":"<p>Cross-domain analogical reasoning is a core creative ability that can be\nchallenging for humans. Recent work has shown some proofs-of concept of Large\nlanguage Models' (LLMs) ability to generate cross-domain analogies. However,\nthe reliability and potential usefulness of this capacity for augmenting human\ncreative work has received little systematic exploration. In this paper, we\nsystematically explore LLMs capacity to augment cross-domain analogical\nreasoning. Across three studies, we found: 1) LLM-generated cross-domain\nanalogies were frequently judged as helpful in the context of a problem\nreformulation task (median 4 out of 5 helpfulness rating), and frequently (~80%\nof cases) led to observable changes in problem formulations, and 2) there was\nan upper bound of 25% of outputs bring rated as potentially harmful, with a\nmajority due to potentially upsetting content, rather than biased or toxic\ncontent. These results demonstrate the potential utility -- and risks -- of\nLLMs for augmenting cross-domain analogical creativity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zijian Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_A/0/1/0/all/0/1\">Arvind Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacNeil_S/0/1/0/all/0/1\">Stephen MacNeil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1\">Joel Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"N-best T5: Robust ASR Error Correction using Multiple Input Hypotheses and Constrained Decoding Space. (arXiv:2303.00456v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.00456","description":"<p>Error correction models form an important part of Automatic Speech\nRecognition (ASR) post-processing to improve the readability and quality of\ntranscriptions. Most prior works use the 1-best ASR hypothesis as input and\ntherefore can only perform correction by leveraging the context within one\nsentence. In this work, we propose a novel N-best T5 model for this task, which\nis fine-tuned from a T5 model and utilizes ASR N-best lists as model input. By\ntransferring knowledge from the pre-trained language model and obtaining richer\ninformation from the ASR decoding space, the proposed approach outperforms a\nstrong Conformer-Transducer baseline. Another issue with standard error\ncorrection is that the generation process is not well-guided. To address this a\nconstrained decoding process, either based on the N-best list or an ASR\nlattice, is used which allows additional information to be propagated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark J. F. Gales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knill_K/0/1/0/all/0/1\">Kate M. Knill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_M/0/1/0/all/0/1\">Mengjie Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quick Dense Retrievers Consume KALE: Post Training Kullback Leibler Alignment of Embeddings for Asymmetrical dual encoders. (arXiv:2304.01016v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01016","description":"<p>In this paper, we consider the problem of improving the inference latency of\nlanguage model-based dense retrieval systems by introducing structural\ncompression and model size asymmetry between the context and query encoders.\nFirst, we investigate the impact of pre and post-training compression on the\nMSMARCO, Natural Questions, TriviaQA, SQUAD, and SCIFACT, finding that\nasymmetry in the dual encoders in dense retrieval can lead to improved\ninference efficiency. Knowing this, we introduce Kullback Leibler Alignment of\nEmbeddings (KALE), an efficient and accurate method for increasing the\ninference efficiency of dense retrieval methods by pruning and aligning the\nquery encoder after training. Specifically, KALE extends traditional Knowledge\nDistillation after bi-encoder training, allowing for effective query encoder\ncompression without full retraining or index generation. Using KALE and\nasymmetric training, we can generate models which exceed the performance of\nDistilBERT despite having 3x faster inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Campos_D/0/1/0/all/0/1\">Daniel Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magnani_A/0/1/0/all/0/1\">Alessandro Magnani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Possibilities of AI-Generated Text Detection. (arXiv:2304.04736v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.04736","description":"<p>Our work focuses on the challenge of detecting outputs generated by Large\nLanguage Models (LLMs) to distinguish them from those generated by humans. This\nability is of the utmost importance in numerous applications. However, the\npossibility of such discernment has been the subject of debate within the\ncommunity. Therefore, a central question is whether we can detect AI-generated\ntext and, if so, when. In this work, we provide evidence that it should almost\nalways be possible to detect AI-generated text unless the distributions of\nhuman and machine-generated texts are exactly the same over the entire support.\nThis observation follows from the standard results in information theory and\nrelies on the fact that if the machine text becomes more human-like, we need\nmore samples to detect it. We derive a precise sample complexity bound of\nAI-generated text detection, which tells how many samples are needed to detect\nAI-generated text. This gives rise to additional challenges of designing more\ncomplicated detectors that take in $n$ samples for detection (rather than just\none), which is the scope of future research on this topic. Our empirical\nevaluations on various real and synthetic datasets support our claim about the\nexistence of better detectors, demonstrating that AI-generated text detection\nshould be achievable in the majority of scenarios. Our theory and results align\nwith OpenAI's empirical findings, (in relation to sequence length), and we are\nthe first to provide a solid theoretical justification for these outcomes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Souradip Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bedi_A/0/1/0/all/0/1\">Amrit Singh Bedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Sicheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1\">Bang An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Furong Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end spoken language understanding using joint CTC loss and self-supervised, pretrained acoustic encoders. (arXiv:2305.02937v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.02937","description":"<p>It is challenging to extract semantic meanings directly from audio signals in\nspoken language understanding (SLU), due to the lack of textual information.\nPopular end-to-end (E2E) SLU models utilize sequence-to-sequence automatic\nspeech recognition (ASR) models to extract textual embeddings as input to infer\nsemantics, which, however, require computationally expensive auto-regressive\ndecoding. In this work, we leverage self-supervised acoustic encoders\nfine-tuned with Connectionist Temporal Classification (CTC) to extract textual\nembeddings and use joint CTC and SLU losses for utterance-level SLU tasks.\nExperiments show that our model achieves 4% absolute improvement over the the\nstate-of-the-art (SOTA) dialogue act classification model on the DSTC2 dataset\nand 1.3% absolute improvement over the SOTA SLU model on the SLURP dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jixuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radfar_M/0/1/0/all/0/1\">Martin Radfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1\">Kai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_C/0/1/0/all/0/1\">Clement Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProKnow: Process Knowledge for Safety Constrained and Explainable Question Generation for Mental Health Diagnostic Assistance. (arXiv:2305.08010v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08010","description":"<p>Current Virtual Mental Health Assistants (VMHAs) provide counseling and\nsuggestive care. They refrain from patient diagnostic assistance because they\nlack training in safety-constrained and specialized clinical process knowledge.\nIn this work, we define Proknow as an ordered set of information that maps to\nevidence-based guidelines or categories of conceptual understanding to experts\nin a domain. We also introduce a new dataset of diagnostic conversations guided\nby safety constraints and Proknow that healthcare professionals use. We develop\na method for natural language question generation (NLG) that collects\ndiagnostic information from the patient interactively. We demonstrate the\nlimitations of using state-of-the-art large-scale language models (LMs) on this\ndataset. Our algorithm models the process knowledge through explicitly modeling\nsafety, knowledge capture, and explainability. LMs augmented with ProKnow\nguided method generated 89% safer questions in the depression and anxiety\ndomain. The Explainability of the generated question is assessed by computing\nsimilarity with concepts in depression and anxiety knowledge bases. Overall,\nirrespective of the type of LMs augmented with our ProKnow, we achieved an\naverage 82% improvement over simple pre-trained LMs on safety, explainability,\nand process-guided question generation. We qualitatively and quantitatively\nevaluate the efficacy of the proposed ProKnow-guided methods by introducing\nthree new evaluation metrics for safety, explainability, and process knowledge\nadherence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_M/0/1/0/all/0/1\">Manas Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltani_M/0/1/0/all/0/1\">Misagh Soltani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawte_V/0/1/0/all/0/1\">Vipula Rawte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1\">Ashwin Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Consistency in Text-based Financial Forecasting Models. (arXiv:2305.08524v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08524","description":"<p>Financial forecasting has been an important and active area of machine\nlearning research, as even the most modest advantage in predictive accuracy can\nbe parlayed into significant financial gains. Recent advances in natural\nlanguage processing (NLP) bring the opportunity to leverage textual data, such\nas earnings reports of publicly traded companies, to predict the return rate\nfor an asset. However, when dealing with such a sensitive task, the consistency\nof models -- their invariance under meaning-preserving alternations in input --\nis a crucial property for building user trust. Despite this, current financial\nforecasting methods do not consider consistency. To address this problem, we\npropose FinTrust, an evaluation tool that assesses logical consistency in\nfinancial text. Using FinTrust, we show that the consistency of\nstate-of-the-art NLP models for financial forecasting is poor. Our analysis of\nthe performance degradation caused by meaning-preserving alternations suggests\nthat current text-based methods are not suitable for robustly predicting market\ninformation. All resources are available at\nhttps://github.com/yingpengma/fintrust.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yingpeng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"I'm fully who I am\": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.09941","description":"<p>Transgender and non-binary (TGNB) individuals disproportionately experience\ndiscrimination and exclusion from daily life. Given the recent popularity and\nadoption of language generation technologies, the potential to further\nmarginalize this population only grows. Although a multitude of NLP fairness\nliterature focuses on illuminating and addressing gender biases, assessing\ngender harms for TGNB identities requires understanding how such identities\nuniquely interact with societal gender norms and how they differ from gender\nbinary-centric perspectives. Such measurement frameworks inherently require\ncentering TGNB voices to help guide the alignment between gender-inclusive NLP\nand whom they are intended to serve. Towards this goal, we ground our work in\nthe TGNB community and existing interdisciplinary literature to assess how the\nsocial reality surrounding experienced marginalization of TGNB persons\ncontributes to and persists within Open Language Generation (OLG). This social\nknowledge serves as a guide for evaluating popular large language models (LLMs)\non two key aspects: (1) misgendering and (2) harmful responses to gender\ndisclosure. To do this, we introduce TANGO, a dataset of template-based\nreal-world text curated from a TGNB-oriented community. We discover a dominance\nof binary gender norms reflected by the models; LLMs least misgendered subjects\nin generated text when triggered by prompts whose subjects used binary\npronouns. Meanwhile, misgendering was most prevalent when triggering generation\nwith singular they and neopronouns. When prompted with gender disclosures, TGNB\ndisclosure generated the most stigmatizing language and scored most toxic, on\naverage. Our findings warrant further research on how TGNB harms manifest in\nLLMs and serve as a broader case study toward concretely grounding the design\nof gender-inclusive AI in community voices and interdisciplinary literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ovalle_A/0/1/0/all/0/1\">Anaelia Ovalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Palash Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamala_J/0/1/0/all/0/1\">Jwala Dhamala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggers_Z/0/1/0/all/0/1\">Zachary Jaggers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark. (arXiv:2305.10036v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10036","description":"<p>Large language models (LLMs) have demonstrated powerful capabilities in both\ntext understanding and generation. Companies have begun to offer Embedding as a\nService (EaaS) based on these LLMs, which can benefit various natural language\nprocessing (NLP) tasks for customers. However, previous studies have shown that\nEaaS is vulnerable to model extraction attacks, which can cause significant\nlosses for the owners of LLMs, as training these models is extremely expensive.\nTo protect the copyright of LLMs for EaaS, we propose an Embedding Watermark\nmethod called EmbMarker that implants backdoors on embeddings. Our method\nselects a group of moderate-frequency words from a general text corpus to form\na trigger set, then selects a target embedding as the watermark, and inserts it\ninto the embeddings of texts containing trigger words as the backdoor. The\nweight of insertion is proportional to the number of trigger words included in\nthe text. This allows the watermark backdoor to be effectively transferred to\nEaaS-stealer's model for copyright verification while minimizing the adverse\nimpact on the original embeddings' utility. Our extensive experiments on\nvarious datasets show that our method can effectively protect the copyright of\nEaaS models without compromising service quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wenjun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jingwei Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shangxi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lingjuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_B/0/1/0/all/0/1\">Binxing Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangzhong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Off-Target Problem of Zero-Shot Multilingual Neural Machine Translation. (arXiv:2305.10930v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10930","description":"<p>While multilingual neural machine translation has achieved great success, it\nsuffers from the off-target issue, where the translation is in the wrong\nlanguage. This problem is more pronounced on zero-shot translation tasks. In\nthis work, we find that failing in encoding discriminative target language\nsignal will lead to off-target and a closer lexical distance (i.e.,\nKL-divergence) between two languages' vocabularies is related with a higher\noff-target rate. We also find that solely isolating the vocab of different\nlanguages in the decoder can alleviate the problem. Motivated by the findings,\nwe propose Language Aware Vocabulary Sharing (LAVS), a simple and effective\nalgorithm to construct the multilingual vocabulary, that greatly alleviates the\noff-target problem of the translation model by increasing the KL-divergence\nbetween languages. We conduct experiments on a multilingual machine translation\nbenchmark in 11 languages. Experiments show that the off-target rate for 90\ntranslation tasks is reduced from 29\\% to 8\\%, while the overall BLEU score is\nimproved by an average of 1.9 points without extra training cost or sacrificing\nthe supervised directions' performance. We release the code at\nhttps://github.com/PKUnlp-icler/Off-Target-MNMT for reproduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing Non-Autoregressive Transformers with Contrastive Learning. (arXiv:2305.13667v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13667","description":"<p>Non-autoregressive Transformers (NATs) reduce the inference latency of\nAutoregressive Transformers (ATs) by predicting words all at once rather than\nin sequential order. They have achieved remarkable progress in machine\ntranslation as well as many other applications. However, a long-standing\nchallenge for NATs is the learning of multi-modality data distribution, which\nis the main cause of the performance gap between NATs and ATs. In this paper,\nwe propose to ease the difficulty of modality learning via sampling from the\nmodel distribution instead of the data distribution. We derive contrastive\nconstraints to stabilize the training process and integrate this resulting\nobjective with the state-of-the-art NAT architecture DA-Transformer. Our model\n\\method is examined on 3 different tasks, including machine translation, text\nsummarization, and paraphrasing with 5 benchmarks. Results show that our\napproach outperforms previous non-autoregressive baselines by a significant\nmargin and establishes new state-of-the-art results for non-autoregressive\ntransformers on all the benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_C/0/1/0/all/0/1\">Chenxin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiangtao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations. (arXiv:2305.14728v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14728","description":"<p>Although deep language representations have become the dominant form of\nlanguage featurization in recent years, in many settings it is important to\nunderstand a model's decision-making process. This necessitates not only an\ninterpretable model but also interpretable features. In particular, language\nmust be featurized in a way that is interpretable while still characterizing\nthe original text well. We present SenteCon, a method for introducing human\ninterpretability in deep language representations. Given a passage of text,\nSenteCon encodes the text as a layer of interpretable categories in which each\ndimension corresponds to the relevance of a specific category. Our empirical\nevaluations indicate that encoding language with SenteCon provides high-level\ninterpretability at little to no cost to predictive performance on downstream\ntasks. Moreover, we find that SenteCon outperforms existing interpretable\nlanguage representations with respect to both its downstream performance and\nits agreement with human characterizations of the text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_V/0/1/0/all/0/1\">Victoria Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linguistic Properties of Truthful Response. (arXiv:2305.15875v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.15875","description":"<p>We investigate the phenomenon of an LLM's untruthful response using a large\nset of 220 handcrafted linguistic features. We focus on GPT-3 models and find\nthat the linguistic profiles of responses are similar across model sizes. That\nis, how varying-sized LLMs respond to given prompts stays similar on the\nlinguistic properties level. We expand upon this finding by training support\nvector machines that rely only upon the stylistic components of model responses\nto classify the truthfulness of statements. Though the dataset size limits our\ncurrent findings, we show the possibility that truthfulness detection is\npossible without evaluating the content itself. But at the same time, the\nlimited scope of our experiments must be taken into account in interpreting the\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bruce W. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arockiaraj_B/0/1/0/all/0/1\">Benedict Florance Arockiaraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Helen Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Non-Autoregressive Translation at Scale. (arXiv:2305.16155v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16155","description":"<p>In real-world systems, scaling has been critical for improving the\ntranslation quality in autoregressive translation (AT), which however has not\nbeen well studied for non-autoregressive translation (NAT). In this work, we\nbridge the gap by systematically studying the impact of scaling on NAT\nbehaviors. Extensive experiments on six WMT benchmarks over two advanced NAT\nmodels show that scaling can alleviate the commonly-cited weaknesses of NAT\nmodels, resulting in better translation performance. To reduce the side-effect\nof scaling on decoding speed, we empirically investigate the impact of NAT\nencoder and decoder on the translation performance. Experimental results on the\nlarge-scale WMT20 En-De show that the asymmetric architecture (e.g. bigger\nencoder and smaller decoder) can achieve comparable performance with the\nscaling model, while maintaining the superiority of decoding speed with\nstandard NAT models. To this end, we establish a new benchmark by validating\nscaled NAT models on the scaled dataset, which can be regarded as a strong\nbaseline for future works. We release code and system outputs at\nhttps://github.com/DeepLearnXMU/Scaling4NAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Junfeng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art. (arXiv:2305.16259v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16259","description":"<p>The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural\nLanguage Processing (NLP) during the past decade. However, the demands of long\ndocument analysis are quite different from those of shorter texts, while the\never increasing size of documents uploaded on-line renders automated\nunderstanding of long texts a critical area of research. This article has two\ngoals: a) it overviews the relevant neural building blocks, thus serving as a\nshort tutorial, and b) it surveys the state-of-the-art in long document NLP,\nmainly focusing on two central tasks: document classification and document\nsummarization. Sentiment analysis for long texts is also covered, since it is\ntypically treated as a particular case of document classification.\nAdditionally, this article discusses the main challenges, issues and current\nsolutions related to long document NLP. Finally, the relevant, publicly\navailable, annotated datasets are presented, in order to facilitate further\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsirmpas_D/0/1/0/all/0/1\">Dimitrios Tsirmpas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkionis_I/0/1/0/all/0/1\">Ioannis Gkionis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mademlis_I/0/1/0/all/0/1\">Ioannis Mademlis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motion-Based Sign Language Video Summarization using Curvature and Torsion. (arXiv:2305.16801v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.16801","description":"<p>An interesting problem in many video-based applications is the generation of\nshort synopses by selecting the most informative frames, a procedure which is\nknown as video summarization. For sign language videos the benefits of using\nthe $t$-parameterized counterpart of the curvature of the 2-D signer's wrist\ntrajectory to identify keyframes, have been recently reported in the\nliterature. In this paper we extend these ideas by modeling the 3-D hand motion\nthat is extracted from each frame of the video. To this end we propose a new\ninformative function based on the $t$-parameterized curvature and torsion of\nthe 3-D trajectory. The method to characterize video frames as keyframes\ndepends on whether the motion occurs in 2-D or 3-D space. Specifically, in the\ncase of 3-D motion we look for the maxima of the harmonic mean of the curvature\nand torsion of the target's trajectory; in the planar motion case we seek for\nthe maxima of the trajectory's curvature. The proposed 3-D feature is\nexperimentally evaluated in applications of sign language videos on (1)\nobjective measures using ground-truth keyframe annotations, (2) human-based\nevaluation of understanding, and (3) gloss classification and the results\nobtained are promising.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sartinas_E/0/1/0/all/0/1\">Evangelos G. Sartinas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Psarakis_E/0/1/0/all/0/1\">Emmanouil Z. Psarakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosmopoulos_D/0/1/0/all/0/1\">Dimitrios I. Kosmopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Better Text Image Translation with Multimodal Codebook. (arXiv:2305.17415v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.17415","description":"<p>Text image translation (TIT) aims to translate the source texts embedded in\nthe image to target translations, which has a wide range of applications and\nthus has important research value. However, current studies on TIT are\nconfronted with two main bottlenecks: 1) this task lacks a publicly available\nTIT dataset, 2) dominant models are constructed in a cascaded manner, which\ntends to suffer from the error propagation of optical character recognition\n(OCR). In this work, we first annotate a Chinese-English TIT dataset named\nOCRMT30K, providing convenience for subsequent studies. Then, we propose a TIT\nmodel with a multimodal codebook, which is able to associate the image with\nrelevant texts, providing useful supplementary information for translation.\nMoreover, we present a multi-stage training framework involving text machine\ntranslation, image-text alignment, and TIT tasks, which fully exploits\nadditional bilingual texts, OCR dataset and our OCRMT30K dataset to train our\nmodel. Extensive experiments and in-depth analyses strongly demonstrate the\neffectiveness of our proposed model and training framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1\">Zhibin Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiawei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_J/0/1/0/all/0/1\">Jian Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Degen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiscale Positive-Unlabeled Detection of AI-Generated Texts. (arXiv:2305.18149v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18149","description":"<p>Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are\nastonishing at generating human-like texts, but they may get misused for fake\nscholarly texts, fake news, fake tweets, et cetera. Previous works have\nproposed methods to detect these multiscale AI-generated texts, including\nsimple ML classifiers, pretrained-model-based training-agnostic methods, and\nfinetuned language classification models. However, mainstream detectors are\nformulated without considering the factor of corpus length: shorter corpuses\nare harder to detect compared with longer ones for shortage of informative\nfeatures. In this paper, a Multiscale Positive-Unlabeled (MPU) training\nframework is proposed to address the challenge of multiscale text detection.\nFirstly, we acknowledge the human-resemblance property of short machine texts,\nand rephrase text classification as a Positive-Unlabeled (PU) problem by\nmarking these short machine texts as \"unlabeled\" during training. In this PU\ncontext, we propose the length-sensitive Multiscale PU Loss, where we use a\nrecurrent model in abstraction to estimate positive priors of scale-variant\ncorpuses. Additionally, we introduce a Text Multiscaling module to enrich\ntraining corpuses. Experiments show that our MPU method augments detection\nperformance on long AI-generated text, and significantly improves short-corpus\ndetection of language model detectors. Language Models trained with MPU could\noutcompete existing detectors by large margins on multiscale AI-generated\ntexts. The codes are available at\nhttps://github.com/mindspore-lab/mindone/tree/master/examples/detect_chatgpt\nand https://github.com/YuchuanTian/AIGC_text_detector.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuchuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xutao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1\">Zheyuan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinghua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faith and Fate: Limits of Transformers on Compositionality. (arXiv:2305.18654v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18654","description":"<p>Transformer large language models (LLMs) have sparked admiration for their\nexceptional performance on tasks that demand intricate multi-step reasoning.\nYet, these models simultaneously show failures on surprisingly trivial\nproblems. This begs the question: Are these errors incidental, or do they\nsignal more substantial limitations? In an attempt to demystify Transformers,\nwe investigate the limits of these models across three representative\ncompositional tasks -- multi-digit multiplication, logic grid puzzles, and a\nclassic dynamic programming problem. These tasks require breaking problems down\ninto sub-steps and synthesizing these steps into a precise answer. We formulate\ncompositional tasks as computation graphs to systematically quantify the level\nof complexity, and break down reasoning steps into intermediate sub-procedures.\nOur empirical findings suggest that Transformers solve compositional tasks by\nreducing multi-step compositional reasoning into linearized subgraph matching,\nwithout necessarily developing systematic problem-solving skills. To round off\nour empirical study, we provide theoretical arguments on abstract multi-step\nreasoning problems that highlight how Transformers' performance will rapidly\ndecay with increased task complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dziri_N/0/1/0/all/0/1\">Nouha Dziri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sclar_M/0/1/0/all/0/1\">Melanie Sclar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Lorraine Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1\">Soumya Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1\">Sean Welleck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ettinger_A/0/1/0/all/0/1\">Allyson Ettinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harchaoui_Z/0/1/0/all/0/1\">Zaid Harchaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KEYword based Sampling (KEYS) for Large Language Models. (arXiv:2305.18679v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.18679","description":"<p>Question answering (Q/A) can be formulated as a generative task (Mitra, 2017)\nwhere the task is to generate an answer given the question and the passage\n(knowledge, if available). Recent advances in QA task is focused a lot on\nlanguage model advancements and less on other areas such as sampling(Krishna et\nal., 2021), (Nakano et al., 2021). Keywords play very important role for humans\nin language generation. (Humans formulate keywords and use grammar to connect\nthose keywords and work). In the research community, very little focus is on\nhow humans generate answers to a question and how this behavior can be\nincorporated in a language model. In this paper, we want to explore these two\nareas combined, i.e., how sampling can be to used generate answers which are\nclose to human-like behavior and factually correct. Hence, the type of decoding\nalgorithm we think should be used for Q/A tasks should also depend on the\nkeywords. These keywords can be obtained from the question, passage or internet\nresults. We use knowledge distillation techniques to extract keywords and\nsample using these extracted keywords on top of vanilla decoding algorithms\nwhen formulating the answer to generate a human-like answer. In this paper, we\nshow that our decoding method outperforms most commonly used decoding methods\nfor Q/A task\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+V_J/0/1/0/all/0/1\">Jyothir S V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_Z/0/1/0/all/0/1\">Zuhaib Akhtar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UKP-SQuARE: An Interactive Tool for Teaching Question Answering. (arXiv:2305.19748v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.19748","description":"<p>The exponential growth of question answering (QA) has made it an\nindispensable topic in any Natural Language Processing (NLP) course.\nAdditionally, the breadth of QA derived from this exponential growth makes it\nan ideal scenario for teaching related NLP topics such as information\nretrieval, explainability, and adversarial attacks among others. In this paper,\nwe introduce UKP-SQuARE as a platform for QA education. This platform provides\nan interactive environment where students can run, compare, and analyze various\nQA models from different perspectives, such as general behavior,\nexplainability, and robustness. Therefore, students can get a first-hand\nexperience in different QA techniques during the class. Thanks to this, we\npropose a learner-centered approach for QA education in which students\nproactively learn theoretical concepts and acquire problem-solving skills\nthrough interactive exploration, experimentation, and practical assignments,\nrather than solely relying on traditional lectures. To evaluate the\neffectiveness of UKP-SQuARE in teaching scenarios, we adopted it in a\npostgraduate NLP course and surveyed the students after the course. Their\npositive feedback shows the platform's effectiveness in their course and\ninvites a wider adoption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Haishuo Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puerto_H/0/1/0/all/0/1\">Haritz Puerto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AfriNames: Most ASR models \"butcher\" African Names. (arXiv:2306.00253v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.00253","description":"<p>Useful conversational agents must accurately capture named entities to\nminimize error for downstream tasks, for example, asking a voice assistant to\nplay a track from a certain artist, initiating navigation to a specific\nlocation, or documenting a laboratory result for a patient. However, where\nnamed entities such as ``Ukachukwu`` (Igbo), ``Lakicia`` (Swahili), or\n``Ingabire`` (Rwandan) are spoken, automatic speech recognition (ASR) models'\nperformance degrades significantly, propagating errors to downstream systems.\nWe model this problem as a distribution shift and demonstrate that such model\nbias can be mitigated through multilingual pre-training, intelligent data\naugmentation strategies to increase the representation of African-named\nentities, and fine-tuning multilingual ASR models on multiple African accents.\nThe resulting fine-tuned models show an 81.5\\% relative WER improvement\ncompared with the baseline on samples with African-named entities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Olatunji_T/0/1/0/all/0/1\">Tobi Olatunji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afonja_T/0/1/0/all/0/1\">Tejumade Afonja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dossou_B/0/1/0/all/0/1\">Bonaventure F. P. Dossou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonja_A/0/1/0/all/0/1\">Atnafu Lambebo Tonja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1\">Chris Chinenye Emezue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rufai_A/0/1/0/all/0/1\">Amina Mardiyyah Rufai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sahib Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TopEx: Topic-based Explanations for Model Comparison. (arXiv:2306.00976v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.00976","description":"<p>Meaningfully comparing language models is challenging with current\nexplanation methods. Current explanations are overwhelming for humans due to\nlarge vocabularies or incomparable across models. We present TopEx, an\nexplanation method that enables a level playing field for comparing language\nmodels via model-agnostic topics. We demonstrate how TopEx can identify\nsimilarities and differences between DistilRoBERTa and GPT-2 on a variety of\nNLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Havaldar_S/0/1/0/all/0/1\">Shreya Havaldar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_A/0/1/0/all/0/1\">Adam Stein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_E/0/1/0/all/0/1\">Eric Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1\">Lyle Ungar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classifying YouTube Comments Based on Sentiment and Type of Sentence. (arXiv:2111.01908v1 [cs.IR] CROSS LISTED)","link":"http://arxiv.org/abs/2111.01908","description":"<p>As a YouTube channel grows, each video can potentially collect enormous\namounts of comments that provide direct feedback from the viewers. These\ncomments are a major means of understanding viewer expectations and improving\nchannel engagement. However, the comments only represent a general collection\nof user opinions about the channel and the content. Many comments are poorly\nconstructed, trivial, and have improper spellings and grammatical errors. As a\nresult, it is a tedious job to identify the comments that best interest the\ncontent creators. In this paper, we extract and classify the raw comments into\ndifferent categories based on both sentiment and sentence types that will help\nYouTubers find relevant comments for growing their viewership. Existing studies\nhave focused either on sentiment analysis (positive and negative) or\nclassification of sub-types within the same sentence types (e.g., types of\nquestions) on a text corpus. These have limited application on non-traditional\ntext corpus like YouTube comments. We address this challenge of text extraction\nand classification from YouTube comments using well-known statistical measures\nand machine learning models. We evaluate each combination of statistical\nmeasure and the machine learning model using cross validation and $F_1$ scores.\nThe results show that our approach that incorporates conventional methods\nperforms well on the classification task, validating its potential in assisting\ncontent creators increase viewer engagement on their channel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pokharel_R/0/1/0/all/0/1\">Rhitabrat Pokharel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatta_D/0/1/0/all/0/1\">Dixit Bhatta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-06-04T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}