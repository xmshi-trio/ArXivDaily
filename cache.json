{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-12-29T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Automatic Text Simplification of News Articles in the Context of Public Broadcasting. (arXiv:2212.13317v1 [cs.CL])","link":"http://arxiv.org/abs/2212.13317","description":"<p>This report summarizes the work carried out by the authors during the Twelfth\nMontreal Industrial Problem Solving Workshop, held at Universit\\'e de\nMontr\\'eal in August 2022. The team tackled a problem submitted by\nCBC/Radio-Canada on the theme of Automatic Text Simplification (ATS).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maupome_D/0/1/0/all/0/1\">Diego Maupom&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rancourt_F/0/1/0/all/0/1\">Fanny Rancourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soulas_T/0/1/0/all/0/1\">Thomas Soulas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lachance_A/0/1/0/all/0/1\">Alexandre Lachance</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meurs_M/0/1/0/all/0/1\">Marie-Jean Meurs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aleksandrova_D/0/1/0/all/0/1\">Desislava Aleksandrova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dufour_O/0/1/0/all/0/1\">Olivier Brochu Dufour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pontes_I/0/1/0/all/0/1\">Igor Pontes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardon_R/0/1/0/all/0/1\">R&#xe9;mi Cardon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simard_M/0/1/0/all/0/1\">Michel Simard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vajjala_S/0/1/0/all/0/1\">Sowmya Vajjala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Be So Sure! Boosting ASR Decoding via Confidence Relaxation. (arXiv:2212.13378v1 [cs.CL])","link":"http://arxiv.org/abs/2212.13378","description":"<p>Automatic Speech Recognition (ASR) systems frequently use a search-based\ndecoding strategy aiming to find the best attainable transcript by considering\nmultiple candidates. One prominent speech recognition decoding heuristic is\nbeam search, which seeks the transcript with the greatest likelihood computed\nusing the predicted distribution. While showing substantial performance gains\nin various tasks, beam search loses some of its effectiveness when the\npredicted probabilities are highly confident, i.e., the predicted distribution\nis massed for a single or very few classes. We show that recently proposed\nSelf-Supervised Learning (SSL)-based ASR models tend to yield exceptionally\nconfident predictions that may hamper beam search from truly considering a\ndiverse set of candidates. We perform a layer analysis to reveal and visualize\nhow predictions evolve, and propose a decoding procedure that improves the\nperformance of fine-tuned ASR models. Our proposed approach does not require\nfurther training beyond the original fine-tuning, nor additional model\nparameters. In fact, we find that our proposed method requires significantly\nless inference computation than current approaches. We propose aggregating the\ntop M layers, potentially leveraging useful information encoded in intermediate\nlayers, and relaxing model confidence. We demonstrate the effectiveness of our\napproach by conducting an empirical study on varying amounts of labeled\nresources and different model sizes, showing consistent improvements in\nparticular when applied to low-resource scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wullach_T/0/1/0/all/0/1\">Tomer Wullach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chazan_S/0/1/0/all/0/1\">Shlomo E. Chazan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepCuts: Single-Shot Interpretability based Pruning for BERT. (arXiv:2212.13392v1 [cs.CL])","link":"http://arxiv.org/abs/2212.13392","description":"<p>As language models have grown in parameters and layers, it has become much\nharder to train and infer with them on single GPUs. This is severely\nrestricting the availability of large language models such as GPT-3,\nBERT-Large, and many others. A common technique to solve this problem is\npruning the network architecture by removing transformer heads, fully-connected\nweights, and other modules. The main challenge is to discern the important\nparameters from the less important ones. Our goal is to find strong metrics for\nidentifying such parameters. We thus propose two strategies: Cam-Cut based on\nthe GradCAM interpretations, and Smooth-Cut based on the SmoothGrad, for\ncalculating the importance scores. Through this work, we show that our scoring\nfunctions are able to assign more relevant task-based scores to the network\nparameters, and thus both our pruning approaches significantly outperform the\nstandard weight and gradient-based strategies, especially at higher compression\nratios in BERT-based models. We also analyze our pruning masks and find them to\nbe significantly different from the ones obtained using standard metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grover_J/0/1/0/all/0/1\">Jasdeep Singh Grover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gawri_B/0/1/0/all/0/1\">Bhavesh Gawri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manku_R/0/1/0/all/0/1\">Ruskin Raj Manku</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NEEDED: Introducing Hierarchical Transformer to Eye Diseases Diagnosis. (arXiv:2212.13408v1 [cs.CL])","link":"http://arxiv.org/abs/2212.13408","description":"<p>With the development of natural language processing techniques(NLP),\nautomatic diagnosis of eye diseases using ophthalmology electronic medical\nrecords (OEMR) has become possible. It aims to evaluate the condition of both\neyes of a patient respectively, and we formulate it as a particular multi-label\nclassification task in this paper. Although there are a few related studies in\nother diseases, automatic diagnosis of eye diseases exhibits unique\ncharacteristics. First, descriptions of both eyes are mixed up in OEMR\ndocuments, with both free text and templated asymptomatic descriptions,\nresulting in sparsity and clutter of information. Second, OEMR documents\ncontain multiple parts of descriptions and have long document lengths. Third,\nit is critical to provide explainability to the disease diagnosis model. To\novercome those challenges, we present an effective automatic eye disease\ndiagnosis framework, NEEDED. In this framework, a preprocessing module is\nintegrated to improve the density and quality of information. Then, we design a\nhierarchical transformer structure for learning the contextualized\nrepresentations of each sentence in the OEMR document. For the diagnosis part,\nwe propose an attention-based predictor that enables traceable diagnosis by\nobtaining disease-specific information. Experiments on the real dataset and\ncomparison with several baseline models show the advantage and explainability\nof our framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xu Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1\">Meng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_Z/0/1/0/all/0/1\">Zhiyuan Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Weiwei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1\">Wenjuan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuanchun Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Knowledge-Enhanced Pre-trained Language Models. (arXiv:2212.13428v1 [cs.CL])","link":"http://arxiv.org/abs/2212.13428","description":"<p>Natural Language Processing (NLP) has been revolutionized by the use of\nPre-trained Language Models (PLMs) such as BERT. Despite setting new records in\nnearly every NLP task, PLMs still face a number of challenges including poor\ninterpretability, weak reasoning capability, and the need for a lot of\nexpensive annotated data when applied to downstream tasks. By integrating\nexternal knowledge into PLMs,\n\\textit{\\underline{K}nowledge-\\underline{E}nhanced \\underline{P}re-trained\n\\underline{L}anguage \\underline{M}odels} (KEPLMs) have the potential to\novercome the above-mentioned limitations. In this paper, we examine KEPLMs\nsystematically through a series of studies. Specifically, we outline the common\ntypes and different formats of knowledge to be integrated into KEPLMs, detail\nthe existing methods for building and evaluating KEPLMS, present the\napplications of KEPLMs in downstream tasks, and discuss the future research\ndirections. Researchers will benefit from this survey by gaining a quick and\ncomprehensive overview of the latest developments in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhen_C/0/1/0/all/0/1\">Chaoqi Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1\">Yanlei Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dell Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TegFormer: Topic-to-Essay Generation with Good Topic Coverage and High Text Coherence. (arXiv:2212.13456v1 [cs.CL])","link":"http://arxiv.org/abs/2212.13456","description":"<p>Creating an essay based on a few given topics is a challenging NLP task.\nAlthough several effective methods for this problem, topic-to-essay generation,\nhave appeared recently, there is still much room for improvement, especially in\nterms of the coverage of the given topics and the coherence of the generated\ntext. In this paper, we propose a novel approach called TegFormer which\nutilizes the Transformer architecture where the encoder is enriched with\ndomain-specific contexts while the decoder is enhanced by a large-scale\npre-trained language model. Specifically, a \\emph{Topic-Extension} layer\ncapturing the interaction between the given topics and their domain-specific\ncontexts is plugged into the encoder. Since the given topics are usually\nconcise and sparse, such an additional layer can bring more topic-related\nsemantics in to facilitate the subsequent natural language generation.\nMoreover, an \\emph{Embedding-Fusion} module that combines the domain-specific\nword embeddings learnt from the given corpus and the general-purpose word\nembeddings provided by a GPT-2 model pre-trained on massive text data is\nintegrated into the decoder. Since GPT-2 is at a much larger scale, it contains\na lot more implicit linguistic knowledge which would help the decoder to\nproduce more grammatical and readable text. Extensive experiments have shown\nthat the pieces of text generated by TegFormer have better topic coverage and\nhigher text coherence than those from SOTA topic-to-essay techniques, according\nto automatic and human evaluations. As revealed by ablation studies, both the\nTopic-Extension layer and the Embedding-Fusion module contribute substantially\nto TegFormer's performance advantage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1\">Wang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_Y/0/1/0/all/0/1\">Yuan Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dell Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Table-and-Text HybridQA: Concepts, Methods, Challenges and Future Directions. (arXiv:2212.13465v1 [cs.CL])","link":"http://arxiv.org/abs/2212.13465","description":"<p>Table-and-text hybrid question answering (HybridQA) is a widely used and\nchallenging NLP task commonly applied in the financial and scientific domain.\nThe early research focuses on migrating other QA task methods to HybridQA,\nwhile with further research, more and more HybridQA-specific methods have been\npresent. With the rapid development of HybridQA, the systematic survey is still\nunder-explored to summarize the main techniques and advance further research.\nSo we present this work to summarize the current HybridQA benchmarks and\nmethods, then analyze the challenges and future directions of this task. The\ncontributions of this paper can be summarized in three folds: (1) first survey,\nto our best knowledge, including benchmarks, methods and challenges for\nHybridQA; (2) systematic investigation with the reasonable comparison of the\nexisting systems to articulate their advantages and shortcomings; (3) detailed\nanalysis of challenges in four important dimensions to shed light on future\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dingzirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_L/0/1/0/all/0/1\">Longxu Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiSpider: Towards Benchmarking Multilingual Text-to-SQL Semantic Parsing. (arXiv:2212.13492v1 [cs.CL])","link":"http://arxiv.org/abs/2212.13492","description":"<p>Text-to-SQL semantic parsing is an important NLP task, which greatly\nfacilitates the interaction between users and the database and becomes the key\ncomponent in many human-computer interaction systems. Much recent progress in\ntext-to-SQL has been driven by large-scale datasets, but most of them are\ncentered on English. In this work, we present MultiSpider, the largest\nmultilingual text-to-SQL dataset which covers seven languages (English, German,\nFrench, Spanish, Japanese, Chinese, and Vietnamese). Upon MultiSpider, we\nfurther identify the lexical and structural challenges of text-to-SQL (caused\nby specific language properties and dialect sayings) and their intensity across\ndifferent languages. Experimental results under three typical settings\n(zero-shot, monolingual and multilingual) reveal a 6.1% absolute drop in\naccuracy in non-English languages. Qualitative and quantitative analyses are\nconducted to understand the reason for the performance drop of each language.\nBesides the dataset, we also propose a simple schema augmentation framework\nSAVe (Schema-Augmentation-with-Verification), which significantly boosts the\noverall performance by about 1.8% and closes the 29.5% performance gap across\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_L/0/1/0/all/0/1\">Longxu Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_M/0/1/0/all/0/1\">Mingyang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dingzirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">Dechen Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TempCLR: Temporal Alignment Representation with Contrastive Learning. (arXiv:2212.13738v1 [cs.CV])","link":"http://arxiv.org/abs/2212.13738","description":"<p>Video representation learning has been successful in video-text pre-training\nfor zero-shot transfer, where each sentence is trained to be close to the\npaired video clips in a common feature space. For long videos, given a\nparagraph of description where the sentences describe different segments of the\nvideo, by matching all sentence-clip pairs, the paragraph and the full video\nare aligned implicitly. However, such unit-level similarity measure may ignore\nthe global temporal context over a long time span, which inevitably limits the\ngeneralization ability. In this paper, we propose a contrastive learning\nframework TempCLR to compare the full video and the paragraph explicitly. As\nthe video/paragraph is formulated as a sequence of clips/sentences, under the\nconstraint of their temporal order, we use dynamic time warping to compute the\nminimum cumulative cost over sentence-clip pairs as the sequence-level\ndistance. To explore the temporal dynamics, we break the consistency of\ntemporal order by shuffling the video clips or sentences according to the\ntemporal granularity. In this way, we obtain the representations for\nclips/sentences, which perceive the temporal information and thus facilitate\nthe sequence alignment. In addition to pre-training on the video and paragraph,\nour approach can also generalize on the matching between different video\ninstances. We evaluate our approach on video retrieval, action step\nlocalization, and few-shot action recognition, and achieve consistent\nperformance gain over all three tasks. Detailed ablation studies are provided\nto justify the approach design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuncong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiawei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shiyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1\">Guangxing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Recognition and Classification of Future Work Sentences from Academic Articles in a Specific Domain. (arXiv:2212.13860v1 [cs.CL])","link":"http://arxiv.org/abs/2212.13860","description":"<p>Future work sentences (FWS) are the particular sentences in academic papers\nthat contain the author's description of their proposed follow-up research\ndirection. This paper presents methods to automatically extract FWS from\nacademic papers and classify them according to the different future directions\nembodied in the paper's content. FWS recognition methods will enable subsequent\nresearchers to locate future work sentences more accurately and quickly and\nreduce the time and cost of acquiring the corpus. The current work on automatic\nidentification of future work sentences is relatively small, and the existing\nresearch cannot accurately identify FWS from academic papers, and thus cannot\nconduct data mining on a large scale. Furthermore, there are many aspects to\nthe content of future work, and the subdivision of the content is conducive to\nthe analysis of specific development directions. In this paper, Nature Language\nProcessing (NLP) is used as a case study, and FWS are extracted from academic\npapers and classified into different types. We manually build an annotated\ncorpus with six different types of FWS. Then, automatic recognition and\nclassification of FWS are implemented using machine learning models, and the\nperformance of these models is compared based on the evaluation metrics. The\nresults show that the Bernoulli Bayesian model has the best performance in the\nautomatic recognition task, with the Macro F1 reaching 90.73%, and the SCIBERT\nmodel has the best performance in the automatic classification task, with the\nweighted average F1 reaching 72.63%. Finally, we extract keywords from FWS and\ngain a deep understanding of the key content described in FWS, and we also\ndemonstrate that content determination in FWS will be reflected in the\nsubsequent research work by measuring the similarity between future work\nsentences and the abstracts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yi Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1\">Wenke Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhicheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yuchen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuzhuo Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attentive Deep Neural Networks for Legal Document Retrieval. (arXiv:2212.13899v1 [cs.IR])","link":"http://arxiv.org/abs/2212.13899","description":"<p>Legal text retrieval serves as a key component in a wide range of legal text\nprocessing tasks such as legal question answering, legal case entailment, and\nstatute law retrieval. The performance of legal text retrieval depends, to a\nlarge extent, on the representation of text, both query and legal documents.\nBased on good representations, a legal text retrieval model can effectively\nmatch the query to its relevant documents. Because legal documents often\ncontain long articles and only some parts are relevant to queries, it is quite\na challenge for existing models to represent such documents. In this paper, we\nstudy the use of attentive neural network-based text representation for statute\nlaw document retrieval. We propose a general approach using deep neural\nnetworks with attention mechanisms. Based on it, we develop two hierarchical\narchitectures with sparse attention to represent long sentences and articles,\nand we name them Attentive CNN and Paraformer. The methods are evaluated on\ndatasets of different sizes and characteristics in English, Japanese, and\nVietnamese. Experimental results show that: i) Attentive neural methods\nsubstantially outperform non-neural methods in terms of retrieval performance\nacross datasets and languages; ii) Pretrained transformer-based models achieve\nbetter accuracy on small datasets at the cost of high computational complexity\nwhile lighter weight Attentive CNN achieves better accuracy on large datasets;\nand iii) Our proposed Paraformer outperforms state-of-the-art methods on COLIEE\ndataset, achieving the highest recall and F2 scores in the top-N retrieval\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha-Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phi_M/0/1/0/all/0/1\">Manh-Kien Phi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_X/0/1/0/all/0/1\">Xuan-Bach Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Le-Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_M/0/1/0/all/0/1\">Minh-Phuong Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Emotion Recognition among Couples from Lab Settings to Daily Life using Smartwatches. (arXiv:2212.13917v1 [cs.HC])","link":"http://arxiv.org/abs/2212.13917","description":"<p>Couples generally manage chronic diseases together and the management takes\nan emotional toll on both patients and their romantic partners. Consequently,\nrecognizing the emotions of each partner in daily life could provide an insight\ninto their emotional well-being in chronic disease management. The emotions of\npartners are currently inferred in the lab and daily life using self-reports\nwhich are not practical for continuous emotion assessment or observer reports\nwhich are manual, time-intensive, and costly. Currently, there exists no\ncomprehensive overview of works on emotion recognition among couples.\nFurthermore, approaches for emotion recognition among couples have (1) focused\non English-speaking couples in the U.S., (2) used data collected from the lab,\nand (3) performed recognition using observer ratings rather than partner's\nself-reported / subjective emotions. In this body of work contained in this\nthesis (8 papers - 5 published and 3 currently under review in various\njournals), we fill the current literature gap on couples' emotion recognition,\ndevelop emotion recognition systems using 161 hours of data from a total of\n1,051 individuals, and make contributions towards taking couples' emotion\nrecognition from the lab which is the status quo, to daily life. This thesis\ncontributes toward building automated emotion recognition systems that would\neventually enable partners to monitor their emotions in daily life and enable\nthe delivery of interventions to improve their emotional well-being.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boateng_G/0/1/0/all/0/1\">George Boateng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Page Layout Analysis of Text-heavy Historical Documents: a Comparison of Textual and Visual Approaches. (arXiv:2212.13924v1 [cs.IR])","link":"http://arxiv.org/abs/2212.13924","description":"<p>Page layout analysis is a fundamental step in document processing which\nenables to segment a page into regions of interest. With highly complex layouts\nand mixed scripts, scholarly commentaries are text-heavy documents which remain\nchallenging for state-of-the-art models. Their layout considerably varies\nacross editions and their most important regions are mainly defined by semantic\nrather than graphical characteristics such as position or appearance. This\nsetting calls for a comparison between textual, visual and hybrid approaches.\nWe therefore assess the performances of two transformers (LayoutLMv3 and\nRoBERTa) and an objection-detection network (YOLOv5). If results show a clear\nadvantage in favor of the latter, we also list several caveats to this finding.\nIn addition to our experiments, we release a dataset of ca. 300 annotated pages\nsampled from 19th century commentaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sven_N/0/1/0/all/0/1\">Najem-Meyer Sven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matteo_R/0/1/0/all/0/1\">Romanello Matteo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification. (arXiv:2212.13939v1 [cs.CL])","link":"http://arxiv.org/abs/2212.13939","description":"<p>Learning models are highly dependent on data to work effectively, and they\ngive a better performance upon training on big datasets. Massive research\nexists in the literature to address the dataset adequacy issue. One promising\napproach for solving dataset adequacy issues is the data augmentation (DA)\napproach. In DA, the amount of training data instances is increased by making\ndifferent transformations on the available data instances to generate new\ncorrect and representative data instances. DA increases the dataset size and\nits variability, which enhances the model performance and its prediction\naccuracy. DA also solves the class imbalance problem in the classification\nlearning techniques. Few studies have recently considered DA in the Arabic\nlanguage. These studies rely on traditional augmentation approaches, such as\nparaphrasing by using rules or noising-based techniques. In this paper, we\npropose a new Arabic DA method that employs the recent powerful modeling\ntechnique, namely the AraGPT-2, for the augmentation process. The generated\nsentences are evaluated in terms of context, semantics, diversity, and novelty\nusing the Euclidean, cosine, Jaccard, and BLEU distances. Finally, the AraBERT\ntransformer is used on sentiment classification tasks to evaluate the\nclassification performance of the augmented Arabic dataset. The experiments\nwere conducted on four sentiment Arabic datasets, namely AraSarcasm, ASTD, ATT,\nand MOVIE. The selected datasets vary in size, label number, and unbalanced\nclasses. The results show that the proposed methodology enhanced the Arabic\nsentiment text classification on all datasets with an increase in F1 score by\n4% in AraSarcasm, 6% in ASTD, 9% in ATT, and 13% in MOVIE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Refai_D/0/1/0/all/0/1\">Dania Refai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abo_Soud_S/0/1/0/all/0/1\">Saleh Abo-Soud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdel_Rahman_M/0/1/0/all/0/1\">Mohammad Abdel-Rahman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP. (arXiv:2212.14024v1 [cs.CL])","link":"http://arxiv.org/abs/2212.14024","description":"<p>Retrieval-augmented in-context learning has emerged as a powerful approach\nfor addressing knowledge-intensive tasks using frozen language models (LM) and\nretrieval models (RM). Existing work has combined these in simple\n\"retrieve-then-read\" pipelines in which the RM retrieves passages that are\ninserted into the LM prompt. To begin to fully realize the potential of frozen\nLMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that\nrelies on passing natural language texts in sophisticated pipelines between an\nLM and an RM. DSP can express high-level programs that bootstrap pipeline-aware\ndemonstrations, search for relevant passages, and generate grounded\npredictions, systematically breaking down problems into small transformations\nthat the LM and RM can handle more reliably. We have written novel DSP programs\nfor answering questions in open-domain, multi-hop, and conversational settings,\nestablishing in early evaluations new state-of-the-art in-context learning\nresults and delivering 37-200%, 8-40%, and 80-290% relative gains against\nvanilla LMs, a standard retrieve-then-read pipeline, and a contemporaneous\nself-ask pipeline, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1\">Omar Khattab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santhanam_K/0/1/0/all/0/1\">Keshav Santhanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Lisa Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_D/0/1/0/all/0/1\">David Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cramming: Training a Language Model on a Single GPU in One Day. (arXiv:2212.14034v1 [cs.CL])","link":"http://arxiv.org/abs/2212.14034","description":"<p>Recent trends in language modeling have focused on increasing performance\nthrough scaling, and have resulted in an environment where training language\nmodels is out of reach for most researchers and practitioners. While most in\nthe community are asking how to push the limits of extreme computation, we ask\nthe opposite question: How far can we get with a single GPU in just one day?\n</p>\n<p>We investigate the downstream performance achievable with a transformer-based\nlanguage model trained completely from scratch with masked language modeling\nfor a single day on a single consumer GPU. Aside from re-analyzing nearly all\ncomponents of the pretraining pipeline for this scenario and providing a\nmodified pipeline with performance close to BERT, we investigate why scaling\ndown is hard, and which modifications actually improve performance in this\nscenario. We provide evidence that even in this constrained setting,\nperformance closely follows scaling laws observed in large-compute settings.\nThrough the lens of scaling laws, we categorize a range of recent improvements\nto training and architecture and discuss their merit and practical\napplicability (or lack thereof) for the limited compute setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1\">Jonas Geiping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HateBR: A Large Expert Annotated Corpus of Brazilian Instagram Comments for Offensive Language and Hate Speech Detection. (arXiv:2103.14972v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.14972","description":"<p>Due to the severity of the social media offensive and hateful comments in\nBrazil, and the lack of research in Portuguese, this paper provides the first\nlarge-scale expert annotated corpus of Brazilian Instagram comments for hate\nspeech and offensive language detection. The HateBR corpus was collected from\nthe comment section of Brazilian politicians' accounts on Instagram and\nmanually annotated by specialists, reaching a high inter-annotator agreement.\nThe corpus consists of 7,000 documents annotated according to three different\nlayers: a binary classification (offensive versus non-offensive comments),\noffensiveness-level classification (highly, moderately, and slightly\noffensive), and nine hate speech groups (xenophobia, racism, homophobia,\nsexism, religious intolerance, partyism, apology for the dictatorship,\nantisemitism, and fatphobia). We also implemented baseline experiments for\noffensive language and hate speech detection and compared them with a\nliterature baseline. Results show that the baseline experiments on our corpus\noutperform the current state-of-the-art for the Portuguese language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vargas_F/0/1/0/all/0/1\">Francielle Alves Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_I/0/1/0/all/0/1\">Isabelle Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goes_F/0/1/0/all/0/1\">Fabiana Rodrigues de G&#xf3;es</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benevenuto_F/0/1/0/all/0/1\">Fabr&#xed;cio Benevenuto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pardo_T/0/1/0/all/0/1\">Thiago Alexandre Salgueiro Pardo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Robust Cybersecurity Topic Classification Tool. (arXiv:2109.02473v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2109.02473","description":"<p>In this research, we use user defined labels from three internet text sources\n(Reddit, Stackexchange, Arxiv) to train 21 different machine learning models\nfor the topic classification task of detecting cybersecurity discussions in\nnatural text. We analyze the false positive and false negative rates of each of\nthe 21 model's in a cross validation experiment. Then we present a\nCybersecurity Topic Classification (CTC) tool, which takes the majority vote of\nthe 21 trained machine learning models as the decision mechanism for detecting\ncybersecurity related text. We also show that the majority vote mechanism of\nthe CTC tool provides lower false negative and false positive rates on average\nthan any of the 21 individual models. We show that the CTC tool is scalable to\nthe hundreds of thousands of documents with a wall clock time on the order of\nhours.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pelofske_E/0/1/0/all/0/1\">Elijah Pelofske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liebrock_L/0/1/0/all/0/1\">Lorie M. Liebrock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urias_V/0/1/0/all/0/1\">Vincent Urias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Many Heads but One Brain: Fusion Brain -- a Competition and a Single Multimodal Multitask Architecture. (arXiv:2111.10974v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10974","description":"<p>Supporting the current trend in the AI community, we present the AI Journey\n2021 Challenge called Fusion Brain, the first competition which is targeted to\nmake the universal architecture which could process different modalities (in\nthis case, images, texts, and code) and solve multiple tasks for vision and\nlanguage. The Fusion Brain Challenge combines the following specific tasks:\nCode2code Translation, Handwritten Text recognition, Zero-shot Object\nDetection, and Visual Question Answering. We have created datasets for each\ntask to test the participants' submissions on it. Moreover, we have collected\nand made publicly available a new handwritten dataset in both English and\nRussian, which consists of 94,128 pairs of images and texts. We also propose a\nmultimodal and multitask architecture - a baseline solution, in the center of\nwhich is a frozen foundation model and which has been trained in Fusion mode\nalong with Single-task mode. The proposed Fusion approach proves to be\ncompetitive and more energy-efficient compared to the task-specific one.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bakshandaeva_D/0/1/0/all/0/1\">Daria Bakshandaeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1\">Denis Dimitrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arkhipkin_V/0/1/0/all/0/1\">Vladimir Arkhipkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shonenkov_A/0/1/0/all/0/1\">Alex Shonenkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potanin_M/0/1/0/all/0/1\">Mark Potanin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karachev_D/0/1/0/all/0/1\">Denis Karachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_A/0/1/0/all/0/1\">Andrey Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voronov_A/0/1/0/all/0/1\">Anton Voronov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davydova_V/0/1/0/all/0/1\">Vera Davydova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tutubalina_E/0/1/0/all/0/1\">Elena Tutubalina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petiushko_A/0/1/0/all/0/1\">Aleksandr Petiushko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Self-learning End-to-End Task-Oriented Dialog Systems. (arXiv:2201.06849v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.06849","description":"<p>End-to-end task bots are typically learned over a static and usually\nlimited-size corpus. However, when deployed in dynamic, changing, and open\nenvironments to interact with users, task bots tend to fail when confronted\nwith data that deviate from the training corpus, i.e., out-of-distribution\nsamples. In this paper, we study the problem of automatically adapting task\nbots to changing environments by learning from human-bot interactions with\nminimum or zero human annotations. We propose SL-AGENT, a novel self-learning\nframework for building end-to-end task bots. SL-AGENT consists of a dialog\nmodel and a pre-trained reward model to predict the quality of an agent\nresponse. It enables task bots to automatically adapt to changing environments\nby learning from the unlabeled human-bot dialog logs accumulated after\ndeployment via reinforcement learning with the incorporated reward model.\nExperimental results on four well-studied dialog tasks show the effectiveness\nof SL-AGENT to automatically adapt to changing environments, using both\nautomatic and human evaluations. We will release code and data for further\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt Consistency for Zero-Shot Task Generalization. (arXiv:2205.00049v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.00049","description":"<p>One of the most impressive results of recent NLP history is the ability of\npre-trained language models to solve new tasks in a zero-shot setting. To\nachieve this, NLP tasks are framed as natural language prompts, generating a\nresponse indicating the predicted output. Nonetheless, the performance in such\nsettings often lags far behind its supervised counterpart, suggesting a large\nspace for potential improvement. In this paper, we explore methods to utilize\nunlabeled data to improve zero-shot performance. Specifically, we take\nadvantage of the fact that multiple prompts can be used to specify a single\ntask, and propose to regularize prompt consistency, encouraging consistent\npredictions over this diverse set of prompts. Our method makes it possible to\nfine-tune the model either with extra unlabeled training data, or directly on\ntest input at inference time in an unsupervised manner. In experiments, our\napproach outperforms the state-of-the-art zero-shot learner, T0 (Sanh et al.,\n2022), on 9 out of 11 datasets across 4 NLP tasks by up to 10.6 absolute points\nin terms of accuracy. The gains are often attained with a small number of\nunlabeled examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chunting Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junxian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xuezhe Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Do Compressed Multilingual Machine Translation Models Forget?. (arXiv:2205.10828v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10828","description":"<p>Recently, very large pre-trained models achieve state-of-the-art results in\nvarious natural language processing (NLP) tasks, but their size makes it more\nchallenging to apply them in resource-constrained environments. Compression\ntechniques allow to drastically reduce the size of the models and therefore\ntheir inference time with negligible impact on top-tier metrics. However, the\ngeneral performance averaged across multiple tasks and/or languages may hide a\ndrastic performance drop on under-represented features, which could result in\nthe amplification of biases encoded by the models. In this work, we assess the\nimpact of compression methods on Multilingual Neural Machine Translation models\n(MNMT) for various language groups, gender, and semantic biases by extensive\nanalysis of compressed models on different machine translation benchmarks, i.e.\nFLORES-101, MT-Gender, and DiBiMT. We show that the performance of\nunder-represented languages drops significantly, while the average BLEU metric\nonly slightly decreases. Interestingly, the removal of noisy memorization with\ncompression leads to a significant improvement for some medium-resource\nlanguages. Finally, we demonstrate that compression amplifies intrinsic gender\nand semantic biases, even in high-resource languages. Code:\nhttps://github.com/alirezamshi/bias-compressedMT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadshahi_A/0/1/0/all/0/1\">Alireza Mohammadshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1\">Vassilina Nikoulina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berard_A/0/1/0/all/0/1\">Alexandre Berard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brun_C/0/1/0/all/0/1\">Caroline Brun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1\">Laurent Besacier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaptivePaste: Code Adaptation through Learning Semantics-aware Variable Usage Representations. (arXiv:2205.11023v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2205.11023","description":"<p>In software development, it is common for programmers to copy-paste or port\ncode snippets and then adapt them to their use case. This scenario motivates\nthe code adaptation task -- a variant of program repair which aims to adapt\nvariable identifiers in a pasted snippet of code to the surrounding,\npreexisting source code. However, no existing approach has been shown to\neffectively address this task. In this paper, we introduce AdaptivePaste, a\nlearning-based approach to source code adaptation, based on transformers and a\ndedicated dataflow-aware deobfuscation pre-training task to learn meaningful\nrepresentations of variable usage patterns. We evaluate AdaptivePaste on a\ndataset of code snippets in Python. Results suggest that our model can learn to\nadapt source code with 79.8% accuracy. To evaluate how valuable is\nAdaptivePaste in practice, we perform a user study with 10 Python developers on\na hundred real-world copy-paste instances. The results show that AdaptivePaste\nreduces the dwell time to nearly half the time it takes for manual code\nadaptation, and helps to avoid bugs. In addition, we utilize the participant\nfeedback to identify potential avenues for improvement of AdaptivePaste.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Jinu Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1\">Neel Sundaresan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allamanis_M/0/1/0/all/0/1\">Miltiadis Allamanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svyatkovskiy_A/0/1/0/all/0/1\">Alexey Svyatkovskiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using attention methods to predict judicial outcomes. (arXiv:2207.08823v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2207.08823","description":"<p>Legal Judgment Prediction is one of the most acclaimed fields for the\ncombined area of NLP, AI, and Law. By legal prediction we mean an intelligent\nsystems capable to predict specific judicial characteristics, such as judicial\noutcome, a judicial class, predict an specific case. In this research, we have\nused AI classifiers to predict judicial outcomes in the Brazilian legal system.\nFor this purpose, we developed a text crawler to extract data from the official\nBrazilian electronic legal systems. These texts formed a dataset of\nsecond-degree murder and active corruption cases. We applied different\nclassifiers, such as Support Vector Machines and Neural Networks, to predict\njudicial outcomes by analyzing textual features from the dataset. Our research\nshowed that Regression Trees, Gated Recurring Units and Hierarchical Attention\nNetworks presented higher metrics for different subsets. As a final goal, we\nexplored the weights of one of the algorithms, the Hierarchical Attention\nNetworks, to find a sample of the most important words used to absolve or\nconvict defendants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bertalan_V/0/1/0/all/0/1\">Vithor Gomes Ferreira Bertalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_E/0/1/0/all/0/1\">Evandro Eduardo Seron Ruiz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks. (arXiv:2207.13243v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2207.13243","description":"<p>The last decade of machine learning has seen drastic increases in scale and\ncapabilities. Deep neural networks (DNNs) are increasingly being deployed in\nthe real world. However, they are difficult to analyze, raising concerns about\nusing them without a rigorous understanding of how they function. Effective\ntools for interpreting them will be important for building more trustworthy AI\nby helping to identify problems, fix bugs, and improve basic understanding. In\nparticular, \"inner\" interpretability techniques, which focus on explaining the\ninternal components of DNNs, are well-suited for developing a mechanistic\nunderstanding, guiding manual modifications, and reverse engineering solutions.\n</p>\n<p>Much recent work has focused on DNN interpretability, and rapid progress has\nthus far made a thorough systematization of methods difficult. In this survey,\nwe review over 300 works with a focus on inner interpretability tools. We\nintroduce a taxonomy that classifies methods by what part of the network they\nhelp to explain (weights, neurons, subnetworks, or latent representations) and\nwhether they are implemented during (intrinsic) or after (post hoc) training.\nTo our knowledge, we are also the first to survey a number of connections\nbetween interpretability research and work in adversarial robustness, continual\nlearning, modularity, network compression, and studying the human visual\nsystem. We discuss key challenges and argue that the status quo in\ninterpretability research is largely unproductive. Finally, we highlight the\nimportance of future work that emphasizes diagnostics, debugging, adversaries,\nand benchmarking in order to make interpretability tools more useful to\nengineers in practical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rauker_T/0/1/0/all/0/1\">Tilman R&#xe4;uker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_A/0/1/0/all/0/1\">Anson Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1\">Stephen Casper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1\">Dylan Hadfield-Menell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Long-Text Understanding with Short-Text Models. (arXiv:2208.00748v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.00748","description":"<p>Transformer-based pretrained language models (LMs) are ubiquitous across\nnatural language understanding, but cannot be applied to long sequences such as\nstories, scientific articles and long documents, due to their quadratic\ncomplexity. While a myriad of efficient transformer variants have been\nproposed, they are typically based on custom implementations that require\nexpensive pretraining from scratch. In this work, we propose SLED:\nSLiding-Encoder and Decoder, a simple approach for processing long sequences\nthat re-uses and leverages battle-tested short-text pretrained LMs.\nSpecifically, we partition the input into overlapping chunks, encode each with\na short-text LM encoder and use the pretrained decoder to fuse information\nacross chunks (fusion-in-decoder). We illustrate through controlled experiments\nthat SLED offers a viable strategy for long text understanding and evaluate our\napproach on SCROLLS, a benchmark with seven datasets across a wide range of\nlanguage understanding tasks. We find that SLED is competitive with specialized\nmodels that are up to 50x larger and require a dedicated and expensive\npretraining step.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivgi_M/0/1/0/all/0/1\">Maor Ivgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaham_U/0/1/0/all/0/1\">Uri Shaham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Entity Detection with Proposer and Regressor. (arXiv:2210.10260v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10260","description":"<p>Named entity recognition is a traditional task in natural language\nprocessing. In particular, nested entity recognition receives extensive\nattention for the widespread existence of the nesting scenario. The latest\nresearch migrates the well-established paradigm of set prediction in object\ndetection to cope with entity nesting. However, the manual creation of query\nvectors, which fail to adapt to the rich semantic information in the context,\nlimits these approaches. An end-to-end entity detection approach with proposer\nand regressor is presented in this paper to tackle the issues. First, the\nproposer utilizes the feature pyramid network to generate high-quality entity\nproposals. Then, the regressor refines the proposals for generating the final\nprediction. The model adopts encoder-only architecture and thus obtains the\nadvantages of the richness of query semantics, high precision of entity\nlocalization, and easiness of model training. Moreover, we introduce the novel\nspatially modulated attention and progressive refinement for further\nimprovement. Extensive experiments demonstrate that our model achieves advanced\nperformance in flat and nested NER, achieving a new state-of-the-art F1 score\nof 80.74 on the GENIA dataset and 72.38 on the WeiboNER dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xueru Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Changjiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haotian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Luguang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Hong Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QuaLA-MiniLM: a Quantized Length Adaptive MiniLM. (arXiv:2210.17114v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.17114","description":"<p>Limited computational budgets often prevent transformers from being used in\nproduction and from having their high accuracy utilized. A knowledge\ndistillation approach addresses the computational efficiency by self-distilling\nBERT into a smaller transformer representation having fewer layers and smaller\ninternal embedding. However, the performance of these models drops as we reduce\nthe number of layers, notably in advanced NLP tasks such as span question\nanswering. In addition, a separate model must be trained for each inference\nscenario with its distinct computational budget. Dynamic-TinyBERT tackles both\nlimitations by partially implementing the Length Adaptive Transformer (LAT)\ntechnique onto TinyBERT, achieving x3 speedup over BERT-base with minimal\naccuracy loss. In this work, we expand the Dynamic-TinyBERT approach to\ngenerate a much more highly efficient model. We use MiniLM distillation jointly\nwith the LAT method, and we further enhance the efficiency by applying low-bit\nquantization. Our quantized length-adaptive MiniLM model (QuaLA-MiniLM) is\ntrained only once, dynamically fits any inference scenario, and achieves an\naccuracy-efficiency trade-off superior to any other efficient approaches per\nany computational budget on the SQuAD1.1 dataset (up to x8.8 speedup with &lt;1%\naccuracy loss). The code to reproduce this work is publicly available on\nGithub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guskin_S/0/1/0/all/0/1\">Shira Guskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasserblat_M/0/1/0/all/0/1\">Moshe Wasserblat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Haihao Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Switching to Discriminative Image Captioning by Relieving a Bottleneck of Reinforcement Learning. (arXiv:2212.03230v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2212.03230","description":"<p>Discriminativeness is a desirable feature of image captions: captions should\ndescribe the characteristic details of input images. However, recent\nhigh-performing captioning models, which are trained with reinforcement\nlearning (RL), tend to generate overly generic captions despite their high\nperformance in various other criteria. First, we investigate the cause of the\nunexpectedly low discriminativeness and show that RL has a deeply rooted side\neffect of limiting the output words to high-frequency words. The limited\nvocabulary is a severe bottleneck for discriminativeness as it is difficult for\na model to describe the details beyond its vocabulary. Then, based on this\nidentification of the bottleneck, we drastically recast discriminative image\ncaptioning as a much simpler task of encouraging low-frequency word generation.\nHinted by long-tail classification and debiasing methods, we propose methods\nthat easily switch off-the-shelf RL models to discriminativeness-aware models\nwith only a single-epoch fine-tuning on the part of the parameters. Extensive\nexperiments demonstrate that our methods significantly enhance the\ndiscriminativeness of off-the-shelf RL models and even outperform previous\ndiscriminativeness-aware methods with much smaller computational costs.\nDetailed analysis and human evaluation also verify that our methods boost the\ndiscriminativeness without sacrificing the overall quality of captions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Honda_U/0/1/0/all/0/1\">Ukyo Honda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1\">Taro Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsumoto_Y/0/1/0/all/0/1\">Yuji Matsumoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Associations Between Natural Language Processing (NLP) Enriched Social Determinants of Health and Suicide Death among US Veterans. (arXiv:2212.05546v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.05546","description":"<p>Importance: Social determinants of health (SDOH) are known to be associated\nwith increased risk of suicidal behaviors, but few studies utilized SDOH from\nunstructured electronic health record (EHR) notes.\n</p>\n<p>Objective: To investigate associations between suicide and recent SDOH,\nidentified using structured and unstructured data.\n</p>\n<p>Design: Nested case-control study.\n</p>\n<p>Setting: EHR data from the US Veterans Health Administration (VHA).\n</p>\n<p>Participants: 6,122,785 Veterans who received care in the US VHA between\nOctober 1, 2010, and September 30, 2015.\n</p>\n<p>Exposures: Occurrence of SDOH over a maximum span of two years compared with\nno occurrence of SDOH.\n</p>\n<p>Main Outcomes and Measures: Cases of suicide deaths were matched with 4\ncontrols on birth year, cohort entry date, sex, and duration of follow-up. We\ndeveloped an NLP system to extract SDOH from unstructured notes. Structured\ndata, NLP on unstructured data, and combining them yielded six, eight and nine\nSDOH respectively. Adjusted odds ratios (aORs) and 95% confidence intervals\n(CIs) were estimated using conditional logistic regression.\n</p>\n<p>Results: In our cohort, 8,821 Veterans committed suicide during 23,725,382\nperson-years of follow-up (incidence rate 37.18/100,000 person-years). Our\ncohort was mostly male (92.23%) and white (76.99%). Across the five common SDOH\nas covariates, NLP-extracted SDOH, on average, covered 80.03% of all SDOH\noccurrences. All SDOH, measured by structured data and NLP, were significantly\nassociated with increased risk of suicide. The SDOH with the largest effects\nwas legal problems (aOR=2.66, 95% CI=.46-2.89), followed by violence (aOR=2.12,\n95% CI=1.98-2.27). NLP-extracted and structured SDOH were also associated with\nsuicide.\n</p>\n<p>Conclusions and Relevance: NLP-extracted SDOH were always significantly\nassociated with increased risk of suicide among Veterans, suggesting the\npotential of NLP in public health studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1\">Avijit Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pradhan_R/0/1/0/all/0/1\">Richeek Pradhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melamed_R/0/1/0/all/0/1\">Rachel D Melamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoaglin_D/0/1/0/all/0/1\">David C Hoaglin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tucker_K/0/1/0/all/0/1\">Katherine L Tucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reisman_J/0/1/0/all/0/1\">Joel I Reisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weisong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_J/0/1/0/all/0/1\">Jack Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Linguistically Informed Multi-Objective Pre-Training for Natural Language Inference. (arXiv:2212.07428v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.07428","description":"<p>We introduce a linguistically enhanced combination of pre-training methods\nfor transformers. The pre-training objectives include POS-tagging, synset\nprediction based on semantic knowledge graphs, and parent prediction based on\ndependency parse trees. Our approach achieves competitive results on the\nNatural Language Inference task, compared to the state of the art. Specifically\nfor smaller models, the method results in a significant performance boost,\nemphasizing the fact that intelligent pre-training can make up for fewer\nparameters and help building more efficient models. Combining POS-tagging and\nsynset prediction yields the overall best results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pielka_M/0/1/0/all/0/1\">Maren Pielka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_S/0/1/0/all/0/1\">Svetlana Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pucknat_L/0/1/0/all/0/1\">Lisa Pucknat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifa_R/0/1/0/all/0/1\">Rafet Sifa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt-Augmented Linear Probing: Scaling Beyond The Limit of Few-shot In-Context Learners. (arXiv:2212.10873v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10873","description":"<p>Through in-context learning (ICL), large-scale language models are effective\nfew-shot learners without additional model fine-tuning. However, the ICL\nperformance does not scale well with the number of available training samples\nas it is limited by the inherent input length constraint of the underlying\nlanguage model. Meanwhile, many studies have revealed that language models are\nalso powerful feature extractors, allowing them to be utilized in a black-box\nmanner and enabling the linear probing paradigm, where lightweight\ndiscriminators are trained on top of the pre-extracted input representations.\nThis paper proposes prompt-augmented linear probing (PALP), a hybrid of linear\nprobing and ICL, which leverages the best of both worlds. PALP inherits the\nscalability of linear probing and the capability of enforcing language models\nto derive more meaningful representations via tailoring input into a more\nconceivable form. Throughout in-depth investigations on various datasets, we\nverified that PALP significantly enhances the input representations closing the\ngap between ICL in the data-hungry scenario and fine-tuning in the\ndata-abundant scenario with little training overhead, potentially making PALP a\nstrong alternative in a black-box scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyunsoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyuhng Joon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junyeob Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-goo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">Kang Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taeuk Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Safe and Usable Chatbots for Promoting Voter Participation. (arXiv:2212.11219v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2212.11219","description":"<p>Chatbots, or bots for short, are multi-modal collaborative assistants that\ncan help people complete useful tasks. Usually, when chatbots are referenced in\nconnection with elections, they often draw negative reactions due to the fear\nof mis-information and hacking. Instead, in this paper, we explore how chatbots\nmay be used to promote voter participation in vulnerable segments of society\nlike senior citizens and first-time voters. In particular, we build a system\nthat amplifies official information while personalizing it to users' unique\nneeds transparently. We discuss its design, build prototypes with frequently\nasked questions (FAQ) election information for two US states that are low on an\nease-of-voting scale, and report on its initial evaluation in a focus group.\nOur approach can be a win-win for voters, election agencies trying to fulfill\ntheir mandate and democracy at large.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muppasani_B/0/1/0/all/0/1\">Bharath Muppasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pallagani_V/0/1/0/all/0/1\">Vishal Pallagani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_K/0/1/0/all/0/1\">Kausik Lakkaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Shuge Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_B/0/1/0/all/0/1\">Biplav Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robertson_B/0/1/0/all/0/1\">Brett Robertson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hickerson_A/0/1/0/all/0/1\">Andrea Hickerson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_V/0/1/0/all/0/1\">Vignesh Narayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization. (arXiv:2212.12017v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.12017","description":"<p>Recent work has shown that fine-tuning large pre-trained language models on a\ncollection of tasks described via instructions, a.k.a. instruction-tuning,\nimproves their zero and few-shot generalization to unseen tasks. However, there\nis a limited understanding of the performance trade-offs of different decisions\nmade during the instruction-tuning process. These decisions include the scale\nand diversity of the instruction-tuning benchmark, different task sampling\nstrategies, fine-tuning with and without demonstrations, training using\nspecialized datasets for reasoning and dialogue, and finally, the fine-tuning\nobjectives themselves. In this paper, we characterize the effect of\ninstruction-tuning decisions on downstream task performance when scaling both\nmodel and benchmark sizes. To this end, we create OPT-IML Bench: a large\nbenchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated\ninto task categories from 8 existing benchmarks, and prepare an evaluation\nframework to measure three types of model generalizations: to tasks from fully\nheld-out categories, to held-out tasks from seen categories, and to held-out\ninstances from seen tasks. Through the lens of this framework, we first present\ninsights about instruction-tuning decisions as applied to OPT-30B and further\nexploit these insights to train OPT-IML 30B and 175B, which are\ninstruction-tuned versions of OPT. OPT-IML demonstrates all three\ngeneralization abilities at both scales on four different evaluation benchmarks\nwith diverse tasks and input formats -- PromptSource, FLAN,\nSuper-NaturalInstructions, and UnifiedSKG. Not only does it significantly\noutperform OPT on all benchmarks but is also highly competitive with existing\nmodels fine-tuned on each specific benchmark. We release OPT-IML at both\nscales, together with the OPT-IML Bench evaluation framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iyer_S/0/1/0/all/0/1\">Srinivasan Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xi Victoria Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1\">Ramakanth Pasunuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihaylov_T/0/1/0/all/0/1\">Todor Mihaylov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simig_D/0/1/0/all/0/1\">Daniel Simig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Ping Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuster_K/0/1/0/all/0/1\">Kurt Shuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianlu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koura_P/0/1/0/all/0/1\">Punit Singh Koura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OHoro_B/0/1/0/all/0/1\">Brian O&#x27;Horo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereyra_G/0/1/0/all/0/1\">Gabriel Pereyra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jeff Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dewan_C/0/1/0/all/0/1\">Christopher Dewan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_V/0/1/0/all/0/1\">Ves Stoyanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Framework of Customer Review Analysis Using the Aspect-Based Opinion Mining Approach. (arXiv:2212.10051v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2212.10051","description":"<p>Opinion mining is the branch of computation that deals with opinions,\nappraisals, attitudes, and emotions of people and their different aspects. This\nfield has attracted substantial research interest in recent years. Aspect-level\n(called aspect-based opinion mining) is often desired in practical applications\nas it provides detailed opinions or sentiments about different aspects of\nentities and entities themselves, which are usually required for action. Aspect\nextraction and entity extraction are thus two core tasks of aspect-based\nopinion mining. his paper has presented a framework of aspect-based opinion\nmining based on the concept of transfer learning. on real-world customer\nreviews available on the Amazon website. The model has yielded quite\nsatisfactory results in its task of aspect-based opinion mining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1\">Subhasis Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_J/0/1/0/all/0/1\">Jaydip Sen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-12-28T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}