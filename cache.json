{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-01-13T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"SensePOLAR: Word sense aware interpretability for pre-trained contextual word embeddings. (arXiv:2301.04704v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04704","description":"<p>Adding interpretability to word embeddings represents an area of active\nresearch in text representation. Recent work has explored thepotential of\nembedding words via so-called polar dimensions (e.g. good vs. bad, correct vs.\nwrong). Examples of such recent approaches include SemAxis, POLAR, FrameAxis,\nand BiImp. Although these approaches provide interpretable dimensions for\nwords, they have not been designed to deal with polysemy, i.e. they can not\neasily distinguish between different senses of words. To address this\nlimitation, we present SensePOLAR, an extension of the original POLAR framework\nthat enables word-sense aware interpretability for pre-trained contextual word\nembeddings. The resulting interpretable word embeddings achieve a level of\nperformance that is comparable to original contextual word embeddings across a\nvariety of natural language processing tasks including the GLUE and SQuAD\nbenchmarks. Our work removes a fundamental limitation of existing approaches by\noffering users sense aware interpretations for contextual word embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Engler_J/0/1/0/all/0/1\">Jan Engler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sikdar_S/0/1/0/all/0/1\">Sandipan Sikdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lutz_M/0/1/0/all/0/1\">Marlene Lutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strohmaier_M/0/1/0/all/0/1\">Markus Strohmaier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Web Enabled Geographic Question Answering Framework: GeoTR. (arXiv:2301.04752v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04752","description":"<p>With the considerable growth of linked data, researchers have focused on how\nto increase the availability of semantic web technologies to provide practical\nusages for real life systems. Question answering systems are an example of\nreal-life systems that communicate directly with end users, understand user\nintention and generate answers. End users do not care about the structural\nquery language or the vocabulary of the knowledge base where the point of a\nproblem arises. In this study, a question answering framework that converts\nTurkish natural language input into SPARQL queries in the geographical domain\nis proposed. Additionally, a novel Turkish ontology, which covers a 10th grade\ngeography lesson named Spatial Synthesis Turkey, has been developed to be used\nas a linked data provider. Moreover, a gap in the literature on Turkish\nquestion answering systems, which utilizes linked data in the geographical\ndomain, is addressed. A hybrid system architecture that combines natural\nlanguage processing techniques with linked data technologies to generate\nanswers is also proposed. Further related research areas are suggested.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tasar_C/0/1/0/all/0/1\">Ceren Ocal Tasar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komesli_M/0/1/0/all/0/1\">Murat Komesli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unalir_M/0/1/0/all/0/1\">Murat Osman Unalir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NarrowBERT: Accelerating Masked Language Model Pretraining and Inference. (arXiv:2301.04761v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04761","description":"<p>Large-scale language model pretraining is a very successful form of\nself-supervised learning in natural language processing, but it is increasingly\nexpensive to perform as the models and pretraining corpora have become larger\nover time. We propose NarrowBERT, a modified transformer encoder that increases\nthe throughput for masked language model pretraining by more than $2\\times$.\nNarrowBERT sparsifies the transformer model such that the self-attention\nqueries and feedforward layers only operate on the masked tokens of each\nsentence during pretraining, rather than all of the tokens as with the usual\ntransformer encoder. We also show that NarrowBERT increases the throughput at\ninference time by as much as $3.5\\times$ with minimal (or no) performance\ndegradation on sentence encoding tasks like MNLI. Finally, we examine the\nperformance of NarrowBERT on the IMDB and Amazon reviews classification and\nCoNLL NER tasks and show that it is also comparable to standard BERT\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keung_P/0/1/0/all/0/1\">Phillip Keung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1\">Daniel Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KAER: A Knowledge Augmented Pre-Trained Language Model for Entity Resolution. (arXiv:2301.04770v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04770","description":"<p>Entity resolution has been an essential and well-studied task in data\ncleaning research for decades. Existing work has discussed the feasibility of\nutilizing pre-trained language models to perform entity resolution and achieved\npromising results. However, few works have discussed injecting domain knowledge\nto improve the performance of pre-trained language models on entity resolution\ntasks. In this study, we propose Knowledge Augmented Entity Resolution (KAER),\na novel framework named for augmenting pre-trained language models with\nexternal knowledge for entity resolution. We discuss the results of utilizing\ndifferent knowledge augmentation and prompting methods to improve entity\nresolution performance. Our model improves on Ditto, the existing\nstate-of-the-art entity resolution method. In particular, 1) KAER performs more\nrobustly and achieves better results on \"dirty data\", and 2) with more general\nknowledge injection, KAER outperforms the existing baseline models on the\ntextual dataset and dataset from the online product domain. 3) KAER achieves\ncompetitive results on highly domain-specific datasets, such as citation\ndatasets, requiring the injection of expert knowledge in future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Liri Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiren Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torvik_V/0/1/0/all/0/1\">Vetle I. Torvik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludascher_B/0/1/0/all/0/1\">Bertram Lud&#xe4;scher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Cognition and Language Computation -- Human and Machine Language Understanding. (arXiv:2301.04788v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04788","description":"<p>Language understanding is a key scientific issue in the fields of cognitive\nand computer science. However, the two disciplines differ substantially in the\nspecific research questions. Cognitive science focuses on analyzing the\nspecific mechanism of the brain and investigating the brain's response to\nlanguage; few studies have examined the brain's language system as a whole. By\ncontrast, computer scientists focus on the efficiency of practical applications\nwhen choosing research questions but may ignore the most essential laws of\nlanguage. Given these differences, can a combination of the disciplines offer\nnew insights for building intelligent language models and studying language\ncognitive mechanisms? In the following text, we first review the research\nquestions, history, and methods of language understanding in cognitive and\ncomputer science, focusing on the current progress and challenges. We then\ncompare and contrast the research of language understanding in cognitive and\ncomputer sciences. Finally, we review existing work that combines insights from\nlanguage cognition and language computation and offer prospects for future\ndevelopment trends.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaonan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Nai Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_C/0/1/0/all/0/1\">Chengqing Zong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Deep Learning. (arXiv:2301.04856v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04856","description":"<p>This book is the result of a seminar in which we reviewed multimodal\napproaches and attempted to create a solid overview of the field, starting with\nthe current state-of-the-art approaches in the two subfields of Deep Learning\nindividually. Further, modeling frameworks are discussed where one modality is\ntransformed into the other, as well as models in which one modality is utilized\nto enhance representation learning for the other. To conclude the second part,\narchitectures with a focus on handling both modalities simultaneously are\nintroduced. Finally, we also cover other modalities as well as general-purpose\nmulti-modal models, which are able to handle different tasks on different\nmodalities within one unified architecture. One interesting application\n(Generative Art) eventually caps off this booklet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akkus_C/0/1/0/all/0/1\">Cem Akkus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1\">Luyang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Djakovic_V/0/1/0/all/0/1\">Vladana Djakovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jauch_Walser_S/0/1/0/all/0/1\">Steffen Jauch-Walser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_P/0/1/0/all/0/1\">Philipp Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loss_G/0/1/0/all/0/1\">Giacomo Loss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marquardt_C/0/1/0/all/0/1\">Christopher Marquardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moldovan_M/0/1/0/all/0/1\">Marco Moldovan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sauter_N/0/1/0/all/0/1\">Nadja Sauter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_M/0/1/0/all/0/1\">Maximilian Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulte_R/0/1/0/all/0/1\">Rickmer Schulte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urbanczyk_K/0/1/0/all/0/1\">Karol Urbanczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goschenhofer_J/0/1/0/all/0/1\">Jann Goschenhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heumann_C/0/1/0/all/0/1\">Christian Heumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hvingelby_R/0/1/0/all/0/1\">Rasmus Hvingelby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schalk_D/0/1/0/all/0/1\">Daniel Schalk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assenmacher_M/0/1/0/all/0/1\">Matthias A&#xdf;enmacher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Memorize Entailment and Discourse Relations for Persona-Consistent Dialogues. (arXiv:2301.04871v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04871","description":"<p>Maintaining engagement and consistency is particularly important in dialogue\nsystems. Existing works have improved the performance of dialogue systems by\nintentionally learning interlocutor personas with sophisticated network\nstructures. One issue with this approach is that it requires more personal\ncorpora with annotations. Additionally, these models typically perform the next\nutterance prediction to generate a response but neglect the discourse coherence\nin the entire conversation. To address these issues, this study proposes a\nmethod of learning to memorize entailment and discourse relations for\npersona-consistent dialogue tasks. Entailment text pairs in natural language\ninference dataset were applied to learn latent entailment relations as external\nmemories by premise-to-hypothesis generation task. Furthermore, an internal\nmemory with a similar architecture was applied to the discourse information in\nthe dialogue. Placing orthogonality restrictions on these two memory spaces\nensures that the latent entailment relations remain dialogue-independent. Both\nmemories collaborate to obtain entailment and discourse representation for the\ngeneration, allowing a deeper understanding of both consistency and coherence.\nExperiments on two large public datasets, PersonaChat and DSTC7-AVSD,\ndemonstrated the effectiveness of the proposed method. Both automatic and human\nevaluations indicate that the proposed model outperforms several strong\nbaselines in terms of both persona consistency and response coherence. Our\nsource code is available at https://github.com/Chenrj233/LMEDR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ruijun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Liang-Chih Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuejie Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images. (arXiv:2301.04883v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04883","description":"<p>Visual question answering on document images that contain textual, visual,\nand layout information, called document VQA, has received much attention\nrecently. Although many datasets have been proposed for developing document VQA\nsystems, most of the existing datasets focus on understanding the content\nrelationships within a single image and not across multiple images. In this\nstudy, we propose a new multi-image document VQA dataset, SlideVQA, containing\n2.6k+ slide decks composed of 52k+ slide images and 14.5k questions about a\nslide deck. SlideVQA requires complex reasoning, including single-hop,\nmulti-hop, and numerical reasoning, and also provides annotated arithmetic\nexpressions of numerical answers for enhancing the ability of numerical\nreasoning. Moreover, we developed a new end-to-end document VQA model that\ntreats evidence selection and question answering in a unified\nsequence-to-sequence format. Experiments on SlideVQA show that our model\noutperformed existing state-of-the-art QA models, but that it still has a large\ngap behind human performance. We believe that our dataset will facilitate\nresearch on document VQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_R/0/1/0/all/0/1\">Ryota Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishida_K/0/1/0/all/0/1\">Kyosuke Nishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishida_K/0/1/0/all/0/1\">Kosuke Nishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasegawa_T/0/1/0/all/0/1\">Taku Hasegawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_I/0/1/0/all/0/1\">Itsumi Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_K/0/1/0/all/0/1\">Kuniko Saito</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Think Twice: A Human-like Two-stage Conversational Agent for Emotional Response Generation. (arXiv:2301.04907v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04907","description":"<p>Towards human-like dialogue systems, current emotional dialogue approaches\njointly model emotion and semantics with a unified neural network. This\nstrategy tends to generate safe responses due to the mutual restriction between\nemotion and semantics, and requires rare emotion-annotated large-scale dialogue\ncorpus. Inspired by the \"think twice\" behavior in human dialogue, we propose a\ntwo-stage conversational agent for the generation of emotional dialogue.\nFirstly, a dialogue model trained without the emotion-annotated dialogue corpus\ngenerates a prototype response that meets the contextual semantics. Secondly,\nthe first-stage prototype is modified by a controllable emotion refiner with\nthe empathy hypothesis. Experimental results on the DailyDialog and\nEmpatheticDialogues datasets demonstrate that the proposed conversational\noutperforms the comparison models in emotion generation and maintains the\nsemantic performance in automatic and human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yushan Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shangzhao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bin_W/0/1/0/all/0/1\">Wu Bin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yuexian Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dataset of Kurdish (Sorani) Named Entities -- An Amendment to Kurdish-BLARK Named Entities. (arXiv:2301.04962v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04962","description":"<p>Named Entity Recognition (NER) is one of the essential applications of\nNatural Language Processing (NLP). It is also an instrument that plays a\nsignificant role in many other NLP applications, such as Machine Translation\n(MT), Information Retrieval (IR), and Part of Speech Tagging (POST). Kurdish is\nan under-resourced language from the NLP perspective. Particularly, in all the\ncategories, the lack of NER resources hinders other aspects of Kurdish\nprocessing. In this work, we present a data set that covers several categories\nof NEs in Kurdish (Sorani). The dataset is a significant amendment to a\npreviously developed dataset in the Kurdish BLARK (Basic Language Resource\nKit). It covers 11 categories and 33261 entries in total. The dataset is\npublicly available for non-commercial use under CC BY-NC-SA 4.0 license at\nhttps://kurdishblark.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salar_S/0/1/0/all/0/1\">Sazan Salar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassani_H/0/1/0/all/0/1\">Hossein Hassani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Everyone's Voice Matters: Quantifying Annotation Disagreement Using Demographic Information. (arXiv:2301.05036v1 [cs.CL])","link":"http://arxiv.org/abs/2301.05036","description":"<p>In NLP annotation, it is common to have multiple annotators label the text\nand then obtain the ground truth labels based on the agreement of major\nannotators. However, annotators are individuals with different backgrounds, and\nminors' opinions should not be simply ignored. As annotation tasks become\nsubjective and topics are controversial in modern NLP tasks, we need NLP\nsystems that can represent people's diverse voices on subjective matters and\npredict the level of diversity. This paper examines whether the text of the\ntask and annotators' demographic background information can be used to estimate\nthe level of disagreement among annotators. Particularly, we extract\ndisagreement labels from the annotators' voting histories in the five\nsubjective datasets, and then fine-tune language models to predict annotators'\ndisagreement. Our results show that knowing annotators' demographic\ninformation, like gender, ethnicity, and education level, helps predict\ndisagreements. In order to distinguish the disagreement from the inherent\ncontroversy from text content and the disagreement in the annotators' different\nperspectives, we simulate everyone's voices with different combinations of\nannotators' artificial demographics and examine its variance of the finetuned\ndisagreement predictor. Our paper aims to improve the annotation process for\nmore efficient and inclusive NLP systems through a novel disagreement\nprediction mechanism. Our code and dataset are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_R/0/1/0/all/0/1\">Ruyuan Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jaehyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Cognitive Evaluation of Instruction Generation Agents tl;dr They Need Better Theory-of-Mind Capabilities. (arXiv:2301.05149v1 [cs.CL])","link":"http://arxiv.org/abs/2301.05149","description":"<p>We mathematically characterize the cognitive capabilities that enable humans\nto effectively guide others through natural language. We show that\nneural-network-based instruction generation agents possess similar cognitive\ncapabilities, and design an evaluation scheme for probing those capabilities.\nOur results indicate that these agents, while capable of effectively narrowing\nthe search space, poorly predict the listener's interpretations of their\ninstructions and thus often fail to select the best instructions even from a\nsmall candidate set. We augment the agents with better theory-of-mind models of\nthe listener and obtain significant performance boost in guiding real humans.\nYet, there remains a considerable gap between our best agent and human guides.\nWe discuss the challenges in closing this gap, emphasizing the need to\nconstruct better models of human behavior when interacting with AI-based\nagents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lingjun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daume_H/0/1/0/all/0/1\">Hal Daum&#xe9; III</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Question Duplicate and Related Questions Detection in e-learning platforms. (arXiv:2301.05150v1 [cs.CL])","link":"http://arxiv.org/abs/2301.05150","description":"<p>Online learning platforms provide diverse questions to gauge the learners'\nunderstanding of different concepts. The repository of questions has to be\nconstantly updated to ensure a diverse pool of questions to conduct assessments\nfor learners. However, it is impossible for the academician to manually skim\nthrough the large repository of questions to check for duplicates when\nonboarding new questions from external sources. Hence, we propose a tool QDup\nin this paper that can surface near-duplicate and semantically related\nquestions without any supervised data. The proposed tool follows an\nunsupervised hybrid pipeline of statistical and neural approaches for\nincorporating different nuances in similarity for the task of question\nduplicate detection. We demonstrate that QDup can detect near-duplicate\nquestions and also suggest related questions for practice with remarkable\naccuracy and speed from a large repository of questions. The demo video of the\ntool can be found at https://www.youtube.com/watch?v=loh0_-7XLW4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhary_M/0/1/0/all/0/1\">Maksimjeet Chowdhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_S/0/1/0/all/0/1\">Sanyam Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+V_V/0/1/0/all/0/1\">Venktesh V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohania_M/0/1/0/all/0/1\">Mukesh Mohania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1\">Vikram Goyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Adaptation for French Named Entity Recognition. (arXiv:2301.05220v1 [cs.CL])","link":"http://arxiv.org/abs/2301.05220","description":"<p>Named Entity Recognition (NER) is the task of identifying and classifying\nnamed entities in large-scale texts into predefined classes. NER in French and\nother relatively limited-resource languages cannot always benefit from\napproaches proposed for languages like English due to a dearth of large, robust\ndatasets. In this paper, we present our work that aims to mitigate the effects\nof this dearth of large, labeled datasets. We propose a Transformer-based NER\napproach for French, using adversarial adaptation to similar domain or general\ncorpora to improve feature extraction and enable better generalization. Our\napproach allows learning better features using large-scale unlabeled corpora\nfrom the same domain or mixed domains to introduce more variations during\ntraining and reduce overfitting. Experimental results on three labeled datasets\nshow that our adaptation framework outperforms the corresponding non-adaptive\nmodels for various combinations of Transformer models, source datasets, and\ntarget corpora. We also show that adversarial adaptation to large-scale\nunlabeled corpora can help mitigate the performance dip incurred on using\nTransformer models pre-trained on smaller corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choudhry_A/0/1/0/all/0/1\">Arjun Choudhry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khatri_I/0/1/0/all/0/1\">Inder Khatri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Pankaj Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aaryan Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicol_M/0/1/0/all/0/1\">Maxime Nicol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meurs_M/0/1/0/all/0/1\">Marie-Jean Meurs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vishwakarma_D/0/1/0/all/0/1\">Dinesh Kumar Vishwakarma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning. (arXiv:2301.05226v1 [cs.CV])","link":"http://arxiv.org/abs/2301.05226","description":"<p>Large pre-trained vision and language models have demonstrated remarkable\ncapacities for various tasks. However, solving the knowledge-based visual\nreasoning tasks remains challenging, which requires a model to comprehensively\nunderstand image content, connect the external world knowledge, and perform\nstep-by-step reasoning to answer the questions correctly. To this end, we\npropose a novel framework named Interactive Prompting Visual Reasoner (IPVR)\nfor few-shot knowledge-based visual reasoning. IPVR contains three stages, see,\nthink and confirm. The see stage scans the image and grounds the visual concept\ncandidates with a visual perception model. The think stage adopts a pre-trained\nlarge language model (LLM) to attend to the key concepts from candidates\nadaptively. It then transforms them into text context for prompting with a\nvisual captioning model and adopts the LLM to generate the answer. The confirm\nstage further uses the LLM to generate the supporting rationale to the answer,\nverify the generated rationale with a cross-modality classifier and ensure that\nthe rationale can infer the predicted output consistently. We conduct\nexperiments on a range of knowledge-based visual reasoning datasets. We found\nour IPVR enjoys several benefits, 1). it achieves better performance than the\nprevious few-shot learning baselines; 2). it enjoys the total transparency and\ntrustworthiness of the whole reasoning process by providing rationales for each\nreasoning step; 3). it is computation-efficient compared with other fine-tuning\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenfang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qinhong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yikang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yining Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Transformers with Natural Language Explanations. (arXiv:2110.00125v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.00125","description":"<p>Transformers changed modern NLP in many ways. However, like many other neural\narchitectures, they are still weak on exploiting domain knowledge and on\ninterpretability. Unfortunately, the exploitation of external, structured\nknowledge is notoriously prone to a knowledge acquisition bottleneck. We thus\npropose a memory enhancement of transformer models that makes use of\nunstructured knowledge. That, expressed in plain text, can be used to carry out\nclassification tasks and as a source of explanations for the model output. An\nexperimental evaluation conducted on two challenging datasets demonstrates that\nour approach produces relevant explanations without losing in performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruggeri_F/0/1/0/all/0/1\">Federico Ruggeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippi_M/0/1/0/all/0/1\">Marco Lippi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torroni_P/0/1/0/all/0/1\">Paolo Torroni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-01-12T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}