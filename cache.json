{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-01-11T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Neighborhood-Regularized Self-Training for Learning with Few Labels. (arXiv:2301.03726v1 [cs.LG])","link":"http://arxiv.org/abs/2301.03726","description":"<p>Training deep neural networks (DNNs) with limited supervision has been a\npopular research topic as it can significantly alleviate the annotation burden.\nSelf-training has been successfully applied in semi-supervised learning tasks,\nbut one drawback of self-training is that it is vulnerable to the label noise\nfrom incorrect pseudo labels. Inspired by the fact that samples with similar\nlabels tend to share similar representations, we develop a neighborhood-based\nsample selection approach to tackle the issue of noisy pseudo labels. We\nfurther stabilize self-training via aggregating the predictions from different\nrounds during sample selection. Experiments on eight tasks show that our\nproposed method outperforms the strongest self-training baseline with 1.83% and\n2.51% performance gain for text and graph datasets on average. Our further\nanalysis demonstrates that our proposed data selection strategy reduces the\nnoise of pseudo labels by 36.8% and saves 57.3% of the time when compared with\nthe best baseline. Our code and appendices will be uploaded to\nhttps://github.com/ritaranx/NeST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Hejie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_X/0/1/0/all/0/1\">Xuan Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanqiao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Joyce Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Laws for Generative Mixed-Modal Language Models. (arXiv:2301.03728v1 [cs.CL])","link":"http://arxiv.org/abs/2301.03728","description":"<p>Generative language models define distributions over sequences of tokens that\ncan represent essentially any combination of data modalities (e.g., any\npermutation of image tokens from VQ-VAEs, speech tokens from HuBERT, BPE tokens\nfor language or code, and so on). To better understand the scaling properties\nof such mixed-modal models, we conducted over 250 experiments using seven\ndifferent modalities and model sizes ranging from 8 million to 30 billion,\ntrained on 5-100 billion tokens. We report new mixed-modal scaling laws that\nunify the contributions of individual modalities and the interactions between\nthem. Specifically, we explicitly model the optimal synergy and competition due\nto data and model size as an additive term to previous uni-modal scaling laws.\nWe also find four empirical phenomena observed during the training, such as\nemergent coordinate-ascent style training that naturally alternates between\nmodalities, guidelines for selecting critical hyper-parameters, and connections\nbetween mixed-modal competition and training stability. Finally, we test our\nscaling law by training a 30B speech-text model, which significantly\noutperforms the corresponding unimodal models. Overall, our research provides\nvaluable insights into the design and training of mixed-modal generative\nmodels, an important new class of unified models that have unique\ndistributional properties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lili Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conneau_A/0/1/0/all/0/1\">Alexis Conneau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hambardzumyan_K/0/1/0/all/0/1\">Karen Hambardzumyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Susan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1\">Stephen Roller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Naman Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Model Comparative Loss for Enhancing Neuronal Utility in Language Understanding. (arXiv:2301.03765v1 [cs.CL])","link":"http://arxiv.org/abs/2301.03765","description":"<p>Current natural language understanding (NLU) models have been continuously\nscaling up, both in terms of model size and input context, introducing more\nhidden and input neurons. While this generally improves performance on average,\nthe extra neurons do not yield a consistent improvement for all instances. This\nis because some hidden neurons are redundant, and the noise mixed in input\nneurons tends to distract the model. Previous work mainly focuses on\nextrinsically reducing low-utility neurons by additional post- or\npre-processing, such as network pruning and context selection, to avoid this\nproblem. Beyond that, can we make the model reduce redundant parameters and\nsuppress input noise by intrinsically enhancing the utility of each neuron? If\na model can efficiently utilize neurons, no matter which neurons are ablated\n(disabled), the ablated submodel should perform no better than the original\nfull model. Based on such a comparison principle between models, we propose a\ncross-model comparative loss for a broad range of tasks. Comparative loss is\nessentially a ranking loss on top of the task-specific losses of the full and\nablated models, with the expectation that the task-specific loss of the full\nmodel is minimal. We demonstrate the universal effectiveness of comparative\nloss through extensive experiments on 14 datasets from 3 distinct NLU tasks\nbased on 4 widely used pretrained language models, and find it particularly\nsuperior for models with few parameters or long input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yunchang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kangxi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UnifySpeech: A Unified Framework for Zero-shot Text-to-Speech and Voice Conversion. (arXiv:2301.03801v1 [cs.SD])","link":"http://arxiv.org/abs/2301.03801","description":"<p>Text-to-speech (TTS) and voice conversion (VC) are two different tasks both\naiming at generating high quality speaking voice according to different input\nmodality. Due to their similarity, this paper proposes UnifySpeech, which\nbrings TTS and VC into a unified framework for the first time. The model is\nbased on the assumption that speech can be decoupled into three independent\ncomponents: content information, speaker information, prosody information. Both\nTTS and VC can be regarded as mining these three parts of information from the\ninput and completing the reconstruction of speech. For TTS, the speech content\ninformation is derived from the text, while in VC it's derived from the source\nspeech, so all the remaining units are shared except for the speech content\nextraction module in the two tasks. We applied vector quantization and domain\nconstrain to bridge the gap between the content domains of TTS and VC.\nObjective and subjective evaluation shows that by combining the two task, TTS\nobtains better speaker modeling ability while VC gets hold of impressive speech\ncontent decoupling capability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haogeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1\">Ruibo Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jiangyan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zhengqi Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1\">Jianhua Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Streaming Punctuation: A Novel Punctuation Technique Leveraging Bidirectional Context for Continuous Speech Recognition. (arXiv:2301.03819v1 [cs.CL])","link":"http://arxiv.org/abs/2301.03819","description":"<p>While speech recognition Word Error Rate (WER) has reached human parity for\nEnglish, continuous speech recognition scenarios such as voice typing and\nmeeting transcriptions still suffer from segmentation and punctuation problems,\nresulting from irregular pausing patterns or slow speakers. Transformer\nsequence tagging models are effective at capturing long bi-directional context,\nwhich is crucial for automatic punctuation. Automatic Speech Recognition (ASR)\nproduction systems, however, are constrained by real-time requirements, making\nit hard to incorporate the right context when making punctuation decisions.\nContext within the segments produced by ASR decoders can be helpful but\nlimiting in overall punctuation performance for a continuous speech session. In\nthis paper, we propose a streaming approach for punctuation or re-punctuation\nof ASR output using dynamic decoding windows and measure its impact on\npunctuation and segmentation accuracy across scenarios. The new system tackles\nover-segmentation issues, improving segmentation F0.5-score by 13.9%. Streaming\npunctuation achieves an average BLEUscore improvement of 0.66 for the\ndownstream task of Machine Translation (MT).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Behre_P/0/1/0/all/0/1\">Piyush Behre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Sharman Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varadharajan_P/0/1/0/all/0/1\">Padma Varadharajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuangyu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The use of new technologies to support Public Administration. Sentiment analysis and the case of the app IO. (arXiv:2301.03848v1 [cs.SI])","link":"http://arxiv.org/abs/2301.03848","description":"<p>App IO is an app developed for the Italian PA. It is definitely useful for\ncitizens to interact with the PA and to get services that were not digitized\nyet. Nevertheless, it was not perceived in a good way by the citizens and it\nhas been criticized. As we wanted to find the root that caused all these bad\nreviews we scraped feedback from mobile app stores using custom-coded automated\ntools and - after that - we trained two machine learning models to perform both\nsentiment analysis and emotion detection to understand what caused the bad\nreviews.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miracula_V/0/1/0/all/0/1\">Vincenzo Miracula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picone_A/0/1/0/all/0/1\">Antonio Picone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Channel-aware Decoupling Network for Multi-turn Dialogue Comprehension. (arXiv:2301.03953v1 [cs.CL])","link":"http://arxiv.org/abs/2301.03953","description":"<p>Training machines to understand natural language and interact with humans is\none of the major goals of artificial intelligence. Recent years have witnessed\nan evolution from matching networks to pre-trained language models (PrLMs). In\ncontrast to the plain-text modeling as the focus of the PrLMs, dialogue texts\ninvolve multiple speakers and reflect special characteristics such as topic\ntransitions and structure dependencies between distant utterances. However, the\nrelated PrLM models commonly represent dialogues sequentially by processing the\npairwise dialogue history as a whole. Thus the hierarchical information on\neither utterance interrelation or speaker roles coupled in such representations\nis not well addressed. In this work, we propose compositional learning for\nholistic interaction across the utterances beyond the sequential\ncontextualization from PrLMs, in order to capture the utterance-aware and\nspeaker-aware representations entailed in a dialogue history. We decouple the\ncontextualized word representations by masking mechanisms in Transformer-based\nPrLM, making each word only focus on the words in current utterance, other\nutterances, and two speaker roles (i.e., utterances of sender and utterances of\nthe receiver), respectively. In addition, we employ domain-adaptive training\nstrategies to help the model adapt to the dialogue domains. Experimental\nresults show that our method substantially boosts the strong PrLM baselines in\nfour public benchmark datasets, achieving new state-of-the-art performance over\nprevious methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Longxiang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI based approach to Trailer Generation for Online Educational Courses. (arXiv:2301.03957v1 [cs.CL])","link":"http://arxiv.org/abs/2301.03957","description":"<p>In this paper, we propose an AI based approach to Trailer Generation in the\nform of short videos for online educational courses. Trailers give an overview\nof the course to the learners and help them make an informed choice about the\ncourses they want to learn. It also helps to generate curiosity and interest\namong the learners and encourages them to pursue a course. While it is possible\nto manually generate the trailers, it requires extensive human efforts and\nskills over a broad spectrum of design, span selection, video editing, domain\nknowledge, etc., thus making it time-consuming and expensive, especially in an\nacademic setting. The framework we propose in this work is a template based\nmethod for video trailer generation, where most of the textual content of the\ntrailer is auto-generated and the trailer video is automatically generated, by\nleveraging Machine Learning and Natural Language Processing techniques. The\nproposed trailer is in the form of a timeline consisting of various fragments\ncreated by selecting, para-phrasing or generating content using various\nproposed techniques. The fragments are further enhanced by adding voice-over\ntext, subtitles, animations, etc., to create a holistic experience. Finally, we\nperform user evaluation with 63 human evaluators for evaluating the trailers\ngenerated by our system and the results obtained were encouraging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1\">Prakhar Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diwan_C/0/1/0/all/0/1\">Chaitali Diwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasa_S/0/1/0/all/0/1\">Srinath Srinivasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasaraghavan_G/0/1/0/all/0/1\">G. Srinivasaraghavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Mandarin-Cantonese Machine Translation. (arXiv:2301.03971v1 [cs.CL])","link":"http://arxiv.org/abs/2301.03971","description":"<p>Advancements in unsupervised machine translation have enabled the development\nof machine translation systems that can translate between languages for which\nthere is not an abundance of parallel data available. We explored unsupervised\nmachine translation between Mandarin Chinese and Cantonese. Despite the vast\nnumber of native speakers of Cantonese, there is still no large-scale corpus\nfor the language, due to the fact that Cantonese is primarily used for oral\ncommunication. The key contributions of our project include: 1. The creation of\na new corpus containing approximately 1 million Cantonese sentences, and 2. A\nlarge-scale comparison across different model architectures, tokenization\nschemes, and embedding structures. Our best model trained with character-based\ntokenization and a Transformer architecture achieved a character-level BLEU of\n25.1 when translating from Mandarin to Cantonese and of 24.4 when translating\nfrom Cantonese to Mandarin. In this paper we discuss our research process,\nexperiments, and results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dare_M/0/1/0/all/0/1\">Megan Dare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_V/0/1/0/all/0/1\">Valentina Fajardo Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+So_A/0/1/0/all/0/1\">Averie Ho Zoen So</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shibingfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models sounds the Death Knell of Knowledge Graphs. (arXiv:2301.03980v1 [cs.CL])","link":"http://arxiv.org/abs/2301.03980","description":"<p>Healthcare domain generates a lot of unstructured and semi-structured text.\nNatural Language processing (NLP) has been used extensively to process this\ndata. Deep Learning based NLP especially Large Language Models (LLMs) such as\nBERT have found broad acceptance and are used extensively for many\napplications. A Language Model is a probability distribution over a word\nsequence. Self-supervised Learning on a large corpus of data automatically\ngenerates deep learning-based language models. BioBERT and Med-BERT are\nlanguage models pre-trained for the healthcare domain. Healthcare uses typical\nNLP tasks such as question answering, information extraction, named entity\nrecognition, and search to simplify and improve processes. However, to ensure\nrobust application of the results, NLP practitioners need to normalize and\nstandardize them. One of the main ways of achieving normalization and\nstandardization is the use of Knowledge Graphs. A Knowledge Graph captures\nconcepts and their relationships for a specific domain, but their creation is\ntime-consuming and requires manual intervention from domain experts, which can\nprove expensive. SNOMED CT (Systematized Nomenclature of Medicine -- Clinical\nTerms), Unified Medical Language System (UMLS), and Gene Ontology (GO) are\npopular ontologies from the healthcare domain. SNOMED CT and UMLS capture\nconcepts such as disease, symptoms and diagnosis and GO is the world's largest\nsource of information on the functions of genes. Healthcare has been dealing\nwith an explosion in information about different types of drugs, diseases, and\nprocedures. This paper argues that using Knowledge Graphs is not the best\nsolution for solving problems in this domain. We present experiments using LLMs\nfor the healthcare domain to demonstrate that language models provide the same\nfunctionality as knowledge graphs, thereby making knowledge graphs redundant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suri_K/0/1/0/all/0/1\">Kunal Suri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Atul Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1\">Prakhar Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rout_S/0/1/0/all/0/1\">Swapna Sourav Rout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabapathy_R/0/1/0/all/0/1\">Rajesh Sabapathy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"There is No Big Brother or Small Brother: Knowledge Infusion in Language Models for Link Prediction and Question Answering. (arXiv:2301.04013v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04013","description":"<p>The integration of knowledge graphs with deep learning is thriving in\nimproving the performance of various natural language processing (NLP) tasks.\nIn this paper, we focus on knowledge-infused link prediction and question\nanswering using language models, T5, and BLOOM across three domains: Aviation,\nMovie, and Web. In this context, we infuse knowledge in large and small\nlanguage models and study their performance, and find the performance to be\nsimilar. For the link prediction task on the Aviation Knowledge Graph, we\nobtain a 0.2 hits@1 score using T5-small, T5-base, T5-large, and BLOOM. Using\ntemplate-based scripts, we create a set of 1 million synthetic factoid QA pairs\nin the aviation domain from National Transportation Safety Board (NTSB)\nreports. On our curated QA pairs, the three models of T5 achieve a 0.7 hits@1\nscore. We validate out findings with the paired student t-test and Cohen's\nkappa scores. For link prediction on Aviation Knowledge Graph using T5-small\nand T5-large, we obtain a Cohen's kappa score of 0.76, showing substantial\nagreement between the models. Thus, we infer that small language models perform\nsimilar to large language models with the infusion of knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Ankush Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gawade_S/0/1/0/all/0/1\">Sakharam Gawade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Channabasavarajendra_S/0/1/0/all/0/1\">Sachin Channabasavarajendra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Conversational Search Behavior For Domain Exploration. (arXiv:2301.04098v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04098","description":"<p>Conversational search has evolved as a new information retrieval paradigm,\nmarking a shift from traditional search systems towards interactive dialogues\nwith intelligent search agents. This change especially affects exploratory\ninformation-seeking contexts, where conversational search systems can guide the\ndiscovery of unfamiliar domains. In these scenarios, users find it often\ndifficult to express their information goals due to insufficient background\nknowledge. Conversational interfaces can provide assistance by eliciting\ninformation needs and narrowing down the search space. However, due to the\ncomplexity of information-seeking behavior, the design of conversational\ninterfaces for retrieving information remains a great challenge. Although prior\nwork has employed user studies to empirically ground the system design, most\nexisting studies are limited to well-defined search tasks or known domains,\nthus being less exploratory in nature. Therefore, we conducted a laboratory\nstudy to investigate open-ended search behavior for navigation through unknown\ninformation landscapes. The study comprised of 26 participants who were\nrestricted in their search to a text chat interface. Based on the collected\ndialogue transcripts, we applied statistical analyses and process mining\ntechniques to uncover general information-seeking patterns across five\ndifferent domains. We not only identify core dialogue acts and their\ninterrelations that enable users to discover domain knowledge, but also derive\ndesign suggestions for conversational search systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_P/0/1/0/all/0/1\">Phillip Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afzal_A/0/1/0/all/0/1\">Anum Afzal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vladika_J/0/1/0/all/0/1\">Juraj Vladika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braun_D/0/1/0/all/0/1\">Daniel Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthes_F/0/1/0/all/0/1\">Florian Matthes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Case-based Reasoning for Inference-time Adaptation of Text-to-SQL parsers. (arXiv:2301.04110v1 [cs.CL])","link":"http://arxiv.org/abs/2301.04110","description":"<p>Inference-time adaptation methods for semantic parsing are useful for\nleveraging examples from newly-observed domains without repeated fine-tuning.\nExisting approaches typically bias the decoder by simply concatenating\ninput-output example pairs (cases) from the new domain at the encoder's input\nin a Seq-to-Seq model. Such methods cannot adequately leverage the structure of\nlogical forms in the case examples. We propose StructCBR, a structured\ncase-based reasoning approach, which leverages subtree-level similarity between\nlogical forms of cases and candidate outputs, resulting in better decoder\ndecisions. For the task of adapting Text-to-SQL models to unseen schemas, we\nshow that exploiting case examples in a structured manner via StructCBR offers\nconsistent performance improvements over prior inference-time adaptation\nmethods across five different databases. To the best of our knowledge, we are\nthe first to attempt inference-time adaptation of Text-to-SQL models, and\nharness trainable structured similarity between subqueries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Awasthi_A/0/1/0/all/0/1\">Abhijeet Awasthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1\">Soumen Chakrabarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1\">Sunita Sarawagi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BASPRO: a balanced script producer for speech corpus collection based on the genetic algorithm. (arXiv:2301.04120v1 [cs.NE])","link":"http://arxiv.org/abs/2301.04120","description":"<p>The performance of speech-processing models is heavily influenced by the\nspeech corpus that is used for training and evaluation. In this study, we\npropose BAlanced Script PROducer (BASPRO) system, which can automatically\nconstruct a phonetically balanced and rich set of Chinese sentences for\ncollecting Mandarin Chinese speech data. First, we used pretrained natural\nlanguage processing systems to extract ten-character candidate sentences from a\nlarge corpus of Chinese news texts. Then, we applied a genetic algorithm-based\nmethod to select 20 phonetically balanced sentence sets, each containing 20\nsentences, from the candidate sentences. Using BASPRO, we obtained a recording\nscript called TMNews, which contains 400 ten-character sentences. TMNews covers\n84% of the syllables used in the real world. Moreover, the syllable\ndistribution has 0.96 cosine similarity to the real-world syllable\ndistribution. We converted the script into a speech corpus using two\ntext-to-speech systems. Using the designed speech corpus, we tested the\nperformances of speech enhancement (SE) and automatic speech recognition (ASR),\nwhich are one of the most important regression- and classification-based speech\nprocessing tasks, respectively. The experimental results show that the SE and\nASR models trained on the designed speech corpus outperform their counterparts\ntrained on a randomly composed speech corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Wen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Scheduled Sampling with Elastic Weight Consolidation for Neural Machine Translation. (arXiv:2109.06308v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06308","description":"<p>Despite strong performance in many sequence-to-sequence tasks, autoregressive\nmodels trained with maximum likelihood estimation suffer from exposure bias,\ni.e. the discrepancy between the ground-truth prefixes used during training and\nthe model-generated prefixes used at inference time. Scheduled sampling is a\nsimple and empirically successful approach which addresses this issue by\nincorporating model-generated prefixes into training. However, it has been\nargued that it is an inconsistent training objective leading to models ignoring\nthe prefixes altogether. In this paper, we conduct systematic experiments and\nfind that scheduled sampling, while it ameliorates exposure bias by increasing\nmodel reliance on the input sequence, worsens performance when the prefix at\ninference time is correct, a form of catastrophic forgetting. We propose to use\nElastic Weight Consolidation to better balance mitigating exposure bias with\nretaining performance. Experiments on four IWSLT'14 and WMT'14 translation\ndatasets demonstrate that our approach alleviates catastrophic forgetting and\nsignificantly outperforms maximum likelihood estimation and scheduled sampling\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Korakakis_M/0/1/0/all/0/1\">Michalis Korakakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why Exposure Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation. (arXiv:2204.01171v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.01171","description":"<p>Current language generation models suffer from issues such as repetition,\nincoherence, and hallucinations. An often-repeated hypothesis is that this\nbrittleness of generation models is caused by the training and the generation\nprocedure mismatch, also referred to as exposure bias. In this paper, we verify\nthis hypothesis by analyzing exposure bias from an imitation learning\nperspective. We show that exposure bias leads to an accumulation of errors,\nanalyze why perplexity fails to capture this accumulation, and empirically show\nthat this accumulation results in poor generation quality. Source code to\nreproduce these experiments is available at\nhttps://github.com/kushalarora/quantifying_exposure_bias\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_K/0/1/0/all/0/1\">Kushal Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asri_L/0/1/0/all/0/1\">Layla El Asri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahuleyan_H/0/1/0/all/0/1\">Hareesh Bahuleyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jackie Chi Kit Cheung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Accurate and Faithful Discharge Instructions: Task, Dataset, and Model. (arXiv:2210.12777v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12777","description":"<p>The \"Patient Instruction\" (PI), known as \"Discharge Instruction\", which\ncontains critical instructional information provided both to carers and to the\npatient at the time of discharge, is essential for the patient to manage their\ncondition outside hospital. An accurate and easy-to-follow PI can improve the\nself-management of patients which can in turn reduce hospital readmission\nrates. However, writing an appropriate PI can be extremely time-consuming for\nphysicians, and is subject to being incomplete or error-prone for (potentially\noverworked) physicians. Therefore, we propose a new task that can provide an\nobjective means of avoiding incompleteness, while reducing clinical workload:\nthe automatic generation of the PI, which is imagined as being a document that\nthe clinician can review, modify, and approve as necessary (rather than taking\nthe human \"out of the loop\"). We build a benchmark clinical dataset and propose\nthe Re3Writer, which imitates the working patterns of physicians to first\nretrieve related working experience from historical PIs written by physicians,\nthen reason related medical knowledge. Finally, it refines the retrieved\nworking experience and reasoned medical knowledge to extract useful\ninformation, which is used to generate the PI for previously-unseen patient\naccording to their health records during hospitalization. Our experiments show\nthat, using our method, the performance of five different models can be\nsubstantially boosted across all metrics, with up to 20%, 11%, and 19% relative\nimprovements in BLEU-4, ROUGE-L, and METEOR, respectively. Meanwhile, we show\nresults from human evaluations to measure the effectiveness in terms of its\nusefulness for clinical practice. The code is available at\nhttps://github.com/AI-in-Hospitals/Patient-Instructions\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhangdaihong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1\">David A. Clifton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning over Different Types of Knowledge Graphs: Static, Temporal and Multi-Modal. (arXiv:2212.05767v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2212.05767","description":"<p>Knowledge graph reasoning (KGR), aiming to deduce new facts from existing\nfacts based on mined logic rules underlying knowledge graphs (KGs), has become\na fast-growing research direction. It has been proven to significantly benefit\nthe usage of KGs in many AI applications, such as question answering and\nrecommendation systems, etc. According to the graph types, the existing KGR\nmodels can be roughly divided into three categories, i.e., static models,\ntemporal models, and multi-modal models. The early works in this domain mainly\nfocus on static KGR and tend to directly apply general knowledge graph\nembedding models to the reasoning task. However, these models are not suitable\nfor more complex but practical tasks, such as inductive static KGR, temporal\nKGR, and multi-modal KGR. To this end, multiple works have been developed\nrecently, but no survey papers and open-source repositories comprehensively\nsummarize and discuss models in this important direction. To fill the gap, we\nconduct a survey for knowledge graph reasoning tracing from static to temporal\nand then to multi-modal KGs. Concretely, the preliminaries, summaries of KGR\nmodels, and typical datasets are introduced and discussed consequently.\nMoreover, we discuss the challenges and potential opportunities. The\ncorresponding open-source repository is shared on GitHub:\nhttps://github.com/LIANGKE23/Awesome-Knowledge-Graph-Reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1\">Ke Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Lingyuan Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Meng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1\">Wenxuan Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sihang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinwang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fuchun Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CREPE: Can Vision-Language Foundation Models Reason Compositionally?. (arXiv:2212.07796v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.07796","description":"<p>A fundamental characteristic common to both human vision and natural language\nis their compositional nature. Yet, despite the performance gains contributed\nby large vision and language pretraining, we find that - across 6 architectures\ntrained with 4 algorithms on massive datasets - they exhibit little\ncompositionality. To arrive at this conclusion, we introduce a new\ncompositionality evaluation benchmark CREPE which measures two important\naspects of compositionality identified by cognitive science literature:\nsystematicity and productivity. To measure systematicity, CREPE consists of\nthree test datasets. The three test sets are designed to test models trained on\nthree of the popular training datasets: CC-12M, YFCC-15M, and LAION-400M. They\ncontain 385K, 385K, and 373K image-text pairs and 237K, 210K, and 178K hard\nnegative captions. To test productivity, CREPE contains 17K image-text pairs\nwith nine different complexities plus 246K hard negative captions with atomic,\nswapping, and negation foils. The datasets are generated by repurposing the\nVisual Genome scene graphs and region descriptions and applying handcrafted\ntemplates and GPT-3. For systematicity, we find that model performance\ndecreases consistently when novel compositions dominate the retrieval set, with\nRecall@1 dropping by up to 8%. For productivity, models' retrieval success\ndecays as complexity increases, frequently nearing random chance at high\ncomplexity. These results hold regardless of model and training dataset size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zixian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jerry Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gul_M/0/1/0/all/0/1\">Mustafa Omer Gul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_M/0/1/0/all/0/1\">Mona Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_I/0/1/0/all/0/1\">Irena Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1\">Ranjay Krishna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AnnoBERT: Effectively Representing Multiple Annotators' Label Choices to Improve Hate Speech Detection. (arXiv:2212.10405v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10405","description":"<p>Supervised approaches generally rely on majority-based labels. However, it is\nhard to achieve high agreement among annotators in subjective tasks such as\nhate speech detection. Existing neural network models principally regard labels\nas categorical variables, while ignoring the semantic information in diverse\nlabel texts. In this paper, we propose AnnoBERT, a first-of-its-kind\narchitecture integrating annotator characteristics and label text with a\ntransformer-based model to detect hate speech, with unique representations\nbased on each annotator's characteristics via Collaborative Topic Regression\n(CTR) and integrate label text to enrich textual representations. During\ntraining, the model associates annotators with their label choices given a\npiece of text; during evaluation, when label information is not available, the\nmodel predicts the aggregated label given by the participating annotators by\nutilising the learnt association. The proposed approach displayed an advantage\nin detecting hate speech, especially in the minority class and edge cases with\nannotator disagreement. Improvement in the overall performance is the largest\nwhen the dataset is more label-imbalanced, suggesting its practical value in\nidentifying real-world hate speech, as the volume of hate speech in-the-wild is\nextremely small on social media, when compared with normal (non-hate) speech.\nThrough ablation studies, we show the relative contributions of annotator\nembeddings and label text to the model performance, and tested a range of\nalternative annotator embeddings and label text combinations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenjie Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_V/0/1/0/all/0/1\">Vibhor Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1\">Aiqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sastry_N/0/1/0/all/0/1\">Nishanth Sastry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Structured Object Sequence Encoders. (arXiv:2301.01015v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2301.01015","description":"<p>In this paper we explore the task of modeling (semi) structured object\nsequences; in particular we focus our attention on the problem of developing a\nstructure-aware input representation for such sequences. In such sequences, we\nassume that each structured object is represented by a set of key-value pairs\nwhich encode the attributes of the structured object. Given a universe of keys,\na sequence of structured objects can then be viewed as an evolution of the\nvalues for each key, over time. We encode and construct a sequential\nrepresentation using the values for a particular key (Temporal Value Modeling -\nTVM) and then self-attend over the set of key-conditioned value sequences to a\ncreate a representation of the structured object sequence (Key Aggregation -\nKA). We pre-train and fine-tune the two components independently and present an\ninnovative training schedule that interleaves the training of both modules with\nshared attention heads. We find that this iterative two part-training results\nin better performance than a unified network with hierarchical encoding as well\nas over, other methods that use a {\\em record-view} representation of the\nsequence \\cite{de2021transformers4rec} or a simple {\\em flattened}\nrepresentation of the sequence. We conduct experiments using real-world data to\ndemonstrate the advantage of interleaving TVM-KA on multiple tasks and detailed\nablation studies motivating our modeling choices. We find that our approach\nperforms better than flattening sequence objects and also allows us to operate\non significantly larger sequences than existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+V_R/0/1/0/all/0/1\">Rudra Murthy V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1\">Riyaz Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunasekara_C/0/1/0/all/0/1\">Chulaka Gunasekara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1\">Siva Sankalp Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_H/0/1/0/all/0/1\">Hui Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamecha_T/0/1/0/all/0/1\">Tejas Indulal Dhamecha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Contractor_D/0/1/0/all/0/1\">Danish Contractor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danilevsky_M/0/1/0/all/0/1\">Marina Danilevsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Traditional Readability Formulas Compared for English. (arXiv:2301.02975v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.02975","description":"<p>Traditional English readability formulas, or equations, were largely\ndeveloped in the 20th century. Nonetheless, many researchers still rely on them\nfor various NLP applications. This phenomenon is presumably due to the\nconvenience and straightforwardness of readability formulas. In this work, we\ncontribute to the NLP community by 1. introducing New English Readability\nFormula (NERF), 2. recalibrating the coefficients of old readability formulas\n(Flesch-Kincaid Grade Level, Fog Index, SMOG Index, Coleman-Liau Index, and\nAutomated Readability Index), 3. evaluating the readability formulas, for use\nin text simplification studies and medical texts, and 4. developing a\nPython-based program for the wide application to various NLP projects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bruce W. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jason Hyung-Jong Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Generation of German Drama Texts Using Fine Tuned GPT-2 Models. (arXiv:2301.03119v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.03119","description":"<p>This study is devoted to the automatic generation of German drama texts. We\nsuggest an approach consisting of two key steps: fine-tuning a GPT-2 model (the\noutline model) to generate outlines of scenes based on keywords and fine-tuning\na second model (the generation model) to generate scenes from the scene\noutline. The input for the neural model comprises two datasets: the German\nDrama Corpus (GerDraCor) and German Text Archive (Deutsches Textarchiv or DTA).\nIn order to estimate the effectiveness of the proposed method, our models are\ncompared with baseline GPT-2 models. Our models perform well according to\nautomatic quantitative evaluation, but, conversely, manual qualitative analysis\nreveals a poor quality of generated texts. This may be due to the quality of\nthe dataset or training inputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bangura_M/0/1/0/all/0/1\">Mariam Bangura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barabashova_K/0/1/0/all/0/1\">Kristina Barabashova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karnysheva_A/0/1/0/all/0/1\">Anna Karnysheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semczuk_S/0/1/0/all/0/1\">Sarah Semczuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-01-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}