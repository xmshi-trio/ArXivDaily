{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-12-23T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"KL Regularized Normalization Framework for Low Resource Tasks. (arXiv:2212.11275v1 [cs.CL])","link":"http://arxiv.org/abs/2212.11275","description":"<p>Large pre-trained models, such as Bert, GPT, and Wav2Vec, have demonstrated\ngreat potential for learning representations that are transferable to a wide\nvariety of downstream tasks . It is difficult to obtain a large quantity of\nsupervised data due to the limited availability of resources and time. In light\nof this, a significant amount of research has been conducted in the area of\nadopting large pre-trained datasets for diverse downstream tasks via fine\ntuning, linear probing, or prompt tuning in low resource settings.\nNormalization techniques are essential for accelerating training and improving\nthe generalization of deep neural networks and have been successfully used in a\nwide variety of applications. A lot of normalization techniques have been\nproposed but the success of normalization in low resource downstream NLP and\nspeech tasks is limited. One of the reasons is the inability to capture\nexpressiveness by rescaling parameters of normalization. We propose\nKullbackLeibler(KL) Regularized normalization (KL-Norm) which make the\nnormalized data well behaved and helps in better generalization as it reduces\nover-fitting, generalises well on out of domain distributions and removes\nirrelevant biases and features with negligible increase in model parameters and\nmemory overheads. Detailed experimental evaluation on multiple low resource NLP\nand speech tasks, demonstrates the superior performance of KL-Norm as compared\nto other popular normalization and regularization techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1\">Neeraj Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_A/0/1/0/all/0/1\">Ankur Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lall_B/0/1/0/all/0/1\">Brejesh Lall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language models are better than humans at next-token prediction. (arXiv:2212.11281v1 [cs.CL])","link":"http://arxiv.org/abs/2212.11281","description":"<p>Current language models are considered to have sub-human capabilities at\nnatural language tasks like question-answering or writing code. However,\nlanguage models are not trained to perform well at these tasks, they are\ntrained to accurately predict the next token given previous tokes in tokenized\ntext. It is not clear whether language models are better or worse than humans\nat next token prediction. To try to answer this question, we performed two\ndistinct experiments to directly compare humans and language models on this\nfront: one measuring top-1 accuracy and the other measuring perplexity. In both\nexperiments, we find humans to be consistently \\emph{worse} than even\nrelatively small language models like GPT3-Ada at next-token prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shlegeris_B/0/1/0/all/0/1\">Buck Shlegeris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roger_F/0/1/0/all/0/1\">Fabien Roger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_L/0/1/0/all/0/1\">Lawrence Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McLean_E/0/1/0/all/0/1\">Euan McLean</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What do LLMs Know about Financial Markets? A Case Study on Reddit Market Sentiment Analysis. (arXiv:2212.11311v1 [cs.CL])","link":"http://arxiv.org/abs/2212.11311","description":"<p>Market sentiment analysis on social media content requires knowledge of both\nfinancial markets and social media jargon, which makes it a challenging task\nfor human raters. The resulting lack of high-quality labeled data stands in the\nway of conventional supervised learning methods. Instead, we approach this\nproblem using semi-supervised learning with a large language model (LLM). Our\npipeline generates weak financial sentiment labels for Reddit posts with an LLM\nand then uses that data to train a small model that can be served in\nproduction. We find that prompting the LLM to produce Chain-of-Thought\nsummaries and forcing it through several reasoning paths helps generate more\nstable and accurate labels, while using a regression loss further improves\ndistillation quality. With only a handful of prompts, the final model performs\non par with existing supervised models. Though production applications of our\nmodel are limited by ethical considerations, the model's competitive\nperformance points to the great potential of using LLMs for tasks that\notherwise require skill-intensive annotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bashlovkina_V/0/1/0/all/0/1\">Vasilisa Bashlovkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1\">Feng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumgartner_S/0/1/0/all/0/1\">Simon Baumgartner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bendersky_M/0/1/0/all/0/1\">Michael Bendersky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Distillation Is a Sample-Efficient Self-Supervised Loss Policy for Transfer Learning. (arXiv:2212.11353v1 [cs.CL])","link":"http://arxiv.org/abs/2212.11353","description":"<p>Traditional approaches to RL have focused on learning decision policies\ndirectly from episodic decisions, while slowly and implicitly learning the\nsemantics of compositional representations needed for generalization. While\nsome approaches have been adopted to refine representations via auxiliary\nself-supervised losses while simultaneously learning decision policies,\nlearning compositional representations from hand-designed and\ncontext-independent self-supervised losses (multi-view) still adapts relatively\nslowly to the real world, which contains many non-IID subspaces requiring rapid\ndistribution shift in both time and spatial attention patterns at varying\nlevels of abstraction. In contrast, supervised language model cascades have\nshown the flexibility to adapt to many diverse manifolds, and hints of\nself-learning needed for autonomous task transfer. However, to date, transfer\nmethods for language models like few-shot learning and fine-tuning still\nrequire human supervision and transfer learning using self-learning methods has\nbeen underexplored. We propose a self-supervised loss policy called contrastive\ndistillation which manifests latent variables with high mutual information with\nboth source and target tasks from weights to tokens. We show how this\noutperforms common methods of transfer learning and suggests a useful design\naxis of trading off compute for generalizability for online transfer.\nContrastive distillation is improved through sampling from memory and suggests\na simple algorithm for more efficiently sampling negative examples for\ncontrastive losses than random sampling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lengerich_C/0/1/0/all/0/1\">Chris Lengerich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Amy Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leather_H/0/1/0/all/0/1\">Hugh Leather</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuster_K/0/1/0/all/0/1\">Kurt Shuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charton_F/0/1/0/all/0/1\">Fran&#xe7;ois Charton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Redwood_C/0/1/0/all/0/1\">Charysse Redwood</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Emotion Modelling in Written Stories. (arXiv:2212.11382v1 [cs.CL])","link":"http://arxiv.org/abs/2212.11382","description":"<p>Telling stories is an integral part of human communication which can evoke\nemotions and influence the affective states of the audience. Automatically\nmodelling emotional trajectories in stories has thus attracted considerable\nscholarly interest. However, as most existing works have been limited to\nunsupervised dictionary-based approaches, there is no labelled benchmark for\nthis task. We address this gap by introducing continuous valence and arousal\nannotations for an existing dataset of children's stories annotated with\ndiscrete emotion categories. We collect additional annotations for this data\nand map the originally categorical labels to the valence and arousal space.\nLeveraging recent advances in Natural Language Processing, we propose a set of\nnovel Transformer-based methods for predicting valence and arousal signals over\nthe course of written stories. We explore several strategies for fine-tuning a\npretrained ELECTRA model and study the benefits of considering a sentence's\ncontext when inferring its emotionality. Moreover, we experiment with\nadditional LSTM and Transformer layers. The best configuration achieves a\nConcordance Correlation Coefficient (CCC) of .7338 for valence and .6302 for\narousal on the test set, demonstrating the suitability of our proposed\napproach. Our code and additional annotations are made available at\nhttps://github.com/lc0197/emotion_modelling_stories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Christ_L/0/1/0/all/0/1\">Lukas Christ</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amiriparian_S/0/1/0/all/0/1\">Shahin Amiriparian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milling_M/0/1/0/all/0/1\">Manuel Milling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aslan_I/0/1/0/all/0/1\">Ilhan Aslan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Postpartum Parents' Experiences via Two Digital Platforms. (arXiv:2212.11455v1 [cs.CL])","link":"http://arxiv.org/abs/2212.11455","description":"<p>Digital platforms, including online forums and helplines, have emerged as\navenues of support for caregivers suffering from postpartum mental health\ndistress. Understanding support seekers' experiences as shared on these\nplatforms could provide crucial insight into caregivers' needs during this\nvulnerable time. In the current work, we provide a descriptive analysis of the\nconcerns, psychological states, and motivations shared by healthy and\ndistressed postpartum support seekers on two digital platforms, a one-on-one\ndigital helpline and a publicly available online forum. Using a combination of\nhuman annotations, dictionary models and unsupervised techniques, we find stark\ndifferences between the experiences of distressed and healthy mothers.\nDistressed mothers described interpersonal problems and a lack of support, with\n8.60% - 14.56% reporting severe symptoms including suicidal ideation. In\ncontrast, the majority of healthy mothers described childcare issues, such as\nquestions about breastfeeding or sleeping, and reported no severe mental health\nconcerns. Across the two digital platforms, we found that distressed mothers\nshared similar content. However, the patterns of speech and affect shared by\ndistressed mothers differed between the helpline vs. the online forum,\nsuggesting the design of these platforms may shape meaningful measures of their\nsupport-seeking experiences. Our results provide new insight into the\nexperiences of caregivers suffering from postpartum mental health distress. We\nconclude by discussing methodological considerations for understanding content\nshared by support seekers and design considerations for the next generation of\nsupport tools for postpartum parents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xuewen Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikhelson_M/0/1/0/all/0/1\">Miriam Mikhelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Micheletti_M/0/1/0/all/0/1\">Megan Micheletti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watkins_S/0/1/0/all/0/1\">S Craig Watkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomaz_E/0/1/0/all/0/1\">Edison Thomaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbaro_K/0/1/0/all/0/1\">Kaya De Barbaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAMeMBERT: Cascading Assistant-Mediated Multilingual BERT. (arXiv:2212.11456v1 [cs.CL])","link":"http://arxiv.org/abs/2212.11456","description":"<p>Large language models having hundreds of millions, and even billions, of\nparameters have performed extremely well on a variety of natural language\nprocessing (NLP) tasks. Their widespread use and adoption, however, is hindered\nby the lack of availability and portability of sufficiently large computational\nresources. This paper proposes a knowledge distillation (KD) technique building\non the work of LightMBERT, a student model of multilingual BERT (mBERT). By\nrepeatedly distilling mBERT through increasingly compressed toplayer distilled\nteacher assistant networks, CAMeMBERT aims to improve upon the time and space\ncomplexities of mBERT while keeping loss of accuracy beneath an acceptable\nthreshold. At present, CAMeMBERT has an average accuracy of around 60.1%, which\nis subject to change after future improvements to the hyperparameters used in\nfine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DeGenaro_D/0/1/0/all/0/1\">Dan DeGenaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1\">Jugal Kalita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation for Change. (arXiv:2212.11670v1 [cs.CL])","link":"http://arxiv.org/abs/2212.11670","description":"<p>Evaluation is the central means for assessing, understanding, and\ncommunicating about NLP models. In this position paper, we argue evaluation\nshould be more than that: it is a force for driving change, carrying a\nsociological and political character beyond its technical dimensions. As a\nforce, evaluation's power arises from its adoption: under our view, evaluation\nsucceeds when it achieves the desired change in the field. Further, by framing\nevaluation as a force, we consider how it competes with other forces. Under our\nanalysis, we conjecture that the current trajectory of NLP suggests\nevaluation's power is waning, in spite of its potential for realizing more\npluralistic ambitions in the field. We conclude by discussing the legitimacy of\nthis power, who acquires this power and how it distributes. Ultimately, we hope\nthe research community will more aggressively harness evaluation for change.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bommasani_R/0/1/0/all/0/1\">Rishi Bommasani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trustworthy Social Bias Measurement. (arXiv:2212.11672v1 [cs.CL])","link":"http://arxiv.org/abs/2212.11672","description":"<p>How do we design measures of social bias that we trust? While prior work has\nintroduced several measures, no measure has gained widespread trust: instead,\nmounting evidence argues we should distrust these measures. In this work, we\ndesign bias measures that warrant trust based on the cross-disciplinary theory\nof measurement modeling. To combat the frequently fuzzy treatment of social\nbias in NLP, we explicitly define social bias, grounded in principles drawn\nfrom social science research. We operationalize our definition by proposing a\ngeneral bias measurement framework DivDist, which we use to instantiate 5\nconcrete bias measures. To validate our measures, we propose a rigorous testing\nprotocol with 8 testing criteria (e.g. predictive validity: do measures predict\nbiases in US employment?). Through our testing, we demonstrate considerable\nevidence to trust our measures, showing they overcome conceptual, technical,\nand empirical deficiencies present in prior measures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bommasani_R/0/1/0/all/0/1\">Rishi Bommasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smooth Sailing: Improving Active Learning for Pre-trained Language Models with Representation Smoothness Analysis. (arXiv:2212.11680v1 [cs.LG])","link":"http://arxiv.org/abs/2212.11680","description":"<p>Developed as a solution to a practical need, active learning (AL) methods aim\nto reduce label complexity and the annotations costs in supervised learning.\nWhile recent work has demonstrated the benefit of using AL in combination with\nlarge pre-trained language models (PLMs), it has often overlooked the practical\nchallenges that hinder the feasibility of AL in realistic settings. We address\nthese challenges by leveraging representation smoothness analysis to improve\nthe effectiveness of AL. We develop an early stopping technique that does not\nrequire a validation set -- often unavailable in realistic AL settings -- and\nobserve significant improvements across multiple datasets and AL methods.\nAdditionally, we find that task adaptation improves AL, whereas standard short\nfine-tuning in AL does not provide improvements over random sampling. Our work\nestablishes the usefulness of representation smoothness analysis in AL and\npresents an AL stopping criterion that reduces label complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jukic_J/0/1/0/all/0/1\">Josip Juki&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snajder_J/0/1/0/all/0/1\">Jan &#x160;najder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GENIE: Large Scale Pre-training for Text Generation with Diffusion Model. (arXiv:2212.11685v1 [cs.CL])","link":"http://arxiv.org/abs/2212.11685","description":"<p>In this paper, we propose a large-scale language pre-training for text\nGENeration using dIffusion modEl, which is named GENIE. GENIE is a pre-training\nsequence-to-sequence text generation model which combines Transformer and\ndiffusion. The diffusion model accepts the latent information from the encoder,\nwhich is used to guide the denoising of the current time step. After multiple\nsuch denoise iterations, the diffusion model can restore the Gaussian noise to\nthe diverse output text which is controlled by the input text. Moreover, such\narchitecture design also allows us to adopt large scale pre-training on the\nGENIE. We propose a novel pre-training method named continuous paragraph\ndenoise based on the characteristics of the diffusion model. Extensive\nexperiments on the XSum, CNN/DailyMail, and Gigaword benchmarks shows that\nGENIE can achieves comparable performance with various strong baselines,\nespecially after pre-training, the generation quality of GENIE is greatly\nimproved. We have also conduct a lot of experiments on the generation diversity\nand parameter impact of GENIE. The code for GENIE will be made publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhenghao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhihao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Self-Adjusting Fusion Representation Learning Model for Unaligned Text-Audio Sequences. (arXiv:2212.11772v1 [cs.CL])","link":"http://arxiv.org/abs/2212.11772","description":"<p>Inter-modal interaction plays an indispensable role in multimodal sentiment\nanalysis. Due to different modalities sequences are usually non-alignment, how\nto integrate relevant information of each modality to learn fusion\nrepresentations has been one of the central challenges in multimodal learning.\nIn this paper, a Self-Adjusting Fusion Representation Learning Model (SA-FRLM)\nis proposed to learn robust crossmodal fusion representations directly from the\nunaligned text and audio sequences. Different from previous works, our model\nnot only makes full use of the interaction between different modalities but\nalso maximizes the protection of the unimodal characteristics. Specifically, we\nfirst employ a crossmodal alignment module to project different modalities\nfeatures to the same dimension. The crossmodal collaboration attention is then\nadopted to model the inter-modal interaction between text and audio sequences\nand initialize the fusion representations. After that, as the core unit of the\nSA-FRLM, the crossmodal adjustment transformer is proposed to protect original\nunimodal characteristics. It can dynamically adapt the fusion representations\nby using single modal streams. We evaluate our approach on the public\nmultimodal sentiment analysis datasets CMU-MOSI and CMU-MOSEI. The experiment\nresults show that our model has significantly improved the performance of all\nthe metrics on the unaligned text-audio sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kaicheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1\">Kai Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Integer-Only Deep Recurrent Neural Networks. (arXiv:2212.11791v1 [cs.LG])","link":"http://arxiv.org/abs/2212.11791","description":"<p>Recurrent neural networks (RNN) are the backbone of many text and speech\napplications. These architectures are typically made up of several\ncomputationally complex components such as; non-linear activation functions,\nnormalization, bi-directional dependence and attention. In order to maintain\ngood accuracy, these components are frequently run using full-precision\nfloating-point computation, making them slow, inefficient and difficult to\ndeploy on edge devices. In addition, the complex nature of these operations\nmakes them challenging to quantize using standard quantization methods without\na significant performance drop. We present a quantization-aware training method\nfor obtaining a highly accurate integer-only recurrent neural network (iRNN).\nOur approach supports layer normalization, attention, and an adaptive piecewise\nlinear (PWL) approximation of activation functions, to serve a wide range of\nstate-of-the-art RNNs. The proposed method enables RNN-based language models to\nrun on edge devices with $2\\times$ improvement in runtime, and $4\\times$\nreduction in model size while maintaining similar accuracy as its\nfull-precision counterpart.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nia_V/0/1/0/all/0/1\">Vahid Partovi Nia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sari_E/0/1/0/all/0/1\">Eyy&#xfc;b Sari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_V/0/1/0/all/0/1\">Vanessa Courville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asgharian_M/0/1/0/all/0/1\">Masoud Asgharian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Mutation-based Text Generation for Adversarial Machine Learning Applications. (arXiv:2212.11808v1 [cs.CL])","link":"http://arxiv.org/abs/2212.11808","description":"<p>Many natural language related applications involve text generation, created\nby humans or machines. While in many of those applications machines support\nhumans, yet in few others, (e.g. adversarial machine learning, social bots and\ntrolls) machines try to impersonate humans. In this scope, we proposed and\nevaluated several mutation-based text generation approaches. Unlike\nmachine-based generated text, mutation-based generated text needs human text\nsamples as inputs. We showed examples of mutation operators but this work can\nbe extended in many aspects such as proposing new text-based mutation operators\nbased on the nature of the application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_J/0/1/0/all/0/1\">Jesus Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1\">Gongbo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alsmadi_I/0/1/0/all/0/1\">Izzat Alsmadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual News Location Detection using an Entity-Based Siamese Network with Semi-Supervised Contrastive Learning and Knowledge Base. (arXiv:2212.11856v1 [cs.CL])","link":"http://arxiv.org/abs/2212.11856","description":"<p>Early detection of relevant locations in a piece of news is especially\nimportant in extreme events such as environmental disasters, war conflicts,\ndisease outbreaks, or political turmoils. Additionally, this detection also\nhelps recommender systems to promote relevant news based on user locations.\nNote that, when the relevant locations are not mentioned explicitly in the\ntext, state-of-the-art methods typically fail to recognize them because these\nmethods rely on syntactic recognition. In contrast, by incorporating a\nknowledge base and connecting entities with their locations, our system\nsuccessfully infers the relevant locations even when they are not mentioned\nexplicitly in the text. To evaluate the effectiveness of our approach, and due\nto the lack of datasets in this area, we also contribute to the research\ncommunity with a gold-standard multilingual news-location dataset, NewsLOC. It\ncontains the annotation of the relevant locations (and their WikiData IDs) of\n600+ Wikinews articles in five different languages: English, French, German,\nItalian, and Spanish. Through experimental evaluations, we show that our\nproposed system outperforms the baselines and the fine-tuned version of the\nmodel using semi-supervised data that increases the classification rate. The\nsource code and the NewsLOC dataset are publicly available for being used by\nthe research community at https://github.com/vsuarezpaniagua/NewsLocation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suarez_Paniagua_V/0/1/0/all/0/1\">V&#xed;ctor Su&#xe1;rez-Paniagua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derby_S/0/1/0/all/0/1\">Steven Derby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijaya_T/0/1/0/all/0/1\">Tri Kurniawan Wijaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Induction of Language Models Via Probabilistic Concept Formation. (arXiv:2212.11937v1 [cs.CL])","link":"http://arxiv.org/abs/2212.11937","description":"<p>This paper presents a novel approach to the acquisition of language models\nfrom corpora. The framework builds on Cobweb, an early system for constructing\ntaxonomic hierarchies of probabilistic concepts that used a tabular,\nattribute-value encoding of training cases and concepts, making it unsuitable\nfor sequential input like language. In response, we explore three new\nextensions to Cobweb -- the Word, Leaf, and Path variants. These systems encode\neach training case as an anchor word and surrounding context words, and they\nstore probabilistic descriptions of concepts as distributions over anchor and\ncontext information. As in the original Cobweb, a performance element sorts a\nnew instance downward through the hierarchy and uses the final node to predict\nmissing features. Learning is interleaved with performance, updating concept\nprobabilities and hierarchy structure as classification occurs. Thus, the new\napproaches process training cases in an incremental, online manner that it very\ndifferent from most methods for statistical language learning. We examine how\nwell the three variants place synonyms together and keep homonyms apart, their\nability to recall synonyms as a function of training set size, and their\ntraining efficiency. Finally, we discuss related work on incremental learning\nand directions for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+MacLellan_C/0/1/0/all/0/1\">Christopher J. MacLellan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsakis_P/0/1/0/all/0/1\">Peter Matsakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langley_P/0/1/0/all/0/1\">Pat Langley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NarrativeTime: Dense Temporal Annotation on a Timeline. (arXiv:1908.11443v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1908.11443","description":"<p>For the past decade, temporal annotation has been sparse: only a small\nportion of event pairs in a text was annotated. We present NarrativeTime, the\nfirst timeline-based annotation framework that achieves full coverage of all\npossible TLinks. To compare with the previous SOTA in dense temporal\nannotation, we perform full re-annotation of TimeBankDense corpus, which shows\ncomparable agreement with a significant increase in density. We contribute\nTimeBankNT corpus (with each text fully annotated by two expert annotators),\nextensive annotation guidelines, open-source tools for annotation and\nconversion to TimeML format, baseline results, as well as quantitative and\nqualitative analysis of inter-annotator agreement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rogers_A/0/1/0/all/0/1\">Anna Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpinska_M/0/1/0/all/0/1\">Marzena Karpinska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ankita Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lialin_V/0/1/0/all/0/1\">Vladislav Lialin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smelkov_G/0/1/0/all/0/1\">Gregory Smelkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1\">Anna Rumshisky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keyphrase Generation with Cross-Document Attention. (arXiv:2004.09800v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.09800","description":"<p>Keyphrase generation aims to produce a set of phrases summarizing the\nessentials of a given document. Conventional methods normally apply an\nencoder-decoder architecture to generate the output keyphrases for an input\ndocument, where they are designed to focus on each current document so they\ninevitably omit crucial corpus-level information carried by other similar\ndocuments, i.e., the cross-document dependency and latent topics. In this\npaper, we propose CDKGen, a Transformer-based keyphrase generator, which\nexpands the Transformer to global attention with cross-document attention\nnetworks to incorporate available documents as references so as to generate\nbetter keyphrases with the guidance of topic information. On top of the\nproposed Transformer + cross-document attention architecture, we also adopt a\ncopy mechanism to enhance our model via selecting appropriate words from\ndocuments to deal with out-of-vocabulary words in keyphrases. Experiment\nresults on five benchmark datasets illustrate the validity and effectiveness of\nour model, which achieves the state-of-the-art performance on all datasets.\nFurther analyses confirm that the proposed model is able to generate keyphrases\nconsistent with references while keeping sufficient diversity. The code of\nCDKGen is available at https://github.com/SVAIGBA/CDKGen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diao_S/0/1/0/all/0/1\">Shizhe Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InvBERT: Reconstructing Text from Contextualized Word Embeddings by inverting the BERT pipeline. (arXiv:2109.10104v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10104","description":"<p>Digital Humanities and Computational Literary Studies apply text mining\nmethods to investigate literature. Such automated approaches enable\nquantitative studies on large corpora which would not be feasible by manual\ninspection alone. However, due to copyright restrictions, the availability of\nrelevant digitized literary works is limited. Derived Text Formats (DTFs) have\nbeen proposed as a solution. Here, textual materials are transformed in such a\nway that copyright-critical features are removed, but that the use of certain\nanalytical methods remains possible. Contextualized word embeddings produced by\ntransformer-encoders (like BERT) are promising candidates for DTFs because they\nallow for state-of-the-art performance on various analytical tasks and, at\nfirst sight, do not disclose the original text. However, in this paper we\ndemonstrate that under certain conditions the reconstruction of the original\ncopyrighted text becomes feasible and its publication in the form of\ncontextualized token representations is not safe. Our attempts to invert BERT\nsuggest, that publishing the encoder as a black box together with the\ncontextualized embeddings is critical, since it allows to generate data to\ntrain a decoder with a reconstruction accuracy sufficient to violate copyright\nlaws.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kugler_K/0/1/0/all/0/1\">Kai Kugler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munker_S/0/1/0/all/0/1\">Simon M&#xfc;nker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hohmann_J/0/1/0/all/0/1\">Johannes H&#xf6;hmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rettinger_A/0/1/0/all/0/1\">Achim Rettinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional generalization in semantic parsing with pretrained transformers. (arXiv:2109.15101v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.15101","description":"<p>Large-scale pretraining instills large amounts of knowledge in deep neural\nnetworks. This, in turn, improves the generalization behavior of these models\nin downstream tasks. What exactly are the limits to the generalization benefits\nof large-scale pretraining? Here, we report observations from some simple\nexperiments aimed at addressing this question in the context of two semantic\nparsing tasks involving natural language, SCAN and COGS. We show that language\nmodels pretrained exclusively with non-English corpora, or even with\nprogramming language corpora, significantly improve out-of-distribution\ngeneralization in these benchmarks, compared with models trained from scratch,\neven though both benchmarks are English-based. This demonstrates the\nsurprisingly broad transferability of pretrained representations and knowledge.\nPretraining with a large-scale protein sequence prediction task, on the other\nhand, mostly deteriorates the generalization performance in SCAN and COGS,\nsuggesting that pretrained representations do not transfer universally and that\nthere are constraints on the similarity between the pretraining and downstream\ndomains for successful transfer. Finally, we show that larger models are harder\nto train from scratch and their generalization accuracy is lower when trained\nup to convergence on the relatively small SCAN and COGS datasets, but the\nbenefits of large-scale pretraining become much clearer with larger models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orhan_A/0/1/0/all/0/1\">A. Emin Orhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Neural Diarization of Unlimited Numbers of Speakers Using Global and Local Attractors. (arXiv:2206.02432v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2206.02432","description":"<p>A method to perform offline and online speaker diarization for an unlimited\nnumber of speakers is described in this paper. End-to-end neural diarization\n(EEND) has achieved overlap-aware speaker diarization by formulating it as a\nmulti-label classification problem. It has also been extended for a flexible\nnumber of speakers by introducing speaker-wise attractors. However, the output\nnumber of speakers of attractor-based EEND is empirically capped; it cannot\ndeal with cases where the number of speakers appearing during inference is\nhigher than that during training because its speaker counting is trained in a\nfully supervised manner. Our method, EEND-GLA, solves this problem by\nintroducing unsupervised clustering into attractor-based EEND. In the method,\nthe input audio is first divided into short blocks, then attractor-based\ndiarization is performed for each block, and finally, the results of each block\nare clustered on the basis of the similarity between locally-calculated\nattractors. While the number of output speakers is limited within each block,\nthe total number of speakers estimated for the entire input can be higher than\nthe limitation. To use EEND-GLA in an online manner, our method also extends\nthe speaker-tracing buffer, which was originally proposed to enable online\ninference of conventional EEND. We introduce a block-wise buffer update to make\nthe speaker-tracing buffer compatible with EEND-GLA. Finally, to improve online\ndiarization, our method improves the buffer update method and revisits the\nvariable chunk-size training of EEND. The experimental results demonstrate that\nEEND-GLA can perform speaker diarization of an unseen number of speakers in\nboth offline and online inferences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Horiguchi_S/0/1/0/all/0/1\">Shota Horiguchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garcia_P/0/1/0/all/0/1\">Paola Garcia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Takashima_Y/0/1/0/all/0/1\">Yuki Takashima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kawaguchi_Y/0/1/0/all/0/1\">Yohei Kawaguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Synthesis with Mixed Emotions. (arXiv:2208.05890v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.05890","description":"<p>Emotional speech synthesis aims to synthesize human voices with various\nemotional effects. The current studies are mostly focused on imitating an\naveraged style belonging to a specific emotion type. In this paper, we seek to\ngenerate speech with a mixture of emotions at run-time. We propose a novel\nformulation that measures the relative difference between the speech samples of\ndifferent emotions. We then incorporate our formulation into a\nsequence-to-sequence emotional text-to-speech framework. During the training,\nthe framework does not only explicitly characterize emotion styles, but also\nexplores the ordinal nature of emotions by quantifying the differences with\nother emotions. At run-time, we control the model to produce the desired\nemotion mixture by manually defining an emotion attribute vector. The objective\nand subjective evaluations have validated the effectiveness of the proposed\nframework. To our best knowledge, this research is the first study on\nmodelling, synthesizing, and evaluating mixed emotions in speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sisman_B/0/1/0/all/0/1\">Berrak Sisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_R/0/1/0/all/0/1\">Rajib Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">B.W.Schuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConvNeXt Based Neural Network for Audio Anti-Spoofing. (arXiv:2209.06434v5 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2209.06434","description":"<p>With the rapid development of speech conversion and speech synthesis\nalgorithms, automatic speaker verification (ASV) systems are vulnerable to\nspoofing attacks. In recent years, researchers had proposed a number of\nanti-spoofing methods based on hand-crafted features. However, using\nhand-crafted features rather than raw waveform will lose implicit information\nfor anti-spoofing. Inspired by the promising performance of ConvNeXt in image\nclassification tasks, we revise the ConvNeXt network architecture and propose a\nlightweight end-to-end anti-spoofing model. By integrating with the channel\nattention block and using the focal loss function, the proposed model can focus\non the most informative sub-bands of speech representations and the difficult\nsamples that are hard to classify. Experiments show that our proposed system\ncould achieve an equal error rate of 0.64% and min-tDCF of 0.0187 for the\nASVSpoof 2019 LA evaluation dataset, which outperforms the state-of-the-art\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qiaowei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1\">Jinghui Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yitao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Ying Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_W/0/1/0/all/0/1\">Wing W.Y. Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detect-Localize-Repair: A Unified Framework for Learning to Debug with CodeT5. (arXiv:2211.14875v3 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2211.14875","description":"<p>Automated software debugging is a crucial task for improving the productivity\nof software developers. Many neural-based techniques have been proven effective\nfor debugging-related tasks such as bug localization and program repair (or bug\nfixing). However, these techniques often focus only on either one of them or\napproach them in a stage-wise manner, ignoring the mutual benefits between\nthem. In this work, we propose a novel unified \\emph{Detect-Localize-Repair}\nframework based on a pretrained programming language model CodeT5 to seamlessly\naddress these tasks, named CodeT5-DLR. Specifically, we propose three\nobjectives to adapt the generic CodeT5 for debugging: a bug detection objective\nto determine whether a given code snippet is buggy or not, a bug localization\nobjective to identify the buggy lines, and a program repair objective to\ntranslate the buggy code to its fixed version. We evaluate it on each of these\ntasks and their combined setting on two newly collected line-level debugging\ndatasets in Java and Python. Extensive results show that our model\nsignificantly outperforms existing baselines from both NLP and software\nengineering domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bui_N/0/1/0/all/0/1\">Nghi D. Q. Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better Transcription of UK Supreme Court Hearings. (arXiv:2211.17094v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2211.17094","description":"<p>Transcription of legal proceedings is very important to enable access to\njustice. However, speech transcription is an expensive and slow process. In\nthis paper we describe part of a combined research and industrial project for\nbuilding an automated transcription tool designed specifically for the Justice\nsector in the UK. We explain the challenges involved in transcribing court room\nhearings and the Natural Language Processing (NLP) techniques we employ to\ntackle these challenges. We will show that fine-tuning a generic off-the-shelf\npre-trained Automatic Speech Recognition (ASR) system with an in-domain\nlanguage model as well as infusing common phrases extracted with a collocation\ndetection model can improve not only the Word Error Rate (WER) of the\ntranscribed hearings but avoid critical errors that are specific of the legal\njargon and terminology commonly used in British courts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Saadany_H/0/1/0/all/0/1\">Hadeel Saadany</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breslin_C/0/1/0/all/0/1\">Catherine Breslin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Orasan_C/0/1/0/all/0/1\">Constantin Or&#x103;san</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Walker_S/0/1/0/all/0/1\">Sophie Walker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards mapping the contemporary art world with ArtLM: an art-specific NLP model. (arXiv:2212.07127v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.07127","description":"<p>With an increasing amount of data in the art world, discovering artists and\nartworks suitable to collectors' tastes becomes a challenge. It is no longer\nenough to use visual information, as contextual information about the artist\nhas become just as important in contemporary art. In this work, we present a\ngeneric Natural Language Processing framework (called ArtLM) to discover the\nconnections among contemporary artists based on their biographies. In this\napproach, we first continue to pre-train the existing general English language\nmodels with a large amount of unlabelled art-related data. We then fine-tune\nthis new pre-trained model with our biography pair dataset manually annotated\nby a team of professionals in the art industry. With extensive experiments, we\ndemonstrate that our ArtLM achieves 85.6% accuracy and 84.0% F1 score and\noutperforms other baseline models. We also provide a visualisation and a\nqualitative analysis of the artist network built from ArtLM's outputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qinkai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Mennaoui_M/0/1/0/all/0/1\">Mohamed El-Mennaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fosset_A/0/1/0/all/0/1\">Antoine Fosset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rebei_A/0/1/0/all/0/1\">Amine Rebei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Haoyang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouscasse_P/0/1/0/all/0/1\">Philine Bouscasse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OBeirne_C/0/1/0/all/0/1\">Christy E&#xf3;in O&#x27;Beirne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shevchenko_S/0/1/0/all/0/1\">Sasha Shevchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenbaum_M/0/1/0/all/0/1\">Mathieu Rosenbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implementation of general formal translators. (arXiv:2212.08482v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08482","description":"<p>The general translator formalism and computing specific implementations are\nproposed. The implementation of specific elements necessary to process the\nsource and destination information within the translators are presented. Some\ncommon directives or instructions, such as classes and procedures, were unified\nand generalized in order to allow general translations implementations. In\norder to cover general cases, two levels of processing are required, related to\nthe source and destination information appropriate transformations, with the\nrelated control and processing instructions. The proposed general translator\nelements are useful for processing natural or artificial information described\nthrough any types of languages or systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petrila_I/0/1/0/all/0/1\">Iosif Iulian Petrila</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Drug-Drug Interactions Prediction Technology for Molecularly Intelligent Manufacturing. (arXiv:2212.09400v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09400","description":"<p>Drug-Drug Interactions (DDIs) prediction is an essential issue in the\nmolecular field. Traditional methods of observing DDIs in medical experiments\nrequire plenty of resources and labor. In this paper, we present a\ncomputational model dubbed MedKGQA based on Graph Neural Networks to\nautomatically predict the DDIs after reading multiple medical documents in the\nform of multi-hop machine reading comprehension. We introduced a knowledge\nfusion system to obtain the complete nature of drugs and proteins and exploited\na graph reasoning system to infer the drugs and proteins contained in the\ndocuments. Our model significantly improves the performance compared to\nprevious state-of-the-art models on the QANGAROO MedHop dataset, which obtained\na 4.5% improvement in terms of DDIs prediction accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Feng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jian-Cheng Ni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Augmentation Strategy for Visually Rich Documents. (arXiv:2212.10047v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10047","description":"<p>Many business workflows require extracting important fields from form-like\ndocuments (e.g. bank statements, bills of lading, purchase orders, etc.).\nRecent techniques for automating this task work well only when trained with\nlarge datasets. In this work we propose a novel data augmentation technique to\nimprove performance when training data is scarce, e.g. 10-250 documents. Our\ntechnique, which we call FieldSwap, works by swapping out the key phrases of a\nsource field with the key phrases of a target field to generate new synthetic\nexamples of the target field for use in training. We demonstrate that this\napproach can yield 1-7 F1 point improvements in extraction performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wendt_J/0/1/0/all/0/1\">James B. Wendt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebner_S/0/1/0/all/0/1\">Seth Ebner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tata_S/0/1/0/all/0/1\">Sandeep Tata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localising In-Domain Adaptation of Transformer-Based Biomedical Language Models. (arXiv:2212.10422v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10422","description":"<p>In the era of digital healthcare, the huge volumes of textual information\ngenerated every day in hospitals constitute an essential but underused asset\nthat could be exploited with task-specific, fine-tuned biomedical language\nrepresentation models, improving patient care and management. For such\nspecialized domains, previous research has shown that fine-tuning models\nstemming from broad-coverage checkpoints can largely benefit additional\ntraining rounds over large-scale in-domain resources. However, these resources\nare often unreachable for less-resourced languages like Italian, preventing\nlocal medical institutions to employ in-domain adaptation. In order to reduce\nthis gap, our work investigates two accessible approaches to derive biomedical\nlanguage models in languages other than English, taking Italian as a concrete\nuse-case: one based on neural machine translation of English resources,\nfavoring quantity over quality; the other based on a high-grade, narrow-scoped\ncorpus natively written in Italian, thus preferring quality over quantity. Our\nstudy shows that data quantity is a harder constraint than data quality for\nbiomedical adaptation, but the concatenation of high-quality data can improve\nmodel performance even when dealing with relatively size-limited corpora. The\nmodels published from our investigations have the potential to unlock important\nresearch opportunities for Italian hospitals and academia. Finally, the set of\nlessons learned from the study constitutes valuable insights towards a solution\nto build biomedical language models that are generalizable to other\nless-resourced languages and different domain settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Buonocore_T/0/1/0/all/0/1\">Tommaso Mario Buonocore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crema_C/0/1/0/all/0/1\">Claudio Crema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Redolfi_A/0/1/0/all/0/1\">Alberto Redolfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellazzi_R/0/1/0/all/0/1\">Riccardo Bellazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parimbelli_E/0/1/0/all/0/1\">Enea Parimbelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimpleStyle: An Adaptable Style Transfer Approach. (arXiv:2212.10498v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10498","description":"<p>Attribute-controlled text rewriting, also known as text style-transfer, has a\ncrucial role in regulating attributes and biases of textual training data and a\nmachine generated text. In this work we present SimpleStyle, a minimalist yet\neffective approach for style-transfer composed of two simple ingredients:\ncontrolled denoising and output filtering. Despite the simplicity of our\napproach, which can be succinctly described with a few lines of code, it is\ncompetitive with previous state-of-the-art methods both in automatic and in\nhuman evaluation. To demonstrate the adaptability and practical value of our\nsystem beyond academic data, we apply SimpleStyle to transfer a wide range of\ntext attributes appearing in real-world textual data from social networks.\nAdditionally, we introduce a novel \"soft noising\" technique that further\nimproves the performance of our system. We also show that teaching a student\nmodel to generate the output of SimpleStyle can result in a system that\nperforms style transfer of equivalent quality with only a single greedy-decoded\nsample. Finally, we suggest our method as a remedy for the fundamental\nincompatible baseline issue that holds progress in the field. We offer our\nprotocol as a simple yet strong baseline for works that wish to make\nincremental advancements in the field of attribute controlled text rewriting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bandel_E/0/1/0/all/0/1\">Elron Bandel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1\">Yoav Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ein_Dor_L/0/1/0/all/0/1\">Liat Ein-Dor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chatbots in a Botnet World. (arXiv:2212.11126v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2212.11126","description":"<p>Question-and-answer formats provide a novel experimental platform for\ninvestigating cybersecurity questions. Unlike previous chatbots, the latest\nChatGPT model from OpenAI supports an advanced understanding of complex coding\nquestions. The research demonstrates thirteen coding tasks that generally\nqualify as stages in the MITRE ATT&amp;CK framework, ranging from credential access\nto defense evasion. With varying success, the experimental prompts generate\nexamples of keyloggers, logic bombs, obfuscated worms, and payment-fulfilled\nransomware. The empirical results illustrate cases that support the broad gain\nof functionality, including self-replication and self-modification, evasion,\nand strategic understanding of complex cybersecurity goals. One surprising\nfeature of ChatGPT as a language-only model centers on its ability to spawn\ncoding approaches that yield images that obfuscate or embed executable\nprogramming steps or links.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McKee_F/0/1/0/all/0/1\">Forrest McKee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1\">David Noever</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-12-22T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}