{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-05-29T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Talking with Machines: A Comprehensive Survey of Emergent Dialogue Systems. (arXiv:2305.16324v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16324","description":"<p>From the earliest experiments in the 20th century to the utilization of large\nlanguage models and transformers, dialogue systems research has continued to\nevolve, playing crucial roles in numerous fields. This paper offers a\ncomprehensive review of these systems, tracing their historical development and\nexamining their fundamental operations. We analyze popular and emerging\ndatasets for training and survey key contributions in dialogue systems\nresearch, including traditional systems and advanced machine learning methods.\nFinally, we consider conventional and transformer-based evaluation metrics,\nfollowed by a short discussion of prevailing challenges and future prospects in\nthe field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tholke_W/0/1/0/all/0/1\">William Tholke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. (arXiv:2305.16326v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16326","description":"<p>Biomedical literature is growing rapidly, making it challenging to curate and\nextract knowledge manually. Biomedical natural language processing (BioNLP)\ntechniques that can automatically extract information from biomedical\nliterature help alleviate this burden. Recently, large Language Models (LLMs),\nsuch as GPT-3 and GPT-4, have gained significant attention for their impressive\nperformance. However, their effectiveness in BioNLP tasks and impact on method\ndevelopment and downstream users remain understudied. This pilot study (1)\nestablishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and\none-shot settings in eight BioNLP datasets across four applications: named\nentity recognition, relation extraction, multi-label document classification,\nand semantic similarity and reasoning, (2) examines the errors produced by the\nLLMs and categorized the errors into three types: missingness, inconsistencies,\nand unwanted artificial content, and (3) provides suggestions for using LLMs in\nBioNLP applications. We make the datasets, baselines, and results publicly\navailable to the community via\nhttps://github.com/qingyu-qc/gpt_bionlp_benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jingcheng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keloth_V/0/1/0/all/0/1\">Vipina Kuttichi Keloth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xueqing Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raja_K/0/1/0/all/0/1\">Kalpana Raja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Composition in Visually Grounded Language Models. (arXiv:2305.16328v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16328","description":"<p>What is sentence meaning and its ideal representation? Much of the expressive\npower of human language derives from semantic composition, the mind's ability\nto represent meaning hierarchically &amp; relationally over constituents. At the\nsame time, much sentential meaning is outside the text and requires grounding\nin sensory, motor, and experiential modalities to be adequately learned.\nAlthough large language models display considerable compositional ability,\nrecent work shows that visually-grounded language models drastically fail to\nrepresent compositional structure. In this thesis, we explore whether &amp; how\nmodels compose visually grounded semantics, and how we might improve their\nability to do so.\n</p>\n<p>Specifically, we introduce 1) WinogroundVQA, a new compositional visual\nquestion answering benchmark, 2) Syntactic Neural Module Distillation, a\nmeasure of compositional ability in sentence embedding models, 3) Causal\nTracing for Image Captioning Models to locate neural representations vital for\nvision-language composition, 4) Syntactic MeanPool to inject a compositional\ninductive bias into sentence embeddings, and 5) Cross-modal Attention\nCongruence Regularization, a self-supervised objective function for\nvision-language relation alignment. We close by discussing connections of our\nwork to neuroscience, psycholinguistics, formal semantics, and philosophy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_R/0/1/0/all/0/1\">Rohan Pandey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Generation with Speech Synthesis for ASR Data Augmentation. (arXiv:2305.16333v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16333","description":"<p>Aiming at reducing the reliance on expensive human annotations, data\nsynthesis for Automatic Speech Recognition (ASR) has remained an active area of\nresearch. While prior work mainly focuses on synthetic speech generation for\nASR data augmentation, its combination with text generation methods is\nconsiderably less explored. In this work, we explore text augmentation for ASR\nusing large-scale pre-trained neural networks, and systematically compare those\nto traditional text augmentation methods. The generated synthetic texts are\nthen converted to synthetic speech using a text-to-speech (TTS) system and\nadded to the ASR training data. In experiments conducted on three datasets, we\nfind that neural models achieve 9%-15% relative WER improvement and outperform\ntraditional methods. We conclude that text augmentation, particularly through\nmodern neural approaches, is a viable tool for improving the accuracy of ASR\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhuangqun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keren_G/0/1/0/all/0/1\">Gil Keren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Ziran Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Shashank Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goss_Grubbs_D/0/1/0/all/0/1\">David Goss-Grubbs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Nelson Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abtahi_F/0/1/0/all/0/1\">Farnaz Abtahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Duc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAvirro_A/0/1/0/all/0/1\">Antony D&#x27;Avirro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_Taylor_E/0/1/0/all/0/1\">Ethan Campbell-Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salas_J/0/1/0/all/0/1\">Jessie Salas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veliche_I/0/1/0/all/0/1\">Irina-Elena Veliche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OlaGPT: Empowering LLMs With Human-like Problem-Solving Abilities. (arXiv:2305.16334v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16334","description":"<p>In most current research, large language models (LLMs) are able to perform\nreasoning tasks by generating chains of thought through the guidance of\nspecific prompts. However, there still exists a significant discrepancy between\ntheir capability in solving complex reasoning problems and that of humans. At\npresent, most approaches focus on chains of thought (COT) and tool use, without\nconsidering the adoption and application of human cognitive frameworks. It is\nwell-known that when confronting complex reasoning challenges, humans typically\nemploy various cognitive abilities, and necessitate interaction with all\naspects of tools, knowledge, and the external environment information to\naccomplish intricate tasks. This paper introduces a novel intelligent\nframework, referred to as OlaGPT. OlaGPT carefully studied a cognitive\narchitecture framework, and propose to simulate certain aspects of human\ncognition. The framework involves approximating different cognitive modules,\nincluding attention, memory, reasoning, learning, and corresponding scheduling\nand decision-making mechanisms. Inspired by the active learning mechanism of\nhuman beings, it proposes a learning unit to record previous mistakes and\nexpert opinions, and dynamically refer to them to strengthen their ability to\nsolve similar problems. The paper also outlines common effective reasoning\nframeworks for human problem-solving and designs Chain-of-Thought (COT)\ntemplates accordingly. A comprehensive decision-making mechanism is also\nproposed to maximize model accuracy. The efficacy of OlaGPT has been\nstringently evaluated on multiple reasoning datasets, and the experimental\noutcomes reveal that OlaGPT surpasses state-of-the-art benchmarks,\ndemonstrating its superior performance. Our implementation of OlaGPT is\navailable on GitHub: \\url{https://github.com/oladata-team/OlaGPT}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuanzhen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingxiong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">WenTao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_B/0/1/0/all/0/1\">Beibei Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_C/0/1/0/all/0/1\">Chengxiang Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Representation Learning with Reliable Pseudo-labels Generation via Self-Adaptive Optimal Transport for Short Text Clustering. (arXiv:2305.16335v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16335","description":"<p>Short text clustering is challenging since it takes imbalanced and noisy data\nas inputs. Existing approaches cannot solve this problem well, since (1) they\nare prone to obtain degenerate solutions especially on heavy imbalanced\ndatasets, and (2) they are vulnerable to noises. To tackle the above issues, we\npropose a Robust Short Text Clustering (RSTC) model to improve robustness\nagainst imbalanced and noisy data. RSTC includes two modules, i.e.,\npseudo-label generation module and robust representation learning module. The\nformer generates pseudo-labels to provide supervision for the later, which\ncontributes to more robust representations and correctly separated clusters. To\nprovide robustness against the imbalance in data, we propose self-adaptive\noptimal transport in the pseudo-label generation module. To improve robustness\nagainst the noise in data, we further introduce both class-wise and\ninstance-wise contrastive learning in the robust representation learning\nmodule. Our empirical studies on eight short text clustering datasets\ndemonstrate that RSTC significantly outperforms the state-of-the-art models.\nThe code is available at: https://github.com/hmllmh/RSTC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaolin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Mengling Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaochao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1\">Xinting Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Handling Realistic Label Noise in BERT Text Classification. (arXiv:2305.16337v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16337","description":"<p>Labels noise refers to errors in training labels caused by cheap data\nannotation methods, such as web scraping or crowd-sourcing, which can be\ndetrimental to the performance of supervised classifiers. Several methods have\nbeen proposed to counteract the effect of random label noise in supervised\nclassification, and some studies have shown that BERT is already robust against\nhigh rates of randomly injected label noise. However, real label noise is not\nrandom; rather, it is often correlated with input features or other\nannotator-specific factors. In this paper, we evaluate BERT in the presence of\ntwo types of realistic label noise: feature-dependent label noise, and\nsynthetic label noise from annotator disagreements. We show that the presence\nof these types of noise significantly degrades BERT classification performance.\nTo improve robustness, we evaluate different types of ensembles and\nnoise-cleaning methods and compare their effectiveness against label noise\nacross different datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agro_M/0/1/0/all/0/1\">Maha Tufail Agro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldarmaki_H/0/1/0/all/0/1\">Hanan Aldarmaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Think Before You Act: Decision Transformers with Internal Working Memory. (arXiv:2305.16338v1 [cs.LG])","link":"http://arxiv.org/abs/2305.16338","description":"<p>Large language model (LLM)-based decision-making agents have shown the\nability to generalize across multiple tasks. However, their performance relies\non massive data and compute. We argue that this inefficiency stems from the\nforgetting phenomenon, in which a model memorizes its behaviors in parameters\nthroughout training. As a result, training on a new task may deteriorate the\nmodel's performance on previous tasks. In contrast to LLMs' implicit memory\nmechanism, the human brain utilizes distributed memory storage, which helps\nmanage and organize multiple skills efficiently, mitigating the forgetting\nphenomenon. Thus inspired, we propose an internal working memory module to\nstore, blend, and retrieve information for different downstream tasks.\nEvaluation results show that the proposed method improves training efficiency\nand generalization in both Atari games and meta-world object manipulation\ntasks. Moreover, we demonstrate that memory fine-tuning further enhances the\nadaptability of the proposed architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jikun Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laroche_R/0/1/0/all/0/1\">Romain Laroche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xindi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trischler_A/0/1/0/all/0/1\">Adam Trischler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Trust GPT When Your Question Is Not In English. (arXiv:2305.16339v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16339","description":"<p>Large Language Models (LLMs) have demonstrated exceptional natural language\nunderstanding abilities and have excelled in a variety of natural language\nprocessing (NLP)tasks in recent years. Despite the fact that most LLMs are\ntrained predominantly in English, multiple studies have demonstrated their\ncomparative performance in many other languages. However, fundamental questions\npersist regarding how LLMs acquire their multi-lingual abilities and how\nperformance varies across different languages. These inquiries are crucial for\nthe study of LLMs since users and researchers often come from diverse language\nbackgrounds, potentially influencing their utilization and interpretation of\nLLMs' results. In this work, we propose a systematic way of qualifying the\nperformance disparities of LLMs under multilingual settings. We investigate the\nphenomenon of across-language generalizations in LLMs, wherein insufficient\nmulti-lingual training data leads to advanced multi-lingual capabilities. To\naccomplish this, we employ a novel back-translation-based prompting method. The\nresults show that GPT exhibits highly translating-like behaviour in\nmultilingual settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Senyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauer_B/0/1/0/all/0/1\">Bradley Hauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_N/0/1/0/all/0/1\">Ning Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondrak_G/0/1/0/all/0/1\">Grzegorz Kondrak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model. (arXiv:2305.16340v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16340","description":"<p>Transformers have shown dominant performance across a range of domains\nincluding language and vision. However, their computational cost grows\nquadratically with the sequence length, making their usage prohibitive for\nresource-constrained applications. To counter this, our approach is to divide\nthe whole sequence into segments. The information across segments can then be\naggregated using neurons with recurrence leveraging their inherent memory. Such\nan approach leads to models with sequential processing capability at a lower\ncomputation/memory cost. To investigate this idea, first, we examine the\neffects of using local attention mechanism on the individual segments. Then we\npropose a segmented recurrent transformer (SRformer) that combines segmented\nattention with recurrent attention. It uses recurrent accumulate and fire (RAF)\nlayers to process information between consecutive segments. The loss caused by\nreducing the attention window length is compensated by updating the product of\nkeys and values with RAF neurons' inherent recurrence. The segmented attention\nand lightweight RAF gates ensure the efficiency of the proposed transformer. We\napply the proposed method to T5 and BART transformers. The modified models are\ntested on summarization datasets including CNN-dailymail and XSUM. Notably,\nusing segmented inputs of different sizes, the proposed model achieves 4-19%\nhigher ROUGE1 scores than the segmented transformer baseline. Compared to full\nattention, the proposed model largely reduces the complexity of cross attention\nand results in around 40% reduction in computation cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yinghan Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Sayeed Shafayet Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InterFormer: Interactive Local and Global Features Fusion for Automatic Speech Recognition. (arXiv:2305.16342v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16342","description":"<p>The local and global features are both essential for automatic speech\nrecognition (ASR). Many recent methods have verified that simply combining\nlocal and global features can further promote ASR performance. However, these\nmethods pay less attention to the interaction of local and global features, and\ntheir series architectures are rigid to reflect local and global relationships.\nTo address these issues, this paper proposes InterFormer for interactive local\nand global features fusion to learn a better representation for ASR.\nSpecifically, we combine the convolution block with the transformer block in a\nparallel design. Besides, we propose a bidirectional feature interaction module\n(BFIM) and a selective fusion module (SFM) to implement the interaction and\nfusion of local and global features, respectively. Extensive experiments on\npublic ASR datasets demonstrate the effectiveness of our proposed InterFormer\nand its superior performance over the other Transformer and Conformer models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhi-Hao/0/1/0/all/0/1\">Zhi-Hao</a>, Lai"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Distributed Automatic Domain-Specific Multi-Word Term Recognition Architecture using Spark Ecosystem. (arXiv:2305.16343v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16343","description":"<p>Automatic Term Recognition is used to extract domain-specific terms that\nbelong to a given domain. In order to be accurate, these corpus and\nlanguage-dependent methods require large volumes of textual data that need to\nbe processed to extract candidate terms that are afterward scored according to\na given metric. To improve text preprocessing and candidate terms extraction\nand scoring, we propose a distributed Spark-based architecture to automatically\nextract domain-specific terms. The main contributions are as follows: (1)\npropose a novel distributed automatic domain-specific multi-word term\nrecognition architecture built on top of the Spark ecosystem; (2) perform an\nin-depth analysis of our architecture in terms of accuracy and scalability; (3)\ndesign an easy-to-integrate Python implementation that enables the use of Big\nData processing in fields such as Computational Linguistics and Natural\nLanguage Processing. We prove empirically the feasibility of our architecture\nby performing experiments on two real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Truica_C/0/1/0/all/0/1\">Ciprian-Octavian Truic&#x103;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Istrate_N/0/1/0/all/0/1\">Neculai-Ovidiu Istrate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apostol_E/0/1/0/all/0/1\">Elena-Simona Apostol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging LLMs for KPIs Retrieval from Hybrid Long-Document: A Comprehensive Framework and Dataset. (arXiv:2305.16344v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16344","description":"<p>Large Language Models (LLMs) demonstrate exceptional performance in textual\nunderstanding and tabular reasoning tasks. However, their ability to comprehend\nand analyze hybrid text, containing textual and tabular data, remains\nunderexplored. In this research, we specialize in harnessing the potential of\nLLMs to comprehend critical information from financial reports, which are\nhybrid long-documents. We propose an Automated Financial Information Extraction\n(AFIE) framework that enhances LLMs' ability to comprehend and extract\ninformation from financial reports. To evaluate AFIE, we develop a Financial\nReports Numerical Extraction (FINE) dataset and conduct an extensive\nexperimental analysis. Our framework is effectively validated on GPT-3.5 and\nGPT-4, yielding average accuracy increases of 53.94% and 33.77%, respectively,\ncompared to a naive method. These results suggest that the AFIE framework\noffers accuracy for automated numerical extraction from complex, hybrid\ndocuments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_C/0/1/0/all/0/1\">Chongjian Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xinrun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hengyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhiming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yanbing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lexinvariant Language Models. (arXiv:2305.16349v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16349","description":"<p>Token embeddings, a mapping from discrete lexical symbols to continuous\nvectors, are at the heart of any language model (LM). However, lexical symbol\nmeanings can also be determined and even redefined by their structural role in\na long context. In this paper, we ask: is it possible for a language model to\nbe performant without \\emph{any} fixed token embeddings? Such a language model\nwould have to rely entirely on the co-occurence and repetition of tokens in the\ncontext rather than the \\textit{a priori} identity of any token. To answer\nthis, we study \\textit{lexinvariant}language models that are invariant to\nlexical symbols and therefore do not need fixed token embeddings in practice.\nFirst, we prove that we can construct a lexinvariant LM to converge to the true\nlanguage model at a uniform rate that is polynomial in terms of the context\nlength, with a constant factor that is sublinear in the vocabulary size.\nSecond, to build a lexinvariant LM, we simply encode tokens using random\nGaussian vectors, such that each token maps to the same representation within\neach sequence but different representations across sequences. Empirically, we\ndemonstrate that it can indeed attain perplexity comparable to that of a\nstandard language model, given a sufficiently long context. We further explore\ntwo properties of the lexinvariant language models: First, given text generated\nfrom a substitution cipher of English, it implicitly implements Bayesian\nin-context deciphering and infers the mapping to the underlying real tokens\nwith high accuracy. Second, it has on average 4X better accuracy over synthetic\nin-context reasoning tasks. Finally, we discuss regularizing standard language\nmodels towards lexinvariance and potential practical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelikman_E/0/1/0/all/0/1\">Eric Zelikman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sarah Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuhuai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valiant_G/0/1/0/all/0/1\">Gregory Valiant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Betray Oneself: A Novel Audio DeepFake Detection Model via Mono-to-Stereo Conversion. (arXiv:2305.16353v1 [cs.SD])","link":"http://arxiv.org/abs/2305.16353","description":"<p>Audio Deepfake Detection (ADD) aims to detect the fake audio generated by\ntext-to-speech (TTS), voice conversion (VC) and replay, etc., which is an\nemerging topic. Traditionally we take the mono signal as input and focus on\nrobust feature extraction and effective classifier design. However, the\ndual-channel stereo information in the audio signal also includes important\ncues for deepfake, which has not been studied in the prior work. In this paper,\nwe propose a novel ADD model, termed as M2S-ADD, that attempts to discover\naudio authenticity cues during the mono-to-stereo conversion process. We first\nprojects the mono to a stereo signal using a pretrained stereo synthesizer,\nthen employs a dual-branch neural architecture to process the left and right\nchannel signals, respectively. In this way, we effectively reveal the artifacts\nin the fake audio, thus improve the ADD performance. The experiments on the\nASVspoof2019 database show that M2S-ADD outperforms all baselines that input\nmono. We release the source code at \\url{https://github.com/AI-S2-Lab/M2S-ADD}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinhua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Guanglai Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PandaGPT: One Model To Instruction-Follow Them All. (arXiv:2305.16355v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16355","description":"<p>We present PandaGPT, an approach to emPower large lANguage moDels with visual\nand Auditory instruction-following capabilities. Our pilot experiments show\nthat PandaGPT can perform complex tasks such as detailed image description\ngeneration, writing stories inspired by videos, and answering questions about\naudios. More interestingly, PandaGPT can take multimodal inputs simultaneously\nand compose their semantics naturally. For example, PandaGPT can connect how\nobjects look in an image/video and how they sound in an audio. To do so,\nPandaGPT combines the multimodal encoders from ImageBind and the large language\nmodels from Vicuna. Notably, only aligned image-text pairs are required for the\ntraining of PandaGPT. Thanks to the strong capability of ImageBind in embedding\ndata from different modalities into the same space, PandaGPT displays emergent,\ni.e. zero-shot, cross-modal behaviors for data other than image and text (e.g.,\nvideo, audio, depth, thermal, and IMU). We hope that PandaGPT serves as an\ninitial step toward building AGI that can perceive and understand inputs in\ndifferent modalities holistically, as we humans do. Our project page is at\nhttps://panda-gpt.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1\">Tian Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huayang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jialu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EDM3: Event Detection as Multi-task Text Generation. (arXiv:2305.16357v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16357","description":"<p>Event detection refers to identifying event occurrences in a text and\ncomprises of two subtasks; event identification and classification. We present\nEDM3, a novel approach for Event Detection that formulates three generative\ntasks: identification, classification, and combined detection. We show that\nEDM3 helps to learn transferable knowledge that can be leveraged to perform\nEvent Detection and its subtasks concurrently, mitigating the error propagation\ninherent in pipelined approaches. Unlike previous dataset- or domain-specific\napproaches, EDM3 utilizes the existing knowledge of language models, allowing\nit to be trained over any classification schema. We evaluate EDM3 on multiple\nevent detection datasets: RAMS, WikiEvents, MAVEN, and MLEE, showing that EDM3\noutperforms 1) single-task performance by 8.4% on average and 2) multi-task\nperformance without instructional prompts by 2.4% on average. We obtain SOTA\nresults on RAMS (71.3% vs. 65.1% F-1) and competitive performance on other\ndatasets. We analyze our approach to demonstrate its efficacy in low-resource\nand multi-sentence settings. We also show the effectiveness of this approach on\nnon-standard event configurations such as multi-word and multi-class event\ntriggers. Overall, our results show that EDM3 is a promising approach for Event\nDetection that has the potential for real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anantheswaran_U/0/1/0/all/0/1\">Ujjwala Anantheswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Mihir Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_K/0/1/0/all/0/1\">Kuntal Kumar Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving. (arXiv:2305.16366v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16366","description":"<p>Large language models~(LLMs) present an intriguing avenue of exploration in\nthe domain of formal theorem proving. Nonetheless, the full utilization of\nthese models, particularly in terms of demonstration formatting and\norganization, remains an underexplored area. In an endeavor to enhance the\nefficacy of LLMs, we introduce a subgoal-based demonstration learning\nframework, consisting of two primary elements: Firstly, drawing upon the\ninsights of subgoal learning from the domains of reinforcement learning and\nrobotics, we propose the construction of distinct subgoals for each\ndemonstration example and refine these subgoals in accordance with the\npertinent theories of subgoal learning. Secondly, we build upon recent advances\nin diffusion models to predict the optimal organization, simultaneously\naddressing two intricate issues that persist within the domain of demonstration\norganization: subset selection and order determination. Through the integration\nof subgoal-based learning methodologies, we have successfully increased the\nprevailing proof accuracy from 38.9\\% to 44.3\\% on the miniF2F benchmark.\nFurthermore, the adoption of diffusion models for demonstration organization\ncan lead to an additional enhancement in accuracy to 45.5\\%, or a $5\\times$\nimprovement in sampling efficiency compared with the long-standing\nstate-of-the-art method. Our code is available at\n\\url{https://github.com/HKUNLP/subgoal-theorem-prover}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xueliang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Role-Play with Large Language Models. (arXiv:2305.16367v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16367","description":"<p>As dialogue agents become increasingly human-like in their performance, it is\nimperative that we develop effective ways to describe their behaviour in\nhigh-level terms without falling into the trap of anthropomorphism. In this\npaper, we foreground the concept of role-play. Casting dialogue agent behaviour\nin terms of role-play allows us to draw on familiar folk psychological terms,\nwithout ascribing human characteristics to language models they in fact lack.\nTwo important cases of dialogue agent behaviour are addressed this way, namely\n(apparent) deception and (apparent) self-awareness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shanahan_M/0/1/0/all/0/1\">Murray Shanahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonell_K/0/1/0/all/0/1\">Kyle McDonell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reynolds_L/0/1/0/all/0/1\">Laria Reynolds</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"INTapt: Information-Theoretic Adversarial Prompt Tuning for Enhanced Non-Native Speech Recognition. (arXiv:2305.16371v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16371","description":"<p>Automatic Speech Recognition (ASR) systems have attained unprecedented\nperformance with large speech models pre-trained based on self-supervised\nspeech representation learning. However, these pre-trained speech models suffer\nfrom representational bias as they tend to better represent those prominent\naccents (i.e., native (L1) English accent) in the pre-training speech corpus\nthan less represented accents, resulting in a deteriorated performance for\nnon-native (L2) English accents. Although there have been some approaches to\nmitigate this issue, all of these methods require updating the pre-trained\nmodel weights. In this paper, we propose Information Theoretic Adversarial\nPrompt Tuning (INTapt), which introduces prompts concatenated to the original\ninput that can re-modulate the attention of the pre-trained model such that the\ncorresponding input resembles a native (L1) English speech without updating the\nbackbone weights. INTapt is trained simultaneously in the following two\nmanners: (1) adversarial training to reduce accent feature dependence between\nthe original input and the prompt-concatenated input and (2) training to\nminimize CTC loss for improving ASR performance to a prompt-concatenated input.\nExperimental results show that INTapt improves the performance of L2 English\nand increases feature similarity between L2 and L1 accents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_E/0/1/0/all/0/1\">Eunseop Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1\">Hee Suk Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harvill_J/0/1/0/all/0/1\">John Harvill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasegawa_Johnson_M/0/1/0/all/0/1\">Mark Hasegawa-Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1\">Chang D. Yoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer. (arXiv:2305.16380v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16380","description":"<p>Transformer architecture has shown impressive performance in multiple\nresearch domains and has become the backbone of many neural network models.\nHowever, there is limited understanding on how it works. In particular, with a\nsimple predictive loss, how the representation emerges from the gradient\n\\emph{training dynamics} remains a mystery. In this paper, for 1-layer\ntransformer with one self-attention layer plus one decoder layer, we analyze\nits SGD training dynamics for the task of next token prediction in a\nmathematically rigorous manner. We open the black box of the dynamic process of\nhow the self-attention layer combines input tokens, and reveal the nature of\nunderlying inductive bias. More specifically, with the assumption (a) no\npositional encoding, (b) long input sequence, and (c) the decoder layer learns\nfaster than the self-attention layer, we prove that self-attention acts as a\n\\emph{discriminative scanning algorithm}: starting from uniform attention, it\ngradually attends more to distinct key tokens for a specific next token to be\npredicted, and pays less attention to common key tokens that occur across\ndifferent next tokens. Among distinct tokens, it progressively drops attention\nweights, following the order of low to high co-occurrence between the key and\nthe query token in the training set. Interestingly, this procedure does not\nlead to winner-takes-all, but decelerates due to a \\emph{phase transition} that\nis controllable by the learning rates of the two layers, leaving (almost) fixed\ntoken combination. We verify this \\textbf{\\emph{scan and snap}} dynamics on\nsynthetic and real-world data (WikiText).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Beidi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Simon Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Diffusion Models Vision-And-Language Reasoners?. (arXiv:2305.16397v1 [cs.CV])","link":"http://arxiv.org/abs/2305.16397","description":"<p>Text-conditioned image generation models have recently shown immense\nqualitative success using denoising diffusion processes. However, unlike\ndiscriminative vision-and-language models, it is a non-trivial task to subject\nthese diffusion-based generative models to automatic fine-grained quantitative\nevaluation of high-level phenomena such as compositionality. Towards this goal,\nwe perform two innovations. First, we transform diffusion-based models (in our\ncase, Stable Diffusion) for any image-text matching (ITM) task using a novel\nmethod called DiffusionITM. Second, we introduce the Generative-Discriminative\nEvaluation Benchmark (GDBench) benchmark with 7 complex vision-and-language\ntasks, bias evaluation and detailed analysis. We find that Stable Diffusion +\nDiffusionITM is competitive on many tasks and outperforms CLIP on compositional\ntasks like like CLEVR and Winoground. We further boost its compositional\nperformance with a transfer setup by fine-tuning on MS-COCO while retaining\ngenerative capabilities. We also measure the stereotypical bias in diffusion\nmodels, and find that Stable Diffusion 2.1 is, for the most part, less biased\nthan Stable Diffusion 1.5. Overall, our results point in an exciting direction\nbringing discriminative and generative model evaluation closer. We will release\ncode and benchmark setup soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krojer_B/0/1/0/all/0/1\">Benno Krojer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poole_Dayan_E/0/1/0/all/0/1\">Elinor Poole-Dayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1\">Vikram Voleti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Aware Attention Layers coupled with Optimal Transport Domain Adaptation methods for recognizing dementia from spontaneous speech. (arXiv:2305.16406v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16406","description":"<p>Alzheimer's disease (AD) constitutes a complex neurocognitive disease and is\nthe main cause of dementia. Although many studies have been proposed targeting\nat diagnosing dementia through spontaneous speech, there are still limitations.\nExisting state-of-the-art approaches, which propose multimodal methods, train\nseparately language and acoustic models, employ majority-vote approaches, and\nconcatenate the representations of the different modalities either at the input\nlevel, i.e., early fusion, or during training. Also, some of them employ\nself-attention layers, which calculate the dependencies between representations\nwithout considering the contextual information. In addition, no prior work has\ntaken into consideration the model calibration. To address these limitations,\nwe propose some new methods for detecting AD patients, which capture the intra-\nand cross-modal interactions. First, we convert the audio files into log-Mel\nspectrograms, their delta, and delta-delta and create in this way an image per\naudio file consisting of three channels. Next, we pass each transcript and\nimage through BERT and DeiT models respectively. After that, context-based\nself-attention layers, self-attention layers with a gate model, and optimal\ntransport domain adaptation methods are employed for capturing the intra- and\ninter-modal interactions. Finally, we exploit two methods for fusing the self\nand cross-attended features. For taking into account the model calibration, we\napply label smoothing. We use both performance and calibration metrics.\nExperiments conducted on the ADReSS Challenge dataset indicate the efficacy of\nour introduced approaches over existing research initiatives with our best\nperforming model reaching Accuracy and F1-score up to 91.25% and 91.06%\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ilias_L/0/1/0/all/0/1\">Loukas Ilias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askounis_D/0/1/0/all/0/1\">Dimitris Askounis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Script Normalization for Unconventional Writing of Under-Resourced Languages in Bilingual Communities. (arXiv:2305.16407v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16407","description":"<p>The wide accessibility of social media has provided linguistically\nunder-represented communities with an extraordinary opportunity to create\ncontent in their native languages. This, however, comes with certain challenges\nin script normalization, particularly where the speakers of a language in a\nbilingual community rely on another script or orthography to write their native\nlanguage. This paper addresses the problem of script normalization for several\nsuch languages that are mainly written in a Perso-Arabic script. Using\nsynthetic data with various levels of noise and a transformer-based model, we\ndemonstrate that the problem can be effectively remediated. We conduct a\nsmall-scale evaluation of real data as well. Our experiments indicate that\nscript normalization is also beneficial to improve the performance of\ndownstream tasks such as machine translation and language identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmadi_S/0/1/0/all/0/1\">Sina Ahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual Probing for the influence of affect and specificity on Intergroup Bias. (arXiv:2305.16409v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16409","description":"<p>While existing work on studying bias in NLP focues on negative or pejorative\nlanguage use, Govindarajan et al. (2023) offer a revised framing of bias in\nterms of intergroup social context, and its effects on language behavior. In\nthis paper, we investigate if two pragmatic features (specificity and affect)\nsystematically vary in different intergroup contexts -- thus connecting this\nnew framing of bias to language output. Preliminary analysis finds modest\ncorrelations between specificity and affect of tweets with supervised\nintergroup relationship (IGR) labels. Counterfactual probing further reveals\nthat while neural models finetuned for predicting IGR labels reliably use\naffect in classification, the model's usage of specificity is inconclusive.\nCode and data can be found at: https://github.com/venkatasg/intergroup-probing\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Govindarajan_V/0/1/0/all/0/1\">Venkata S Govindarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1\">Kyle Mahowald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaver_D/0/1/0/all/0/1\">David I. Beaver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not wacky vs. definitely wacky: A study of scalar adverbs in pretrained language models. (arXiv:2305.16426v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16426","description":"<p>Vector space models of word meaning all share the assumption that words\noccurring in similar contexts have similar meanings. In such models, words that\nare similar in their topical associations but differ in their logical force\ntend to emerge as semantically close, creating well-known challenges for NLP\napplications that involve logical reasoning. Modern pretrained language models,\nsuch as BERT, RoBERTa and GPT-3 hold the promise of performing better on\nlogical tasks than classic static word embeddings. However, reports are mixed\nabout their success. In the current paper, we advance this discussion through a\nsystematic study of scalar adverbs, an under-explored class of words with\nstrong logical force. Using three different tasks, involving both naturalistic\nsocial media data and constructed examples, we investigate the extent to which\nBERT, RoBERTa, GPT-2 and GPT-3 exhibit general, human-like, knowledge of these\ncommon words. We ask: 1) Do the models distinguish amongst the three semantic\ncategories of MODALITY, FREQUENCY and DEGREE? 2) Do they have implicit\nrepresentations of full scales from maximally negative to maximally positive?\n3) How do word frequency and contextual factors impact model performance? We\nfind that despite capturing some aspects of logical meaning, the models fall\nfar short of human performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lorge_I/0/1/0/all/0/1\">Isabelle Lorge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierrehumbert_J/0/1/0/all/0/1\">Janet Pierrehumbert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Machine Translation for Mathematical Formulae. (arXiv:2305.16433v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16433","description":"<p>We tackle the problem of neural machine translation of mathematical formulae\nbetween ambiguous presentation languages and unambiguous content languages.\nCompared to neural machine translation on natural language, mathematical\nformulae have a much smaller vocabulary and much longer sequences of symbols,\nwhile their translation requires extreme precision to satisfy mathematical\ninformation needs. In this work, we perform the tasks of translating from LaTeX\nto Mathematica as well as from LaTeX to semantic LaTeX. While recurrent,\nrecursive, and transformer networks struggle with preserving all contained\ninformation, we find that convolutional sequence-to-sequence networks achieve\n95.1% and 90.7% exact matches, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petersen_F/0/1/0/all/0/1\">Felix Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubotz_M/0/1/0/all/0/1\">Moritz Schubotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greiner_Petter_A/0/1/0/all/0/1\">Andre Greiner-Petter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Retrain, Just Rewrite: Countering Adversarial Perturbations by Rewriting Text. (arXiv:2305.16444v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16444","description":"<p>Can language models transform inputs to protect text classifiers against\nadversarial attacks? In this work, we present ATINTER, a model that intercepts\nand learns to rewrite adversarial inputs to make them non-adversarial for a\ndownstream text classifier. Our experiments on four datasets and five attack\nmechanisms reveal that ATINTER is effective at providing better adversarial\nrobustness than existing defense approaches, without compromising task\naccuracy. For example, on sentiment classification using the SST-2 dataset, our\nmethod improves the adversarial accuracy over the best existing defense\napproach by more than 4% with a smaller decrease in task accuracy (0.5% vs\n2.5%). Moreover, we show that ATINTER generalizes across multiple downstream\ntasks and classifiers without having to explicitly retrain it for those\nsettings. Specifically, we find that when ATINTER is trained to remove\nadversarial perturbations for the sentiment classification task on the SST-2\ndataset, it even transfers to a semantically different task of news\nclassification (on AGNews) and improves the adversarial robustness by more than\n10%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ashim Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blum_C/0/1/0/all/0/1\">Carter Wood Blum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choji_T/0/1/0/all/0/1\">Temma Choji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Y/0/1/0/all/0/1\">Yingjie Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Shalin Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vempala_A/0/1/0/all/0/1\">Alakananda Vempala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring the Effect of Influential Messages on Varying Personas. (arXiv:2305.16470v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16470","description":"<p>Predicting how a user responds to news events enables important applications\nsuch as allowing intelligent agents or content producers to estimate the effect\non different communities and revise unreleased messages to prevent unexpected\nbad outcomes such as social conflict and moral injury. We present a new task,\nResponse Forecasting on Personas for News Media, to estimate the response a\npersona (characterizing an individual or a group) might have upon seeing a news\nmessage. Compared to the previous efforts which only predict generic comments\nto news, the proposed task not only introduces personalization in the modeling\nbut also predicts the sentiment polarity and intensity of each response. This\nenables more accurate and comprehensive inference on the mental state of the\npersona. Meanwhile, the generated sentiment dimensions make the evaluation and\napplication more reliable. We create the first benchmark dataset, which\nconsists of 13,357 responses to 3,847 news headlines from Twitter. We further\nevaluate the SOTA neural language models with our dataset. The empirical\nresults suggest that the included persona attributes are helpful for the\nperformance of all response dimensions. Our analysis shows that the\nbest-performing models are capable of predicting responses that are consistent\nwith the personas, and as a byproduct, the task formulation also enables many\ninteresting applications in the analysis of social network groups and their\nopinions, such as the discovery of extreme opinion groups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chenkai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1\">Hou Pong Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prototype-Based Interpretability for Legal Citation Prediction. (arXiv:2305.16490v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16490","description":"<p>Deep learning has made significant progress in the past decade, and\ndemonstrates potential to solve problems with extensive social impact. In\nhigh-stakes decision making areas such as law, experts often require\ninterpretability for automatic systems to be utilized in practical settings. In\nthis work, we attempt to address these requirements applied to the important\nproblem of legal citation prediction (LCP). We design the task with parallels\nto the thought-process of lawyers, i.e., with reference to both precedents and\nlegislative provisions. After initial experimental results, we refine the\ntarget citation predictions with the feedback of legal experts. Additionally,\nwe introduce a prototype architecture to add interpretability, achieving strong\nperformance while adhering to decision parameters used by lawyers. Our study\nbuilds on and leverages the state-of-the-art language processing models for\nlaw, while addressing vital considerations for high-stakes tasks with practical\nsocietal impact.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chu Fei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhambhoria_R/0/1/0/all/0/1\">Rohan Bhambhoria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahan_S/0/1/0/all/0/1\">Samuel Dahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IMBERT: Making BERT Immune to Insertion-based Backdoor Attacks. (arXiv:2305.16503v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16503","description":"<p>Backdoor attacks are an insidious security threat against machine learning\nmodels. Adversaries can manipulate the predictions of compromised models by\ninserting triggers into the training phase. Various backdoor attacks have been\ndevised which can achieve nearly perfect attack success without affecting model\npredictions for clean inputs. Means of mitigating such vulnerabilities are\nunderdeveloped, especially in natural language processing. To fill this gap, we\nintroduce IMBERT, which uses either gradients or self-attention scores derived\nfrom victim models to self-defend against backdoor attacks at inference time.\nOur empirical studies demonstrate that IMBERT can effectively identify up to\n98.5% of inserted triggers. Thus, it significantly reduces the attack success\nrate while attaining competitive accuracy on the clean dataset across\nwidespread insertion-based attacks compared to two baselines. Finally, we show\nthat our approach is model-agnostic, and can be easily ported to several\npre-trained transformer models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuanli He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1\">Benjamin Rubinstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Tool Manipulation Capability of Open-source Large Language Models. (arXiv:2305.16504v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16504","description":"<p>Recent studies on software tool manipulation with large language models\n(LLMs) mostly rely on closed model APIs. The industrial adoption of these\nmodels is substantially constrained due to the security and robustness risks in\nexposing information to closed LLM API services. In this paper, we ask can we\nenhance open-source LLMs to be competitive to leading closed LLM APIs in tool\nmanipulation, with practical amount of human supervision. By analyzing common\ntool manipulation failures, we first demonstrate that open-source LLMs may\nrequire training with usage examples, in-context demonstration and generation\nstyle regulation to resolve failures. These insights motivate us to revisit\nclassical methods in LLM literature, and demonstrate that we can adapt them as\nmodel alignment with programmatic data generation, system prompts and\nin-context demonstration retrievers to enhance open-source LLMs for tool\nmanipulation. To evaluate these techniques, we create the ToolBench, a tool\nmanipulation benchmark consisting of diverse software tools for real-world\ntasks. We demonstrate that our techniques can boost leading open-source LLMs by\nup to 90% success rate, showing capabilities competitive to OpenAI GPT-4 in 4\nout of 8 ToolBench tasks. We show that such enhancement typically requires\nabout one developer day to curate data for each tool, rendering a recipe with\npractical amount of human supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_F/0/1/0/all/0/1\">Fenglu Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Changran Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Dangers of trusting Stochastic Parrots: Faithfulness and Trust in Open-domain Conversational Question Answering. (arXiv:2305.16519v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16519","description":"<p>Large language models are known to produce output which sounds fluent and\nconvincing, but is also often wrong, e.g. \"unfaithful\" with respect to a\nrationale as retrieved from a knowledge base. In this paper, we show that\ntask-based systems which exhibit certain advanced linguistic dialog behaviors,\nsuch as lexical alignment (repeating what the user said), are in fact preferred\nand trusted more, whereas other phenomena, such as pronouns and ellipsis are\ndis-preferred. We use open-domain question answering systems as our test-bed\nfor task based dialog generation and compare several open- and closed-book\nmodels. Our results highlight the danger of systems that appear to be\ntrustworthy by parroting user input while providing an unfaithful response.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiesurin_S/0/1/0/all/0/1\">Sabrina Chiesurin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimakopoulos_D/0/1/0/all/0/1\">Dimitris Dimakopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabezudo_M/0/1/0/all/0/1\">Marco Antonio Sobrevilla Cabezudo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eshghi_A/0/1/0/all/0/1\">Arash Eshghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papaioannou_I/0/1/0/all/0/1\">Ioannis Papaioannou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1\">Verena Rieser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konstas_I/0/1/0/all/0/1\">Ioannis Konstas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label Agnostic Pre-training for Zero-shot Text Classification. (arXiv:2305.16521v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16521","description":"<p>Conventional approaches to text classification typically assume the existence\nof a fixed set of predefined labels to which a given text can be classified.\nHowever, in real-world applications, there exists an infinite label space for\ndescribing a given text. In addition, depending on the aspect (sentiment,\ntopic, etc.) and domain of the text (finance, legal, etc.), the interpretation\nof the label can vary greatly. This makes the task of text classification,\nparticularly in the zero-shot scenario, extremely challenging. In this paper,\nwe investigate the task of zero-shot text classification with the aim of\nimproving the ability of pre-trained language models (PLMs) to generalize to\nboth seen and unseen data across varying aspects and domains. To solve this we\nintroduce two new simple yet effective pre-training strategies, Implicit and\nExplicit pre-training. These methods inject aspect-level understanding into the\nmodel at train time with the goal of conditioning the model to build task-level\nunderstanding. To evaluate this, we construct and release UTCD, a new benchmark\ndataset for evaluating text classification in zero-shot settings. Experimental\nresults on UTCD show that our approach achieves improved zero-shot\ngeneralization on a suite of challenging datasets across an array of zero-shot\nformalizations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clarke_C/0/1/0/all/0/1\">Christopher Clarke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_Y/0/1/0/all/0/1\">Yuzhao Heng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yiping Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flautner_K/0/1/0/all/0/1\">Krisztian Flautner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Lingjia Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mars_J/0/1/0/all/0/1\">Jason Mars</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Annotating and Detecting Fine-grained Factual Errors for Dialogue Summarization. (arXiv:2305.16548v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16548","description":"<p>A series of datasets and models have been proposed for summaries generated\nfor well-formatted documents such as news articles. Dialogue summaries,\nhowever, have been under explored. In this paper, we present the first dataset\nwith fine-grained factual error annotations named DIASUMFACT. We define\nfine-grained factual error detection as a sentence-level multi-label\nclassification problem, and we evaluate two state-of-the-art (SOTA) models on\nour dataset. Both models yield sub-optimal results, with a macro-averaged F1\nscore of around 0.25 over 6 error classes. We further propose an unsupervised\nmodel ENDERANKER via candidate ranking using pretrained encoder-decoder models.\nOur model performs on par with the SOTA models while requiring fewer resources.\nThese observations confirm the challenges in detecting factual errors from\ndialogue summaries, which call for further studies, for which our dataset and\nresults offer a solid foundation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Rongxin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jianzhong Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teamwork Is Not Always Good: An Empirical Study of Classifier Drift in Class-incremental Information Extraction. (arXiv:2305.16559v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16559","description":"<p>Class-incremental learning (CIL) aims to develop a learning system that can\ncontinually learn new classes from a data stream without forgetting previously\nlearned classes. When learning classes incrementally, the classifier must be\nconstantly updated to incorporate new classes, and the drift in decision\nboundary may lead to severe forgetting. This fundamental challenge, however,\nhas not yet been studied extensively, especially in the setting where no\nsamples from old classes are stored for rehearsal. In this paper, we take a\ncloser look at how the drift in the classifier leads to forgetting, and\naccordingly, design four simple yet (super-) effective solutions to alleviate\nthe classifier drift: an Individual Classifiers with Frozen Feature Extractor\n(ICE) framework where we individually train a classifier for each learning\nsession, and its three variants ICE-PL, ICE-O, and ICE-PL&amp;O which further take\nthe logits of previously learned classes from old sessions or a constant logit\nof an Other class as a constraint to the learning of new classifiers. Extensive\nexperiments and analysis on 6 class-incremental information extraction tasks\ndemonstrate that our solutions, especially ICE-O, consistently show significant\nimprovement over the previous state-of-the-art approaches with up to 44.7%\nabsolute F-score gain, providing a strong baseline and insights for future\nresearch on class-incremental learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minqian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual reasoning: Testing language models' understanding of hypothetical scenarios. (arXiv:2305.16572v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16572","description":"<p>Current pre-trained language models have enabled remarkable improvements in\ndownstream tasks, but it remains difficult to distinguish effects of\nstatistical correlation from more systematic logical reasoning grounded on the\nunderstanding of real world. We tease these factors apart by leveraging\ncounterfactual conditionals, which force language models to predict unusual\nconsequences based on hypothetical propositions. We introduce a set of tests\nfrom psycholinguistic experiments, as well as larger-scale controlled datasets,\nto probe counterfactual predictions from five pre-trained language models. We\nfind that models are consistently able to override real-world knowledge in\ncounterfactual scenarios, and that this effect is more robust in case of\nstronger baseline world knowledge -- however, we also find that for most models\nthis effect appears largely to be driven by simple lexical cues. When we\nmitigate effects of both world knowledge and lexical cues to test knowledge of\nlinguistic nuances of counterfactuals, we find that only GPT-3 shows\nsensitivity to these nuances, though this sensitivity is also non-trivially\nimpacted by lexical associative factors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ettinger_A/0/1/0/all/0/1\">Allyson Ettinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nichelle and Nancy: The Influence of Demographic Attributes and Tokenization Length on First Name Biases. (arXiv:2305.16577v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16577","description":"<p>Through the use of first name substitution experiments, prior research has\ndemonstrated the tendency of social commonsense reasoning models to\nsystematically exhibit social biases along the dimensions of race, ethnicity,\nand gender (An et al., 2023). Demographic attributes of first names, however,\nare strongly correlated with corpus frequency and tokenization length, which\nmay influence model behavior independent of or in addition to demographic\nfactors. In this paper, we conduct a new series of first name substitution\nexperiments that measures the influence of these factors while controlling for\nthe others. We find that demographic attributes of a name (race, ethnicity, and\ngender) and name tokenization length are both factors that systematically\naffect the behavior of social commonsense reasoning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_H/0/1/0/all/0/1\">Haozhe An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudinger_R/0/1/0/all/0/1\">Rachel Rudinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLP Reproducibility For All: Understanding Experiences of Beginners. (arXiv:2305.16579v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16579","description":"<p>As natural language processing (NLP) has recently seen an unprecedented level\nof excitement, and more people are eager to enter the field, it is unclear\nwhether current research reproducibility efforts are sufficient for this group\nof beginners to apply the latest developments. To understand their needs, we\nconducted a study with 93 students in an introductory NLP course, where\nstudents reproduced the results of recent NLP papers. Surprisingly, we find\nthat their programming skill and comprehension of research papers have a\nlimited impact on their effort spent completing the exercise. Instead, we find\naccessibility efforts by research authors to be the key to success, including\ncomplete documentation, better coding practice, and easier access to data\nfiles. Going forward, we recommend that NLP researchers pay close attention to\nthese simple aspects of open-sourcing their work, and use insights from\nbeginners' feedback to provide actionable ideas on how to better support them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Storks_S/0/1/0/all/0/1\">Shane Storks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Keunwoo Peter Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Ziqiao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Investigation of Noise in Morphological Inflection. (arXiv:2305.16581v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16581","description":"<p>With a growing focus on morphological inflection systems for languages where\nhigh-quality data is scarce, training data noise is a serious but so far\nlargely ignored concern. We aim at closing this gap by investigating the types\nof noise encountered within a pipeline for truly unsupervised morphological\nparadigm completion and its impact on morphological inflection systems: First,\nwe propose an error taxonomy and annotation pipeline for inflection training\ndata. Then, we compare the effect of different types of noise on multiple\nstate-of-the-art inflection models. Finally, we propose a novel character-level\nmasked language modeling (CMLM) pretraining objective and explore its impact on\nthe models' resistance to noise. Our experiments show that various\narchitectures are impacted differently by separate types of noise, but\nencoder-decoders tend to be more robust to noise than models trained with a\ncopy bias. CMLM pretraining helps transformers, but has lower impact on LSTMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wiemerslage_A/0/1/0/all/0/1\">Adam Wiemerslage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Changbing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicolai_G/0/1/0/all/0/1\">Garrett Nicolai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silfverberg_M/0/1/0/all/0/1\">Miikka Silfverberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kann_K/0/1/0/all/0/1\">Katharina Kann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models. (arXiv:2305.16582v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16582","description":"<p>With the widespread use of large language models (LLMs) in NLP tasks,\nresearchers have discovered the potential of Chain-of-thought (CoT) to assist\nLLMs in accomplishing complex reasoning tasks by generating intermediate steps.\nHowever, human thought processes are often non-linear, rather than simply\nsequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT)\nreasoning, which models human thought processes not only as a chain but also as\na graph. By representing thought units as nodes and connections between them as\nedges, our approach captures the non-sequential nature of human thinking and\nallows for a more realistic modeling of thought processes. Similar to\nMultimodal-CoT, we modeled GoT reasoning as a two-stage framework, generating\nrationales first and then producing the final answer. Specifically, we employ\nan additional graph-of-thoughts encoder for GoT representation learning and\nfuse the GoT representation with the original input representation through a\ngated fusion mechanism. We implement a GoT reasoning model on the T5\npre-trained model and evaluate its performance on a text-only reasoning task\n(GSM8K) and a multimodal reasoning task (ScienceQA). Our model achieves\nsignificant improvement over the strong CoT baseline with 3.41% and 5.08% on\nthe GSM8K test set with T5-base and T5-large architectures, respectively.\nAdditionally, our model boosts accuracy from 84.91% to 91.54% using the T5-base\nmodel and from 91.68% to 92.77% using the T5-large model over the\nstate-of-the-art Multimodal-CoT on the ScienceQA test set. Experiments have\nshown that GoT achieves comparable results to Multimodal-CoT(large) with over\n700M parameters, despite having fewer than 250M backbone model parameters,\ndemonstrating the effectiveness of GoT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation. (arXiv:2305.16585v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16585","description":"<p>Paraphrase generation is a long-standing task in natural language processing\n(NLP). Supervised paraphrase generation models, which rely on human-annotated\nparaphrase pairs, are cost-inefficient and hard to scale up. On the other hand,\nautomatically annotated paraphrase pairs (e.g., by machine back-translation),\nusually suffer from the lack of syntactic diversity -- the generated paraphrase\nsentences are very similar to the source sentences in terms of syntax. In this\nwork, we present ParaAMR, a large-scale syntactically diverse paraphrase\ndataset created by abstract meaning representation back-translation. Our\nquantitative analysis, qualitative examples, and human evaluation demonstrate\nthat the paraphrases of ParaAMR are syntactically more diverse compared to\nexisting large-scale paraphrase datasets while preserving good semantic\nsimilarity. In addition, we show that ParaAMR can be used to improve on three\nNLP tasks: learning sentence embeddings, syntactically controlled paraphrase\ngeneration, and data augmentation for few-shot learning. Our results thus\nshowcase the potential of ParaAMR for improving various NLP applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_V/0/1/0/all/0/1\">Varun Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1\">I-Hung Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Anoop Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models. (arXiv:2305.16597v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16597","description":"<p>Parameter-efficient tuning (PET) methods fit pre-trained language models\n(PLMs) to downstream tasks by either computing a small compressed update for a\nsubset of model parameters, or appending and fine-tuning a small number of new\nmodel parameters to the pre-trained network. Hand-designed PET architectures\nfrom the literature perform well in practice, but have the potential to be\nimproved via automated neural architecture search (NAS). We propose an\nefficient NAS method for learning PET architectures via structured and\nunstructured pruning. We present experiments on GLUE demonstrating the\neffectiveness of our algorithm and discuss how PET architectural design choices\naffect performance in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lawton_N/0/1/0/all/0/1\">Neal Lawton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Anoop Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thattai_G/0/1/0/all/0/1\">Govind Thattai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steeg_G/0/1/0/all/0/1\">Greg Ver Steeg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NormMark: A Weakly Supervised Markov Model for Socio-cultural Norm Discovery. (arXiv:2305.16598v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16598","description":"<p>Norms, which are culturally accepted guidelines for behaviours, can be\nintegrated into conversational models to generate utterances that are\nappropriate for the socio-cultural context. Existing methods for norm\nrecognition tend to focus only on surface-level features of dialogues and do\nnot take into account the interactions within a conversation. To address this\nissue, we propose NormMark, a probabilistic generative Markov model to carry\nthe latent features throughout a dialogue. These features are captured by\ndiscrete and continuous latent variables conditioned on the conversation\nhistory, and improve the model's ability in norm recognition. The model is\ntrainable on weakly annotated data using the variational technique. On a\ndataset with limited norm annotations, we show that our approach achieves\nhigher F1 score, outperforming current state-of-the-art methods, including\nGPT3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moghimifar_F/0/1/0/all/0/1\">Farhad Moghimifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_S/0/1/0/all/0/1\">Shilin Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongtong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging the Domain Gaps in Context Representations for k-Nearest Neighbor Neural Machine Translation. (arXiv:2305.16599v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16599","description":"<p>$k$-Nearest neighbor machine translation ($k$NN-MT) has attracted increasing\nattention due to its ability to non-parametrically adapt to new translation\ndomains. By using an upstream NMT model to traverse the downstream training\ncorpus, it is equipped with a datastore containing vectorized key-value pairs,\nwhich are retrieved during inference to benefit translation. However, there\noften exists a significant gap between upstream and downstream domains, which\nhurts the retrieval accuracy and the final translation quality. To deal with\nthis issue, we propose a novel approach to boost the datastore retrieval of\n$k$NN-MT by reconstructing the original datastore. Concretely, we design a\nreviser to revise the key representations, making them better fit for the\ndownstream domain. The reviser is trained using the collected\nsemantically-related key-queries pairs, and optimized by two proposed losses:\none is the key-queries semantic distance ensuring each revised key\nrepresentation is semantically related to its corresponding queries, and the\nother is an L2-norm loss encouraging revised key representations to effectively\nretain the knowledge learned by the upstream NMT model. Extensive experiments\non domain adaptation tasks demonstrate that our method can effectively boost\nthe datastore retrieval and translation quality of $k$NN-MT.\\footnote{Our code\nis available at \\url{https://github.com/DeepLearnXMU/RevisedKey-knn-mt}.}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhiwei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baosong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Huan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Suhang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiangpeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dayiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model. (arXiv:2305.16617v1 [cs.LG])","link":"http://arxiv.org/abs/2305.16617","description":"<p>The detection of machine-generated text, especially from large language\nmodels (LLMs), is crucial in preventing serious social problems resulting from\ntheir misuse. Some methods train dedicated detectors on specific datasets but\nfall short in generalizing to unseen test data, while other zero-shot ones\noften yield suboptimal performance. Although the recent DetectGPT has shown\npromising detection performance, it suffers from significant inefficiency\nissues, as detecting a single candidate requires scoring hundreds of its\nperturbations with the source LLM. This paper aims to bridge this gap.\nTechnically, we propose to incorporate a Bayesian surrogate model, which allows\nus to select typical samples based on Bayesian uncertainty and interpolate\nscores from typical samples to other ones, to improve query efficiency. Our\nempirical results demonstrate that our method significantly outperforms\nexisting approaches under a low query budget. Notably, our method achieves\nsimilar performance with up to 2 times fewer queries than DetectGPT and 3.7%\nhigher AUROC at a query number of 5.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhijie Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Hongcheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1\">Yibo Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of Question Generation Needs More References. (arXiv:2305.16626v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16626","description":"<p>Question generation (QG) is the task of generating a valid and fluent\nquestion based on a given context and the target answer. According to various\npurposes, even given the same context, instructors can ask questions about\ndifferent concepts, and even the same concept can be written in different ways.\nHowever, the evaluation for QG usually depends on single reference-based\nsimilarity metrics, such as n-gram-based metric or learned metric, which is not\nsufficient to fully evaluate the potential of QG methods. To this end, we\npropose to paraphrase the reference question for a more robust QG evaluation.\nUsing large language models such as GPT-3, we created semantically and\nsyntactically diverse questions, then adopt the simple aggregation of the\npopular evaluation metrics as the final scores. Through our experiments, we\nfound that using multiple (pseudo) references is more effective for QG\nevaluation while showing a higher correlation with human evaluations than\nevaluation with a single reference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Shinhyeok Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Go_H/0/1/0/all/0/1\">Hyojun Go</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_H/0/1/0/all/0/1\">Hyeongdon Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yunsung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1\">Myeongho Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyun Seung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Seungtaek Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero is Not Hero Yet: Benchmarking Zero-Shot Performance of LLMs for Financial Tasks. (arXiv:2305.16633v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16633","description":"<p>Recently large language models (LLMs) like ChatGPT have shown impressive\nperformance on many natural language processing tasks with zero-shot. In this\npaper, we investigate the effectiveness of zero-shot LLMs in the financial\ndomain. We compare the performance of ChatGPT along with some open-source\ngenerative LLMs in zero-shot mode with RoBERTa fine-tuned on annotated data. We\naddress three inter-related research questions on data annotation, performance\ngaps, and the feasibility of employing generative models in the finance domain.\nOur findings demonstrate that ChatGPT performs well even without labeled data\nbut fine-tuned models generally outperform it. Our research also highlights how\nannotating with generative models can be time-intensive. Our codebase is\npublicly available on GitHub under CC BY-NC 4.0 license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Agam Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chava_S/0/1/0/all/0/1\">Sudheer Chava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Impossible Distillation: from Low-Quality Model to High-Quality Dataset & Model for Summarization and Paraphrasing. (arXiv:2305.16635v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16635","description":"<p>It is commonly perceived that the strongest language models (LMs) rely on a\ncombination of massive scale, instruction data, and human feedback to perform\nspecialized tasks -- e.g. summarization and paraphrasing, without supervision.\nIn this paper, we propose that language models can learn to summarize and\nparaphrase sentences, with none of these 3 factors. We present Impossible\nDistillation, a framework that distills a task-specific dataset directly from\nan off-the-shelf LM, even when it is impossible for the LM itself to reliably\nsolve the task. By training a student model on the generated dataset and\namplifying its capability through self-distillation, our method yields a\nhigh-quality model and dataset from a low-quality teacher model, without the\nneed for scale or supervision. Using Impossible Distillation, we are able to\ndistill an order of magnitude smaller model (with only 770M parameters) that\noutperforms 175B parameter GPT-3, in both quality and controllability, as\nconfirmed by automatic and human evaluations. Furthermore, as a useful\nbyproduct of our approach, we obtain DIMSUM+, a high-quality dataset with 3.4M\nsentence summaries and paraphrases. Our analyses show that this dataset, as a\npurely LM-generated corpus, is more diverse and more effective for\ngeneralization to unseen domains than all human-authored datasets -- including\nGigaword with 4M samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1\">Jaehun Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_J/0/1/0/all/0/1\">Jillian Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorensen_T/0/1/0/all/0/1\">Taylor Sorensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions. (arXiv:2305.16636v1 [cs.IR])","link":"http://arxiv.org/abs/2305.16636","description":"<p>Modern machine learning relies on datasets to develop and validate research\nideas. Given the growth of publicly available data, finding the right dataset\nto use is increasingly difficult. Any research question imposes explicit and\nimplicit constraints on how well a given dataset will enable researchers to\nanswer this question, such as dataset size, modality, and domain. We introduce\na new task of recommending relevant datasets given a short natural language\ndescription of a research idea, to help people find relevant datasets for their\nneeds. Dataset recommendation poses unique challenges as an information\nretrieval problem; datasets are hard to directly index for search and there are\nno corpora readily available for this task. To operationalize this task, we\nbuild the DataFinder Dataset which consists of a larger\nautomatically-constructed training set (17.5K queries) and a smaller\nexpert-annotated evaluation set (392 queries). Using this data, we compare\nvarious information retrieval algorithms on our test set and present the\nfirst-ever published system for text-based dataset recommendation using machine\nlearning techniques. This system, trained on the DataFinder Dataset, finds more\nrelevant search results than existing third-party dataset search engines. To\nencourage progress on dataset recommendation, we release our dataset and models\nto the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Viswanathan_V/0/1/0/all/0/1\">Vijay Viswanathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Luyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Multi-task Learning for End-to-end Metaphor Detection. (arXiv:2305.16638v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16638","description":"<p>Metaphor detection (MD) suffers from limited training data. In this paper, we\nstarted with a linguistic rule called Metaphor Identification Procedure and\nthen proposed a novel multi-task learning framework to transfer knowledge in\nbasic sense discrimination (BSD) to MD. BSD is constructed from word sense\ndisambiguation (WSD), which has copious amounts of data. We leverage\nadversarial training to align the data distributions of MD and BSD in the same\nfeature space, so task-invariant representations can be learned. To capture\nfine-grained alignment patterns, we utilize the multi-mode structures of MD and\nBSD. Our method is totally end-to-end and can mitigate the data scarcity\nproblem in MD. Competitive results are reported on four public datasets. Our\ncode and datasets are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shenglong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ying Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Fairy Tales Fair? Analyzing Gender Bias in Temporal Narrative Event Chains of Children's Fairy Tales. (arXiv:2305.16641v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16641","description":"<p>Social biases and stereotypes are embedded in our culture in part through\ntheir presence in our stories, as evidenced by the rich history of humanities\nand social science literature analyzing such biases in children stories.\nBecause these analyses are often conducted manually and at a small scale, such\ninvestigations can benefit from the use of more recent natural language\nprocessing methods that examine social bias in models and data corpora. Our\nwork joins this interdisciplinary effort and makes a unique contribution by\ntaking into account the event narrative structures when analyzing the social\nbias of stories. We propose a computational pipeline that automatically\nextracts a story's temporal narrative verb-based event chain for each of its\ncharacters as well as character attributes such as gender. We also present a\nverb-based event annotation scheme that can facilitate bias analysis by\nincluding categories such as those that align with traditional stereotypes.\nThrough a case study analyzing gender bias in fairy tales, we demonstrate that\nour framework can reveal bias in not only the unigram verb-based events in\nwhich female and male characters participate but also in the temporal narrative\norder of such event participation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Isaza_P/0/1/0/all/0/1\">Paulina Toro Isaza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangxuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oloko_A/0/1/0/all/0/1\">Akintoye Oloko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yufang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dakuo Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning. (arXiv:2305.16646v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16646","description":"<p>Large language models have shown astonishing performance on a wide range of\nreasoning tasks. In this paper, we investigate whether they could reason about\nreal-world events and help improve the prediction accuracy of event sequence\nmodels. We design a modeling and prediction framework where a large language\nmodel performs abductive reasoning to assist an event sequence model: the event\nmodel proposes predictions on future events given the past; instructed by a few\nexpert-annotated demonstrations, the language model learns to suggest possible\ncauses for each proposal; a search module finds out the previous events that\nmatch the causes; a scoring function learns to examine whether the retrieved\nevents could actually cause the proposal. Through extensive experiments on two\nchallenging real-world datasets (Amazon Review and GDELT), we demonstrate that\nour framework -- thanks to the reasoning ability of language models -- could\nsignificantly outperform the state-of-the-art event sequence models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaoming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_S/0/1/0/all/0/1\">Siqiao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kangrui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Fan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">James Y. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1\">Hongyuan Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dramatic Conversation Disentanglement. (arXiv:2305.16648v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16648","description":"<p>We present a new dataset for studying conversation disentanglement in movies\nand TV series. While previous work has focused on conversation disentanglement\nin IRC chatroom dialogues, movies and TV shows provide a space for studying\ncomplex pragmatic patterns of floor and topic change in face-to-face\nmulti-party interactions. In this work, we draw on theoretical research in\nsociolinguistics, sociology, and film studies to operationalize a\nconversational thread (including the notion of a floor change) in dramatic\ntexts, and use that definition to annotate a dataset of 10,033 dialogue turns\n(comprising 2,209 threads) from 831 movies. We compare the performance of\nseveral disentanglement models on this dramatic dataset, and apply the\nbest-performing model to disentangle 808 movies. We see that, contrary to\nexpectation, average thread lengths do not decrease significantly over the past\n40 years, and characters portrayed by actors who are women, while\nunderrepresented, initiate more new conversational threads relative to their\nspeaking time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kent K. Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danica Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamman_D/0/1/0/all/0/1\">David Bamman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TADA: Task-Agnostic Dialect Adapters for English. (arXiv:2305.16651v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16651","description":"<p>Large Language Models, the dominant starting point for Natural Language\nProcessing (NLP) applications, fail at a higher rate for speakers of English\ndialects other than Standard American English (SAE). Prior work addresses this\nusing task-specific data or synthetic data augmentation, both of which require\nintervention for each dialect and task pair. This poses a scalability issue\nthat prevents the broad adoption of robust dialectal English NLP. We introduce\na simple yet effective method for task-agnostic dialect adaptation by aligning\nnon-SAE dialects using adapters and composing them with task-specific adapters\nfrom SAE. Task-Agnostic Dialect Adapters (TADA) improve dialectal robustness on\n4 dialectal variants of the GLUE benchmark without task-specific supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Held_W/0/1/0/all/0/1\">Will Held</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziems_C/0/1/0/all/0/1\">Caleb Ziems</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaPlanner: Adaptive Planning from Feedback with Language Models. (arXiv:2305.16653v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16653","description":"<p>Large language models (LLMs) have recently demonstrated the potential in\nacting as autonomous agents for sequential decision-making tasks. However, most\nexisting methods either take actions greedily without planning or rely on\nstatic plans that are not adaptable to environmental feedback. Consequently,\nthe sequential decision-making performance of LLM agents degenerates with\nproblem complexity and plan horizons increase. We propose a closed-loop\napproach, AdaPlanner, which allows the LLM agent to refine its self-generated\nplan adaptively in response to environmental feedback. In AdaPlanner, the LLM\nagent adaptively refines its plan from feedback with both in-plan and\nout-of-plan refinement strategies. To mitigate hallucination, we develop a\ncode-style LLM prompt structure that facilitates plan generation across a\nvariety of tasks, environments, and agent capabilities. Furthermore, we propose\na skill discovery mechanism that leverages successful plans as few-shot\nexemplars, enabling the agent to plan and refine with fewer task\ndemonstrations. Our experiments in the ALFWorld and MiniWoB++ environments\ndemonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and\n4.11% while utilizing 2x and 600x fewer samples, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haotian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yuchen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingkai Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GDA: Generative Data Augmentation Techniques for Relation Extraction Tasks. (arXiv:2305.16663v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16663","description":"<p>Relation extraction (RE) tasks show promising performance in extracting\nrelations from two entities mentioned in sentences, given sufficient\nannotations available during training. Such annotations would be\nlabor-intensive to obtain in practice. Existing work adopts data augmentation\ntechniques to generate pseudo-annotated sentences beyond limited annotations.\nThese techniques neither preserve the semantic consistency of the original\nsentences when rule-based augmentations are adopted, nor preserve the syntax\nstructure of sentences when expressing relations using seq2seq models,\nresulting in less diverse augmentations. In this work, we propose a dedicated\naugmentation technique for relational texts, named GDA, which uses two\ncomplementary modules to preserve both semantic consistency and syntax\nstructures. We adopt a generative formulation and design a multi-tasking\nsolution to achieve synergies. Furthermore, GDA adopts entity hints as the\nprior knowledge of the generative model to augment diverse sentences.\nExperimental results in three datasets under a low-resource setting showed that\nGDA could bring {\\em 2.0\\%} F1 improvements compared with no augmentation\ntechnique. Source code and data are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zeqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Score-balanced Loss for Multi-aspect Pronunciation Assessment. (arXiv:2305.16664v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16664","description":"<p>With rapid technological growth, automatic pronunciation assessment has\ntransitioned toward systems that evaluate pronunciation in various aspects,\nsuch as fluency and stress. However, despite the highly imbalanced score labels\nwithin each aspect, existing studies have rarely tackled the data imbalance\nproblem. In this paper, we suggest a novel loss function, score-balanced loss,\nto address the problem caused by uneven data, such as bias toward the majority\nscores. As a re-weighting approach, we assign higher costs when the predicted\nscore is of the minority class, thus, guiding the model to gain positive\nfeedback for sparse score prediction. Specifically, we design two weighting\nfactors by leveraging the concept of an effective number of samples and using\nthe ranks of scores. We evaluate our method on the speechocean762 dataset,\nwhich has noticeably imbalanced scores for several aspects. Improved results\nparticularly on such uneven aspects prove the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Do_H/0/1/0/all/0/1\">Heejin Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yunsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gary Geunbae Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiview Identifiers Enhanced Generative Retrieval. (arXiv:2305.16675v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16675","description":"<p>Instead of simply matching a query to pre-existing passages, generative\nretrieval generates identifier strings of passages as the retrieval target. At\na cost, the identifier must be distinctive enough to represent a passage.\nCurrent approaches use either a numeric ID or a text piece (such as a title or\nsubstrings) as the identifier. However, these identifiers cannot cover a\npassage's content well. As such, we are motivated to propose a new type of\nidentifier, synthetic identifiers, that are generated based on the content of a\npassage and could integrate contextualized information that text pieces lack.\nFurthermore, we simultaneously consider multiview identifiers, including\nsynthetic identifiers, titles, and substrings. These views of identifiers\ncomplement each other and facilitate the holistic ranking of passages from\nmultiple perspectives. We conduct a series of experiments on three public\ndatasets, and the results indicate that our proposed approach performs the best\nin generative retrieval, demonstrating its effectiveness and robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1\">Nan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjie Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DKAF: KB Arbitration for Learning Task-Oriented Dialog Systems with Dialog-KB Inconsistencies. (arXiv:2305.16697v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16697","description":"<p>Task-oriented dialog (TOD) agents often ground their responses on external\nknowledge bases (KBs). These KBs can be dynamic and may be updated frequently.\nExisting approaches for learning TOD agents assume the KB snapshot contemporary\nto each individual dialog is available during training. However, in real-world\nscenarios, only the latest KB snapshot is available during training and as a\nresult, the train dialogs may contain facts conflicting with the latest KB.\nThese dialog-KB inconsistencies in the training data may potentially confuse\nthe TOD agent learning algorithm.\n</p>\n<p>In this work, we define the novel problem of learning a TOD agent with\ndialog-KB inconsistencies in the training data. We propose a Dialog-KB\nArbitration Framework (DKAF) which reduces the dialog-KB inconsistencies by\npredicting the contemporary KB snapshot for each train dialog. These predicted\nKB snapshots are then used for training downstream TOD agents. As there are no\nexisting datasets with dialog-KB inconsistencies, we systematically introduce\ninconsistencies in two publicly available dialog datasets. We show that TOD\nagents trained with DKAF perform better than existing baselines on both these\ndatasets\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saley_V/0/1/0/all/0/1\">Vishal Vivek Saley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1\">Rocktim Jyoti Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghu_D/0/1/0/all/0/1\">Dinesh Raghu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PIP: Parse-Instructed Prefix for Syntactically Controlled Paraphrase Generation. (arXiv:2305.16701v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16701","description":"<p>Syntactically controlled paraphrase generation requires language models to\ngenerate paraphrases for sentences according to specific syntactic structures.\nExisting fine-tuning methods for this task are costly as all the parameters of\nthe model need to be updated during the training process. Inspired by recent\nstudies on parameter-efficient learning, we propose Parse-Instructed Prefix\n(PIP), a novel adaptation of prefix-tuning to tune large pre-trained language\nmodels on syntactically controlled paraphrase generation task in a low-data\nsetting with significantly less training cost. We introduce two methods to\ninstruct a model's encoder prefix to capture syntax-related knowledge: direct\ninitiation (PIP-Direct) and indirect optimization (PIP-Indirect). In contrast\nto traditional fine-tuning methods for this task, PIP is a compute-efficient\nalternative with 10 times less learnable parameters. Compared to existing\nprefix-tuning methods, PIP excels at capturing syntax control information,\nachieving significantly higher performance at the same level of learnable\nparameter count.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yixin Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"People and Places of Historical Europe: Bootstrapping Annotation Pipeline and a New Corpus of Named Entities in Late Medieval Texts. (arXiv:2305.16718v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16718","description":"<p>Although pre-trained named entity recognition (NER) models are highly\naccurate on modern corpora, they underperform on historical texts due to\ndifferences in language OCR errors. In this work, we develop a new NER corpus\nof 3.6M sentences from late medieval charters written mainly in Czech, Latin,\nand German.\n</p>\n<p>We show that we can start with a list of known historical figures and\nlocations and an unannotated corpus of historical texts, and use information\nretrieval techniques to automatically bootstrap a NER-annotated corpus. Using\nour corpus, we train a NER model that achieves entity-level Precision of\n72.81-93.98% with 58.14-81.77% Recall on a manually-annotated test dataset.\nFurthermore, we show that using a weighted loss function helps to combat class\nimbalance in token classification tasks. To make it easy for others to\nreproduce and build upon our work, we publicly release our corpus, models, and\nexperimental code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Novotny_V/0/1/0/all/0/1\">V&#xed;t Novotn&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luger_K/0/1/0/all/0/1\">Krist&#xfd;na Luger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1\">Michal &#x160;tef&#xe1;nik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vrabcova_T/0/1/0/all/0/1\">Tereza Vrabcov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horak_A/0/1/0/all/0/1\">Ale&#x161; Hor&#xe1;k</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code-Switched Text Synthesis in Unseen Language Pairs. (arXiv:2305.16724v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16724","description":"<p>Existing efforts on text synthesis for code-switching mostly require training\non code-switched texts in the target language pairs, limiting the deployment of\nthe models to cases lacking code-switched data. In this work, we study the\nproblem of synthesizing code-switched texts for language pairs absent from the\ntraining data. We introduce GLOSS, a model built on top of a pre-trained\nmultilingual machine translation model (PMMTM) with an additional\ncode-switching module. This module, either an adapter or extra prefixes, learns\ncode-switching patterns from code-switched data during training, while the\nprimary component of GLOSS, i.e., the PMMTM, is frozen. The design of only\nadjusting the code-switching module prevents our model from overfitting to the\nconstrained training data for code-switching. Hence, GLOSS exhibits the ability\nto generalize and synthesize code-switched texts across a broader spectrum of\nlanguage pairs. Additionally, we develop a self-training algorithm on target\nlanguage pairs further to enhance the reliability of GLOSS. Automatic\nevaluations on four language pairs show that GLOSS achieves at least 55%\nrelative BLEU and METEOR scores improvements compared to strong baselines.\nHuman evaluations on two language pairs further validate the success of GLOSS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1\">I-Hung Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1\">Avik Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Shubham Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RankCSE: Unsupervised Sentence Representations Learning via Learning to Rank. (arXiv:2305.16726v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16726","description":"<p>Unsupervised sentence representation learning is one of the fundamental\nproblems in natural language processing with various downstream applications.\nRecently, contrastive learning has been widely adopted which derives\nhigh-quality sentence representations by pulling similar semantics closer and\npushing dissimilar ones away. However, these methods fail to capture the\nfine-grained ranking information among the sentences, where each sentence is\nonly treated as either positive or negative. In many real-world scenarios, one\nneeds to distinguish and rank the sentences based on their similarities to a\nquery sentence, e.g., very relevant, moderate relevant, less relevant,\nirrelevant, etc. In this paper, we propose a novel approach, RankCSE, for\nunsupervised sentence representation learning, which incorporates ranking\nconsistency and ranking distillation with contrastive learning into a unified\nframework. In particular, we learn semantically discriminative sentence\nrepresentations by simultaneously ensuring ranking consistency between two\nrepresentations with different dropout masks, and distilling listwise ranking\nknowledge from the teacher. An extensive set of experiments are conducted on\nboth semantic textual similarity (STS) and transfer (TR) tasks. Experimental\nresults demonstrate the superior performance of our approach over several\nstate-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiduan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiahao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xian_Y/0/1/0/all/0/1\">Yunsen Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion Experiencer Recognition as a Prerequisite for Experiencer-Specific Emotion Analysis. (arXiv:2305.16731v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16731","description":"<p>Emotion role labeling aims at extracting who is described in text to\nexperience an emotion, why, and towards whom. This is often a challenging\nmodelling task which might be overly sophisticated if the main question to\nanswer is who feels which emotion. Recently, Troiano et al. (2022) proposed a\ndata set that focuses on assigning emotion labels and appraisal labels to\nindividual entities in text and Wegge et al. (2022) presented the first\nmodelling experiments. Their experiencer-specific emotion prediction model has,\nhowever, only been evaluated on gold-annotated experiencers, due to the\nunavailability of an automatic experiencer detection approach. We fill this gap\nwith the first experiments to automatically detect emotion experiencers in text\nand, subsequently, assign them emotions. We show that experiencer detection in\ntext is a challenging task, with a precision of .82 and a recall of .56 (F1\n=.66). Consequently, the performance of the experiencer-specific emotion\ndetection pipeline drops with these predictions in comparison to using gold\nexperiencer annotations. This motivates future work of jointly modelling\nemotion experiencer detection and emotion/appraisal recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wegge_M/0/1/0/all/0/1\">Maximilian Wegge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AMPERE: AMR-Aware Prefix for Generation-Based Event Argument Extraction Model. (arXiv:2305.16734v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16734","description":"<p>Event argument extraction (EAE) identifies event arguments and their specific\nroles for a given event. Recent advancement in generation-based EAE models has\nshown great performance and generalizability over classification-based models.\nHowever, existing generation-based EAE models mostly focus on problem\nre-formulation and prompt design, without incorporating additional information\nthat has been shown to be effective for classification-based models, such as\nthe abstract meaning representation (AMR) of the input passages. Incorporating\nsuch information into generation-based models is challenging due to the\nheterogeneous nature of the natural language form prevalently used in\ngeneration-based models and the structured form of AMRs. In this work, we study\nstrategies to incorporate AMR into generation-based EAE models. We propose\nAMPERE, which generates AMR-aware prefixes for every layer of the generation\nmodel. Thus, the prefix introduces AMR information to the generation-based EAE\nmodel and then improves the generation. We also introduce an adjusted copy\nmechanism to AMPERE to help overcome potential noises brought by the AMR graph.\nComprehensive experiments and analyses on ACE2005 and ERE datasets show that\nAMPERE can get 4% - 10% absolute F1 score improvements with reduced training\ndata and it is in general powerful across different training sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1\">I-Hung Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhiyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Prem Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AlignScore: Evaluating Factual Consistency with a Unified Alignment Function. (arXiv:2305.16739v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16739","description":"<p>Many text generation applications require the generated text to be factually\nconsistent with input information. Automatic evaluation of factual consistency\nis challenging. Previous work has developed various metrics that often depend\non specific functions, such as natural language inference (NLI) or question\nanswering (QA), trained on limited data. Those metrics thus can hardly assess\ndiverse factual inconsistencies (e.g., contradictions, hallucinations) that\noccur in varying inputs/outputs (e.g., sentences, documents) from different\ntasks. In this paper, we propose AlignScore, a new holistic metric that applies\nto a variety of factual inconsistency scenarios as above. AlignScore is based\non a general function of information alignment between two arbitrary text\npieces. Crucially, we develop a unified training framework of the alignment\nfunction by integrating a large diversity of data sources, resulting in 4.7M\ntraining examples from 7 well-established tasks (NLI, QA, paraphrasing, fact\nverification, information retrieval, semantic similarity, and summarization).\nWe conduct extensive experiments on large-scale benchmarks including 22\nevaluation datasets, where 19 of the datasets were never seen in the alignment\ntraining. AlignScore achieves substantial improvement over a wide range of\nprevious metrics. Moreover, AlignScore (355M parameters) matches or even\noutperforms metrics based on ChatGPT and GPT-4 that are orders of magnitude\nlarger.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zha_Y/0/1/0/all/0/1\">Yuheng Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yichi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruichen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conjunct Resolution in the Face of Verbal Omissions. (arXiv:2305.16740v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16740","description":"<p>Verbal omissions are complex syntactic phenomena in VP coordination\nstructures. They occur when verbs and (some of) their arguments are omitted\nfrom subsequent clauses after being explicitly stated in an initial clause.\nRecovering these omitted elements is necessary for accurate interpretation of\nthe sentence, and while humans easily and intuitively fill in the missing\ninformation, state-of-the-art models continue to struggle with this task.\nPrevious work is limited to small-scale datasets, synthetic data creation\nmethods, and to resolution methods in the dependency-graph level. In this work\nwe propose a conjunct resolution task that operates directly on the text and\nmakes use of a split-and-rephrase paradigm in order to recover the missing\nelements in the coordination structure. To this end, we first formulate a\npragmatic framework of verbal omissions which describes the different types of\nomissions, and develop an automatic scalable collection method. Based on this\nmethod, we curate a large dataset, containing over 10K examples of\nnaturally-occurring verbal omissions with crowd-sourced annotations of the\nresolved conjuncts. We train various neural baselines for this task, and show\nthat while our best method obtains decent performance, it leaves ample space\nfor improvement. We propose our dataset, metrics and models as a starting point\nfor future research on this topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rassin_R/0/1/0/all/0/1\">Royi Rassin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Fine-Tuning without Introducing New Latency. (arXiv:2305.16742v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16742","description":"<p>Parameter-efficient fine-tuning (PEFT) of pre-trained language models has\nrecently demonstrated remarkable achievements, effectively matching the\nperformance of full fine-tuning while utilizing significantly fewer trainable\nparameters, and consequently addressing the storage and communication\nconstraints. Nonetheless, various PEFT methods are limited by their inherent\ncharacteristics. In the case of sparse fine-tuning, which involves modifying\nonly a small subset of the existing parameters, the selection of fine-tuned\nparameters is task- and domain-specific, making it unsuitable for federated\nlearning. On the other hand, PEFT methods with adding new parameters typically\nintroduce additional inference latency. In this paper, we demonstrate the\nfeasibility of generating a sparse mask in a task-agnostic manner, wherein all\ndownstream tasks share a common mask. Our approach, which relies solely on the\nmagnitude information of pre-trained parameters, surpasses existing\nmethodologies by a significant margin when evaluated on the GLUE benchmark.\nAdditionally, we introduce a novel adapter technique that directly applies the\nadapter to pre-trained parameters instead of the hidden representation, thereby\nachieving identical inference speed to that of full fine-tuning. Through\nextensive experiments, our proposed method attains a new state-of-the-art\noutcome in terms of both performance and storage efficiency, storing only 0.03%\nparameters of full fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1\">Baohao Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yan Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monz_C/0/1/0/all/0/1\">Christof Monz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automating the Analysis of Institutional Design in International Agreements. (arXiv:2305.16750v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16750","description":"<p>This paper explores the automatic knowledge extraction of formal\ninstitutional design - norms, rules, and actors - from international\nagreements. The focus was to analyze the relationship between the visibility\nand centrality of actors in the formal institutional design in regulating\ncritical aspects of cultural heritage relations. The developed tool utilizes\ntechniques such as collecting legal documents, annotating them with\nInstitutional Grammar, and using graph analysis to explore the formal\ninstitutional design. The system was tested against the 2003 UNESCO Convention\nfor the Safeguarding of the Intangible Cultural Heritage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wroblewska_A/0/1/0/all/0/1\">Anna Wr&#xf3;blewska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pielinski_B/0/1/0/all/0/1\">Bartosz Pieli&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seweryn_K/0/1/0/all/0/1\">Karolina Seweryn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sysko_Romanczuk_S/0/1/0/all/0/1\">Sylwia Sysko-Roma&#x144;czuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saputa_K/0/1/0/all/0/1\">Karol Saputa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wichrowska_A/0/1/0/all/0/1\">Aleksandra Wichrowska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schreiber_H/0/1/0/all/0/1\">Hanna Schreiber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can large language models generate salient negative statements?. (arXiv:2305.16755v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16755","description":"<p>We examine the ability of large language models (LLMs) to generate salient\n(interesting) negative statements about real-world entities; an emerging\nresearch topic of the last few years. We probe the LLMs using zero- and k-shot\nunconstrained probes, and compare with traditional methods for negation\ngeneration, i.e., pattern-based textual extractions and knowledge-graph-based\ninferences, as well as crowdsourced gold statements. We measure the correctness\nand salience of the generated lists about subjects from different domains. Our\nevaluation shows that guided probes do in fact improve the quality of generated\nnegatives, compared to the zero-shot variant. Nevertheless, using both prompts,\nLLMs still struggle with the notion of factuality of negatives, frequently\ngenerating many ambiguous statements, or statements with negative keywords but\na positive meaning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arnaout_H/0/1/0/all/0/1\">Hiba Arnaout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1\">Simon Razniewski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Domain Knowledge for Inclusive and Bias-aware Humanitarian Response Entry Classification. (arXiv:2305.16756v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16756","description":"<p>Accurate and rapid situation analysis during humanitarian crises is critical\nto delivering humanitarian aid efficiently and is fundamental to humanitarian\nimperatives and the Leave No One Behind (LNOB) principle. This data analysis\ncan highly benefit from language processing systems, e.g., by classifying the\ntext data according to a humanitarian ontology. However, approaching this by\nsimply fine-tuning a generic large language model (LLM) involves considerable\npractical and ethical issues, particularly the lack of effectiveness on\ndata-sparse and complex subdomains, and the encoding of societal biases and\nunwanted associations. In this work, we aim to provide an effective and\nethically-aware system for humanitarian data analysis. We approach this by (1)\nintroducing a novel architecture adjusted to the humanitarian analysis\nframework, (2) creating and releasing a novel humanitarian-specific LLM called\nHumBert, and (3) proposing a systematic way to measure and mitigate biases. Our\nexperiments' results show the better performance of our approach on zero-shot\nand full-training settings in comparison with strong baseline models, while\nalso revealing the existence of biases in the resulting LLMs. Utilizing a\ntargeted counterfactual data augmentation approach, we significantly reduce\nthese biases without compromising performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tamagnone_N/0/1/0/all/0/1\">Nicol&#xf2; Tamagnone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fekih_S/0/1/0/all/0/1\">Selim Fekih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Contla_X/0/1/0/all/0/1\">Ximena Contla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orozco_N/0/1/0/all/0/1\">Nayid Orozco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekabsaz_N/0/1/0/all/0/1\">Navid Rekabsaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Backpack Language Models. (arXiv:2305.16765v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16765","description":"<p>We present Backpacks: a new neural architecture that marries strong modeling\nperformance with an interface for interpretability and control. Backpacks learn\nmultiple non-contextual sense vectors for each word in a vocabulary, and\nrepresent a word in a sequence as a context-dependent, non-negative linear\ncombination of sense vectors in this sequence. We find that, after training,\nsense vectors specialize, each encoding a different aspect of a word. We can\ninterpret a sense vector by inspecting its (non-contextual, linear) projection\nonto the output space, and intervene on these interpretable hooks to change the\nmodel's behavior in predictable ways. We train a 170M-parameter Backpack\nlanguage model on OpenWebText, matching the loss of a GPT-2 small\n(124Mparameter) Transformer. On lexical similarity evaluations, we find that\nBackpack sense vectors outperform even a 6B-parameter Transformer LM's word\nembeddings. Finally, we present simple algorithms that intervene on sense\nvectors to perform controllable text generation and debiasing. For example, we\ncan edit the sense vocabulary to tend more towards a topic, or localize a\nsource of gender bias to a sense vector and globally suppress that sense.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hewitt_J/0/1/0/all/0/1\">John Hewitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thickstun_J/0/1/0/all/0/1\">John Thickstun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Common Understanding of Contributing Factors for Cross-Lingual Transfer in Multilingual Language Models: A Review. (arXiv:2305.16768v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16768","description":"<p>In recent years, pre-trained Multilingual Language Models (MLLMs) have shown\na strong ability to transfer knowledge across different languages. However,\ngiven that the aspiration for such an ability has not been explicitly\nincorporated in the design of the majority of MLLMs, it is challenging to\nobtain a unique and straightforward explanation for its emergence. In this\nreview paper, we survey literature that investigates different factors\ncontributing to the capacity of MLLMs to perform zero-shot cross-lingual\ntransfer and subsequently outline and discuss these factors in detail. To\nenhance the structure of this review and to facilitate consolidation with\nfuture studies, we identify five categories of such factors. In addition to\nproviding a summary of empirical evidence from past studies, we identify\nconsensuses among studies with consistent findings and resolve conflicts among\ncontradictory ones. Our work contextualizes and unifies existing research\nstreams which aim at explaining the cross-lingual potential of MLLMs. This\nreview provides, first, an aligned reference point for future research and,\nsecond, guidance for a better-informed and more efficient way of leveraging the\ncross-lingual capacity of MLLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Philippy_F/0/1/0/all/0/1\">Fred Philippy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Siwen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddadan_S/0/1/0/all/0/1\">Shohreh Haddadan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Distributions of Discourse Structure for Long Document Abstractive Summarization. (arXiv:2305.16784v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16784","description":"<p>For text summarization, the role of discourse structure is pivotal in\ndiscerning the core content of a text. Regrettably, prior studies on\nincorporating Rhetorical Structure Theory (RST) into transformer-based\nsummarization models only consider the nuclearity annotation, thereby\noverlooking the variety of discourse relation types. This paper introduces the\n'RSTformer', a novel summarization model that comprehensively incorporates both\nthe types and uncertainty of rhetorical relations. Our RST-attention mechanism,\nrooted in document-level rhetorical structure, is an extension of the recently\ndevised Longformer framework. Through rigorous evaluation, the model proposed\nherein exhibits significant superiority over state-of-the-art models, as\nevidenced by its notable performance on several automatic metrics and human\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pu_D/0/1/0/all/0/1\">Dongqi Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demberg_V/0/1/0/all/0/1\">Vera Demberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibration of Transformer-based Models for Identifying Stress and Depression in Social Media. (arXiv:2305.16797v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16797","description":"<p>In today's fast-paced world, the rates of stress and depression present a\nsurge. Social media provide assistance for the early detection of mental health\nconditions. Existing methods mainly introduce feature extraction approaches and\ntrain shallow machine learning classifiers. Other researches use deep neural\nnetworks or transformers. Despite the fact that transformer-based models\nachieve noticeable improvements, they cannot often capture rich factual\nknowledge. Although there have been proposed a number of studies aiming to\nenhance the pretrained transformer-based models with extra information or\nadditional modalities, no prior work has exploited these modifications for\ndetecting stress and depression through social media. In addition, although the\nreliability of a machine learning model's confidence in its predictions is\ncritical for high-risk applications, there is no prior work taken into\nconsideration the model calibration. To resolve the above issues, we present\nthe first study in the task of depression and stress detection in social media,\nwhich injects extra linguistic information in transformer-based models, namely\nBERT and MentalBERT. Specifically, the proposed approach employs a Multimodal\nAdaptation Gate for creating the combined embeddings, which are given as input\nto a BERT (or MentalBERT) model. For taking into account the model calibration,\nwe apply label smoothing. We test our proposed approaches in three publicly\navailable datasets and demonstrate that the integration of linguistic features\ninto transformer-based models presents a surge in the performance. Also, the\nusage of label smoothing contributes to both the improvement of the model's\nperformance and the calibration of the model. We finally perform a linguistic\nanalysis of the posts and show differences in language between stressful and\nnon-stressful texts, as well as depressive and non-depressive posts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ilias_L/0/1/0/all/0/1\">Loukas Ilias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouzakitis_S/0/1/0/all/0/1\">Spiros Mouzakitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askounis_D/0/1/0/all/0/1\">Dimitris Askounis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Schema-Guided User Satisfaction Modeling for Task-Oriented Dialogues. (arXiv:2305.16798v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16798","description":"<p>User Satisfaction Modeling (USM) is one of the popular choices for\ntask-oriented dialogue systems evaluation, where user satisfaction typically\ndepends on whether the user's task goals were fulfilled by the system.\nTask-oriented dialogue systems use task schema, which is a set of task\nattributes, to encode the user's task goals. Existing studies on USM neglect\nexplicitly modeling the user's task goals fulfillment using the task schema. In\nthis paper, we propose SG-USM, a novel schema-guided user satisfaction modeling\nframework. It explicitly models the degree to which the user's preferences\nregarding the task attributes are fulfilled by the system for predicting the\nuser's satisfaction level. SG-USM employs a pre-trained language model for\nencoding dialogue context and task attributes. Further, it employs a\nfulfillment representation layer for learning how many task attributes have\nbeen fulfilled in the dialogue, an importance predictor component for\ncalculating the importance of task attributes. Finally, it predicts the user\nsatisfaction based on task attribute fulfillment and task attribute importance.\nExperimental results on benchmark datasets (i.e. MWOZ, SGD, ReDial, and JDDC)\nshow that SG-USM consistently outperforms competitive existing methods. Our\nextensive analysis demonstrates that SG-USM can improve the interpretability of\nuser satisfaction modeling, has good scalability as it can effectively deal\nwith unseen tasks and can also effectively work in low-resource settings by\nleveraging unlabeled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yue Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yunlong Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1\">Animesh Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_E/0/1/0/all/0/1\">Emine Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazai_G/0/1/0/all/0/1\">Gabriella Kazai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To Revise or Not to Revise: Learning to Detect Improvable Claims for Argumentative Writing Support. (arXiv:2305.16799v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16799","description":"<p>Optimizing the phrasing of argumentative text is crucial in higher education\nand professional development. However, assessing whether and how the different\nclaims in a text should be revised is a hard task, especially for novice\nwriters. In this work, we explore the main challenges to identifying\nargumentative claims in need of specific revisions. By learning from\ncollaborative editing behaviors in online debates, we seek to capture implicit\nrevision patterns in order to develop approaches aimed at guiding writers in\nhow to further improve their arguments. We systematically compare the ability\nof common word embedding models to capture the differences between different\nversions of the same text, and we analyze their impact on various types of\nwriting issues. To deal with the noisy nature of revision-based corpora, we\npropose a new sampling strategy based on revision distance. Opposed to\napproaches from prior work, such sampling can be done without employing\nadditional annotations and judgments. Moreover, we provide evidence that using\ncontextual information and domain knowledge can further improve prediction\nresults. How useful a certain type of context is, depends on the issue the\nclaim is suffering from, though.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Skitalinskaya_G/0/1/0/all/0/1\">Gabriella Skitalinskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachsmuth_H/0/1/0/all/0/1\">Henning Wachsmuth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motion-Based Sign Language Video Summarization using Curvature and Torsion. (arXiv:2305.16801v1 [cs.CV])","link":"http://arxiv.org/abs/2305.16801","description":"<p>An interesting problem in many video-based applications is the generation of\nshort synopses by selecting the most informative frames, a procedure which is\nknown as video summarization. For sign language videos the benefits of using\nthe $t$-parameterized counterpart of the curvature of the 2-D signer's wrist\ntrajectory to identify keyframes, have been recently reported in the\nliterature. In this paper we extend these ideas by modeling the 3-D hand motion\nthat is extracted from each frame of the video. To this end we propose a new\ninformative function based on the $t$-parameterized curvature and torsion of\nthe 3-D trajectory. The method to characterize video frames as keyframes\ndepends on whether the motion occurs in 2-D or 3-D space. Specifically, in the\ncase of 3-D motion we look for the maxima of the harmonic mean of the curvature\nand torsion of the target's trajectory; in the planar motion case we seek for\nthe maxima of the trajectory's curvature. The proposed 3-D feature is\nexperimentally evaluated in applications of sign language videos on (1)\nobjective measures using ground-truth keyframe annotations, (2) human-based\nevaluation of understanding, and (3) gloss classification and the results\nobtained are promising.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sartinas_E/0/1/0/all/0/1\">Evangelos G. Sartinas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Psarakis_E/0/1/0/all/0/1\">Emmanouil Z. Psarakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosmopoulos_D/0/1/0/all/0/1\">Dimitrios I. Kosmopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do GPTs Produce Less Literal Translations?. (arXiv:2305.16806v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16806","description":"<p>Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose\nlanguage models capable of addressing many natural language generation or\nunderstanding tasks. On the task of Machine Translation (MT), multiple works\nhave investigated few-shot prompting mechanisms to elicit better translations\nfrom LLMs. However, there has been relatively little investigation on how such\ntranslations differ qualitatively from the translations generated by standard\nNeural Machine Translation (NMT) models. In this work, we investigate these\ndifferences in terms of the literalness of translations produced by the two\nsystems. Using literalness measures involving word alignment and monotonicity,\nwe find that translations out of English (E-X) from GPTs tend to be less\nliteral, while exhibiting similar or better scores on MT quality metrics. We\ndemonstrate that this finding is borne out in human evaluations as well. We\nthen show that these differences are especially pronounced when translating\nsentences that contain idiomatic expressions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raunak_V/0/1/0/all/0/1\">Vikas Raunak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menezes_A/0/1/0/all/0/1\">Arul Menezes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Post_M/0/1/0/all/0/1\">Matt Post</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_H/0/1/0/all/0/1\">Hany Hassan Awadallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GenQ: Automated Question Generation to Support Caregivers While Reading Stories with Children. (arXiv:2305.16809v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16809","description":"<p>When caregivers ask open--ended questions to motivate dialogue with children,\nit facilitates the child's reading comprehension skills.Although there is scope\nfor use of technological tools, referred here as \"intelligent tutoring\nsystems\", to scaffold this process, it is currently unclear whether existing\nintelligent systems that generate human--language like questions is beneficial.\nAdditionally, training data used in the development of these automated question\ngeneration systems is typically sourced without attention to demographics, but\npeople with different cultural backgrounds may ask different questions. As a\npart of a broader project to design an intelligent reading support app for\nLatinx children, we crowdsourced questions from Latinx caregivers and\nnoncaregivers as well as caregivers and noncaregivers from other demographics.\nWe examine variations in question--asking within this dataset mediated by\nindividual, cultural, and contextual factors. We then design a system that\nautomatically extracts templates from this data to generate open--ended\nquestions that are representative of those asked by Latinx caregivers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_A/0/1/0/all/0/1\">Arun Balajiee Lekshmi Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_L/0/1/0/all/0/1\">Ligia E. Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_M/0/1/0/all/0/1\">Martha Michelle Soto Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tri Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blais_C/0/1/0/all/0/1\">Chris Blais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Restrepo_M/0/1/0/all/0/1\">M. Adelaida Restrepo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glenberg_A/0/1/0/all/0/1\">Art Glenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Visual Story Generation with Adaptive Context Modeling. (arXiv:2305.16811v1 [cs.CV])","link":"http://arxiv.org/abs/2305.16811","description":"<p>Diffusion models developed on top of powerful text-to-image generation models\nlike Stable Diffusion achieve remarkable success in visual story generation.\nHowever, the best-performing approach considers historically generated results\nas flattened memory cells, ignoring the fact that not all preceding images\ncontribute equally to the generation of the characters and scenes at the\ncurrent stage. To address this, we present a simple method that improves the\nleading system with adaptive context modeling, which is not only incorporated\nin the encoder but also adopted as additional guidance in the sampling stage to\nboost the global consistency of the generated story. We evaluate our model on\nPororoSV and FlintstonesSV datasets and show that our approach achieves\nstate-of-the-art FID scores on both story visualization and continuation\nscenarios. We conduct detailed model analysis and show that our model excels at\ngenerating semantically consistent images for stories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhangyin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yuchen Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinmiao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiaocheng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Duyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Songs Across Borders: Singable and Controllable Neural Lyric Translation. (arXiv:2305.16816v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16816","description":"<p>The development of general-domain neural machine translation (NMT) methods\nhas advanced significantly in recent years, but the lack of naturalness and\nmusical constraints in the outputs makes them unable to produce singable lyric\ntranslations. This paper bridges the singability quality gap by formalizing\nlyric translation into a constrained translation problem, converting\ntheoretical guidance and practical techniques from translatology literature to\nprompt-driven NMT approaches, exploring better adaptation methods, and\ninstantiating them to an English-Chinese lyric translation system. Our model\nachieves 99.85%, 99.00%, and 95.52% on length accuracy, rhyme accuracy, and\nword boundary recall. In our subjective evaluation, our model shows a 75%\nrelative enhancement on overall quality, compared against naive fine-tuning\n(Code available at https://github.com/Sonata165/ControllableLyricTranslation).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ou_L/0/1/0/all/0/1\">Longshen Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xichu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Min-Yen Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ye Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"With a Little Push, NLI Models can Robustly and Efficiently Predict Faithfulness. (arXiv:2305.16819v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16819","description":"<p>Conditional language models still generate unfaithful output that is not\nsupported by their input. These unfaithful generations jeopardize trust in\nreal-world applications such as summarization or human-machine interaction,\nmotivating a need for automatic faithfulness metrics. To implement such\nmetrics, NLI models seem attractive, since they solve a strongly related task\nthat comes with a wealth of prior research and data. But recent research\nsuggests that NLI models require costly additional machinery to perform\nreliably across datasets, e.g., by running inference on a cartesian product of\ninput and generated sentences, or supporting them with a\nquestion-generation/answering step.\n</p>\n<p>In this work we show that pure NLI models _can_ outperform more complex\nmetrics when combining task-adaptive data augmentation with robust inference\nprocedures. We propose: (1) Augmenting NLI training data to adapt NL inferences\nto the specificities of faithfulness prediction in dialogue; (2) Making use of\nboth entailment and contradiction probabilities in NLI, and (3) Using\nMonte-Carlo dropout during inference. Applied to the TRUE benchmark, which\ncombines faithfulness datasets across diverse domains and tasks, our approach\nstrongly improves a vanilla NLI model and significantly outperforms previous\nwork, while showing favourable computational cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Steen_J/0/1/0/all/0/1\">Julius Steen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Opitz_J/0/1/0/all/0/1\">Juri Opitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1\">Anette Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markert_K/0/1/0/all/0/1\">Katja Markert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Aligned Prefix Averaging for Domain Generalization in Abstractive Summarization. (arXiv:2305.16820v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16820","description":"<p>Domain generalization is hitherto an underexplored area applied in\nabstractive summarization. Moreover, most existing works on domain\ngeneralization have sophisticated training algorithms. In this paper, we\npropose a lightweight, weight averaging based, Domain Aligned Prefix Averaging\napproach to domain generalization for abstractive summarization. Given a number\nof source domains, our method first trains a prefix for each one of them. These\nsource prefixes generate summaries for a small number of target domain\ndocuments. The similarity of the generated summaries to their corresponding\ndocuments is used for calculating weights required to average source prefixes.\nIn DAPA, prefix tuning allows for lightweight finetuning, and weight averaging\nallows for the computationally efficient addition of new source domains. When\nevaluated on four diverse summarization domains, DAPA shows comparable or\nbetter performance against the baselines, demonstrating the effectiveness of\nits prefix averaging scheme.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nair_P/0/1/0/all/0/1\">Pranav Ajit Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1\">Sukomal Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verm_P/0/1/0/all/0/1\">Pradeepika Verm</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt- and Trait Relation-aware Cross-prompt Essay Trait Scoring. (arXiv:2305.16826v1 [cs.CL])","link":"http://arxiv.org/abs/2305.16826","description":"<p>Automated essay scoring (AES) aims to score essays written for a given\nprompt, which defines the writing topic. Most existing AES systems assume to\ngrade essays of the same prompt as used in training and assign only a holistic\nscore. However, such settings conflict with real-education situations;\npre-graded essays for a particular prompt are lacking, and detailed trait\nscores of sub-rubrics are required. Thus, predicting various trait scores of\nunseen-prompt essays (called cross-prompt essay trait scoring) is a remaining\nchallenge of AES. In this paper, we propose a robust model: prompt- and trait\nrelation-aware cross-prompt essay trait scorer. We encode prompt-aware essay\nrepresentation by essay-prompt attention and utilizing the topic-coherence\nfeature extracted by the topic-modeling mechanism without access to labeled\ndata; therefore, our model considers the prompt adherence of an essay, even in\na cross-prompt setting. To facilitate multi-trait scoring, we design\ntrait-similarity loss that encapsulates the correlations of traits. Experiments\nprove the efficacy of our model, showing state-of-the-art results for all\nprompts and traits. Significant improvements in low-resource-prompt and\ninferior traits further indicate our model's strength.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Do_H/0/1/0/all/0/1\">Heejin Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yunsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gary Geunbae Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Attentive Residual Networks for Argument Mining. (arXiv:2102.12227v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.12227","description":"<p>We explore the use of residual networks and neural attention for multiple\nargument mining tasks. We propose a residual architecture that exploits\nattention, multi-task learning, and makes use of ensemble, without any\nassumption on document or argument structure. We present an extensive\nexperimental evaluation on five different corpora of user-generated comments,\nscientific publications, and persuasive essays. Our results show that our\napproach is a strong competitor against state-of-the-art architectures with a\nhigher computational footprint or corpus-specific design, representing an\ninteresting compromise between generality, performance accuracy and reduced\nmodel size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galassi_A/0/1/0/all/0/1\">Andrea Galassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippi_M/0/1/0/all/0/1\">Marco Lippi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torroni_P/0/1/0/all/0/1\">Paolo Torroni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fact-driven Logical Reasoning for Machine Reading Comprehension. (arXiv:2105.10334v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.10334","description":"<p>Recent years have witnessed an increasing interest in training machines with\nreasoning ability, which deeply relies on accurately and clearly presented clue\nforms. The clues are usually modeled as entity-aware knowledge in existing\nstudies. However, those entity-aware clues are primarily focused on\ncommonsense, making them insufficient for tasks that require knowledge of\ntemporary facts or events, particularly in logical reasoning for reading\ncomprehension. To address this challenge, we are motivated to cover both\ncommonsense and temporary knowledge clues hierarchically. Specifically, we\npropose a general formalism of knowledge units by extracting backbone\nconstituents of the sentence, such as the subject-verb-object formed ``facts''.\nWe then construct a supergraph on top of the fact units, allowing for the\nbenefit of sentence-level (relations among fact groups) and entity-level\ninteractions (concepts or actions inside a fact). Experimental results on\nlogical reasoning benchmarks and dialogue modeling datasets show that our\napproach improves the baselines substantially, and it is general across\nbackbone models. Code is available at\n\\url{https://github.com/ozyyshr/FocalReasoner}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1\">Siru Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrossSum: Beyond English-Centric Cross-Lingual Summarization for 1,500+ Language Pairs. (arXiv:2112.08804v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08804","description":"<p>We present CrossSum, a large-scale cross-lingual summarization dataset\ncomprising 1.68 million article-summary samples in 1,500+ language pairs. We\ncreate CrossSum by aligning parallel articles written in different languages\nvia cross-lingual retrieval from a multilingual abstractive summarization\ndataset and perform a controlled human evaluation to validate its quality. We\npropose a multistage data sampling algorithm to effectively train a\ncross-lingual summarization model capable of summarizing an article in any\ntarget language. We also introduce LaSE, an embedding-based metric for\nautomatically evaluating model-generated summaries. LaSE is strongly correlated\nwith ROUGE and, unlike ROUGE, can be reliably measured even in the absence of\nreferences in the target language. Performance on ROUGE and LaSE indicate that\nour proposed model consistently outperforms baseline models. To the best of our\nknowledge, CrossSum is the largest cross-lingual summarization dataset and the\nfirst ever that is not centered around English. We are releasing the dataset,\ntraining and evaluation scripts, and models to spur future research on\ncross-lingual summarization. The resources can be found at\nhttps://github.com/csebuetnlp/CrossSum\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1\">Abhik Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_T/0/1/0/all/0/1\">Tahmid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yong-Bin Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahriyar_R/0/1/0/all/0/1\">Rifat Shahriyar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SummaReranker: A Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization. (arXiv:2203.06569v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.06569","description":"<p>Sequence-to-sequence neural networks have recently achieved great success in\nabstractive summarization, especially through fine-tuning large pre-trained\nlanguage models on the downstream dataset. These models are typically decoded\nwith beam search to generate a unique summary. However, the search space is\nvery large, and with the exposure bias, such decoding is not optimal. In this\npaper, we show that it is possible to directly train a second-stage model\nperforming re-ranking on a set of summary candidates. Our mixture-of-experts\nSummaReranker learns to select a better candidate and consistently improves the\nperformance of the base model. With a base PEGASUS, we push ROUGE scores by\n5.44% on CNN-DailyMail (47.16 ROUGE-1), 1.31% on XSum (48.12 ROUGE-1) and 9.34%\non Reddit TIFU (29.83 ROUGE-1), reaching a new state-of-the-art. Our code and\ncheckpoints will be available at https://github.com/ntunlp/SummaReranker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravaut_M/0/1/0/all/0/1\">Mathieu Ravaut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are We Really Making Much Progress in Text Classification? A Comparative Review. (arXiv:2204.03954v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.03954","description":"<p>This study reviews and compares methods for single-label and multi-label text\nclassification, categorized into bag-of-words, sequence-based, graph-based, and\nhierarchical methods. The comparison aggregates results from the literature\nover five single-label and seven multi-label datasets and complements them with\nnew experiments. The findings reveal that all recently proposed graph-based and\nhierarchy-based methods fail to outperform pre-trained language models and\nsometimes perform worse than standard machine learning methods like a\nmultilayer perceptron on a bag-of-words. To assess the true scientific progress\nin text classification, future work should thoroughly test against strong\nbag-of-words baselines and state-of-the-art pre-trained language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galke_L/0/1/0/all/0/1\">Lukas Galke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diera_A/0/1/0/all/0/1\">Andor Diera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bao Xin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khera_B/0/1/0/all/0/1\">Bhakti Khera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuser_T/0/1/0/all/0/1\">Tim Meuser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_T/0/1/0/all/0/1\">Tushar Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karl_F/0/1/0/all/0/1\">Fabian Karl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1\">Ansgar Scherp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GENEVA: Benchmarking Generalizability for Event Argument Extraction with Hundreds of Event Types and Argument Roles. (arXiv:2205.12505v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12505","description":"<p>Recent works in Event Argument Extraction (EAE) have focused on improving\nmodel generalizability to cater to new events and domains. However, standard\nbenchmarking datasets like ACE and ERE cover less than 40 event types and 25\nentity-centric argument roles. Limited diversity and coverage hinder these\ndatasets from adequately evaluating the generalizability of EAE models. In this\npaper, we first contribute by creating a large and diverse EAE ontology. This\nontology is created by transforming FrameNet, a comprehensive semantic role\nlabeling (SRL) dataset for EAE, by exploiting the similarity between these two\ntasks. Then, exhaustive human expert annotations are collected to build the\nontology, concluding with 115 events and 220 argument roles, with a significant\nportion of roles not being entities. We utilize this ontology to further\nintroduce GENEVA, a diverse generalizability benchmarking dataset comprising\nfour test suites, aimed at evaluating models' ability to handle limited data\nand unseen event type generalization. We benchmark six EAE models from various\nfamilies. The results show that owing to non-entity argument roles, even the\nbest-performing model can only achieve 39% F1 score, indicating how GENEVA\nprovides new challenges for generalization in EAE. Overall, our large and\ndiverse EAE ontology can aid in creating more comprehensive future resources,\nwhile GENEVA is a challenging benchmarking dataset encouraging further research\nfor improving generalizability in EAE. The code and data can be found at\nhttps://github.com/PlusLabNLP/GENEVA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parekh_T/0/1/0/all/0/1\">Tanmay Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1\">I-Hung Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TAGPRIME: A Unified Framework for Relational Structure Extraction. (arXiv:2205.12585v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12585","description":"<p>Many tasks in natural language processing require the extraction of\nrelationship information for a given condition, such as event argument\nextraction, relation extraction, and task-oriented semantic parsing. Recent\nworks usually propose sophisticated models for each task independently and pay\nless attention to the commonality of these tasks and to have a unified\nframework for all the tasks. In this work, we propose to take a unified view of\nall these tasks and introduce TAGPRIME to address relational structure\nextraction problems. TAGPRIME is a sequence tagging model that appends priming\nwords about the information of the given condition (such as an event trigger)\nto the input text. With the self-attention mechanism in pre-trained language\nmodels, the priming words make the output contextualized representations\ncontain more information about the given condition, and hence become more\nsuitable for extracting specific relationships for the condition. Extensive\nexperiments and analyses on three different tasks that cover ten datasets\nacross five different languages demonstrate the generality and effectiveness of\nTAGPRIME.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1\">I-Hung Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wenxin Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Premkumar Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors. (arXiv:2205.12854v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12854","description":"<p>The propensity of abstractive summarization models to make factual errors has\nbeen studied extensively, including design of metrics to detect factual errors\nand annotation of errors in current systems' outputs. However, the\never-evolving nature of summarization systems, metrics, and annotated\nbenchmarks makes factuality evaluation a moving target, and drawing clear\ncomparisons among metrics has become increasingly difficult. In this work, we\naggregate factuality error annotations from nine existing datasets and stratify\nthem according to the underlying summarization model. We compare performance of\nstate-of-the-art factuality metrics, including recent ChatGPT-based metrics, on\nthis stratified benchmark and show that their performance varies significantly\nacross different types of summarization models. Critically, our analysis shows\nthat much of the recent improvement in the factuality detection space has been\non summaries from older (pre-Transformer) models instead of more relevant\nrecent summarization models. We further perform a finer-grained analysis per\nerror-type and find similar performance variance across error types for\ndifferent factuality metrics. Our results show that no one metric is superior\nin all settings or for all error types, and we provide recommendations for best\npractices given these insights.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Liyan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_T/0/1/0/all/0/1\">Tanya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1\">Alexander R. Fabbri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1\">Philippe Laban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiacheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1\">Semih Yavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryscinski_W/0/1/0/all/0/1\">Wojciech Kry&#x15b;ci&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rousseau_J/0/1/0/all/0/1\">Justin F. Rousseau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"B2T Connection: Serving Stability and Performance in Deep Transformers. (arXiv:2206.00330v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.00330","description":"<p>From the perspective of the layer normalization (LN) positions, the\narchitectures of Transformers can be categorized into two types: Post-LN and\nPre-LN. Recent Transformers tend to be Pre-LN because, in Post-LN with deep\nTransformers (e.g., those with ten or more layers), the training is often\nunstable, resulting in useless models. However, Post-LN has consistently\nachieved better performance than Pre-LN in relatively shallow Transformers\n(e.g., those with six or fewer layers). This study first investigates the\nreason for these discrepant observations empirically and theoretically and made\nthe following discoveries: 1, the LN in Post-LN is the main source of the\nvanishing gradient problem that leads to unstable training, whereas Pre-LN\nprevents it, and 2, Post-LN tends to preserve larger gradient norms in higher\nlayers during the back-propagation, which may lead to effective training.\nExploiting the new findings, we propose a method that can provide both high\nstability and effective training by a simple modification of Post-LN. We\nconduct experiments on a wide range of text generation tasks. The experimental\nresults demonstrate that our method outperforms Pre-LN, and enables stable\ntraining regardless of the shallow or deep layer settings. Our code is publicly\navailable at https://github.com/takase/b2t_connection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Takase_S/0/1/0/all/0/1\">Sho Takase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiyono_S/0/1/0/all/0/1\">Shun Kiyono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobayashi_S/0/1/0/all/0/1\">Sosuke Kobayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_J/0/1/0/all/0/1\">Jun Suzuki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hidden Schema Networks. (arXiv:2207.03777v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.03777","description":"<p>Large, pretrained language models infer powerful representations that encode\nrich semantic and syntactic content, albeit implicitly. In this work we\nintroduce a novel neural language model that enforces, via inductive biases,\nexplicit relational structures which allow for compositionality onto the output\nrepresentations of pretrained language models. Specifically, the model encodes\nsentences into sequences of symbols (composed representations), which\ncorrespond to the nodes visited by biased random walkers on a global latent\ngraph, and infers the posterior distribution of the latter. We first\ndemonstrate that the model is able to uncover ground-truth graphs from\nartificially generated datasets of random token sequences. Next, we leverage\npretrained BERT and GPT-2 language models as encoder and decoder, respectively,\nto infer networks of symbols (schemata) from natural language datasets. Our\nexperiments show that (i) the inferred symbols can be interpreted as encoding\ndifferent aspects of language, as e.g. topics or sentiments, and that (ii)\nGPT-like models can effectively be conditioned on symbolic representations.\nFinally, we explore training autoregressive, random walk ``reasoning\" models on\nschema networks inferred from commonsense knowledge databases, and using the\nsampled paths to enhance the performance of pretrained language models on\ncommonsense If-Then reasoning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_R/0/1/0/all/0/1\">Rams&#xe9;s J. S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conrads_L/0/1/0/all/0/1\">Lukas Conrads</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welke_P/0/1/0/all/0/1\">Pascal Welke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cvejoski_K/0/1/0/all/0/1\">Kostadin Cvejoski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ojeda_C/0/1/0/all/0/1\">C&#xe9;sar Ojeda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimedia Generative Script Learning for Task Planning. (arXiv:2208.12306v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.12306","description":"<p>Goal-oriented generative script learning aims to generate subsequent steps\nbased on a goal, which is an essential task to assist robots in performing\nstereotypical activities of daily life. We show that the performance of this\ntask can be improved if historical states are not just captured by the\nlinguistic instructions given to people, but are augmented with the additional\ninformation provided by accompanying images. Therefore, we propose a new task,\nMultimedia Generative Script Learning, to generate subsequent steps by tracking\nhistorical states in both text and vision modalities, as well as presenting the\nfirst benchmark containing 2,338 tasks and 31,496 steps with descriptive\nimages. We aim to generate scripts that are visual-state trackable, inductive\nfor unseen tasks, and diverse in their individual steps. We propose to encode\nvisual state changes through a multimedia selective encoder, transferring\nknowledge from previously observed tasks using a retrieval-augmented decoder,\nand presenting the distinct information at each step by optimizing a\ndiversity-oriented contrastive learning objective. We define metrics to\nevaluate both generation quality and inductive quality. Experiment results\ndemonstrate that our approach significantly outperforms strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingyun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Manling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1\">Hou Pong Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hockenmaier_J/0/1/0/all/0/1\">Julia Hockenmaier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhary_G/0/1/0/all/0/1\">Girish Chowdhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Environmental Claim Detection. (arXiv:2209.00507v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.00507","description":"<p>To transition to a green economy, environmental claims made by companies must\nbe reliable, comparable, and verifiable. To analyze such claims at scale,\nautomated methods are needed to detect them in the first place. However, there\nexist no datasets or models for this. Thus, this paper introduces the task of\nenvironmental claim detection. To accompany the task, we release an\nexpert-annotated dataset and models trained on this dataset. We preview one\npotential application of such models: We detect environmental claims made in\nquarterly earning calls and find that the number of environmental claims has\nsteadily increased since the Paris Agreement in 2015.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stammbach_D/0/1/0/all/0/1\">Dominik Stammbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webersinke_N/0/1/0/all/0/1\">Nicolas Webersinke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bingler_J/0/1/0/all/0/1\">Julia Anna Bingler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraus_M/0/1/0/all/0/1\">Mathias Kraus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leippold_M/0/1/0/all/0/1\">Markus Leippold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Aligned Simple German Corpus. (arXiv:2209.01106v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.01106","description":"<p>\"Leichte Sprache\", the German counterpart to Simple English, is a regulated\nlanguage aiming to facilitate complex written language that would otherwise\nstay inaccessible to different groups of people. We present a new\nsentence-aligned monolingual corpus for Simple German -- German. It contains\nmultiple document-aligned sources which we have aligned using automatic\nsentence-alignment methods. We evaluate our alignments based on a manually\nlabelled subset of aligned documents. The quality of our sentence alignments,\nas measured by F1-score, surpasses previous work. We publish the dataset under\nCC BY-SA and the accompanying code under MIT license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toborek_V/0/1/0/all/0/1\">Vanessa Toborek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_M/0/1/0/all/0/1\">Moritz Busch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bossert_M/0/1/0/all/0/1\">Malte Bo&#xdf;ert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauckhage_C/0/1/0/all/0/1\">Christian Bauckhage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welke_P/0/1/0/all/0/1\">Pascal Welke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Document-Level Event Argument Extraction. (arXiv:2209.02203v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.02203","description":"<p>Event argument extraction (EAE) has been well studied at the sentence level\nbut under-explored at the document level. In this paper, we study to capture\nevent arguments that actually spread across sentences in documents. Prior works\nusually assume full access to rich document supervision, ignoring the fact that\nthe available argument annotation is usually limited. To fill this gap, we\npresent FewDocAE, a Few-Shot Document-Level Event Argument Extraction\nbenchmark, based on the existing document-level event extraction dataset. We\nfirst define the new problem and reconstruct the corpus by a novel N -Way-D-Doc\nsampling instead of the traditional N -Way-K-Shot strategy. Then we adjust the\ncurrent document-level neural models into the few-shot setting to provide\nbaseline results under in- and cross-domain settings. Since the argument\nextraction depends on the context from multiple sentences and the learning\nprocess is limited to very few examples, we find this novel task to be very\nchallenging with substantively low performance. Considering FewDocAE is closely\nrelated to practical use under low-resource regimes, we hope this benchmark\nencourages more research in this direction. Our data and codes will be\navailable online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xianjun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petzold_L/0/1/0/all/0/1\">Linda Petzold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Downstream Datasets Make Surprisingly Good Pretraining Corpora. (arXiv:2209.14389v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.14389","description":"<p>For most natural language processing tasks, the dominant practice is to\nfinetune large pretrained transformer models (e.g., BERT) using smaller\ndownstream datasets. Despite the success of this approach, it remains unclear\nto what extent these gains are attributable to the massive background corpora\nemployed for pretraining versus to the pretraining objectives themselves. This\npaper introduces a large-scale study of self-pretraining, where the same\n(downstream) training data is used for both pretraining and finetuning. In\nexperiments addressing both ELECTRA and RoBERTa models and 10 distinct\ndownstream classification datasets, we observe that self-pretraining rivals\nstandard pretraining on the BookWiki corpus (despite using around\n$10\\times$--$500\\times$ less data), outperforming the latter on $7$ and $5$\ndatasets, respectively. Surprisingly, these task-specific pretrained models\noften perform well on other tasks, including the GLUE benchmark. Besides\nclassification tasks, self-pretraining also provides benefits on structured\noutput prediction tasks such as span based question answering and commonsense\ninference, often providing more than $50\\%$ of the performance boosts provided\nby pretraining on the BookWiki corpus. Our results hint that in many scenarios,\nperformance gains attributable to pretraining are driven primarily by the\npretraining objective itself and are not always attributable to the use of\nexternal pretraining data in massive amounts. These findings are especially\nrelevant in light of concerns about intellectual property and offensive content\nin web-scale pretraining data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">Kundan Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Saurabh Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bigham_J/0/1/0/all/0/1\">Jeffrey P. Bigham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A dynamic programming algorithm for span-based nested named-entity recognition in O(n^2). (arXiv:2210.04738v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04738","description":"<p>Span-based nested named-entity recognition (NER) has a cubic-time complexity\nusing a variant of the CYK algorithm. We show that by adding a supplementary\nstructural constraint on the search space, nested NER has a quadratic-time\ncomplexity, that is the same asymptotic complexity than the non-nested case.\nThe proposed algorithm covers a large part of three standard English benchmarks\nand delivers comparable experimental results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Corro_C/0/1/0/all/0/1\">Caio Corro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REV: Information-Theoretic Evaluation of Free-Text Rationales. (arXiv:2210.04982v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04982","description":"<p>Generating free-text rationales is a promising step towards explainable NLP,\nyet evaluating such rationales remains a challenge. Existing metrics have\nmostly focused on measuring the association between the rationale and a given\nlabel. We argue that an ideal metric should focus on the new information\nuniquely provided in the rationale that is otherwise not provided in the input\nor the label. We investigate this research problem from an\ninformation-theoretic perspective using conditional V-information (Hewitt et\nal., 2021). More concretely, we propose a metric called REV (Rationale\nEvaluation with conditional V-information), to quantify the amount of new,\nlabel-relevant information in a rationale beyond the information already\navailable in the input or the label. Experiments across four benchmarks with\nreasoning tasks, including chain-of-thought, demonstrate the effectiveness of\nREV in evaluating rationale-label pairs, compared to existing metrics. We\nfurther demonstrate REV is consistent with human judgments on rationale\nevaluations and provides more sensitive measurements of new information in\nfree-text rationales. When used alongside traditional performance metrics, REV\nprovides deeper insights into models' reasoning and prediction processes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Language Models Be Specific? How?. (arXiv:2210.05159v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.05159","description":"<p>\"He is a person\", \"Paris is located on the earth\". Both statements are\ncorrect but meaningless - due to lack of specificity. In this paper, we propose\nto measure how specific the language of pre-trained language models (PLMs) is.\nTo achieve this, we introduce a novel approach to build a benchmark for\nspecificity testing by forming masked token prediction tasks with prompts. For\ninstance, given \"Toronto is located in [MASK].\", we want to test whether a more\nspecific answer will be better filled in by PLMs, e.g., Ontario instead of\nCanada. From our evaluations, we show that existing PLMs have only a slight\npreference for more specific answers. We identify underlying factors affecting\nthe specificity and design two prompt-based methods to improve the specificity.\nResults show that the specificity of the models can be improved by the proposed\nmethods without additional training. We hope this work can bring to awareness\nthe notion of specificity of language models and encourage the research\ncommunity to further explore this important but understudied problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwu_W/0/1/0/all/0/1\">Wen-mei Hwu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Agnostic Multilingual Information Retrieval with Contrastive Learning. (arXiv:2210.06633v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2210.06633","description":"<p>Multilingual information retrieval (IR) is challenging since annotated\ntraining data is costly to obtain in many languages. We present an effective\nmethod to train multilingual IR systems when only English IR training data and\nsome parallel corpora between English and other languages are available. We\nleverage parallel and non-parallel corpora to improve the pretrained\nmultilingual language models' cross-lingual transfer ability. We design a\nsemantic contrastive loss to align representations of parallel sentences that\nshare the same semantics in different languages, and a new language contrastive\nloss to leverage parallel sentence pairs to remove language-specific\ninformation in sentence representations from non-parallel corpora. When trained\non English IR data with these losses and evaluated zero-shot on non-English\ndata, our model demonstrates significant improvement to prior work on retrieval\nperformance, while it requires much less computational effort. We also\ndemonstrate the value of our model for a practical setting when a parallel\ncorpus is only available for a few languages, but a lack of parallel corpora\nresources persists for many other low-resource languages. Our model can work\nwell even with a small number of parallel sentences, and be used as an add-on\nmodule to any backbones and other tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiyang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinchi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_P/0/1/0/all/0/1\">Peng Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Deguang Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kunlun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiheng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Can Have Your Data and Balance It Too: Towards Balanced and Efficient Multilingual Models. (arXiv:2210.07135v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07135","description":"<p>Multilingual models have been widely used for cross-lingual transfer to\nlow-resource languages. However, the performance on these languages is hindered\nby their underrepresentation in the pretraining data. To alleviate this\nproblem, we propose a novel multilingual training technique based on\nteacher-student knowledge distillation. In this setting, we utilize monolingual\nteacher models optimized for their language. We use those teachers along with\nbalanced (sub-sampled) data to distill the teachers' knowledge into a single\nmultilingual student. Our method outperforms standard training methods in\nlow-resource languages and retrains performance on high-resource languages\nwhile using the same amount of data. If applied widely, our approach can\nincrease the representation of low-resource languages in NLP systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Limisiewicz_T/0/1/0/all/0/1\">Tomasz Limisiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malkin_D/0/1/0/all/0/1\">Dan Malkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Creation of Named Entity Recognition Datasets by Querying Phrase Representations. (arXiv:2210.07586v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07586","description":"<p>Most weakly supervised named entity recognition (NER) models rely on\ndomain-specific dictionaries provided by experts. This approach is infeasible\nin many domains where dictionaries do not exist. While a phrase retrieval model\nwas used to construct pseudo-dictionaries with entities retrieved from\nWikipedia automatically in a recent study, these dictionaries often have\nlimited coverage because the retriever is likely to retrieve popular entities\nrather than rare ones. In this study, we present a novel framework, HighGEN,\nthat generates NER datasets with high-coverage pseudo-dictionaries.\nSpecifically, we create entity-rich dictionaries with a novel search method,\ncalled phrase embedding search, which encourages the retriever to search a\nspace densely populated with various entities. In addition, we use a new\nverification process based on the embedding distance between candidate entity\nmentions and entity types to reduce the false-positive noise in weak labels\ngenerated by high-coverage dictionaries. We demonstrate that HighGEN\noutperforms the previous best model by an average F1 score of 4.7 across five\nNER benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunjae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jaehyo Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Summary Candidates Fusion. (arXiv:2210.08779v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08779","description":"<p>Sequence-to-sequence deep neural models fine-tuned for abstractive\nsummarization can achieve great performance on datasets with enough human\nannotations. Yet, it has been shown that they have not reached their full\npotential, with a wide gap between the top beam search output and the oracle\nbeam. Recently, re-ranking methods have been proposed, to learn to select a\nbetter summary candidate. However, such methods are limited by the summary\nquality aspects captured by the first-stage candidates. To bypass this\nlimitation, we propose a new paradigm in second-stage abstractive summarization\ncalled SummaFusion that fuses several summary candidates to produce a novel\nabstractive second-stage summary. Our method works well on several\nsummarization datasets, improving both the ROUGE scores and qualitative\nproperties of fused summaries. It is especially good when the candidates to\nfuse are worse, such as in the few-shot setup where we set a new\nstate-of-the-art. We will make our code and checkpoints available at\nhttps://github.com/ntunlp/SummaFusion/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravaut_M/0/1/0/all/0/1\">Mathieu Ravaut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity-to-Text based Data Augmentation for various Named Entity Recognition Tasks. (arXiv:2210.10343v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10343","description":"<p>Data augmentation techniques have been used to alleviate the problem of\nscarce labeled data in various NER tasks (flat, nested, and discontinuous NER\ntasks). Existing augmentation techniques either manipulate the words in the\noriginal text that break the semantic coherence of the text, or exploit\ngenerative models that ignore preserving entities in the original text, which\nimpedes the use of augmentation techniques on nested and discontinuous NER\ntasks. In this work, we propose a novel Entity-to-Text based data augmentation\ntechnique named EnTDA to add, delete, replace or swap entities in the entity\nlist of the original texts, and adopt these augmented entity lists to generate\nsemantically coherent and entity preserving texts for various NER tasks.\nFurthermore, we introduce a diversity beam search to increase the diversity\nduring the text generation process. Experiments on thirteen NER datasets across\nthree tasks (flat, nested, and discontinuous NER tasks) and two settings (full\ndata and low resource settings) show that EnTDA could bring more performance\nimprovements compared to the baseline augmentation techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Experiencer-Specific Emotion and Appraisal Prediction. (arXiv:2210.12078v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.12078","description":"<p>Emotion classification in NLP assigns emotions to texts, such as sentences or\nparagraphs. With texts like \"I felt guilty when he cried\", focusing on the\nsentence level disregards the standpoint of each participant in the situation:\nthe writer (\"I\") and the other entity (\"he\") could in fact have different\naffective states. The emotions of different entities have been considered only\npartially in emotion semantic role labeling, a task that relates semantic roles\nto emotion cue words. Proposing a related task, we narrow the focus on the\nexperiencers of events, and assign an emotion (if any holds) to each of them.\nTo this end, we represent each emotion both categorically and with appraisal\nvariables, as a psychological access to explaining why a person develops a\nparticular emotion. On an event description corpus, our experiencer-aware\nmodels of emotions and appraisals outperform the experiencer-agnostic\nbaselines, showing that disregarding event participants is an\noversimplification for the emotion detection task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wegge_M/0/1/0/all/0/1\">Maximilian Wegge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troiano_E/0/1/0/all/0/1\">Enrica Troiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberlander_L/0/1/0/all/0/1\">Laura Oberl&#xe4;nder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JECC: Commonsense Reasoning Tasks Derived from Interactive Fictions. (arXiv:2210.15456v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.15456","description":"<p>Commonsense reasoning simulates the human ability to make presumptions about\nour physical world, and it is an essential cornerstone in building general AI\nsystems. We propose a new commonsense reasoning dataset based on human's\nInteractive Fiction (IF) gameplay walkthroughs as human players demonstrate\nplentiful and diverse commonsense reasoning. The new dataset provides a natural\nmixture of various reasoning types and requires multi-hop reasoning. Moreover,\nthe IF game-based construction procedure requires much less human interventions\nthan previous ones. Different from existing benchmarks, our dataset focuses on\nthe assessment of functional commonsense knowledge rules rather than factual\nknowledge. Hence, in order to achieve higher performance on our tasks, models\nneed to effectively utilize such functional knowledge to infer the outcomes of\nactions, rather than relying solely on memorizing facts. Experiments show that\nthe introduced dataset is challenging to previous machine reading models as\nwell as the new large language models with a significant 20% performance gap\ncompared to human experts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaoxiao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yufei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenspan_M/0/1/0/all/0/1\">Michael Greenspan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_M/0/1/0/all/0/1\">Murray Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy. (arXiv:2210.17546v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2210.17546","description":"<p>Studying data memorization in neural language models helps us understand the\nrisks (e.g., to privacy or copyright) associated with models regurgitating\ntraining data, and aids in the evaluation of potential countermeasures. Many\nprior works -- and some recently deployed defenses -- focus on \"verbatim\nmemorization\", defined as a model generation that exactly matches a substring\nfrom the training set. We argue that verbatim memorization definitions are too\nrestrictive and fail to capture more subtle forms of memorization.\nSpecifically, we design and implement an efficient defense based on Bloom\nfilters that perfectly prevents all verbatim memorization. And yet, we\ndemonstrate that this \"perfect\" filter does not prevent the leakage of training\ndata. Indeed, it is easily circumvented by plausible and minimally modified\n\"style-transfer\" prompts -- and in some cases even the non-modified original\nprompts -- to extract memorized information. For example, instructing the model\nto output ALL-CAPITAL texts bypasses memorization checks based on verbatim\nmatching. We conclude by discussing potential alternative definitions and why\ndefining memorization is a difficult yet crucial open question for neural\nlanguage models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1\">Florian Tram&#xe8;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasr_M/0/1/0/all/0/1\">Milad Nasr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagielski_M/0/1/0/all/0/1\">Matthew Jagielski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Katherine Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choquette_Choo_C/0/1/0/all/0/1\">Christopher A. Choquette-Choo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RQUGE: Reference-Free Metric for Evaluating Question Generation by Answering the Question. (arXiv:2211.01482v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.01482","description":"<p>Existing metrics for evaluating the quality of automatically generated\nquestions such as BLEU, ROUGE, BERTScore, and BLEURT compare the reference and\npredicted questions, providing a high score when there is a considerable\nlexical overlap or semantic similarity between the candidate and the reference\nquestions. This approach has two major shortcomings. First, we need expensive\nhuman-provided reference questions. Second, it penalises valid questions that\nmay not have high lexical or semantic similarity to the reference questions. In\nthis paper, we propose a new metric, RQUGE, based on the answerability of the\ncandidate question given the context. The metric consists of a\nquestion-answering and a span scorer modules, using pre-trained models from\nexisting literature, thus it can be used without any further training. We\ndemonstrate that RQUGE has a higher correlation with human judgment without\nrelying on the reference question. Additionally, RQUGE is shown to be more\nrobust to several adversarial corruptions. Furthermore, we illustrate that we\ncan significantly improve the performance of QA models on out-of-domain\ndatasets by fine-tuning on synthetic data generated by a question generation\nmodel and re-ranked by RQUGE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadshahi_A/0/1/0/all/0/1\">Alireza Mohammadshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazdani_M/0/1/0/all/0/1\">Majid Yazdani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yanki_P/0/1/0/all/0/1\">Pouya Yanki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1\">Angela Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeidi_M/0/1/0/all/0/1\">Marzieh Saeidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GreenPLM: Cross-Lingual Transfer of Monolingual Pre-Trained Language Models at Almost No Cost. (arXiv:2211.06993v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.06993","description":"<p>Large pre-trained models have revolutionized natural language processing\n(NLP) research and applications, but high training costs and limited data\nresources have prevented their benefits from being shared equally amongst\nspeakers of all the world's languages. To address issues of cross-linguistic\naccess to such models and reduce energy consumption for sustainability during\nlarge-scale model training, this study proposes an effective and\nenergy-efficient framework called GreenPLM that uses bilingual lexicons to\ndirectly \"translate\" pre-trained language models of one language into another\nat almost no additional cost. We validate this approach in 18 languages' BERT\nmodels and show that this framework is comparable to, if not better than, other\nheuristics with high training costs. In addition, given lightweight continued\npre-training on limited data where available, this framework outperforms the\noriginal monolingual language models in six out of seven tested languages with\nup to 200x less pre-training efforts. Aiming at the Leave No One Behind\nPrinciple (LNOB), our approach manages to reduce inequalities between languages\nand energy consumption greatly. We make our codes and models publicly available\nhere: \\url{https://github.com/qcznlp/GreenPLMs}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1\">Qingcheng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garay_L/0/1/0/all/0/1\">Lucas Garay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Peilin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_D/0/1/0/all/0/1\">Dading Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yining Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiageng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yikang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Han Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voigt_R/0/1/0/all/0/1\">Rob Voigt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Pronunciation Assessment with Multi-Aspect Attention. (arXiv:2211.08102v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08102","description":"<p>Automatic pronunciation assessment is a major component of a\ncomputer-assisted pronunciation training system. To provide in-depth feedback,\nscoring pronunciation at various levels of granularity such as phoneme, word,\nand utterance, with diverse aspects such as accuracy, fluency, and\ncompleteness, is essential. However, existing multi-aspect multi-granularity\nmethods simultaneously predict all aspects at all granularity levels;\ntherefore, they have difficulty in capturing the linguistic hierarchy of\nphoneme, word, and utterance. This limitation further leads to neglecting\nintimate cross-aspect relations at the same linguistic unit. In this paper, we\npropose a Hierarchical Pronunciation Assessment with Multi-aspect Attention\n(HiPAMA) model, which hierarchically represents the granularity levels to\ndirectly capture their linguistic structures and introduces multi-aspect\nattention that reflects associations across aspects at the same level to create\nmore connotative representations. By obtaining relational information from both\nthe granularity- and aspect-side, HiPAMA can take full advantage of multi-task\nlearning. Remarkable improvements in the experimental results on the\nspeachocean762 datasets demonstrate the robustness of HiPAMA, particularly in\nthe difficult-to-assess aspects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Do_H/0/1/0/all/0/1\">Heejin Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yunsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gary Geunbae Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations. (arXiv:2211.08794v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.08794","description":"<p>Due to the huge amount of parameters, fine-tuning of pretrained language\nmodels (PLMs) is prone to overfitting in the low resource scenarios. In this\nwork, we present a novel method that operates on the hidden representations of\na PLM to reduce overfitting. During fine-tuning, our method inserts random\nautoencoders between the hidden layers of a PLM, which transform activations\nfrom the previous layers into multi-view compressed representations before\nfeeding them into the upper layers. The autoencoders are plugged out after\nfine-tuning, so our method does not add extra parameters or increase\ncomputation cost during inference. Our method demonstrates promising\nperformance improvement across a wide range of sequence- and token-level\nlow-resource NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_M/0/1/0/all/0/1\">Megh Thakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SongRewriter: A Chinese Song Rewriting System with Controllable Content and Rhyme Scheme. (arXiv:2211.15037v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.15037","description":"<p>Although lyrics generation has achieved significant progress in recent years,\nit has limited practical applications because the generated lyrics cannot be\nperformed without composing compatible melodies. In this work, we bridge this\npractical gap by proposing a song rewriting system which rewrites the lyrics of\nan existing song such that the generated lyrics are compatible with the rhythm\nof the existing melody and thus singable. In particular, we propose\nSongRewriter,a controllable Chinese lyrics generation and editing system which\nassists users without prior knowledge of melody composition. The system is\ntrained by a randomized multi-level masking strategy which produces a unified\nmodel for generating entirely new lyrics or editing a few fragments. To improve\nthe controllabiliy of the generation process, we further incorporate a keyword\nprompt to control the lexical choices of the content and propose novel decoding\nconstraints and a vowel modeling task to enable flexible end and internal rhyme\nschemes. While prior rhyming metrics are mainly for rap lyrics, we propose\nthree novel rhyming evaluation metrics for song lyrics. Both automatic and\nhuman evaluations show that the proposed model performs better than the\nstate-of-the-art models in both contents and rhyming quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yusen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangyou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1\">Dit-Yan Yeung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Novelty-Augmented Learning: Anticipating Outliers with Large Language Models. (arXiv:2211.15718v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.15718","description":"<p>In many task settings, text classification models are likely to encounter\nexamples from novel classes on which they cannot predict correctly. Selective\nprediction, in which models abstain on low-confidence examples, provides a\npossible solution, but existing models are often overly confident on unseen\nclasses. To remedy this overconfidence, we introduce Contrastive\nNovelty-Augmented Learning (CoNAL), a two-step method that generates OOD\nexamples representative of novel classes, then trains to decrease confidence on\nthem. First, we generate OOD examples by prompting a large language model\ntwice: we prompt it to enumerate relevant novel classes, then generate examples\nfrom each novel class matching the task format. Second, we train a classifier\nwith a novel contrastive objective that encourages lower confidence on\ngenerated OOD examples than training examples. When trained with CoNAL,\nclassifiers improve in their ability to detect and abstain on novel class\nexamples over prior methods by an average of 2.3% in terms of accuracy under\nthe accuracy-coverage curve (AUAC) and 5.5% AUROC across 4 NLP datasets, with\nno cost to in-distribution accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_A/0/1/0/all/0/1\">Albert Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Soft Alignment Objectives for Robust Adaptation of Language Generation. (arXiv:2211.16550v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.16550","description":"<p>Domain adaptation allows generative language models to address specific flaws\ncaused by the domain shift of their application. However, the traditional\nadaptation by further training on in-domain data rapidly weakens the model's\nability to generalize to other domains, making the open-ended deployments of\nthe adapted models prone to errors. This work introduces novel training\nobjectives built upon a semantic similarity of the predicted tokens to the\nreference.\n</p>\n<p>Our results show that (1) avoiding the common assumption of a single correct\nprediction by constructing the training target from tokens' semantic similarity\ncan mitigate catastrophic forgetting during domain adaptation, while (2)\npreserving the quality of the adaptation, (3) with negligible additions to\ncompute costs.\n</p>\n<p>In the broader context, the objectives grounded in a continuous token\nsimilarity pioneer the exploration of the middle ground between the efficient\nbut na\\\"{\\i}ve exact-match token-level objectives and expressive but\ncomputationally- and resource-intensive sequential objectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1\">Michal &#x160;tef&#xe1;nik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadlcik_M/0/1/0/all/0/1\">Marek Kadl&#x10d;&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1\">Petr Sojka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning. (arXiv:2212.01117v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.01117","description":"<p>The spread of rumors along with breaking events seriously hinders the truth\nin the era of social media. Previous studies reveal that due to the lack of\nannotated resources, rumors presented in minority languages are hard to be\ndetected. Furthermore, the unforeseen breaking events not involved in\nyesterday's news exacerbate the scarcity of data resources. In this work, we\npropose a novel zero-shot framework based on prompt learning to detect rumors\nfalling in different domains or presented in different languages. More\nspecifically, we firstly represent rumor circulated on social media as diverse\npropagation threads, then design a hierarchical prompt encoding mechanism to\nlearn language-agnostic contextual representations for both prompts and rumor\ndata. To further enhance domain adaptation, we model the domain-invariant\nstructural features from the propagation threads, to incorporate structural\nposition representations of influential community response. In addition, a new\nvirtual response augmentation method is used to improve model training.\nExtensive experiments conducted on three real-world datasets demonstrate that\nour proposed model achieves much better performance than state-of-the-art\nmethods and exhibits a superior capacity for detecting rumors at early stages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongzhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_P/0/1/0/all/0/1\">Pengyao Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Ziyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruifang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nonparametric Masked Language Modeling. (arXiv:2212.01349v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.01349","description":"<p>Existing language models (LMs) predict tokens with a softmax over a finite\nvocabulary, which can make it difficult to predict rare tokens or phrases. We\nintroduce NPM, the first nonparametric masked language model that replaces this\nsoftmax with a nonparametric distribution over every phrase in a reference\ncorpus. NPM fills in the [MASK] solely from retrieving a token from a text\ncorpus. We show that NPM can be efficiently trained with a contrastive\nobjective and an in-batch approximation to full corpus retrieval. Zero-shot\nevaluation on 16 tasks including classification, fact probing and question\nanswering demonstrates that NPM outperforms significantly larger parametric\nmodels, either with or without a retrieve-and-generate approach. It is\nparticularly better at dealing with rare patterns (word senses or facts) and\npredicting rare or nearly unseen words (e.g., non-Latin script). We release the\nmodel and code at github.com/facebookresearch/NPM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weijia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-trained Language Models Can be Fully Zero-Shot Learners. (arXiv:2212.06950v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.06950","description":"<p>How can we extend a pre-trained model to many language understanding tasks,\nwithout labeled or additional unlabeled data? Pre-trained language models\n(PLMs) have been effective for a wide range of NLP tasks. However, existing\napproaches either require fine-tuning on downstream labeled datasets or\nmanually constructing proper prompts. In this paper, we propose nonparametric\nprompting PLM (NPPrompt) for fully zero-shot language understanding. Unlike\nprevious methods, NPPrompt uses only pre-trained language models and does not\nrequire any labeled data or additional raw corpus for further fine-tuning, nor\ndoes it rely on humans to construct a comprehensive set of prompt label words.\nWe evaluate NPPrompt against previous major few-shot and zero-shot learning\nmethods on diverse NLP tasks: including text classification, text entailment,\nsimilar text retrieval, and paraphrasing. Experimental results demonstrate that\nour NPPrompt outperforms the previous best fully zero-shot method by big\nmargins, with absolute gains of 12.8% in accuracy on text classification and\n18.9% on the GLUE benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xuandong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1\">Siqi Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiguo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Ming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visually-augmented pretrained language models for NLP tasks without images. (arXiv:2212.07937v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.07937","description":"<p>Although pre-trained language models~(PLMs) have shown impressive performance\nby text-only self-supervised training, they are found lack of visual semantics\nor commonsense. Existing solutions often rely on explicit images for visual\nknowledge augmentation (requiring time-consuming retrieval or generation), and\nthey also conduct the augmentation for the whole input text, without\nconsidering whether it is actually needed in specific inputs or tasks. To\naddress these issues, we propose a novel \\textbf{V}isually-\\textbf{A}ugmented\nfine-tuning approach that can be generally applied to various PLMs or NLP\ntasks, \\textbf{W}ithout using any retrieved or generated \\textbf{I}mages,\nnamely \\textbf{VAWI}. Experimental results show that our approach can\nconsistently improve the performance of BERT, RoBERTa, BART, and T5 at\ndifferent scales, and outperform several competitive baselines on ten tasks.\nOur codes and data are publicly available\nat~\\url{https://github.com/RUCAIBox/VAWI}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hangyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-VALUE: A Framework for Cross-Dialectal English NLP. (arXiv:2212.08011v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08011","description":"<p>Dialect differences caused by regional, social, and economic factors cause\nperformance discrepancies for many groups of language technology users.\nInclusive and equitable language technology must critically be dialect\ninvariant, meaning that performance remains constant over dialectal shifts.\nCurrent systems often fall short of this ideal since they are designed and\ntested on a single dialect: Standard American English (SAE). We introduce a\nsuite of resources for evaluating and achieving English dialect invariance. The\nresource is called Multi-VALUE, a controllable rule-based translation system\nspanning 50 English dialects and 189 unique linguistic features. Multi-VALUE\nmaps SAE to synthetic forms of each dialect. First, we use this system to\nstress tests question answering, machine translation, and semantic parsing.\nStress tests reveal significant performance disparities for leading models on\nnon-standard dialects. Second, we use this system as a data augmentation\ntechnique to improve the dialect robustness of existing systems. Finally, we\npartner with native speakers of Chicano and Indian English to release new\ngold-standard variants of the popular CoQA task. To execute the transformation\ncode, run model checkpoints, and download both synthetic and gold-standard\ndialectal benchmark datasets, see <a href=\"http://value-nlp.org.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ziems_C/0/1/0/all/0/1\">Caleb Ziems</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Held_W/0/1/0/all/0/1\">William Held</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingfeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue. (arXiv:2212.08054v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08054","description":"<p>Modern virtual assistants use internal semantic parsing engines to convert\nuser utterances to actionable commands. However, prior work has demonstrated\nthat semantic parsing is a difficult multilingual transfer task with low\ntransfer efficiency compared to other tasks. In global markets such as India\nand Latin America, this is a critical issue as switching between languages is\nprevalent for bilingual users. In this work we dramatically improve the\nzero-shot performance of a multilingual and codeswitched semantic parsing\nsystem using two stages of multilingual alignment. First, we show that\nconstrastive alignment pretraining improves both English performance and\ntransfer efficiency. We then introduce a constrained optimization approach for\nhyperparameter-free adversarial alignment during finetuning. Our Doubly Aligned\nMultilingual Parser (DAMP) improves mBERT transfer performance by 3x, 6x, and\n81x on the Spanglish, Hinglish and Multilingual Task Oriented Parsing\nbenchmarks respectively and outperforms XLM-R and mT5-Large using 3.2x fewer\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Held_W/0/1/0/all/0/1\">William Held</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hidey_C/0/1/0/all/0/1\">Christopher Hidey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1\">Eric Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_R/0/1/0/all/0/1\">Rahul Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rushin Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units. (arXiv:2212.08055v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08055","description":"<p>Direct speech-to-speech translation (S2ST), in which all components can be\noptimized jointly, is advantageous over cascaded approaches to achieve fast\ninference with a simplified pipeline. We present a novel two-pass direct S2ST\narchitecture, UnitY, which first generates textual representations and predicts\ndiscrete acoustic units subsequently. We enhance the model performance by\nsubword prediction in the first-pass decoder, advanced two-pass decoder\narchitecture design and search strategy, and better training regularization. To\nleverage large amounts of unlabeled text data, we pre-train the first-pass text\ndecoder based on the self-supervised denoising auto-encoding task. Experimental\nevaluations on benchmark datasets at various data scales demonstrate that UnitY\noutperforms a single-pass speech-to-unit translation model by 2.5-4.2 ASR-BLEU\nwith 2.83x decoding speed-up. We show that the proposed methods boost the\nperformance even when predicting spectrogram in the second pass. However,\npredicting discrete units achieves 2.51x decoding speed-up compared to that\ncase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popuri_S/0/1/0/all/0/1\">Sravya Popuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulikov_I/0/1/0/all/0/1\">Ilia Kulikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng-Jen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1\">Yu-An Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rarely a problem? Language models exhibit inverse scaling in their predictions following few-type quantifiers. (arXiv:2212.08700v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.08700","description":"<p>How well do language models deal with quantification? In this study, we focus\non 'few'-type quantifiers, as in 'few children like toys', which might pose a\nparticular challenge for language models because the sentence components with\nout the quantifier are likely to co-occur, and 'few'-type quantifiers are rare.\nWe present 960 English sentence stimuli from two human neurolinguistic\nexperiments to 22 autoregressive transformer models of differing sizes. Not\nonly do all the models perform poorly on 'few'-type quantifiers, but overall\nthe larger the model, the worse its performance. This inverse scaling is\nconsistent with previous work suggesting that larger models increasingly\nreflect online rather than offline human processing, and we argue that the\ndecreasing performance of larger models may challenge uses of language models\nas the basis for natural language systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michaelov_J/0/1/0/all/0/1\">James A. Michaelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1\">Benjamin K. Bergen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems. (arXiv:2212.09180v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09180","description":"<p>There has been great recent advancement in human-computer chat. However,\nproper evaluation currently requires human judgements that produce notoriously\nhigh-variance metrics due to their inherent subjectivity. Furthermore, there is\nlittle standardization in the methods and labels used for evaluation, with an\noverall lack of work to compare and assess the validity of various evaluation\napproaches. As a consequence, existing evaluation results likely leave an\nincomplete picture of the strengths and weaknesses of open-domain chatbots. We\naim towards a dimensional evaluation of human-computer chat that can reliably\nmeasure several distinct aspects of chat quality. To this end, we present our\nnovel human evaluation method that quantifies the rate of several\nquality-related chatbot behaviors. Our results demonstrate our method to be\nmore suitable for dimensional chat evaluation than alternative likert-style or\ncomparative methods. We then use our validated method and existing methods to\nevaluate four open-domain chat models from the recent literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Finch_S/0/1/0/all/0/1\">Sarah E. Finch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finch_J/0/1/0/all/0/1\">James D. Finch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OASum: Large-Scale Open Domain Aspect-based Summarization. (arXiv:2212.09233v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09233","description":"<p>Aspect or query-based summarization has recently caught more attention, as it\ncan generate differentiated summaries based on users' interests. However, the\ncurrent dataset for aspect or query-based summarization either focuses on\nspecific domains, contains relatively small-scale instances, or includes only a\nfew aspect types. Such limitations hinder further explorations in this\ndirection. In this work, we take advantage of crowd-sourcing knowledge on\nWikipedia.org and automatically create a high-quality, large-scale open-domain\naspect-based summarization dataset named OASum, which contains more than 3.7\nmillion instances with around 1 million different aspects on 2 million\nWikipedia pages. We provide benchmark results on OASum and demonstrate its\nability for diverse aspect-based summarization generation. To overcome the data\nscarcity problem on specific domains, we also perform zero-shot, few-shot, and\nfine-tuning on seven downstream datasets. Specifically, zero/few-shot and\nfine-tuning results show that the model pre-trained on our corpus demonstrates\na strong aspect or query-focused generation ability compared with the backbone\nmodel. Our dataset and pre-trained checkpoints are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xianjun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaiqiang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sangwoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiaoman Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petzold_L/0/1/0/all/0/1\">Linda Petzold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation. (arXiv:2212.09246v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09246","description":"<p>Commonsense capabilities of pre-trained language models dramatically improve\nwith scale, leading many to believe that scale is the only winning recipe. But\nis it? Here, we investigate an alternative that a priori seems impossible: can\nsmaller language models (e.g., GPT-2) win over models that are orders of\nmagnitude larger and better (e.g., GPT-3), if powered with novel commonsense\ndistillation algorithms? The key intellectual challenge is to design a learning\nalgorithm that achieve a competitive level of commonsense acquisition, without\nrelying on the benefits of scale. In particular, we study generative models of\ncommonsense knowledge, focusing on the task of generating generics, statements\nof commonsense facts about everyday concepts, e.g., birds can fly.\n</p>\n<p>We introduce I2D2, a novel commonsense distillation framework that loosely\nfollows the Symbolic Knowledge Distillation of West et al. but breaks the\ndependence on the extreme-scale teacher model with two innovations: (1) the\nnovel adaptation of NeuroLogic Decoding to enhance the generation quality of\nthe weak, off-the-shelf language models, and (2) self-imitation learning to\niteratively learn from the model's own enhanced commonsense acquisition\ncapabilities. Empirical results suggest that scale is not the only way, as\nnovel algorithms can be a promising alternative. Moreover, our study leads to a\nnew corpus of generics, Gen-A-tomic, that is the largest and highest quality\navailable to date.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Lianhui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaguchi_K/0/1/0/all/0/1\">Keisuke Sakaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query Enhanced Knowledge-Intensive Conversation via Unsupervised Joint Modeling. (arXiv:2212.09588v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09588","description":"<p>In this paper, we propose an unsupervised query enhanced approach for\nknowledge-intensive conversations, namely QKConv. There are three modules in\nQKConv: a query generator, an off-the-shelf knowledge selector, and a response\ngenerator. QKConv is optimized through joint training, which produces the\nresponse by exploring multiple candidate queries and leveraging corresponding\nselected knowledge. The joint training solely relies on the dialogue context\nand target response, getting exempt from extra query annotations or knowledge\nprovenances. To evaluate the effectiveness of the proposed QKConv, we conduct\nexperiments on three representative knowledge-intensive conversation datasets:\nconversational question-answering, task-oriented dialogue, and\nknowledge-grounded conversation. Experimental results reveal that QKConv\nperforms better than all unsupervised methods across three datasets and\nachieves competitive performance compared to supervised methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Mingzhu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1\">Siqi Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Huang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Summarization Re-ranking. (arXiv:2212.09593v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09593","description":"<p>With the rise of task-specific pre-training objectives, abstractive\nsummarization models like PEGASUS offer appealing zero-shot performance on\ndownstream summarization tasks. However, the performance of such unsupervised\nmodels still lags significantly behind their supervised counterparts. Similarly\nto the supervised setup, we notice a very high variance in quality among\nsummary candidates from these models while only one candidate is kept as the\nsummary output. In this paper, we propose to re-rank summary candidates in an\nunsupervised manner, aiming to close the performance gap between unsupervised\nand supervised models. Our approach improves the unsupervised PEGASUS by up to\n7.27% and ChatGPT by up to 6.86% relative mean ROUGE across four widely-adopted\nsummarization benchmarks ; and achieves relative gains of 7.51% (up to 23.73%\nfrom XSum to WikiHow) averaged over 30 zero-shot transfer setups (finetuning on\na dataset, evaluating on another).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravaut_M/0/1/0/all/0/1\">Mathieu Ravaut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Python Code Generation by Asking Clarification Questions. (arXiv:2212.09885v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09885","description":"<p>Code generation from text requires understanding the user's intent from a\nnatural language description and generating an executable code snippet that\nsatisfies this intent. While recent pretrained language models demonstrate\nremarkable performance for this task, these models fail when the given natural\nlanguage description is under-specified. In this work, we introduce a novel and\nmore realistic setup for this task. We hypothesize that the under-specification\nof a natural language description can be resolved by asking clarification\nquestions. Therefore, we collect and introduce a new dataset named CodeClarQA\ncontaining pairs of natural language descriptions and code with created\nsynthetic clarification questions and answers. The empirical results of our\nevaluation of pretrained language model performance on code generation show\nthat clarifications result in more precisely generated code, as shown by the\nsubstantial improvement of model performance in all evaluation metrics.\nAlongside this, our task and dataset introduce new challenges to the community,\nincluding when and what clarification questions should be asked. Our code and\ndataset are available on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haau-Sing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mesgar_M/0/1/0/all/0/1\">Mohsen Mesgar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DIONYSUS: A Pre-trained Model for Low-Resource Dialogue Summarization. (arXiv:2212.10018v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10018","description":"<p>Dialogue summarization has recently garnered significant attention due to its\nwide range of applications. However, existing methods for summarizing dialogues\nhave limitations because they do not take into account the inherent structure\nof dialogue and rely heavily on labeled data, which can lead to poor\nperformance in new domains. In this work, we propose DIONYSUS (dynamic input\noptimization in pre-training for dialogue summarization), a pre-trained\nencoder-decoder model for summarizing dialogues in any new domain. To pre-train\nDIONYSUS, we create two pseudo summaries for each dialogue example: one is\nproduced by a fine-tuned summarization model, and the other is a collection of\ndialogue turns that convey important information. We then choose one of these\npseudo summaries based on the difference in information distribution across\ndifferent types of dialogues. This selected pseudo summary serves as the\nobjective for pre-training DIONYSUS using a self-supervised approach on a large\ndialogue corpus. Our experiments show that DIONYSUS outperforms existing\nmethods on six datasets, as demonstrated by its ROUGE scores in zero-shot and\nfew-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1\">Michel Galley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tackling Ambiguity with Images: Improved Multimodal Machine Translation and Contrastive Evaluation. (arXiv:2212.10140v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10140","description":"<p>One of the major challenges of machine translation (MT) is ambiguity, which\ncan in some cases be resolved by accompanying context such as images. However,\nrecent work in multimodal MT (MMT) has shown that obtaining improvements from\nimages is challenging, limited not only by the difficulty of building effective\ncross-modal representations, but also by the lack of specific evaluation and\ntraining data. We present a new MMT approach based on a strong text-only MT\nmodel, which uses neural adapters, a novel guided self-attention mechanism and\nwhich is jointly trained on both visually-conditioned masking and MMT. We also\nintroduce CoMMuTE, a Contrastive Multilingual Multimodal Translation Evaluation\nset of ambiguous sentences and their possible translations, accompanied by\ndisambiguating images corresponding to each translation. Our approach obtains\ncompetitive results compared to strong text-only models on standard\nEnglish-to-French, English-to-German and English-to-Czech benchmarks and\noutperforms baselines and state-of-the-art MMT systems by a large margin on our\ncontrastive test set. Our code and CoMMuTE are freely available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Futeral_M/0/1/0/all/0/1\">Matthieu Futeral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Beno&#xee;t Sagot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bawden_R/0/1/0/all/0/1\">Rachel Bawden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Reasoning in Large Language Models: A Survey. (arXiv:2212.10403v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10403","description":"<p>Reasoning is a fundamental aspect of human intelligence that plays a crucial\nrole in activities such as problem solving, decision making, and critical\nthinking. In recent years, large language models (LLMs) have made significant\nprogress in natural language processing, and there is observation that these\nmodels may exhibit reasoning abilities when they are sufficiently large.\nHowever, it is not yet clear to what extent LLMs are capable of reasoning. This\npaper provides a comprehensive overview of the current state of knowledge on\nreasoning in LLMs, including techniques for improving and eliciting reasoning\nin these models, methods and benchmarks for evaluating reasoning abilities,\nfindings and implications of previous research in this field, and suggestions\non future directions. Our aim is to provide a detailed and up-to-date review of\nthis topic and stimulate meaningful discussion and future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Socratic Pretraining: Question-Driven Pretraining for Controllable Summarization. (arXiv:2212.10449v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10449","description":"<p>In long document controllable summarization, where labeled data is scarce,\npretrained models struggle to adapt to the task and effectively respond to user\nqueries. In this paper, we introduce Socratic pretraining, a question-driven,\nunsupervised pretraining objective specifically designed to improve\ncontrollability in summarization tasks. By training a model to generate and\nanswer relevant questions in a given context, Socratic pretraining enables the\nmodel to more effectively adhere to user-provided queries and identify relevant\ncontent to be summarized. We demonstrate the effectiveness of this approach\nthrough extensive experimentation on two summarization domains, short stories\nand dialogue, and multiple control strategies: keywords, questions, and factoid\nQA pairs. Our pretraining method relies only on unlabeled documents and a\nquestion generation system and outperforms pre-finetuning approaches that use\nadditional supervised data. Furthermore, our results show that Socratic\npretraining cuts task-specific labeled data requirements in half, is more\nfaithful to user-provided queries, and achieves state-of-the-art performance on\nQMSum and SQuALITY.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pagnoni_A/0/1/0/all/0/1\">Artidoro Pagnoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1\">Alexander R. Fabbri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryscinski_W/0/1/0/all/0/1\">Wojciech Kry&#x15b;ci&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Instruct: Aligning Language Models with Self-Generated Instructions. (arXiv:2212.10560v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10560","description":"<p>Large \"instruction-tuned\" language models (i.e., finetuned to respond to\ninstructions) have demonstrated a remarkable ability to generalize zero-shot to\nnew tasks. Nevertheless, they depend heavily on human-written instruction data\nthat is often limited in quantity, diversity, and creativity, therefore\nhindering the generality of the tuned model. We introduce Self-Instruct, a\nframework for improving the instruction-following capabilities of pretrained\nlanguage models by bootstrapping off their own generations. Our pipeline\ngenerates instructions, input, and output samples from a language model, then\nfilters invalid or similar ones before using them to finetune the original\nmodel. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute\nimprovement over the original model on Super-NaturalInstructions, on par with\nthe performance of InstructGPT-001, which was trained with private user data\nand human annotations. For further evaluation, we curate a set of\nexpert-written instructions for novel tasks, and show through human evaluation\nthat tuning GPT3 with Self-Instruct outperforms using existing public\ninstruction datasets by a large margin, leaving only a 5% absolute gap behind\nInstructGPT-001. Self-Instruct provides an almost annotation-free method for\naligning pre-trained language models with instructions, and we release our\nlarge synthetic dataset to facilitate future studies on instruction tuning. Our\ncode and data are available at https://github.com/yizhongw/self-instruct.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kordi_Y/0/1/0/all/0/1\">Yeganeh Kordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alisa Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoralDial: A Framework to Train and Evaluate Moral Dialogue Systems via Moral Discussions. (arXiv:2212.10720v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10720","description":"<p>Morality in dialogue systems has raised great attention in research recently.\nA moral dialogue system aligned with users' values could enhance conversation\nengagement and user connections. In this paper, we propose a framework,\nMoralDial to train and evaluate moral dialogue systems. In our framework, we\nfirst explore the communication mechanisms of morality and resolve expressed\nmorality into three parts, which indicate the roadmap for building a moral\ndialogue system. Based on that, we design a simple yet effective method:\nconstructing moral discussions between simulated specific users and the\ndialogue system. The constructed discussions consist of expressing, explaining,\nrevising, and inferring moral views in dialogue exchanges, which makes\nconversational models learn morality well in a natural manner. Furthermore, we\npropose a novel evaluation method under the framework. We evaluate the multiple\naspects of morality by judging the relation between dialogue responses and\nhuman values in discussions, where the multifaceted nature of morality is\nparticularly considered. Automatic and manual experiments demonstrate that our\nframework is promising to train and evaluate moral dialogue systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhexin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jianwei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gender Neutralization for an Inclusive Machine Translation: from Theoretical Foundations to Open Challenges. (arXiv:2301.10075v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.10075","description":"<p>Gender inclusivity in language technologies has become a prominent research\ntopic. In this study, we explore gender-neutral translation (GNT) as a form of\ngender inclusivity and a goal to be achieved by machine translation (MT)\nmodels, which have been found to perpetuate gender bias and discrimination.\nSpecifically, we focus on translation from English into Italian, a language\npair representative of salient gender-related linguistic transfer problems. To\ndefine GNT, we review a selection of relevant institutional guidelines for\ngender-inclusive language, discuss its scenarios of use, and examine the\ntechnical challenges of performing GNT in MT, concluding with a discussion of\npotential solutions to encourage advancements toward greater inclusivity in MT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Piergentili_A/0/1/0/all/0/1\">Andrea Piergentili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fucci_D/0/1/0/all/0/1\">Dennis Fucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savoldi_B/0/1/0/all/0/1\">Beatrice Savoldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bentivogli_L/0/1/0/all/0/1\">Luisa Bentivogli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding Language Models to Images for Multimodal Inputs and Outputs. (arXiv:2301.13823v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.13823","description":"<p>We propose an efficient method to ground pretrained text-only language models\nto the visual domain, enabling them to process arbitrarily interleaved\nimage-and-text data, and generate text interleaved with retrieved images. Our\nmethod leverages the abilities of language models learnt from large scale\ntext-only pretraining, such as in-context learning and free-form text\ngeneration. We keep the language model frozen, and finetune input and output\nlinear layers to enable cross-modality interactions. This allows our model to\nprocess arbitrarily interleaved image-and-text inputs, and generate free-form\ntext interleaved with retrieved images. We achieve strong zero-shot performance\non grounded tasks such as contextual image retrieval and multimodal dialogue,\nand showcase compelling interactive abilities. Our approach works with any\noff-the-shelf language model and paves the way towards an effective, general\nsolution for leveraging pretrained language models in visually grounded\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koh_J/0/1/0/all/0/1\">Jing Yu Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1\">Daniel Fried</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking. (arXiv:2302.07189v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.07189","description":"<p>Discovering entity mentions that are out of a Knowledge Base (KB) from texts\nplays a critical role in KB maintenance, but has not yet been fully explored.\nThe current methods are mostly limited to the simple threshold-based approach\nand feature-based classification, and the datasets for evaluation are\nrelatively rare. We propose BLINKout, a new BERT-based Entity Linking (EL)\nmethod which can identify mentions that do not have corresponding KB entities\nby matching them to a special NIL entity. To better utilize BERT, we propose\nnew techniques including NIL entity representation and classification, with\nsynonym enhancement. We also propose KB Pruning and Versioning strategies to\nautomatically construct out-of-KB datasets from common in-KB EL datasets.\nResults on five datasets of clinical notes, biomedical publications, and\nWikipedia articles in various domains show the advantages of BLINKout over\nexisting methods to identify out-of-KB mentions for the medical ontologies,\nUMLS, SNOMED CT, and the general KB, WikiData.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horrocks_I/0/1/0/all/0/1\">Ian Horrocks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation. (arXiv:2303.08302v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2303.08302","description":"<p>Post-training quantization (PTQ) has emerged as a promising technique for\nmitigating memory consumption and computational costs in large language models\n(LLMs). However, a systematic examination of various quantization schemes,\nmodel families, and quantization bit precision has been absent from the\nliterature. In this paper, we conduct a comprehensive analysis of these factors\nby investigating the effects of PTQ on weight-only, activation-only, and\nweight-and-activation quantization using diverse methods such as\nround-to-nearest (RTN), GPTQ, ZeroQuant, and their variants. We apply these\nmethods to two distinct model families with parameters ranging from 125M to\n176B. Our contributions include: (1) a sensitivity analysis revealing that\nactivation quantization is generally more susceptible to weight quantization,\nwith smaller models often outperforming larger models in terms of activation\nquantization; (2) an evaluation and comparison of existing PTQ methods to\noptimize model size reduction while minimizing the impact on accuracy,\nrevealing that none of the current methods can achieve the original model\nquality for quantization with either INT4-weight or\nINT4-weight-and-INT8-activation; (3) based on these insights, we propose an\noptimized method called Low-Rank Compensation (LoRC), which employs low-rank\nmatrices to enhance model quality recovery with a minimal increase in model\nsize.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaoxia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Youn_S/0/1/0/all/0/1\">Stephen Youn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxiong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Refine: Iterative Refinement with Self-Feedback. (arXiv:2303.17651v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.17651","description":"<p>Like humans, large language models (LLMs) do not always generate the best\noutput on their first try. Motivated by how humans refine their written text,\nwe introduce Self-Refine, an approach for improving initial outputs from LLMs\nthrough iterative feedback and refinement. The main idea is to generate an\ninitial output using an LLMs; then, the same LLMs provides feedback for its\noutput and uses it to refine itself, iteratively. Self-Refine does not require\nany supervised training data, additional training, or reinforcement learning,\nand instead uses a single LLM as the generator, refiner, and feedback provider.\nWe evaluate Self-Refine across 7 diverse tasks, ranging from dialog response\ngeneration to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT,\nand GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine\nare preferred by humans and automatic metrics over those generated with the\nsame LLM using conventional one-step generation, improving by ~20% absolute on\naverage in task performance. Our work demonstrates that even state-of-the-art\nLLMs like GPT-4 can be further improved at test time using our simple,\nstandalone approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_N/0/1/0/all/0/1\">Niket Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Prakhar Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hallinan_S/0/1/0/all/0/1\">Skyler Hallinan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Luyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiegreffe_S/0/1/0/all/0/1\">Sarah Wiegreffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1\">Uri Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziri_N/0/1/0/all/0/1\">Nouha Dziri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhumoye_S/0/1/0/all/0/1\">Shrimai Prabhumoye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shashank Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_B/0/1/0/all/0/1\">Bodhisattwa Prasad Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermann_K/0/1/0/all/0/1\">Katherine Hermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1\">Sean Welleck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazdanbakhsh_A/0/1/0/all/0/1\">Amir Yazdanbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Data-centric Framework for Improving Domain-specific Machine Reading Comprehension Datasets. (arXiv:2304.00483v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.00483","description":"<p>Low-quality data can cause downstream problems in high-stakes applications.\nData-centric approach emphasizes on improving dataset quality to enhance model\nperformance. High-quality datasets are needed for general-purpose Large\nLanguage Models (LLMs) training, as well as for domain-specific models, which\nare usually small in size as it is costly to engage a large number of domain\nexperts for their creation. Thus, it is vital to ensure high-quality\ndomain-specific training data. In this paper, we propose a framework for\nenhancing the data quality of original datasets. We applied the proposed\nframework to four biomedical datasets and showed relative improvement of up to\n33%/40% for fine-tuning of retrieval/reader models on the BioASQ dataset when\nusing back translation to enhance the original dataset quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bojic_I/0/1/0/all/0/1\">Iva Bojic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halim_J/0/1/0/all/0/1\">Josef Halim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suharman_V/0/1/0/all/0/1\">Verena Suharman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tar_S/0/1/0/all/0/1\">Sreeja Tar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_Q/0/1/0/all/0/1\">Qi Chwen Ong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Duy Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravaut_M/0/1/0/all/0/1\">Mathieu Ravaut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Car_J/0/1/0/all/0/1\">Josip Car</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information. (arXiv:2305.01528v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01528","description":"<p>Dungeons &amp; Dragons (D&amp;D) is a tabletop roleplaying game with complex natural\nlanguage interactions between players and hidden state information. Recent work\nhas shown that large language models (LLMs) that have access to state\ninformation can generate higher quality game turns than LLMs that use dialog\nhistory alone. However, previous work used game state information that was\nheuristically created and was not a true gold standard game state. We present\nFIREBALL, a large dataset containing nearly 25,000 unique sessions from real\nD&amp;D gameplay on Discord with true game state info. We recorded game play\nsessions of players who used the Avrae bot, which was developed to aid people\nin playing D&amp;D online, capturing language, game commands and underlying game\nstate information. We demonstrate that FIREBALL can improve natural language\ngeneration (NLG) by using Avrae state information, improving both automated\nmetrics and human judgments of quality. Additionally, we show that LLMs can\ngenerate executable Avrae commands, particularly after finetuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_A/0/1/0/all/0/1\">Andrew Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_K/0/1/0/all/0/1\">Karmanya Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_A/0/1/0/all/0/1\">Alexander Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1\">Lara J. Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training. (arXiv:2305.02031v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.02031","description":"<p>Modern Natural Language Generation (NLG) models come with massive\ncomputational and storage requirements. In this work, we study the potential of\ncompressing them, which is crucial for real-world applications serving millions\nof users. We focus on Knowledge Distillation (KD) techniques, in which a small\nstudent model learns to imitate a large teacher model, allowing to transfer\nknowledge from the teacher to the student. In contrast to much of the previous\nwork, our goal is to optimize the model for a specific NLG task and a specific\ndataset. Typically in real-world applications, in addition to labeled data\nthere is abundant unlabeled task-specific data, which is crucial for attaining\nhigh compression rates via KD. In this work, we conduct a systematic study of\ntask-specific KD techniques for various NLG tasks under realistic assumptions.\nWe discuss the special characteristics of NLG distillation and particularly the\nexposure bias problem. Following, we derive a family of Pseudo-Target (PT)\naugmentation methods, substantially extending prior work on sequence-level KD.\nWe propose the Joint-Teaching method, which applies word-level KD to multiple\nPTs generated by both the teacher and the student. Finally, we validate our\nfindings in an extreme setup with no labeled examples using GPT-4 as the\nteacher. Our study provides practical model design observations and\ndemonstrates the effectiveness of PT training for task-specific KD in NLG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Calderon_N/0/1/0/all/0/1\">Nitay Calderon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kantor_A/0/1/0/all/0/1\">Amir Kantor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives. (arXiv:2305.02364v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.02364","description":"<p>Sustaining coherent and engaging narratives requires dialogue or storytelling\nagents to understand how the personas of speakers or listeners ground the\nnarrative. Specifically, these agents must infer personas of their listeners to\nproduce statements that cater to their interests. They must also learn to\nmaintain consistent speaker personas for themselves throughout the narrative,\nso that their counterparts feel involved in a realistic conversation or story.\n</p>\n<p>However, personas are diverse and complex: they entail large quantities of\nrich interconnected world knowledge that is challenging to robustly represent\nin general narrative systems (e.g., a singer is good at singing, and may have\nattended conservatoire). In this work, we construct a new large-scale persona\ncommonsense knowledge graph, PeaCoK, containing ~100K human-validated persona\nfacts. Our knowledge graph schematizes five dimensions of persona knowledge\nidentified in previous studies of human interactive behaviours, and distils\nfacts in this schema from both existing commonsense knowledge graphs and\nlarge-scale pretrained language models. Our analysis indicates that PeaCoK\ncontains rich and precise world persona inferences that help downstream systems\ngenerate more consistent and engaging narratives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Silin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borges_B/0/1/0/all/0/1\">Beatriz Borges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Soyoung Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayazit_D/0/1/0/all/0/1\">Deniz Bayazit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanno_S/0/1/0/all/0/1\">Saya Kanno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wakaki_H/0/1/0/all/0/1\">Hiromi Wakaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitsufuji_Y/0/1/0/all/0/1\">Yuki Mitsufuji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain-of-Skills: A Configurable Model for Open-domain Question Answering. (arXiv:2305.03130v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03130","description":"<p>The retrieval model is an indispensable component for real-world\nknowledge-intensive tasks, e.g., open-domain question answering (ODQA). As\nseparate retrieval skills are annotated for different datasets, recent work\nfocuses on customized methods, limiting the model transferability and\nscalability. In this work, we propose a modular retriever where individual\nmodules correspond to key skills that can be reused across datasets. Our\napproach supports flexible skill configurations based on the target domain to\nboost performance. To mitigate task interference, we design a novel\nmodularization parameterization inspired by sparse Transformer. We demonstrate\nthat our model can benefit from self-supervised pretraining on Wikipedia and\nfine-tuning using multiple ODQA datasets, both in a multi-task fashion. Our\napproach outperforms recent self-supervised retrievers in zero-shot evaluations\nand achieves state-of-the-art fine-tuned retrieval performance on NQ, HotpotQA\nand OTT-QA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaixin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyberg_E/0/1/0/all/0/1\">Eric Nyberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Edit: Fault-Aware Code Editor for Code Generation. (arXiv:2305.04087v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2305.04087","description":"<p>Large language models (LLMs) have demonstrated an impressive ability to\ngenerate codes on competitive programming tasks. However, with limited sample\nnumbers, LLMs still suffer from poor accuracy. Inspired by the process of human\nprogramming, we propose a generate-and-edit approach named Self-Edit that\nutilizes execution results of the generated code from LLMs to improve the code\nquality on the competitive programming task. We execute the generated code on\nthe example test case provided in the question and wrap execution results into\na supplementary comment. Utilizing this comment as guidance, our fault-aware\ncode editor is employed to correct errors in the generated code. We perform\nextensive evaluations across two competitive programming datasets with nine\ndifferent LLMs. Compared to directly generating from LLMs, our approach can\nimprove the average of pass@1 by 89\\% on APPS-dev, 31\\% on APPS-test, and 48\\%\non HumanEval over nine popular code generation LLMs with parameter sizes\nranging from 110M to 175B. Compared to other post-processing methods, our\nmethod demonstrates superior accuracy and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kechi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhi Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models. (arXiv:2305.04091v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.04091","description":"<p>Large language models (LLMs) have recently been shown to deliver impressive\nperformance in various NLP tasks. To tackle multi-step reasoning tasks,\nfew-shot chain-of-thought (CoT) prompting includes a few manually crafted\nstep-by-step reasoning demonstrations which enable LLMs to explicitly generate\nreasoning steps and improve their reasoning task accuracy. To eliminate the\nmanual effort, Zero-shot-CoT concatenates the target problem statement with\n\"Let's think step by step\" as an input prompt to LLMs. Despite the success of\nZero-shot-CoT, it still suffers from three pitfalls: calculation errors,\nmissing-step errors, and semantic misunderstanding errors. To address the\nmissing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of\ntwo components: first, devising a plan to divide the entire task into smaller\nsubtasks, and then carrying out the subtasks according to the plan. To address\nthe calculation errors and improve the quality of generated reasoning steps, we\nextend PS prompting with more detailed instructions and derive PS+ prompting.\nWe evaluate our proposed prompting strategy on ten datasets across three\nreasoning problems. The experimental results over GPT-3 show that our proposed\nzero-shot prompting consistently outperforms Zero-shot-CoT across all datasets\nby a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought\nPrompting, and has comparable performance with 8-shot CoT prompting on the math\nreasoning problem. The code can be found at\nhttps://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wanyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yihuai Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiqiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yunshi Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1\">Ee-Peng Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explanation-based Finetuning Makes Models More Robust to Spurious Cues. (arXiv:2305.04990v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.04990","description":"<p>Large Language Models (LLMs) are so powerful that they sometimes learn\ncorrelations between labels and features that are irrelevant to the task,\nleading to poor generalization on out-of-distribution data. We propose\nexplanation-based finetuning as a general approach to mitigate LLMs' reliance\non spurious correlations. Unlike standard finetuning where the model only\npredicts the answer given the input, we finetune the model to additionally\ngenerate a free-text explanation supporting its answer. To evaluate our method,\nwe finetune the model on artificially constructed training sets containing\ndifferent types of spurious cues, and test it on a test set without these cues.\nCompared to standard finetuning, our method makes GPT-3 (davinci) remarkably\nmore robust against spurious cues in terms of accuracy drop across four\nclassification tasks: ComVE (+1.2), CREAK (+9.1), e-SNLI (+15.4), and SBIC\n(+6.5). The efficacy generalizes across multiple model families and scales,\nwith greater gains for larger models. Finally, our method also works well with\nexplanations generated by the model, implying its applicability to more\ndatasets without human-written explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ludan_J/0/1/0/all/0/1\">Josh Magnus Ludan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yixuan Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Saurabh Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apidianaki_M/0/1/0/all/0/1\">Marianna Apidianaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ANALOGICAL -- A Novel Benchmark for Long Text Analogy Evaluation in Large Language Models. (arXiv:2305.05050v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.05050","description":"<p>Over the past decade, analogies, in the form of word-level analogies, have\nplayed a significant role as an intrinsic measure of evaluating the quality of\nword embedding methods such as word2vec. Modern large language models (LLMs),\nhowever, are primarily evaluated on extrinsic measures based on benchmarks such\nas GLUE and SuperGLUE, and there are only a few investigations on whether LLMs\ncan draw analogies between long texts. In this paper, we present ANALOGICAL, a\nnew benchmark to intrinsically evaluate LLMs across a taxonomy of analogies of\nlong text with six levels of complexity -- (i) word, (ii) word vs. sentence,\n(iii) syntactic, (iv) negation, (v) entailment, and (vi) metaphor. Using\nthirteen datasets and three different distance measures, we evaluate the\nabilities of eight LLMs in identifying analogical pairs in the semantic vector\nspace. Our evaluation finds that it is increasingly challenging for LLMs to\nidentify analogies when going up the analogy taxonomy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wijesiriwardene_T/0/1/0/all/0/1\">Thilini Wijesiriwardene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wickramarachchi_R/0/1/0/all/0/1\">Ruwan Wickramarachchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gajera_B/0/1/0/all/0/1\">Bimal G. Gajera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gowaikar_S/0/1/0/all/0/1\">Shreeyash Mukul Gowaikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_C/0/1/0/all/0/1\">Chandan Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reganti_A/0/1/0/all/0/1\">Aishwarya Naresh Reganti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Amitava Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Script Knowledge from Large Language Models for Constrained Language Planning. (arXiv:2305.05252v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.05252","description":"<p>In everyday life, humans often plan their actions by following step-by-step\ninstructions in the form of goal-oriented scripts. Previous work has exploited\nlanguage models (LMs) to plan for abstract goals of stereotypical activities\n(e.g., \"make a cake\"), but leaves more specific goals with multi-facet\nconstraints understudied (e.g., \"make a cake for diabetics\"). In this paper, we\ndefine the task of constrained language planning for the first time. We propose\nan overgenerate-then-filter approach to improve large language models (LLMs) on\nthis task, and use it to distill a novel constrained language planning dataset,\nCoScript, which consists of 55,000 scripts. Empirical results demonstrate that\nour method significantly improves the constrained language planning ability of\nLLMs, especially on constraint faithfulness. Furthermore, CoScript is\ndemonstrated to be quite effective in endowing smaller LMs with constrained\nlanguage planning ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Siyu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiangjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Ziquan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1\">Xuyang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Soham Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jankowski_C/0/1/0/all/0/1\">Charles Robert Jankowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Deqing Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Zero-shot Cross-lingual Retrieval by Training on Artificially Code-Switched Data. (arXiv:2305.05295v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.05295","description":"<p>Transferring information retrieval (IR) models from a high-resource language\n(typically English) to other languages in a zero-shot fashion has become a\nwidely adopted approach. In this work, we show that the effectiveness of\nzero-shot rankers diminishes when queries and documents are present in\ndifferent languages. Motivated by this, we propose to train ranking models on\nartificially code-switched data instead, which we generate by utilizing\nbilingual lexicons. To this end, we experiment with lexicons induced from (1)\ncross-lingual word embeddings and (2) parallel Wikipedia page titles. We use\nthe mMARCO dataset to extensively evaluate reranking models on 36 language\npairs spanning Monolingual IR (MoIR), Cross-lingual IR (CLIR), and Multilingual\nIR (MLIR). Our results show that code-switching can yield consistent and\nsubstantial gains of 5.1 MRR@10 in CLIR and 3.9 MRR@10 in MLIR, while\nmaintaining stable performance in MoIR. Encouragingly, the gains are especially\npronounced for distant languages (up to 2x absolute gain). We further show that\nour approach is robust towards the ratio of code-switched tokens and also\nextends to unseen languages. Our results demonstrate that training on\ncode-switched data is a cheap and effective way of generalizing zero-shot\nrankers for cross-lingual and multilingual retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Litschko_R/0/1/0/all/0/1\">Robert Litschko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment. (arXiv:2305.05940v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.05940","description":"<p>In-context learning (ICL) unfolds as large language models become capable of\ninferring test labels conditioned on a few labeled samples without any gradient\nupdate. ICL-enabled large language models provide a promising step forward\ntoward bypassing recurrent annotation costs in a low-resource setting. Yet,\nonly a handful of past studies have explored ICL in a cross-lingual setting, in\nwhich the need for transferring label-knowledge from a high-resource language\nto a low-resource one is immensely crucial. To bridge the gap, we provide the\nfirst in-depth analysis of ICL for cross-lingual text classification. We find\nthat the prevalent mode of selecting random input-label pairs to construct the\nprompt-context is severely limited in the case of cross-lingual ICL, primarily\ndue to the lack of alignment in the input as well as the output spaces. To\nmitigate this, we propose a novel prompt construction strategy -- Cross-lingual\nIn-context Source-Target Alignment (X-InSTA). With an injected coherence in the\nsemantics of the input examples and a task-based alignment across the source\nand target languages, X-InSTA is able to outperform random prompt selection by\na large margin across three different tasks using 44 different cross-lingual\npairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tanwar_E/0/1/0/all/0/1\">Eshaan Tanwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Subhabrata Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borthakur_M/0/1/0/all/0/1\">Manish Borthakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from Book Reviews. (arXiv:2305.06595v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.06595","description":"<p>The analysis of consumer sentiment, as expressed through reviews, can provide\na wealth of insight regarding the quality of a product. While the study of\nsentiment analysis has been widely explored in many popular languages,\nrelatively less attention has been given to the Bangla language, mostly due to\na lack of relevant data and cross-domain adaptability. To address this\nlimitation, we present BanglaBook, a large-scale dataset of Bangla book reviews\nconsisting of 158,065 samples classified into three broad categories: positive,\nnegative, and neutral. We provide a detailed statistical analysis of the\ndataset and employ a range of machine learning models to establish baselines\nincluding SVM, LSTM, and Bangla-BERT. Our findings demonstrate a substantial\nperformance advantage of pre-trained models over models that rely on manually\ncrafted features, emphasizing the necessity for additional training resources\nin this domain. Additionally, we conduct an in-depth error analysis by\nexamining sentiment unigrams, which may provide insight into common\nclassification errors in under-resourced languages like Bangla. Our codes and\ndata are publicly available at https://github.com/mohsinulkabir14/BanglaBook.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Mohsinul Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahfuz_O/0/1/0/all/0/1\">Obayed Bin Mahfuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raiyan_S/0/1/0/all/0/1\">Syed Rifat Raiyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmud_H/0/1/0/all/0/1\">Hasan Mahmud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Md Kamrul Hasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PALR: Personalization Aware LLMs for Recommendation. (arXiv:2305.07622v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2305.07622","description":"<p>Large language models (LLMs) have recently received significant attention for\ntheir exceptional capabilities. Despite extensive efforts in developing\ngeneral-purpose LLMs that can be utilized in various natural language\nprocessing (NLP) tasks, there has been less research exploring their potential\nin recommender systems. In this paper, we propose a novel framework, named\nPALR, which aiming to combine user history behaviors (such as clicks,\npurchases, ratings, etc.) with LLMs to generate user preferred items.\nSpecifically, we first use user/item interactions as guidance for candidate\nretrieval. Then we adopt a LLM-based ranking model to generate recommended\nitems. Unlike existing approaches that typically adopt general-purpose LLMs for\nzero/few-shot recommendation testing or training on small-sized language models\n(with less than 1 billion parameters), which cannot fully elicit LLMs'\nreasoning abilities and leverage rich item side parametric knowledge, we\nfine-tune a 7 billion parameters LLM for the ranking purpose. This model takes\nretrieval candidates in natural language format as input, with instruction\nwhich explicitly asking to select results from input candidates during\ninference. Our experimental results demonstrate that our solution outperforms\nstate-of-the-art models on various sequential recommendation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zheng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Melody-Guided Lyrics Generation. (arXiv:2305.07760v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2305.07760","description":"<p>Automatic song writing is a topic of significant practical interest. However,\nits research is largely hindered by the lack of training data due to copyright\nconcerns and challenged by its creative nature. Most noticeably, prior works\noften fall short of modeling the cross-modal correlation between melody and\nlyrics due to limited parallel data, hence generating lyrics that are less\nsingable. Existing works also lack effective mechanisms for content control, a\nmuch desired feature for democratizing song creation for people with limited\nmusic background. In this work, we propose to generate pleasantly listenable\nlyrics without training on melody-lyric aligned data. Instead, we design a\nhierarchical lyric generation framework that disentangles training (based\npurely on text) from inference (melody-guided text generation). At inference\ntime, we leverage the crucial alignments between melody and lyrics and compile\nthe given melody into constraints to guide the generation process. Evaluation\nresults show that our model can generate high-quality lyrics that are more\nsingable, intelligible, coherent, and in rhyme than strong baselines including\nthose supervised on parallel data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yufei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_Chen_A/0/1/0/all/0/1\">Anjali Narayan-Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oraby_S/0/1/0/all/0/1\">Shereen Oraby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cervone_A/0/1/0/all/0/1\">Alessandra Cervone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigurdsson_G/0/1/0/all/0/1\">Gunnar Sigurdsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chenyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1\">Tagyoung Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLG Evaluation Metrics Beyond Correlation Analysis: An Empirical Metric Preference Checklist. (arXiv:2305.08566v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08566","description":"<p>In this study, we analyze automatic evaluation metrics for Natural Language\nGeneration (NLG), specifically task-agnostic metrics and human-aligned metrics.\nTask-agnostic metrics, such as Perplexity, BLEU, BERTScore, are cost-effective\nand highly adaptable to diverse NLG tasks, yet they have a weak correlation\nwith human. Human-aligned metrics (CTC, CtrlEval, UniEval) improves correlation\nlevel by incorporating desirable human-like qualities as training objective.\nHowever, their effectiveness at discerning system-level performance and quality\nof system outputs remain unclear.\n</p>\n<p>We present metric preference checklist as a framework to assess the\neffectiveness of automatic metrics in three NLG tasks: Text Summarization,\nDialogue Response Generation, and Controlled Generation. Our proposed framework\nprovides access: (i) for verifying whether automatic metrics are faithful to\nhuman preference, regardless of their correlation level to human; and (ii) for\ninspecting the strengths and limitations of NLG systems via pairwise\nevaluation. We show that automatic metrics provide a better guidance than human\non discriminating system-level performance in Text Summarization and Controlled\nGeneration tasks. We also show that multi-aspect human-aligned metric (UniEval)\nis not necessarily dominant over single-aspect human-aligned metrics (CTC,\nCtrlEval) and task-agnostic metrics (BLEU, BERTScore), particularly in\nControlled Generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nimah_I/0/1/0/all/0/1\">Iftitahu Ni&#x27;mah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1\">Vlado Menkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"sustain.AI: a Recommender System to analyze Sustainability Reports. (arXiv:2305.08711v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08711","description":"<p>We present sustainAI, an intelligent, context-aware recommender system that\nassists auditors and financial investors as well as the general public to\nefficiently analyze companies' sustainability reports. The tool leverages an\nend-to-end trainable architecture that couples a BERT-based encoding module\nwith a multi-label classification head to match relevant text passages from\nsustainability reports to their respective law regulations from the Global\nReporting Initiative (GRI) standards. We evaluate our model on two novel German\nsustainability reporting data sets and consistently achieve a significantly\nhigher recommendation performance compared to multiple strong baselines.\nFurthermore, sustainAI is publicly available for everyone at\nhttps://sustain.ki.nrw/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hillebrand_L/0/1/0/all/0/1\">Lars Hillebrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pielka_M/0/1/0/all/0/1\">Maren Pielka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonhard_D/0/1/0/all/0/1\">David Leonhard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deusser_T/0/1/0/all/0/1\">Tobias Deu&#xdf;er</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dilmaghani_T/0/1/0/all/0/1\">Tim Dilmaghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kliem_B/0/1/0/all/0/1\">Bernd Kliem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loitz_R/0/1/0/all/0/1\">R&#xfc;diger Loitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morad_M/0/1/0/all/0/1\">Milad Morad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Temath_C/0/1/0/all/0/1\">Christian Temath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bell_T/0/1/0/all/0/1\">Thiago Bell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenzel_R/0/1/0/all/0/1\">Robin Stenzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifa_R/0/1/0/all/0/1\">Rafet Sifa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personality Understanding of Fictional Characters during Book Reading. (arXiv:2305.10156v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10156","description":"<p>Comprehending characters' personalities is a crucial aspect of story reading.\nAs readers engage with a story, their understanding of a character evolves\nbased on new events and information; and multiple fine-grained aspects of\npersonalities can be perceived. This leads to a natural problem of situated and\nfine-grained personality understanding. The problem has not been studied in the\nNLP field, primarily due to the lack of appropriate datasets mimicking the\nprocess of book reading. We present the first labeled dataset PersoNet for this\nproblem. Our novel annotation strategy involves annotating user notes from\nonline reading apps as a proxy for the original books. Experiments and human\nstudies indicate that our dataset construction is both efficient and accurate;\nand our task heavily relies on long-term context to achieve accurate\npredictions for both machines and humans. The dataset is available at\nhttps://github.com/Gorov/personet_acl23.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1\">Shunyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_W/0/1/0/all/0/1\">Wenjie Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaochen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhou Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paxion: Patching Action Knowledge in Video-Language Foundation Models. (arXiv:2305.10683v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2305.10683","description":"<p>Action knowledge involves the understanding of textual, visual, and temporal\naspects of actions. We introduce the Action Dynamics Benchmark (ActionBench)\ncontaining two carefully designed probing tasks: Action Antonym and Video\nReversal, which targets multimodal alignment capabilities and temporal\nunderstanding skills of the model, respectively. Despite recent video-language\nmodels' (VidLM) impressive performance on various benchmark tasks, our\ndiagnostic tasks reveal their surprising deficiency (near-random performance)\nin action knowledge, suggesting that current models rely on object recognition\nabilities as a shortcut for action understanding. To remedy this, we propose a\nnovel framework, Paxion, along with a new Discriminative Video Dynamics\nModeling (DVDM) objective. The Paxion framework utilizes a Knowledge Patcher\nnetwork to encode new action knowledge and a Knowledge Fuser component to\nintegrate the Patcher into frozen VidLMs without compromising their existing\ncapabilities. Due to limitations of the widely-used Video-Text Contrastive\n(VTC) loss for learning action knowledge, we introduce the DVDM objective to\ntrain the Knowledge Patcher. DVDM forces the model to encode the correlation\nbetween the action text and the correct ordering of video frames. Our extensive\nanalyses show that Paxion and DVDM together effectively fill the gap in action\nknowledge understanding (~50% to 80%), while maintaining or improving\nperformance on a wide spectrum of both object- and action-centric downstream\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhailong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blume_A/0/1/0/all/0/1\">Ansel Blume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Genglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zineng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MolXPT: Wrapping Molecules with Text for Generative Pre-training. (arXiv:2305.10688v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.10688","description":"<p>Generative pre-trained Transformer (GPT) has demonstrates its great success\nin natural language processing and related techniques have been adapted into\nmolecular modeling. Considering that text is the most important record for\nscientific discovery, in this paper, we propose MolXPT, a unified language\nmodel of text and molecules pre-trained on SMILES (a sequence representation of\nmolecules) wrapped by text. Briefly, we detect the molecule names in each\nsequence and replace them to the corresponding SMILES. In this way, the SMILES\ncould leverage the information from surrounding text, and vice versa. The above\nwrapped sequences, text sequences from PubMed and SMILES sequences from PubChem\nare all fed into a language model for pre-training. Experimental results\ndemonstrate that MolXPT outperforms strong baselines of molecular property\nprediction on MoleculeNet, performs comparably to the best model in\ntext-molecule translation while using less than half of its parameters, and\nenables zero-shot molecular generation without finetuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zequn Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yingce Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lijun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Shufang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction. (arXiv:2305.11029v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.11029","description":"<p>Document-level relation extraction (DocRE) aims to infer complex semantic\nrelations among entities in a document. Distant supervision (DS) is able to\ngenerate massive auto-labeled data, which can improve DocRE performance. Recent\nworks leverage pseudo labels generated by the pre-denoising model to reduce\nnoise in DS data. However, unreliable pseudo labels bring new noise, e.g.,\nadding false pseudo labels and losing correct DS labels. Therefore, how to\nselect effective pseudo labels to denoise DS data is still a challenge in\ndocument-level distant relation extraction. To tackle this issue, we introduce\nuncertainty estimation technology to determine whether pseudo labels can be\ntrusted. In this work, we propose a Document-level distant Relation Extraction\nframework with Uncertainty Guided label denoising, UGDRE. Specifically, we\npropose a novel instance-level uncertainty estimation method, which measures\nthe reliability of the pseudo labels with overlapping relations. By further\nconsidering the long-tail problem, we design dynamic uncertainty thresholds for\ndifferent types of relations to filter high-uncertainty pseudo labels. We\nconduct experiments on two public datasets. Our framework outperforms strong\nbaselines by 1.91 F1 and 2.28 Ign F1 on the RE-DocRED dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaocui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1\">Pengfei Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning In-context Learning for Named Entity Recognition. (arXiv:2305.11038v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.11038","description":"<p>Named entity recognition in real-world applications suffers from the\ndiversity of entity types, the emergence of new entity types, and the lack of\nhigh-quality annotations. To address the above problems, this paper proposes an\nin-context learning-based NER approach, which can effectively inject in-context\nNER ability into PLMs and recognize entities of novel types on-the-fly using\nonly a few demonstrative instances. Specifically, we model PLMs as a\nmeta-function $\\mathcal{ \\lambda_ {\\text{instruction, demonstrations, text}}.\nM}$, and a new entity extractor can be implicitly constructed by applying new\ninstruction and demonstrations to PLMs, i.e., $\\mathcal{ (\\lambda . M)\n}$(instruction, demonstrations) $\\to$ $\\mathcal{F}$ where $\\mathcal{F}$ will be\na new entity extractor, i.e., $\\mathcal{F}$: text $\\to$ entities. To inject the\nabove in-context NER ability into PLMs, we propose a meta-function pre-training\nalgorithm, which pre-trains PLMs by comparing the (instruction,\ndemonstration)-initialized extractor with a surrogate golden extractor.\nExperimental results on 4 few-shot NER datasets show that our method can\neffectively inject in-context NER ability into PLMs and significantly\noutperforms the PLMs+fine-tuning counterparts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yaojie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jie Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Wei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1\">Boxi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages. (arXiv:2305.12182v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.12182","description":"<p>The NLP community has mainly focused on scaling Large Language Models (LLMs)\nvertically, i.e., making them better for about 100 languages. We instead scale\nLLMs horizontally: we create, through continued pretraining, Glot500-m, an LLM\nthat covers 511 predominantly low-resource languages. An important part of this\neffort is to collect and clean Glot500-c, a corpus that covers these 511\nlanguages and allows us to train Glot500-m. We evaluate Glot500-m on five\ndiverse tasks across these languages. We observe large improvements for both\nhigh-resource and low-resource languages compared to an XLM-R baseline. Our\nanalysis shows that no single factor explains the quality of multilingual LLM\nrepresentations. Rather, a combination of factors determines quality including\ncorpus size, script, \"help\" from related languages and the total capacity of\nthe model. Our work addresses an important goal of NLP research: we should not\nlimit NLP to a small fraction of the world's languages and instead strive to\nsupport as many languages as possible to bring the benefits of NLP technology\nto all languages and cultures. Code, data and models are available at\nhttps://github.com/cisnlp/Glot500.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imani_A/0/1/0/all/0/1\">Ayyoob Imani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1\">Peiqin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kargaran_A/0/1/0/all/0/1\">Amir Hossein Kargaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Severini_S/0/1/0/all/0/1\">Silvia Severini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabet_M/0/1/0/all/0/1\">Masoud Jalili Sabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kassner_N/0/1/0/all/0/1\">Nora Kassner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chunlan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_H/0/1/0/all/0/1\">Helmut Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yvon_F/0/1/0/all/0/1\">Fran&#xe7;ois Yvon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Head State Space Model for Speech Recognition. (arXiv:2305.12498v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2305.12498","description":"<p>State space models (SSMs) have recently shown promising results on\nsmall-scale sequence and language modelling tasks, rivalling and outperforming\nmany attention-based approaches. In this paper, we propose a multi-head state\nspace (MH-SSM) architecture equipped with special gating mechanisms, where\nparallel heads are taught to learn local and global temporal dynamics on\nsequence data. As a drop-in replacement for multi-head attention in transformer\nencoders, this new model significantly outperforms the transformer transducer\non the LibriSpeech speech recognition corpus. Furthermore, we augment the\ntransformer block with MH-SSMs layers, referred to as the Stateformer,\nachieving state-of-the-art performance on the LibriSpeech task, with word error\nrates of 1.76\\%/4.37\\% on the development and 1.91\\%/4.36\\% on the test sets\nwithout using an external language model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fathullah_Y/0/1/0/all/0/1\">Yassir Fathullah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_C/0/1/0/all/0/1\">Chunyang Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shangguan_Y/0/1/0/all/0/1\">Yuan Shangguan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_J/0/1/0/all/0/1\">Junteng Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiong_W/0/1/0/all/0/1\">Wenhan Xiong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahadeokar_J/0/1/0/all/0/1\">Jay Mahadeokar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Chunxi Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1\">Yangyang Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seltzer_M/0/1/0/all/0/1\">Mike Seltzer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gales_M/0/1/0/all/0/1\">Mark J. F. Gales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Energy-based Language Models with Different Architectures and Training Methods for Speech Recognition. (arXiv:2305.12676v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.12676","description":"<p>Energy-based language models (ELMs) parameterize an unnormalized distribution\nfor natural sentences and are radically different from popular autoregressive\nlanguage models (ALMs). As an important application, ELMs have been\nsuccessfully used as a means for calculating sentence scores in speech\nrecognition, but they all use less-modern CNN or LSTM networks. The recent\nprogress in Transformer networks and large pretrained models such as BERT and\nGPT2 opens new possibility to further advancing ELMs. In this paper, we explore\ndifferent architectures of energy functions and different training methods to\ninvestigate the capabilities of ELMs in rescoring for speech recognition, all\nusing large pretrained models as backbones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Z/0/1/0/all/0/1\">Zhaobiao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zhijian Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Q/0/1/0/all/0/1\">Qing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gene Set Summarization using Large Language Models. (arXiv:2305.13338v2 [q-bio.GN] UPDATED)","link":"http://arxiv.org/abs/2305.13338","description":"<p>Molecular biologists frequently interpret gene lists derived from\nhigh-throughput experiments and computational analysis. This is typically done\nas a statistical enrichment analysis that measures the over- or\nunder-representation of biological function terms associated with genes or\ntheir properties, based on curated assertions from a knowledge base (KB) such\nas the Gene Ontology (GO). Interpreting gene lists can also be framed as a\ntextual summarization task, enabling the use of Large Language Models (LLMs),\npotentially utilizing scientific texts directly and avoiding reliance on a KB.\n</p>\n<p>We developed SPINDOCTOR (Structured Prompt Interpolation of Natural Language\nDescriptions of Controlled Terms for Ontology Reporting), a method that uses\nGPT models to perform gene set function summarization as a complement to\nstandard enrichment analysis. This method can use different sources of gene\nfunctional information: (1) structured text derived from curated ontological KB\nannotations, (2) ontology-free narrative gene summaries, or (3) direct model\nretrieval.\n</p>\n<p>We demonstrate that these methods are able to generate plausible and\nbiologically valid summary GO term lists for gene sets. However, GPT-based\napproaches are unable to deliver reliable scores or p-values and often return\nterms that are not statistically significant. Crucially, these methods were\nrarely able to recapitulate the most precise and informative term from standard\nenrichment, likely due to an inability to generalize and reason using an\nontology. Results are highly nondeterministic, with minor variations in prompt\nresulting in radically different term lists. Our results show that at this\npoint, LLM-based methods are unsuitable as a replacement for standard term\nenrichment analysis and that manual curation of ontological assertions remains\nnecessary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Joachimiak_M/0/1/0/all/0/1\">Marcin P. Joachimiak</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Caufield_J/0/1/0/all/0/1\">J. Harry Caufield</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Harris_N/0/1/0/all/0/1\">Nomi L. Harris</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kim_H/0/1/0/all/0/1\">Hyeongsik Kim</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Mungall_C/0/1/0/all/0/1\">Christopher J. Mungall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aligning the Norwegian UD Treebank with Entity and Coreference Information. (arXiv:2305.13527v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13527","description":"<p>This paper presents a merged collection of entity and coreference annotated\ndata grounded in the Universal Dependencies (UD) treebanks for the two written\nforms of Norwegian: Bokm{\\aa}l and Nynorsk. The aligned and converted corpora\nare the Norwegian Named Entities (NorNE) and Norwegian Anaphora Resolution\nCorpus (NARC). While NorNE is aligned with an older version of the treebank,\nNARC is misaligned and requires extensive transformation from the original\nannotations to the UD structure and CoNLL-U format. We here demonstrate the\nconversion and alignment processes, along with an analysis of discovered issues\nand errors in the data - some of which include data split overlaps in the\noriginal treebank. These procedures and the developed system may prove helpful\nfor future corpus alignment and coreference annotation endeavors. The merged\ncorpora comprise the first Norwegian UD treebank enriched with named entities\nand coreference information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jorgensen_T/0/1/0/all/0/1\">Tollef Emil J&#xf8;rgensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaasen_A/0/1/0/all/0/1\">Andre K&#xe5;sen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoupled Rationalization with Asymmetric Learning Rates: A Flexible Lipschitz Restraint. (arXiv:2305.13599v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2305.13599","description":"<p>A self-explaining rationalization model is generally constructed by a\ncooperative game where a generator selects the most human-intelligible pieces\nfrom the input text as rationales, followed by a predictor that makes\npredictions based on the selected rationales. However, such a cooperative game\nmay incur the degeneration problem where the predictor overfits to the\nuninformative pieces generated by a not yet well-trained generator and in turn,\nleads the generator to converge to a sub-optimal model that tends to select\nsenseless pieces. In this paper, we theoretically bridge degeneration with the\npredictor's Lipschitz continuity. Then, we empirically propose a simple but\neffective method named DR, which can naturally and flexibly restrain the\nLipschitz constant of the predictor, to address the problem of degeneration.\nThe main idea of DR is to decouple the generator and predictor to allocate them\nwith asymmetric learning rates. A series of experiments conducted on two widely\nused benchmarks have verified the effectiveness of the proposed method. Codes:\n\\href{https://github.com/jugechengzi/Rationalization-DR}{https://github.com/jugechengzi/Rationalization-DR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haozhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">YuanKai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jie Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yixiong Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres. (arXiv:2305.13617v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.13617","description":"<p>Event-centric structured prediction involves predicting structured outputs of\nevents. In most NLP cases, event structures are complex with manifold\ndependency, and it is challenging to effectively represent these complicated\nstructured events. To address these issues, we propose Structured Prediction\nwith Energy-based Event-Centric Hyperspheres (SPEECH). SPEECH models complex\ndependency among event structured components with energy-based modeling, and\nrepresents event classes with simple but effective hyperspheres. Experiments on\ntwo unified-annotated event datasets indicate that SPEECH is predominant in\nevent detection and event-relation extraction tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shengyu Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating OpenAI's Whisper ASR for Punctuation Prediction and Topic Modeling of life histories of the Museum of the Person. (arXiv:2305.14580v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14580","description":"<p>Automatic speech recognition (ASR) systems play a key role in applications\ninvolving human-machine interactions. Despite their importance, ASR models for\nthe Portuguese language proposed in the last decade have limitations in\nrelation to the correct identification of punctuation marks in automatic\ntranscriptions, which hinder the use of transcriptions by other systems,\nmodels, and even by humans. However, recently Whisper ASR was proposed by\nOpenAI, a general-purpose speech recognition model that has generated great\nexpectations in dealing with such limitations. This chapter presents the first\nstudy on the performance of Whisper for punctuation prediction in the\nPortuguese language. We present an experimental evaluation considering both\ntheoretical aspects involving pausing points (comma) and complete ideas\n(exclamation, question, and fullstop), as well as practical aspects involving\ntranscript-based topic modeling - an application dependent on punctuation marks\nfor promising performance. We analyzed experimental results from videos of\nMuseum of the Person, a virtual museum that aims to tell and preserve people's\nlife histories, thus discussing the pros and cons of Whisper in a real-world\nscenario. Although our experiments indicate that Whisper achieves\nstate-of-the-art results, we conclude that some punctuation marks require\nimprovements, such as exclamation, semicolon and colon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gris_L/0/1/0/all/0/1\">Lucas Rafael Stefanel Gris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcacini_R/0/1/0/all/0/1\">Ricardo Marcacini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_A/0/1/0/all/0/1\">Arnaldo Candido Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casanova_E/0/1/0/all/0/1\">Edresson Casanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_A/0/1/0/all/0/1\">Anderson Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aluisio_S/0/1/0/all/0/1\">Sandra Maria Alu&#xed;sio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification. (arXiv:2305.15282v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.15282","description":"<p>In recent years, large language models (LLMs) have achieved strong\nperformance on benchmark tasks, especially in zero or few-shot settings.\nHowever, these benchmarks often do not adequately address the challenges posed\nin the real-world, such as that of hierarchical classification. In order to\naddress this challenge, we propose refactoring conventional tasks on\nhierarchical datasets into a more indicative long-tail prediction task. We\nobserve LLMs are more prone to failure in these cases. To address these\nlimitations, we propose the use of entailment-contradiction prediction in\nconjunction with LLMs, which allows for strong performance in a strict\nzero-shot setting. Importantly, our method does not require any parameter\nupdates, a resource-intensive process and achieves strong performance across\nmultiple datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhambhoria_R/0/1/0/all/0/1\">Rohan Bhambhoria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConvGQR: Generative Query Reformulation for Conversational Search. (arXiv:2305.15645v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2305.15645","description":"<p>In conversational search, the user's real search intent for the current turn\nis dependent on the previous conversation history. It is challenging to\ndetermine a good search query from the whole conversation context. To avoid the\nexpensive re-training of the query encoder, most existing methods try to learn\na rewriting model to de-contextualize the current query by mimicking the manual\nquery rewriting. However, manually rewritten queries are not always the best\nsearch queries. Training a rewriting model on them would limit the model's\nability to produce good search queries. Another useful hint is the potential\nanswer to the question. In this paper, we propose ConvGQR, a new framework to\nreformulate conversational queries based on generative pre-trained language\nmodels (PLMs), one for query rewriting and another for generating potential\nanswers. By combining both, ConvGQR can produce better search queries. In\naddition, to relate query reformulation to retrieval performance, we propose a\nknowledge infusion mechanism to optimize both query reformulation and\nretrieval. Extensive experiments on four conversational search datasets\ndemonstrate the effectiveness of ConvGQR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mo_F/0/1/0/all/0/1\">Fengran Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1\">Kelong Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yutao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yihong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaiyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparative Study of Pre-Trained BERT Models for Code-Mixed Hindi-English Data. (arXiv:2305.15722v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.15722","description":"<p>The term \"Code Mixed\" refers to the use of more than one language in the same\ntext. This phenomenon is predominantly observed on social media platforms, with\nan increasing amount of adaptation as time goes on. It is critical to detect\nforeign elements in a language and process them correctly, as a considerable\nnumber of individuals are using code-mixed languages that could not be\ncomprehended by understanding one of those languages. In this work, we focus on\nlow-resource Hindi-English code-mixed language and enhancing the performance of\ndifferent code-mixed natural language processing tasks such as sentiment\nanalysis, emotion recognition, and hate speech identification. We perform a\ncomparative analysis of different Transformer-based language Models pre-trained\nusing unsupervised approaches. We have included the code-mixed models like\nHingBERT, HingRoBERTa, HingRoBERTa-Mixed, mBERT, and non-code-mixed models like\nAlBERT, BERT, and RoBERTa for comparative analysis of code-mixed Hindi-English\ndownstream tasks. We report state-of-the-art results on respective datasets\nusing HingBERT-based models which are specifically pre-trained on real\ncode-mixed text. Our HingBERT-based models provide significant improvements\nthus highlighting the poor performance of vanilla BERT models on code-mixed\ntext.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patil_A/0/1/0/all/0/1\">Aryan Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwardhan_V/0/1/0/all/0/1\">Varad Patwardhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phaltankar_A/0/1/0/all/0/1\">Abhishek Phaltankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takawane_G/0/1/0/all/0/1\">Gauri Takawane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diversity-Aware Coherence Loss for Improving Neural Topic Models. (arXiv:2305.16199v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16199","description":"<p>The standard approach for neural topic modeling uses a variational\nautoencoder (VAE) framework that jointly minimizes the KL divergence between\nthe estimated posterior and prior, in addition to the reconstruction loss.\nSince neural topic models are trained by recreating individual input documents,\nthey do not explicitly capture the coherence between topic words on the corpus\nlevel. In this work, we propose a novel diversity-aware coherence loss that\nencourages the model to learn corpus-level coherence scores while maintaining a\nhigh diversity between topics. Experimental results on multiple datasets show\nthat our method significantly improves the performance of neural topic models\nwithout requiring any pretraining or additional parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Raymond Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Pizarro_F/0/1/0/all/0/1\">Felipe Gonz&#xe1;lez-Pizarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Linzi Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_G/0/1/0/all/0/1\">Gabriel Murray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1\">Giuseppe Carenini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNITE: A Unified Benchmark for Text-to-SQL Evaluation. (arXiv:2305.16265v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16265","description":"<p>A practical text-to-SQL system should generalize well on a wide variety of\nnatural language questions, unseen database schemas, and novel SQL query\nstructures. To comprehensively evaluate text-to-SQL systems, we introduce a\n\\textbf{UNI}fied benchmark for \\textbf{T}ext-to-SQL \\textbf{E}valuation\n(UNITE). It is composed of publicly available text-to-SQL datasets, containing\nnatural language questions from more than 12 domains, SQL queries from more\nthan 3.9K patterns, and 29K databases. Compared to the widely used Spider\nbenchmark \\cite{yu-etal-2018-spider}, we introduce $\\sim$120K additional\nexamples and a threefold increase in SQL patterns, such as comparative and\nboolean questions. We conduct a systematic study of six state-of-the-art (SOTA)\ntext-to-SQL parsers on our new benchmark and show that: 1) Codex performs\nsurprisingly well on out-of-domain datasets; 2) specially designed decoding\nmethods (e.g. constrained beam search) can improve performance for both\nin-domain and out-of-domain settings; 3) explicitly modeling the relationship\nbetween questions and schemas further improves the Seq2Seq models. More\nimportantly, our benchmark presents key challenges towards compositional\ngeneralization and robustness issues -- which these SOTA models cannot address\nwell. \\footnote{Our code and data processing script will be available at\n\\url{https://github.com/XXXX.}}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_W/0/1/0/all/0/1\">Wuwei Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiguo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_A/0/1/0/all/0/1\">Anuj Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Henghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Alexander Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hang_C/0/1/0/all/0/1\">Chung-Wei Hang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lilien_J/0/1/0/all/0/1\">Joseph Lilien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yiqun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lin Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Mingwen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jiarong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ash_S/0/1/0/all/0/1\">Stephen Ash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castelli_V/0/1/0/all/0/1\">Vittorio Castelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_P/0/1/0/all/0/1\">Patrick Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can NLI Provide Proper Indirect Supervision for Low-resource Biomedical Relation Extraction?. (arXiv:2212.10784v2 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2212.10784","description":"<p>Two key obstacles in biomedical relation extraction (RE) are the scarcity of\nannotations and the prevalence of instances without explicitly pre-defined\nlabels due to low annotation coverage. Existing approaches, which treat\nbiomedical RE as a multi-class classification task, often result in poor\ngeneralization in low-resource settings and do not have the ability to make\nselective prediction on unknown cases but give a guess from seen relations,\nhindering the applicability of those approaches. We present NBR, which converts\nbiomedical RE as natural language inference formulation through indirect\nsupervision. By converting relations to natural language hypotheses, NBR is\ncapable of exploiting semantic cues to alleviate annotation scarcity. By\nincorporating a ranking-based loss that implicitly calibrates abstinent\ninstances, NBR learns a clearer decision boundary and is instructed to abstain\non uncertain instances. Extensive experiments on three widely-used biomedical\nRE benchmarks, namely ChemProt, DDI and GAD, verify the effectiveness of NBR in\nboth full-set and low-resource regimes. Our analysis demonstrates that indirect\nsupervision benefits biomedical RE even when a domain gap exists, and combining\nNLI knowledge with biomedical knowledge leads to the best performance gains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiashu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingyu Derek Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-05-28T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}