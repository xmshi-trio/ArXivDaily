{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-05-18T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"OOD-Speech: A Large Bengali Speech Recognition Dataset for Out-of-Distribution Benchmarking. (arXiv:2305.09688v1 [eess.AS])","link":"http://arxiv.org/abs/2305.09688","description":"<p>We present OOD-Speech, the first out-of-distribution (OOD) benchmarking\ndataset for Bengali automatic speech recognition (ASR). Being one of the most\nspoken languages globally, Bengali portrays large diversity in dialects and\nprosodic features, which demands ASR frameworks to be robust towards\ndistribution shifts. For example, islamic religious sermons in Bengali are\ndelivered with a tonality that is significantly different from regular speech.\nOur training dataset is collected via massively online crowdsourcing campaigns\nwhich resulted in 1177.94 hours collected and curated from $22,645$ native\nBengali speakers from South Asia. Our test dataset comprises 23.03 hours of\nspeech collected and manually annotated from 17 different sources, e.g.,\nBengali TV drama, Audiobook, Talk show, Online class, and Islamic sermons to\nname a few. OOD-Speech is jointly the largest publicly available speech\ndataset, as well as the first out-of-distribution ASR benchmarking dataset for\nBengali.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rakib_F/0/1/0/all/0/1\">Fazle Rabbi Rakib</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dip_S/0/1/0/all/0/1\">Souhardya Saha Dip</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alam_S/0/1/0/all/0/1\">Samiul Alam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tasnim_N/0/1/0/all/0/1\">Nazia Tasnim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shihab_M/0/1/0/all/0/1\">Md. Istiak Hossain Shihab</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ansary_M/0/1/0/all/0/1\">Md. Nazmuddoha Ansary</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hossen_S/0/1/0/all/0/1\">Syed Mobassir Hossen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meghla_M/0/1/0/all/0/1\">Marsia Haque Meghla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mamun_M/0/1/0/all/0/1\">Mamunur Mamun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sadeque_F/0/1/0/all/0/1\">Farig Sadeque</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chowdhury_S/0/1/0/all/0/1\">Sayma Sultana Chowdhury</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reasat_T/0/1/0/all/0/1\">Tahsin Reasat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sushmit_A/0/1/0/all/0/1\">Asif Sushmit</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Humayun_A/0/1/0/all/0/1\">Ahmed Imtiaz Humayun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Table Pre-training Empowers Models for Tabular Prediction. (arXiv:2305.09696v1 [cs.LG])","link":"http://arxiv.org/abs/2305.09696","description":"<p>Recently, the topic of table pre-training has attracted considerable research\ninterest. However, how to employ table pre-training to boost the performance of\ntabular prediction remains an open challenge. In this paper, we propose TapTap,\nthe first attempt that leverages table pre-training to empower models for\ntabular prediction. After pre-training on a large corpus of real-world tabular\ndata, TapTap can generate high-quality synthetic tables to support various\napplications on tabular data, including privacy protection, low resource\nregime, missing value imputation, and imbalanced classification. Extensive\nexperiments on 12 datasets demonstrate that TapTap outperforms a total of 16\nbaselines in different scenarios. Meanwhile, it can be easily combined with\nvarious backbone models, including LightGBM, Multilayer Perceptron (MLP) and\nTransformer. Moreover, with the aid of table pre-training, models trained using\nsynthetic data generated by TapTap can even compete with models using the\noriginal dataset on half of the experimental datasets, marking a milestone in\nthe development of synthetic tabular data generation. The codes are available\nat https://github.com/ZhangTP1996/TapTap.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaowen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning. (arXiv:2305.09731v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09731","description":"<p>Large language models (LLMs) exploit in-context learning (ICL) to solve tasks\nwith only a few demonstrations, but its mechanisms are not yet well-understood.\nSome works suggest that LLMs only recall already learned concepts from\npre-training, while others hint that ICL performs implicit learning over\ndemonstrations. We characterize two ways through which ICL leverages\ndemonstrations. Task recognition (TR) captures the extent to which LLMs can\nrecognize a task through demonstrations -- even without ground-truth labels --\nand apply their pre-trained priors, whereas task learning (TL) is the ability\nto capture new input-label mappings unseen in pre-training. Using a wide range\nof classification datasets and three LLM families (GPT-3, LLaMA and OPT), we\ndesign controlled experiments to disentangle the roles of TR and TL in ICL. We\nshow that (1) models can achieve non-trivial performance with only TR, and TR\ndoes not further improve with larger models or more demonstrations; (2) LLMs\nacquire TL as the model scales, and TL's performance consistently improves with\nmore demonstrations in context. Our findings unravel two different forces\nbehind ICL and we advocate for discriminating them in future ICL research due\nto their distinct nature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jane Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Howard Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clinical Note Owns its Hierarchy: Multi-Level Hypergraph Neural Networks for Patient-Level Representation Learning. (arXiv:2305.09756v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09756","description":"<p>Leveraging knowledge from electronic health records (EHRs) to predict a\npatient's condition is essential to the effective delivery of appropriate care.\nClinical notes of patient EHRs contain valuable information from healthcare\nprofessionals, but have been underused due to their difficult contents and\ncomplex hierarchies. Recently, hypergraph-based methods have been proposed for\ndocument classifications. Directly adopting existing hypergraph methods on\nclinical notes cannot sufficiently utilize the hierarchy information of the\npatient, which can degrade clinical semantic information by (1) frequent\nneutral words and (2) hierarchies with imbalanced distribution. Thus, we\npropose a taxonomy-aware multi-level hypergraph neural network (TM-HGNN), where\nmulti-level hypergraphs assemble useful neutral words with rare keywords via\nnote and taxonomy level hyperedges to retain the clinical semantic information.\nThe constructed patient hypergraphs are fed into hierarchical message passing\nlayers for learning more balanced multi-level knowledge at the note and\ntaxonomy levels. We validate the effectiveness of TM-HGNN by conducting\nextensive experiments with MIMIC-III dataset on benchmark in-hospital-mortality\nprediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Nayeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piao_Y/0/1/0/all/0/1\">Yinhua Piao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sun Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Video Is Worth 4096 Tokens: Verbalize Story Videos To Understand Them In Zero Shot. (arXiv:2305.09758v1 [cs.CV])","link":"http://arxiv.org/abs/2305.09758","description":"<p>Multimedia content, such as advertisements and story videos, exhibit a rich\nblend of creativity and multiple modalities. They incorporate elements like\ntext, visuals, audio, and storytelling techniques, employing devices like\nemotions, symbolism, and slogans to convey meaning. While previous research in\nmultimedia understanding has focused mainly on videos with specific actions\nlike cooking, there is a dearth of large annotated training datasets, hindering\nthe development of supervised learning models with satisfactory performance for\nreal-world applications. However, the rise of large language models (LLMs) has\nwitnessed remarkable zero-shot performance in various natural language\nprocessing (NLP) tasks, such as emotion classification, question-answering, and\ntopic classification. To bridge this performance gap in multimedia\nunderstanding, we propose verbalizing story videos to generate their\ndescriptions in natural language and then performing video-understanding tasks\non the generated story as opposed to the original video. Through extensive\nexperiments on five video-understanding tasks, we demonstrate that our method,\ndespite being zero-shot, achieves significantly better results than supervised\nbaselines for video understanding. Further, alleviating a lack of story\nunderstanding benchmarks, we publicly release the first dataset on a crucial\ntask in computational social science, persuasion strategy identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Aanisha Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_Y/0/1/0/all/0/1\">Yaman K Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1\">Balaji Krishnamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Application-Agnostic Language Modeling for On-Device ASR. (arXiv:2305.09764v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09764","description":"<p>On-device automatic speech recognition systems face several challenges\ncompared to server-based systems. They have to meet stricter constraints in\nterms of speed, disk size and memory while maintaining the same accuracy. Often\nthey have to serve several applications with different distributions at once,\nsuch as communicating with a virtual assistant and speech-to-text. The simplest\nsolution to serve multiple applications is to build application-specific\n(language) models, but this leads to an increase in memory. Therefore, we\nexplore different data- and architecture-driven language modeling approaches to\nbuild a single application-agnostic model. We propose two novel feed-forward\narchitectures that find an optimal trade off between different on-device\nconstraints. In comparison to the application-specific solution, one of our\nnovel approaches reduces the disk size by half, while maintaining speed and\naccuracy of the original model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nussbaum_Thom_M/0/1/0/all/0/1\">Markus Nu&#xdf;baum-Thom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verwimp_L/0/1/0/all/0/1\">Lyan Verwimp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oualil_Y/0/1/0/all/0/1\">Youssef Oualil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing. (arXiv:2305.09770v1 [cs.HC])","link":"http://arxiv.org/abs/2305.09770","description":"<p>While various AI explanation (XAI) methods have been proposed to interpret AI\nsystems, whether the state-of-the-art XAI methods are practically useful for\nhumans remains inconsistent findings. To improve the usefulness of XAI methods,\na line of studies identifies the gaps between the diverse and dynamic\nreal-world user needs with the status quo of XAI methods. Although prior\nstudies envision mitigating these gaps by integrating multiple XAI methods into\nthe universal XAI interfaces (e.g., conversational or GUI-based XAI systems),\nthere is a lack of work investigating how these systems should be designed to\nmeet practical user needs. In this study, we present ConvXAI, a conversational\nXAI system that incorporates multiple XAI types, and empowers users to request\na variety of XAI questions via a universal XAI dialogue interface.\nParticularly, we innovatively embed practical user needs (i.e., four principles\ngrounding on the formative study) into ConvXAI design to improve practical\nusefulness. Further, we design the domain-specific language (DSL) to implement\nthe essential conversational XAI modules and release the core conversational\nuniversal XAI API for generalization. The findings from two within-subjects\nstudies with 21 users show that ConvXAI is more useful for humans in perceiving\nthe understanding and writing improvement, and improving the writing process in\nterms of productivity and sentence quality. Finally, this work contributes\ninsight into the design space of useful XAI, reveals humans' XAI usage patterns\nwith empirical evidence in practice, and identifies opportunities for future\nuseful XAI work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chieh-Yang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Hao &#x27;Kenneth&#x27; Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification. (arXiv:2305.09781v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09781","description":"<p>The high computational and memory requirements of generative large language\nmodels (LLMs) make it challenging to serve them quickly and cheaply. This paper\nintroduces SpecInfer, an LLM serving system that accelerates generative LLM\ninference with speculative inference and token tree verification. A key insight\nbehind SpecInfer is to combine various collectively boost-tuned small language\nmodels to jointly predict the LLM's outputs; the predictions are organized as a\ntoken tree, whose nodes each represent a candidate token sequence. The\ncorrectness of all candidate token sequences represented by a token tree is\nverified by the LLM in parallel using a novel tree-based parallel decoding\nmechanism. SpecInfer uses an LLM as a token tree verifier instead of an\nincremental decoder, which significantly reduces the end-to-end latency and\ncomputational requirement for serving generative LLMs while provably preserving\nmodel quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miao_X/0/1/0/all/0/1\">Xupeng Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliaro_G/0/1/0/all/0/1\">Gabriele Oliaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xinhao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zeyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_R/0/1/0/all/0/1\">Rae Ying Yee Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuoming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arfeen_D/0/1/0/all/0/1\">Daiyaan Arfeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abhyankar_R/0/1/0/all/0/1\">Reyna Abhyankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhihao Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of Visual Question Answering Algorithms with attention model. (arXiv:2305.09782v1 [cs.CV])","link":"http://arxiv.org/abs/2305.09782","description":"<p>Visual question answering (VQA) usesimage processing algorithms to process\nthe image and natural language processing methods to understand and answer the\nquestion. VQA is helpful to a visually impaired person, can be used for the\nsecurity surveillance system and online chatbots that learn from the web. It\nuses NLP methods to learn the semantic of the question and to derive the\ntextual features. Computer vision techniques are used for generating image\nrepresentation in such a way that they can identify the objects about which\nquestion is asked. The Attention model tries to mimic the human behavior of\ngiving attention to a different region of an image according to our\nunderstanding of its context. This paper critically examines and reviews\nmethods of VQA algorithm such as generation of semantics of text,\nidentification of objects and answer classification techniques that use the\nco-attention approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahir_P/0/1/0/all/0/1\">Param Ahir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diwanji_H/0/1/0/all/0/1\">Hiteishi M. Diwanji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Semantic Concept Embeddings from Contrastively Fine-Tuned Language Models. (arXiv:2305.09785v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09785","description":"<p>Learning vectors that capture the meaning of concepts remains a fundamental\nchallenge. Somewhat surprisingly, perhaps, pre-trained language models have\nthus far only enabled modest improvements to the quality of such concept\nembeddings. Current strategies for using language models typically represent a\nconcept by averaging the contextualised representations of its mentions in some\ncorpus. This is potentially sub-optimal for at least two reasons. First,\ncontextualised word vectors have an unusual geometry, which hampers downstream\ntasks. Second, concept embeddings should capture the semantic properties of\nconcepts, whereas contextualised word vectors are also affected by other\nfactors. To address these issues, we propose two contrastive learning\nstrategies, based on the view that whenever two sentences reveal similar\nproperties, the corresponding contextualised vectors should also be similar.\nOne strategy is fully unsupervised, estimating the properties which are\nexpressed in a sentence from the neighbourhood structure of the contextualised\nword embeddings. The second strategy instead relies on a distant supervision\nsignal from ConceptNet. Our experimental results show that the resulting\nvectors substantially outperform existing concept embeddings in predicting the\nsemantic properties of concepts, with the ConceptNet-based strategy achieving\nthe best results. These findings are furthermore confirmed in a clustering task\nand in the downstream task of ontology completion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Na Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kteich_H/0/1/0/all/0/1\">Hanane Kteich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouraoui_Z/0/1/0/all/0/1\">Zied Bouraoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1\">Steven Schockaert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Ways of Words: The Impact of Word Choice on Information Engagement and Decision Making. (arXiv:2305.09798v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09798","description":"<p>Little research has explored how information engagement (IE), the degree to\nwhich individuals interact with and use information in a manner that manifests\ncognitively, behaviorally, and affectively. This study explored the impact of\nphrasing, specifically word choice, on IE and decision making. Synthesizing two\ntheoretical models, User Engagement Theory UET and Information Behavior Theory\nIBT, a theoretical framework illustrating the impact of and relationships among\nthe three IE dimensions of perception, participation, and perseverance was\ndeveloped and hypotheses generated. The framework was empirically validated in\na large-scale user study measuring how word choice impacts the dimensions of\nIE. The findings provide evidence that IE differs from other forms of\nengagement in that it is driven and fostered by the expression of the\ninformation itself, regardless of the information system used to view, interact\nwith, and use the information. The findings suggest that phrasing can have a\nsignificant effect on the interpretation of and interaction with digital\ninformation, indicating the importance of expression of information, in\nparticular word choice, on decision making and IE. The research contributes to\nthe literature by identifying methods for assessment and improvement of IE and\ndecision making with digital text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dvir_N/0/1/0/all/0/1\">Nimrod Dvir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_E/0/1/0/all/0/1\">Elaine Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Commuri_S/0/1/0/all/0/1\">Suraj Commuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romano_J/0/1/0/all/0/1\">Jennifer Romano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mirages: On Anthropomorphism in Dialogue Systems. (arXiv:2305.09800v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09800","description":"<p>Automated dialogue or conversational systems are anthropomorphised by\ndevelopers and personified by users. While a degree of anthropomorphism is\ninevitable, conscious and unconscious design choices can guide users to\npersonify them to varying degrees. Encouraging users to relate to automated\nsystems as if they were human can lead to transparency and trust issues, and\nhigh risk scenarios caused by over-reliance on their outputs. As a result,\nnatural language processing researchers have begun to investigate factors that\ninduce personification and develop resources to mitigate such effects. However,\nthese efforts are fragmented, and many aspects of anthropomorphism have yet to\nbe considered. In this paper, we discuss the linguistic factors that contribute\nto the anthropomorphism of dialogue systems and the harms that can arise,\narguing that it can reinforce stereotypes of gender roles and notions of\nacceptable language. We recommend that future efforts towards developing\ndialogue systems take particular care in their design, development, release,\nand description; and attend to the many linguistic cues that can elicit\npersonification by users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abercrombie_G/0/1/0/all/0/1\">Gavin Abercrombie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curry_A/0/1/0/all/0/1\">Amanda Cercas Curry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinkar_T/0/1/0/all/0/1\">Tanvi Dinkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talat_Z/0/1/0/all/0/1\">Zeerak Talat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Dataset Transferability in Active Learning for Transformers. (arXiv:2305.09807v1 [cs.LG])","link":"http://arxiv.org/abs/2305.09807","description":"<p>Active learning (AL) aims to reduce labeling costs by querying the examples\nmost beneficial for model learning. While the effectiveness of AL for\nfine-tuning transformer-based pre-trained language models (PLMs) has been\ndemonstrated, it is less clear to what extent the AL gains obtained with one\nmodel transfer to others. We consider the problem of transferability of\nactively acquired datasets in text classification and investigate whether AL\ngains persist when a dataset built using AL coupled with a specific PLM is used\nto train a different PLM. We link the AL dataset transferability to the\nsimilarity of instances queried by the different PLMs and show that AL methods\nwith similar acquisition sequences produce highly transferable datasets\nregardless of the models used. Additionally, we show that the similarity of\nacquisition sequences is influenced more by the choice of the AL method than\nthe choice of the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jelenic_F/0/1/0/all/0/1\">Fran Jeleni&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jukic_J/0/1/0/all/0/1\">Josip Juki&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drobac_N/0/1/0/all/0/1\">Nina Drobac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snajder_J/0/1/0/all/0/1\">Jan &#x160;najder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPL-NoViD: Context-Aware Prompt-based Learning for Norm Violation Detection in Online Communities. (arXiv:2305.09846v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09846","description":"<p>Detecting norm violations in online communities is critical to maintaining\nhealthy and safe spaces for online discussions. Existing machine learning\napproaches often struggle to adapt to the diverse rules and interpretations\nacross different communities due to the inherent challenges of fine-tuning\nmodels for such context-specific tasks. In this paper, we introduce\nContext-aware Prompt-based Learning for Norm Violation Detection (CPL-NoViD), a\nnovel method that employs prompt-based learning to detect norm violations\nacross various types of rules. CPL-NoViD outperforms the baseline by\nincorporating context through natural language prompts and demonstrates\nimproved performance across different rule types. Significantly, it not only\nexcels in cross-rule-type and cross-community norm violation detection but also\nexhibits adaptability in few-shot learning scenarios. Most notably, it\nestablishes a new state-of-the-art in norm violation detection, surpassing\nexisting benchmarks. Our work highlights the potential of prompt-based learning\nfor context-sensitive norm violation detection and paves the way for future\nresearch on more adaptable, context-aware models to better support online\ncommunity moderators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zihao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerman_K/0/1/0/all/0/1\">Kristina Lerman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoEdIT: Text Editing by Task-Specific Instruction Tuning. (arXiv:2305.09857v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09857","description":"<p>Text editing or revision is an essential function of the human writing\nprocess. Understanding the capabilities of LLMs for making high-quality\nrevisions and collaborating with human writers is a critical step toward\nbuilding effective writing assistants. With the prior success of LLMs and\ninstruction tuning, we leverage instruction-tuned LLMs for text revision to\nimprove the quality of user-generated text and improve the efficiency of the\nprocess. We introduce CoEdIT, a state-of-the-art text editing model for writing\nassistance. CoEdIT takes instructions from the user specifying the attributes\nof the desired text, such as \"Make the sentence simpler\" or \"Write it in a more\nneutral style,\" and outputs the edited text. We present a large language model\nfine-tuned on a diverse collection of task-specific instructions for text\nediting (a total of 82K instructions). Our model (1) achieves state-of-the-art\nperformance on various text editing benchmarks, (2) is competitive with\npublicly available largest-sized LLMs trained on instructions while being\n$\\sim$60x smaller, (3) is capable of generalizing to unseen edit instructions,\nand (4) exhibits compositional comprehension abilities to generalize to\ninstructions containing different combinations of edit actions. Through\nextensive qualitative and quantitative analysis, we show that writers prefer\nthe edits suggested by CoEdIT, relative to other state-of-the-art text editing\nmodels. Our code and dataset are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raheja_V/0/1/0/all/0/1\">Vipul Raheja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1\">Dhruv Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_R/0/1/0/all/0/1\">Ryan Koo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph Completion Models are Few-shot Learners: An Empirical Study of Relation Labeling in E-commerce with LLMs. (arXiv:2305.09858v1 [cs.IR])","link":"http://arxiv.org/abs/2305.09858","description":"<p>Knowledge Graphs (KGs) play a crucial role in enhancing e-commerce system\nperformance by providing structured information about entities and their\nrelationships, such as complementary or substitutable relations between\nproducts or product types, which can be utilized in recommender systems.\nHowever, relation labeling in KGs remains a challenging task due to the dynamic\nnature of e-commerce domains and the associated cost of human labor. Recently,\nbreakthroughs in Large Language Models (LLMs) have shown surprising results in\nnumerous natural language processing tasks. In this paper, we conduct an\nempirical study of LLMs for relation labeling in e-commerce KGs, investigating\ntheir powerful learning capabilities in natural language and effectiveness in\npredicting relations between product types with limited labeled data. We\nevaluate various LLMs, including PaLM and GPT-3.5, on benchmark datasets,\ndemonstrating their ability to achieve competitive performance compared to\nhumans on relation labeling tasks using just 1 to 5 labeled examples per\nrelation. Additionally, we experiment with different prompt engineering\ntechniques to examine their impact on model performance. Our results show that\nLLMs significantly outperform existing KG completion models in relation\nlabeling for e-commerce KGs and exhibit performance strong enough to replace\nhuman labeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Luyi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakurdesai_N/0/1/0/all/0/1\">Nikhil Thakurdesai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jianpeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jason H.D. Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nag_K/0/1/0/all/0/1\">Kaushiki Nag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korpeoglu_E/0/1/0/all/0/1\">Evren Korpeoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sushant Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achan_K/0/1/0/all/0/1\">Kannan Achan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smaller Language Models are Better Black-box Machine-Generated Text Detectors. (arXiv:2305.09859v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09859","description":"<p>With the advent of fluent generative language models that can produce\nconvincing utterances very similar to those written by humans, distinguishing\nwhether a piece of text is machine-generated or human-written becomes more\nchallenging and more important, as such models could be used to spread\nmisinformation, fake news, fake reviews and to mimic certain authors and\nfigures. To this end, there have been a slew of methods proposed to detect\nmachine-generated text. Most of these methods need access to the logits of the\ntarget model or need the ability to sample from the target. One such black-box\ndetection method relies on the observation that generated text is locally\noptimal under the likelihood function of the generator, while human-written\ntext is not. We find that overall, smaller and partially-trained models are\nbetter universal text detectors: they can more precisely detect text generated\nfrom both small and larger models. Interestingly, we find that whether the\ndetector and generator were trained on the same data is not critically\nimportant to the detection success. For instance the OPT-125M model has an AUC\nof 0.81 in detecting ChatGPT generations, whereas a larger model from the GPT\nfamily, GPTJ-6B, has AUC of 0.45.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattern_J/0/1/0/all/0/1\">Justus Mattern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Sicun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shokri_R/0/1/0/all/0/1\">Reza Shokri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Epsilon Sampling Rocks: Investigating Sampling Strategies for \\\\Minimum Bayes Risk Decoding for Machine Translation. (arXiv:2305.09860v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09860","description":"<p>Recent advances in machine translation (MT) have shown that Minimum Bayes\nRisk (MBR) decoding can be a powerful alternative to beam search decoding,\nespecially when combined with neural-based utility functions. However, the\nperformance of MBR decoding depends heavily on how and how many candidates are\nsampled from the model. In this paper, we explore how different sampling\napproaches for generating candidate lists for MBR decoding affect performance.\nWe evaluate popular sampling approaches, such as ancestral, nucleus, and top-k\nsampling. Based on our insights into their limitations, we experiment with the\nrecently proposed epsilon-sampling approach, which prunes away all tokens with\na probability smaller than epsilon, ensuring that each token in a sample\nreceives a fair probability mass. Through extensive human evaluations, we\ndemonstrate that MBR decoding based on epsilon-sampling significantly\noutperforms not only beam search decoding, but also MBR decoding with all other\ntested sampling methods across four language pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1\">Markus Freitag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghorbani_B/0/1/0/all/0/1\">Behrooz Ghorbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1\">Patrick Fernandes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining black box text modules in natural language with language models. (arXiv:2305.09863v1 [cs.AI])","link":"http://arxiv.org/abs/2305.09863","description":"<p>Large language models (LLMs) have demonstrated remarkable prediction\nperformance for a growing array of tasks. However, their rapid proliferation\nand increasing opaqueness have created a growing need for interpretability.\nHere, we ask whether we can automatically obtain natural language explanations\nfor black box text modules. A \"text module\" is any function that maps text to a\nscalar continuous value, such as a submodule within an LLM or a fitted model of\na brain region. \"Black box\" indicates that we only have access to the module's\ninputs/outputs.\n</p>\n<p>We introduce Summarize and Score (SASC), a method that takes in a text module\nand returns a natural language explanation of the module's selectivity along\nwith a score for how reliable the explanation is. We study SASC in 3 contexts.\nFirst, we evaluate SASC on synthetic modules and find that it often recovers\nground truth explanations. Second, we use SASC to explain modules found within\na pre-trained BERT model, enabling inspection of the model's internals.\nFinally, we show that SASC can generate explanations for the response of\nindividual fMRI voxels to language stimuli, with potential applications to\nfine-grained brain mapping. All code for using SASC and reproducing results is\nmade available on Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_C/0/1/0/all/0/1\">Chandan Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_A/0/1/0/all/0/1\">Aliyah R. Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antonello_R/0/1/0/all/0/1\">Richard Antonello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Shailee Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huth_A/0/1/0/all/0/1\">Alexander G. Huth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Jaseci Programming Paradigm and Runtime Stack: Building Scale-out Production Applications Easy and Fast. (arXiv:2305.09864v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09864","description":"<p>Today's production scale-out applications include many sub-application\ncomponents, such as storage backends, logging infrastructure and AI models.\nThese components have drastically different characteristics, are required to\nwork in collaboration, and interface with each other as microservices. This\nleads to increasingly high complexity in developing, optimizing, configuring,\nand deploying scale-out applications, raising the barrier to entry for most\nindividuals and small teams. We developed a novel co-designed runtime system,\nJaseci, and programming language, Jac, which aims to reduce this complexity.\nThe key design principle throughout Jaseci's design is to raise the level of\nabstraction by moving as much of the scale-out data management, microservice\ncomponentization, and live update complexity into the runtime stack to be\nautomated and optimized automatically. We use real-world AI applications to\ndemonstrate Jaseci's benefit for application performance and developer\nproductivity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mars_j/0/1/0/all/0/1\">jason Mars</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yiping Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daynauth_R/0/1/0/all/0/1\">Roland Daynauth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baichuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahendra_A/0/1/0/all/0/1\">Ashish Mahendra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flautner_K/0/1/0/all/0/1\">Krisztian Flautner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+tang_L/0/1/0/all/0/1\">Lingjia tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Similarity Measure of Natural Language Text through Machine Learning and a Keyword-Aware Cross-Encoder-Ranking Summarizer -- A Case Study Using UCGIS GIS&T Body of Knowledge. (arXiv:2305.09877v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09877","description":"<p>Initiated by the University Consortium of Geographic Information Science\n(UCGIS), GIS&amp;T Body of Knowledge (BoK) is a community-driven endeavor to\ndefine, develop, and document geospatial topics related to geographic\ninformation science and technologies (GIS&amp;T). In recent years, GIS&amp;T BoK has\nundergone rigorous development in terms of its topic re-organization and\ncontent updating, resulting in a new digital version of the project. While the\nBoK topics provide useful materials for researchers and students to learn about\nGIS, the semantic relationships among the topics, such as semantic similarity,\nshould also be identified so that a better and automated topic navigation can\nbe achieved. Currently, the related topics are either defined manually by\neditors or authors, which may result in an incomplete assessment of topic\nrelationship. To address this challenge, our research evaluates the\neffectiveness of multiple natural language processing (NLP) techniques in\nextracting semantics from text, including both deep neural networks and\ntraditional machine learning approaches. Besides, a novel text summarization -\nKACERS (Keyword-Aware Cross-Encoder-Ranking Summarizer) - is proposed to\ngenerate a semantic summary of scientific publications. By identifying the\nsemantic linkages among key topics, this work provides guidance for future\ndevelopment and content organization of the GIS&amp;T BoK project. It also offers a\nnew perspective on the use of machine learning techniques for analyzing\nscientific publications, and demonstrate the potential of KACERS summarizer in\nsemantic understanding of long text documents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuanyuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenwen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sizhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1\">Zhining Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clustering-Aware Negative Sampling for Unsupervised Sentence Representation. (arXiv:2305.09892v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09892","description":"<p>Contrastive learning has been widely studied in sentence representation\nlearning. However, earlier works mainly focus on the construction of positive\nexamples, while in-batch samples are often simply treated as negative examples.\nThis approach overlooks the importance of selecting appropriate negative\nexamples, potentially leading to a scarcity of hard negatives and the inclusion\nof false negatives. To address these issues, we propose ClusterNS\n(Clustering-aware Negative Sampling), a novel method that incorporates cluster\ninformation into contrastive learning for unsupervised sentence representation\nlearning. We apply a modified K-means clustering algorithm to supply hard\nnegatives and recognize in-batch false negatives during training, aiming to\nsolve the two issues in one unified framework. Experiments on semantic textual\nsimilarity (STS) tasks demonstrate that our proposed ClusterNS compares\nfavorably with baselines in unsupervised sentence representation learning. Our\ncode has been made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jinghao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_F/0/1/0/all/0/1\">Fanqi Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1\">Xiaojun Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balancing Lexical and Semantic Quality in Abstractive Summarization. (arXiv:2305.09898v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09898","description":"<p>An important problem of the sequence-to-sequence neural models widely used in\nabstractive summarization is exposure bias. To alleviate this problem,\nre-ranking systems have been applied in recent years. Despite some performance\nimprovements, this approach remains underexplored. Previous works have mostly\nspecified the rank through the ROUGE score and aligned candidate summaries, but\nthere can be quite a large gap between the lexical overlap metric and semantic\nsimilarity. In this paper, we propose a novel training method in which a\nre-ranker balances the lexical and semantic quality. We further newly define\nfalse positives in ranking and present a strategy to reduce their influence.\nExperiments on the CNN/DailyMail and XSum datasets show that our method can\nestimate the meaning of summaries without seriously degrading the lexical\naspect. More specifically, it achieves an 89.67 BERTScore on the CNN/DailyMail\ndataset, reaching new state-of-the-art performance. Our code is publicly\navailable at https://github.com/jeewoo1025/BalSum.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sul_J/0/1/0/all/0/1\">Jeewoo Sul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yong Suk Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Equivariant Few-Shot Learning from Pretrained Models. (arXiv:2305.09900v1 [cs.LG])","link":"http://arxiv.org/abs/2305.09900","description":"<p>Efficient transfer learning algorithms are key to the success of foundation\nmodels on diverse downstream tasks even with limited data. Recent works of\n\\cite{basu2022equi} and \\cite{kaba2022equivariance} propose group averaging\n(\\textit{equitune}) and optimization-based methods, respectively, over features\nfrom group-transformed inputs to obtain equivariant outputs from\nnon-equivariant neural networks. While \\cite{kaba2022equivariance} are only\nconcerned with training from scratch, we find that equitune performs poorly on\nequivariant zero-shot tasks despite good finetuning results. We hypothesize\nthat this is because pretrained models provide better quality features for\ncertain transformations than others and simply averaging them is deleterious.\nHence, we propose $\\lambda$-\\textit{equitune} that averages the features using\n\\textit{importance weights}, $\\lambda$s. These weights are learned directly\nfrom the data using a small neural network, leading to excellent zero-shot and\nfinetuned results that outperform equitune. Further, we prove that\n$\\lambda$-equitune is equivariant and a universal approximator of equivariant\nfunctions. Additionally, we show that the method of \\cite{kaba2022equivariance}\nused with appropriate loss functions, which we call \\textit{equizero}, also\ngives excellent zero-shot and finetuned performance. Both equitune and equizero\nare special cases of $\\lambda$-equitune. To show the simplicity and generality\nof our method, we validate on a wide range of diverse applications and models\nsuch as 1) image classification using CLIP, 2) deep Q-learning, 3) fairness in\nnatural language generation (NLG), 4) compositional generalization in\nlanguages, and 5) image classification using pretrained CNNs such as Resnet and\nAlexnet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1\">Sourya Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katdare_P/0/1/0/all/0/1\">Pulkit Katdare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattigeri_P/0/1/0/all/0/1\">Prasanna Sattigeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chenthamarakshan_V/0/1/0/all/0/1\">Vijil Chenthamarakshan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driggs_Campbell_K/0/1/0/all/0/1\">Katherine Driggs-Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Payel Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varshney_L/0/1/0/all/0/1\">Lav R. Varshney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"I'm fully who I am\": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09941","description":"<p>Transgender and non-binary (TGNB) individuals disproportionately experience\ndiscrimination and exclusion from daily life. Given the recent popularity and\nadoption of language generation technologies, the potential to further\nmarginalize this population only grows. Although a multitude of NLP fairness\nliterature focuses on illuminating and addressing gender biases, assessing\ngender harms for TGNB identities requires understanding how such identities\nuniquely interact with societal gender norms and how they differ from gender\nbinary-centric perspectives. Such measurement frameworks inherently require\ncentering TGNB voices to help guide the alignment between gender-inclusive NLP\nand whom they are intended to serve. Towards this goal, we ground our work in\nthe TGNB community and existing interdisciplinary literature to assess how the\nsocial reality surrounding experienced marginalization by TGNB persons\ncontributes to and persists within Open Language Generation (OLG). By first\nunderstanding their marginalization stressors, we evaluate (1) misgendering and\n(2) harmful responses to gender disclosure. To do this, we introduce the TANGO\ndataset, comprising of template-based text curated from real-world text within\na TGNB-oriented community. We discover a dominance of binary gender norms\nwithin the models; LLMs least misgendered subjects in generated text when\ntriggered by prompts whose subjects used binary pronouns. Meanwhile,\nmisgendering was most prevalent when triggering generation with singular they\nand neopronouns. When prompted with gender disclosures, LLM text contained\nstigmatizing language and scored most toxic when triggered by TGNB gender\ndisclosure. Our findings warrant further research on how TGNB harms manifest in\nLLMs and serve as a broader case study toward concretely grounding the design\nof gender-inclusive AI in community voices and interdisciplinary literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ovalle_A/0/1/0/all/0/1\">Anaelia Ovalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Palash Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhamala_J/0/1/0/all/0/1\">Jwala Dhamala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggers_Z/0/1/0/all/0/1\">Zachary Jaggers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CooK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge. (arXiv:2305.09955v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09955","description":"<p>Large language models (LLMs) are increasingly adopted for knowledge-intensive\ntasks and contexts. Existing approaches improve the knowledge capabilities of\ngeneral-purpose LLMs through retrieval or generated knowledge prompting, but\nthey fall short of reflecting two key properties of knowledge-rich models:\nknowledge should be modular, ever-growing, sourced from diverse domains;\nknowledge acquisition and production should be a collaborative process, where\ndiverse stakeholders contribute new information. To this end, we propose CooK,\na novel framework to empower general-purpose large language models with modular\nand collaboratively sourced knowledge. We first introduce specialized language\nmodels, autoregressive models trained on corpora from a wide range of domains\nand sources. These specialized LMs serve as parametric knowledge repositories\nthat are later prompted to generate background knowledge for general-purpose\nLLMs. We then propose three knowledge filters to dynamically select and retain\ninformation in generated documents by controlling for relevance, brevity, and\nfactuality. Finally, we propose bottom-up and top-down knowledge integration\napproaches to augment general-purpose LLMs with the curated (relevant, factual)\nknowledge from community-driven specialized LMs that enable multi-domain\nknowledge synthesis and on-demand knowledge requests. Through extensive\nexperiments, we demonstrate that CooK achieves state-of-the-art performance on\nsix benchmark datasets. Our results highlight the potential of enriching\ngeneral-purpose LLMs with evolving and modular knowledge -- relevant knowledge\nthat can be continuously updated through the collective efforts of the research\ncommunity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shangbin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weijia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yuyang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balachandran_V/0/1/0/all/0/1\">Vidhisha Balachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianxing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smart Word Suggestions for Writing Assistance. (arXiv:2305.09975v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09975","description":"<p>Enhancing word usage is a desired feature for writing assistance. To further\nadvance research in this area, this paper introduces \"Smart Word Suggestions\"\n(SWS) task and benchmark. Unlike other works, SWS emphasizes end-to-end\nevaluation and presents a more realistic writing assistance scenario. This task\ninvolves identifying words or phrases that require improvement and providing\nsubstitution suggestions. The benchmark includes human-labeled data for\ntesting, a large distantly supervised dataset for training, and the framework\nfor evaluation. The test data includes 1,000 sentences written by English\nlearners, accompanied by over 16,000 substitution suggestions annotated by 10\nnative speakers. The training dataset comprises over 3.7 million sentences and\n12.7 million suggestions generated through rules. Our experiments with seven\nbaselines demonstrate that SWS is a challenging task. Based on experimental\nanalysis, we suggest potential directions for future research on SWS. The\ndataset and related codes is available at\nhttps://github.com/microsoft/SmartWordSuggestions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenshuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shaoguang Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenshan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tien_J/0/1/0/all/0/1\">Jonathan Tien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Semantic Knowledge Composed Multimodal Dialog Systems. (arXiv:2305.09990v1 [cs.CL])","link":"http://arxiv.org/abs/2305.09990","description":"<p>Textual response generation is an essential task for multimodal task-oriented\ndialog systems.Although existing studies have achieved fruitful progress, they\nstill suffer from two critical limitations: 1) focusing on the attribute\nknowledge but ignoring the relation knowledge that can reveal the correlations\nbetween different entities and hence promote the response generation}, and 2)\nonly conducting the cross-entropy loss based output-level supervision but\nlacking the representation-level regularization. To address these limitations,\nwe devise a novel multimodal task-oriented dialog system (named MDS-S2).\nSpecifically, MDS-S2 first simultaneously acquires the context related\nattribute and relation knowledge from the knowledge base, whereby the\nnon-intuitive relation knowledge is extracted by the n-hop graph walk.\nThereafter, considering that the attribute knowledge and relation knowledge can\nbenefit the responding to different levels of questions, we design a\nmulti-level knowledge composition module in MDS-S2 to obtain the latent\ncomposed response representation. Moreover, we devise a set of latent query\nvariables to distill the semantic information from the composed response\nrepresentation and the ground truth response representation, respectively, and\nthus conduct the representation-level semantic regularization. Extensive\nexperiments on a public dataset have verified the superiority of our proposed\nMDS-S2. We have released the codes and parameters to facilitate the research\ncommunity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaolin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xuemeng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yinwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling. (arXiv:2305.09993v1 [cs.LG])","link":"http://arxiv.org/abs/2305.09993","description":"<p>We introduce Reprompting, an iterative sampling algorithm that searches for\nthe Chain-of-Thought (CoT) recipes for a given task without human intervention.\nThrough Gibbs sampling, we infer CoT recipes that work consistently well for a\nset of training samples. Our method iteratively samples new recipes using\npreviously sampled solutions as parent prompts to solve other training\nproblems. On five Big-Bench Hard tasks that require multi-step reasoning,\nReprompting achieves consistently better performance than the zero-shot,\nfew-shot, and human-written CoT baselines. Reprompting can also facilitate\ntransfer of knowledge from a stronger model to a weaker model leading to\nsubstantially improved performance of the weaker model. Overall, Reprompting\nbrings up to +17 point improvements over the previous state-of-the-art method\nthat uses human-written CoT prompts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weijia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banburski_Fahey_A/0/1/0/all/0/1\">Andrzej Banburski-Fahey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jojic_N/0/1/0/all/0/1\">Nebojsa Jojic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DinoSR: Self-Distillation and Online Clustering for Self-supervised Speech Representation Learning. (arXiv:2305.10005v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10005","description":"<p>In this paper, we introduce self-distillation and online clustering for\nself-supervised speech representation learning (DinoSR) which combines masked\nlanguage modeling, self-distillation, and online clustering. We show that these\nconcepts complement each other and result in a strong representation learning\nmodel for speech. DinoSR first extracts contextualized embeddings from the\ninput audio with a teacher network, then runs an online clustering system on\nthe embeddings to yield a machine-discovered phone inventory, and finally uses\nthe discretized tokens to guide a student network. We show that DinoSR\nsurpasses previous state-of-the-art performance in several downstream tasks,\nand provide a detailed analysis of the model and the learned discrete units.\nThe source code will be made available after the anonymity period.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alexander H. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James R. Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EfficientSCI: Densely Connected Network with Space-time Factorization for Large-scale Video Snapshot Compressive Imaging. (arXiv:2305.10006v1 [cs.CV])","link":"http://arxiv.org/abs/2305.10006","description":"<p>Video snapshot compressive imaging (SCI) uses a two-dimensional detector to\ncapture consecutive video frames during a single exposure time. Following this,\nan efficient reconstruction algorithm needs to be designed to reconstruct the\ndesired video frames. Although recent deep learning-based state-of-the-art\n(SOTA) reconstruction algorithms have achieved good results in most tasks, they\nstill face the following challenges due to excessive model complexity and GPU\nmemory limitations:\n</p>\n<p>1) these models need high computational cost, and\n</p>\n<p>2) they are usually unable to reconstruct large-scale video frames at high\ncompression ratios.\n</p>\n<p>To address these issues, we develop an {\\bf{\\em efficient network}} for video\nSCI by using {\\bf {\\em dense connections and space-time factorization\nmechanism}} within a single residual block, dubbed {\\bf \\emph{EfficientSCI}}.\nThe EfficientSCI network can well establish spatial-temporal correlation by\nusing {\\bf {\\em convolution in the spatial domain and Transformer in the\ntemporal domain}}, respectively. We are the first time to show that an UHD\ncolor video with high compression ratio can be reconstructed from a snapshot 2D\nmeasurement using a single end-to-end deep learning model with PSNR above 32\ndB. Extensive results on both simulation and real data show that our method\nsignificantly outperforms all previous SOTA algorithms with better real-time\nperformance. The code is at\n\\url{https://github.com/ucaswangls/EfficientSCI.git}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lishun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Miao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AD-KD: Attribution-Driven Knowledge Distillation for Language Model Compression. (arXiv:2305.10010v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10010","description":"<p>Knowledge distillation has attracted a great deal of interest recently to\ncompress pre-trained language models. However, existing knowledge distillation\nmethods suffer from two limitations. First, the student model simply imitates\nthe teacher's behavior while ignoring the underlying reasoning. Second, these\nmethods usually focus on the transfer of sophisticated model-specific knowledge\nbut overlook data-specific knowledge. In this paper, we present a novel\nattribution-driven knowledge distillation approach, which explores the\ntoken-level rationale behind the teacher model based on Integrated Gradients\n(IG) and transfers attribution knowledge to the student model. To enhance the\nknowledge transfer of model reasoning and generalization, we further explore\nmulti-view attribution distillation on all potential decisions of the teacher.\nComprehensive experiments are conducted with BERT on the GLUE benchmark. The\nexperimental results demonstrate the superior performance of our approach to\nseveral state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Siyue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongzhan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1\">Xiaojun Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Gradient Descent Meets Derivative-Free Optimization: A Match Made in Black-Box Scenario. (arXiv:2305.10013v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10013","description":"<p>Large pre-trained language models (PLMs) have garnered significant attention\nfor their versatility and potential for solving a wide spectrum of natural\nlanguage processing (NLP) tasks. However, the cost of running these PLMs may be\nprohibitive. Furthermore, PLMs may not be open-sourced due to commercial\nconsiderations and potential risks of misuse, such as GPT-3. The parameters and\ngradients of PLMs are unavailable in this scenario. To solve the issue,\nblack-box tuning has been proposed, which utilizes derivative-free optimization\n(DFO), instead of gradient descent, for training task-specific continuous\nprompts. However, these gradient-free methods still exhibit a significant gap\ncompared to gradient-based methods. In this paper, we introduce gradient\ndescent into black-box tuning scenario through knowledge distillation.\nFurthermore, we propose a novel method GDFO, which integrates gradient descent\nand derivative-free optimization to optimize task-specific continuous prompts\nin a harmonized manner. Experimental results show that GDFO can achieve\nsignificant performance gains over previous state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chengcheng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Liqing Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Renyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qiushi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Ming Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark. (arXiv:2305.10036v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10036","description":"<p>Large language models (LLMs) have demonstrated powerful capabilities in both\ntext understanding and generation. Companies have begun to offer Embedding as a\nService (EaaS) based on these LLMs, which can benefit various natural language\nprocessing (NLP) tasks for customers. However, previous studies have shown that\nEaaS is vulnerable to model extraction attacks, which can cause significant\nlosses for the owners of LLMs, as training these models is extremely expensive.\nTo protect the copyright of LLMs for EaaS, we propose an Embedding Watermark\nmethod called EmbMarker that implants backdoors on embeddings. Our method\nselects a group of moderate-frequency words from a general text corpus to form\na trigger set, then selects a target embedding as the watermark, and inserts it\ninto the embeddings of texts containing trigger words as the backdoor. The\nweight of insertion is proportional to the number of trigger words included in\nthe text. This allows the watermark backdoor to be effectively transferred to\nEaaS-stealer's model for copyright verification while minimizing the adverse\nimpact on the original embeddings' utility. Our extensive experiments on\nvarious datasets show that our method can effectively protect the copyright of\nEaaS models without compromising service quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wenjun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jingwei Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shangxi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lingjuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_B/0/1/0/all/0/1\">Binxing Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangzhong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Language Models Solve Graph Problems in Natural Language?. (arXiv:2305.10037v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10037","description":"<p>Large language models (LLMs) are increasingly adopted for a variety of tasks\nwith implicit graphical structures, such as planning in robotics, multi-hop\nquestion answering or knowledge probing, structured commonsense reasoning, and\nmore. While LLMs have advanced the state-of-the-art on these tasks with\nstructure implications, whether LLMs could explicitly process textual\ndescriptions of graphs and structures, map them to grounded conceptual spaces,\nand perform structured operations remains underexplored. To this end, we\npropose NLGraph (Natural Language Graph), a comprehensive benchmark of\ngraph-based problem solving designed in natural language. NLGraph contains\n29,370 problems, covering eight graph reasoning tasks with varying complexity\nfrom simple tasks such as connectivity and shortest path up to complex problems\nsuch as maximum flow and simulating graph neural networks. We evaluate LLMs\n(GPT-3/4) with various prompting approaches on the NLGraph benchmark and find\nthat 1) language models do demonstrate preliminary graph reasoning abilities,\n2) the benefit of advanced prompting and in-context learning diminishes on more\ncomplex graph problems, while 3) LLMs are also (un)surprisingly brittle in the\nface of spurious correlations in graph and problem settings. We then propose\nBuild-a-Graph Prompting and Algorithmic Prompting, two instruction-based\napproaches to enhance LLMs in solving natural language graph problems.\nBuild-a-Graph and Algorithmic prompting improve the performance of LLMs on\nNLGraph by 3.07% to 16.85% across multiple tasks and settings, while how to\nsolve the most complicated graph reasoning tasks in our setup with language\nmodels remains an open research question. The NLGraph benchmark and evaluation\ncode are available at https://github.com/Arthur-Heng/NLGraph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Heng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shangbin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianxing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhaoxuan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaochuang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing the Role of Positional Information in Vision-Language Models. (arXiv:2305.10046v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10046","description":"<p>In most Vision-Language models (VL), the understanding of the image structure\nis enabled by injecting the position information (PI) about objects in the\nimage. In our case study of LXMERT, a state-of-the-art VL model, we probe the\nuse of the PI in the representation and study its effect on Visual Question\nAnswering. We show that the model is not capable of leveraging the PI for the\nimage-text matching task on a challenge set where only position differs. Yet,\nour experiments with probing confirm that the PI is indeed present in the\nrepresentation. We introduce two strategies to tackle this: (i) Positional\nInformation Pre-training and (ii) Contrastive Learning on PI using\nCross-Modality Matching. Doing so, the model can correctly classify if images\nwith detailed PI statements match. Additionally to the 2D information from\nbounding boxes, we introduce the object's depth as new feature for a better\nobject localization in the space. Even though we were able to improve the model\nproperties as defined by our probes, it only has a negligible effect on the\ndownstream performance. Our results thus highlight an important issue of\nmultimodal modeling: the mere presence of information detectable by a probing\nclassifier is not a guarantee that the information is available in a\ncross-modal setup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosch_P/0/1/0/all/0/1\">Philipp J. R&#xf6;sch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Libovicky_J/0/1/0/all/0/1\">Jind&#x159;ich Libovick&#xfd;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Use of a Taxonomy of Empathetic Response Intents to Control and Interpret Empathy in Neural Chatbots. (arXiv:2305.10096v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10096","description":"<p>A recent trend in the domain of open-domain conversational agents is enabling\nthem to converse empathetically to emotional prompts. Current approaches either\nfollow an end-to-end approach or condition the responses on similar emotion\nlabels to generate empathetic responses. But empathy is a broad concept that\nrefers to the cognitive and emotional reactions of an individual to the\nobserved experiences of another and it is more complex than mere mimicry of\nemotion. Hence, it requires identifying complex human conversational strategies\nand dynamics in addition to generic emotions to control and interpret\nempathetic responding capabilities of chatbots. In this work, we make use of a\ntaxonomy of eight empathetic response intents in addition to generic emotion\ncategories in building a dialogue response generation model capable of\ngenerating empathetic responses in a controllable and interpretable manner. It\nconsists of two modules: 1) a response emotion/intent prediction module; and 2)\na response generation module. We propose several rule-based and neural\napproaches to predict the next response's emotion/intent and generate responses\nconditioned on these predicted emotions/intents. Automatic and human evaluation\nresults emphasize the importance of the use of the taxonomy of empathetic\nresponse intents in producing more diverse and empathetically more appropriate\nresponses than end-to-end models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Welivita_A/0/1/0/all/0/1\">Anuradha Welivita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_P/0/1/0/all/0/1\">Pearl Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empirical Analysis of Oral and Nasal Vowels of Konkani. (arXiv:2305.10122v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10122","description":"<p>Konkani is a highly nasalised language which makes it unique among Indo-Aryan\nlanguages. This work investigates the acoustic-phonetic properties of Konkani\noral and nasal vowels. For this study, speech samples from six speakers (3 male\nand 3 female) were collected. A total of 74 unique sentences were used as a\npart of the recording script, 37 each for oral and nasal vowels, respectively.\nThe final data set consisted of 1135 vowel phonemes. A comparative F1-F2 plot\nof Konkani oral and nasal vowels is presented with an experimental result and\nformant analysis. The average F1, F2 and F3 values are also reported for the\nfirst time through experimentation for all nasal and oral vowels. This study\ncan be helpful for the linguistic research on vowels and speech synthesis\nsystems specific to the Konkani language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fadte_S/0/1/0/all/0/1\">Swapnil Fadte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaz_E/0/1/0/all/0/1\">Edna Vaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ojha_A/0/1/0/all/0/1\">Atul Kr. Ojha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karmali_R/0/1/0/all/0/1\">Ramdas Karmali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pawar_J/0/1/0/all/0/1\">Jyoti D. Pawar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Additive manifesto decomposition: A policy domain aware method for understanding party positioning. (arXiv:2305.10136v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10136","description":"<p>Automatic extraction of party (dis)similarities from texts such as party\nelection manifestos or parliamentary speeches plays an increasing role in\ncomputational political science. However, existing approaches are fundamentally\nlimited to targeting only global party (dis)-similarity: they condense the\nrelationship between a pair of parties into a single figure, their similarity.\nIn aggregating over all policy domains (e.g., health or foreign policy), they\ndo not provide any qualitative insights into which domains parties agree or\ndisagree on. This paper proposes a workflow for estimating policy domain aware\nparty similarity that overcomes this limitation. The workflow covers (a)\ndefinition of suitable policy domains; (b) automatic labeling of domains, if no\nmanual labels are available; (c) computation of domain-level similarities and\naggregation at a global level; (d) extraction of interpretable party positions\non major policy axes via multidimensional scaling. We evaluate our workflow on\nmanifestos from the German federal elections. We find that our method (a)\nyields high correlation when predicting party similarity at a global level and\n(b) provides accurate party-specific positions, even with automatically\nlabelled policy domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ceron_T/0/1/0/all/0/1\">Tanise Ceron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolaev_D/0/1/0/all/0/1\">Dmitry Nikolaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1\">Sebastian Pad&#xf3;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback. (arXiv:2305.10142v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10142","description":"<p>We study whether multiple large language models (LLMs) can autonomously\nimprove each other in a negotiation game by playing, reflecting, and\ncriticizing. We are interested in this question because if LLMs were able to\nimprove each other, it would imply the possibility of creating strong AI agents\nwith minimal human intervention. We ask two LLMs to negotiate with each other,\nplaying the roles of a buyer and a seller, respectively. They aim to reach a\ndeal with the buyer targeting a lower price and the seller a higher one. A\nthird language model, playing the critic, provides feedback to a player to\nimprove the player's negotiation strategies. We let the two agents play\nmultiple rounds, using previous negotiation history and AI feedback as\nin-context demonstrations to improve the model's negotiation strategy\niteratively. We use different LLMs (GPT and Claude) for different roles and use\nthe deal price as the evaluation metric. Our experiments reveal multiple\nintriguing findings: (1) Only a subset of the language models we consider can\nself-play and improve the deal price from AI feedback, weaker models either do\nnot understand the game's rules or cannot incorporate AI feedback for further\nimprovement. (2) Models' abilities to learn from the feedback differ when\nplaying different roles. For example, it is harder for Claude-instant to\nimprove as the buyer than as the seller. (3) When unrolling the game to\nmultiple rounds, stronger agents can consistently improve their performance by\nmeaningfully using previous experiences and iterative AI feedback, yet have a\nhigher risk of breaking the deal. We hope our work provides insightful initial\nexplorations of having models autonomously improve each other with game playing\nand AI feedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1\">Tushar Khot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Grained Knowledge Retrieval for End-to-End Task-Oriented Dialog. (arXiv:2305.10149v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10149","description":"<p>Retrieving proper domain knowledge from an external database lies at the\nheart of end-to-end task-oriented dialog systems to generate informative\nresponses. Most existing systems blend knowledge retrieval with response\ngeneration and optimize them with direct supervision from reference responses,\nleading to suboptimal retrieval performance when the knowledge base becomes\nlarge-scale. To address this, we propose to decouple knowledge retrieval from\nresponse generation and introduce a multi-grained knowledge retriever (MAKER)\nthat includes an entity selector to search for relevant entities and an\nattribute selector to filter out irrelevant attributes. To train the retriever,\nwe propose a novel distillation objective that derives supervision signals from\nthe response generator. Experiments conducted on three standard benchmarks with\nboth small and large-scale knowledge bases demonstrate that our retriever\nperforms knowledge retrieval more effectively than existing methods. Our code\nhas been made publicly\navailable.\\footnote{https://github.<a href=\"/abs/com/1890730\">com/1890730</a>5772/MAKER}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_F/0/1/0/all/0/1\">Fanqi Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Weizhou Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Ke Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1\">Xiaojun Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1\">Wei Bi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterated learning and communication jointly explain efficient color naming systems. (arXiv:2305.10154v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10154","description":"<p>It has been argued that semantic systems reflect pressure for efficiency, and\na current debate concerns the cultural evolutionary process that produces this\npattern. We consider efficiency as instantiated in the Information Bottleneck\n(IB) principle, and a model of cultural evolution that combines iterated\nlearning and communication. We show that this model, instantiated in neural\nnetworks, converges to color naming systems that are efficient in the IB sense\nand similar to human color naming systems. We also show that iterated learning\nalone, and communication alone, do not yield the same outcome as clearly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carlsson_E/0/1/0/all/0/1\">Emil Carlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubhashi_D/0/1/0/all/0/1\">Devdatt Dubhashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Regier_T/0/1/0/all/0/1\">Terry Regier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personality Understanding of Fictional Characters during Book Reading. (arXiv:2305.10156v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10156","description":"<p>Comprehending characters' personalities is a crucial aspect of story reading.\nAs readers engage with a story, their understanding of a character evolves\nbased on new events and information; and multiple fine-grained aspects of\npersonalities can be perceived. This leads to a natural problem of situated and\nfine-grained personality understanding. The problem has not been studied in the\nNLP field, primarily due to the lack of appropriate datasets mimicking the\nprocess of book reading. We present the first labeled dataset PersoNet for this\nproblem. Our novel annotation strategy involves annotating user notes from\nonline reading apps as a proxy for the original books. Experiments and human\nstudies indicate that our dataset construction is both efficient and accurate;\nand our task heavily relies on long-term context to achieve accurate\npredictions for both machines and humans. The dataset is available at\nhttps://github.com/Gorov/personet_acl23.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1\">Shunyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_W/0/1/0/all/0/1\">Wenjie Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaochen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhou Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks. (arXiv:2305.10160v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10160","description":"<p>Data contamination has become especially prevalent and challenging with the\nrise of models pretrained on very large, automatically-crawled corpora. For\nclosed models, the training data becomes a trade secret, and even for open\nmodels, it is not trivial to ascertain whether a particular test instance has\nbeen compromised. Strategies such as live leaderboards with hidden answers, or\nusing test data which is guaranteed to be unseen, are expensive and become\nfragile with time. Assuming that all relevant actors value clean test data and\nwill cooperate to mitigate data contamination, what can be done? We propose\nthree strategies that can make a difference: (1) Test data made public should\nbe encrypted with a public key and licensed to disallow derivative\ndistribution; (2) demand training exclusion controls from closed API holders,\nand protect your test data by refusing to evaluate until demands are met; (3)\nin case of test data based on internet text, avoid data which appears with its\nsolution on the internet, and release the context of internet-derived data\nalong with the data. These strategies are practical and can be effective in\npreventing data contamination and allowing trustworthy evaluation of models'\ncapabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jacovi_A/0/1/0/all/0/1\">Alon Jacovi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1\">Avi Caciularu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldman_O/0/1/0/all/0/1\">Omer Goldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model. (arXiv:2305.10163v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10163","description":"<p>Generative Pre-Training (GPT) models like ChatGPT have demonstrated\nexceptional performance in various Natural Language Processing (NLP) tasks.\nAlthough ChatGPT has been integrated into the overall workflow to boost\nefficiency in many domains, the lack of flexibility in the finetuning process\nhinders its applications in areas that demand extensive domain expertise and\nsemantic knowledge, such as healthcare. In this paper, we evaluate ChatGPT on\nthe China National Medical Licensing Examination (CNMLE) and propose a novel\napproach to improve ChatGPT from two perspectives: integrating medical domain\nknowledge and enabling few-shot learning. By using a simple but effective\nretrieval method, medical background knowledge is extracted as semantic\ninstructions to guide the inference of ChatGPT. Similarly, relevant medical\nquestions are identified and fed as demonstrations to ChatGPT. Experimental\nresults show that directly applying ChatGPT fails to qualify the CNMLE at a\nscore of 51 (i.e., only 51\\% of questions are answered correctly). While our\nknowledge-enhanced model achieves a high score of 70 on CNMLE-2022 which not\nonly passes the qualification but also surpasses the average score of humans\n(61). This research demonstrates the potential of knowledge-enhanced ChatGPT to\nserve as versatile medical assistants, capable of analyzing real-world medical\nproblems in a more accessible, user-friendly, and adaptable manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiageng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhaopeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pragmatic Reasoning in Structured Signaling Games. (arXiv:2305.10167v1 [cs.AI])","link":"http://arxiv.org/abs/2305.10167","description":"<p>In this work we introduce a structured signaling game, an extension of the\nclassical signaling game with a similarity structure between meanings in the\ncontext, along with a variant of the Rational Speech Act (RSA) framework which\nwe call structured-RSA (sRSA) for pragmatic reasoning in structured domains. We\nexplore the behavior of the sRSA in the domain of color and show that pragmatic\nagents using sRSA on top of semantic representations, derived from the World\nColor Survey, attain efficiency very close to the information theoretic limit\nafter only 1 or 2 levels of recursion. We also explore the interaction between\npragmatic reasoning and learning in multi-agent reinforcement learning\nframework. Our results illustrate that artificial agents using sRSA develop\ncommunication closer to the information theoretic frontier compared to agents\nusing RSA and just reinforcement learning. We also find that the ambiguity of\nthe semantic representation increases as the pragmatic agents are allowed to\nperform deeper reasoning about each other during learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carlsson_E/0/1/0/all/0/1\">Emil Carlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubhashi_D/0/1/0/all/0/1\">Devdatt Dubhashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-enhanced Mixed-initiative Dialogue System for Emotional Support Conversations. (arXiv:2305.10172v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10172","description":"<p>Unlike empathetic dialogues, the system in emotional support conversations\n(ESC) is expected to not only convey empathy for comforting the help-seeker,\nbut also proactively assist in exploring and addressing their problems during\nthe conversation. In this work, we study the problem of mixed-initiative ESC\nwhere the user and system can both take the initiative in leading the\nconversation. Specifically, we conduct a novel analysis on mixed-initiative ESC\nsystems with a tailor-designed schema that divides utterances into different\ntypes with speaker roles and initiative types. Four emotional support metrics\nare proposed to evaluate the mixed-initiative interactions. The analysis\nreveals the necessity and challenges of building mixed-initiative ESC systems.\nIn the light of this, we propose a knowledge-enhanced mixed-initiative\nframework (KEMI) for ESC, which retrieves actual case knowledge from a\nlarge-scale mental health knowledge graph for generating mixed-initiative\nresponses. Experimental results on two ESC datasets show the superiority of\nKEMI in both content-preserving evaluation and mixed initiative related\nanalyses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yifei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variable-length Neural Interlingua Representations for Zero-shot Neural Machine Translation. (arXiv:2305.10190v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10190","description":"<p>The language-independency of encoded representations within multilingual\nneural machine translation (MNMT) models is crucial for their generalization\nability on zero-shot translation. Neural interlingua representations have been\nshown as an effective method for achieving this. However, fixed-length neural\ninterlingua representations introduced in previous work can limit its\nflexibility and representation ability. In this study, we introduce a novel\nmethod to enhance neural interlingua representations by making their length\nvariable, thereby overcoming the constraint of fixed-length neural interlingua\nrepresentations. Our empirical results on zero-shot translation on OPUS, IWSLT,\nand Europarl datasets demonstrate stable model convergence and superior\nzero-shot translation results compared to fixed-length neural interlingua\nrepresentations. However, our analysis reveals the suboptimal efficacy of our\napproach in translating from certain source languages, wherein we pinpoint the\ndefective model component in our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhuoyuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Haiyue Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1\">Raj Dabre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1\">Chenhui Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurohashi_S/0/1/0/all/0/1\">Sadao Kurohashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Distress Support Dialogue Responses with Motivational Interviewing Strategy. (arXiv:2305.10195v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10195","description":"<p>AI-driven chatbots have become an emerging solution to address psychological\ndistress. Due to the lack of psychotherapeutic data, researchers use dialogues\nscraped from online peer support forums to train them. But since the responses\nin such platforms are not given by professionals, they contain both conforming\nand non-conforming responses. In this work, we attempt to recognize these\nconforming and non-conforming response types present in online distress-support\ndialogues using labels adapted from a well-established behavioral coding scheme\nnamed Motivational Interviewing Treatment Integrity (MITI) code and show how\nsome response types could be rephrased into a more MI adherent form that can,\nin turn, enable chatbot responses to be more compliant with the MI strategy. As\na proof of concept, we build several rephrasers by fine-tuning Blender and GPT3\nto rephrase MI non-adherent \"Advise without permission\" responses into \"Advise\nwith permission\". We show how this can be achieved with the construction of\npseudo-parallel corpora avoiding costs for human labor. Through automatic and\nhuman evaluation we show that in the presence of less training data, techniques\nsuch as prompting and data augmentation can be used to produce substantially\ngood rephrasings that reflect the intended style and preserve the content of\nthe original text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Welivita_A/0/1/0/all/0/1\">Anuradha Welivita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_P/0/1/0/all/0/1\">Pearl Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Zero Pronoun Translation. (arXiv:2305.10196v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10196","description":"<p>Zero pronouns (ZPs) are frequently omitted in pro-drop languages (e.g.\nChinese, Hungarian, and Hindi), but should be recalled in non-pro-drop\nlanguages (e.g. English). This phenomenon has been studied extensively in\nmachine translation (MT), as it poses a significant challenge for MT systems\ndue to the difficulty in determining the correct antecedent for the pronoun.\nThis survey paper highlights the major works that have been undertaken in zero\npronoun translation (ZPT) after the neural revolution, so that researchers can\nrecognise the current state and future directions of this field. We provide an\norganisation of the literature based on evolution, dataset, method and\nevaluation. In addition, we compare and analyze competing models and evaluation\nmetrics on different benchmarks. We uncover a number of insightful findings\nsuch as: 1) ZPT is in line with the development trend of large language model;\n2) data limitation causes learning bias in languages and domains; 3)\nperformance improvements are often reported on single benchmarks, but advanced\nmethods are still far from real-world use; 4) general-purpose metrics are not\nreliable on nuances and complexities of ZPT, emphasizing the necessity of\ntargeted metrics; 5) apart from commonly-cited errors, ZPs will cause risks of\ngender bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Siyou Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingzhou Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linfeng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shielded Representations: Protecting Sensitive Attributes Through Iterative Gradient-Based Projection. (arXiv:2305.10204v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10204","description":"<p>Natural language processing models tend to learn and encode social biases\npresent in the data. One popular approach for addressing such biases is to\neliminate encoded information from the model's representations. However,\ncurrent methods are restricted to removing only linearly encoded information.\nIn this work, we propose Iterative Gradient-Based Projection (IGBP), a novel\nmethod for removing non-linear encoded concepts from neural representations.\nOur method consists of iteratively training neural classifiers to predict a\nparticular attribute we seek to eliminate, followed by a projection of the\nrepresentation on a hypersurface, such that the classifiers become oblivious to\nthe target attribute. We evaluate the effectiveness of our method on the task\nof removing gender and race information as sensitive attributes. Our results\ndemonstrate that IGBP is effective in mitigating bias through intrinsic and\nextrinsic evaluations, with minimal impact on downstream task accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iskander_S/0/1/0/all/0/1\">Shadi Iskander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radinsky_K/0/1/0/all/0/1\">Kira Radinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenSLU: A Unified, Modularized, and Extensible Toolkit for Spoken Language Understanding. (arXiv:2305.10231v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10231","description":"<p>Spoken Language Understanding (SLU) is one of the core components of a\ntask-oriented dialogue system, which aims to extract the semantic meaning of\nuser queries (e.g., intents and slots). In this work, we introduce OpenSLU, an\nopen-source toolkit to provide a unified, modularized, and extensible toolkit\nfor spoken language understanding. Specifically, OpenSLU unifies 10 SLU models\nfor both single-intent and multi-intent scenarios, which support both\nnon-pretrained and pretrained models simultaneously. Additionally, OpenSLU is\nhighly modularized and extensible by decomposing the model architecture,\ninference, and learning process into reusable modules, which allows researchers\nto quickly set up SLU experiments with highly flexible configurations. OpenSLU\nis implemented based on PyTorch, and released at\n\\url{https://github.com/LightChen233/OpenSLU}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Libo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiguang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yunlong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A quantitative study of NLP approaches to question difficulty estimation. (arXiv:2305.10236v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10236","description":"<p>Recent years witnessed an increase in the amount of research on the task of\nQuestion Difficulty Estimation from Text QDET with Natural Language Processing\n(NLP) techniques, with the goal of targeting the limitations of traditional\napproaches to question calibration. However, almost the entirety of previous\nresearch focused on single silos, without performing quantitative comparisons\nbetween different models or across datasets from different educational domains.\nIn this work, we aim at filling this gap, by quantitatively analyzing several\napproaches proposed in previous research, and comparing their performance on\nthree publicly available real world datasets containing questions of different\ntypes from different educational domains. Specifically, we consider reading\ncomprehension Multiple Choice Questions (MCQs), science MCQs, and math\nquestions. We find that Transformer based models are the best performing across\ndifferent educational domains, with DistilBERT performing almost as well as\nBERT, and that they outperform other approaches even on smaller datasets. As\nfor the other models, the hybrid ones often outperform the ones based on a\nsingle type of features, the ones based on linguistic features perform well on\nreading comprehension questions, while frequency based features (TF-IDF) and\nword embeddings (word2vec) perform better in domain knowledge assessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benedetto_L/0/1/0/all/0/1\">Luca Benedetto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MemoryBank: Enhancing Large Language Models with Long-Term Memory. (arXiv:2305.10250v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10250","description":"<p>Revolutionary advancements in Large Language Models have drastically reshaped\nour interactions with artificial intelligence systems. Despite this, a notable\nhindrance remains-the deficiency of a long-term memory mechanism within these\nmodels. This shortfall becomes increasingly evident in situations demanding\nsustained interaction, such as personal companion systems and psychological\ncounseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored\nfor LLMs. MemoryBank enables the models to summon relevant memories,\ncontinually evolve through continuous memory updates, comprehend, and adapt to\na user personality by synthesizing information from past interactions. To mimic\nanthropomorphic behaviors and selectively preserve memory, MemoryBank\nincorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting\nCurve theory, which permits the AI to forget and reinforce memory based on time\nelapsed and the relative significance of the memory, thereby offering a\nhuman-like memory mechanism. MemoryBank is versatile in accommodating both\nclosed-source models like ChatGPT and open-source models like ChatGLM. We\nexemplify application of MemoryBank through the creation of an LLM-based\nchatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned\nwith psychological dialogs, SiliconFriend displays heightened empathy in its\ninteractions. Experiment involves both qualitative analysis with real-world\nuser dialogs and quantitative analysis with simulated dialogs. In the latter,\nChatGPT acts as users with diverse characteristics and generates long-term\ndialog contexts covering a wide array of topics. The results of our analysis\nreveal that SiliconFriend, equipped with MemoryBank, exhibits a strong\ncapability for long-term companionship as it can provide emphatic response,\nrecall relevant memories and understand user personality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wanjun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Lianghong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanlin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models. (arXiv:2305.10263v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10263","description":"<p>Large language models have recently made tremendous progress in a variety of\naspects, e.g., cross-task generalization, instruction following.\nComprehensively evaluating the capability of large language models in multiple\ntasks is of great importance. In this paper, we propose M3KE, a Massive\nMulti-Level Multi-Subject Knowledge Evaluation benchmark, which is developed to\nmeasure knowledge acquired by Chinese large language models by testing their\nmultitask accuracy in zero- and few-shot settings. We have collected 20,477\nquestions from 71 tasks. Our selection covers all major levels of Chinese\neducation system, ranging from the primary school to college, as well as a wide\nvariety of subjects, including humanities, history, politics, law, education,\npsychology, science, technology, art and religion. All questions are\nmultiple-choice questions with four options, hence guaranteeing a standardized\nand unified assessment process. We've assessed a number of state-of-the-art\nopen-source Chinese large language models on the proposed benchmark. The size\nof these models varies from 335M to 130B parameters. Experiment results\ndemonstrate that they perform significantly worse than GPT-3.5 that reaches an\naccuracy of ~ 48% on M3KE. The dataset is available at\nhttps://github.com/tjunlp-lab/M3KE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Renren Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yuqi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Linhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_T/0/1/0/all/0/1\">Tianyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiaohan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jianxiang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peiyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qingqing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1\">Xiaowen Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Deyi Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM's Translation Capability. (arXiv:2305.10266v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10266","description":"<p>Large, multilingual language models exhibit surprisingly good zero- or\nfew-shot machine translation capabilities, despite having never seen the\nintentionally-included translation examples provided to typical neural\ntranslation systems. We investigate the role of incidental bilingualism -- the\nunintentional consumption of bilingual signals, including translation examples\n-- in explaining the translation capabilities of large language models, taking\nthe Pathways Language Model (PaLM) as a case study. We introduce a mixed-method\napproach to measure and understand incidental bilingualism at scale. We show\nthat PaLM is exposed to over 30 million translation pairs across at least 44\nlanguages. Furthermore, the amount of incidental bilingual content is highly\ncorrelated with the amount of monolingual in-language content for non-English\nlanguages. We relate incidental bilingual content to zero-shot prompts and show\nthat it can be used to mine new prompts to improve PaLM's out-of-English\nzero-shot translation quality. Finally, in a series of small-scale ablations,\nwe show that its presence has a substantial impact on translation capabilities,\nalthough this impact diminishes with model scale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Briakou_E/0/1/0/all/0/1\">Eleftheria Briakou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherry_C/0/1/0/all/0/1\">Colin Cherry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_G/0/1/0/all/0/1\">George Foster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Local Spectro-Temporal Features for Speech Analysis. (arXiv:2305.10270v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10270","description":"<p>We introduce the problem of phone classification in the context of speech\nrecognition, and explore several sets of local spectro-temporal features that\ncan be used for phone classification. In particular, we present some\npreliminary results for phone classification using two sets of features that\nare commonly used for object detection: Haar features and SVM-classified\nHistograms of Gradients (HoG)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guerzhoy_M/0/1/0/all/0/1\">Michael Guerzhoy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models. (arXiv:2305.10276v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10276","description":"<p>In this paper, we take the initiative to investigate the performance of LLMs\non complex planning tasks that require LLMs to understand a virtual spatial\nenvironment simulated via natural language and act correspondingly in text. We\npropose a benchmark named Natural Language Planning (NLP) composed of a set of\nnovel tasks: Brick World, NLVR-based Manipulations, and Natural Language\nNavigation. We found that current popular LLMs such as ChatGPT still lack\nabilities in complex planning. This arises a question -- do the LLMs have a\ngood understanding of the environments described in natural language, or maybe\nother alternatives such as symbolic representations are neater and hence better\nto be understood by LLMs? To this end, we propose a novel method called CoS\n(Chain-of-Symbol Prompting) that represents the complex environments with\ncondensed symbolic spatial representations during the chained intermediate\nthinking steps. CoS is easy to use and does not need additional training on\nLLMs. Extensive experiments indicate that CoS clearly surpasses the performance\nof the Chain-of-Thought (CoT) Prompting in all three planning tasks with even\nfewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT.\nThe performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%)\non Brick World for ChatGPT. CoS also reduces the number of tokens in the prompt\nobviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate\nsteps from demonstrations on Brick World.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hanxu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hongyuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huajian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks. (arXiv:2305.10284v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10284","description":"<p>The evaluation of natural language processing (NLP) systems is crucial for\nadvancing the field, but current benchmarking approaches often assume that all\nsystems have scores available for all tasks, which is not always practical. In\nreality, several factors such as the cost of running baseline, private systems,\ncomputational limitations, or incomplete data may prevent some systems from\nbeing evaluated on entire tasks. This paper formalize an existing problem in\nNLP research: benchmarking when some systems scores are missing on the task,\nand proposes a novel approach to address it. Our method utilizes a compatible\npartial ranking approach to impute missing data, which is then aggregated using\nthe Borda count method. It includes two refinements designed specifically for\nscenarios where either task-level or instance-level scores are available. We\nalso introduce an extended benchmark, which contains over 131 million scores,\nan order of magnitude larger than existing benchmarks. We validate our methods\nand demonstrate their effectiveness in addressing the challenge of missing\nsystem evaluation on an entire task. This work highlights the need for more\ncomprehensive benchmarking approaches that can handle real-world scenarios\nwhere not all systems are evaluated on the entire task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Himmi_A/0/1/0/all/0/1\">Anas Himmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irurozki_E/0/1/0/all/0/1\">Ekhine Irurozki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noiry_N/0/1/0/all/0/1\">Nathan Noiry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clemencon_S/0/1/0/all/0/1\">Stephan Clemencon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective. (arXiv:2305.10306v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10306","description":"<p>We propose a new paradigm for universal information extraction (IE) that is\ncompatible with any schema format and applicable to a list of IE tasks, such as\nnamed entity recognition, relation extraction, event extraction and sentiment\nanalysis. Our approach converts the text-based IE tasks as the token-pair\nproblem, which uniformly disassembles all extraction targets into joint span\ndetection, classification and association problems with a unified extractive\nframework, namely UniEX. UniEX can synchronously encode schema-based prompt and\ntextual information, and collaboratively learn the generalized knowledge from\npre-defined information using the auto-encoder language models. We develop a\ntraffine attention mechanism to integrate heterogeneous factors including\ntasks, labels and inside tokens, and obtain the extraction target via a scoring\nmatrix. Experiment results show that UniEX can outperform generative universal\nIE models in terms of performance and inference-speed on $14$ benchmarks IE\ndatasets with the supervised setting. The state-of-the-art performance in\nlow-resource scenarios also verifies the transferability and effectiveness of\nUniEX.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Junyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Ping Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_R/0/1/0/all/0/1\">Ruyi Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingjian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy. (arXiv:2305.10307v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10307","description":"<p>Measuring the distance between machine-produced and human language is\nacritical open problem. Inspired by empirical findings from psycholinguistics\non theperiodicity of entropy in language, we propose FACE, a set of metrics\nbased onFourier Analysis of the estimated Cross-Entropy of language, for\nmeasuring thesimilarity between model-generated and human-written languages.\nBased on anopen-ended generation task and the experimental data from previous\nstudies, weind that FACE can effectively identify the human-model gap, scales\nwith modelsize, reflects the outcomes of different sampling methods for\ndecoding, correlateswell with other evaluation metrics and with human judgment\nscores. FACE iscomputationally efficient and provides intuitive\ninterpretations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zuhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yingfang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_S/0/1/0/all/0/1\">Shuo Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Huajun Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kefan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LeTI: Learning to Generate from Textual Interactions. (arXiv:2305.10314v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10314","description":"<p>Finetuning pre-trained language models (LMs) enhances the models'\ncapabilities. Prior techniques fine-tune a pre-trained LM on input-output pairs\n(e.g., instruction fine-tuning), or with numerical rewards that gauge the\nquality of its outputs (e.g., reinforcement learning from human feedback). We\nexplore LMs' potential to learn from textual interactions (LeTI) that not only\ncheck their correctness with binary labels, but also pinpoint and explain\nerrors in their outputs through textual feedback. Our investigation focuses on\nthe code generation task, where the model produces code pieces in response to\nnatural language instructions. This setting invites a natural and scalable way\nto acquire the textual feedback: the error messages and stack traces from code\nexecution using a Python interpreter. LeTI iteratively fine-tunes the model,\nusing the LM objective, on a concatenation of natural language instructions,\nLM-generated programs, and textual feedback, which is only provided when the\ngenerated program fails to solve the task. Prepended to this fine-tuning text,\na binary reward token is used to differentiate correct and buggy solutions. On\nMBPP, a code generation dataset, LeTI substantially improves the performance of\ntwo base LMs of different scales. LeTI requires no ground-truth outputs for\ntraining and even outperforms a fine-tuned baseline that does. LeTI's strong\nperformance generalizes to other datasets. Trained on MBPP, it achieves\ncomparable or better performance than the base LMs on unseen problems in\nHumanEval. Furthermore, compared to binary feedback, we observe that textual\nfeedback leads to improved generation quality and sample efficiency, achieving\nthe same performance with fewer than half of the gradient steps. LeTI is\nequally applicable in natural language tasks when they can be formulated as\ncode generation, which we empirically verified on event argument extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jabbarvand_R/0/1/0/all/0/1\">Reyhaneh Jabbarvand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using a Large Language Model to Control Speaking Style for Expressive TTS. (arXiv:2305.10321v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10321","description":"<p>Appropriate prosody is critical for successful spoken communication.\nContextual word embeddings are proven to be helpful in predicting prosody but\ndo not allow for choosing between plausible prosodic renditions.\nReference-based TTS models attempt to address this by conditioning speech\ngeneration on a reference speech sample. These models can generate expressive\nspeech but this requires finding an appropriate reference.\n</p>\n<p>Sufficiently large generative language models have been used to solve various\nlanguage-related tasks. We explore whether such models can be used to suggest\nappropriate prosody for expressive TTS. We train a TTS model on a\nnon-expressive corpus and then prompt the language model to suggest changes to\npitch, energy and duration. The prompt can be designed for any task and we\nprompt the model to make suggestions based on target speaking style and\ndialogue context. The proposed method is rated most appropriate in 49.9\\% of\ncases compared to 31.0\\% for a baseline model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sigurgeirsson_A/0/1/0/all/0/1\">Atli Thor Sigurgeirsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_S/0/1/0/all/0/1\">Simon King</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Learning of Hierarchical Tasks from Dialog with GPT. (arXiv:2305.10349v1 [cs.HC])","link":"http://arxiv.org/abs/2305.10349","description":"<p>We present a system for interpretable, symbolic, interactive task learning\nfrom dialog using a GPT model as a conversational front-end. The learned tasks\nare represented as hierarchical decompositions of predicate-argument structures\nwith scoped variable arguments. By using a GPT model to convert interactive\ndialog into a semantic representation, and then recursively asking for\ndefinitions of unknown steps, we show that hierarchical task knowledge can be\nacquired and re-used in a natural and unrestrained conversational environment.\nWe compare our system to a similar architecture using a more conventional\nparser and show that our system tolerates a much wider variety of linguistic\nvariance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lawley_L/0/1/0/all/0/1\">Lane Lawley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacLellan_C/0/1/0/all/0/1\">Christopher J. MacLellan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Object Hallucination in Large Vision-Language Models. (arXiv:2305.10355v1 [cs.CV])","link":"http://arxiv.org/abs/2305.10355","description":"<p>Inspired by the superior language abilities of large language models (LLM),\nlarge vision-language models (LVLM) have been recently explored by integrating\npowerful LLMs for improving the performance on complex multimodal tasks.\nDespite the promising progress on LVLMs, we find that LVLMs suffer from the\nhallucination problem, i.e. they tend to generate objects that are inconsistent\nwith the target images in the descriptions. To investigate it, this work\npresents the first systematic study on object hallucination of LVLMs. We\nconduct the evaluation experiments on several representative LVLMs, and show\nthat they mostly suffer from severe object hallucination issue. We further\ndiscuss that the visual instructions may influence the hallucination, and find\nthat: objects that frequently occur in the visual instructions or co-occur with\nthe image objects, are obviously prone to be hallucinated by LVLMs. Besides, we\nfind that existing evaluation methods might be affected by the input\ninstructions and generation styles of LVLMs. Thus, we further design an\nimproved evaluation method for object hallucination by proposing a\npolling-based query method called \\emph{POPE}. Experiment results demonstrate\nthat our POPE can evaluate the object hallucination in a more stable and\nflexible way. Our codes and data are publicly available at\nhttps://github.com/RUCAIBox/POPE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yifan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-Scale Text Analysis Using Generative Language Models: A Case Study in Discovering Public Value Expressions in AI Patents. (arXiv:2305.10383v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10383","description":"<p>Labeling data is essential for training text classifiers but is often\ndifficult to accomplish accurately, especially for complex and abstract\nconcepts. Seeking an improved method, this paper employs a novel approach using\na generative language model (GPT-4) to produce labels and rationales for\nlarge-scale text analysis. We apply this approach to the task of discovering\npublic value expressions in US AI patents. We collect a database comprising\n154,934 patent documents using an advanced Boolean query submitted to\nInnovationQ+. The results are merged with full patent text from the USPTO,\nresulting in 5.4 million sentences. We design a framework for identifying and\nlabeling public value expressions in these AI patent sentences. A prompt for\nGPT-4 is developed which includes definitions, guidelines, examples, and\nrationales for text classification. We evaluate the quality of the labels and\nrationales produced by GPT-4 using BLEU scores and topic modeling and find that\nthey are accurate, diverse, and faithful. These rationales also serve as a\nchain-of-thought for the model, a transparent mechanism for human verification,\nand support for human annotators to overcome cognitive limitations. We conclude\nthat GPT-4 achieved a high-level of recognition of public value theory from our\nframework, which it also uses to discover unseen public value expressions. We\nuse the labels produced by GPT-4 to train BERT-based classifiers and predict\nsentences on the entire database, achieving high F1 scores for the 3-class\n(0.85) and 2-class classification (0.91) tasks. We discuss the implications of\nour approach for conducting large-scale text analyses with complex and abstract\nconcepts and suggest that, with careful framework design and interactive human\noversight, generative language models can offer significant advantages in\nquality and in reduced time and costs for producing labels and rationales.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pelaez_S/0/1/0/all/0/1\">Sergio Pelaez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_G/0/1/0/all/0/1\">Gaurav Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_B/0/1/0/all/0/1\">Barbara Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shapira_P/0/1/0/all/0/1\">Philip Shapira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logit-Based Ensemble Distribution Distillation for Robust Autoregressive Sequence Uncertainties. (arXiv:2305.10384v1 [cs.LG])","link":"http://arxiv.org/abs/2305.10384","description":"<p>Efficiently and reliably estimating uncertainty is an important objective in\ndeep learning. It is especially pertinent to autoregressive sequence tasks,\nwhere training and inference costs are typically very high. However, existing\nresearch has predominantly focused on tasks with static data such as image\nclassification. In this work, we investigate Ensemble Distribution Distillation\n(EDD) applied to large-scale natural language sequence-to-sequence data. EDD\naims to compress the superior uncertainty performance of an expensive (teacher)\nensemble into a cheaper (student) single model. Importantly, the ability to\nseparate knowledge (epistemic) and data (aleatoric) uncertainty is retained.\nExisting probability-space approaches to EDD, however, are difficult to scale\nto large vocabularies. We show, for modern transformer architectures on\nlarge-scale translation tasks, that modelling the ensemble logits, instead of\nsoftmax probabilities, leads to significantly better students. Moreover, the\nstudents surprisingly even outperform Deep Ensembles by up to ~10% AUROC on\nout-of-distribution detection, whilst matching them at in-distribution\ntranslation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fathullah_Y/0/1/0/all/0/1\">Yassir Fathullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Guoxuan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark Gales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Elaborative Simplification as Implicit Questions Under Discussion. (arXiv:2305.10387v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10387","description":"<p>Automated text simplification, a technique useful for making text more\naccessible to people such as children and emergent bilinguals, is often thought\nof as a monolingual translation task from complex sentences to simplified\nsentences using encoder-decoder models. This view fails to account for\nelaborative simplification, where new information is added into the simplified\ntext. This paper proposes to view elaborative simplification through the lens\nof the Question Under Discussion (QUD) framework, providing a robust way to\ninvestigate what writers elaborate upon, how they elaborate, and how\nelaborations fit into the discourse context by viewing elaborations as explicit\nanswers to implicit questions. We introduce ElabQUD, consisting of 1.3K\nelaborations accompanied with implicit QUDs, to study these phenomena. We show\nthat explicitly modeling QUD (via question generation) not only provides\nessential understanding of elaborative simplification and how the elaborations\nconnect with the rest of the discourse, but also substantially improves the\nquality of elaboration generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yating Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheffield_W/0/1/0/all/0/1\">William Sheffield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1\">Kyle Mahowald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What You See is What You Read? Improving Text-Image Alignment Evaluation. (arXiv:2305.10400v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10400","description":"<p>Automatically determining whether a text and a corresponding image are\nsemantically aligned is a significant challenge for vision-language models,\nwith applications in generative text-to-image and image-to-text tasks. In this\nwork, we study methods for automatic text-image alignment evaluation. We first\nintroduce SeeTRUE: a comprehensive evaluation set, spanning multiple datasets\nfrom both text-to-image and image-to-text generation tasks, with human\njudgements for whether a given text-image pair is semantically aligned. We then\ndescribe two automatic methods to determine alignment: the first involving a\npipeline based on question generation and visual question answering models, and\nthe second employing an end-to-end classification approach by finetuning\nmultimodal pretrained models. Both methods surpass prior approaches in various\ntext-image alignment tasks, with significant improvements in challenging cases\nthat involve complex composition or unnatural images. Finally, we demonstrate\nhow our approaches can localize specific misalignments between an image and a\ngiven text, and how they can be used to automatically re-rank candidates in\ntext-to-image generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yarom_M/0/1/0/all/0/1\">Michal Yarom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1\">Yonatan Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Changpinyo_S/0/1/0/all/0/1\">Soravit Changpinyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aharoni_R/0/1/0/all/0/1\">Roee Aharoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1\">Jonathan Herzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_O/0/1/0/all/0/1\">Oran Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ofek_E/0/1/0/all/0/1\">Eran Ofek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szpektor_I/0/1/0/all/0/1\">Idan Szpektor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PaLM 2 Technical Report. (arXiv:2305.10403v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10403","description":"<p>We introduce PaLM 2, a new state-of-the-art language model that has better\nmultilingual and reasoning capabilities and is more compute-efficient than its\npredecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture\nof objectives. Through extensive evaluations on English and multilingual\nlanguage, and reasoning tasks, we demonstrate that PaLM 2 has significantly\nimproved quality on downstream tasks across different model sizes, while\nsimultaneously exhibiting faster and more efficient inference compared to PaLM.\nThis improved efficiency enables broader deployment while also allowing the\nmodel to respond faster, for a more natural pace of interaction. PaLM 2\ndemonstrates robust reasoning capabilities exemplified by large improvements\nover PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable\nperformance on a suite of responsible AI evaluations, and enables\ninference-time control over toxicity without additional overhead or impact on\nother capabilities. Overall, PaLM 2 achieves state-of-the-art performance\nacross a diverse set of tasks and capabilities.\n</p>\n<p>When discussing the PaLM 2 family, it is important to distinguish between\npre-trained models (of various sizes), fine-tuned variants of these models, and\nthe user-facing products that use these models. In particular, user-facing\nproducts typically include additional pre- and post-processing steps.\nAdditionally, the underlying models may evolve over time. Therefore, one should\nnot expect the performance of user-facing products to exactly match the results\nreported in this report.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anil_R/0/1/0/all/0/1\">Rohan Anil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew M. Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Melvin Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepikhin_D/0/1/0/all/0/1\">Dmitry Lepikhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passos_A/0/1/0/all/0/1\">Alexandre Passos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakeri_S/0/1/0/all/0/1\">Siamak Shakeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taropa_E/0/1/0/all/0/1\">Emanuel Taropa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailey_P/0/1/0/all/0/1\">Paige Bailey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_E/0/1/0/all/0/1\">Eric Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jonathan H. Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafey_L/0/1/0/all/0/1\">Laurent El Shafey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meier_Hellstern_K/0/1/0/all/0/1\">Kathy Meier-Hellstern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_G/0/1/0/all/0/1\">Gaurav Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreira_E/0/1/0/all/0/1\">Erica Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omernick_M/0/1/0/all/0/1\">Mark Omernick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_K/0/1/0/all/0/1\">Kevin Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1\">Kefan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanzhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yujing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abrego_G/0/1/0/all/0/1\">Gustavo Hernandez Abrego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_J/0/1/0/all/0/1\">Junwhan Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1\">Jacob Austin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barham_P/0/1/0/all/0/1\">Paul Barham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botha_J/0/1/0/all/0/1\">Jan Botha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradbury_J/0/1/0/all/0/1\">James Bradbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahma_S/0/1/0/all/0/1\">Siddhartha Brahma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brooks_K/0/1/0/all/0/1\">Kevin Brooks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catasta_M/0/1/0/all/0/1\">Michele Catasta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherry_C/0/1/0/all/0/1\">Colin Cherry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choquette_Choo_C/0/1/0/all/0/1\">Christopher A. Choquette-Choo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhery_A/0/1/0/all/0/1\">Aakanksha Chowdhery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crepy_C/0/1/0/all/0/1\">Cl&#xe9;ment Crepy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dave_S/0/1/0/all/0/1\">Shachi Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devlin_J/0/1/0/all/0/1\">Jacob Devlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_M/0/1/0/all/0/1\">Mark D&#xed;az</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1\">Nan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dyer_E/0/1/0/all/0/1\">Ethan Dyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feinberg_V/0/1/0/all/0/1\">Vlad Feinberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fangxiaoyu Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fienber_V/0/1/0/all/0/1\">Vlad Fienber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1\">Markus Freitag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_L/0/1/0/all/0/1\">Lucas Gonzalez</a>, et al. (76 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BAD: BiAs Detection for Large Language Models in the context of candidate screening. (arXiv:2305.10407v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10407","description":"<p>Application Tracking Systems (ATS) have allowed talent managers, recruiters,\nand college admissions committees to process large volumes of potential\ncandidate applications efficiently. Traditionally, this screening process was\nconducted manually, creating major bottlenecks due to the quantity of\napplications and introducing many instances of human bias. The advent of large\nlanguage models (LLMs) such as ChatGPT and the potential of adopting methods to\ncurrent automated application screening raises additional bias and fairness\nissues that must be addressed. In this project, we wish to identify and\nquantify the instances of social bias in ChatGPT and other OpenAI LLMs in the\ncontext of candidate screening in order to demonstrate how the use of these\nmodels could perpetuate existing biases and inequalities in the hiring process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koh_N/0/1/0/all/0/1\">Nam Ho Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plata_J/0/1/0/all/0/1\">Joseph Plata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Blockchain Concepts from Text. (arXiv:2305.10408v1 [cs.IR])","link":"http://arxiv.org/abs/2305.10408","description":"<p>Blockchains provide a mechanism through which mutually distrustful remote\nparties can reach consensus on the state of a ledger of information. With the\ngreat acceleration with which this space is developed, the demand for those\nseeking to learn about blockchain also grows. Being a technical subject, it can\nbe quite intimidating to start learning. For this reason, the main objective of\nthis project was to apply machine learning models to extract information from\nwhitepapers and academic articles focused on the blockchain area to organize\nthis information and aid users to navigate the space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Veiga_R/0/1/0/all/0/1\">Rodrigo Veiga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Endler_M/0/1/0/all/0/1\">Markus Endler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paiva_V/0/1/0/all/0/1\">Valeria de Paiva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SLiC-HF: Sequence Likelihood Calibration with Human Feedback. (arXiv:2305.10425v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10425","description":"<p>Learning from human feedback has been shown to be effective at aligning\nlanguage models with human preferences. Past work has often relied on\nReinforcement Learning from Human Feedback (RLHF), which optimizes the language\nmodel using reward scores assigned from a reward model trained on human\npreference data. In this work we show how the recently introduced Sequence\nLikelihood Calibration (SLiC), can also be used to effectively learn from human\npreferences (SLiC-HF). Furthermore, we demonstrate this can be done with human\nfeedback data collected for a different model, similar to off-policy, offline\nRL data. Automatic and human evaluation experiments on the TL;DR summarization\ntask show that SLiC-HF significantly improves supervised fine-tuning baselines.\nFurthermore, SLiC-HF presents a competitive alternative to the PPO RLHF\nimplementation used in past work while being much simpler to implement, easier\nto tune and more computationally efficient in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Rishabh Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalman_M/0/1/0/all/0/1\">Misha Khalman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saleh_M/0/1/0/all/0/1\">Mohammad Saleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peter J. Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating Transformer Inference for Translation via Parallel Decoding. (arXiv:2305.10427v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10427","description":"<p>Autoregressive decoding limits the efficiency of transformers for Machine\nTranslation (MT). The community proposed specific network architectures and\nlearning-based methods to solve this issue, which are expensive and require\nchanges to the MT model, trading inference speed at the cost of the translation\nquality. In this paper, we propose to address the problem from the point of\nview of decoding algorithms, as a less explored but rather compelling\ndirection. We propose to reframe the standard greedy autoregressive decoding of\nMT with a parallel formulation leveraging Jacobi and Gauss-Seidel fixed-point\niteration methods for fast inference. This formulation allows to speed up\nexisting models without training or modifications while retaining translation\nquality. We present three parallel decoding algorithms and test them on\ndifferent languages and models showing how the parallelization introduces a\nspeedup up to 38% w.r.t. the standard autoregressive decoding and nearly 2x\nwhen scaling the method on parallel resources. Finally, we introduce a decoding\ndependency graph visualizer (DDGviz) that let us see how the model has learned\nthe conditional dependence between tokens and inspect the decoding procedure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santilli_A/0/1/0/all/0/1\">Andrea Santilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Severino_S/0/1/0/all/0/1\">Silvio Severino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Postolache_E/0/1/0/all/0/1\">Emilian Postolache</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maiorca_V/0/1/0/all/0/1\">Valentino Maiorca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancusi_M/0/1/0/all/0/1\">Michele Mancusi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marin_R/0/1/0/all/0/1\">Riccardo Marin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1\">Emanuele Rodol&#xe0;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining. (arXiv:2305.10429v1 [cs.CL])","link":"http://arxiv.org/abs/2305.10429","description":"<p>The mixture proportions of pretraining data domains (e.g., Wikipedia, books,\nweb text) greatly affect language model (LM) performance. In this paper, we\npropose Domain Reweighting with Minimax Optimization (DoReMi), which first\ntrains a small proxy model using group distributionally robust optimization\n(Group DRO) over domains to produce domain weights (mixture proportions)\nwithout knowledge of downstream tasks. We then resample a dataset with these\ndomain weights and train a larger, full-sized model. In our experiments, we use\nDoReMi on a 280M-parameter proxy model to find domain weights for training an\n8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves\nperplexity across all domains, even when it downweights a domain. DoReMi\nimproves average few-shot downstream accuracy by 6.5% over a baseline model\ntrained using The Pile's default domain weights and reaches the baseline\naccuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has\nno knowledge of downstream tasks, even matches the performance of using domain\nweights tuned on downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sang Michael Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1\">Hieu Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xuanyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1\">Nan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hanxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yifeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Adams Wei Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mining Legal Arguments in Court Decisions. (arXiv:2208.06178v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.06178","description":"<p>Identifying, classifying, and analyzing arguments in legal discourse has been\na prominent area of research since the inception of the argument mining field.\nHowever, there has been a major discrepancy between the way natural language\nprocessing (NLP) researchers model and annotate arguments in court decisions\nand the way legal experts understand and analyze legal argumentation. While\ncomputational approaches typically simplify arguments into generic premises and\nclaims, arguments in legal research usually exhibit a rich typology that is\nimportant for gaining insights into the particular case and applications of law\nin general. We address this problem and make several substantial contributions\nto move the field forward. First, we design a new annotation scheme for legal\narguments in proceedings of the European Court of Human Rights (ECHR) that is\ndeeply rooted in the theory and practice of legal argumentation research.\nSecond, we compile and annotate a large corpus of 373 court decisions (2.3M\ntokens and 15k annotated argument spans). Finally, we train an argument mining\nmodel that outperforms state-of-the-art models in the legal NLP domain and\nprovide a thorough expert-based evaluation. All datasets and source codes are\navailable under open lincenses at\nhttps://github.com/trusthlt/mining-legal-arguments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Habernal_I/0/1/0/all/0/1\">Ivan Habernal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faber_D/0/1/0/all/0/1\">Daniel Faber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recchia_N/0/1/0/all/0/1\">Nicola Recchia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bretthauer_S/0/1/0/all/0/1\">Sebastian Bretthauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohmann_I/0/1/0/all/0/1\">Indra Spiecker genannt D&#xf6;hmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burchard_C/0/1/0/all/0/1\">Christoph Burchard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KGLM: Integrating Knowledge Graph Structure in Language Models for Link Prediction. (arXiv:2211.02744v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.02744","description":"<p>The ability of knowledge graphs to represent complex relationships at scale\nhas led to their adoption for various needs including knowledge representation,\nquestion-answering, and recommendation systems. Knowledge graphs are often\nincomplete in the information they represent, necessitating the need for\nknowledge graph completion tasks. Pre-trained and fine-tuned language models\nhave shown promise in these tasks although these models ignore the intrinsic\ninformation encoded in the knowledge graph, namely the entity and relation\ntypes. In this work, we propose the Knowledge Graph Language Model (KGLM)\narchitecture, where we introduce a new entity/relation embedding layer that\nlearns to differentiate distinctive entity and relation types, therefore\nallowing the model to learn the structure of the knowledge graph. In this work,\nwe show that further pre-training the language models with this additional\nembedding layer using the triples extracted from the knowledge graph, followed\nby the standard fine-tuning phase sets a new state-of-the-art performance for\nthe link prediction task on the benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Youn_J/0/1/0/all/0/1\">Jason Youn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagkopoulos_I/0/1/0/all/0/1\">Ilias Tagkopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Inclusive Notion of Text. (arXiv:2211.05604v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05604","description":"<p>Natural language processing (NLP) researchers develop models of grammar,\nmeaning and communication based on written text. Due to task and data\ndifferences, what is considered text can vary substantially across studies. A\nconceptual framework for systematically capturing these differences is lacking.\nWe argue that clarity on the notion of text is crucial for reproducible and\ngeneralizable NLP. Towards that goal, we propose common terminology to discuss\nthe production and transformation of textual data, and introduce a two-tier\ntaxonomy of linguistic and non-linguistic elements that are available in\ntextual sources and can be used in NLP modeling. We apply this taxonomy to\nsurvey existing work that extends the notion of text beyond the conservative\nlanguage-centered view. We outline key desiderata and challenges of the\nemerging inclusive approach to text in NLP, and suggest community-level\nreporting as a crucial next step to consolidate the discussion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_I/0/1/0/all/0/1\">Ilia Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Exploration of Knowledge-Preserving Prompts for Document Summarisation. (arXiv:2301.11719v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.11719","description":"<p>Despite the great development of document summarisation techniques nowadays,\nfactual inconsistencies between the generated summaries and the original texts\nstill occur from time to time. This study explores the possibility of adopting\nprompts to incorporate factual knowledge into generated summaries. We\nspecifically study prefix-tuning that uses a set of trainable continuous prefix\nprompts together with discrete natural language prompts to aid summary\ngeneration. Experimental results demonstrate that the trainable prefixes can\nhelp the summarisation model extract information from discrete prompts\nprecisely, thus generating knowledge-preserving summaries that are factually\nconsistent with the discrete prompts. The ROUGE improvements of the generated\nsummaries indicate that explicitly adding factual knowledge into the\nsummarisation process could boost the overall performance, showing great\npotential for applying it to other natural language processing tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Emma Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakeri_A/0/1/0/all/0/1\">Alireza Seyed Shakeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiza_M/0/1/0/all/0/1\">Makhmoor Fiza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation. (arXiv:2303.12112v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2303.12112","description":"<p>The CLIP model has been recently proven to be very effective for a variety of\ncross-modal tasks, including the evaluation of captions generated from\nvision-and-language architectures. In this paper, we propose a new recipe for a\ncontrastive-based evaluation metric for image captioning, namely\nPositive-Augmented Contrastive learning Score (PAC-S), that in a novel way\nunifies the learning of a contrastive visual-semantic space with the addition\nof generated images and text on curated data. Experiments spanning several\ndatasets demonstrate that our new metric achieves the highest correlation with\nhuman judgments on both images and videos, outperforming existing\nreference-based metrics like CIDEr and SPICE and reference-free metrics like\nCLIP-Score. Finally, we test the system-level correlation of the proposed\nmetric when considering popular image captioning approaches, and assess the\nimpact of employing different cross-modal features. Our source code and trained\nmodels are publicly available at: https://github.com/aimagelab/pacscore.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarto_S/0/1/0/all/0/1\">Sara Sarto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barraco_M/0/1/0/all/0/1\">Manuele Barraco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UKP-SQuARE v3: A Platform for Multi-Agent QA Research. (arXiv:2303.18120v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.18120","description":"<p>The continuous development of Question Answering (QA) datasets has drawn the\nresearch community's attention toward multi-domain models. A popular approach\nis to use multi-dataset models, which are models trained on multiple datasets\nto learn their regularities and prevent overfitting to a single dataset.\nHowever, with the proliferation of QA models in online repositories such as\nGitHub or Hugging Face, an alternative is becoming viable. Recent works have\ndemonstrated that combining expert agents can yield large performance gains\nover multi-dataset models. To ease research in multi-agent models, we extend\nUKP-SQuARE, an online platform for QA research, to support three families of\nmulti-agent systems: i) agent selection, ii) early-fusion of agents, and iii)\nlate-fusion of agents. We conduct experiments to evaluate their inference speed\nand discuss the performance vs. speed trade-off compared to multi-dataset\nmodels. UKP-SQuARE is open-source and publicly available at\n<a href=\"http://square.ukp-lab.de.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Puerto_H/0/1/0/all/0/1\">Haritz Puerto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumgartner_T/0/1/0/all/0/1\">Tim Baumg&#xe4;rtner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachdeva_R/0/1/0/all/0/1\">Rachneet Sachdeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Haishuo Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tariverdian_S/0/1/0/all/0/1\">Sewin Tariverdian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RPTQ: Reorder-based Post-training Quantization for Large Language Models. (arXiv:2304.01089v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01089","description":"<p>Large-scale language models (LLMs) have demonstrated impressive performance,\nbut their deployment presents challenges due to their significant memory usage.\nThis issue can be alleviated through quantization. In this paper, we identify\nthat the challenge in quantizing activations in LLMs arises from varying ranges\nacross channels, rather than solely the presence of outliers. To address this\nchallenge, we introduce a quantization method called RPTQ, which utilizes a\nreorder-based approach. By rearranging the channels and quantizing them in\nclusters, RPTQ effectively mitigates the impact of range differences between\nchannels. To minimize the overhead of the reorder operation, we fuse it into\nthe layer norm operation and weights in linear layers. In our experiments, RPTQ\nachieved a significant breakthrough by utilizing 3-bit activation in LLMs for\nthe first time, resulting in a substantial reduction in memory usage. For\ninstance, quantizing OPT-175b can lead to a memory consumption reduction of up\nto 80%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhihang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Lin Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1\">Yuzhang Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiaxiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bingzhe Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of US Supreme Court Cases using BERT-Based Techniques. (arXiv:2304.08649v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.08649","description":"<p>Models based on bidirectional encoder representations from transformers\n(BERT) produce state of the art (SOTA) results on many natural language\nprocessing (NLP) tasks such as named entity recognition (NER), part-of-speech\n(POS) tagging etc. An interesting phenomenon occurs when classifying long\ndocuments such as those from the US supreme court where BERT-based models can\nbe considered difficult to use on a first-pass or out-of-the-box basis. In this\npaper, we experiment with several BERT-based classification techniques for US\nsupreme court decisions or supreme court database (SCDB) and compare them with\nthe previous SOTA results. We then compare our results specifically with SOTA\nmodels for long documents. We compare our results for two classification tasks:\n(1) a broad classification task with 15 categories and (2) a fine-grained\nclassification task with 279 categories. Our best result produces an accuracy\nof 80\\% on the 15 broad categories and 60\\% on the fine-grained 279 categories\nwhich marks an improvement of 8\\% and 28\\% respectively from previously\nreported SOTA results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vatsal_S/0/1/0/all/0/1\">Shubham Vatsal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyers_A/0/1/0/all/0/1\">Adam Meyers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_J/0/1/0/all/0/1\">John E. Ortega</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner. (arXiv:2305.01711v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.01711","description":"<p>Language models (LMs) trained on vast quantities of unlabelled data have\ngreatly advanced the field of natural language processing (NLP). In this study,\nwe re-visit the widely accepted notion in NLP that continued pre-training LMs\non task-related texts improves the performance of fine-tuning (FT) in\ndownstream tasks. Through experiments on eight single-sentence tasks and eight\nsentence-pair tasks in both semi-supervised and fully-supervised settings, we\nfind that conventional continued pre-training does not consistently provide\nbenefits and can even be detrimental for sentence-pair tasks or when\nprompt-based FT is used. To tackle these issues, we propose Prompt-based\nContinued Pre-training (PCP), which combines the idea of instruction tuning\nwith conventional continued pre-training. Our approach aims to improve the\nperformance of prompt-based FT by presenting both task-related texts and prompt\ntemplates to LMs through unsupervised pre-training objectives before\nfine-tuning for the target task. Our empirical evaluations on 21 benchmarks\ndemonstrate that the PCP consistently improves the performance of\nstate-of-the-art prompt-based FT approaches (up to 20.1% absolute) in both\nsemi-supervised and fully-supervised settings, even with only hundreds of\nunlabelled examples. Additionally, prompt-based FT with the PCP outperforms\nstate-of-the-art semi-supervised approaches with greater simplicity,\neliminating the need for an iterative process and extra data augmentation. Our\nfurther analysis explores the performance lower bound of the PCP and reveals\nthat the advantages of PCP persist across different sizes of models and\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhengxiang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1\">Aldo Lipani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory. (arXiv:2305.02437v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.02437","description":"<p>With direct access to human-written reference as memory, retrieval-augmented\ngeneration has achieved much progress in a wide range of text generation tasks.\nSince better memory would typically prompt better generation~(we define this as\nprimal problem). The traditional approach for memory retrieval involves\nselecting memory that exhibits the highest similarity to the input. However,\nthis method is constrained by the quality of the fixed corpus from which memory\nis retrieved. In this paper, by exploring the duality of the primal problem:\nbetter generation also prompts better memory, we propose a novel framework,\nselfmem, which addresses this limitation by iteratively employing a\nretrieval-augmented generator to create an unbounded memory pool and using a\nmemory selector to choose one output as memory for the subsequent generation\nround. This enables the model to leverage its own output, referred to as\nself-memory, for improved generation. We evaluate the effectiveness of selfmem\non three distinct text generation tasks: neural machine translation,\nabstractive text summarization, and dialogue generation, under two generation\nparadigms: fine-tuned small model and few-shot LLM. Our approach achieves\nstate-of-the-art results in four directions in JRC-Acquis, XSum (50.3 ROUGE-1),\nand BigPatent (62.9 ROUGE-1), demonstrating the potential of self-memory in\nenhancing retrieval-augmented generation models. Furthermore, we conduct\nthorough analyses of each component in the selfmem framework to identify\nbottlenecks and provide insights for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xin Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1\">Di Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Politics of Language Choice: How the Russian-Ukrainian War Influences Ukrainians' Language Use on Twitter. (arXiv:2305.02770v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2305.02770","description":"<p>The use of language is innately political and often a vehicle of cultural\nidentity as well as the basis for nation building. Here, we examine language\nchoice and tweeting activity of Ukrainian citizens based on more than 4 million\ngeo-tagged tweets from over 62,000 users before and during the\nRussian-Ukrainian War, from January 2020 to October 2022. Using statistical\nmodels, we disentangle sample effects, arising from the in- and outflux of\nusers on Twitter, from behavioural effects, arising from behavioural changes of\nthe users. We observe a steady shift from the Russian language towards the\nUkrainian language already before the war, which drastically speeds up with its\noutbreak. We attribute these shifts in large part to users' behavioural\nchanges. Notably, we find that more than half of the Russian-tweeting users\nperform a hard-switch to Ukrainian as a result of the war.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Racek_D/0/1/0/all/0/1\">Daniel Racek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davidson_B/0/1/0/all/0/1\">Brittany I. Davidson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thurner_P/0/1/0/all/0/1\">Paul W. Thurner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kauermann_G/0/1/0/all/0/1\">G&#xf6;ran Kauermann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAMO-NLP at SemEval-2023 Task 2: A Unified Retrieval-augmented System for Multilingual Named Entity Recognition. (arXiv:2305.03688v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03688","description":"<p>The MultiCoNER \\RNum{2} shared task aims to tackle multilingual named entity\nrecognition (NER) in fine-grained and noisy scenarios, and it inherits the\nsemantic ambiguity and low-context setting of the MultiCoNER \\RNum{1} task. To\ncope with these problems, the previous top systems in the MultiCoNER \\RNum{1}\neither incorporate the knowledge bases or gazetteers. However, they still\nsuffer from insufficient knowledge, limited context length, single retrieval\nstrategy. In this paper, our team \\textbf{DAMO-NLP} proposes a unified\nretrieval-augmented system (U-RaNER) for fine-grained multilingual NER. We\nperform error analysis on the previous top systems and reveal that their\nperformance bottleneck lies in insufficient knowledge. Also, we discover that\nthe limited context length causes the retrieval knowledge to be invisible to\nthe model. To enhance the retrieval context, we incorporate the entity-centric\nWikidata knowledge base, while utilizing the infusion approach to broaden the\ncontextual scope of the model. Also, we explore various search strategies and\nrefine the quality of retrieval knowledge. Our system\\footnote{We will release\nthe dataset, code, and scripts of our system at {\\small\n\\url{https://github.com/modelscope/AdaSeq/tree/master/examples/U-RaNER}}.} wins\n9 out of 13 tracks in the MultiCoNER \\RNum{2} shared task. Additionally, we\ncompared our system with ChatGPT, one of the large language models which have\nunlocked strong capabilities on many tasks. The results show that there is\nstill much room for improvement for ChatGPT on the extraction task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zeqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zixia Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jiong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weiming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"K-UniMorph: Korean Universal Morphology and its Feature Schema. (arXiv:2305.06335v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.06335","description":"<p>We present in this work a new Universal Morphology dataset for Korean.\nPreviously, the Korean language has been underrepresented in the field of\nmorphological paradigms amongst hundreds of diverse world languages. Hence, we\npropose this Universal Morphological paradigms for the Korean language that\npreserve its distinct characteristics. For our K-UniMorph dataset, we outline\neach grammatical criterion in detail for the verbal endings, clarify how to\nextract inflected forms, and demonstrate how we generate the morphological\nschemata. This dataset adopts morphological feature schema from Sylak-Glassman\net al. (2015) and Sylak-Glassman (2016) for the Korean language as we extract\ninflected verb forms from the Sejong morphologically analyzed corpus that is\none of the largest annotated corpora for Korean. During the data creation, our\nmethodology also includes investigating the correctness of the conversion from\nthe Sejong corpus. Furthermore, we carry out the inflection task using three\ndifferent Korean word forms: letters, syllables and morphemes. Finally, we\ndiscuss and describe future perspectives on Korean morphological paradigms and\nthe dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jo_E/0/1/0/all/0/1\">Eunkyul Leah Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyuwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xihan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1\">KyungTae Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jungyeul Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chulwoo Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain-of-Dictionary Prompting Elicits Translation in Large Language Models. (arXiv:2305.06575v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.06575","description":"<p>Large language models (LLMs) have shown surprisingly good performance in\nmultilingual neural machine translation (MNMT) even when trained without\nparallel data. Yet, despite the fact that the amount of training data is\ngigantic, they still struggle with translating rare words, particularly for\nlow-resource languages. Even worse, it is usually unrealistic to retrieve\nrelevant demonstrations for in-context learning with low-resource languages on\nLLMs, which restricts the practical use of LLMs for translation -- how should\nwe mitigate this problem? To this end, we present a novel method, CoD, which\naugments LLMs with prior knowledge with the chains of multilingual dictionaries\nfor a subset of input words to elicit translation abilities for LLMs. Extensive\nexperiments indicate that augmenting ChatGPT with CoD elicits large gains by up\nto 13x chrF++ points for MNMT (3.08 to 42.63 for English to Serbian written in\nCyrillic script) on FLORES-200 full devtest set. We further demonstrate the\nimportance of chaining the multilingual dictionaries, as well as the\nsuperiority of CoD to few-shot demonstration for low-resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hongyuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haoyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haoran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Large Language Models in Conversational Recommender Systems. (arXiv:2305.07961v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2305.07961","description":"<p>A Conversational Recommender System (CRS) offers increased transparency and\ncontrol to users by enabling them to engage with the system through a real-time\nmulti-turn dialogue. Recently, Large Language Models (LLMs) have exhibited an\nunprecedented ability to converse naturally and incorporate world knowledge and\ncommon-sense reasoning into language understanding, unlocking the potential of\nthis paradigm. However, effectively leveraging LLMs within a CRS introduces new\ntechnical challenges, including properly understanding and controlling a\ncomplex conversation and retrieving from external sources of information. These\nissues are exacerbated by a large, evolving item corpus and a lack of\nconversational data for training. In this paper, we provide a roadmap for\nbuilding an end-to-end large-scale CRS using LLMs. In particular, we propose\nnew implementations for user preference understanding, flexible dialogue\nmanagement and explainable recommendations as part of an integrated\narchitecture powered by LLMs. For improved personalization, we describe how an\nLLM can consume interpretable natural language user profiles and use them to\nmodulate session-level context. To overcome conversational data limitations in\nthe absence of an existing production CRS, we propose techniques for building a\ncontrollable LLM-based user simulator to generate synthetic conversations. As a\nproof of concept we introduce RecLLM, a large-scale CRS for YouTube videos\nbuilt on LaMDA, and demonstrate its fluency and diverse functionality through\nsome illustrative example conversations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Friedman_L/0/1/0/all/0/1\">Luke Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_S/0/1/0/all/0/1\">Sameer Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allen_D/0/1/0/all/0/1\">David Allen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhenning Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidahmed_H/0/1/0/all/0/1\">Hakim Sidahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Changbo Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubiner_G/0/1/0/all/0/1\">Gabriel Schubiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Ajay Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lara_H/0/1/0/all/0/1\">Harsh Lara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_B/0/1/0/all/0/1\">Brian Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zexi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_M/0/1/0/all/0/1\">Manoj Tiwari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. (arXiv:2305.08322v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08322","description":"<p>New NLP benchmarks are urgently needed to align with the rapid development of\nlarge language models (LLMs). We present C-Eval, the first comprehensive\nChinese evaluation suite designed to assess advanced knowledge and reasoning\nabilities of foundation models in a Chinese context. C-Eval comprises\nmultiple-choice questions across four difficulty levels: middle school, high\nschool, college, and professional. The questions span 52 diverse disciplines,\nranging from humanities to science and engineering. C-Eval is accompanied by\nC-Eval Hard, a subset of very challenging subjects in C-Eval that requires\nadvanced reasoning abilities to solve. We conduct a comprehensive evaluation of\nthe most advanced LLMs on C-Eval, including both English- and Chinese-oriented\nmodels. Results indicate that only GPT-4 could achieve an average accuracy of\nover 60%, suggesting that there is still significant room for improvement for\ncurrent LLMs. We anticipate C-Eval will help analyze important strengths and\nshortcomings of foundation models, and foster their development and growth for\nChinese users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuzhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yuzhuo Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junlei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinghan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_T/0/1/0/all/0/1\">Tangjun Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junteng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1\">Chuancheng Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yikai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jiayi Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junxian He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLG Evaluation Metrics Beyond Correlation Analysis: An Empirical Metric Preference Checklist. (arXiv:2305.08566v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.08566","description":"<p>In this study, we analyze NLG automatic metrics based on whether human\nevaluation aspect is used as context or objective to compute the metrics: (i)\nTask-agnostic and (ii) Human-aligned. Task-agnostic metrics, such as\nPerplexity, BLEU, BERTScore, are cost-effective and highly adaptable to diverse\nNLG tasks, yet they have a weak correlation with human. Human-aligned metrics\n(CTC, CtrlEval, UniEval) improves correlation level by incorporating desirable\nhuman-like qualities as training objective. However, their effectiveness at\ndiscerning system-level performance and quality of system outputs remains\nunclear.\n</p>\n<p>We present metric preference checklist as a framework to assess the\ndiscriminative power of automatic metrics in three NLG tasks: Text\nSummarization, Dialogue Response Generation, and Controlled Generation. We show\nthat multi-aspect human-aligned metric (UniEval) is not necessarily dominant\nover single-aspect human-aligned metrics (CTC, CtrlEval) and task-agnostic\nmetrics (BLEU, BERTScore), particularly when a disagreement between human\nevaluation aspects is present. We also show particular use cases in which\nautomatic metrics provide a better guidance than human on discriminating\nsystem-level performance. Our proposed framework provides access: (i) for\nverifying whether automatic metrics are faithful to human preference,\nregardless their correlation level to human; and (ii) for scrutinizing the\nstrengths and limitations of NLG systems, which are often obscured by a\nstandard averaging method of evaluation scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nimah_I/0/1/0/all/0/1\">Iftitahu Ni&#x27;mah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1\">Vlado Menkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTTM: Leveraging Contextualized Word Embeddings from Pre-trained Language Models for Neural Topic Modeling. (arXiv:2305.09329v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.09329","description":"<p>With the development of neural topic models in recent years, topic modelling\nis playing an increasingly important role in natural language understanding.\nHowever, most existing topic models still rely on bag-of-words (BoW)\ninformation, either as training input or training target. This limits their\nability to capture word order information in documents and causes them to\nsuffer from the out-of-vocabulary (OOV) issue, i.e. they cannot handle\nunobserved words in new documents. Contextualized word embeddings from\npre-trained language models show superiority in the ability of word sense\ndisambiguation and prove to be effective in dealing with OOV words. In this\nwork, we developed a novel neural topic model combining contextualized word\nembeddings from the pre-trained language model BERT. The model can infer the\ntopic distribution of a document without using any BoW information. In\naddition, the model can infer the topic distribution of each word in a document\ndirectly from the contextualized word embeddings. Experiments on several\ndatasets show that our model outperforms existing topic models in terms of both\ndocument classification and topic coherence metrics and can accommodate unseen\nwords from newly arrived documents. Experiments on the NER dataset also show\nthat our model can produce high-quality word topic representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Procter_R/0/1/0/all/0/1\">Rob Procter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GIFT: Graph-Induced Fine-Tuning for Multi-Party Conversation Understanding. (arXiv:2305.09360v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.09360","description":"<p>Addressing the issues of who saying what to whom in multi-party conversations\n(MPCs) has recently attracted a lot of research attention. However, existing\nmethods on MPC understanding typically embed interlocutors and utterances into\nsequential information flows, or utilize only the superficial of inherent graph\nstructures in MPCs. To this end, we present a plug-and-play and lightweight\nmethod named graph-induced fine-tuning (GIFT) which can adapt various\nTransformer-based pre-trained language models (PLMs) for universal MPC\nunderstanding. In detail, the full and equivalent connections among utterances\nin regular Transformer ignore the sparse but distinctive dependency of an\nutterance on another in MPCs. To distinguish different relationships between\nutterances, four types of edges are designed to integrate graph-induced signals\ninto attention mechanisms to refine PLMs originally designed for processing\nsequential texts. We evaluate GIFT by implementing it into three PLMs, and test\nthe performance on three downstream tasks including addressee recognition,\nspeaker identification and response selection. Experimental results show that\nGIFT can significantly improve the performance of three PLMs on three\ndownstream tasks and two benchmarks with only 4 additional parameters per\nencoding layer, achieving new state-of-the-art performance on MPC\nunderstanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jia-Chen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhen-Hua Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1\">Guoping Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Life of PII -- A PII Obfuscation Transformer. (arXiv:2305.09550v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.09550","description":"<p>Protecting sensitive information is crucial in today's world of Large\nLanguage Models (LLMs) and data-driven services. One common method used to\npreserve privacy is by using data perturbation techniques to reduce\noverreaching utility of (sensitive) Personal Identifiable Information (PII)\ndata while maintaining its statistical and semantic properties. Data\nperturbation methods often result in significant information loss, making them\nimpractical for use. In this paper, we propose 'Life of PII', a novel\nObfuscation Transformer framework for transforming PII into faux-PII while\npreserving the original information, intent, and context as much as possible.\nOur approach includes an API to interface with the given document, a\nconfiguration-based obfuscator, and a model based on the Transformer\narchitecture, which has shown high context preservation and performance in\nnatural language processing tasks and LLMs.\n</p>\n<p>Our Transformer-based approach learns mapping between the original PII and\nits transformed faux-PII representation, which we call \"obfuscated\" data. Our\nexperiments demonstrate that our method, called Life of PII, outperforms\ntraditional data perturbation techniques in terms of both utility preservation\nand privacy protection. We show that our approach can effectively reduce\nutility loss while preserving the original information, offering greater\nflexibility in the trade-off between privacy protection and data utility. Our\nwork provides a solution for protecting PII in various real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deshmukh_A/0/1/0/all/0/1\">Ajinkya Deshmukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banthia_S/0/1/0/all/0/1\">Saumya Banthia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Anantha Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Satisfiability-Aided Language Models Using Declarative Prompting. (arXiv:2305.09656v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.09656","description":"<p>Prior work has combined chain-of-thought prompting in large language models\n(LLMs) with programmatic representations to perform effective and transparent\nreasoning. While such an approach works very well for tasks that only require\nforward reasoning (e.g., straightforward arithmetic), it is less effective for\nconstraint solving problems that require more sophisticated planning and\nsearch. In this paper, we propose a new satisfiability-aided language modeling\n(SATLM) approach for improving the reasoning capabilities of LLMs. We use an\nLLM to generate a declarative task specification rather than an imperative\nprogram and leverage an off-the-shelf automated theorem prover to derive the\nfinal answer. This approach has two key advantages. The declarative\nspecification is closer to the problem description than the reasoning steps\nare, so the LLM can parse it out of the description more accurately.\nFurthermore, by offloading the actual reasoning task to an automated theorem\nprover, our approach can guarantee the correctness of the answer with respect\nto the parsed specification and avoid planning errors in the solving process.\nWe evaluate SATLM on 6 different datasets and show that it consistently\noutperforms program-aided LMs in an imperative paradigm. In particular, SATLM\noutperforms program-aided LMs by 23% on a challenging subset of the GSM\narithmetic reasoning dataset; SATLM also achieves a new SoTA on LSAT,\nsurpassing previous models that are trained on the full training set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiaochu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dillig_I/0/1/0/all/0/1\">Isil Dillig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pruning Pre-trained Language Models Without Fine-Tuning. (arXiv:2210.06210v2 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2210.06210","description":"<p>To overcome the overparameterized problem in Pre-trained Language Models\n(PLMs), pruning is widely used as a simple and straightforward compression\nmethod by directly removing unimportant weights. Previous first-order methods\nsuccessfully compress PLMs to extremely high sparsity with little performance\ndrop. These methods, such as movement pruning, use first-order information to\nprune PLMs while fine-tuning the remaining weights. In this work, we argue\nfine-tuning is redundant for first-order pruning, since first-order pruning is\nsufficient to converge PLMs to downstream tasks without fine-tuning. Under this\nmotivation, we propose Static Model Pruning (SMP), which only uses first-order\npruning to adapt PLMs to downstream tasks while achieving the target sparsity\nlevel. In addition, we also design a new masking function and training\nobjective to further improve SMP. Extensive experiments at various sparsity\nlevels show SMP has significant improvements over first-order and zero-order\nmethods. Unlike previous first-order methods, SMP is also applicable to low\nsparsity and outperforms zero-order methods. Meanwhile, SMP is more parameter\nefficient than other methods due to it does not require fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Ting Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Deqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Ruobing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Feng Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-05-17T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/"}}]}]}