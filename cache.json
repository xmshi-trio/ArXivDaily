{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-01-03T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Memory Augmented Lookup Dictionary based Language Modeling for Automatic Speech Recognition. (arXiv:2301.00066v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00066","description":"<p>Recent studies have shown that using an external Language Model (LM) benefits\nthe end-to-end Automatic Speech Recognition (ASR). However, predicting tokens\nthat appear less frequently in the training set is still quite challenging. The\nlong-tail prediction problems have been widely studied in many applications,\nbut only been addressed by a few studies for ASR and LMs. In this paper, we\npropose a new memory augmented lookup dictionary based Transformer architecture\nfor LM. The newly introduced lookup dictionary incorporates rich contextual\ninformation in training set, which is vital to correctly predict long-tail\ntokens. With intensive experiments on Chinese and English data sets, our\nproposed method is proved to outperform the baseline Transformer LM by a great\nmargin on both word/character error rate and tail tokens error rate. This is\nachieved without impact on the decoding efficiency. Overall, we demonstrate the\neffectiveness of our proposed method in boosting the ASR decoding performance,\nespecially for long-tail tokens.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yukun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_M/0/1/0/all/0/1\">Ming Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_R/0/1/0/all/0/1\">Rui Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chuanzeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Inconsistencies of Conditionals Learned by Masked Language Models. (arXiv:2301.00068v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00068","description":"<p>Learning to predict masked tokens in a sequence has been shown to be a\npowerful pretraining objective for large-scale language models. After training,\nsuch masked language models can provide distributions of tokens conditioned on\nbidirectional context.\n</p>\n<p>In this short draft, we show that such bidirectional conditionals often\ndemonstrate considerable inconsistencies, i.e., they can not be derived from a\ncoherent joint distribution when considered together. We empirically quantify\nsuch inconsistencies in the simple scenario of bigrams for two common styles of\nmasked language models: T5-style and BERT-style. For example, we show that T5\nmodels often confuse its own preference regarding two similar bigrams.\n</p>\n<p>Such inconsistencies may represent a theoretical pitfall for the research\nwork on sampling sequences based on the bidirectional conditionals learned by\nBERT-style MLMs. This phenomenon also means that T5-style MLMs capable of\ninfilling will generate discrepant results depending on how much masking is\ngiven, which may represent a particular trust issue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Young_T/0/1/0/all/0/1\">Tom Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Proactively Forecasting Sentence-Specific Information Popularity within Online News Documents. (arXiv:2301.00152v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00152","description":"<p>Multiple studies have focused on predicting the prospective popularity of an\nonline document as a whole, without paying attention to the contributions of\nits individual parts. We introduce the task of proactively forecasting\npopularities of sentences within online news documents solely utilizing their\nnatural language content. We model sentence-specific popularity forecasting as\na sequence regression task. For training our models, we curate InfoPop, the\nfirst dataset containing popularity labels for over 1.7 million sentences from\nover 50,000 online news documents. To the best of our knowledge, this is the\nfirst dataset automatically created using streams of incoming search engine\nqueries to generate sentence-level popularity annotations. We propose a novel\ntransfer learning approach involving sentence salience prediction as an\nauxiliary task. Our proposed technique coupled with a BERT-based neural model\nexceeds nDCG values of 0.8 for proactive sentence-specific popularity\nforecasting. Notably, our study presents a non-trivial takeaway: though\npopularity and salience are different concepts, transfer learning from salience\nprediction enhances popularity forecasting. We release InfoPop and make our\ncode publicly available: https://github.com/sayarghoshroy/InfoPopularity\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Sayar Ghosh Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padhi_A/0/1/0/all/0/1\">Anshul Padhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Risubh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_V/0/1/0/all/0/1\">Vasudeva Varma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logic Mill -- A Knowledge Navigation System. (arXiv:2301.00200v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00200","description":"<p>Logic Mill is a scalable and openly accessible software system that\nidentifies semantically similar documents within either one domain-specific\ncorpus or multi-domain corpora. It uses advanced Natural Language Processing\n(NLP) techniques to generate numerical representations of documents. Currently\nit leverages a large pre-trained language model to generate these document\nrepresentations. The system focuses on scientific publications and patent\ndocuments and contains more than 200 million documents. It is easily accessible\nvia a simple Application Programming Interface (API) or via a web interface.\nMoreover, it is continuously being updated and can be extended to text corpora\nfrom other domains. We see this system as a general-purpose tool for future\nresearch applications in the social sciences and other domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Erhardt_S/0/1/0/all/0/1\">Sebastian Erhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_M/0/1/0/all/0/1\">Mainak Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buunk_E/0/1/0/all/0/1\">Erik Buunk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rose_M/0/1/0/all/0/1\">Michael E. Rose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harhoff_D/0/1/0/all/0/1\">Dietmar Harhoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey for In-context Learning. (arXiv:2301.00234v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00234","description":"<p>With the increasing ability of large language models (LLMs), in-context\nlearning (ICL) has become a new paradigm for natural language processing (NLP),\nwhere LLMs make predictions only based on contexts augmented with a few\ntraining examples. It has been a new trend exploring ICL to evaluate and\nextrapolate the ability of LLMs. In this paper, we aim to survey and summarize\nthe progress, challenges, and future work in ICL. We first present a formal\ndefinition of ICL and clarify its correlation to related studies. Then, we\norganize and discuss advanced techniques of ICL, including training strategies,\nprompting strategies, and so on. Finally, we present the challenges of ICL and\nprovide potential directions for further research. We hope our work can\nencourage more research on uncovering how ICL works and improving ICL in future\nwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qingxiu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Damai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Ce Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking with Retrieval: Faithful Large Language Model Inference. (arXiv:2301.00303v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00303","description":"<p>Despite the success of large language models (LLMs) in various natural\nlanguage processing (NLP) tasks, the stored knowledge in these models may\ninevitably be incomplete, out-of-date, or incorrect. This motivates the need to\nutilize external knowledge to assist LLMs. Unfortunately, current methods for\nincorporating external knowledge often require additional training or\nfine-tuning, which can be costly and may not be feasible for LLMs. To address\nthis issue, we propose a novel post-processing approach, rethinking with\nretrieval (RR), which retrieves relevant external knowledge based on the\ndecomposed reasoning steps obtained from the chain-of-thought (CoT) prompting.\nThis lightweight approach does not require additional training or fine-tuning\nand is not limited by the input length of LLMs. We evaluate the effectiveness\nof RR through extensive experiments with GPT-3 on three complex reasoning\ntasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our\nresults show that RR can produce more faithful explanations and improve the\nperformance of LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hangfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sample-Efficient Unsupervised Domain Adaptation of Speech Recognition Systems A case study for Modern Greek. (arXiv:2301.00304v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00304","description":"<p>Modern speech recognition systems exhibits rapid performance degradation\nunder domain shift. This issue is especially prevalent in data-scarce settings,\nsuch as low-resource languages, where diversity of training data is limited. In\nthis work we propose M2DS2, a simple and sample-efficient finetuning strategy\nfor large pretrained speech models, based on mixed source and target domain\nself-supervision. We find that including source domain self-supervision\nstabilizes training and avoids mode collapse of the latent representations. For\nevaluation, we collect HParl, a $120$ hour speech corpus for Greek, consisting\nof plenary sessions in the Greek Parliament. We merge HParl with two popular\nGreek corpora to create GREC-MD, a test-bed for multi-domain evaluation of\nGreek ASR systems. In our experiments we find that, while other Unsupervised\nDomain Adaptation baselines fail in this resource-constrained environment,\nM2DS2 yields significant improvements for cross-domain adaptation, even when a\nonly a few hours of in-domain audio are available. When we relax the problem in\na weakly supervised setting, we find that independent adaptation for audio\nusing M2DS2 and language using simple LM augmentation techniques is\nparticularly effective, yielding word error rates comparable to the fully\nsupervised baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paraskevopoulos_G/0/1/0/all/0/1\">Georgios Paraskevopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouzelis_T/0/1/0/all/0/1\">Theodoros Kouzelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouvalis_G/0/1/0/all/0/1\">Georgios Rouvalis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsamanis_A/0/1/0/all/0/1\">Athanasios Katsamanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsouros_V/0/1/0/all/0/1\">Vassilis Katsouros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potamianos_A/0/1/0/all/0/1\">Alexandros Potamianos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relevance Classification of Flood-related Twitter Posts via Multiple Transformers. (arXiv:2301.00320v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00320","description":"<p>In recent years, social media has been widely explored as a potential source\nof communication and information in disasters and emergency situations. Several\ninteresting works and case studies of disaster analytics exploring different\naspects of natural disasters have been already conducted. Along with the great\npotential, disaster analytics comes with several challenges mainly due to the\nnature of social media content. In this paper, we explore one such challenge\nand propose a text classification framework to deal with Twitter noisy data.\nMore specifically, we employed several transformers both individually and in\ncombination, so as to differentiate between relevant and non-relevant Twitter\nposts, achieving the highest F1-score of 0.87.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mukhtiar_W/0/1/0/all/0/1\">Wisal Mukhtiar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizwan_W/0/1/0/all/0/1\">Waliiya Rizwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habib_A/0/1/0/all/0/1\">Aneela Habib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afridi_Y/0/1/0/all/0/1\">Yasir Saleem Afridi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_L/0/1/0/all/0/1\">Laiq Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_K/0/1/0/all/0/1\">Kashif Ahmad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Floods Relevancy and Identification of Location from Twitter Posts using NLP Techniques. (arXiv:2301.00321v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00321","description":"<p>This paper presents our solutions for the MediaEval 2022 task on DisasterMM.\nThe task is composed of two subtasks, namely (i) Relevance Classification of\nTwitter Posts (RCTP), and (ii) Location Extraction from Twitter Texts (LETT).\nThe RCTP subtask aims at differentiating flood-related and non-relevant social\nposts while LETT is a Named Entity Recognition (NER) task and aims at the\nextraction of location information from the text. For RCTP, we proposed four\ndifferent solutions based on BERT, RoBERTa, Distil BERT, and ALBERT obtaining\nan F1-score of 0.7934, 0.7970, 0.7613, and 0.7924, respectively. For LETT, we\nused three models namely BERT, RoBERTa, and Distil BERTA obtaining an F1-score\nof 0.6256, 0.6744, and 0.6723, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suleman_M/0/1/0/all/0/1\">Muhammad Suleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1\">Muhammad Asif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamir_T/0/1/0/all/0/1\">Tayyab Zamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehmood_A/0/1/0/all/0/1\">Ayaz Mehmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_J/0/1/0/all/0/1\">Jebran Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_N/0/1/0/all/0/1\">Nasir Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_K/0/1/0/all/0/1\">Kashif Ahmad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Second Thoughts are Best: Learning to Re-Align With Human Values from Text Edits. (arXiv:2301.00355v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00355","description":"<p>We present Second Thought, a new learning paradigm that enables language\nmodels (LMs) to re-align with human values. By modeling the chain-of-edits\nbetween value-unaligned and value-aligned text, with LM fine-tuning and\nadditional refinement through reinforcement learning, Second Thought not only\nachieves superior performance in three value alignment benchmark datasets but\nalso shows strong human-value transfer learning ability in few-shot scenarios.\nThe generated editing steps also offer better interpretability and ease for\ninteractive error correction. Extensive human evaluations further confirm its\neffectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Chenyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Ge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1\">Ziyu Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tony X Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing Readability Using Genetic Algorithms. (arXiv:2301.00374v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00374","description":"<p>This research presents ORUGA, a method that tries to automatically optimize\nthe readability of any text in English. The core idea behind the method is that\ncertain factors affect the readability of a text, some of which are\nquantifiable (number of words, syllables, presence or absence of adverbs, and\nso on). The nature of these factors allows us to implement a genetic learning\nstrategy to replace some existing words with their most suitable synonyms to\nfacilitate optimization. In addition, this research seeks to preserve both the\noriginal text's content and form through multi-objective optimization\ntechniques. In this way, neither the text's syntactic structure nor the\nsemantic content of the original message is significantly distorted. An\nexhaustive study on a substantial number and diversity of texts confirms that\nour method was able to optimize the degree of readability in all cases without\nsignificantly altering their form or meaning. The source code of this approach\nis available at https://github.com/jorge-martinez-gil/oruga.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Gil_J/0/1/0/all/0/1\">Jorge Martinez-Gil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CORGI-PM: A Chinese Corpus For Gender Bias Probing and Mitigation. (arXiv:2301.00395v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00395","description":"<p>As natural language processing (NLP) for gender bias becomes a significant\ninterdisciplinary topic, the prevalent data-driven techniques such as\nlarge-scale language models suffer from data inadequacy and biased corpus,\nespecially for languages with insufficient resources such as Chinese. To this\nend, we propose a Chinese cOrpus foR Gender bIas Probing and Mitigation\nCORGI-PM, which contains 32.9k sentences with high-quality labels derived by\nfollowing an annotation scheme specifically developed for gender bias in the\nChinese context. Moreover, we address three challenges for automatic textual\ngender bias mitigation, which requires the models to detect, classify, and\nmitigate textual gender bias. We also conduct experiments with state-of-the-art\nlanguage models to provide baselines. To our best knowledge, CORGI-PM is the\nfirst sentence-level Chinese corpus for gender bias probing and mitigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Ge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yizhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yaoyao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_J/0/1/0/all/0/1\">Jiayi Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jie Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inflected Forms Are Redundant in Question Generation Models. (arXiv:2301.00397v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00397","description":"<p>Neural models with an encoder-decoder framework provide a feasible solution\nto Question Generation (QG). However, after analyzing the model vocabulary we\nfind that current models (both RNN-based and pre-training based) have more than\n23\\% inflected forms. As a result, the encoder will generate separate\nembeddings for the inflected forms, leading to a waste of training data and\nparameters. Even worse, in decoding these models are vulnerable to irrelevant\nnoise and they suffer from high computational costs. In this paper, we propose\nan approach to enhance the performance of QG by fusing word transformation.\nFirstly, we identify the inflected forms of words from the input of encoder,\nand replace them with the root words, letting the encoder pay more attention to\nthe repetitive root words. Secondly, we propose to adapt QG as a combination of\nthe following actions in the encode-decoder framework: generating a question\nword, copying a word from the source sequence or generating a word\ntransformation type. Such extension can greatly decrease the size of predicted\nwords in the decoder as well as noise. We apply our approach to a typical\nRNN-based model and \\textsc{UniLM} to get the improved versions. We conduct\nextensive experiments on SQuAD and MS MARCO datasets. The experimental results\nshow that the improved versions can significantly outperform the corresponding\nbaselines in terms of BLEU, ROUGE-L and METEOR as well as time cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xingwu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hongyin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_c/0/1/0/all/0/1\">chengzhong Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Operator Prediction and Applications. (arXiv:2301.00399v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00399","description":"<p>In the present paper, semantic parsing challenges are briefly introduced and\nQDMR formalism in semantic parsing is implemented using sequence to sequence\nmodel with attention but uses only part of speech(POS) as a representation of\nwords of a sentence to make the training as simple and as fast as possible and\nalso avoiding curse of dimensionality as well as overfitting. It is shown how\nsemantic operator prediction could be augmented with other models like the\nCopyNet model or the recursive neural net model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Noravesh_F/0/1/0/all/0/1\">Farshad Noravesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is word segmentation necessary for Vietnamese sentiment classification?. (arXiv:2301.00418v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00418","description":"<p>To the best of our knowledge, this paper made the first attempt to answer\nwhether word segmentation is necessary for Vietnamese sentiment classification.\nTo do this, we presented five pre-trained monolingual S4- based language models\nfor Vietnamese, including one model without word segmentation, and four models\nusing RDRsegmenter, uitnlp, pyvi, or underthesea toolkits in the pre-processing\ndata phase. According to comprehensive experimental results on two corpora,\nincluding the VLSP2016-SA corpus of technical article reviews from the news and\nsocial media and the UIT-VSFC corpus of the educational survey, we have two\nsuggestions. Firstly, using traditional classifiers like Naive Bayes or Support\nVector Machines, word segmentation maybe not be necessary for the Vietnamese\nsentiment classification corpus, which comes from the social domain. Secondly,\nword segmentation is necessary for Vietnamese sentiment classification when\nword segmentation is used before using the BPE method and feeding into the deep\nlearning model. In this way, the RDRsegmenter is the stable toolkit for word\nsegmentation among the uitnlp, pyvi, and underthesea toolkits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duc-Vu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Semantic Representations Combined with Contextual Word Representations for Recognizing Textual Entailment in Vietnamese. (arXiv:2301.00422v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00422","description":"<p>RTE is a significant problem and is a reasonably active research community.\nThe proposed research works on the approach to this problem are pretty diverse\nwith many different directions. For Vietnamese, the RTE problem is moderately\nnew, but this problem plays a vital role in natural language understanding\nsystems. Currently, methods to solve this problem based on contextual word\nrepresentation learning models have given outstanding results. However,\nVietnamese is a semantically rich language. Therefore, in this paper, we want\nto present an experiment combining semantic word representation through the SRL\ntask with context representation of BERT relative models for the RTE problem.\nThe experimental results give conclusions about the influence and role of\nsemantic representation on Vietnamese in understanding natural language. The\nexperimental results show that the semantic-aware contextual representation\nmodel has about 1% higher performance than the model that does not incorporate\nsemantic representation. In addition, the effects on the data domain in\nVietnamese are also higher than those in English. This result also shows the\npositive influence of SRL on RTE problem in Vietnamese.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duong_Q/0/1/0/all/0/1\">Quoc-Loc Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duc-Vu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Semantic Information into Sketchy Reading Module of Retro-Reader for Vietnamese Machine Reading Comprehension. (arXiv:2301.00429v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00429","description":"<p>Machine Reading Comprehension has become one of the most advanced and popular\nresearch topics in the fields of Natural Language Processing in recent years.\nThe classification of answerability questions is a relatively significant\nsub-task in machine reading comprehension; however, there haven't been many\nstudies. Retro-Reader is one of the studies that has solved this problem\neffectively. However, the encoders of most traditional machine reading\ncomprehension models in general and Retro-Reader, in particular, have not been\nable to exploit the contextual semantic information of the context completely.\nInspired by SemBERT, we use semantic role labels from the SRL task to add\nsemantics to pre-trained language models such as mBERT, XLM-R, PhoBERT. This\nexperiment was conducted to compare the influence of semantics on the\nclassification of answerability for the Vietnamese machine reading\ncomprehension. Additionally, we hope this experiment will enhance the encoder\nfor the Retro-Reader model's Sketchy Reading Module. The improved Retro-Reader\nmodel's encoder with semantics was first applied to the Vietnamese Machine\nReading Comprehension task and obtained positive results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hang Thi-Thu Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_V/0/1/0/all/0/1\">Viet-Duc Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duc-Vu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Design Principle of Blockchain: An Initiative for the SoK of SoKs. (arXiv:2301.00479v1 [cs.CR])","link":"http://arxiv.org/abs/2301.00479","description":"<p>Blockchain, also coined as decentralized AI, has the potential to empower AI\nto be more trustworthy by creating a decentralized trust of privacy, security,\nand audibility. However, systematic studies on the design principle of\nBlockchain as a trust engine for an integrated society of\nCyber-Physical-Socia-System (CPSS) are still absent. In this article, we\nprovide an initiative for seeking the design principle of Blockchain for a\nbetter digital world. Using a hybrid method of qualitative and quantitative\nstudies, we examine the past origin, the current development, and the future\ndirections of Blockchain design principles. We have three findings. First, the\nanswers to whether Blockchain lives up to its original design principle as a\ndistributed database are controversial. Second, the current development of\nBlockchain community reveals a taxonomy of 7 categories, including privacy and\nsecurity, scalability, decentralization, applicability, governance and\nregulation, system design, and cross-chain interoperability. Both research and\npractice are more centered around the first category of privacy and security\nand the fourth category of applicability. Future scholars, practitioners, and\npolicy-makers have vast opportunities in other, much less exploited facets and\nthe synthesis at the interface of multiple aspects. Finally, in\ncounter-examples, we conclude that a synthetic solution that crosses discipline\nboundaries is necessary to close the gaps between the current design of\nBlockchain and the design principle of a trust engine for a truly intelligent\nworld.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sunshine Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Concept Knowledge Graph for User Next Intent Prediction at Alipay. (arXiv:2301.00503v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00503","description":"<p>This paper illustrates the technologies of user next intent prediction with a\nconcept knowledge graph. The system has been deployed on the Web at Alipay,\nserving more than 100 million daily active users. Specifically, we propose\nAlipayKG to explicitly characterize user intent, which is an offline concept\nknowledge graph in the Life-Service domain modeling the historical behaviors of\nusers, the rich content interacted by users and the relations between them. We\nfurther introduce a Transformer-based model which integrates expert rules from\nthe knowledge graph to infer the online user's next intent. Experimental\nresults demonstrate that the proposed system can effectively enhance the\nperformance of the downstream tasks while retaining explainability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yacheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Q/0/1/0/all/0/1\">Qianghuai Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruopeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1\">Yixin Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Statistical Machine Translation for Indic Languages. (arXiv:2301.00539v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00539","description":"<p>Machine Translation (MT) system generally aims at automatic representation of\nsource language into target language retaining the originality of context using\nvarious Natural Language Processing (NLP) techniques. Among various NLP\nmethods, Statistical Machine Translation(SMT). SMT uses probabilistic and\nstatistical techniques to analyze information and conversion. This paper\ncanvasses about the development of bilingual SMT models for translating English\nto fifteen low-resource Indian Languages (ILs) and vice versa. At the outset,\nall 15 languages are briefed with a short description related to our\nexperimental need. Further, a detailed analysis of Samanantar and OPUS dataset\nfor model building, along with standard benchmark dataset (Flores-200) for\nfine-tuning and testing, is done as a part of our experiment. Different\npreprocessing approaches are proposed in this paper to handle the noise of the\ndataset. To create the system, MOSES open-source SMT toolkit is explored.\nDistance reordering is utilized with the aim to understand the rules of grammar\nand context-dependent adjustments through a phrase reordering categorization\nframework. In our experiment, the quality of the translation is evaluated using\nstandard metrics such as BLEU, METEOR, and RIBES\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sudhansu Bala Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_D/0/1/0/all/0/1\">Divyajoti Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_T/0/1/0/all/0/1\">Tapas Kumar Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patra_B/0/1/0/all/0/1\">Bidyut Kr. Patra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using meaning instead of words to track topics. (arXiv:2301.00565v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00565","description":"<p>The ability to monitor the evolution of topics over time is extremely\nvaluable for businesses. Currently, all existing topic tracking methods use\nlexical information by matching word usage. However, no studies has ever\nexperimented with the use of semantic information for tracking topics. Hence,\nwe explore a novel semantic-based method using word embeddings. Our results\nshow that a semantic-based approach to topic tracking is on par with the\nlexical approach but makes different mistakes. This suggest that both methods\nmay complement each other.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poumay_J/0/1/0/all/0/1\">Judicael Poumay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ittoo_A/0/1/0/all/0/1\">Ashwin Ittoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysing Discrete Self Supervised Speech Representation for Spoken Language Modeling. (arXiv:2301.00591v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00591","description":"<p>This work profoundly analyzes discrete self-supervised speech representations\nthrough the eyes of Generative Spoken Language Modeling (GSLM). Following the\nfindings of such an analysis, we propose practical improvements to the discrete\nunit for the GSLM. First, we start comprehending these units by analyzing them\nin three axes: interpretation, visualization, and resynthesis. Our analysis\nfinds a high correlation between the speech units to phonemes and phoneme\nfamilies, while their correlation with speaker or gender is weaker.\nAdditionally, we found redundancies in the extracted units and claim that one\nreason may be the units' context. Following this analysis, we propose a new,\nunsupervised metric to measure unit redundancies. Finally, we use this metric\nto develop new methods that improve the robustness of units clustering and show\nsignificant improvement considering zero-resource speech metrics such as ABX.\nCode and analysis tools are available under the following link.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sicherman_A/0/1/0/all/0/1\">Amitay Sicherman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Russia-Ukraine war: Modeling and Clustering the Sentiments Trends of Various Countries. (arXiv:2301.00604v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00604","description":"<p>With Twitter's growth and popularity, a huge number of views are shared by\nusers on various topics, making this platform a valuable information source on\nvarious political, social, and economic issues. This paper investigates English\ntweets on the Russia-Ukraine war to analyze trends reflecting users' opinions\nand sentiments regarding the conflict. The tweets' positive and negative\nsentiments are analyzed using a BERT-based model, and the time series\nassociated with the frequency of positive and negative tweets for various\ncountries is calculated. Then, we propose a method based on the neighborhood\naverage for modeling and clustering the time series of countries. The\nclustering results provide valuable insight into public opinion regarding this\nconflict. Among other things, we can mention the similar thoughts of users from\nthe United States, Canada, the United Kingdom, and most Western European\ncountries versus the shared views of Eastern European, Scandinavian, Asian, and\nSouth American nations toward the conflict.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vahdat_Nejad_H/0/1/0/all/0/1\">Hamed Vahdat-Nejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbari_M/0/1/0/all/0/1\">Mohammad Ghasem Akbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salmani_F/0/1/0/all/0/1\">Fatemeh Salmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azizi_F/0/1/0/all/0/1\">Faezeh Azizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nili_Sani_H/0/1/0/all/0/1\">Hamid-Reza Nili-Sani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Design and analysis of tweet-based election models for the 2021 Mexican legislative election. (arXiv:2301.00626v1 [cs.SI])","link":"http://arxiv.org/abs/2301.00626","description":"<p>Modelling and forecasting real-life human behaviour using online social media\nis an active endeavour of interest in politics, government, academia, and\nindustry. Since its creation in 2006, Twitter has been proposed as a potential\nlaboratory that could be used to gauge and predict social behaviour. During the\nlast decade, the user base of Twitter has been growing and becoming more\nrepresentative of the general population. Here we analyse this user base in the\ncontext of the 2021 Mexican Legislative Election. To do so, we use a dataset of\n15 million election-related tweets in the six months preceding election day. We\nexplore different election models that assign political preference to either\nthe ruling parties or the opposition. We find that models using data with\ngeographical attributes determine the results of the election with better\nprecision and accuracy than conventional polling methods. These results\ndemonstrate that analysis of public online data can outperform conventional\npolling methods, and that political analysis and general forecasting would\nlikely benefit from incorporating such data in the immediate future. Moreover,\nthe same Twitter dataset with geographical attributes is positively correlated\nwith results from official census data on population and internet usage in\nMexico. These findings suggest that we have reached a period in time when\nonline activity, appropriately curated, can provide an accurate representation\nof offline behaviour.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vigna_Gomez_A/0/1/0/all/0/1\">Alejandro Vigna-G&#xf3;mez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murillo_J/0/1/0/all/0/1\">Javier Murillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_M/0/1/0/all/0/1\">Manelik Ramirez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borbolla_A/0/1/0/all/0/1\">Alberto Borbolla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marquez_I/0/1/0/all/0/1\">Ian M&#xe1;rquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_P/0/1/0/all/0/1\">Prasun K. Ray</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Active Learning Methods to Strategically Select Essays for Automated Scoring. (arXiv:2301.00628v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00628","description":"<p>Research on automated essay scoring has become increasing important because\nit serves as a method for evaluating students' written-responses at scale.\nScalable methods for scoring written responses are needed as students migrate\nto online learning environments resulting in the need to evaluate large numbers\nof written-response assessments. The purpose of this study is to describe and\nevaluate three active learning methods than can be used to minimize the number\nof essays that must be scored by human raters while still providing the data\nneeded to train a modern automated essay scoring system. The three active\nlearning methods are the uncertainty-based, the topological-based, and the\nhybrid method. These three methods were used to select essays included as part\nof the Automated Student Assessment Prize competition that were then classified\nusing a scoring model that was training with the bidirectional encoder\nrepresentations from transformer language model. All three active learning\nmethods produced strong results, with the topological-based method producing\nthe most efficient classification. Growth rate accuracy was also evaluated. The\nactive learning methods produced different levels of efficiency under different\nsample size allocations but, overall, all three methods were highly efficient\nand produced classifications that were similar to one another.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Firoozi_T/0/1/0/all/0/1\">Tahereh Firoozi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_H/0/1/0/all/0/1\">Hamid Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gierl_M/0/1/0/all/0/1\">Mark J. Gierl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Speech Representation Learning with Low-Bit Quantization. (arXiv:2301.00652v1 [eess.AS])","link":"http://arxiv.org/abs/2301.00652","description":"<p>With the development of hardware for machine learning, newer models often\ncome at the cost of both increased sizes and computational complexity. In\neffort to improve the efficiency for these models, we apply and investigate\nrecent quantization techniques on speech representation learning models. The\nquantization techniques were evaluated on the SUPERB benchmark. On the ASR\ntask, with aggressive quantization to 1 bit, we achieved 86.32% storage\nreduction (184.42 -&gt; 25.23), 88% estimated runtime reduction (1.00 -&gt; 0.12)\nwith increased word error rate (7.06 -&gt; 15.96). In comparison with\nDistillHuBERT which also aims for model compression, the 2-bit configuration\nyielded slightly smaller storage (35.84 vs. 46.98), better word error rate\n(12.68 vs. 13.37) and more efficient estimated runtime (0.15 vs. 0.73).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yeh_C/0/1/0/all/0/1\">Ching-Feng Yeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tomasello_P/0/1/0/all/0/1\">Paden Tomasello</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TriNet: stabilizing self-supervised learning from complete or slow collapse. (arXiv:2301.00656v1 [eess.AS])","link":"http://arxiv.org/abs/2301.00656","description":"<p>Self-supervised learning (SSL) models confront challenges of abrupt\ninformational collapse or slow dimensional collapse. We propose TriNet, which\nintroduces a novel triple-branch architecture for preventing collapse and\nstabilizing the pre-training. Our experimental results show that the proposed\nmethod notably stabilizes and accelerates pre-training and achieves a relative\nword error rate reduction (WERR) of 5.32% compared to the state-of-the-art\n(SOTA) Data2vec for a downstream benchmark ASR task. We will release our code\nat https://github.com/tencent-ailab/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cao_L/0/1/0/all/0/1\">Lixin Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_B/0/1/0/all/0/1\">Ben Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MnTTS2: An Open-Source Multi-Speaker Mongolian Text-to-Speech Synthesis Dataset. (arXiv:2301.00657v1 [eess.AS])","link":"http://arxiv.org/abs/2301.00657","description":"<p>Text-to-Speech (TTS) synthesis for low-resource languages is an attractive\nresearch issue in academia and industry nowadays. Mongolian is the official\nlanguage of the Inner Mongolia Autonomous Region and a representative\nlow-resource language spoken by over 10 million people worldwide. However,\nthere is a relative lack of open-source datasets for Mongolian TTS. Therefore,\nwe make public an open-source multi-speaker Mongolian TTS dataset, named\nMnTTS2, for the benefit of related researchers. In this work, we prepare the\ntranscription from various topics and invite three professional Mongolian\nannouncers to form a three-speaker TTS dataset, in which each announcer records\n10 hours of speeches in Mongolian, resulting 30 hours in total. Furthermore, we\nbuild the baseline system based on the state-of-the-art FastSpeech2 model and\nHiFi-GAN vocoder. The experimental results suggest that the constructed MnTTS2\ndataset is sufficient to build robust multi-speaker TTS models for real-world\napplications. The MnTTS2 dataset, training recipe, and pretrained models are\nreleased at: \\url{https://github.com/ssmlkl/MnTTS2}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liang_K/0/1/0/all/0/1\">Kailin Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yifan Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bao_F/0/1/0/all/0/1\">Feilong Bao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_G/0/1/0/all/0/1\">Guanglai Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Targeted Phishing Campaigns using Large Scale Language Models. (arXiv:2301.00665v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00665","description":"<p>In this research, we aim to explore the potential of natural language models\n(NLMs) such as GPT-3 and GPT-2 to generate effective phishing emails. Phishing\nemails are fraudulent messages that aim to trick individuals into revealing\nsensitive information or taking actions that benefit the attackers. We propose\na framework for evaluating the performance of NLMs in generating these types of\nemails based on various criteria, including the quality of the generated text,\nthe ability to bypass spam filters, and the success rate of tricking\nindividuals. Our evaluations show that NLMs are capable of generating phishing\nemails that are difficult to detect and that have a high success rate in\ntricking individuals, but their effectiveness varies based on the specific NLM\nand training data used. Our research indicates that NLMs could have a\nsignificant impact on the prevalence of phishing attacks and emphasizes the\nneed for further study on the ethical and security implications of using NLMs\nfor malicious purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karanjai_R/0/1/0/all/0/1\">Rabimba Karanjai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Political representation bias in DBpedia and Wikidata as a challenge for downstream processing. (arXiv:2301.00671v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00671","description":"<p>Diversity Searcher is a tool originally developed to help analyse diversity\nin news media texts. It relies on a form of automated content analysis and thus\nrests on prior assumptions and depends on certain design choices related to\ndiversity and fairness. One such design choice is the external knowledge\nsource(s) used. In this article, we discuss implications that these sources can\nhave on the results of content analysis. We compare two data sources that\nDiversity Searcher has worked with - DBpedia and Wikidata - with respect to\ntheir ontological coverage and diversity, and describe implications for the\nresulting analyses of text corpora. We describe a case study of the relative\nover- or under-representation of Belgian political parties between 1990 and\n2020 in the English-language DBpedia, the Dutch-language DBpedia, and Wikidata,\nand highlight the many decisions needed with regard to the design of this data\nanalysis and the assumptions behind it, as well as implications from the\nresults. In particular, we came across a staggering over-representation of the\npolitical right in the English-language DBpedia.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karadeniz_O/0/1/0/all/0/1\">Ozgur Karadeniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berendt_B/0/1/0/all/0/1\">Bettina Berendt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiyak_S/0/1/0/all/0/1\">Sercan Kiyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mertens_S/0/1/0/all/0/1\">Stefan Mertens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+dHaenens_L/0/1/0/all/0/1\">Leen d&#x27;Haenens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Sequential Generative Models for Semi-Supervised Language Instruction Following. (arXiv:2301.00676v1 [cs.LG])","link":"http://arxiv.org/abs/2301.00676","description":"<p>Agents that can follow language instructions are expected to be useful in a\nvariety of situations such as navigation. However, training neural\nnetwork-based agents requires numerous paired trajectories and languages. This\npaper proposes using multimodal generative models for semi-supervised learning\nin the instruction following tasks. The models learn a shared representation of\nthe paired data, and enable semi-supervised learning by reconstructing unpaired\ndata through the representation. Key challenges in applying the models to\nsequence-to-sequence tasks including instruction following are learning a\nshared representation of variable-length mulitimodal data and incorporating\nattention mechanisms. To address the problems, this paper proposes a novel\nnetwork architecture to absorb the difference in the sequence lengths of the\nmultimodal data. In addition, to further improve the performance, this paper\nshows how to incorporate the generative model-based approach with an existing\nsemi-supervised method called a speaker-follower model, and proposes a\nregularization term that improves inference using unpaired trajectories.\nExperiments on BabyAI and Room-to-Room (R2R) environments show that the\nproposed method improves the performance of instruction following by leveraging\nunpaired data, and improves the performance of the speaker-follower model by\n2\\% to 4\\% in R2R.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akuzawa_K/0/1/0/all/0/1\">Kei Akuzawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwasawa_Y/0/1/0/all/0/1\">Yusuke Iwasawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_Y/0/1/0/all/0/1\">Yutaka Matsuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning for Neural Machine Translation. (arXiv:2301.00688v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00688","description":"<p>The machine translation mechanism translates texts automatically between\ndifferent natural languages, and Neural Machine Translation (NMT) has gained\nattention for its rational context analysis and fluent translation accuracy.\nHowever, processing low-resource languages that lack relevant training\nattributes like supervised data is a current challenge for Natural Language\nProcessing (NLP). We incorporated a technique known Active Learning with the\nNMT toolkit Joey NMT to reach sufficient accuracy and robust predictions of\nlow-resource language translation. With active learning, a semi-supervised\nmachine learning strategy, the training algorithm determines which unlabeled\ndata would be the most beneficial for obtaining labels using selected query\ntechniques. We implemented two model-driven acquisition functions for selecting\nthe samples to be validated. This work uses transformer-based NMT systems;\nbaseline model (BM), fully trained model (FTM) , active learning least\nconfidence based model (ALLCM), and active learning margin sampling based model\n(ALMSM) when translating English to Hindi. The Bilingual Evaluation Understudy\n(BLEU) metric has been used to evaluate system results. The BLEU scores of BM,\nFTM, ALLCM and ALMSM systems are 16.26, 22.56 , 24.54, and 24.20, respectively.\nThe findings in this paper demonstrate that active learning techniques helps\nthe model to converge early and improve the overall quality of the translation\nsystem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vashistha_N/0/1/0/all/0/1\">Neeraj Vashistha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Kriti Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakya_R/0/1/0/all/0/1\">Ramakant Shakya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tsetlin Machine Embedding: Representing Words Using Logical Expressions. (arXiv:2301.00709v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00709","description":"<p>Embedding words in vector space is a fundamental first step in\nstate-of-the-art natural language processing (NLP). Typical NLP solutions\nemploy pre-defined vector representations to improve generalization by\nco-locating similar words in vector space. For instance, Word2Vec is a\nself-supervised predictive model that captures the context of words using a\nneural network. Similarly, GLoVe is a popular unsupervised model incorporating\ncorpus-wide word co-occurrence statistics. Such word embedding has\nsignificantly boosted important NLP tasks, including sentiment analysis,\ndocument classification, and machine translation. However, the embeddings are\ndense floating-point vectors, making them expensive to compute and difficult to\ninterpret. In this paper, we instead propose to represent the semantics of\nwords with a few defining words that are related using propositional logic. To\nproduce such logical embeddings, we introduce a Tsetlin Machine-based\nautoencoder that learns logical clauses self-supervised. The clauses consist of\ncontextual words like \"black,\" \"cup,\" and \"hot\" to define other words like\n\"coffee,\" thus being human-understandable. We evaluate our embedding approach\non several intrinsic and extrinsic benchmarks, outperforming GLoVe on six\nclassification tasks. Furthermore, we investigate the interpretability of our\nembedding using the logical representations acquired during training. We also\nvisualize word clusters in vector space, demonstrating how our logical\nembedding co-locate similar words.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattarai_B/0/1/0/all/0/1\">Bimal Bhattarai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granmo_O/0/1/0/all/0/1\">Ole-Christoffer Granmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1\">Lei Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_R/0/1/0/all/0/1\">Rohan Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_J/0/1/0/all/0/1\">Jivitesh Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IRT2: Inductive Linking and Ranking in Knowledge Graphs of Varying Scale. (arXiv:2301.00716v1 [cs.LG])","link":"http://arxiv.org/abs/2301.00716","description":"<p>We address the challenge of building domain-specific knowledge models for\nindustrial use cases, where labelled data and taxonomic information is\ninitially scarce. Our focus is on inductive link prediction models as a basis\nfor practical tools that support knowledge engineers with exploring text\ncollections and discovering and linking new (so-called open-world) entities to\nthe knowledge graph. We argue that - though neural approaches to text mining\nhave yielded impressive results in the past years - current benchmarks do not\nreflect the typical challenges encountered in the industrial wild properly.\nTherefore, our first contribution is an open benchmark coined IRT2 (inductive\nreasoning with text) that (1) covers knowledge graphs of varying sizes\n(including very small ones), (2) comes with incidental, low-quality text\nmentions, and (3) includes not only triple completion but also ranking, which\nis relevant for supporting experts with discovery tasks.\n</p>\n<p>We investigate two neural models for inductive link prediction, one based on\nend-to-end learning and one that learns from the knowledge graph and text data\nin separate steps. These models compete with a strong bag-of-words baseline.\nThe results show a significant advance in performance for the neural approaches\nas soon as the available graph data decreases for linking. For ranking, the\nresults are promising, and the neural approaches outperform the sparse\nretriever by a wide margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamann_F/0/1/0/all/0/1\">Felix Hamann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulges_A/0/1/0/all/0/1\">Adrian Ulges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falk_M/0/1/0/all/0/1\">Maurice Falk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Undesirable Dependence on Frequency of Gender Bias Metrics Based on Word Embeddings. (arXiv:2301.00792v1 [cs.CL])","link":"http://arxiv.org/abs/2301.00792","description":"<p>Numerous works use word embedding-based metrics to quantify societal biases\nand stereotypes in texts. Recent studies have found that word embeddings can\ncapture semantic similarity but may be affected by word frequency. In this work\nwe study the effect of frequency when measuring female vs. male gender bias\nwith word embedding-based bias quantification methods. We find that Skip-gram\nwith negative sampling and GloVe tend to detect male bias in high frequency\nwords, while GloVe tends to return female bias in low frequency words. We show\nthese behaviors still exist when words are randomly shuffled. This proves that\nthe frequency-based effect observed in unshuffled corpora stems from properties\nof the metric rather than from word associations. The effect is spurious and\nproblematic since bias metrics should depend exclusively on word co-occurrences\nand not individual word frequencies. Finally, we compare these results with the\nones obtained with an alternative metric based on Pointwise Mutual Information.\nWe find that this metric does not show a clear dependence on frequency, even\nthough it is slightly skewed towards male bias across all frequencies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valentini_F/0/1/0/all/0/1\">Francisco Valentini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosati_G/0/1/0/all/0/1\">Germ&#xe1;n Rosati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slezak_D/0/1/0/all/0/1\">Diego Fernandez Slezak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altszyler_E/0/1/0/all/0/1\">Edgar Altszyler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Style Transfer: A Review and Experimental Evaluation. (arXiv:2010.12742v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.12742","description":"<p>The stylistic properties of text have intrigued computational linguistics\nresearchers in recent years. Specifically, researchers have investigated the\nText Style Transfer (TST) task, which aims to change the stylistic properties\nof the text while retaining its style independent content. Over the last few\nyears, many novel TST algorithms have been developed, while the industry has\nleveraged these algorithms to enable exciting TST applications. The field of\nTST research has burgeoned because of this symbiosis. This article aims to\nprovide a comprehensive review of recent research efforts on text style\ntransfer. More concretely, we create a taxonomy to organize the TST models and\nprovide a comprehensive summary of the state of the art. We review the existing\nevaluation methodologies for TST tasks and conduct a large-scale\nreproducibility study where we experimentally benchmark 19 state-of-the-art TST\nalgorithms on two publicly available datasets. Finally, we expand on current\ntrends and provide new perspectives on the new and exciting developments in the\nTST field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiqiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1\">Charu C. Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning From Human Correction. (arXiv:2102.00225v11 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.00225","description":"<p>In industry NLP application, our manually labeled data has a certain number\nof noisy data. We present a simple method to find the noisy data and re-label\nthem manually, meanwhile we collect the correction information. Then we present\nnovel method to incorporate the human correction information into deep learning\nmodel. Human know how to correct noisy data. So the correction information can\nbe inject into deep learning model. We do the experiment on our own text\nclassification dataset, which is manually labeled, because we re-label the\nnoisy data in our dataset for our industry application. The experiment result\nshows that our method improve the classification accuracy from 91.7% to 92.5%.\nThe 91.7% accuracy is trained on the corrected dataset, which improve the\nbaseline from 83.3% to 91.7%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MyProfessors: Mining Turkish Student Reviews. (arXiv:2109.02325v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02325","description":"<p>We introduce Hocalarim (MyProfessors), the largest student review dataset\navailable for the Turkish language. It consists of over 5000 professor reviews\nleft online by students, with different aspects of education rated on a scale\nof 1 to 5 stars. We investigate the properties of the dataset and present its\nstatistics. We examine the impact of students' institution type on their\nratings and the correlation of students' bias to give positive or negative\nfeedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ceylan_I/0/1/0/all/0/1\">Ibrahim Faruk Ceylan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calik_N/0/1/0/all/0/1\">Necmettin Bera Calik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geographic Adaptation of Pretrained Language Models. (arXiv:2203.08565v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08565","description":"<p>Geographic features are commonly used to improve the performance of\npretrained language models (PLMs) on NLP tasks where they are intuitively\nbeneficial (e.g., geolocation prediction, dialect feature prediction). Existing\nmethods, however, leverage geographic information in task-specific fine-tuning\nand fail to integrate it into the geo-linguistic knowledge encoded by PLMs,\nwhich would make it transferable across different tasks. In this paper, we\nintroduce an approach to task-agnostic geoadaptation of PLMs that forces them\nto learn associations between linguistic phenomena and geographic locations.\nGeoadaptation is an intermediate training step that couples language modeling\nand geolocation prediction in a multi-task learning setup. In our main set of\nexperiments, we geoadapt BERTi\\'{c}, a PLM for\nBosnian-Croatian-Montenegrin-Serbian (BCMS), using a corpus of geotagged BCMS\ntweets. Evaluation on three tasks, namely fine-tuned as well as zero-shot\ngeolocation prediction and zero-shot prediction of dialect features, shows that\ngeoadaptation is very effective: e.g., we obtain state-of-the-art performance\nin supervised geolocation prediction and report massive gains over\ngeographically uninformed PLMs on zero-shot geolocation prediction. Moreover,\nin follow-up experiments we successfully geoadapt two other PLMs, specifically\nScandiBERT on Norwegian, Swedish, and Danish tweets and GermanBERT on Jodel\nposts in German from Austria, Germany, and Switzerland, proving that the\nbenefits of geoadaptation are not limited to a particular language area and\nPLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_V/0/1/0/all/0/1\">Valentin Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ljubesic_N/0/1/0/all/0/1\">Nikola Ljube&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierrehumbert_J/0/1/0/all/0/1\">Janet B. Pierrehumbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Questions Are All You Need to Train a Dense Passage Retriever. (arXiv:2206.10658v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.10658","description":"<p>We introduce ART, a new corpus-level autoencoding approach for training dense\nretrieval models that does not require any labeled training data. Dense\nretrieval is a central challenge for open-domain tasks, such as Open QA, where\nstate-of-the-art methods typically require large supervised datasets with\ncustom hard-negative mining and denoising of positive examples. ART, in\ncontrast, only requires access to unpaired inputs and outputs (e.g. questions\nand potential answer documents). It uses a new document-retrieval autoencoding\nscheme, where (1) an input question is used to retrieve a set of evidence\ndocuments, and (2) the documents are then used to compute the probability of\nreconstructing the original question. Training for retrieval based on question\nreconstruction enables effective unsupervised learning of both document and\nquestion encoders, which can be later incorporated into complete Open QA\nsystems without any further finetuning. Extensive experiments demonstrate that\nART obtains state-of-the-art results on multiple QA retrieval benchmarks with\nonly generic initialization from a pre-trained language model, removing the\nneed for labeled data and task-specific losses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sachan_D/0/1/0/all/0/1\">Devendra Singh Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1\">Joelle Pineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Manzil Zaheer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence to sequence pretraining for a less-resourced Slovenian language. (arXiv:2207.13988v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.13988","description":"<p>Large pretrained language models have recently conquered the area of natural\nlanguage processing. As an alternative to predominant masked language modelling\nintroduced in BERT, the T5 model has introduced a more general training\nobjective, namely sequence to sequence transformation, which includes masked\nlanguage model but more naturally fits text generation tasks such as machine\ntranslation, summarization, question answering, text simplification, dialogue\nsystems, etc. The monolingual variants of T5 models have been limited to\nwell-resourced languages, while the massively multilingual T5 model supports\n101 languages. In contrast, we trained two different sized T5-type sequence to\nsequence models for morphologically rich Slovene language with much less\nresources and analyzed their behavior on 11 tasks. Concerning classification\ntasks, the SloT5 models mostly lag behind the monolingual Slovene SloBERTa\nmodel but are useful for the generative tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ulcar_M/0/1/0/all/0/1\">Matej Ul&#x10d;ar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robnik_Sikonja_M/0/1/0/all/0/1\">Marko Robnik-&#x160;ikonja</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Language Models for Paragraph-Level Question Generation. (arXiv:2210.03992v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03992","description":"<p>Powerful generative models have led to recent progress in question generation\n(QG). However, it is difficult to measure advances in QG research since there\nare no standardized resources that allow a uniform comparison among approaches.\nIn this paper, we introduce QG-Bench, a multilingual and multidomain benchmark\nfor QG that unifies existing question answering datasets by converting them to\na standard QG setting. It includes general-purpose datasets such as SQuAD for\nEnglish, datasets from ten domains and two styles, as well as datasets in eight\ndifferent languages. Using QG-Bench as a reference, we perform an extensive\nanalysis of the capabilities of language models for the task. First, we propose\nrobust QG baselines based on fine-tuning generative language models. Then, we\ncomplement automatic evaluation based on standard metrics with an extensive\nmanual evaluation, which in turn sheds light on the difficulty of evaluating QG\nmodels. Finally, we analyse both the domain adaptability of these models as\nwell as the effectiveness of multilingual models in languages other than\nEnglish. QG-Bench is released along with the fine-tuned models presented in the\npaper https://github.com/asahi417/lm-question-generation, which are also\navailable as a demo https://autoqg.net/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ushio_A/0/1/0/all/0/1\">Asahi Ushio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alva_Manchego_F/0/1/0/all/0/1\">Fernando Alva-Manchego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Algebraic Framework for Stock & Flow Diagrams and Dynamical Systems Using Category Theory. (arXiv:2211.01290v2 [cs.LO] UPDATED)","link":"http://arxiv.org/abs/2211.01290","description":"<p>Stock and flow diagrams are already an important tool in epidemiology, but\ncategory theory lets us go further and treat these diagrams as mathematical\nentities in their own right. In this chapter we use communicable disease models\ncreated with our software, StockFlow.jl, to explain the benefits of the\ncategorical approach. We first explain the category of stock-flow diagrams, and\nnote the clear separation between the syntax of these diagrams and their\nsemantics, demonstrating three examples of semantics already implemented in the\nsoftware: ODEs, causal loop diagrams, and system structure diagrams. We then\nturn to two methods for building large stock-flow diagrams from smaller ones in\na modular fashion: composition and stratification. Finally, we introduce the\nopen-source ModelCollab software for diagram-based collaborative modeling. The\ngraphical user interface of this web-based software lets modelers take\nadvantage of the ideas discussed here without any knowledge of their\ncategorical foundations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baez_J/0/1/0/all/0/1\">John C. Baez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Libkind_S/0/1/0/all/0/1\">Sophie Libkind</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osgood_N/0/1/0/all/0/1\">Nathaniel D. Osgood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Redekopp_E/0/1/0/all/0/1\">Eric Redekopp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human-in-the-Loop Hate Speech Classification in a Multilingual Context. (arXiv:2212.02108v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.02108","description":"<p>The shift of public debate to the digital sphere has been accompanied by a\nrise in online hate speech. While many promising approaches for hate speech\nclassification have been proposed, studies often focus only on a single\nlanguage, usually English, and do not address three key concerns:\npost-deployment performance, classifier maintenance and infrastructural\nlimitations. In this paper, we introduce a new human-in-the-loop BERT-based\nhate speech classification pipeline and trace its development from initial data\ncollection and annotation all the way to post-deployment. Our classifier,\ntrained using data from our original corpus of over 422k examples, is\nspecifically developed for the inherently multilingual setting of Switzerland\nand outperforms with its F1 score of 80.5 the currently best-performing\nBERT-based multilingual classifier by 5.8 F1 points in German and 3.6 F1 points\nin French. Our systematic evaluations over a 12-month period further highlight\nthe vital importance of continuous, human-in-the-loop classifier maintenance to\nensure robust hate speech classification post-deployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kotarcic_A/0/1/0/all/0/1\">Ana Kotarcic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hangartner_D/0/1/0/all/0/1\">Dominik Hangartner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilardi_F/0/1/0/all/0/1\">Fabrizio Gilardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurer_S/0/1/0/all/0/1\">Selina Kurer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donnay_K/0/1/0/all/0/1\">Karsten Donnay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Switching to Discriminative Image Captioning by Relieving a Bottleneck of Reinforcement Learning. (arXiv:2212.03230v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2212.03230","description":"<p>Discriminativeness is a desirable feature of image captions: captions should\ndescribe the characteristic details of input images. However, recent\nhigh-performing captioning models, which are trained with reinforcement\nlearning (RL), tend to generate overly generic captions despite their high\nperformance in various other criteria. First, we investigate the cause of the\nunexpectedly low discriminativeness and show that RL has a deeply rooted side\neffect of limiting the output words to high-frequency words. The limited\nvocabulary is a severe bottleneck for discriminativeness as it is difficult for\na model to describe the details beyond its vocabulary. Then, based on this\nidentification of the bottleneck, we drastically recast discriminative image\ncaptioning as a much simpler task of encouraging low-frequency word generation.\nHinted by long-tail classification and debiasing methods, we propose methods\nthat easily switch off-the-shelf RL models to discriminativeness-aware models\nwith only a single-epoch fine-tuning on the part of the parameters. Extensive\nexperiments demonstrate that our methods significantly enhance the\ndiscriminativeness of off-the-shelf RL models and even outperform previous\ndiscriminativeness-aware methods with much smaller computational costs.\nDetailed analysis and human evaluation also verify that our methods boost the\ndiscriminativeness without sacrificing the overall quality of captions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Honda_U/0/1/0/all/0/1\">Ukyo Honda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1\">Taro Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsumoto_Y/0/1/0/all/0/1\">Yuji Matsumoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NEEDED: Introducing Hierarchical Transformer to Eye Diseases Diagnosis. (arXiv:2212.13408v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.13408","description":"<p>With the development of natural language processing techniques(NLP),\nautomatic diagnosis of eye diseases using ophthalmology electronic medical\nrecords (OEMR) has become possible. It aims to evaluate the condition of both\neyes of a patient respectively, and we formulate it as a particular multi-label\nclassification task in this paper. Although there are a few related studies in\nother diseases, automatic diagnosis of eye diseases exhibits unique\ncharacteristics. First, descriptions of both eyes are mixed up in OEMR\ndocuments, with both free text and templated asymptomatic descriptions,\nresulting in sparsity and clutter of information. Second, OEMR documents\ncontain multiple parts of descriptions and have long document lengths. Third,\nit is critical to provide explainability to the disease diagnosis model. To\novercome those challenges, we present an effective automatic eye disease\ndiagnosis framework, NEEDED. In this framework, a preprocessing module is\nintegrated to improve the density and quality of information. Then, we design a\nhierarchical transformer structure for learning the contextualized\nrepresentations of each sentence in the OEMR document. For the diagnosis part,\nwe propose an attention-based predictor that enables traceable diagnosis by\nobtaining disease-specific information. Experiments on the real dataset and\ncomparison with several baseline models show the advantage and explainability\nof our framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xu Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1\">Meng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_Z/0/1/0/all/0/1\">Zhiyuan Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Weiwei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1\">Wenjuan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuanchun Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification. (arXiv:2212.13939v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.13939","description":"<p>Learning models are highly dependent on data to work effectively, and they\ngive a better performance upon training on big datasets. Massive research\nexists in the literature to address the dataset adequacy issue. One promising\napproach for solving dataset adequacy issues is the data augmentation (DA)\napproach. In DA, the amount of training data instances is increased by making\ndifferent transformations on the available data instances to generate new\ncorrect and representative data instances. DA increases the dataset size and\nits variability, which enhances the model performance and its prediction\naccuracy. DA also solves the class imbalance problem in the classification\nlearning techniques. Few studies have recently considered DA in the Arabic\nlanguage. These studies rely on traditional augmentation approaches, such as\nparaphrasing by using rules or noising-based techniques. In this paper, we\npropose a new Arabic DA method that employs the recent powerful modeling\ntechnique, namely the AraGPT-2, for the augmentation process. The generated\nsentences are evaluated in terms of context, semantics, diversity, and novelty\nusing the Euclidean, cosine, Jaccard, and BLEU distances. Finally, the AraBERT\ntransformer is used on sentiment classification tasks to evaluate the\nclassification performance of the augmented Arabic dataset. The experiments\nwere conducted on four sentiment Arabic datasets, namely AraSarcasm, ASTD, ATT,\nand MOVIE. The selected datasets vary in size, label number, and unbalanced\nclasses. The results show that the proposed methodology enhanced the Arabic\nsentiment text classification on all datasets with an increase in F1 score by\n4% in AraSarcasm, 6% in ASTD, 9% in ATT, and 13% in MOVIE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Refai_D/0/1/0/all/0/1\">Dania Refai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abo_Soud_S/0/1/0/all/0/1\">Saleh Abo-Soud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdel_Rahman_M/0/1/0/all/0/1\">Mohammad Abdel-Rahman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-01-02T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}}]}]}