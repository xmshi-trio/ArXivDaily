{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-05-01T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Generative AI Perceptions: A Survey to Measure the Perceptions of Faculty, Staff, and Students on Generative AI Tools in Academia. (arXiv:2304.14415v1 [cs.HC])","link":"http://arxiv.org/abs/2304.14415","description":"<p>ChatGPT is a natural language processing tool that can engage in human-like\nconversations and generate coherent and contextually relevant responses to\nvarious prompts. ChatGPT is capable of understanding natural text that is input\nby a user and generating appropriate responses in various forms. This tool\nrepresents a major step in how humans are interacting with technology. This\npaper specifically focuses on how ChatGPT is revolutionizing the realm of\nengineering education and the relationship between technology, students, and\nfaculty and staff. Because this tool is quickly changing and improving with the\npotential for even greater future capability, it is a critical time to collect\npertinent data. A survey was created to measure the effects of ChatGPT on\nstudents, faculty, and staff. This survey is shared as a Texas A&amp;M University\ntechnical report to allow other universities and entities to use this survey\nand measure the effects elsewhere.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amani_S/0/1/0/all/0/1\">Sara Amani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_L/0/1/0/all/0/1\">Lance White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balart_T/0/1/0/all/0/1\">Trini Balart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_L/0/1/0/all/0/1\">Laksha Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shryock_D/0/1/0/all/0/1\">Dr. Kristi J. Shryock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brumbelow_D/0/1/0/all/0/1\">Dr. Kelly Brumbelow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watson_D/0/1/0/all/0/1\">Dr. Karan L. Watson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Vietnamese Legal Questions Using Deep Neural Networks with Biaffine Classifiers. (arXiv:2304.14447v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14447","description":"<p>In this paper, we propose using deep neural networks to extract important\ninformation from Vietnamese legal questions, a fundamental task towards\nbuilding a question answering system in the legal domain. Given a legal\nquestion in natural language, the goal is to extract all the segments that\ncontain the needed information to answer the question. We introduce a deep\nmodel that solves the task in three stages. First, our model leverages recent\nadvanced autoencoding language models to produce contextual word embeddings,\nwhich are then combined with character-level and POS-tag information to form\nword representations. Next, bidirectional long short-term memory networks are\nemployed to capture the relations among words and generate sentence-level\nrepresentations. At the third stage, borrowing ideas from graph-based\ndependency parsing methods which provide a global view on the input sentence,\nwe use biaffine classifiers to estimate the probability of each pair of\nstart-end words to be an important segment. Experimental results on a public\nVietnamese legal dataset show that our model outperforms the previous work by a\nlarge margin, achieving 94.79% in the F1 score. The results also prove the\neffectiveness of using contextual features extracted from pre-trained language\nmodels combined with other types of features such as character-level and\nPOS-tag features when training on a limited dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_N/0/1/0/all/0/1\">Nguyen Anh Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uyen_H/0/1/0/all/0/1\">Hoang Thi Thu Uyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phuong_T/0/1/0/all/0/1\">Tu Minh Phuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_N/0/1/0/all/0/1\">Ngo Xuan Bach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PMC-LLaMA: Further Finetuning LLaMA on Medical Papers. (arXiv:2304.14454v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14454","description":"<p>Large Language Models (LLMs) have showcased remarkable capabilities in\nnatural language understanding in various domains. These models can usually\nbehave well on daily dialog, or question answering scenarios, however, in areas\nthat value precision, for example, in medical applications, they often exhibit\nunsatisfactory performance due to a lack of domain-specific knowledge. In this\nreport, we introduce PMC-LLaMA, an open-source language model that is acquired\nby fine-tuning an open-source language model on a total of 4.8 million\nbiomedical academic papers for further injecting medical knowledge, enhancing\nits capability in medical domain. Our preliminary evaluations are conducted on\nthree biomedical QA datasets, including PubMedQA, MedMCQA, and USMLE, showing\nthat the our model after finetuning, i.e., PMC-LLaMA, demonstrates better\nunderstanding of biomedical domain-specific concepts, thus achieving high\nperformance on QA benchmarks. The model and codes, along with an online demo,\nare publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chaoyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoman Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Framing the News:From Human Perception to Large Language Model Inferences. (arXiv:2304.14456v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14456","description":"<p>Identifying the frames of news is important to understand the articles'\nvision, intention, message to be conveyed, and which aspects of the news are\nemphasized. Framing is a widely studied concept in journalism, and has emerged\nas a new topic in computing, with the potential to automate processes and\nfacilitate the work of journalism professionals. In this paper, we study this\nissue with articles related to the Covid-19 anti-vaccine movement. First, to\nunderstand the perspectives used to treat this theme, we developed a protocol\nfor human labeling of frames for 1786 headlines of No-Vax movement articles of\nEuropean newspapers from 5 countries. Headlines are key units in the written\npress, and worth of analysis as many people only read headlines (or use them to\nguide their decision for further reading.) Second, considering advances in\nNatural Language Processing (NLP) with large language models, we investigated\ntwo approaches for frame inference of news headlines: first with a GPT-3.5\nfine-tuning approach, and second with GPT-3.5 prompt-engineering. Our work\ncontributes to the study and analysis of the performance that these models have\nto facilitate journalistic tasks like classification of frames, while\nunderstanding whether the models are able to replicate human perception in the\nidentification of these frames.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barrio_D/0/1/0/all/0/1\">David Alonso del Barrio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatica_Perez_D/0/1/0/all/0/1\">Daniel Gatica-Perez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Referential Games Further the Emergence of Disentangled Representations. (arXiv:2304.14511v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14511","description":"<p>Natural languages are powerful tools wielded by human beings to communicate\ninformation. Among their desirable properties, compositionality has been the\nmain focus in the context of referential games and variants, as it promises to\nenable greater systematicity to the agents which would wield it. The concept of\ndisentanglement has been shown to be of paramount importance to learned\nrepresentations that generalise well in deep learning, and is thought to be a\nnecessary condition to enable systematicity. Thus, this paper investigates how\ndo compositionality at the level of the emerging languages, disentanglement at\nthe level of the learned representations, and systematicity relate to each\nother in the context of visual referential games. Firstly, we find that visual\nreferential games that are based on the Obverter architecture outperforms\nstate-of-the-art unsupervised learning approach in terms of many major\ndisentanglement metrics. Secondly, we expand the previously proposed Positional\nDisentanglement (PosDis) metric for compositionality to (re-)incorporate some\nconcerns pertaining to informativeness and completeness features found in the\nMutual Information Gap (MIG) disentanglement metric it stems from. This\nextension allows for further discrimination between the different kind of\ncompositional languages that emerge in the context of Obverter-based\nreferential games, in a way that neither the referential game accuracy nor\nprevious metrics were able to capture. Finally we investigate whether the\nresulting (emergent) systematicity, as measured by zero-shot compositional\nlearning tests, correlates with any of the disentanglement and compositionality\nmetrics proposed so far. Throughout the training process, statically\nsignificant correlation coefficients can be found both positive and negative\ndepending on the moment of the measure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Denamganai_K/0/1/0/all/0/1\">Kevin Denamgana&#xef;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Missaoui_S/0/1/0/all/0/1\">Sondess Missaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1\">James Alfred Walker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Shared Speech-Text Representations. (arXiv:2304.14514v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14514","description":"<p>Recently, a number of approaches to train speech models by incorpo-rating\ntext into end-to-end models have been developed, with Mae-stro advancing\nstate-of-the-art automatic speech recognition (ASR)and Speech Translation (ST)\nperformance. In this paper, we expandour understanding of the resulting shared\nspeech-text representationswith two types of analyses. First we examine the\nlimits of speech-free domain adaptation, finding that a corpus-specific\nduration modelfor speech-text alignment is the most important component for\nlearn-ing a shared speech-text representation. Second, we inspect the\nsim-ilarities between activations of unimodal (speech or text) encodersas\ncompared to the activations of a shared encoder. We find that theshared encoder\nlearns a more compact and overlapping speech-textrepresentation than the\nuni-modal encoders. We hypothesize that thispartially explains the\neffectiveness of the Maestro shared speech-textrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gary Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kastner_K/0/1/0/all/0/1\">Kyle Kastner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhehuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_A/0/1/0/all/0/1\">Andrew Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramabhadran_B/0/1/0/all/0/1\">Bhuvana Ramabhadran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multivariate Representation Learning for Information Retrieval. (arXiv:2304.14522v1 [cs.IR])","link":"http://arxiv.org/abs/2304.14522","description":"<p>Dense retrieval models use bi-encoder network architectures for learning\nquery and document representations. These representations are often in the form\nof a vector representation and their similarities are often computed using the\ndot product function. In this paper, we propose a new representation learning\nframework for dense retrieval. Instead of learning a vector for each query and\ndocument, our framework learns a multivariate distribution and uses negative\nmultivariate KL divergence to compute the similarity between distributions. For\nsimplicity and efficiency reasons, we assume that the distributions are\nmultivariate normals and then train large language models to produce mean and\nvariance vectors for these distributions. We provide a theoretical foundation\nfor the proposed framework and show that it can be seamlessly integrated into\nthe existing approximate nearest neighbor algorithms to perform retrieval\nefficiently. We conduct an extensive suite of experiments on a wide range of\ndatasets, and demonstrate significant improvements compared to competitive\ndense retrieval models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1\">Hamed Zamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bendersky_M/0/1/0/all/0/1\">Michael Bendersky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Transfer Learning for Automatic Speech Recognition: Towards Better Generalization. (arXiv:2304.14535v1 [cs.SD])","link":"http://arxiv.org/abs/2304.14535","description":"<p>Automatic speech recognition (ASR) has recently become an important challenge\nwhen using deep learning (DL). It requires large-scale training datasets and\nhigh computational and storage resources. Moreover, DL techniques and machine\nlearning (ML) approaches in general, hypothesize that training and testing data\ncome from the same domain, with the same input feature space and data\ndistribution characteristics. This assumption, however, is not applicable in\nsome real-world artificial intelligence (AI) applications. Moreover, there are\nsituations where gathering real data is challenging, expensive, or rarely\noccurring, which can not meet the data requirements of DL models. deep transfer\nlearning (DTL) has been introduced to overcome these issues, which helps\ndevelop high-performing models using real datasets that are small or slightly\ndifferent but related to the training data. This paper presents a comprehensive\nsurvey of DTL-based ASR frameworks to shed light on the latest developments and\nhelps academics and professionals understand current challenges. Specifically,\nafter presenting the DTL background, a well-designed taxonomy is adopted to\ninform the state-of-the-art. A critical analysis is then conducted to identify\nthe limitations and advantages of each framework. Moving on, a comparative\nstudy is introduced to highlight the current challenges before deriving\nopportunities for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kheddar_H/0/1/0/all/0/1\">Hamza Kheddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Himeur_Y/0/1/0/all/0/1\">Yassine Himeur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Maadeed_S/0/1/0/all/0/1\">Somaya Al-Maadeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amira_A/0/1/0/all/0/1\">Abbes Amira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bensaali_F/0/1/0/all/0/1\">Faycal Bensaali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discourse over Discourse: The Need for an Expanded Pragmatic Focus in Conversational AI. (arXiv:2304.14543v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14543","description":"<p>The summarization of conversation, that is, discourse over discourse,\nelevates pragmatic considerations as a pervasive limitation of both\nsummarization and other applications of contemporary conversational AI.\nBuilding on impressive progress in both semantics and syntax, pragmatics\nconcerns meaning in the practical sense. In this paper, we discuss several\nchallenges in both summarization of conversations and other conversational AI\napplications, drawing on relevant theoretical work. We illustrate the\nimportance of pragmatics with so-called star sentences, syntactically\nacceptable propositions that are pragmatically inappropriate in conversation or\nits summary. Because the baseline for quality of AI is indistinguishability\nfrom human behavior, we draw heavily on the psycho-linguistics literature, and\nlabel our complaints as \"Turing Test Triggers\" (TTTs). We discuss implications\nfor the design and evaluation of conversation summarization methods and\nconversational AI applications like voice assistants and chatbots\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seals_S/0/1/0/all/0/1\">S.M. Seals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalin_V/0/1/0/all/0/1\">Valerie L. Shalin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Appropriateness is all you need!. (arXiv:2304.14553v1 [cs.AI])","link":"http://arxiv.org/abs/2304.14553","description":"<p>The strive to make AI applications \"safe\" has led to the development of\nsafety-measures as the main or even sole normative requirement of their\npermissible use. Similar can be attested to the latest version of chatbots,\nsuch as chatGPT. In this view, if they are \"safe\", they are supposed to be\npermissible to deploy. This approach, which we call \"safety-normativity\", is\nrather limited in solving the emerging issues that chatGPT and other chatbots\nhave caused thus far. In answering this limitation, in this paper we argue for\nlimiting chatbots in the range of topics they can chat about according to the\nnormative concept of appropriateness. We argue that rather than looking for\n\"safety\" in a chatbot's utterances to determine what they may and may not say,\nwe ought to assess those utterances according to three forms of\nappropriateness: technical-discursive, social, and moral. We then spell out\nwhat requirements for chatbots follow from these forms of appropriateness to\navoid the limits of previous accounts: positionality, acceptability, and value\nalignment (PAVA). With these in mind, we may be able to determine what a\nchatbot may and may not say. Lastly, one initial suggestion is to use challenge\nsets, specifically designed for appropriateness, as a validation method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kempt_H/0/1/0/all/0/1\">Hendrik Kempt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavie_A/0/1/0/all/0/1\">Alon Lavie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagel_S/0/1/0/all/0/1\">Saskia K. Nagel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Knowledge Graph Entity Alignment with Graph Augmentation. (arXiv:2304.14585v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14585","description":"<p>Entity alignment (EA) which links equivalent entities across different\nknowledge graphs (KGs) plays a crucial role in knowledge fusion. In recent\nyears, graph neural networks (GNNs) have been successfully applied in many\nembedding-based EA methods. However, existing GNN-based methods either suffer\nfrom the structural heterogeneity issue that especially appears in the real KG\ndistributions or ignore the heterogeneous representation learning for unseen\n(unlabeled) entities, which would lead the model to overfit on few alignment\nseeds (i.e., training data) and thus cause unsatisfactory alignment\nperformance. To enhance the EA ability, we propose GAEA, a novel EA approach\nbased on graph augmentation. In this model, we design a simple Entity-Relation\n(ER) Encoder to generate latent representations for entities via jointly\nmodeling comprehensive structural information and rich relation semantics.\nMoreover, we use graph augmentation to create two graph views for margin-based\nalignment learning and contrastive entity representation learning, thus\nmitigating structural heterogeneity and further improving the model's alignment\nperformance. Extensive experiments conducted on benchmark datasets demonstrate\nthe effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1\">Feng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yusong Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A logical word embedding for learning grammar. (arXiv:2304.14590v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14590","description":"<p>We introduce the logical grammar emdebbing (LGE), a model inspired by\npregroup grammars and categorial grammars to enable unsupervised inference of\nlexical categories and syntactic rules from a corpus of text. LGE produces\ncomprehensible output summarizing its inferences, has a completely transparent\nprocess for producing novel sentences, and can learn from as few as a hundred\nsentences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deyo_S/0/1/0/all/0/1\">Sean Deyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elser_V/0/1/0/all/0/1\">Veit Elser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Antisemitic Messages? A Guide to High-Quality Annotation and a Labeled Dataset of Tweets. (arXiv:2304.14599v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14599","description":"<p>One of the major challenges in automatic hate speech detection is the lack of\ndatasets that cover a wide range of biased and unbiased messages and that are\nconsistently labeled. We propose a labeling procedure that addresses some of\nthe common weaknesses of labeled datasets. We focus on antisemitic speech on\nTwitter and create a labeled dataset of 6,941 tweets that cover a wide range of\ntopics common in conversations about Jews, Israel, and antisemitism between\nJanuary 2019 and December 2021 by drawing from representative samples with\nrelevant keywords. Our annotation process aims to strictly apply a commonly\nused definition of antisemitism by forcing annotators to specify which part of\nthe definition applies, and by giving them the option to personally disagree\nwith the definition on a case-by-case basis. Labeling tweets that call out\nantisemitism, report antisemitism, or are otherwise related to antisemitism\n(such as the Holocaust) but are not actually antisemitic can help reduce false\npositives in automated detection. The dataset includes 1,250 tweets (18%) that\nare antisemitic according to the International Holocaust Remembrance Alliance\n(IHRA) definition of antisemitism. It is important to note, however, that the\ndataset is not comprehensive. Many topics are still not covered, and it only\nincludes tweets collected from Twitter between January 2019 and December 2021.\nAdditionally, the dataset only includes tweets that were written in English.\nDespite these limitations, we hope that this is a meaningful contribution to\nimproving the automated detection of antisemitic speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jikeli_G/0/1/0/all/0/1\">Gunther Jikeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karali_S/0/1/0/all/0/1\">Sameer Karali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miehling_D/0/1/0/all/0/1\">Daniel Miehling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soemer_K/0/1/0/all/0/1\">Katharina Soemer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CED: Catalog Extraction from Documents. (arXiv:2304.14662v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14662","description":"<p>Sentence-by-sentence information extraction from long documents is an\nexhausting and error-prone task. As the indicator of document skeleton,\ncatalogs naturally chunk documents into segments and provide informative\ncascade semantics, which can help to reduce the search space. Despite their\nusefulness, catalogs are hard to be extracted without the assist from external\nknowledge. For documents that adhere to a specific template, regular\nexpressions are practical to extract catalogs. However, handcrafted heuristics\nare not applicable when processing documents from different sources with\ndiverse formats. To address this problem, we build a large manually annotated\ncorpus, which is the first dataset for the Catalog Extraction from Documents\n(CED) task. Based on this corpus, we propose a transition-based framework for\nparsing documents into catalog trees. The experimental results demonstrate that\nour proposed method outperforms baseline systems and shows a good ability to\ntransfer. We believe the CED task could fill the gap between raw text segments\nand information extraction tasks on extremely long documents. Data and code are\navailable at \\url{https://github.com/Spico197/CatalogExtraction}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guoliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zechang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zijian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Junfei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mengsong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhefeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huai_B/0/1/0/all/0/1\">Baoxing Huai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_P/0/1/0/all/0/1\">Pingfu Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenliang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards autonomous system: flexible modular production system enhanced with large language model agents. (arXiv:2304.14721v1 [cs.RO])","link":"http://arxiv.org/abs/2304.14721","description":"<p>In this paper, we present a novel framework that combines large language\nmodels (LLMs), digital twins and industrial automation system to enable\nintelligent planning and control of production processes. Our approach involves\ndeveloping a digital twin system that contains descriptive information about\nthe production and retrofitting the automation system to offer unified\ninterfaces of fine-granular functionalities or skills executable by automation\ncomponents or modules. Subsequently, LLM-Agents are designed to interpret\ndescriptive information in the digital twins and control the physical system\nthrough RESTful interfaces. These LLM-Agents serve as intelligent agents within\nan automation system, enabling autonomous planning and control of flexible\nproduction. Given a task instruction as input, the LLM-agents orchestrate a\nsequence of atomic functionalities and skills to accomplish the task. We\ndemonstrate how our implemented prototype can handle un-predefined tasks, plan\na production process, and execute the operations. This research highlights the\npotential of integrating LLMs into industrial automation systems for more\nagile, flexible, and adaptive production processes, while also underscoring the\ncritical insights and limitations for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yuchen Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_M/0/1/0/all/0/1\">Manthan Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jazdi_N/0/1/0/all/0/1\">Nasser Jazdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weyrich_M/0/1/0/all/0/1\">Michael Weyrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Search-in-the-Chain: Towards the Accurate, Credible and Traceable Content Generation for Complex Knowledge-intensive Tasks. (arXiv:2304.14732v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14732","description":"<p>With the wide application of Large Language Models (LLMs) such as ChatGPT,\nhow to make the contents generated by LLM accurate and credible becomes very\nimportant, especially in complex knowledge-intensive tasks. In this paper, we\npropose a novel framework called Search-in-the-Chain (SearChain) to improve the\naccuracy, credibility and traceability of LLM-generated content for multi-hop\nquestion answering, which is a typical complex knowledge-intensive task.\nSearChain is a framework that deeply integrates LLM and information retrieval\n(IR). In SearChain, LLM constructs a chain-of-query, which is the decomposition\nof the multi-hop question. Each node of the chain is a query-answer pair\nconsisting of an IR-oriented query and the answer generated by LLM for this\nquery. IR verifies, completes, and traces the information of each node of the\nchain, so as to guide LLM to construct the correct chain-of-query, and finally\nanswer the multi-hop question. SearChain makes LLM change from trying to give a\nanswer to trying to construct the chain-of-query when faced with the multi-hop\nquestion, which can stimulate the knowledge-reasoning ability and provides the\ninterface for IR to be deeply involved in reasoning process of LLM. IR\ninteracts with each node of chain-of-query of LLM. It verifies the information\nof the node and provides the unknown knowledge to LLM, which ensures the\naccuracy of the whole chain in the process of LLM generating the answer.\nBesides, the contents returned by LLM to the user include not only the final\nanswer but also the reasoning process for the question, that is, the\nchain-of-query and the supporting documents retrieved by IR for each node of\nthe chain, which improves the credibility and traceability of the contents\ngenerated by LLM. Experimental results show SearChain outperforms related\nbaselines on four multi-hop question-answering datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shicheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cost-Sensitive Self-Training for Optimizing Non-Decomposable Metrics. (arXiv:2304.14738v1 [cs.LG])","link":"http://arxiv.org/abs/2304.14738","description":"<p>Self-training based semi-supervised learning algorithms have enabled the\nlearning of highly accurate deep neural networks, using only a fraction of\nlabeled data. However, the majority of work on self-training has focused on the\nobjective of improving accuracy, whereas practical machine learning systems can\nhave complex goals (e.g. maximizing the minimum of recall across classes, etc.)\nthat are non-decomposable in nature. In this work, we introduce the\nCost-Sensitive Self-Training (CSST) framework which generalizes the\nself-training-based methods for optimizing non-decomposable metrics. We prove\nthat our framework can better optimize the desired non-decomposable metric\nutilizing unlabeled data, under similar data distribution assumptions made for\nthe analysis of self-training. Using the proposed CSST framework, we obtain\npractical self-training methods (for both vision and NLP tasks) for optimizing\ndifferent non-decomposable metrics using deep neural networks. Our results\ndemonstrate that CSST achieves an improvement over the state-of-the-art in\nmajority of the cases across datasets and objectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rangwani_H/0/1/0/all/0/1\">Harsh Rangwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramasubramanian_S/0/1/0/all/0/1\">Shrinivas Ramasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takemori_S/0/1/0/all/0/1\">Sho Takemori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takashi_K/0/1/0/all/0/1\">Kato Takashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umeda_Y/0/1/0/all/0/1\">Yuhei Umeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radhakrishnan_V/0/1/0/all/0/1\">Venkatesh Babu Radhakrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Made of Steel? Learning Plausible Materials for Components in the Vehicle Repair Domain. (arXiv:2304.14745v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14745","description":"<p>We propose a novel approach to learn domain-specific plausible materials for\ncomponents in the vehicle repair domain by probing Pretrained Language Models\n(PLMs) in a cloze task style setting to overcome the lack of annotated\ndatasets. We devise a new method to aggregate salient predictions from a set of\ncloze query templates and show that domain-adaptation using either a small,\nhigh-quality or a customized Wikipedia corpus boosts performance. When\nexploring resource-lean alternatives, we find a distilled PLM clearly\noutperforming a classic pattern-based algorithm. Further, given that 98% of our\ndomain-specific components are multiword expressions, we successfully exploit\nthe compositionality assumption as a way to address data sparsity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eichel_A/0/1/0/all/0/1\">Annerose Eichel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlipf_H/0/1/0/all/0/1\">Helena Schlipf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walde_S/0/1/0/all/0/1\">Sabine Schulte im Walde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FlowTransformer: A Transformer Framework for Flow-based Network Intrusion Detection Systems. (arXiv:2304.14746v1 [cs.CR])","link":"http://arxiv.org/abs/2304.14746","description":"<p>This paper presents the FlowTransformer framework, a novel approach for\nimplementing transformer-based Network Intrusion Detection Systems (NIDSs).\nFlowTransformer leverages the strengths of transformer models in identifying\nthe long-term behaviour and characteristics of networks, which are often\noverlooked by most existing NIDSs. By capturing these complex patterns in\nnetwork traffic, FlowTransformer offers a flexible and efficient tool for\nresearchers and practitioners in the cybersecurity community who are seeking to\nimplement NIDSs using transformer-based models. FlowTransformer allows the\ndirect substitution of various transformer components, including the input\nencoding, transformer, classification head, and the evaluation of these across\nany flow-based network dataset. To demonstrate the effectiveness and efficiency\nof the FlowTransformer framework, we utilise it to provide an extensive\nevaluation of various common transformer architectures, such as GPT 2.0 and\nBERT, on three commonly used public NIDS benchmark datasets. We provide results\nfor accuracy, model size and speed. A key finding of our evaluation is that the\nchoice of classification head has the most significant impact on the model\nperformance. Surprisingly, Global Average Pooling, which is commonly used in\ntext classification, performs very poorly in the context of NIDS. In addition,\nwe show that model size can be reduced by over 50\\%, and inference and training\ntimes improved, with no loss of accuracy, by making specific choices of input\nencoding and classification head instead of other commonly used alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manocchio_L/0/1/0/all/0/1\">Liam Daly Manocchio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Layeghy_S/0/1/0/all/0/1\">Siamak Layeghy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_W/0/1/0/all/0/1\">Wai Weng Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulatilleke_G/0/1/0/all/0/1\">Gayan K. Kulatilleke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarhan_M/0/1/0/all/0/1\">Mohanad Sarhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portmann_M/0/1/0/all/0/1\">Marius Portmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dissecting Recall of Factual Associations in Auto-Regressive Language Models. (arXiv:2304.14767v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14767","description":"<p>Transformer-based language models (LMs) are known to capture factual\nknowledge in their parameters. While previous work looked into where factual\nassociations are stored, only little is known about how they are retrieved\ninternally during inference. We investigate this question through the lens of\ninformation flow. Given a subject-relation query, we study how the model\naggregates information about the subject and relation to predict the correct\nattribute. With interventions on attention edges, we first identify two\ncritical points where information propagates to the prediction: one from the\nrelation positions followed by another from the subject positions. Next, by\nanalyzing the information at these points, we unveil a three-step internal\nmechanism for attribute extraction. First, the representation at the\nlast-subject position goes through an enrichment process, driven by the early\nMLP sublayers, to encode many subject-related attributes. Second, information\nfrom the relation propagates to the prediction. Third, the prediction\nrepresentation \"queries\" the enriched subject to extract the attribute. Perhaps\nsurprisingly, this extraction is typically done via attention heads, which\noften encode subject-attribute mappings in their parameters. Overall, our\nfindings introduce a comprehensive view of how factual associations are stored\nand extracted internally in LMs, facilitating future research on knowledge\nlocalization and editing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastings_J/0/1/0/all/0/1\">Jasmijn Bastings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filippova_K/0/1/0/all/0/1\">Katja Filippova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1\">Amir Globerson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RexUIE: A Recursive Method with Explicit Schema Instructor for Universal Information Extraction. (arXiv:2304.14770v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14770","description":"<p>Universal Information Extraction (UIE) is an area of interest due to the\nchallenges posed by varying targets, heterogeneous structures, and\ndemand-specific schemas. However, previous works have only achieved limited\nsuccess by unifying a few tasks, such as Named Entity Recognition (NER) and\nRelation Extraction (RE), which fall short of being authentic UIE models\nparticularly when extracting other general schemas such as quadruples and\nquintuples. Additionally, these models used an implicit structural schema\ninstructor, which could lead to incorrect links between types, hindering the\nmodel's generalization and performance in low-resource scenarios. In this\npaper, we redefine the authentic UIE with a formal formulation that encompasses\nalmost all extraction schemas. To the best of our knowledge, we are the first\nto introduce UIE for any kind of schemas. In addition, we propose RexUIE, which\nis a Recursive Method with Explicit Schema Instructor for UIE. To avoid\ninterference between different types, we reset the position ids and attention\nmask matrices. RexUIE shows strong performance under both full-shot and\nfew-shot settings and achieves State-of-the-Art results on the tasks of\nextracting complex schemas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chengyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fubang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yangyang Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changlong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training and Evaluation of a Multilingual Tokenizer for GPT-SW3. (arXiv:2304.14780v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14780","description":"<p>This paper provides a detailed discussion of the multilingual tokenizer used\nfor GPT-SW3. It was trained on the Nordic Pile using the SentencePiece library\nand the BPE algorithm. We outline the tokenizer's most important features and\nshare details on its learned vocabulary. In addition, we systematically analyze\nthe properties and evaluate the performance of the tokenizer with regard to the\ndifferent languages present in the data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stollenwerk_F/0/1/0/all/0/1\">Felix Stollenwerk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are the Best Multilingual Document Embeddings simply Based on Sentence Embeddings?. (arXiv:2304.14796v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14796","description":"<p>Dense vector representations for textual data are crucial in modern NLP. Word\nembeddings and sentence embeddings estimated from raw texts are key in\nachieving state-of-the-art results in various tasks requiring semantic\nunderstanding. However, obtaining embeddings at the document level is\nchallenging due to computational requirements and lack of appropriate data.\nInstead, most approaches fall back on computing document embeddings based on\nsentence representations. Although there exist architectures and models to\nencode documents fully, they are in general limited to English and few other\nhigh-resourced languages. In this work, we provide a systematic comparison of\nmethods to produce document-level representations from sentences based on\nLASER, LaBSE, and Sentence BERT pre-trained multilingual models. We compare\ninput token number truncation, sentence averaging as well as some simple\nwindowing and in some cases new augmented and learnable approaches, on 3 multi-\nand cross-lingual tasks in 8 languages belonging to 3 different language\nfamilies. Our task-based extrinsic evaluations show that, independently of the\nlanguage, a clever combination of sentence embeddings is usually better than\nencoding the full document as a single unit, even when this is possible. We\ndemonstrate that while a simple sentence average results in a strong baseline\nfor classification tasks, more complex combinations are necessary for semantic\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sannigrahi_S/0/1/0/all/0/1\">Sonal Sannigrahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genabith_J/0/1/0/all/0/1\">Josef van Genabith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espana_Bonet_C/0/1/0/all/0/1\">Cristina Espana-Bonet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ResiDual: Transformer with Dual Residual Connections. (arXiv:2304.14802v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14802","description":"<p>Transformer networks have become the preferred architecture for many tasks\ndue to their state-of-the-art performance. However, the optimal way to\nimplement residual connections in Transformer, which are essential for\neffective training, is still debated. Two widely used variants are the\nPost-Layer-Normalization (Post-LN) and Pre-Layer-Normalization (Pre-LN)\nTransformers, which apply layer normalization after each residual block's\noutput or before each residual block's input, respectively. While both variants\nenjoy their advantages, they also suffer from severe limitations: Post-LN\ncauses gradient vanishing issue that hinders training deep Transformers, and\nPre-LN causes representation collapse issue that limits model capacity. In this\npaper, we propose ResiDual, a novel Transformer architecture with Pre-Post-LN\n(PPLN), which fuses the connections in Post-LN and Pre-LN together and inherits\ntheir advantages while avoids their limitations. We conduct both theoretical\nanalyses and empirical experiments to verify the effectiveness of ResiDual.\nTheoretically, we prove that ResiDual has a lower bound on the gradient to\navoid the vanishing issue due to the residual connection from Pre-LN. Moreover,\nResiDual also has diverse model representations to avoid the collapse issue due\nto the residual connection from Post-LN. Empirically, ResiDual outperforms both\nPost-LN and Pre-LN on several machine translation benchmarks across different\nnetwork depths and data sizes. Thanks to the good theoretical and empirical\nperformance, ResiDual Transformer can serve as a foundation architecture for\ndifferent AI models (e.g., large language models). Our code is available at\nhttps://github.com/microsoft/ResiDual.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Shufang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huishuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Junliang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadalla_H/0/1/0/all/0/1\">Hany Hassan Awadalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menezes_A/0/1/0/all/0/1\">Arul Menezes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemEval-2023 Task 11: Learning With Disagreements (LeWiDi). (arXiv:2304.14803v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14803","description":"<p>NLP datasets annotated with human judgments are rife with disagreements\nbetween the judges. This is especially true for tasks depending on subjective\njudgments such as sentiment analysis or offensive language detection.\nParticularly in these latter cases, the NLP community has come to realize that\nthe approach of 'reconciling' these different subjective interpretations is\ninappropriate. Many NLP researchers have therefore concluded that rather than\neliminating disagreements from annotated corpora, we should preserve\nthem-indeed, some argue that corpora should aim to preserve all annotator\njudgments. But this approach to corpus creation for NLP has not yet been widely\naccepted. The objective of the LeWiDi series of shared tasks is to promote this\napproach to developing NLP models by providing a unified framework for training\nand evaluating with such datasets. We report on the second LeWiDi shared task,\nwhich differs from the first edition in three crucial respects: (i) it focuses\nentirely on NLP, instead of both NLP and computer vision tasks in its first\nedition; (ii) it focuses on subjective tasks, instead of covering different\ntypes of disagreements-as training with aggregated labels for subjective NLP\ntasks is a particularly obvious misrepresentation of the data; and (iii) for\nthe evaluation, we concentrate on soft approaches to evaluation. This second\nedition of LeWiDi attracted a wide array of participants resulting in 13 shared\ntask submission papers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leonardelli_E/0/1/0/all/0/1\">Elisa Leonardelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uma_A/0/1/0/all/0/1\">Alexandra Uma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abercrombie_G/0/1/0/all/0/1\">Gavin Abercrombie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almanea_D/0/1/0/all/0/1\">Dina Almanea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basile_V/0/1/0/all/0/1\">Valerio Basile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fornaciari_T/0/1/0/all/0/1\">Tommaso Fornaciari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1\">Verena Rieser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poesio_M/0/1/0/all/0/1\">Massimo Poesio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatGPT Evaluation on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse Relations. (arXiv:2304.14827v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14827","description":"<p>This paper aims to quantitatively evaluate the performance of ChatGPT, an\ninteractive large language model, on inter-sentential relations such as\ntemporal relations, causal relations, and discourse relations. Given ChatGPT's\npromising performance across various tasks, we conduct extensive evaluations on\nthe whole test sets of 13 datasets, including temporal and causal relations,\nPDTB2.0-based and dialogue-based discourse relations, and downstream\napplications on discourse understanding. To achieve reliable results, we adopt\nthree tailored prompt templates for each task, including the zero-shot prompt\ntemplate, zero-shot prompt engineering (PE) template, and in-context learning\n(ICL) prompt template, to establish the initial baseline scores for all popular\nsentence-pair relation classification tasks for the first time. We find that\nChatGPT exhibits strong performance in detecting and reasoning about causal\nrelations, while it may not be proficient in identifying the temporal order\nbetween two events. It can recognize most discourse relations with existing\nexplicit discourse connectives, but the implicit discourse relation still\nremains a challenging task. Meanwhile, ChatGPT performs poorly in the dialogue\ndiscourse parsing task that requires structural understanding in a dialogue\nbefore being aware of the discourse relation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1\">Chunkit Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jiayang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1\">Tianqing Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HQP: A Human-Annotated Dataset for Detecting Online Propaganda. (arXiv:2304.14931v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14931","description":"<p>Online propaganda poses a severe threat to the integrity of societies.\nHowever, existing datasets for detecting online propaganda have a key\nlimitation: they were annotated using weak labels that can be noisy and even\nincorrect. To address this limitation, our work makes the following\ncontributions: (1) We present \\dataset: a novel dataset (N=30,000) for\ndetecting online propaganda with high-quality labels. To the best of our\nknowledge, \\dataset is the first dataset for detecting online propaganda that\nwas created through human annotation. (2) We show empirically that\nstate-of-the-art language models fail in detecting online propaganda when\ntrained with weak labels (AUC: 64.03). In contrast, state-of-the-art language\nmodels can accurately detect online propaganda when trained with our\nhigh-quality labels (AUC: 92.25), which is an improvement of ~44%. (3) To\naddress the cost of labeling, we extend our work to few-shot learning.\nSpecifically, we show that prompt-based learning using a small sample of\nhigh-quality labels can still achieve a reasonable performance (AUC: 80.27).\nFinally, we discuss implications for the NLP community to balance the cost and\nquality of labeling. Crucially, our work highlights the importance of\nhigh-quality labels for sensitive NLP tasks such as propaganda detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maarouf_A/0/1/0/all/0/1\">Abdurahman Maarouf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bar_D/0/1/0/all/0/1\">Dominik B&#xe4;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geissler_D/0/1/0/all/0/1\">Dominique Geissler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feuerriegel_S/0/1/0/all/0/1\">Stefan Feuerriegel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of Multimodal Model Merging. (arXiv:2304.14933v1 [cs.CV])","link":"http://arxiv.org/abs/2304.14933","description":"<p>Model merging (e.g., via interpolation or task arithmetic) fuses multiple\nmodels trained on different tasks to generate a multi-task solution. The\ntechnique has been proven successful in previous studies, where the models are\ntrained on similar tasks and with the same initialization. In this paper, we\nexpand on this concept to a multimodal setup by merging transformers trained on\ndifferent modalities. Furthermore, we conduct our study for a novel goal where\nwe can merge vision, language, and cross-modal transformers of a\nmodality-specific architecture to create a parameter-efficient\nmodality-agnostic architecture. Through comprehensive experiments, we\nsystematically investigate the key factors impacting model performance after\nmerging, including initialization, merging mechanisms, and model architectures.\nOur analysis leads to an effective training recipe for matching the performance\nof the modality-agnostic baseline (i.e. pre-trained from scratch) via model\nmerging. Our code is available at: https://github.com/ylsung/vl-merging\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yi-Lin Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kevin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information Redundancy and Biases in Public Document Information Extraction Benchmarks. (arXiv:2304.14936v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14936","description":"<p>Advances in the Visually-rich Document Understanding (VrDU) field and\nparticularly the Key-Information Extraction (KIE) task are marked with the\nemergence of efficient Transformer-based approaches such as the LayoutLM\nmodels. Despite the good performance of KIE models when fine-tuned on public\nbenchmarks, they still struggle to generalize on complex real-life use-cases\nlacking sufficient document annotations. Our research highlighted that KIE\nstandard benchmarks such as SROIE and FUNSD contain significant similarity\nbetween training and testing documents and can be adjusted to better evaluate\nthe generalization of models. In this work, we designed experiments to quantify\nthe information redundancy in public benchmarks, revealing a 75% template\nreplication in SROIE official test set and 16% in FUNSD. We also proposed\nresampling strategies to provide benchmarks more representative of the\ngeneralization ability of models. We showed that models not suited for document\nanalysis struggle on the adjusted splits dropping on average 10,5% F1 score on\nSROIE and 3.5% on FUNSD compared to multi-modal models dropping only 7,5% F1 on\nSROIE and 0.5% F1 on FUNSD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laatiri_S/0/1/0/all/0/1\">Seif Laatiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratnamogan_P/0/1/0/all/0/1\">Pirashanth Ratnamogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Joel Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_L/0/1/0/all/0/1\">Laurent Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanhuffel_W/0/1/0/all/0/1\">William Vanhuffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caspani_F/0/1/0/all/0/1\">Fabien Caspani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CCpdf: Building a High Quality Corpus for Visually Rich Documents from Web Crawl Data. (arXiv:2304.14953v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14953","description":"<p>In recent years, the field of document understanding has progressed a lot. A\nsignificant part of this progress has been possible thanks to the use of\nlanguage models pretrained on large amounts of documents. However, pretraining\ncorpora used in the domain of document understanding are single domain,\nmonolingual, or nonpublic. Our goal in this paper is to propose an efficient\npipeline for creating a big-scale, diverse, multilingual corpus of PDF files\nfrom all over the Internet using Common Crawl, as PDF files are the most\ncanonical types of documents as considered in document understanding. We\nanalysed extensively all of the steps of the pipeline and proposed a solution\nwhich is a trade-off between data quality and processing time. We also share a\nCCpdf corpus in a form or an index of PDF files along with a script for\ndownloading them, which produces a collection useful for language model\npretraining. The dataset and tools published with this paper offer researchers\nthe opportunity to develop even better multilingual language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Turski_M/0/1/0/all/0/1\">Micha&#x142; Turski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanislawek_T/0/1/0/all/0/1\">Tomasz Stanis&#x142;awek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaczmarek_K/0/1/0/all/0/1\">Karol Kaczmarek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dyda_P/0/1/0/all/0/1\">Pawe&#x142; Dyda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gralinski_F/0/1/0/all/0/1\">Filip Grali&#x144;ski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpreting Vision and Language Generative Models with Semantic Visual Priors. (arXiv:2304.14986v1 [cs.CV])","link":"http://arxiv.org/abs/2304.14986","description":"<p>When applied to Image-to-text models, interpretability methods often provide\ntoken-by-token explanations namely, they compute a visual explanation for each\ntoken of the generated sequence. Those explanations are expensive to compute\nand unable to comprehensively explain the model's output. Therefore, these\nmodels often require some sort of approximation that eventually leads to\nmisleading explanations. We develop a framework based on SHAP, that allows for\ngenerating comprehensive, meaningful explanations leveraging the meaning\nrepresentation of the output sequence as a whole. Moreover, by exploiting\nsemantic priors in the visual backbone, we extract an arbitrary number of\nfeatures that allows the efficient computation of Shapley values on large-scale\nmodels, generating at the same time highly meaningful visual explanations. We\ndemonstrate that our method generates semantically more expressive explanations\nthan traditional methods at a lower compute cost and that it can be generalized\nover other explainability methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cafagna_M/0/1/0/all/0/1\">Michele Cafagna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rojas_Barahona_L/0/1/0/all/0/1\">Lina M. Rojas-Barahona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deemter_K/0/1/0/all/0/1\">Kees van Deemter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1\">Albert Gatt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empirical Analysis of the Strengths and Weaknesses of PEFT Techniques for LLMs. (arXiv:2304.14999v1 [cs.CL])","link":"http://arxiv.org/abs/2304.14999","description":"<p>As foundation models continue to exponentially scale in size, efficient\nmethods of adaptation become increasingly critical. Parameter-efficient\nfine-tuning (PEFT), a recent class of techniques that require only modifying a\nsmall percentage of the model parameters, is currently the most popular method\nfor adapting large language models (LLMs). Several PEFT techniques have\nrecently been proposed with varying tradeoffs. We provide a comprehensive and\nuniform benchmark of various PEFT techniques across a representative LLM, the\nFLAN-T5 model, and evaluate model performance across different data scales of\nclassification and generation datasets. Based on this, we provide a framework\nfor choosing the optimal fine-tuning techniques given the task type and data\navailability. Contrary to popular belief, we also empirically prove that PEFT\ntechniques converge slower than full tuning in low data scenarios, and posit\nthe amount of data required for PEFT methods to both perform well and converge\nefficiently. Lastly, we further optimize these PEFT techniques by selectively\nchoosing which parts of the model to train, and find that these techniques can\nbe applied with significantly fewer parameters while maintaining and even\nimproving performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1\">George Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anirudh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jihan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplan_R/0/1/0/all/0/1\">Russell Kaplan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model. (arXiv:2304.15010v1 [cs.CV])","link":"http://arxiv.org/abs/2304.15010","description":"<p>How to efficiently transform large language models (LLMs) into instruction\nfollowers is recently a popular research direction, while training LLM for\nmulti-modal reasoning remains less explored. Although the recent LLaMA-Adapter\ndemonstrates the potential to handle visual inputs with LLMs, it still cannot\ngeneralize well to open-ended visual instructions and lags behind GPT-4. In\nthis paper, we present LLaMA-Adapter V2, a parameter-efficient visual\ninstruction model. Specifically, we first augment LLaMA-Adapter by unlocking\nmore learnable parameters (e.g., norm, bias and scale), which distribute the\ninstruction-following ability across the entire LLaMA model besides adapters.\nSecondly, we propose an early fusion strategy to feed visual tokens only into\nthe early LLM layers, contributing to better visual knowledge incorporation.\nThirdly, a joint training paradigm of image-text pairs and\ninstruction-following data is introduced by optimizing disjoint groups of\nlearnable parameters. This strategy effectively alleviates the interference\nbetween the two tasks of image-text alignment and instruction following and\nachieves strong multi-modal reasoning with only a small-scale image-text and\ninstruction dataset. During inference, we incorporate additional expert models\n(e.g. captioning/OCR systems) into LLaMA-Adapter to further enhance its image\nunderstanding capability without incurring training costs. Compared to the\noriginal LLaMA-Adapter, our LLaMA-Adapter V2 can perform open-ended multi-modal\ninstructions by merely introducing 14M parameters over LLaMA. The newly\ndesigned framework also exhibits stronger language-only instruction-following\ncapabilities and even excels in chat interactions. Our code and models are\navailable at https://github.com/ZrrSkywalker/LLaMA-Adapter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiaming Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Ziyi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Shijie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aojun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Conghui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiangyu Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpeechLM: Enhanced Speech Pre-Training with Unpaired Textual Data. (arXiv:2209.15329v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.15329","description":"<p>How to boost speech pre-training with textual data is an unsolved problem due\nto the fact that speech and text are very different modalities with distinct\ncharacteristics. In this paper, we propose a cross-modal Speech and Language\nModel (SpeechLM) to explicitly align speech and text pre-training with a\npre-defined unified discrete representation. Specifically, we introduce two\nalternative discrete tokenizers to bridge the speech and text modalities,\nincluding phoneme-unit and hidden-unit tokenizers, which can be trained using a\nsmall amount of paired speech-text data. Based on the trained tokenizers, we\nconvert the unlabeled speech and text data into tokens of phoneme units or\nhidden units. The pre-training objective is designed to unify the speech and\nthe text into the same discrete semantic space with a unified Transformer\nnetwork. Leveraging only 10K text sentences, our SpeechLM gets a 16\\% relative\nWER reduction over the best base model performance (from 6.8 to 5.7) on the\npublic LibriSpeech ASR benchmark. Moreover, SpeechLM with fewer parameters even\noutperforms previous SOTA models on CoVoST-2 speech translation tasks. We also\nevaluate our SpeechLM on various spoken language processing tasks under the\nuniversal representation evaluation framework SUPERB, demonstrating significant\nimprovements on content-related tasks. Our code and models are available at\nhttps://aka.ms/SpeechLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sanyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuo Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhuoyuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1\">Lirong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Both Demonstrations and Language Instructions to Efficiently Learn Robotic Tasks. (arXiv:2210.04476v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2210.04476","description":"<p>Demonstrations and natural language instructions are two common ways to\nspecify and teach robots novel tasks. However, for many complex tasks, a\ndemonstration or language instruction alone contains ambiguities, preventing\ntasks from being specified clearly. In such cases, a combination of both a\ndemonstration and an instruction more concisely and effectively conveys the\ntask to the robot than either modality alone. To instantiate this problem\nsetting, we train a single multi-task policy on a few hundred challenging\nrobotic pick-and-place tasks and propose DeL-TaCo (Joint Demo-Language Task\nConditioning), a method for conditioning a robotic policy on task embeddings\ncomprised of two components: a visual demonstration and a language instruction.\nBy allowing these two modalities to mutually disambiguate and clarify each\nother during novel task specification, DeL-TaCo (1) substantially decreases the\nteacher effort needed to specify a new task and (2) achieves better\ngeneralization performance on novel objects and instructions over previous\ntask-conditioning methods. To our knowledge, this is the first work to show\nthat simultaneously conditioning a multi-task robotic manipulation policy on\nboth demonstration and language embeddings improves sample efficiency and\ngeneralization over conditioning on either modality alone. See additional\nmaterials at https://deltaco-robot.github.io/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Albert Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1\">Raymond J. Mooney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Severity Classification of Dysarthric speech by using Self-supervised Model with Multi-task Learning. (arXiv:2210.15387v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.15387","description":"<p>Automatic assessment of dysarthric speech is essential for sustained\ntreatments and rehabilitation. However, obtaining atypical speech is\nchallenging, often leading to data scarcity issues. To tackle the problem, we\npropose a novel automatic severity assessment method for dysarthric speech,\nusing the self-supervised model in conjunction with multi-task learning.\nWav2vec 2.0 XLS-R is jointly trained for two different tasks: severity\nclassification and auxiliary automatic speech recognition (ASR). For the\nbaseline experiments, we employ hand-crafted acoustic features and machine\nlearning classifiers such as SVM, MLP, and XGBoost. Explored on the Korean\ndysarthric speech QoLT database, our model outperforms the traditional baseline\nmethods, with a relative percentage increase of 1.25% for F1-score. In\naddition, the proposed model surpasses the model trained without ASR head,\nachieving 10.61% relative percentage improvements. Furthermore, we present how\nmulti-task learning affects the severity classification performance by\nanalyzing the latent representations and regularization effect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeo_E/0/1/0/all/0/1\">Eun Jung Yeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1\">Kwanghee Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sunhee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_M/0/1/0/all/0/1\">Minhwa Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolution-enhanced Evolving Attention Networks. (arXiv:2212.08330v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2212.08330","description":"<p>Attention-based neural networks, such as Transformers, have become ubiquitous\nin numerous applications, including computer vision, natural language\nprocessing, and time-series analysis. In all kinds of attention networks, the\nattention maps are crucial as they encode semantic dependencies between input\ntokens. However, most existing attention networks perform modeling or reasoning\nbased on representations , wherein the attention maps of different layers are\nlearned separately without explicit interactions. In this paper, we propose a\nnovel and generic evolving attention mechanism, which directly models the\nevolution of inter-token relationships through a chain of residual\nconvolutional modules. The major motivations are twofold. On the one hand, the\nattention maps in different layers share transferable knowledge, thus adding a\nresidual connection can facilitate the information flow of inter-token\nrelationships across layers. On the other hand, there is naturally an\nevolutionary trend among attention maps at different abstraction levels, so it\nis beneficial to exploit a dedicated convolution-based module to capture this\nprocess. Equipped with the proposed mechanism, the convolution-enhanced\nevolving attention networks achieve superior performance in various\napplications, including time-series representation, natural language\nunderstanding, machine translation, and image classification. Especially on\ntime-series representation tasks, Evolving Attention-enhanced Dilated\nConvolutional (EA-DC-) Transformer outperforms state-of-the-art models\nsignificantly, achieving an average of 17% improvement compared to the best\nSOTA. To the best of our knowledge, this is the first work that explicitly\nmodels the layer-wise evolution of attention maps. Our implementation is\navailable at https://github.com/pkuyym/EvolvingAttention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiangang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating High-Precision Feedback for Programming Syntax Errors using Large Language Models. (arXiv:2302.04662v2 [cs.PL] UPDATED)","link":"http://arxiv.org/abs/2302.04662","description":"<p>Large language models (LLMs), such as Codex, hold great promise in enhancing\nprogramming education by automatically generating feedback for students. We\ninvestigate using LLMs to generate feedback for fixing syntax errors in Python\nprograms, a key scenario in introductory programming. More concretely, given a\nstudent's buggy program, our goal is to generate feedback comprising a fixed\nprogram along with a natural language explanation describing the errors/fixes,\ninspired by how a human tutor would give feedback. While using LLMs is\npromising, the critical challenge is to ensure high precision in the generated\nfeedback, which is imperative before deploying such technology in classrooms.\nThe main research question we study is: Can we develop LLMs-based feedback\ngeneration techniques with a tunable precision parameter, giving educators\nquality control over the feedback that students receive? To this end, we\nintroduce PyFiXV, our technique to generate high-precision feedback powered by\nCodex. The key idea behind PyFiXV is to use a novel run-time validation\nmechanism to decide whether the generated feedback is suitable for sharing with\nthe student; notably, this validation mechanism also provides a precision knob\nto educators. We perform an extensive evaluation using two real-world datasets\nof Python programs with syntax errors and show the efficacy of PyFiXV in\ngenerating high-precision feedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phung_T/0/1/0/all/0/1\">Tung Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambronero_J/0/1/0/all/0/1\">Jos&#xe9; Cambronero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulwani_S/0/1/0/all/0/1\">Sumit Gulwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohn_T/0/1/0/all/0/1\">Tobias Kohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_R/0/1/0/all/0/1\">Rupak Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_A/0/1/0/all/0/1\">Adish Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_G/0/1/0/all/0/1\">Gustavo Soares</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction Challenges. (arXiv:2304.05351v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.05351","description":"<p>Recently, large language models (LLMs) like ChatGPT have demonstrated\nremarkable performance across a variety of natural language processing tasks.\nHowever, their effectiveness in the financial domain, specifically in\npredicting stock market movements, remains to be explored. In this paper, we\nconduct an extensive zero-shot analysis of ChatGPT's capabilities in multimodal\nstock movement prediction, on three tweets and historical stock price datasets.\nOur findings indicate that ChatGPT is a \"Wall Street Neophyte\" with limited\nsuccess in predicting stock movements, as it underperforms not only\nstate-of-the-art methods but also traditional methods like linear regression\nusing price features. Despite the potential of Chain-of-Thought prompting\nstrategies and the inclusion of tweets, ChatGPT's performance remains subpar.\nFurthermore, we observe limitations in its explainability and stability,\nsuggesting the need for more specialized training or fine-tuning. This research\nprovides insights into ChatGPT's capabilities and serves as a foundation for\nfuture work aimed at improving financial market analysis and prediction by\nleveraging social media sentiment and historical stock data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qianqian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Weiguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yanzhao Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1\">Min Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jimin Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"chatClimate: Grounding Conversational AI in Climate Science. (arXiv:2304.05510v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.05510","description":"<p>Large Language Models (LLMs) have made significant progress in recent years,\nachieving remarkable results in question-answering tasks (QA). However, they\nstill face two major challenges: hallucination and outdated information after\nthe training phase. These challenges take center stage in critical domains like\nclimate change, where obtaining accurate and up-to-date information from\nreliable sources in a limited time is essential and difficult. To overcome\nthese barriers, one potential solution is to provide LLMs with access to\nexternal, scientifically accurate, and robust sources (long-term memory) to\ncontinuously update their knowledge and prevent the propagation of inaccurate,\nincorrect, or outdated information. In this study, we enhanced GPT-4 by\nintegrating the information from the Sixth Assessment Report of the\nIntergovernmental (IPCC AR6), the most comprehensive, up-to-date, and reliable\nsource in this domain. We present our conversational AI prototype, available at\nwww.chatclimate.ai and demonstrate its ability to answer challenging questions\naccurately in three different QA scenarios: asking from 1) GPT-4, 2)\nchatClimate, and 3) hybrid chatClimate. The answers and their sources were\nevaluated by our team of IPCC authors, who used their expert knowledge to score\nthe accuracy of the answers from 1 (very-low) to 5 (very-high). The evaluation\nshowed that the hybrid chatClimate provided more accurate answers, highlighting\nthe effectiveness of our solution. This approach can be easily scaled for\nchatbots in specific domains, enabling the delivery of reliable and accurate\ninformation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vaghefi_S/0/1/0/all/0/1\">Saeid Ashraf Vaghefi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muccione_V/0/1/0/all/0/1\">Veruska Muccione</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jingwei Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraus_M/0/1/0/all/0/1\">Mathias Kraus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bingler_J/0/1/0/all/0/1\">Julia Bingler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schimanski_T/0/1/0/all/0/1\">Tobias Schimanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colesanti_Senni_C/0/1/0/all/0/1\">Chiara Colesanti-Senni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webersinke_N/0/1/0/all/0/1\">Nicolas Webersinke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huggel_C/0/1/0/all/0/1\">Christrian Huggel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leippold_M/0/1/0/all/0/1\">Markus Leippold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatPLUG: Open-Domain Generative Dialogue System with Internet-Augmented Instruction Tuning for Digital Human. (arXiv:2304.07849v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.07849","description":"<p>In this paper, we present ChatPLUG, a Chinese open-domain dialogue system for\ndigital human applications that instruction finetunes on a wide range of\ndialogue tasks in a unified internet-augmented format. Different from other\nopen-domain dialogue models that focus on large-scale pre-training and scaling\nup model size or dialogue corpus, we aim to build a powerful and practical\ndialogue system for digital human with diverse skills and good multi-task\ngeneralization by internet-augmented instruction tuning. To this end, we first\nconduct large-scale pre-training on both common document corpus and dialogue\ndata with curriculum learning, so as to inject various world knowledge and\ndialogue abilities into ChatPLUG. Then, we collect a wide range of dialogue\ntasks spanning diverse features of knowledge, personality, multi-turn memory,\nand empathy, on which we further instruction tune \\modelname via unified\nnatural language instruction templates. External knowledge from an internet\nsearch is also used during instruction finetuning for alleviating the problem\nof knowledge hallucinations. We show that \\modelname outperforms\nstate-of-the-art Chinese dialogue systems on both automatic and human\nevaluation, and demonstrates strong multi-task generalization on a variety of\ntext understanding and generation tasks. In addition, we deploy \\modelname to\nreal-world applications such as Smart Speaker and Instant Message applications\nwith fast inference. Our models and code will be made publicly available on\nModelScope~\\footnote{\\small{https://modelscope.cn/models/damo/ChatPLUG-3.7B}}\nand Github~\\footnote{\\small{https://github.com/X-PLUG/ChatPLUG}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Junfeng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hehong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guohai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianhai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiayi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenshen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1\">Qi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinghao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiejing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLM as A Robotic Brain: Unifying Egocentric Memory and Control. (arXiv:2304.09349v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2304.09349","description":"<p>Embodied AI focuses on the study and development of intelligent systems that\npossess a physical or virtual embodiment (i.e. robots) and are able to\ndynamically interact with their environment. Memory and control are the two\nessential parts of an embodied system and usually require separate frameworks\nto model each of them. In this paper, we propose a novel and generalizable\nframework called LLM-Brain: using Large-scale Language Model as a robotic brain\nto unify egocentric memory and control. The LLM-Brain framework integrates\nmultiple multimodal language models for robotic tasks, utilizing a zero-shot\nlearning approach. All components within LLM-Brain communicate using natural\nlanguage in closed-loop multi-round dialogues that encompass perception,\nplanning, control, and memory. The core of the system is an embodied LLM to\nmaintain egocentric memory and control the robot. We demonstrate LLM-Brain by\nexamining two downstream tasks: active exploration and embodied question\nanswering. The active exploration tasks require the robot to extensively\nexplore an unknown environment within a limited number of actions. Meanwhile,\nthe embodied question answering tasks necessitate that the robot answers\nquestions based on observations acquired during prior explorations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mai_J/0/1/0/all/0/1\">Jinjie Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_G/0/1/0/all/0/1\">Guocheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IXA/Cogcomp at SemEval-2023 Task 2: Context-enriched Multilingual Named Entity Recognition using Knowledge Bases. (arXiv:2304.10637v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.10637","description":"<p>Named Entity Recognition (NER) is a core natural language processing task in\nwhich pre-trained language models have shown remarkable performance. However,\nstandard benchmarks like CoNLL 2003 do not address many of the challenges that\ndeployed NER systems face, such as having to classify emerging or complex\nentities in a fine-grained way. In this paper we present a novel NER cascade\napproach comprising three steps: first, identifying candidate entities in the\ninput sentence; second, linking the each candidate to an existing knowledge\nbase; third, predicting the fine-grained category for each entity candidate. We\nempirically demonstrate the significance of external knowledge bases in\naccurately classifying fine-grained and emerging entities. Our system exhibits\nrobust performance in the MultiCoNER2 shared task, even in the low-resource\nlanguage setting where we leverage knowledge bases of high-resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Ferrero_I/0/1/0/all/0/1\">Iker Garc&#xed;a-Ferrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campos_J/0/1/0/all/0/1\">Jon Ander Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainz_O/0/1/0/all/0/1\">Oscar Sainz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salaberria_A/0/1/0/all/0/1\">Ander Salaberria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Multi-Modal DBMSs for Seamless Querying of Texts and Tables. (arXiv:2304.13559v2 [cs.DB] UPDATED)","link":"http://arxiv.org/abs/2304.13559","description":"<p>In this paper, we propose Multi-Modal Databases (MMDBs), which is a new class\nof database systems that can seamlessly query text and tables using SQL. To\nenable seamless querying of textual data using SQL in an MMDB, we propose to\nextend relational databases with so-called multi-modal operators (MMOps) which\nare based on the advances of recent large language models such as GPT-3. The\nmain idea of MMOps is that they allow text collections to be treated as tables\nwithout the need to manually transform the data. As we show in our evaluation,\nour MMDB prototype can not only outperform state-of-the-art approaches such as\ntext-to-table in terms of accuracy and performance but it also requires\nsignificantly less training data to fine-tune the model for an unseen text\ncollection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Urban_M/0/1/0/all/0/1\">Matthias Urban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binnig_C/0/1/0/all/0/1\">Carsten Binnig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery. (arXiv:2304.13714v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2304.13714","description":"<p>Despite growing interest in using large language models (LLMs) in healthcare,\ncurrent explorations do not assess the real-world utility and safety of LLMs in\nclinical settings. Our objective was to determine whether two LLMs can serve\ninformation needs submitted by physicians as questions to an informatics\nconsultation service in a safe and concordant manner. Sixty six questions from\nan informatics consult service were submitted to GPT-3.5 and GPT-4 via simple\nprompts. 12 physicians assessed the LLM responses' possibility of patient harm\nand concordance with existing reports from an informatics consultation service.\nPhysician assessments were summarized based on majority vote. For no questions\ndid a majority of physicians deem either LLM response as harmful. For GPT-3.5,\nresponses to 8 questions were concordant with the informatics consult report,\n20 discordant, and 9 were unable to be assessed. There were 29 responses with\nno majority on \"Agree\", \"Disagree\", and \"Unable to assess\". For GPT-4,\nresponses to 13 questions were concordant, 15 discordant, and 3 were unable to\nbe assessed. There were 35 responses with no majority. Responses from both LLMs\nwere largely devoid of overt harm, but less than 20% of the responses agreed\nwith an answer from an informatics consultation service, responses contained\nhallucinated references, and physicians were divided on what constitutes harm.\nThese results suggest that while general purpose LLMs are able to provide safe\nand credible responses, they often do not meet the specific information need of\na given question. A definitive evaluation of the usefulness of LLMs in\nhealthcare settings will likely require additional research on prompt\nengineering, calibration, and custom-tailoring of general purpose models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dash_D/0/1/0/all/0/1\">Debadutta Dash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thapa_R/0/1/0/all/0/1\">Rahul Thapa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banda_J/0/1/0/all/0/1\">Juan M. Banda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swaminathan_A/0/1/0/all/0/1\">Akshay Swaminathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheatham_M/0/1/0/all/0/1\">Morgan Cheatham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashyap_M/0/1/0/all/0/1\">Mehr Kashyap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotecha_N/0/1/0/all/0/1\">Nikesh Kotecha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jonathan H. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gombar_S/0/1/0/all/0/1\">Saurabh Gombar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downing_L/0/1/0/all/0/1\">Lance Downing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedreira_R/0/1/0/all/0/1\">Rachel Pedreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_E/0/1/0/all/0/1\">Ethan Goh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnaout_A/0/1/0/all/0/1\">Angel Arnaout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_G/0/1/0/all/0/1\">Garret Kenn Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magon_H/0/1/0/all/0/1\">Honor Magon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1\">Nigam H. Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Data Augmentation for Context-Dependent Text-to-SQL. (arXiv:2304.13902v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.13902","description":"<p>The limited scale of annotated data constraints existing context-dependent\ntext-to-SQL models because of the complexity of labeling. The data augmentation\nmethod is a commonly used method to solve this problem. However, the data\ngenerated by current augmentation methods often lack diversity. In this paper,\nwe introduce ConDA, which generates interactive questions and corresponding SQL\nresults. We designed the SQL dialogue state to enhance the data diversity\nthrough the state transition. Meanwhile, we also present a filter method to\nensure the data quality by a grounding model. Additionally, we utilize a\ngrounding model to identify and filter low-quality questions that mismatch the\nstate information. Experimental results on the SParC and CoSQL datasets show\nthat ConDA boosts the baseline model to achieve an average improvement of\n$3.3\\%$ on complex questions. Moreover, we analyze the augmented data, which\nreveals that the data generated by ConDA are of high quality in both SQL\ntemplate hardness and types, turns, and question consistency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dingzirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_L/0/1/0/all/0/1\">Longxu Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-04-30T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}