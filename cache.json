{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-07-17T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"ACTI at EVALITA 2023: Overview of the Conspiracy Theory Identification Task. (arXiv:2307.06954v1 [cs.CL])","link":"http://arxiv.org/abs/2307.06954","description":"<p>Conspiracy Theory Identication task is a new shared task proposed for the\nfirst time at the Evalita 2023. The ACTI challenge, based exclusively on\ncomments published on conspiratorial channels of telegram, is divided into two\nsubtasks: (i) Conspiratorial Content Classification: identifying conspiratorial\ncontent and (ii) Conspiratorial Category Classification about specific\nconspiracy theory classification. A total of fifteen teams participated in the\ntask for a total of 81 submissions. We illustrate the best performing\napproaches were based on the utilization of large language models. We finally\ndraw conclusions about the utilization of these models for counteracting the\nspreading of misinformation in online platforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Russo_G/0/1/0/all/0/1\">Giuseppe Russo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoehr_N/0/1/0/all/0/1\">Niklas Stoehr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1\">Manoel Horta Ribeiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Copy Is All You Need. (arXiv:2307.06962v1 [cs.CL])","link":"http://arxiv.org/abs/2307.06962","description":"<p>The dominant text generation models compose the output by sequentially\nselecting words from a fixed vocabulary. In this paper, we formulate text\ngeneration as progressively copying text segments (e.g., words or phrases) from\nan existing text collection. We compute the contextualized representations of\nmeaningful text segments and index them using efficient vector search toolkits.\nThe task of text generation is then decomposed into a series of copy-and-paste\noperations: at each time step, we seek suitable text spans from the text\ncollection rather than selecting from a standalone vocabulary. Experiments on\nthe standard language modeling benchmark (WikiText-103) show that our approach\nachieves better generation quality according to both automatic and human\nevaluations. Besides, its inference efficiency is comparable to token-level\nautoregressive models thanks to the reduction of decoding steps. We also show\nthat our approach allows for effective domain adaptation by simply switching to\ndomain-specific text collection without extra training. Finally, we observe\nthat our approach attains additional performance gains by simply scaling up to\nlarger text collections, again without further training.\\footnote{Our source\ncodes are publicly available at\n\\url{https://github.com/gmftbyGMFTBY/Copyisallyouneed}.}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1\">Tian Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tackling Fake News in Bengali: Unraveling the Impact of Summarization vs. Augmentation on Pre-trained Language Models. (arXiv:2307.06979v1 [cs.CL])","link":"http://arxiv.org/abs/2307.06979","description":"<p>With the rise of social media and online news sources, fake news has become a\nsignificant issue globally. However, the detection of fake news in low resource\nlanguages like Bengali has received limited attention in research. In this\npaper, we propose a methodology consisting of four distinct approaches to\nclassify fake news articles in Bengali using summarization and augmentation\ntechniques with five pre-trained language models. Our approach includes\ntranslating English news articles and using augmentation techniques to curb the\ndeficit of fake news articles. Our research also focused on summarizing the\nnews to tackle the token length limitation of BERT based models. Through\nextensive experimentation and rigorous evaluation, we show the effectiveness of\nsummarization and augmentation in the case of Bengali fake news detection. We\nevaluated our models using three separate test datasets. The BanglaBERT Base\nmodel, when combined with augmentation techniques, achieved an impressive\naccuracy of 96% on the first test dataset. On the second test dataset, the\nBanglaBERT model, trained with summarized augmented news articles achieved 97%\naccuracy. Lastly, the mBERT Base model achieved an accuracy of 86% on the third\ntest dataset which was reserved for generalization performance evaluation. The\ndatasets and implementations are available at\nhttps://github.com/arman-sakif/Bengali-Fake-News-Detection\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1\">Arman Sakif Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahariar_G/0/1/0/all/0/1\">G. M. Shahariar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aziz_A/0/1/0/all/0/1\">Ahammed Tarik Aziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1\">Syed Mohibul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheikh_M/0/1/0/all/0/1\">Md. Azad Sheikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belal_T/0/1/0/all/0/1\">Tanveer Ahmed Belal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting the DARPA Communicator Data using Conversation Analysis. (arXiv:2307.06982v1 [cs.CL])","link":"http://arxiv.org/abs/2307.06982","description":"<p>The state of the art in human computer conversation leaves something to be\ndesired and, indeed, talking to a computer can be down-right annoying. This\npaper describes an approach to identifying ``opportunities for improvement'' in\nthese systems by looking for abuse in the form of swear words. The premise is\nthat humans swear at computers as a sanction and, as such, swear words\nrepresent a point of failure where the system did not behave as it should.\nHaving identified where things went wrong, we can work backward through the\ntranscripts and, using conversation analysis (CA) work out how things went\nwrong. Conversation analysis is a qualitative methodology and can appear quite\nalien - indeed unscientific - to those of us from a quantitative background.\nThe paper starts with a description of Conversation analysis in its modern\nform, and then goes on to apply the methodology to transcripts of frustrated\nand annoyed users in the DARPA Communicator project. The conclusion is that\nthere is at least one species of failure caused by the inability of the\nCommunicator systems to handle mixed initiative at the discourse structure\nlevel. Along the way, I hope to demonstrate that there is an alternative future\nfor computational linguistics that does not rely on larger and larger text\ncorpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wallis_P/0/1/0/all/0/1\">Peter Wallis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Populating Generalizable Engineering Design Knowledge. (arXiv:2307.06985v1 [cs.CL])","link":"http://arxiv.org/abs/2307.06985","description":"<p>Aiming to populate generalizable engineering design knowledge, we propose a\nmethod to extract facts of the form head entity :: relationship :: tail entity\nfrom sentences found in patent documents. These facts could be combined within\nand across patent documents to form knowledge graphs that serve as schemes for\nrepresenting as well as storing design knowledge. Existing methods in\nengineering design literature often utilise a set of predefined relationships\nto populate triples that are statistical approximations rather than facts. In\nour method, we train a tagger to identify both entities and relationships from\na sentence. Given a pair of entities thus identified, we train another tagger\nto identify the relationship tokens that specifically denote the relationship\nbetween the pair. For training these taggers, we manually construct a dataset\nof 44,227 sentences and corresponding facts. We also compare the performance of\nthe method against typically recommended approaches, wherein, we predict the\nedges among tokens by pairing the tokens independently and as part of a graph.\nWe apply our method to sentences found in patents related to fan systems and\nbuild a domain knowledge base. Upon providing an overview of the knowledge\nbase, we search for solutions relevant to some key issues prevailing in fan\nsystems. We organize the responses into knowledge graphs and hold a comparative\ndiscussion against the opinions from ChatGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siddharth_L/0/1/0/all/0/1\">L Siddharth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jianxi Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classical Out-of-Distribution Detection Methods Benchmark in Text Classification Tasks. (arXiv:2307.07002v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07002","description":"<p>State-of-the-art models can perform well in controlled environments, but they\noften struggle when presented with out-of-distribution (OOD) examples, making\nOOD detection a critical component of NLP systems. In this paper, we focus on\nhighlighting the limitations of existing approaches to OOD detection in NLP.\nSpecifically, we evaluated eight OOD detection methods that are easily\nintegrable into existing NLP systems and require no additional OOD data or\nmodel modifications. One of our contributions is providing a well-structured\nresearch environment that allows for full reproducibility of the results.\nAdditionally, our analysis shows that existing OOD detection methods for NLP\ntasks are not yet sufficiently sensitive to capture all samples characterized\nby various types of distributional shifts. Particularly challenging testing\nscenarios arise in cases of background shift and randomly shuffled word order\nwithin in domain texts. This highlights the need for future work to develop\nmore effective OOD detection approaches for the NLP problems, and our work\nprovides a well-defined foundation for further research in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baran_M/0/1/0/all/0/1\">Mateusz Baran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baran_J/0/1/0/all/0/1\">Joanna Baran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wojcik_M/0/1/0/all/0/1\">Mateusz W&#xf3;jcik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zieba_M/0/1/0/all/0/1\">Maciej Zi&#x119;ba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonczarek_A/0/1/0/all/0/1\">Adam Gonczarek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Electoral Agitation Data Set: The Use Case of the Polish Election. (arXiv:2307.07007v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07007","description":"<p>The popularity of social media makes politicians use it for political\nadvertisement. Therefore, social media is full of electoral agitation\n(electioneering), especially during the election campaigns. The election\nadministration cannot track the spread and quantity of messages that count as\nagitation under the election code. It addresses a crucial problem, while also\nuncovering a niche that has not been effectively targeted so far. Hence, we\npresent the first publicly open data set for detecting electoral agitation in\nthe Polish language. It contains 6,112 human-annotated tweets tagged with four\nlegally conditioned categories. We achieved a 0.66 inter-annotator agreement\n(Cohen's kappa score). An additional annotator resolved the mismatches between\nthe first two improving the consistency and complexity of the annotation\nprocess. The newly created data set was used to fine-tune a Polish Language\nModel called HerBERT (achieving a 68% F1 score). We also present a number of\npotential use cases for such data sets and models, enriching the paper with an\nanalysis of the Polish 2020 Presidential Election on Twitter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baran_M/0/1/0/all/0/1\">Mateusz Baran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wojcik_M/0/1/0/all/0/1\">Mateusz W&#xf3;jcik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolebski_P/0/1/0/all/0/1\">Piotr Kolebski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernaczyk_M/0/1/0/all/0/1\">Micha&#x142; Bernaczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajda_K/0/1/0/all/0/1\">Krzysztof Rajda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augustyniak_L/0/1/0/all/0/1\">&#x141;ukasz Augustyniak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kajdanowicz_T/0/1/0/all/0/1\">Tomasz Kajdanowicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation for Machine Translation via Dependency Subtree Swapping. (arXiv:2307.07025v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07025","description":"<p>We present a generic framework for data augmentation via dependency subtree\nswapping that is applicable to machine translation. We extract corresponding\nsubtrees from the dependency parse trees of the source and target sentences and\nswap these across bisentences to create augmented samples. We perform thorough\nfiltering based on graphbased similarities of the dependency trees and\nadditional heuristics to ensure that extracted subtrees correspond to the same\nmeaning. We conduct resource-constrained experiments on 4 language pairs in\nboth directions using the IWSLT text translation datasets and the Hunglish2\ncorpus. The results demonstrate consistent improvements in BLEU score over our\nbaseline models in 3 out of 4 language pairs. Our code is available on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagy_A/0/1/0/all/0/1\">Attila Nagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakatos_D/0/1/0/all/0/1\">Dorina Petra Lakatos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barta_B/0/1/0/all/0/1\">Botond Barta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nanys_P/0/1/0/all/0/1\">Patrick Nanys</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acs_J/0/1/0/all/0/1\">Judit &#xc1;cs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DIALGEN: Collaborative Human-LM Generated Dialogues for Improved Understanding of Human-Human Conversations. (arXiv:2307.07047v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07047","description":"<p>Applications that could benefit from automatic understanding of human-human\nconversations often come with challenges associated with private information in\nreal-world data such as call center or clinical conversations. Working with\nprotected data also increases costs of annotation, which limits technology\ndevelopment. To address these challenges, we propose DIALGEN, a\nhuman-in-the-loop semi-automated dialogue generation framework. DIALGEN uses a\nlanguage model (ChatGPT) that can follow schema and style specifications to\nproduce fluent conversational text, generating a complex conversation through\niteratively generating subdialogues and using human feedback to correct\ninconsistencies or redirect the flow. In experiments on structured\nsummarization of agent-client information gathering calls, framed as dialogue\nstate tracking, we show that DIALGEN data enables significant improvement in\nmodel performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1\">Bo-Ru Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haduong_N/0/1/0/all/0/1\">Nikita Haduong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chia-Hsuan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zeqiu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koester_P/0/1/0/all/0/1\">Paul Koester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utke_J/0/1/0/all/0/1\">Jean Utke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1\">Mari Ostendorf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MegaWika: Millions of reports and their sources across 50 diverse languages. (arXiv:2307.07049v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07049","description":"<p>To foster the development of new models for collaborative AI-assisted report\ngeneration, we introduce MegaWika, consisting of 13 million Wikipedia articles\nin 50 diverse languages, along with their 71 million referenced source\nmaterials. We process this dataset for a myriad of applications, going beyond\nthe initial Wikipedia citation extraction and web scraping of content,\nincluding translating non-English articles for cross-lingual applications and\nproviding FrameNet parses for automated semantic analysis. MegaWika is the\nlargest resource for sentence-level report generation and the only report\ngeneration dataset that is multilingual. We manually analyze the quality of\nthis resource through a semantically stratified sample. Finally, we provide\nbaseline results and trained models for crucial steps in automated report\ngeneration: cross-lingual question answering and citation retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barham_S/0/1/0/all/0/1\">Samuel Barham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_O/0/1/0/all/0/1\">Orion Weller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Michelle Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_K/0/1/0/all/0/1\">Kenton Murray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yarmohammadi_M/0/1/0/all/0/1\">Mahsa Yarmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengping Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vashishtha_S/0/1/0/all/0/1\">Siddharth Vashishtha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1\">Alexander Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Anqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1\">Aaron Steven White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1\">Jordan Boyd-Graber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making the Most Out of the Limited Context Length: Predictive Power Varies with Clinical Note Type and Note Section. (arXiv:2307.07051v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07051","description":"<p>Recent advances in large language models have led to renewed interest in\nnatural language processing in healthcare using the free text of clinical\nnotes. One distinguishing characteristic of clinical notes is their long time\nspan over multiple long documents. The unique structure of clinical notes\ncreates a new design choice: when the context length for a language model\npredictor is limited, which part of clinical notes should we choose as the\ninput? Existing studies either choose the inputs with domain knowledge or\nsimply truncate them. We propose a framework to analyze the sections with high\npredictive power. Using MIMIC-III, we show that: 1) predictive power\ndistribution is different between nursing notes and discharge notes and 2)\ncombining different types of notes could improve performance when the context\nlength is large. Our findings suggest that a carefully selected sampling\nfunction could enable more efficient information extraction from clinical\nnotes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hongyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yixin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lavender Yao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oermann_E/0/1/0/all/0/1\">Eric Karl Oermann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Pretrained ASR Encoders for Effective and Efficient End-to-End Speech Intent Classification and Slot Filling. (arXiv:2307.07057v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07057","description":"<p>We study speech intent classification and slot filling (SICSF) by proposing\nto use an encoder pretrained on speech recognition (ASR) to initialize an\nend-to-end (E2E) Conformer-Transformer model, which achieves the new\nstate-of-the-art results on the SLURP dataset, with 90.14% intent accuracy and\n82.27% SLURP-F1. We compare our model with encoders pretrained on\nself-supervised learning (SSL), and show that ASR pretraining is much more\neffective than SSL for SICSF. To explore parameter efficiency, we freeze the\nencoder and add Adapter modules, and show that parameter efficiency is only\nachievable with an ASR-pretrained encoder, while the SSL encoder needs full\nfinetuning to achieve comparable results. In addition, we provide an in-depth\ncomparison on end-to-end models versus cascading models (ASR+NLU), and show\nthat E2E models are better than cascaded models unless an oracle ASR model is\nprovided. Last but not least, our model is the first E2E model that achieves\nthe same performance as cascading models with oracle ASR. Code, checkpoints and\nconfigs are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">He Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balam_J/0/1/0/all/0/1\">Jagadeesh Balam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginsburg_B/0/1/0/all/0/1\">Boris Ginsburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Analysis of Dialogue Repair in Virtual Voice Assistants. (arXiv:2307.07076v1 [cs.HC])","link":"http://arxiv.org/abs/2307.07076","description":"<p>Language speakers often use what are known as repair initiators to mend\nfundamental disconnects that occur between them during verbal communication.\nPrevious research in this field has mainly focused on the human-to-human use of\nrepair initiator. We proposed an examination of dialogue repair structure\nwherein the dialogue initiator is human and the party that initiates or\nresponds to the repair is a virtual assistant. This study examined the use of\nrepair initiators in both English and Spanish with two popular assistants,\nGoogle Assistant and Apple's Siri. Our aim was to codify the differences, if\nany, in responses by voice assistants to dialogues in need of repair as\ncompared to human-human dialogues also in need of repair. Ultimately the data\ndemonstrated that not only were there differences between human-assistant and\nhuman-human dialogue repair strategies, but that there were likewise\ndifferences among the assistants and the languages studied.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galbraith_M/0/1/0/all/0/1\">Matthew Carson Galbraith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_M/0/1/0/all/0/1\">Mireia G&#xf3;mez i Mart&#xed;nez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Efficient Training Data via LLM-based Attribute Manipulation. (arXiv:2307.07099v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07099","description":"<p>In this paper, we propose a novel method, Chain-of-Thoughts Attribute\nManipulation (CoTAM), to guide few-shot learning by carefully crafted data from\nLarge Language Models (LLMs). The main idea is to create data with changes only\nin the attribute targeted by the task. Inspired by facial attribute\nmanipulation, our approach generates label-switched data by leveraging LLMs to\nmanipulate task-specific attributes and reconstruct new sentences in a\ncontrolled manner. Instead of conventional latent representation controlling,\nwe implement chain-of-thoughts decomposition and reconstruction to adapt the\nprocedure to LLMs. Extensive results on text classification and other tasks\nverify the advantage of CoTAM over other LLM-based text generation methods with\nthe same number of training examples. Analysis visualizes the attribute\nmanipulation effectiveness of CoTAM and presents the potential of LLM-guided\nlearning with even less supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Letian Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMSD2.0: Towards a Reliable Multi-modal Sarcasm Detection System. (arXiv:2307.07135v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07135","description":"<p>Multi-modal sarcasm detection has attracted much recent attention.\nNevertheless, the existing benchmark (MMSD) has some shortcomings that hinder\nthe development of reliable multi-modal sarcasm detection system: (1) There are\nsome spurious cues in MMSD, leading to the model bias learning; (2) The\nnegative samples in MMSD are not always reasonable. To solve the aforementioned\nissues, we introduce MMSD2.0, a correction dataset that fixes the shortcomings\nof MMSD, by removing the spurious cues and re-annotating the unreasonable\nsamples. Meanwhile, we present a novel framework called multi-view CLIP that is\ncapable of leveraging multi-grained cues from multiple perspectives (i.e.,\ntext, image, and text-image interaction view) for multi-modal sarcasm\ndetection. Extensive experiments show that MMSD2.0 is a valuable benchmark for\nbuilding reliable multi-modal sarcasm detection systems and multi-view CLIP can\nsignificantly outperform the previous best baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Libo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shijue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiguang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chenran Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yudi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_B/0/1/0/all/0/1\">Bin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do not Mask Randomly: Effective Domain-adaptive Pre-training by Masking In-domain Keywords. (arXiv:2307.07160v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07160","description":"<p>We propose a novel task-agnostic in-domain pre-training method that sits\nbetween generic pre-training and fine-tuning. Our approach selectively masks\nin-domain keywords, i.e., words that provide a compact representation of the\ntarget domain. We identify such keywords using KeyBERT (Grootendorst, 2020). We\nevaluate our approach using six different settings: three datasets combined\nwith two distinct pre-trained language models (PLMs). Our results reveal that\nthe fine-tuned PLMs adapted using our in-domain pre-training strategy\noutperform PLMs that used in-domain pre-training with random masking as well as\nthose that followed the common pre-train-then-fine-tune paradigm. Further, the\noverhead of identifying in-domain keywords is reasonable, e.g., 7-15% of the\npre-training time (for two epochs) for BERT Large (Devlin et al., 2019).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Golchin_S/0/1/0/all/0/1\">Shahriar Golchin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1\">Mihai Surdeanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavabi_N/0/1/0/all/0/1\">Nazgol Tavabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiapour_A/0/1/0/all/0/1\">Ata Kiapour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Drive Like a Human: Rethinking Autonomous Driving with Large Language Models. (arXiv:2307.07162v1 [cs.RO])","link":"http://arxiv.org/abs/2307.07162","description":"<p>In this paper, we explore the potential of using a large language model (LLM)\nto understand the driving environment in a human-like manner and analyze its\nability to reason, interpret, and memorize when facing complex scenarios. We\nargue that traditional optimization-based and modular autonomous driving (AD)\nsystems face inherent performance limitations when dealing with long-tail\ncorner cases. To address this problem, we propose that an ideal AD system\nshould drive like a human, accumulating experience through continuous driving\nand using common sense to solve problems. To achieve this goal, we identify\nthree key abilities necessary for an AD system: reasoning, interpretation, and\nmemorization. We demonstrate the feasibility of employing an LLM in driving\nscenarios by building a closed-loop system to showcase its comprehension and\nenvironment-interaction abilities. Our extensive experiments show that the LLM\nexhibits the impressive ability to reason and solve long-tailed cases,\nproviding valuable insights for the development of human-like autonomous\ndriving. The related code are available at\nhttps://github.com/PJLab-ADG/DriveLikeAHuman .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1\">Daocheng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Licheng Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_M/0/1/0/all/0/1\">Min Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1\">Pinlong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Botian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Retrieve In-Context Examples for Large Language Models. (arXiv:2307.07164v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07164","description":"<p>Large language models (LLMs) have demonstrated their ability to learn\nin-context, allowing them to perform various tasks based on a few input-output\nexamples. However, the effectiveness of in-context learning is heavily reliant\non the quality of the selected examples. In this paper, we propose a novel\nframework to iteratively train dense retrievers that can identify high-quality\nin-context examples for LLMs. Our framework initially trains a reward model\nbased on LLM feedback to evaluate the quality of candidate examples, followed\nby knowledge distillation to train a bi-encoder based dense retriever. Our\nexperiments on a suite of 30 tasks demonstrate that our framework significantly\nenhances in-context learning performance. Furthermore, we show the\ngeneralization ability of our framework to unseen tasks during training. An\nin-depth analysis reveals that our model improves performance by retrieving\nexamples with similar patterns, and the gains are consistent across LLMs of\nvarying sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1\">Nan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Switching Head-Tail Funnel UNITER for Dual Referring Expression Comprehension with Fetch-and-Carry Tasks. (arXiv:2307.07166v1 [cs.RO])","link":"http://arxiv.org/abs/2307.07166","description":"<p>This paper describes a domestic service robot (DSR) that fetches everyday\nobjects and carries them to specified destinations according to free-form\nnatural language instructions. Given an instruction such as \"Move the bottle on\nthe left side of the plate to the empty chair,\" the DSR is expected to identify\nthe bottle and the chair from multiple candidates in the environment and carry\nthe target object to the destination. Most of the existing multimodal language\nunderstanding methods are impractical in terms of computational complexity\nbecause they require inferences for all combinations of target object\ncandidates and destination candidates. We propose Switching Head-Tail Funnel\nUNITER, which solves the task by predicting the target object and the\ndestination individually using a single model. Our method is validated on a\nnewly-built dataset consisting of object manipulation instructions and semi\nphoto-realistic images captured in a standard Embodied AI simulator. The\nresults show that our method outperforms the baseline method in terms of\nlanguage comprehension accuracy. Furthermore, we conduct physical experiments\nin which a DSR delivers standardized everyday objects in a standardized\ndomestic environment as requested by instructions with referring expressions.\nThe experimental results show that the object grasping and placing actions are\nachieved with success rates of more than 90%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Korekata_R/0/1/0/all/0/1\">Ryosuke Korekata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kambara_M/0/1/0/all/0/1\">Motonari Kambara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshida_Y/0/1/0/all/0/1\">Yu Yoshida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishikawa_S/0/1/0/all/0/1\">Shintaro Ishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawasaki_Y/0/1/0/all/0/1\">Yosuke Kawasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takahashi_M/0/1/0/all/0/1\">Masaki Takahashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Komei Sugiura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Certified Robustness for Large Language Models with Self-Denoising. (arXiv:2307.07171v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07171","description":"<p>Although large language models (LLMs) have achieved great success in vast\nreal-world applications, their vulnerabilities towards noisy inputs have\nsignificantly limited their uses, especially in high-stake environments. In\nthese contexts, it is crucial to ensure that every prediction made by large\nlanguage models is stable, i.e., LLM predictions should be consistent given\nminor differences in the input. This largely falls into the study of certified\nrobust LLMs, i.e., all predictions of LLM are certified to be correct in a\nlocal region around the input. Randomized smoothing has demonstrated great\npotential in certifying the robustness and prediction stability of LLMs.\nHowever, randomized smoothing requires adding noise to the input before model\nprediction, and its certification performance depends largely on the model's\nperformance on corrupted data. As a result, its direct application to LLMs\nremains challenging and often results in a small certification radius. To\naddress this issue, we take advantage of the multitasking nature of LLMs and\npropose to denoise the corrupted inputs with LLMs in a self-denoising manner.\nDifferent from previous works like denoised smoothing, which requires training\na separate model to robustify LLM, our method enjoys far better efficiency and\nflexibility. Our experiment results show that our method outperforms the\nexisting certification methods under both certified robustness and empirical\nrobustness. The codes are available at\nhttps://github.com/UCSB-NLP-Chang/SelfDenoise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guanhua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1\">Bairu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wenqi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dialogue Agents 101: A Beginner's Guide to Critical Ingredients for Designing Effective Conversational Systems. (arXiv:2307.07255v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07255","description":"<p>Sharing ideas through communication with peers is the primary mode of human\ninteraction. Consequently, extensive research has been conducted in the area of\nconversational AI, leading to an increase in the availability and diversity of\nconversational tasks, datasets, and methods. However, with numerous tasks being\nexplored simultaneously, the current landscape of conversational AI becomes\nfragmented. Therefore, initiating a well-thought-out model for a dialogue agent\ncan pose significant challenges for a practitioner. Towards highlighting the\ncritical ingredients needed for a practitioner to design a dialogue agent from\nscratch, the current study provides a comprehensive overview of the primary\ncharacteristics of a dialogue agent, the supporting tasks, their corresponding\nopen-domain datasets, and the methods used to benchmark these datasets. We\nobserve that different methods have been used to tackle distinct dialogue\ntasks. However, building separate models for each task is costly and does not\nleverage the correlation among the several tasks of a dialogue agent. As a\nresult, recent trends suggest a shift towards building unified foundation\nmodels. To this end, we propose UNIT, a UNified dIalogue dataseT constructed\nfrom conversations of existing datasets for different dialogue tasks capturing\nthe nuances for each of them. We also examine the evaluation strategies used to\nmeasure the performance of dialogue agents and highlight the scope for future\nresearch in the area of conversational AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shivani Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1\">Sumit Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_M/0/1/0/all/0/1\">Milan Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving BERT with Hybrid Pooling Network and Drop Mask. (arXiv:2307.07258v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07258","description":"<p>Transformer-based pre-trained language models, such as BERT, achieve great\nsuccess in various natural language understanding tasks. Prior research found\nthat BERT captures a rich hierarchy of linguistic information at different\nlayers. However, the vanilla BERT uses the same self-attention mechanism for\neach layer to model the different contextual features. In this paper, we\npropose a HybridBERT model which combines self-attention and pooling networks\nto encode different contextual features in each layer. Additionally, we propose\na simple DropMask method to address the mismatch between pre-training and\nfine-tuning caused by excessive use of special mask tokens during Masked\nLanguage Modeling pre-training. Experiments show that HybridBERT outperforms\nBERT in pre-training with lower loss, faster training speed (8% relative),\nlower memory cost (13% relative), and also in transfer learning with 1.5%\nrelative higher accuracies on downstream tasks. Additionally, DropMask improves\naccuracies of BERT on downstream tasks across various masking rates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yukun_M/0/1/0/all/0/1\">Ma Yukun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Siqi Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MorphPiece : Moving away from Statistical Language Representation. (arXiv:2307.07262v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07262","description":"<p>Tokenization is a critical part of modern NLP pipelines. However,\ncontemporary tokenizers for Large Language Models are based on statistical\nanalysis of text corpora, without much consideration to the linguistic\nfeatures. We propose a linguistically motivated tokenization scheme,\nMorphPiece, which is based partly on morphological segmentation of the\nunderlying text. A GPT-style causal language model trained on this tokenizer\n(called MorphGPT) shows superior convergence compared to the same architecture\ntrained on a standard BPE tokenizer. Specifically we get Language Modeling\nperformance comparable to a 6 times larger model. Additionally, we evaluate\nMorphGPT on a variety of NLP tasks in supervised and unsupervised settings and\nfind superior performance across the board, compared to GPT-2 model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jabbar_H/0/1/0/all/0/1\">Haris Jabbar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are words equally surprising in audio and audio-visual comprehension?. (arXiv:2307.07277v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07277","description":"<p>We report a controlled study investigating the effect of visual information\n(i.e., seeing the speaker) on spoken language comprehension. We compare the ERP\nsignature (N400) associated with each word in audio-only and audio-visual\npresentations of the same verbal stimuli. We assess the extent to which\nsurprisal measures (which quantify the predictability of words in their lexical\ncontext) are generated on the basis of different types of language models\n(specifically n-gram and Transformer models) that predict N400 responses for\neach word. Our results indicate that cognitive effort differs significantly\nbetween multimodal and unimodal settings. In addition, our findings suggest\nthat while Transformer-based models, which have access to a larger lexical\ncontext, provide a better fit in the audio-only setting, 2-gram language models\nare more effective in the multimodal setting. This highlights the significant\nimpact of local lexical context on cognitive processing in a multimodal\nenvironment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madhyastha_P/0/1/0/all/0/1\">Pranava Madhyastha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ye Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vigliocco_G/0/1/0/all/0/1\">Gabriella Vigliocco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Replay to Remember: Continual Layer-Specific Fine-tuning for German Speech Recognition. (arXiv:2307.07280v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07280","description":"<p>While Automatic Speech Recognition (ASR) models have shown significant\nadvances with the introduction of unsupervised or self-supervised training\ntechniques, these improvements are still only limited to a subsection of\nlanguages and speakers. Transfer learning enables the adaptation of large-scale\nmultilingual models to not only low-resource languages but also to more\nspecific speaker groups. However, fine-tuning on data from new domains is\nusually accompanied by a decrease in performance on the original domain.\nTherefore, in our experiments, we examine how well the performance of\nlarge-scale ASR models can be approximated for smaller domains, with our own\ndataset of German Senior Voice Commands (SVC-de), and how much of the general\nspeech recognition performance can be preserved by selectively freezing parts\nof the model during training. To further increase the robustness of the ASR\nmodel to vocabulary and speakers outside of the fine-tuned domain, we apply\nExperience Replay for continual learning. By adding only a fraction of data\nfrom the original domain, we are able to reach Word-Error-Rates (WERs) below\n5\\% on the new domain, while stabilizing performance for general speech\nrecognition at acceptable WERs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosin_T/0/1/0/all/0/1\">Theresa Pekarek Rosin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards dialect-inclusive recognition in a low-resource language: are balanced corpora the answer?. (arXiv:2307.07295v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07295","description":"<p>ASR systems are generally built for the spoken 'standard', and their\nperformance declines for non-standard dialects/varieties. This is a problem for\na language like Irish, where there is no single spoken standard, but rather\nthree major dialects: Ulster (Ul), Connacht (Co) and Munster (Mu). As a\ndiagnostic to quantify the effect of the speaker's dialect on recognition\nperformance, 12 ASR systems were trained, firstly using baseline\ndialect-balanced training corpora, and then using modified versions of the\nbaseline corpora, where dialect-specific materials were either subtracted or\nadded. Results indicate that dialect-balanced corpora do not yield a similar\nperformance across the dialects: the Ul dialect consistently underperforms,\nwhereas Mu yields lowest WERs. There is a close relationship between Co and Mu\ndialects, but one that is not symmetrical. These results will guide future\ncorpus collection and system building strategies to optimise for cross-dialect\nperformance equity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lonergan_L/0/1/0/all/0/1\">Liam Lonergan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_M/0/1/0/all/0/1\">Mengjie Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiarain_N/0/1/0/all/0/1\">Neasa N&#xed; Chiar&#xe1;in</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gobl_C/0/1/0/all/0/1\">Christer Gobl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chasaide_A/0/1/0/all/0/1\">Ailbhe N&#xed; Chasaide</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"C3: Zero-shot Text-to-SQL with ChatGPT. (arXiv:2307.07306v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07306","description":"<p>This paper proposes a ChatGPT-based zero-shot Text-to-SQL method, dubbed C3,\nwhich achieves 82.3\\% in terms of execution accuracy on the holdout test set of\nSpider and becomes the state-of-the-art zero-shot Text-to-SQL method on the\nSpider Challenge. C3 consists of three key components: Clear Prompting (CP),\nCalibration with Hints (CH), and Consistent Output (CO), which are\ncorresponding to the model input, model bias and model output respectively. It\nprovides a systematic treatment for zero-shot Text-to-SQL. Extensive\nexperiments have been conducted to verify the effectiveness and efficiency of\nour proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xuemei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yuhang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuren Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunjun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_l/0/1/0/all/0/1\">lu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jinshu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_D/0/1/0/all/0/1\">Dongfang Lou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs. (arXiv:2307.07312v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07312","description":"<p>In any system that uses structured knowledge graph (KG) data as its\nunderlying knowledge representation, KG-to-text generation is a useful tool for\nturning parts of the graph data into text that can be understood by humans.\nRecent work has shown that models that make use of pretraining on large amounts\nof text data can perform well on the KG-to-text task even with relatively small\nsets of training data on the specific graph-to-text task. In this paper, we\nbuild on this concept by using large language models to perform zero-shot\ngeneration based on nothing but the model's understanding of the triple\nstructure from what it can read. We show that ChatGPT achieves near\nstate-of-the-art performance on some measures of the WebNLG 2020 challenge, but\nfalls behind on others. Additionally, we compare factual, counter-factual and\nfictional statements, and show that there is a significant connection between\nwhat the LLM already knows about the data it is parsing and the quality of the\noutput text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Axelsson_A/0/1/0/all/0/1\">Agnes Axelsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skantze_G/0/1/0/all/0/1\">Gabriel Skantze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid moderation in the newsroom: Recommending featured posts to content moderators. (arXiv:2307.07317v1 [cs.IR])","link":"http://arxiv.org/abs/2307.07317","description":"<p>Online news outlets are grappling with the moderation of user-generated\ncontent within their comment section. We present a recommender system based on\nranking class probabilities to support and empower the moderator in choosing\nfeatured posts, a time-consuming task. By combining user and textual content\nfeatures we obtain an optimal classification F1-score of 0.44 on the test set.\nFurthermore, we observe an optimum mean NDCG@5 of 0.87 on a large set of\nvalidation articles. As an expert evaluation, content moderators assessed the\noutput of a random selection of articles by choosing comments to feature based\non the recommendations, which resulted in a NDCG score of 0.83. We conclude\nthat first, adding text features yields the best score and second, while\nchoosing featured content remains somewhat subjective, content moderators found\nsuitable comments in all but one evaluated recommendations. We end the paper by\nanalyzing our best-performing model, a step towards transparency and\nexplainability in hybrid content moderation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Waterschoot_C/0/1/0/all/0/1\">Cedric Waterschoot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosch_A/0/1/0/all/0/1\">Antal van den Bosch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Different Is Stereotypical Bias Across Languages?. (arXiv:2307.07331v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07331","description":"<p>Recent studies have demonstrated how to assess the stereotypical bias in\npre-trained English language models. In this work, we extend this branch of\nresearch in multiple different dimensions by systematically investigating (a)\nmono- and multilingual models of (b) different underlying architectures with\nrespect to their bias in (c) multiple different languages. To that end, we make\nuse of the English StereoSet data set (Nadeem et al., 2021), which we\nsemi-automatically translate into German, French, Spanish, and Turkish. We find\nthat it is of major importance to conduct this type of analysis in a\nmultilingual setting, as our experiments show a much more nuanced picture as\nwell as notable differences from the English-only analysis. The main takeaways\nfrom our analysis are that mGPT-2 (partly) shows surprising anti-stereotypical\nbehavior across languages, English (monolingual) models exhibit the strongest\nbias, and the stereotypes reflected in the data set are least present in\nTurkish models. Finally, we release our codebase alongside the translated data\nsets and practical guidelines for the semi-automatic translation to encourage a\nfurther extension of our work to other languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ozturk_I/0/1/0/all/0/1\">Ibrahim Tolga &#xd6;zt&#xfc;rk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nedelchev_R/0/1/0/all/0/1\">Rostislav Nedelchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heumann_C/0/1/0/all/0/1\">Christian Heumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arias_E/0/1/0/all/0/1\">Esteban Garces Arias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roger_M/0/1/0/all/0/1\">Marius Roger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1\">Bernd Bischl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assenmacher_M/0/1/0/all/0/1\">Matthias A&#xdf;enmacher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gloss Attention for Gloss-free Sign Language Translation. (arXiv:2307.07361v1 [cs.CV])","link":"http://arxiv.org/abs/2307.07361","description":"<p>Most sign language translation (SLT) methods to date require the use of gloss\nannotations to provide additional supervision information, however, the\nacquisition of gloss is not easy. To solve this problem, we first perform an\nanalysis of existing models to confirm how gloss annotations make SLT easier.\nWe find that it can provide two aspects of information for the model, 1) it can\nhelp the model implicitly learn the location of semantic boundaries in\ncontinuous sign language videos, 2) it can help the model understand the sign\nlanguage video globally. We then propose \\emph{gloss attention}, which enables\nthe model to keep its attention within video segments that have the same\nsemantics locally, just as gloss helps existing models do. Furthermore, we\ntransfer the knowledge of sentence-to-sentence similarity from the natural\nlanguage model to our gloss attention SLT network (GASLT) to help it understand\nsign language videos at the sentence level. Experimental results on multiple\nlarge-scale sign language datasets show that our proposed GASLT model\nsignificantly outperforms existing methods. Our code is provided in\n\\url{https://github.com/YinAoXiong/GASLT}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_A/0/1/0/all/0/1\">Aoxiong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_T/0/1/0/all/0/1\">Tianyun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Li Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Weike Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_T/0/1/0/all/0/1\">Tao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A scoping review on multimodal deep learning in biomedical images and texts. (arXiv:2307.07362v1 [cs.CV])","link":"http://arxiv.org/abs/2307.07362","description":"<p>Computer-assisted diagnostic and prognostic systems of the future should be\ncapable of simultaneously processing multimodal data. Multimodal deep learning\n(MDL), which involves the integration of multiple sources of data, such as\nimages and text, has the potential to revolutionize the analysis and\ninterpretation of biomedical data. However, it only caught researchers'\nattention recently. To this end, there is a critical need to conduct a\nsystematic review on this topic, identify the limitations of current work, and\nexplore future directions. In this scoping review, we aim to provide a\ncomprehensive overview of the current state of the field and identify key\nconcepts, types of studies, and research gaps with a focus on biomedical images\nand texts joint learning, mainly because these two were the most commonly\navailable data types in MDL research. This study reviewed the current uses of\nmultimodal deep learning on five tasks: (1) Report generation, (2) Visual\nquestion answering, (3) Cross-modal retrieval, (4) Computer-aided diagnosis,\nand (5) Semantic segmentation. Our results highlight the diverse applications\nand potential of MDL and suggest directions for future research in the field.\nWe hope our review will facilitate the collaboration of natural language\nprocessing (NLP) and medical imaging communities and support the next\ngeneration of decision-making and computer-assisted diagnostic system\ndevelopment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhaoyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingquan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qingqing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qianqian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Composition-contrastive Learning for Sentence Embeddings. (arXiv:2307.07380v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07380","description":"<p>Vector representations of natural language are ubiquitous in search\napplications. Recently, various methods based on contrastive learning have been\nproposed to learn textual representations from unlabelled data; by maximizing\nalignment between minimally-perturbed embeddings of the same text, and\nencouraging a uniform distribution of embeddings across a broader corpus.\nDifferently, we propose maximizing alignment between texts and a composition of\ntheir phrasal constituents. We consider several realizations of this objective\nand elaborate the impact on representations in each case. Experimental results\non semantic textual similarity tasks show improvements over baselines that are\ncomparable with state-of-the-art approaches. Moreover, this work is the first\nto do so without incurring costs in auxiliary training objectives or additional\nnetwork parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chanchani_S/0/1/0/all/0/1\">Sachin J. Chanchani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Ruihong Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rank Your Summaries: Enhancing Bengali Text Summarization via Ranking-based Approach. (arXiv:2307.07392v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07392","description":"<p>With the increasing need for text summarization techniques that are both\nefficient and accurate, it becomes crucial to explore avenues that enhance the\nquality and precision of pre-trained models specifically tailored for\nsummarizing Bengali texts. When it comes to text summarization tasks, there are\nnumerous pre-trained transformer models at one's disposal. Consequently, it\nbecomes quite a challenge to discern the most informative and relevant summary\nfor a given text among the various options generated by these pre-trained\nsummarization models. This paper aims to identify the most accurate and\ninformative summary for a given text by utilizing a simple but effective\nranking-based approach that compares the output of four different pre-trained\nBengali text summarization models. The process begins by carrying out\npreprocessing of the input text that involves eliminating unnecessary elements\nsuch as special characters and punctuation marks. Next, we utilize four\npre-trained summarization models to generate summaries, followed by applying a\ntext ranking algorithm to identify the most suitable summary. Ultimately, the\nsummary with the highest ranking score is chosen as the final one. To evaluate\nthe effectiveness of this approach, the generated summaries are compared\nagainst human-annotated summaries using standard NLG metrics such as BLEU,\nROUGE, BERTScore, WIL, WER, and METEOR. Experimental results suggest that by\nleveraging the strengths of each pre-trained transformer model and combining\nthem using a ranking-based approach, our methodology significantly improves the\naccuracy and effectiveness of the Bengali text summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahariar_G/0/1/0/all/0/1\">G. M. Shahariar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukder_T/0/1/0/all/0/1\">Tonmoy Talukder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sotez_R/0/1/0/all/0/1\">Rafin Alam Khan Sotez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shawon_M/0/1/0/all/0/1\">Md. Tanvir Rouf Shawon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phoneme-retrieval; voice recognition; vowels recognition. (arXiv:2307.07407v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07407","description":"<p>A phoneme-retrieval technique is proposed, which is due to the particular way\nof the construction of the network. An initial set of neurons is given. The\nnumber of these neurons is approximately equal to the number of typical\nstructures of the data. For example if the network is built for voice retrieval\nthen the number of neurons must be equal to the number of characteristic\nphonemes of the alphabet of the language spoken by the social group to which\nthe particular person belongs. Usually this task is very complicated and the\nnetwork can depend critically on the samples used for the learning. If the\nnetwork is built for image retrieval then it works only if the data to be\nretrieved belong to a particular set of images. If the network is built for\nvoice recognition it works only for some particular set of words. A typical\nexample is the words used for the flight of airplanes. For example a command\nlike the \"airplane should make a turn of 120 degrees towards the east\" can be\neasily recognized by the network if a suitable learning procedure is used.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tirozzi_B/0/1/0/all/0/1\">Brunello Tirozzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lecian_O/0/1/0/all/0/1\">Orchidea Maria Lecian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KU-DMIS-MSRA at RadSum23: Pre-trained Vision-Language Model for Radiology Report Summarization. (arXiv:2307.07409v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07409","description":"<p>In this paper, we introduce CheXOFA, a new pre-trained vision-language model\n(VLM) for the chest X-ray domain. Our model is initially pre-trained on various\nmultimodal datasets within the general domain before being transferred to the\nchest X-ray domain. Following a prominent VLM, we unify various domain-specific\ntasks into a simple sequence-to-sequence schema. It enables the model to\neffectively learn the required knowledge and skills from limited resources in\nthe domain. Demonstrating superior performance on the benchmark datasets\nprovided by the BioNLP shared task, our model benefits from its training across\nmultiple tasks and domains. With subtle techniques including ensemble and\nfactual calibration, our system achieves first place on the RadSum23\nleaderboard for the hidden test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gangwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hajung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1\">Lei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1\">Seongsu Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chanhwi Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1\">Mujeen Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunjae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1\">Kun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1\">Eric Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting LLM-Generated Text in Computing Education: A Comparative Study for ChatGPT Cases. (arXiv:2307.07411v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07411","description":"<p>Due to the recent improvements and wide availability of Large Language Models\n(LLMs), they have posed a serious threat to academic integrity in education.\nModern LLM-generated text detectors attempt to combat the problem by offering\neducators with services to assess whether some text is LLM-generated. In this\nwork, we have collected 124 submissions from computer science students before\nthe creation of ChatGPT. We then generated 40 ChatGPT submissions. We used this\ndata to evaluate eight publicly-available LLM-generated text detectors through\nthe measures of accuracy, false positives, and resilience. The purpose of this\nwork is to inform the community of what LLM-generated text detectors work and\nwhich do not, but also to provide insights for educators to better maintain\nacademic integrity in their courses. Our results find that CopyLeaks is the\nmost accurate LLM-generated text detector, GPTKit is the best LLM-generated\ntext detector to reduce false positives, and GLTR is the most resilient\nLLM-generated text detector. We also express concerns over 52 false positives\n(of 114 human written submissions) generated by GPTZero. Finally, we note that\nall LLM-generated text detectors are less accurate with code, other languages\n(aside from English), and after the use of paraphrasing tools (like QuillBot).\nModern detectors are still in need of improvements so that they can offer a\nfull-proof solution to help maintain academic integrity. Further, their\nusability can be improved by facilitating a smooth API integration, providing\nclear documentation of their features and the understandability of their\nmodel(s), and supporting more commonly used languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orenstrakh_M/0/1/0/all/0/1\">Michael Sheinman Orenstrakh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karnalim_O/0/1/0/all/0/1\">Oscar Karnalim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suarez_C/0/1/0/all/0/1\">Carlos Anibal Suarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liut_M/0/1/0/all/0/1\">Michael Liut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HuCurl: Human-induced Curriculum Discovery. (arXiv:2307.07412v1 [cs.LG])","link":"http://arxiv.org/abs/2307.07412","description":"<p>We introduce the problem of curriculum discovery and describe a curriculum\nlearning framework capable of discovering effective curricula in a curriculum\nspace based on prior knowledge about sample difficulty. Using annotation\nentropy and loss as measures of difficulty, we show that (i): the\ntop-performing discovered curricula for a given model and dataset are often\nnon-monotonic as opposed to monotonic curricula in existing literature, (ii):\nthe prevailing easy-to-hard or hard-to-easy transition curricula are often at\nthe risk of underperforming, and (iii): the curricula discovered for smaller\ndatasets and models perform well on larger datasets and models respectively.\nThe proposed framework encompasses some of the existing curriculum learning\napproaches and can discover curricula that outperform them across several NLP\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elgaar_M/0/1/0/all/0/1\">Mohamed Elgaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amiri_H/0/1/0/all/0/1\">Hadi Amiri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoHint: Automatic Prompt Optimization with Hint Generation. (arXiv:2307.07415v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07415","description":"<p>This paper presents AutoHint, a novel framework for automatic prompt\nengineering and optimization for Large Language Models (LLM). While LLMs have\ndemonstrated remarkable ability in achieving high-quality annotation in various\ntasks, the key to applying this ability to specific tasks lies in developing\nhigh-quality prompts. Thus we propose a framework to inherit the merits of both\nin-context learning and zero-shot learning by incorporating enriched\ninstructions derived from input-output demonstrations to optimize original\nprompt. We refer to the enrichment as the hint and propose a framework to\nautomatically generate the hint from labeled data. More concretely, starting\nfrom an initial prompt, our method first instructs a LLM to deduce new hints\nfor selected samples from incorrect predictions, and then summarizes from\nper-sample hints and adds the results back to the initial prompt to form a new,\nenriched instruction. The proposed method is evaluated on the BIG-Bench\nInstruction Induction dataset for both zero-shot and few-short prompts, where\nexperiments demonstrate our method is able to significantly boost accuracy for\nmultiple tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xue Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinchuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Homma_Y/0/1/0/all/0/1\">Youkow Homma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Min Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jian Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charles_D/0/1/0/all/0/1\">Denis Charles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoPDA: Robust Prompt-based Data Augmentation for Low-Resource Named Entity Recognition. (arXiv:2307.07417v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07417","description":"<p>Data augmentation has been widely used in low-resource NER tasks to tackle\nthe problem of data sparsity. However, previous data augmentation methods have\nthe disadvantages of disrupted syntactic structures, token-label mismatch, and\nrequirement for external knowledge or manual effort. To address these issues,\nwe propose \\textbf{Ro}bust \\textbf{P}rompt-based \\textbf{D}ata\n\\textbf{A}ugmentation (RoPDA) for low-resource NER. Based on pre-trained\nlanguage models (PLMs) with continuous prompt, RoPDA performs entity\naugmentation and context augmentation through five fundamental augmentation\noperations to generate label-flipping and label-preserving examples. To\noptimize the utilization of the augmented samples, we present two techniques:\nSelf-Consistency Filtering and mixup. The former effectively eliminates\nlow-quality samples, while the latter prevents performance degradation arising\nfrom the direct utilization of label-flipping samples. Extensive experiments on\nthree benchmarks from different domains demonstrate that RoPDA significantly\nimproves upon strong baselines, and also outperforms state-of-the-art\nsemi-supervised learning methods when unlabeled data is included.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Sihan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1\">Furao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Named entity recognition using GPT for identifying comparable companies. (arXiv:2307.07420v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07420","description":"<p>For both public and private firms, comparable companies analysis is widely\nused as a method for company valuation. In particular, the method is of great\nvalue for valuation of private equity companies. The several approaches to the\ncomparable companies method usually rely on a qualitative approach to\nidentifying similar peer companies, which tends to use established industry\nclassification schemes and/or analyst intuition and knowledge. However, more\nquantitative methods have started being used in the literature and in the\nprivate equity industry, in particular, machine learning clustering, and\nnatural language processing (NLP). For NLP methods, the process consists of\nextracting product entities from e.g., the company's website or company\ndescriptions from some financial database system and then to perform similarity\nanalysis. Here, using companies descriptions/summaries from publicly available\ncompanies' Wikipedia websites, we show that using large language models (LLMs),\nsuch as GPT from openaAI, has a much higher precision and success rate than\nusing the standard named entity recognition (NER) which uses manual annotation.\nWe demonstrate quantitatively a higher precision rate, and show that,\nqualitatively, it can be used to create appropriate comparable companies peer\ngroups which can then be used for equity valuation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Covas_E/0/1/0/all/0/1\">Eurico Covas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sumformer: A Linear-Complexity Alternative to Self-Attention for Speech Recognition. (arXiv:2307.07421v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07421","description":"<p>Modern speech recognition systems rely on self-attention. Unfortunately,\ntoken mixing with self-attention takes quadratic time in the length of the\nspeech utterance, slowing down inference as well as training and increasing\nmemory consumption. Cheaper alternatives to self-attention for ASR have been\ndeveloped, but fail to consistently reach the same level of accuracy. In\npractice, however, the self-attention weights of trained speech recognizers\ntake the form of a global average over time. This paper, therefore, proposes a\nlinear-time alternative to self-attention for speech recognition. It summarises\na whole utterance with the mean over vectors for all time steps. This single\nsummary is then combined with time-specific information. We call this method\n``Summary Mixing''. Introducing Summary Mixing in state-of-the-art ASR models\nmakes it feasible to preserve or exceed previous speech recognition performance\nwhile lowering the training and inference times by up to 27% and reducing the\nmemory budget by a factor of two.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parcollet_T/0/1/0/all/0/1\">Titouan Parcollet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalen_R/0/1/0/all/0/1\">Rogier van Dalen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shucong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Sourav Bhattacharya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can LLMs be Good Financial Advisors?: An Initial Study in Personal Decision Making for Optimized Outcomes. (arXiv:2307.07422v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07422","description":"<p>Increasingly powerful Large Language Model (LLM) based chatbots, like ChatGPT\nand Bard, are becoming available to users that have the potential to\nrevolutionize the quality of decision-making achieved by the public. In this\ncontext, we set out to investigate how such systems perform in the personal\nfinance domain, where financial inclusion has been an overarching stated aim of\nbanks for decades. We asked 13 questions representing banking products in\npersonal finance: bank account, credit card, and certificate of deposits and\ntheir inter-product interactions, and decisions related to high-value\npurchases, payment of bank dues, and investment advice, and in different\ndialects and languages (English, African American Vernacular English, and\nTelugu). We find that although the outputs of the chatbots are fluent and\nplausible, there are still critical gaps in providing accurate and reliable\nfinancial information using LLM-based chatbots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_K/0/1/0/all/0/1\">Kausik Lakkaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuruma_S/0/1/0/all/0/1\">Sai Krishna Revanth Vuruma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pallagani_V/0/1/0/all/0/1\">Vishal Pallagani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muppasani_B/0/1/0/all/0/1\">Bharath Muppasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_B/0/1/0/all/0/1\">Biplav Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards spoken dialect identification of Irish. (arXiv:2307.07436v1 [cs.CL])","link":"http://arxiv.org/abs/2307.07436","description":"<p>The Irish language is rich in its diversity of dialects and accents. This\ncompounds the difficulty of creating a speech recognition system for the\nlow-resource language, as such a system must contend with a high degree of\nvariability with limited corpora. A recent study investigating dialect bias in\nIrish ASR found that balanced training corpora gave rise to unequal dialect\nperformance, with performance for the Ulster dialect being consistently worse\nthan for the Connacht or Munster dialects. Motivated by this, the present\nexperiments investigate spoken dialect identification of Irish, with a view to\nincorporating such a system into the speech recognition pipeline. Two acoustic\nclassification models are tested, XLS-R and ECAPA-TDNN, in conjunction with a\ntext-based classifier using a pretrained Irish-language BERT model. The\nECAPA-TDNN, particularly a model pretrained for language identification on the\nVoxLingua107 dataset, performed best overall, with an accuracy of 73%. This was\nfurther improved to 76% by fusing the model's outputs with the text-based\nmodel. The Ulster dialect was most accurately identified, with an accuracy of\n94%, however the model struggled to disambiguate between the Connacht and\nMunster dialects, suggesting a more nuanced approach may be necessary to\nrobustly distinguish between the dialects of Irish.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lonergan_L/0/1/0/all/0/1\">Liam Lonergan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_M/0/1/0/all/0/1\">Mengjie Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiarain_N/0/1/0/all/0/1\">Neasa N&#xed; Chiar&#xe1;in</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gobl_C/0/1/0/all/0/1\">Christer Gobl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chasaide_A/0/1/0/all/0/1\">Ailbhe N&#xed; Chasaide</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Population Expansion for Training Language Models with Private Federated Learning. (arXiv:2307.07477v1 [cs.LG])","link":"http://arxiv.org/abs/2307.07477","description":"<p>Federated learning (FL) combined with differential privacy (DP) offers\nmachine learning (ML) training with distributed devices and with a formal\nprivacy guarantee. With a large population of devices, FL with DP produces a\nperformant model in a timely manner. However, for applications with a smaller\npopulation, not only does the model utility degrade as the DP noise is\ninversely proportional to population, but also the training latency increases\nsince waiting for enough clients to become available from a smaller pool is\nslower. In this work, we thus propose expanding the population based on domain\nadaptation techniques to speed up the training and improves the final model\nquality when training with small populations. We empirically demonstrate that\nour techniques can improve the utility by 13% to 30% on real-world language\nmodeling datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koga_T/0/1/0/all/0/1\">Tatsuki Koga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Congzheng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelikan_M/0/1/0/all/0/1\">Martin Pelikan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitnis_M/0/1/0/all/0/1\">Mona Chitnis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeQueries: A Dataset of Semantic Queries over Code. (arXiv:2209.08372v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2209.08372","description":"<p>Developers often have questions about semantic aspects of code they are\nworking on, e.g., \"Is there a class whose parent classes declare a conflicting\nattribute?\". Answering them requires understanding code semantics such as\nattributes and inheritance relation of classes. An answer to such a question\nshould identify code spans constituting the answer (e.g., the declaration of\nthe subclass) as well as supporting facts (e.g., the definitions of the\nconflicting attributes). The existing work on question-answering over code has\nconsidered yes/no questions or method-level context. We contribute a labeled\ndataset, called CodeQueries, of semantic queries over Python code. Compared to\nthe existing datasets, in CodeQueries, the queries are about code semantics,\nthe context is file level and the answers are code spans. We curate the dataset\nbased on queries supported by a widely-used static analysis tool, CodeQL, and\ninclude both positive and negative examples, and queries requiring single-hop\nand multi-hop reasoning.\n</p>\n<p>To assess the value of our dataset, we evaluate baseline neural approaches.\nWe study a large language model (GPT3.5-Turbo) in zero-shot and few-shot\nsettings on a subset of CodeQueries. We also evaluate a BERT style model\n(CuBERT) with fine-tuning. We find that these models achieve limited success on\nCodeQueries. CodeQueries is thus a challenging dataset to test the ability of\nneural models, to understand code semantics, in the extractive\nquestion-answering setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1\">Surya Prakash Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandal_M/0/1/0/all/0/1\">Madhurima Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_S/0/1/0/all/0/1\">Shikhar Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanade_A/0/1/0/all/0/1\">Aditya Kanade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maniatis_P/0/1/0/all/0/1\">Petros Maniatis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shevade_S/0/1/0/all/0/1\">Shirish Shevade</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformer Based Model for Describing a Set of Images as a Story. (arXiv:2210.02762v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2210.02762","description":"<p>Visual Story-Telling is the process of forming a multi-sentence story from a\nset of images. Appropriately including visual variation and contextual\ninformation captured inside the input images is one of the most challenging\naspects of visual storytelling. Consequently, stories developed from a set of\nimages often lack cohesiveness, relevance, and semantic relationship. In this\npaper, we propose a novel Vision Transformer Based Model for describing a set\nof images as a story. The proposed method extracts the distinct features of the\ninput images using a Vision Transformer (ViT). Firstly, input images are\ndivided into 16X16 patches and bundled into a linear projection of flattened\npatches. The transformation from a single image to multiple image patches\ncaptures the visual variety of the input visual patterns. These features are\nused as input to a Bidirectional-LSTM which is part of the sequence encoder.\nThis captures the past and future image context of all image patches. Then, an\nattention mechanism is implemented and used to increase the discriminatory\ncapacity of the data fed into the language model, i.e. a Mogrifier-LSTM. The\nperformance of our proposed model is evaluated using the Visual Story-Telling\ndataset (VIST), and the results show that our model outperforms the current\nstate of the art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malakan_Z/0/1/0/all/0/1\">Zainy M. Malakan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_G/0/1/0/all/0/1\">Ghulam Mubashar Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dialogs Re-enacted Across Languages. (arXiv:2211.11584v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.11584","description":"<p>To support machine learning of cross-language prosodic mappings and other\nways to improve speech-to-speech translation, we present a protocol for\ncollecting closely matched pairs of utterances across languages, a description\nof the resulting data collection and its public release, and some observations\nand musings. This report is intended for: people using this corpus, people\nextending this corpus, and people designing similar collections of bilingual\ndialog data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ward_N/0/1/0/all/0/1\">Nigel G. Ward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avila_J/0/1/0/all/0/1\">Jonathan E. Avila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivas_E/0/1/0/all/0/1\">Emilia Rivas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marco_D/0/1/0/all/0/1\">Divette Marco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Effect of Anticipation on Reading Times. (arXiv:2211.14301v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.14301","description":"<p>Over the past two decades, numerous studies have demonstrated how less\npredictable (i.e., higher surprisal) words take more time to read. In general,\nthese studies have implicitly assumed the reading process is purely responsive:\nReaders observe a new word and allocate time to process it as required. We\nargue that prior results are also compatible with a reading process that is at\nleast partially anticipatory: Readers could make predictions about a future\nword and allocate time to process it based on their expectation. In this work,\nwe operationalize this anticipation as a word's contextual entropy. We assess\nthe effect of anticipation on reading by comparing how well surprisal and\ncontextual entropy predict reading times on four naturalistic reading datasets:\ntwo self-paced and two eye-tracking. Experimentally, across datasets and\nanalyses, we find substantial evidence for effects of contextual entropy over\nsurprisal on a word's reading time (RT): in fact, entropy is sometimes better\nthan surprisal in predicting a word's RT. Spillover effects, however, are\ngenerally not captured by entropy, but only by surprisal. Further, we\nhypothesize four cognitive mechanisms through which contextual entropy could\nimpact RTs -- three of which we are able to design experiments to analyze.\nOverall, our results support a view of reading that is not just responsive, but\nalso anticipatory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilcox_E/0/1/0/all/0/1\">Ethan G. Wilcox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Effective Deployment of Contrastive Learning in Multi-label Text Classification. (arXiv:2212.00552v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.00552","description":"<p>The effectiveness of contrastive learning technology in natural language\nprocessing tasks is yet to be explored and analyzed. How to construct positive\nand negative samples correctly and reasonably is the core challenge of\ncontrastive learning. It is even harder to discover contrastive objects in\nmulti-label text classification tasks. There are very few contrastive losses\nproposed previously. In this paper, we investigate the problem from a different\nangle by proposing five novel contrastive losses for multi-label text\nclassification tasks. These are Strict Contrastive Loss (SCL), Intra-label\nContrastive Loss (ICL), Jaccard Similarity Contrastive Loss (JSCL), Jaccard\nSimilarity Probability Contrastive Loss (JSPCL), and Stepwise Label Contrastive\nLoss (SLCL). We explore the effectiveness of contrastive learning for\nmulti-label text classification tasks by the employment of these novel losses\nand provide a set of baseline models for deploying contrastive learning\ntechniques on specific tasks. We further perform an interpretable analysis of\nour approach to show how different components of contrastive learning losses\nplay their roles. The experimental results show that our proposed contrastive\nlosses can bring improvement to multi-label text classification tasks. Our work\nalso explores how contrastive learning should be adapted for multi-label text\nclassification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_G/0/1/0/all/0/1\">Guanqiu Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jigang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Aimin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dong Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.04391","description":"<p>In industry deep learning application, our manually labeled data has a\ncertain number of noisy data. To solve this problem and achieve more than 90\nscore in dev dataset, we present a simple method to find the noisy data and\nre-label the noisy data by human, given the model predictions as references in\nhuman labeling. In this paper, we illustrate our idea for a broad set of deep\nlearning tasks, includes classification, sequence tagging, object detection,\nsequence generation, click-through rate prediction. The experimental results\nand human evaluation results verify our idea.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction. (arXiv:2303.05063v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.05063","description":"<p>Large language models (LLMs), such as GPT-3 and ChatGPT, have demonstrated\nremarkable results in various natural language processing (NLP) tasks with\nin-context learning, which involves inference based on a few demonstration\nexamples. Despite their successes in NLP tasks, no investigation has been\nconducted to assess the ability of LLMs to perform document information\nextraction (DIE) using in-context learning. Applying LLMs to DIE poses two\nchallenges: the modality and task gap. To this end, we propose a simple but\neffective in-context learning framework called ICL-D3IE, which enables LLMs to\nperform DIE with different types of demonstration examples. Specifically, we\nextract the most difficult and distinct segments from hard training documents\nas hard demonstrations for benefiting all test instances. We design\ndemonstrations describing relationships that enable LLMs to understand\npositional relationships. We introduce formatting demonstrations for easy\nanswer extraction. Additionally, the framework improves diverse demonstrations\nby updating them iteratively. Our experiments on three widely used benchmark\ndatasets demonstrate that the ICL-D3IE framework enables GPT-3/ChatGPT to\nachieve superior performance when compared to previous pre-trained methods\nfine-tuned with full training in both the in-distribution (ID) setting and in\nthe out-of-distribution (OOD) setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiabang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Heng Tao Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images. (arXiv:2303.07274v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2303.07274","description":"<p>Weird, unusual, and uncanny images pique the curiosity of observers because\nthey challenge commonsense. For example, an image released during the 2022\nworld cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo\nplaying chess, which playfully violates our expectation that their competition\nshould occur on the football field. Humans can easily recognize and interpret\nthese unconventional images, but can AI models do the same? We introduce\nWHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is\ncomprised of purposefully commonsense-defying images created by designers using\npublicly-available image generation tools like Midjourney. We consider several\ntasks posed over the dataset. In addition to image captioning, cross-modal\nmatching, and visual question answering, we introduce a difficult explanation\ngeneration task, where models must identify and explain why a given image is\nunusual. Our results show that state-of-the-art models such as GPT3 and BLIP2\nstill lag behind human performance on WHOOPS!. We hope our dataset will inspire\nthe development of AI models with stronger visual commonsense reasoning\nabilities. Data, models and code are available at the project website:\nwhoops-benchmark.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bitton_Guetta_N/0/1/0/all/0/1\">Nitzan Bitton-Guetta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1\">Yonatan Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1\">Yuval Elovici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey for Biomedical Text Summarization: From Pre-trained to Large Language Models. (arXiv:2304.08763v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.08763","description":"<p>The exponential growth of biomedical texts such as biomedical literature and\nelectronic health records (EHRs), poses a significant challenge for clinicians\nand researchers to access clinical information efficiently. To tackle this\nchallenge, biomedical text summarization (BTS) has been proposed as a solution\nto support clinical information retrieval and management. BTS aims at\ngenerating concise summaries that distill key information from single or\nmultiple biomedical documents. In recent years, the rapid advancement of\nfundamental natural language processing (NLP) techniques, from pre-trained\nlanguage models (PLMs) to large language models (LLMs), has greatly facilitated\nthe progress of BTS. This growth has led to numerous proposed summarization\nmethods, datasets, and evaluation metrics, raising the need for a comprehensive\nand up-to-date survey for BTS. In this paper, we present a systematic review of\nrecent advancements in BTS, leveraging cutting-edge NLP techniques from PLMs to\nLLMs, to help understand the latest progress, challenges, and future\ndirections. We begin by introducing the foundational concepts of BTS, PLMs and\nLLMs, followed by an in-depth review of available datasets, recent approaches,\nand evaluation metrics in BTS. We finally discuss existing challenges and\npromising future directions in the era of LLMs. To facilitate the research\ncommunity, we line up open resources including available datasets, recent\napproaches, codes, evaluation metrics, and the leaderboard in a public project:\nhttps://github.com/KenZLuo/Biomedical-Text-Summarization-Survey/tree/master. We\nbelieve that this survey will be a useful resource to researchers, allowing\nthem to quickly track recent advancements and provide guidelines for future BTS\nresearch within the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qianqian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zheheng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Benyou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1\">Sophia Ananiadou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs. (arXiv:2305.03642v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03642","description":"<p>Results from Randomized Controlled Trials (RCTs) establish the comparative\neffectiveness of interventions, and are in turn critical inputs for\nevidence-based care. However, results from RCTs are presented in (often\nunstructured) natural language articles describing the design, execution, and\noutcomes of trials; clinicians must manually extract findings pertaining to\ninterventions and outcomes of interest from such articles. This onerous manual\nprocess has motivated work on (semi-)automating extraction of structured\nevidence from trial reports. In this work we propose and evaluate a\ntext-to-text model built on instruction-tuned Large Language Models (LLMs) to\njointly extract Interventions, Outcomes, and Comparators (ICO elements) from\nclinical abstracts, and infer the associated results reported. Manual (expert)\nand automated evaluations indicate that framing evidence extraction as a\nconditional generation task and fine-tuning LLMs for this purpose realizes\nconsiderable ($\\sim$20 point absolute F1 score) gains over the previous SOTA.\nWe perform ablations and error analyses to assess aspects that contribute to\nmodel performance, and to highlight potential directions for further\nimprovements. We apply our model to a collection of published RCTs through\nmid-2022, and release a searchable database of structured findings:\nbit.ly/joint-relations-extraction-mlhc\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wadhwa_S/0/1/0/all/0/1\">Somin Wadhwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeYoung_J/0/1/0/all/0/1\">Jay DeYoung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nye_B/0/1/0/all/0/1\">Benjamin Nye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amir_S/0/1/0/all/0/1\">Silvio Amir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors. (arXiv:2305.14724v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.14724","description":"<p>Visual metaphors are powerful rhetorical devices used to persuade or\ncommunicate creative ideas through images. Similar to linguistic metaphors,\nthey convey meaning implicitly through symbolism and juxtaposition of the\nsymbols. We propose a new task of generating visual metaphors from linguistic\nmetaphors. This is a challenging task for diffusion-based text-to-image models,\nsuch as DALL$\\cdot$E 2, since it requires the ability to model implicit meaning\nand compositionality. We propose to solve the task through the collaboration\nbetween Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3\n(davinci-002) with Chain-of-Thought prompting generates text that represents a\nvisual elaboration of the linguistic metaphor containing the implicit meaning\nand relevant objects, which is then used as input to the diffusion-based\ntext-to-image models.Using a human-AI collaboration framework, where humans\ninteract both with the LLM and the top-performing diffusion model, we create a\nhigh-quality dataset containing 6,476 visual metaphors for 1,540 linguistic\nmetaphors and their associated visual elaborations. Evaluation by professional\nillustrators shows the promise of LLM-Diffusion Model collaboration for this\ntask . To evaluate the utility of our Human-AI collaboration framework and the\nquality of our dataset, we perform both an intrinsic human-based evaluation and\nan extrinsic evaluation using visual entailment as a downstream task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saakyan_A/0/1/0/all/0/1\">Arkadiy Saakyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winn_O/0/1/0/all/0/1\">Olivia Winn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panagopoulou_A/0/1/0/all/0/1\">Artemis Panagopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apidianaki_M/0/1/0/all/0/1\">Marianna Apidianaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNITE: A Unified Benchmark for Text-to-SQL Evaluation. (arXiv:2305.16265v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.16265","description":"<p>A practical text-to-SQL system should generalize well on a wide variety of\nnatural language questions, unseen database schemas, and novel SQL query\nstructures. To comprehensively evaluate text-to-SQL systems, we introduce a\nUNIfied benchmark for Text-to-SQL Evaluation (UNITE). It is composed of\npublicly available text-to-SQL datasets, containing natural language questions\nfrom more than 12 domains, SQL queries from more than 3.9K patterns, and 29K\ndatabases. Compared to the widely used Spider benchmark, we introduce\n$\\sim$120K additional examples and a threefold increase in SQL patterns, such\nas comparative and boolean questions. We conduct a systematic study of six\nstate-of-the-art (SOTA) text-to-SQL parsers on our new benchmark and show that:\n1) Codex performs surprisingly well on out-of-domain datasets; 2) specially\ndesigned decoding methods (e.g. constrained beam search) can improve\nperformance for both in-domain and out-of-domain settings; 3) explicitly\nmodeling the relationship between questions and schemas further improves the\nSeq2Seq models. More importantly, our benchmark presents key challenges towards\ncompositional generalization and robustness issues -- which these SOTA models\ncannot address well. Our code and data processing script are available at\nhttps://github.com/awslabs/unified-text2sql-benchmark\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_W/0/1/0/all/0/1\">Wuwei Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiguo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_A/0/1/0/all/0/1\">Anuj Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Henghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Alexander Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hang_C/0/1/0/all/0/1\">Chung-Wei Hang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lilien_J/0/1/0/all/0/1\">Joseph Lilien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yiqun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lin Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Mingwen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jiarong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ash_S/0/1/0/all/0/1\">Stephen Ash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castelli_V/0/1/0/all/0/1\">Vittorio Castelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_P/0/1/0/all/0/1\">Patrick Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Edit Distance based RL for RNNT decoding. (arXiv:2306.01789v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2306.01789","description":"<p>RNN-T is currently considered the industry standard in ASR due to its\nexceptional WERs in various benchmark tests and its ability to support seamless\nstreaming and longform transcription. However, its biggest drawback lies in the\nsignificant discrepancy between its training and inference objectives. During\ntraining, RNN-T maximizes all alignment probabilities by teacher forcing, while\nduring inference, it uses beam search which may not necessarily find the\nmaximum probable alignment. Additionally, RNN-T's inability to experience\nmistakes during teacher forcing training makes it more problematic when a\nmistake occurs in inference. To address this issue, this paper proposes a\nReinforcement Learning method that minimizes the gap between training and\ninference time. Our Edit Distance based RL (EDRL) approach computes rewards\nbased on the edit distance, and trains the network at every action level. The\nproposed approach yielded SoTA WERs on LibriSpeech for the 600M Conformer RNN-T\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1\">Dongseong Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_C/0/1/0/all/0/1\">Changwan Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Cross-temporal Summarization: Dataset, Models, Evaluation. (arXiv:2306.12916v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.12916","description":"<p>While summarization has been extensively researched in natural language\nprocessing (NLP), cross-lingual cross-temporal summarization (CLCTS) is a\nlargely unexplored area that has the potential to improve cross-cultural\naccessibility and understanding. This paper comprehensively addresses the CLCTS\ntask, including dataset creation, modeling, and evaluation. We build the first\nCLCTS corpus, leveraging historical fictive texts and Wikipedia summaries in\nEnglish and German, and examine the effectiveness of popular transformer\nend-to-end models with different intermediate finetuning tasks. Additionally,\nwe explore the potential of ChatGPT for CLCTS as a summarizer and an evaluator.\nOverall, we report evaluations from humans, ChatGPT, and several recent\nautomatic evaluation metrics where we find that our intermediate task finetuned\nend-to-end models generate bad to moderate quality summaries; ChatGPT as a\nsummarizer (without any finetuning) provides moderate to good quality outputs\nand as an evaluator correlates moderately with human evaluations but is prone\nto giving lower scores. ChatGPT also seems very adept at normalizing historical\ntext and outperforms context-unaware spelling normalization tools such as\nNorma. We finally test ChatGPT in a scenario with adversarially attacked and\nunseen source documents and find that ChatGPT profits from its prior knowledge\nto a certain degree, with better performances for omission and entity swap than\nnegation against its prior knowledge. This benefit inflates its assessed\nquality as ChatGPT performs slightly worse for unseen source documents compared\nto seen documents. We additionally introspect our models' performances to find\nthat longer, older and more complex source texts (all of which are more\ncharacteristic for historical language variants) are harder to summarize for\nall models, indicating the difficulty of the CLCTS task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouni_J/0/1/0/all/0/1\">Jihed Ouni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios. (arXiv:2306.13734v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2306.13734","description":"<p>The CHiME challenges have played a significant role in the development and\nevaluation of robust automatic speech recognition (ASR) systems. We introduce\nthe CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task\ncomprises joint ASR and diarization in far-field settings with multiple, and\npossibly heterogeneous, recording devices. Different from previous challenges,\nwe evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The\ngoal is for participants to devise a single system that can generalize across\ndifferent array geometries and use cases with no a-priori information. Another\ndeparture from earlier CHiME iterations is that participants are allowed to use\nopen-source pre-trained models and datasets. In this paper, we describe the\nchallenge design, motivation, and fundamental research questions in detail. We\nalso present the baseline system, which is fully array-topology agnostic and\nfeatures multi-channel diarization, channel selection, guided source separation\nand a robust ASR model that leverages self-supervised speech representations\n(SSLR).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cornell_S/0/1/0/all/0/1\">Samuele Cornell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wiesner_M/0/1/0/all/0/1\">Matthew Wiesner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raj_D/0/1/0/all/0/1\">Desh Raj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garcia_P/0/1/0/all/0/1\">Paola Garcia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maciejewski_M/0/1/0/all/0/1\">Matthew Maciejewski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Masuyama_Y/0/1/0/all/0/1\">Yoshiki Masuyama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhong-Qiu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Squartini_S/0/1/0/all/0/1\">Stefano Squartini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khudanpur_S/0/1/0/all/0/1\">Sanjeev Khudanpur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Language Speech Emotion Recognition Using Multimodal Dual Attention Transformers. (arXiv:2306.13804v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2306.13804","description":"<p>Despite the recent progress in speech emotion recognition (SER),\nstate-of-the-art systems are unable to achieve improved performance in\ncross-language settings. In this paper, we propose a Multimodal Dual Attention\nTransformer (MDAT) model to improve cross-language SER. Our model utilises\npre-trained models for multimodal feature extraction and is equipped with a\ndual attention mechanism including graph attention and co-attention to capture\ncomplex dependencies across different modalities and achieve improved\ncross-language SER results using minimal target language data. In addition, our\nmodel also exploits a transformer encoder layer for high-level feature\nrepresentation to improve emotion classification accuracy. In this way, MDAT\nperforms refinement of feature representation at various stages and provides\nemotional salient features to the classification layer. This novel approach\nalso ensures the preservation of modality-specific emotional information while\nenhancing cross-modality and cross-language interactions. We assess our model's\nperformance on four publicly available SER datasets and establish its superior\neffectiveness compared to recent approaches and baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaidi_S/0/1/0/all/0/1\">Syed Aun Muhammad Zaidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Latif_S/0/1/0/all/0/1\">Siddique Latif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qadir_J/0/1/0/all/0/1\">Junaid Qadir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration. (arXiv:2307.05300v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2307.05300","description":"<p>Human intelligence thrives on the concept of cognitive synergy, where\ncollaboration and information integration among different cognitive processes\nyield superior outcomes compared to individual cognitive processes in\nisolation. Although Large Language Models (LLMs) have demonstrated promising\nperformance as general task-solving agents, they still struggle with tasks that\nrequire intensive domain knowledge and complex reasoning. In this work, we\npropose Solo Performance Prompting (SPP), which transforms a single LLM into a\ncognitive synergist by engaging in multi-turn self-collaboration with multiple\npersonas. A cognitive synergist refers to an intelligent agent that\ncollaborates with multiple minds, combining their individual strengths and\nknowledge, to enhance problem-solving and overall performance in complex tasks.\nBy dynamically identifying and simulating different personas based on task\ninputs, SPP unleashes the potential of cognitive synergy in LLMs. We have\ndiscovered that assigning multiple, fine-grained personas in LLMs elicits\nbetter problem-solving abilities compared to using a single or fixed number of\npersonas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,\nCodenames Collaborative, and Logic Grid Puzzle, encompassing both\nknowledge-intensive and reasoning-intensive types. Unlike previous works, such\nas Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, SPP\neffectively elicits internal knowledge acquisition abilities, reduces\nhallucination, and maintains strong reasoning capabilities. Code, data, and\nprompts can be found at:\nhttps://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhailong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shaoguang Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenshan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stack More Layers Differently: High-Rank Training Through Low-Rank Updates. (arXiv:2307.05695v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2307.05695","description":"<p>Despite the dominance and effectiveness of scaling, resulting in large\nnetworks with hundreds of billions of parameters, the necessity to train\noverparametrized models remains poorly understood, and alternative approaches\ndo not necessarily make it cheaper to train high-performance models. In this\npaper, we explore low-rank training techniques as an alternative approach to\ntraining large neural networks. We introduce a novel method called ReLoRA,\nwhich utilizes low-rank updates to train high-rank networks. We apply ReLoRA to\npre-training transformer language models with up to 350M parameters and\ndemonstrate comparable performance to regular neural network training.\nFurthermore, we observe that the efficiency of ReLoRA increases with model\nsize, making it a promising approach for training multi-billion-parameter\nnetworks efficiently. Our findings shed light on the potential of low-rank\ntraining techniques and their implications for scaling laws.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lialin_V/0/1/0/all/0/1\">Vladislav Lialin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shivagunde_N/0/1/0/all/0/1\">Namrata Shivagunde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muckatira_S/0/1/0/all/0/1\">Sherin Muckatira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1\">Anna Rumshisky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-07-16T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}