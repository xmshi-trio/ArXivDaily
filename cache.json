{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-04-17T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study. (arXiv:2304.06762v1 [cs.CL])","link":"http://arxiv.org/abs/2304.06762","description":"<p>Large decoder-only language models (LMs) can be largely improved in terms of\nperplexity by retrieval (e.g., RETRO), but its impact on text generation\nquality and downstream task accuracy is unclear. Thus, it is still an open\nquestion: shall we pretrain large autoregressive LMs with retrieval? To answer\nit, we perform a comprehensive study on a scalable pre-trained\nretrieval-augmented LM (i.e., RETRO) compared with standard GPT and\nretrieval-augmented GPT incorporated at fine-tuning or inference stages. We\nfirst provide the recipe to reproduce RETRO up to 9.5B parameters while\nretrieving a text corpus with 330B tokens. Based on that, we have the following\nnovel findings: i) RETRO outperforms GPT on text generation with much less\ndegeneration (i.e., repetition), moderately higher factual accuracy, and\nslightly lower toxicity with a nontoxic retrieval database. ii) On the LM\nEvaluation Harness benchmark, RETRO largely outperforms GPT on\nknowledge-intensive tasks, but is on par with GPT on other tasks. Furthermore,\nwe introduce a simple variant of the model, RETRO++, which largely improves\nopen-domain QA results of original RETRO (e.g., EM score +8.6 on Natural\nQuestion) and significantly outperforms retrieval-augmented GPT across\ndifferent model sizes. Our findings highlight the promising direction of\npretraining autoregressive LMs with retrieval as future foundation models. We\nrelease our implementation at: https://github.com/NVIDIA/Megatron-LM#retro\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAfee_L/0/1/0/all/0/1\">Lawrence McAfee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuchaiev_O/0/1/0/all/0/1\">Oleksii Kuchaiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v1 [cs.LG])","link":"http://arxiv.org/abs/2304.06767","description":"<p>Generative foundation models are susceptible to implicit biases that can\narise from extensive unsupervised training data. Such biases can produce\nsuboptimal samples, skewed outcomes, and unfairness, with potentially\nsignificant repercussions. Consequently, aligning these models with human\nethics and preferences is an essential step toward ensuring their responsible\nand effective deployment in real-world applications. Prior research has\nprimarily employed Reinforcement Learning from Human Feedback (RLHF) as a means\nof addressing this problem, wherein generative models are fine-tuned using RL\nalgorithms guided by a human-feedback-informed reward model. However, the\ninefficiencies and instabilities associated with RL algorithms frequently\npresent substantial obstacles to the successful alignment of generative models,\nnecessitating the development of a more robust and streamlined approach. To\nthis end, we introduce a new framework, Reward rAnked FineTuning (RAFT),\ndesigned to align generative models more effectively. Utilizing a reward model\nand a sufficient number of samples, our approach selects the high-quality\nsamples, discarding those that exhibit undesired behavior, and subsequently\nassembles a streaming dataset. This dataset serves as the basis for aligning\nthe generative model and can be employed under both offline and online\nsettings. Notably, the sample generation process within RAFT is gradient-free,\nrendering it compatible with black-box generators. Through extensive\nexperiments, we demonstrate that our proposed algorithm exhibits strong\nperformance in the context of both large language models and diffusion models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hanze Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Wei Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_D/0/1/0/all/0/1\">Deepanshu Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_R/0/1/0/all/0/1\">Rui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diao_S/0/1/0/all/0/1\">Shizhe Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_K/0/1/0/all/0/1\">Kashun Shum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Sequence Transduction by Jointly Predicting Tokens and Durations. (arXiv:2304.06795v1 [eess.AS])","link":"http://arxiv.org/abs/2304.06795","description":"<p>This paper introduces a novel Token-and-Duration Transducer (TDT)\narchitecture for sequence-to-sequence tasks. TDT extends conventional\nRNN-Transducer architectures by jointly predicting both a token and its\nduration, i.e. the number of input frames covered by the emitted token. This is\nachieved by using a joint network with two outputs which are independently\nnormalized to generate distributions over tokens and durations. During\ninference, TDT models can skip input frames guided by the predicted duration\noutput, which makes them significantly faster than conventional Transducers\nwhich process the encoder output frame by frame. TDT models achieve both better\naccuracy and significantly faster inference than conventional Transducers on\ndifferent sequence transduction tasks. TDT models for Speech Recognition\nachieve better accuracy and up to 2.82X faster inference than RNN-Transducers.\nTDT models for Speech Translation achieve an absolute gain of over 1 BLEU on\nthe MUST-C test compared with conventional Transducers, and its inference is\n2.27X faster. In Speech Intent Classification and Slot Filling tasks, TDT\nmodels improve the intent accuracy up to over 1% (absolute) over conventional\nTransducers, while running up to 1.28X faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_H/0/1/0/all/0/1\">Hainan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_F/0/1/0/all/0/1\">Fei Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Majumdar_S/0/1/0/all/0/1\">Somshubra Majumdar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">He Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ginsburg_B/0/1/0/all/0/1\">Boris Ginsburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence. (arXiv:2304.06798v1 [cs.AI])","link":"http://arxiv.org/abs/2304.06798","description":"<p>Large pre-trained models, also known as foundation models (FMs), are trained\nin a task-agnostic manner on large-scale data and can be adapted to a wide\nrange of downstream tasks by fine-tuning, few-shot, or even zero-shot learning.\nDespite their successes in language and vision tasks, we have yet seen an\nattempt to develop foundation models for geospatial artificial intelligence\n(GeoAI). In this work, we explore the promises and challenges of developing\nmultimodal foundation models for GeoAI. We first investigate the potential of\nmany existing FMs by testing their performances on seven tasks across multiple\ngeospatial subdomains including Geospatial Semantics, Health Geography, Urban\nGeography, and Remote Sensing. Our results indicate that on several geospatial\ntasks that only involve text modality such as toponym recognition, location\ndescription recognition, and US state-level/county-level dementia time series\nforecasting, these task-agnostic LLMs can outperform task-specific\nfully-supervised models in a zero-shot or few-shot learning setting. However,\non other geospatial tasks, especially tasks that involve multiple data\nmodalities (e.g., POI-based urban function classification, street view\nimage-based urban noise intensity classification, and remote sensing image\nscene classification), existing foundation models still underperform\ntask-specific models. Based on these observations, we propose that one of the\nmajor challenges of developing a FM for GeoAI is to address the multimodality\nnature of geospatial tasks. After discussing the distinct challenges of each\ngeospatial data modality, we suggest the possibility of a multimodal foundation\nmodel which can reason over various types of geospatial data through geospatial\nalignments. We conclude this paper by discussing the unique risks and\nchallenges to develop such a model for GeoAI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mai_G/0/1/0/all/0/1\">Gengchen Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Weiming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Suhang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_D/0/1/0/all/0/1\">Deepak Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ninghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Song Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_G/0/1/0/all/0/1\">Gao Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yingjie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cundy_C/0/1/0/all/0/1\">Chris Cundy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Rui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lao_N/0/1/0/all/0/1\">Ni Lao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemEval-2023 Task 12: Sentiment Analysis for African Languages (AfriSenti-SemEval). (arXiv:2304.06845v1 [cs.CL])","link":"http://arxiv.org/abs/2304.06845","description":"<p>We present the first Africentric SemEval Shared task, Sentiment Analysis for\nAfrican Languages (AfriSenti-SemEval) - the dataset is available at\nhttps://github.com/afrisenti-semeval/afrisent-semeval-2023. AfriSenti-SemEval\nis a sentiment classification challenge in 14 African languages - Amharic,\nAlgerian Arabic, Hausa, Igbo, Kinyarwanda, Moroccan Arabic, Mozambican\nPortuguese, Nigerian Pidgin, Oromo, Swahili, Tigrinya, Twi, Xitsonga, and\nYor\\`ub\\'a (Muhammad et al., 2023), using a 3-class labeled data: positive,\nnegative, and neutral. We present three subtasks: (1) Task A: monolingual\nclassification, which received 44 submissions; (2) Task B: multilingual\nclassification, which received 32 submissions; and (3) Task C: zero-shot\nclassification, which received 34 submissions. The best system for tasks A and\nB was achieved by NLNDE team with 71.31 and 75.06 weighted F1, respectively.\nUCAS-IIE-NLP achieved the best system on average for task C with 58.15 weighted\nF1. We describe the various approaches adopted by the top 10 systems and their\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen Hassan Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdulmumin_I/0/1/0/all/0/1\">Idris Abdulmumin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yimam_S/0/1/0/all/0/1\">Seid Muhie Yimam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_I/0/1/0/all/0/1\">Ibrahim Sa&#x27;id Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ousidhoum_N/0/1/0/all/0/1\">Nedjma Ousidhoum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayele_A/0/1/0/all/0/1\">Abinew Ayele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beloucif_M/0/1/0/all/0/1\">Meriem Beloucif</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vax-Culture: A Dataset for Studying Vaccine Discourse on Twitter. (arXiv:2304.06858v1 [cs.SI])","link":"http://arxiv.org/abs/2304.06858","description":"<p>Vaccine hesitancy continues to be a main challenge for public health\nofficials during the COVID-19 pandemic. As this hesitancy undermines vaccine\ncampaigns, many researchers have sought to identify its root causes, finding\nthat the increasing volume of anti-vaccine misinformation on social media\nplatforms is a key element of this problem. We explored Twitter as a source of\nmisleading content with the goal of extracting overlapping cultural and\npolitical beliefs that motivate the spread of vaccine misinformation. To do\nthis, we have collected a data set of vaccine-related Tweets and annotated them\nwith the help of a team of annotators with a background in communications and\njournalism. Ultimately we hope this can lead to effective and targeted public\nhealth communication strategies for reaching individuals with anti-vaccine\nbeliefs. Moreover, this information helps with developing Machine Learning\nmodels to automatically detect vaccine misinformation posts and combat their\nnegative impacts. In this paper, we present Vax-Culture, a novel Twitter\nCOVID-19 dataset consisting of 6373 vaccine-related tweets accompanied by an\nextensive set of human-provided annotations including vaccine-hesitancy stance,\nindication of any misinformation in tweets, the entities criticized and\nsupported in each tweet and the communicated message of each tweet. Moreover,\nwe define five baseline tasks including four classification and one sequence\ngeneration tasks, and report the results of a set of recent transformer-based\nmodels for them. The dataset and code are publicly available at\nhttps://github.com/mrzarei5/Vax-Culture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zarei_M/0/1/0/all/0/1\">Mohammad Reza Zarei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christensen_M/0/1/0/all/0/1\">Michael Christensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Everts_S/0/1/0/all/0/1\">Sarah Everts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komeili_M/0/1/0/all/0/1\">Majid Komeili</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of Social Biases in Recent Large Pre-Trained Models. (arXiv:2304.06861v1 [cs.CL])","link":"http://arxiv.org/abs/2304.06861","description":"<p>Large pre-trained language models are widely used in the community. These\nmodels are usually trained on unmoderated and unfiltered data from open sources\nlike the Internet. Due to this, biases that we see in platforms online which\nare a reflection of those in society are in turn captured and learned by these\nmodels. These models are deployed in applications that affect millions of\npeople and their inherent biases are harmful to the targeted social groups. In\nthis work, we study the general trend in bias reduction as newer pre-trained\nmodels are released. Three recent models ( ELECTRA, DeBERTa, and DistilBERT)\nare chosen and evaluated against two bias benchmarks, StereoSet and\nCrowS-Pairs. They are compared to the baseline of BERT using the associated\nmetrics. We explore whether as advancements are made and newer, faster, lighter\nmodels are released: are they being developed responsibly such that their\ninherent social biases have been reduced compared to their older counterparts?\nThe results are compiled and we find that all the models under study do exhibit\nbiases but have generally improved as compared to BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Swapnil Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_N/0/1/0/all/0/1\">Nikita Anand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+V%2E_K/0/1/0/all/0/1\">Kranthi Kiran G.V.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Alind Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales. (arXiv:2304.06875v1 [cs.CL])","link":"http://arxiv.org/abs/2304.06875","description":"<p>As language models scale up, it becomes increasingly expensive to verify\nresearch ideas because conclusions on small models do not trivially transfer to\nlarge ones. A possible solution is to establish a generic system that directly\npredicts some metrics for large models solely based on the results and\nhyperparameters from small models. Existing methods based on scaling laws\nrequire hyperparameter search on the largest models, which is impractical with\nlimited resources. We address this issue by presenting our discoveries\nindicating that Maximal Update parametrization (muP) enables accurate fitting\nof scaling laws for hyperparameters close to common loss basins, without any\nsearch. Thus, different models can be directly compared on large scales with\nloss prediction even before the training starts. We propose a new paradigm as a\nfirst step towards reliable academic research for any model scale without heavy\ncomputation. Code will be publicly available shortly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yiqun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yequan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HCAM -- Hierarchical Cross Attention Model for Multi-modal Emotion Recognition. (arXiv:2304.06910v1 [eess.AS])","link":"http://arxiv.org/abs/2304.06910","description":"<p>Emotion recognition in conversations is challenging due to the multi-modal\nnature of the emotion expression. We propose a hierarchical cross-attention\nmodel (HCAM) approach to multi-modal emotion recognition using a combination of\nrecurrent and co-attention neural network models. The input to the model\nconsists of two modalities, i) audio data, processed through a learnable\nwav2vec approach and, ii) text data represented using a bidirectional encoder\nrepresentations from transformers (BERT) model. The audio and text\nrepresentations are processed using a set of bi-directional recurrent neural\nnetwork layers with self-attention that converts each utterance in a given\nconversation to a fixed dimensional embedding. In order to incorporate\ncontextual knowledge and the information across the two modalities, the audio\nand text embeddings are combined using a co-attention layer that attempts to\nweigh the utterance level embeddings relevant to the task of emotion\nrecognition. The neural network parameters in the audio layers, text layers as\nwell as the multi-modal co-attention layers, are hierarchically trained for the\nemotion classification task. We perform experiments on three established\ndatasets namely, IEMOCAP, MELD and CMU-MOSI, where we illustrate that the\nproposed model improves significantly over other benchmarks and helps achieve\nstate-of-art results on all these datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dutta_S/0/1/0/all/0/1\">Soumya Dutta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ganapathy_S/0/1/0/all/0/1\">Sriram Ganapathy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text. (arXiv:2304.06939v1 [cs.CV])","link":"http://arxiv.org/abs/2304.06939","description":"<p>In-context vision and language models like Flamingo support arbitrarily\ninterleaved sequences of images and text as input. This format not only enables\nfew-shot learning via interleaving independent supervised (image, text)\nexamples, but also, more complex prompts involving interaction between images,\ne.g., \"What do image A and image B have in common?\" To support this interface,\npretraining occurs over web corpora that similarly contain interleaved\nimages+text. To date, however, large-scale data of this form have not been\npublicly available.\n</p>\n<p>We release Multimodal C4 (mmc4), an augmentation of the popular text-only c4\ncorpus with images interleaved. We use a linear assignment algorithm to place\nimages into longer bodies of text using CLIP features, a process that we show\noutperforms alternatives. mmc4 spans everyday topics like cooking, travel,\ntechnology, etc. A manual inspection of a random sample of documents shows that\na vast majority (90%) of images are topically relevant, and that linear\nassignment frequently selects individual sentences specifically well-aligned\nwith each image (78%). After filtering NSFW images, ads, etc., the corpus\ncontains 103M documents containing 585M images interleaved with 43B English\ntokens.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanrong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadalla_A/0/1/0/all/0/1\">Anas Awadalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1\">Samir Yitzhak Gadre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodge_J/0/1/0/all/0/1\">Jesse Dodge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_A/0/1/0/all/0/1\">Alex Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt Engineering and Calibration for Zero-Shot Commonsense Reasoning. (arXiv:2304.06962v1 [cs.CL])","link":"http://arxiv.org/abs/2304.06962","description":"<p>Prompt engineering and calibration make large language models excel at\nreasoning tasks, including multiple choice commonsense reasoning. From a\npractical perspective, we investigate and evaluate these strategies on smaller\nlanguage models. Through experiments on five commonsense reasoning benchmarks,\nwe find that each strategy favors certain models, but their joint effects are\nmostly negative.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chenkai Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge. (arXiv:2304.06975v1 [cs.CL])","link":"http://arxiv.org/abs/2304.06975","description":"<p>Large Language Models (LLMs), such as the LLaMA model, have demonstrated\ntheir effectiveness in various general-domain natural language processing (NLP)\ntasks. Nevertheless, LLMs have not yet performed optimally in biomedical domain\ntasks due to the need for medical expertise in the responses. In response to\nthis challenge, we propose HuaTuo, a LLaMA-based model that has been\nsupervised-fine-tuned with generated QA (Question-Answer) instances. The\nexperimental results demonstrate that HuaTuo generates responses that possess\nmore reliable medical knowledge. Our proposed HuaTuo model is accessible at\nhttps://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haochun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_N/0/1/0/all/0/1\">Nuwa Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiang_Z/0/1/0/all/0/1\">Zewen Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sendong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimpLex: a lexical text simplification architecture. (arXiv:2304.07002v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07002","description":"<p>Text simplification (TS) is the process of generating easy-to-understand\nsentences from a given sentence or piece of text. The aim of TS is to reduce\nboth the lexical (which refers to vocabulary complexity and meaning) and\nsyntactic (which refers to the sentence structure) complexity of a given text\nor sentence without the loss of meaning or nuance. In this paper, we present\n\\textsc{SimpLex}, a novel simplification architecture for generating simplified\nEnglish sentences. To generate a simplified sentence, the proposed architecture\nuses either word embeddings (i.e., Word2Vec) and perplexity, or sentence\ntransformers (i.e., BERT, RoBERTa, and GPT2) and cosine similarity. The\nsolution is incorporated into a user-friendly and simple-to-use software. We\nevaluate our system using two metrics, i.e., SARI, and Perplexity Decrease.\nExperimentally, we observe that the transformer models outperform the other\nmodels in terms of the SARI score. However, in terms of Perplexity, the\nWord-Embeddings-based models achieve the biggest decrease. Thus, the main\ncontributions of this paper are: (1) We propose a new Word Embedding and\nTransformer based algorithm for text simplification; (2) We design\n\\textsc{SimpLex} -- a modular novel text simplification system -- that can\nprovide a baseline for further research; and (3) We perform an in-depth\nanalysis of our solution and compare our results with two state-of-the-art\nmodels, i.e., LightLS [19] and NTS-w2v [44]. We also make the code publicly\navailable online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Truica_C/0/1/0/all/0/1\">Ciprian-Octavian Truic&#x103;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stan_A/0/1/0/all/0/1\">Andrei-Ionut Stan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apostol_E/0/1/0/all/0/1\">Elena-Simona Apostol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dialogue Games for Benchmarking Language Understanding: Motivation, Taxonomy, Strategy. (arXiv:2304.07007v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07007","description":"<p>How does one measure \"ability to understand language\"? If it is a person's\nability that is being measured, this is a question that almost never poses\nitself in an unqualified manner: Whatever formal test is applied, it takes\nplace on the background of the person's language use in daily social practice,\nand what is measured is a specialised variety of language understanding (e.g.,\nof a second language; or of written, technical language). Computer programs do\nnot have this background. What does that mean for the applicability of formal\ntests of language understanding? I argue that such tests need to be\ncomplemented with tests of language use embedded in a practice, to arrive at a\nmore comprehensive evaluation of \"artificial language understanding\". To do\nsuch tests systematically, I propose to use \"Dialogue Games\" -- constructed\nactivities that provide a situational embedding for language use. I describe a\ntaxonomy of Dialogue Game types, linked to a model of underlying capabilites\nthat are tested, and thereby giving an argument for the \\emph{construct\nvalidity} of the test. I close with showing how the internal structure of the\ntaxonomy suggests an ordering from more specialised to more general situational\nlanguage understanding, which potentially can provide some strategic guidance\nfor development in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlangen_D/0/1/0/all/0/1\">David Schlangen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label Dependencies-aware Set Prediction Networks for Multi-label Text Classification. (arXiv:2304.07022v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07022","description":"<p>Multi-label text classification aims to extract all the related labels from a\nsentence, which can be viewed as a sequence generation problem. However, the\nlabels in training dataset are unordered. We propose to treat it as a direct\nset prediction problem and don't need to consider the order of labels. Besides,\nin order to model the correlation between labels, the adjacency matrix is\nconstructed through the statistical relations between labels and GCN is\nemployed to learn the label information. Based on the learned label\ninformation, the set prediction networks can both utilize the sentence\ninformation and label information for multi-label text classification\nsimultaneously. Furthermore, the Bhattacharyya distance is imposed on the\noutput probability distributions of the set prediction networks to increase the\nrecall ability. Experimental results on four multi-label datasets show the\neffectiveness of the proposed method and it outperforms previous method a\nsubstantial margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quanjie_H/0/1/0/all/0/1\">Han Quanjie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xinkai_D/0/1/0/all/0/1\">Du Xinkai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yalin_S/0/1/0/all/0/1\">Sun Yalin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1\">Lv Chao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SEA: A Scalable Entity Alignment System. (arXiv:2304.07065v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07065","description":"<p>Entity alignment (EA) aims to find equivalent entities in different knowledge\ngraphs (KGs). State-of-the-art EA approaches generally use Graph Neural\nNetworks (GNNs) to encode entities. However, most of them train the models and\nevaluate the results in a fullbatch fashion, which prohibits EA from being\nscalable on largescale datasets. To enhance the usability of GNN-based EA\nmodels in real-world applications, we present SEA, a scalable entity alignment\nsystem that enables to (i) train large-scale GNNs for EA, (ii) speed up the\nnormalization and the evaluation process, and (iii) report clear results for\nusers to estimate different models and parameter settings. SEA can be run on a\ncomputer with merely one graphic card. Moreover, SEA encompasses six\nstate-of-the-art EA models and provides access for users to quickly establish\nand evaluate their own models. Thus, SEA allows users to perform EA without\nbeing involved in tedious implementations, such as negative sampling and\nGPU-accelerated evaluation. With SEA, users can gain a clear view of the model\nperformance. In the demonstration, we show that SEA is user-friendly and is of\nhigh scalability even on computers with limited computational resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunjun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Ziheng Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-oriented Document-Grounded Dialog Systems by HLTPR@RWTH for DSTC9 and DSTC10. (arXiv:2304.07101v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07101","description":"<p>This paper summarizes our contributions to the document-grounded dialog tasks\nat the 9th and 10th Dialog System Technology Challenges (DSTC9 and DSTC10). In\nboth iterations the task consists of three subtasks: first detect whether the\ncurrent turn is knowledge seeking, second select a relevant knowledge document,\nand third generate a response grounded on the selected document. For DSTC9 we\nproposed different approaches to make the selection task more efficient. The\nbest method, Hierarchical Selection, actually improves the results compared to\nthe original baseline and gives a speedup of 24x. In the DSTC10 iteration of\nthe task, the challenge was to adapt systems trained on written dialogs to\nperform well on noisy automatic speech recognition transcripts. Therefore, we\nproposed data augmentation techniques to increase the robustness of the models\nas well as methods to adapt the style of generated responses to fit well into\nthe proceeding dialog. Additionally, we proposed a noisy channel model that\nallows for increasing the factuality of the generated responses. In addition to\nsummarizing our previous contributions, in this work, we also report on a few\nsmall improvements and reconsider the automatic evaluation metrics for the\ngeneration task which have shown a low correlation to human judgments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thulke_D/0/1/0/all/0/1\">David Thulke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daheim_N/0/1/0/all/0/1\">Nico Daheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dugast_C/0/1/0/all/0/1\">Christian Dugast</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keeping the Questions Conversational: Using Structured Representations to Resolve Dependency in Conversational Question Answering. (arXiv:2304.07125v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07125","description":"<p>Having an intelligent dialogue agent that can engage in conversational\nquestion answering (ConvQA) is now no longer limited to Sci-Fi movies only and\nhas, in fact, turned into a reality. These intelligent agents are required to\nunderstand and correctly interpret the sequential turns provided as the context\nof the given question. However, these sequential questions are sometimes left\nimplicit and thus require the resolution of some natural language phenomena\nsuch as anaphora and ellipsis. The task of question rewriting has the potential\nto address the challenges of resolving dependencies amongst the contextual\nturns by transforming them into intent-explicit questions. Nonetheless, the\nsolution of rewriting the implicit questions comes with some potential\nchallenges such as resulting in verbose questions and taking conversational\naspect out of the scenario by generating self-contained questions. In this\npaper, we propose a novel framework, CONVSR (CONVQA using Structured\nRepresentations) for capturing and generating intermediate representations as\nconversational cues to enhance the capability of the QA model to better\ninterpret the incomplete questions. We also deliberate how the strengths of\nthis task could be leveraged in a bid to design more engaging and eloquent\nconversational agents. We test our model on the QuAC and CANARD datasets and\nillustrate by experimental results that our proposed framework achieves a\nbetter F1 score than the standard question rewriting model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaib_M/0/1/0/all/0/1\">Munazza Zaib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1\">Quan Z. Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Emma Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_A/0/1/0/all/0/1\">Adnan Mahmood</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OPI at SemEval 2023 Task 1: Image-Text Embeddings and Multimodal Information Retrieval for Visual Word Sense Disambiguation. (arXiv:2304.07127v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07127","description":"<p>The goal of visual word sense disambiguation is to find the image that best\nmatches the provided description of the word's meaning. It is a challenging\nproblem, requiring approaches that combine language and image understanding. In\nthis paper, we present our submission to SemEval 2023 visual word sense\ndisambiguation shared task. The proposed system integrates multimodal\nembeddings, learning to rank methods, and knowledge-based approaches. We build\na classifier based on the CLIP model, whose results are enriched with\nadditional information retrieved from Wikipedia and lexical databases. Our\nsolution was ranked third in the multilingual task and won in the Persian\ntrack, one of the three language subtasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dadas_S/0/1/0/all/0/1\">S&#x142;awomir Dadas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OPI at SemEval 2023 Task 9: A Simple But Effective Approach to Multilingual Tweet Intimacy Analysis. (arXiv:2304.07130v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07130","description":"<p>This paper describes our submission to the SemEval 2023 multilingual tweet\nintimacy analysis shared task. The goal of the task was to assess the level of\nintimacy of Twitter posts in ten languages. The proposed approach consists of\nseveral steps. First, we perform in-domain pre-training to create a language\nmodel adapted to Twitter data. In the next step, we train an ensemble of\nregression models to expand the training set with pseudo-labeled examples. The\nextended dataset is used to train the final solution. Our method was ranked\nfirst in five out of ten language subtasks, obtaining the highest average score\nacross all languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dadas_S/0/1/0/all/0/1\">S&#x142;awomir Dadas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Just Tell Me: Prompt Engineering in Business Process Management. (arXiv:2304.07183v1 [cs.AI])","link":"http://arxiv.org/abs/2304.07183","description":"<p>GPT-3 and several other language models (LMs) can effectively address various\nnatural language processing (NLP) tasks, including machine translation and text\nsummarization. Recently, they have also been successfully employed in the\nbusiness process management (BPM) domain, e.g., for predictive process\nmonitoring and process extraction from text. This, however, typically requires\nfine-tuning the employed LM, which, among others, necessitates large amounts of\nsuitable training data. A possible solution to this problem is the use of\nprompt engineering, which leverages pre-trained LMs without fine-tuning them.\nRecognizing this, we argue that prompt engineering can help bring the\ncapabilities of LMs to BPM research. We use this position paper to develop a\nresearch agenda for the use of prompt engineering for BPM research by\nidentifying the associated potentials and challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Busch_K/0/1/0/all/0/1\">Kiran Busch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rochlitzer_A/0/1/0/all/0/1\">Alexander Rochlitzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sola_D/0/1/0/all/0/1\">Diana Sola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leopold_H/0/1/0/all/0/1\">Henrik Leopold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimal inference of a generalised Potts model by single-layer transformers with factored attention. (arXiv:2304.07235v1 [cond-mat.dis-nn])","link":"http://arxiv.org/abs/2304.07235","description":"<p>Transformers are the type of neural networks that has revolutionised natural\nlanguage processing and protein science. Their key building block is a\nmechanism called self-attention which is trained to predict missing words in\nsentences. Despite the practical success of transformers in applications it\nremains unclear what self-attention learns from data, and how. Here, we give a\nprecise analytical and numerical characterisation of transformers trained on\ndata drawn from a generalised Potts model with interactions between sites and\nPotts colours. While an off-the-shelf transformer requires several layers to\nlearn this distribution, we show analytically that a single layer of\nself-attention with a small modification can learn the Potts model exactly in\nthe limit of infinite sampling. We show that this modified self-attention, that\nwe call ``factored'', has the same functional form as the conditional\nprobability of a Potts spin given the other spins, compute its generalisation\nerror using the replica method from statistical physics, and derive an exact\nmapping to pseudo-likelihood methods for solving the inverse Ising and Potts\nproblem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cond-mat/1/au:+Rende_R/0/1/0/all/0/1\">Riccardo Rende</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Gerace_F/0/1/0/all/0/1\">Federica Gerace</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Laio_A/0/1/0/all/0/1\">Alessandro Laio</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Goldt_S/0/1/0/all/0/1\">Sebastian Goldt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Covidia: COVID-19 Interdisciplinary Academic Knowledge Graph. (arXiv:2304.07242v1 [cs.IR])","link":"http://arxiv.org/abs/2304.07242","description":"<p>The pandemic of COVID-19 has inspired extensive works across different\nresearch fields. Existing literature and knowledge platforms on COVID-19 only\nfocus on collecting papers on biology and medicine, neglecting the\ninterdisciplinary efforts, which hurdles knowledge sharing and research\ncollaborations between fields to address the problem. Studying\ninterdisciplinary researches requires effective paper category classification\nand efficient cross-domain knowledge extraction and integration. In this work,\nwe propose Covidia, COVID-19 interdisciplinary academic knowledge graph to\nbridge the gap between knowledge of COVID-19 on different domains. We design\nframeworks based on contrastive learning for disciplinary classification, and\npropose a new academic knowledge graph scheme for entity extraction, relation\nclassification and ontology management in accordance with interdisciplinary\nresearches. Based on Covidia, we also establish knowledge discovery benchmarks\nfor finding COVID-19 research communities and predicting potential links.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Cheng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jiaxin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1\">Luoyi Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinbing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chenghu Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn What Is Possible, Then Choose What Is Best: Disentangling One-To-Many Relations in Language Through Text-based Games. (arXiv:2304.07258v1 [cs.CL])","link":"http://arxiv.org/abs/2304.07258","description":"<p>Language models pre-trained on large self-supervised corpora, followed by\ntask-specific fine-tuning has become the dominant paradigm in NLP. These\npre-training datasets often have a one-to-many structure--e.g. in dialogue\nthere are many valid responses for a given context. However, only some of these\nresponses will be desirable in our downstream task. This raises the question of\nhow we should train the model such that it can emulate the desirable\nbehaviours, but not the undesirable ones. Current approaches train in a\none-to-one setup--only a single target response is given for a single dialogue\ncontext--leading to models only learning to predict the average response, while\nignoring the full range of possible responses. Using text-based games as a\ntestbed, our approach, PASA, uses discrete latent variables to capture the\nrange of different behaviours represented in our larger pre-training dataset.\nWe then use knowledge distillation to distil the posterior probability\ndistribution into a student model. This probability distribution is far richer\nthan learning from only the hard targets of the dataset, and thus allows the\nstudent model to benefit from the richer range of actions the teacher model has\nlearned. Results show up to 49% empirical improvement over the previous\nstate-of-the-art model on the Jericho Walkthroughs dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Towle_B/0/1/0/all/0/1\">Benjamin Towle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Ke Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniCausal: Unified Benchmark and Repository for Causal Text Mining. (arXiv:2208.09163v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.09163","description":"<p>Current causal text mining datasets vary in objectives, data coverage, and\nannotation schemes. These inconsistent efforts prevent modeling capabilities\nand fair comparisons of model performance. Furthermore, few datasets include\ncause-effect span annotations, which are needed for end-to-end causal relation\nextraction. To address these issues, we propose UniCausal, a unified benchmark\nfor causal text mining across three tasks: (I) Causal Sequence Classification,\n(II) Cause-Effect Span Detection and (III) Causal Pair Classification. We\nconsolidated and aligned annotations of six high quality, mainly\nhuman-annotated, corpora, resulting in a total of 58,720, 12,144 and 69,165\nexamples for each task respectively. Since the definition of causality can be\nsubjective, our framework was designed to allow researchers to work on some or\nall datasets and tasks. To create an initial benchmark, we fine-tuned BERT\npre-trained language models to each task, achieving 70.10% Binary F1, 52.42%\nMacro F1, and 84.68% Binary F1 scores respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1\">Fiona Anting Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xinyu Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1\">See-Kiong Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DABERT: Dual Attention Enhanced BERT for Semantic Matching. (arXiv:2210.03454v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03454","description":"<p>Transformer-based pre-trained language models such as BERT have achieved\nremarkable results in Semantic Sentence Matching. However, existing models\nstill suffer from insufficient ability to capture subtle differences. Minor\nnoise like word addition, deletion, and modification of sentences may cause\nflipped predictions. To alleviate this problem, we propose a novel Dual\nAttention Enhanced BERT (DABERT) to enhance the ability of BERT to capture\nfine-grained differences in sentence pairs. DABERT comprises (1) Dual Attention\nmodule, which measures soft word matches by introducing a new dual channel\nalignment mechanism to model affinity and difference attention. (2) Adaptive\nFusion module, this module uses attention to learn the aggregation of\ndifference and affinity features, and generates a vector describing the\nmatching details of sentence pairs. We conduct extensive experiments on\nwell-studied semantic matching and robustness test datasets, and the\nexperimental results show the effectiveness of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Di Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jian Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyLEx: Explaining Style Using Human Lexical Annotations. (arXiv:2210.07469v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.07469","description":"<p>Large pre-trained language models have achieved impressive results on various\nstyle classification tasks, but they often learn spurious domain-specific words\nto make predictions (Hayati et al., 2021). While human explanation highlights\nstylistic tokens as important features for this task, we observe that model\nexplanations often do not align with them. To tackle this issue, we introduce\nStyLEx, a model that learns from human-annotated explanations of stylistic\nfeatures and jointly learns to perform the task and predict these features as\nmodel explanations. Our experiments show that StyLEx can provide human-like\nstylistic lexical explanations without sacrificing the performance of\nsentence-level style prediction on both in-domain and out-of-domain datasets.\nExplanations from StyLEx show significant improvements in explanation metrics\n(sufficiency, plausibility) and when evaluated with human annotations. They are\nalso more understandable by human judges compared to the widely-used\nsaliency-based explanation baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hayati_S/0/1/0/all/0/1\">Shirley Anugrah Hayati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kyumin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajagopal_D/0/1/0/all/0/1\">Dheeraj Rajagopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1\">Lyle Ungar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion. (arXiv:2210.08471v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.08471","description":"<p>Transformer-based pre-trained models like BERT have achieved great progress\non Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also\nshown general benefits in multiple NLP tasks. However, how to efficiently\nintegrate dependency prior structure into pre-trained models to better model\ncomplex semantic matching relations is still unsettled. In this paper, we\npropose the \\textbf{D}ependency-Enhanced \\textbf{A}daptive \\textbf{F}usion\n\\textbf{A}ttention (\\textbf{DAFA}), which explicitly introduces dependency\nstructure into pre-trained models and adaptively fuses it with semantic\ninformation. Specifically, \\textbf{\\emph{(i)}} DAFA first proposes a\nstructure-sensitive paradigm to construct a dependency matrix for calibrating\nattention weights. It adopts an adaptive fusion module to integrate the\nobtained dependency information and the original semantic signals. Moreover,\nDAFA reconstructs the attention calculation flow and provides better\ninterpretability. By applying it on BERT, our method achieves state-of-the-art\nor competitive performance on 10 public datasets, demonstrating the benefits of\nadaptively fusing dependency structure in semantic matching task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jian Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Di Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rumei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1\">Minlong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yongxin Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Entity Detection with Proposer and Regressor. (arXiv:2210.10260v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.10260","description":"<p>Named entity recognition is a traditional task in natural language\nprocessing. In particular, nested entity recognition receives extensive\nattention for the widespread existence of the nesting scenario. The latest\nresearch migrates the well-established paradigm of set prediction in object\ndetection to cope with entity nesting. However, the manual creation of query\nvectors, which fail to adapt to the rich semantic information in the context,\nlimits these approaches. An end-to-end entity detection approach with proposer\nand regressor is presented in this paper to tackle the issues. First, the\nproposer utilizes the feature pyramid network to generate high-quality entity\nproposals. Then, the regressor refines the proposals for generating the final\nprediction. The model adopts encoder-only architecture and thus obtains the\nadvantages of the richness of query semantics, high precision of entity\nlocalization, and easiness of model training. Moreover, we introduce the novel\nspatially modulated attention and progressive refinement for further\nimprovement. Extensive experiments demonstrate that our model achieves advanced\nperformance in flat and nested NER, achieving a new state-of-the-art F1 score\nof 80.74 on the GENIA dataset and 72.38 on the WeiboNER dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xueru Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Changjiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haotian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Luguang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Hong Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Make More of Your Data: Minimal Effort Data Augmentation for Automatic Speech Recognition and Translation. (arXiv:2210.15398v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.15398","description":"<p>Data augmentation is a technique to generate new training data based on\nexisting data. We evaluate the simple and cost-effective method of\nconcatenating the original data examples to build new training instances.\nContinued training with such augmented data is able to improve off-the-shelf\nTransformer and Conformer models that were optimized on the original data only.\nWe demonstrate considerable improvements on the LibriSpeech-960h test sets (WER\n2.83 and 6.87 for test-clean and test-other), which carry over to models\ncombined with shallow fusion (WER 2.55 and 6.27). Our method of continued\ntraining also leads to improvements of up to 0.9 WER on the ASR part of\nCoVoST-2 for four non English languages, and we observe that the gains are\nhighly dependent on the size of the original training data. We compare\ndifferent concatenation strategies and found that our method does not need\nspeaker information to achieve its improvements. Finally, we demonstrate on two\ndatasets that our methods also works for speech translation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1\">Tsz Kin Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schamoni_S/0/1/0/all/0/1\">Shigehiko Schamoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1\">Stefan Riezler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AUC Maximization for Low-Resource Named Entity Recognition. (arXiv:2212.04800v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.04800","description":"<p>Current work in named entity recognition (NER) uses either cross entropy (CE)\nor conditional random fields (CRF) as the objective/loss functions to optimize\nthe underlying NER model. Both of these traditional objective functions for the\nNER problem generally produce adequate performance when the data distribution\nis balanced and there are sufficient annotated training examples. But since NER\nis inherently an imbalanced tagging problem, the model performance under the\nlow-resource settings could suffer using these standard objective functions.\nBased on recent advances in area under the ROC curve (AUC) maximization, we\npropose to optimize the NER model by maximizing the AUC score. We give evidence\nthat by simply combining two binary-classifiers that maximize the AUC score,\nsignificant performance improvement over traditional loss functions is achieved\nunder low-resource NER settings. We also conduct extensive experiments to\ndemonstrate the advantages of our method under the low-resource and\nhighly-imbalanced data distribution settings. To the best of our knowledge,\nthis is the first work that brings AUC maximization to the NER setting.\nFurthermore, we show that our method is agnostic to different types of NER\nembeddings, models and domains. The code to replicate this work will be\nprovided upon request.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngoc Dang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1\">Wray Buntine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beare_R/0/1/0/all/0/1\">Richard Beare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lan Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Natural Language Processing to Augment Structured Social Determinants of Health Data in the Electronic Health Record. (arXiv:2212.07538v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.07538","description":"<p>Objective: Social determinants of health (SDOH) impact health outcomes and\nare documented in the electronic health record (EHR) through structured data\nand unstructured clinical notes. However, clinical notes often contain more\ncomprehensive SDOH information, detailing aspects such as status, severity, and\ntemporality. This work has two primary objectives: i) develop a natural\nlanguage processing (NLP) information extraction model to capture detailed SDOH\ninformation and ii) evaluate the information gain achieved by applying the SDOH\nextractor to clinical narratives and combining the extracted representations\nwith existing structured data.\n</p>\n<p>Materials and Methods: We developed a novel SDOH extractor using a deep\nlearning entity and relation extraction architecture to characterize SDOH\nacross various dimensions. In an EHR case study, we applied the SDOH extractor\nto a large clinical data set with 225,089 patients and 430,406 notes with\nsocial history sections and compared the extracted SDOH information with\nexisting structured data.\n</p>\n<p>Results: The SDOH extractor achieved 0.86 F1 on a withheld test set. In the\nEHR case study, we found extracted SDOH information complements existing\nstructured data with 32% of homeless patients, 19% of current tobacco users,\nand 10% of drug users only having these health risk factors documented in the\nclinical narrative.\n</p>\n<p>Conclusions: Utilizing EHR data to identify SDOH health risk factors and\nsocial needs may improve patient care and outcomes. Semantic representations of\ntext-encoded SDOH information can augment existing structured data, and this\nmore comprehensive SDOH representation can assist health systems in identifying\nand addressing these social needs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lybarger_K/0/1/0/all/0/1\">Kevin Lybarger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobbins_N/0/1/0/all/0/1\">Nicholas J Dobbins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_R/0/1/0/all/0/1\">Ritche Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Angad Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wedgeworth_P/0/1/0/all/0/1\">Patrick Wedgeworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozuner_O/0/1/0/all/0/1\">Ozlem Ozuner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yetisgen_M/0/1/0/all/0/1\">Meliha Yetisgen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Active Learning Methods to Strategically Select Essays for Automated Scoring. (arXiv:2301.00628v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.00628","description":"<p>Research on automated essay scoring has become increasing important because\nit serves as a method for evaluating students' written-responses at scale.\nScalable methods for scoring written responses are needed as students migrate\nto online learning environments resulting in the need to evaluate large numbers\nof written-response assessments. The purpose of this study is to describe and\nevaluate three active learning methods than can be used to minimize the number\nof essays that must be scored by human raters while still providing the data\nneeded to train a modern automated essay scoring system. The three active\nlearning methods are the uncertainty-based, the topological-based, and the\nhybrid method. These three methods were used to select essays included as part\nof the Automated Student Assessment Prize competition that were then classified\nusing a scoring model that was training with the bidirectional encoder\nrepresentations from transformer language model. All three active learning\nmethods produced strong results, with the topological-based method producing\nthe most efficient classification. Growth rate accuracy was also evaluated. The\nactive learning methods produced different levels of efficiency under different\nsample size allocations but, overall, all three methods were highly efficient\nand produced classifications that were similar to one another.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Firoozi_T/0/1/0/all/0/1\">Tahereh Firoozi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_H/0/1/0/all/0/1\">Hamid Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gierl_M/0/1/0/all/0/1\">Mark J. Gierl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata. (arXiv:2301.04647v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2301.04647","description":"<p>We learn a visual representation that captures information about the camera\nthat recorded a given photo. To do this, we train a multimodal embedding\nbetween image patches and the EXIF metadata that cameras automatically insert\ninto image files. Our model represents this metadata by simply converting it to\ntext and then processing it with a transformer. The features that we learn\nsignificantly outperform other self-supervised and supervised features on\ndownstream image forensics and calibration tasks. In particular, we\nsuccessfully localize spliced image regions \"zero shot\" by clustering the\nvisual embeddings for all of the patches within an image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chenhao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Ayush Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owens_A/0/1/0/all/0/1\">Andrew Owens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alloprof: a new French question-answer education dataset and its use in an information retrieval case study. (arXiv:2302.07738v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.07738","description":"<p>Teachers and students are increasingly relying on online learning resources\nto supplement the ones provided in school. This increase in the breadth and\ndepth of available resources is a great thing for students, but only provided\nthey are able to find answers to their queries. Question-answering and\ninformation retrieval systems have benefited from public datasets to train and\nevaluate their algorithms, but most of these datasets have been in English text\nwritten by and for adults. We introduce a new public French question-answering\ndataset collected from Alloprof, a Quebec-based primary and high-school help\nwebsite, containing 29 349 questions and their explanations in a variety of\nschool subjects from 10 368 students, with more than half of the explanations\ncontaining links to other questions or some of the 2 596 reference pages on the\nwebsite. We also present a case study of this dataset in an information\nretrieval task. This dataset was collected on the Alloprof public forum, with\nall questions verified for their appropriateness and the explanations verified\nboth for their appropriateness and their relevance to the question. To predict\nrelevant documents, architectures using pre-trained BERT models were fine-tuned\nand evaluated. This dataset will allow researchers to develop\nquestion-answering, information retrieval and other algorithms specifically for\nthe French speaking education context. Furthermore, the range of language\nproficiency, images, mathematical symbols and spelling mistakes will\nnecessitate algorithms based on a multimodal comprehension. The case study we\npresent as a baseline shows an approach that relies on recent techniques\nprovides an acceptable performance level, but more work is necessary before it\ncan reliably be used and trusted in a production setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lefebvre_Brossard_A/0/1/0/all/0/1\">Antoine Lefebvre-Brossard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gazaille_S/0/1/0/all/0/1\">Stephane Gazaille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desmarais_M/0/1/0/all/0/1\">Michel C. Desmarais</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inseq: An Interpretability Toolkit for Sequence Generation Models. (arXiv:2302.13942v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2302.13942","description":"<p>Past work in natural language processing interpretability focused mainly on\npopular classification tasks while largely overlooking generation settings,\npartly due to a lack of dedicated tools. In this work, we introduce Inseq, a\nPython library to democratize access to interpretability analyses of sequence\ngeneration models. Inseq enables intuitive and optimized extraction of models'\ninternal information and feature importance scores for popular decoder-only and\nencoder-decoder Transformers architectures. We showcase its potential by\nadopting it to highlight gender biases in machine translation models and locate\nfactual knowledge inside GPT-2. Thanks to its extensible interface supporting\ncutting-edge techniques such as contrastive feature attribution, Inseq can\ndrive future advances in explainable natural language generation, centralizing\ngood practices and enabling fair and reproducible model evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarti_G/0/1/0/all/0/1\">Gabriele Sarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldhus_N/0/1/0/all/0/1\">Nils Feldhus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sickert_L/0/1/0/all/0/1\">Ludwig Sickert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wal_O/0/1/0/all/0/1\">Oskar van der Wal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nissim_M/0/1/0/all/0/1\">Malvina Nissim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisazza_A/0/1/0/all/0/1\">Arianna Bisazza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoLT5: Faster Long-Range Transformers with Conditional Computation. (arXiv:2303.09752v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.09752","description":"<p>Many natural language processing tasks benefit from long inputs, but\nprocessing long documents with Transformers is expensive -- not only due to\nquadratic attention complexity but also from applying feedforward and\nprojection layers to every token. However, not all tokens are equally\nimportant, especially for longer documents. We propose CoLT5, a long-input\nTransformer model that builds on this intuition by employing conditional\ncomputation, devoting more resources to important tokens in both feedforward\nand attention layers. We show that CoLT5 achieves stronger performance than\nLongT5 with much faster training and inference, achieving SOTA on the\nlong-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably\nmake use of extremely long inputs, showing strong gains up to 64k input length.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1\">Tao Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jong_M/0/1/0/all/0/1\">Michiel de Jong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Onta&#xf1;&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahma_S/0/1/0/all/0/1\">Siddhartha Brahma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemlyanskiy_Y/0/1/0/all/0/1\">Yury Zemlyanskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uthus_D/0/1/0/all/0/1\">David Uthus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mandy Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Thorp_J/0/1/0/all/0/1\">James Lee-Thorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yun-Hsuan Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghai_S/0/1/0/all/0/1\">Sumit Sanghai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning. (arXiv:2303.10475v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.10475","description":"<p>Task semantics can be expressed by a set of input-to-output examples or a\npiece of textual instruction. Conventional machine learning approaches for\nnatural language processing (NLP) mainly rely on the availability of\nlarge-scale sets of task-specific examples. Two issues arise: first, collecting\ntask-specific labeled examples does not apply to scenarios where tasks may be\ntoo complicated or costly to annotate, or the system is required to handle a\nnew task immediately; second, this is not user-friendly since end-users are\nprobably more willing to provide task description rather than a set of examples\nbefore using the system. Therefore, the community is paying increasing interest\nin a new supervision-seeking paradigm for NLP: learning from task instructions.\nDespite its impressive progress, there are some common issues that the\ncommunity struggles with. This survey paper tries to summarize the current\nresearch on instruction learning, particularly, by answering the following\nquestions: (i) what is task instruction, and what instruction types exist? (ii)\nhow to model instructions? (iii) what factors influence and explain the\ninstructions' performance? (iv) what challenges remain in instruction learning?\nTo our knowledge, this is the first comprehensive survey about textual\ninstructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_R/0/1/0/all/0/1\">Renze Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenpeng Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparks of Artificial General Intelligence: Early experiments with GPT-4. (arXiv:2303.12712v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.12712","description":"<p>Artificial intelligence (AI) researchers have been developing and refining\nlarge language models (LLMs) that exhibit remarkable capabilities across a\nvariety of domains and tasks, challenging our understanding of learning and\ncognition. The latest model developed by OpenAI, GPT-4, was trained using an\nunprecedented scale of compute and data. In this paper, we report on our\ninvestigation of an early version of GPT-4, when it was still in active\ndevelopment by OpenAI. We contend that (this early version of) GPT-4 is part of\na new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that\nexhibit more general intelligence than previous AI models. We discuss the\nrising capabilities and implications of these models. We demonstrate that,\nbeyond its mastery of language, GPT-4 can solve novel and difficult tasks that\nspan mathematics, coding, vision, medicine, law, psychology and more, without\nneeding any special prompting. Moreover, in all of these tasks, GPT-4's\nperformance is strikingly close to human-level performance, and often vastly\nsurpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's\ncapabilities, we believe that it could reasonably be viewed as an early (yet\nstill incomplete) version of an artificial general intelligence (AGI) system.\nIn our exploration of GPT-4, we put special emphasis on discovering its\nlimitations, and we discuss the challenges ahead for advancing towards deeper\nand more comprehensive versions of AGI, including the possible need for\npursuing a new paradigm that moves beyond next-word prediction. We conclude\nwith reflections on societal influences of the recent technological leap and\nfuture research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1\">S&#xe9;bastien Bubeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_V/0/1/0/all/0/1\">Varun Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eldan_R/0/1/0/all/0/1\">Ronen Eldan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrke_J/0/1/0/all/0/1\">Johannes Gehrke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamar_E/0/1/0/all/0/1\">Ece Kamar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1\">Peter Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yin Tat Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundberg_S/0/1/0/all/0/1\">Scott Lundberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nori_H/0/1/0/all/0/1\">Harsha Nori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1\">Hamid Palangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1\">Marco Tulio Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebQAmGaze: A Multilingual Webcam Eye-Tracking-While-Reading Dataset. (arXiv:2303.17876v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.17876","description":"<p>We create WebQAmGaze, a multilingual low-cost eye-tracking-while-reading\ndataset, designed to support the development of fair and transparent NLP\nmodels. WebQAmGaze includes webcam eye-tracking data from 332 participants\nnaturally reading English, Spanish, and German texts. Each participant performs\ntwo reading tasks composed of five texts, a normal reading and an\ninformation-seeking task. After preprocessing the data, we find that fixations\non relevant spans seem to indicate correctness when answering the comprehension\nquestions. Additionally, we perform a comparative analysis of the data\ncollected to high-quality eye-tracking data. The results show a moderate\ncorrelation between the features obtained with the webcam-ET compared to those\nof a commercial ET device. We believe this data can advance webcam-based\nreading studies and open a way to cheaper and more accessible data collection.\nWebQAmGaze is useful to learn about the cognitive processes behind question\nanswering (QA) and to apply these insights to computational models of language\nunderstanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_T/0/1/0/all/0/1\">Tiago Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandl_S/0/1/0/all/0/1\">Stephanie Brandl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollenstein_N/0/1/0/all/0/1\">Nora Hollenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PEACH: Pre-Training Sequence-to-Sequence Multilingual Models for Translation with Semi-Supervised Pseudo-Parallel Document Generation. (arXiv:2304.01282v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.01282","description":"<p>Multilingual pre-training significantly improves many multilingual NLP tasks,\nincluding machine translation. Most existing methods are based on some variants\nof masked language modeling and text-denoising objectives on monolingual data.\nMultilingual pre-training on monolingual data ignores the availability of\nparallel data in many language pairs. Also, some other works integrate the\navailable human-generated parallel translation data in their pre-training. This\nkind of parallel data is definitely helpful, but it is limited even in\nhigh-resource language pairs. This paper introduces a novel semi-supervised\nmethod, SPDG, that generates high-quality pseudo-parallel data for multilingual\npre-training. First, a denoising model is pre-trained on monolingual data to\nreorder, add, remove, and substitute words, enhancing the pre-training\ndocuments' quality. Then, we generate different pseudo-translations for each\npre-training document using dictionaries for word-by-word translation and\napplying the pre-trained denoising model. The resulting pseudo-parallel data is\nthen used to pre-train our multilingual sequence-to-sequence model, PEACH. Our\nexperiments show that PEACH outperforms existing approaches used in training\nmT5 and mBART on various translation tasks, including supervised, zero- and\nfew-shot scenarios. Moreover, PEACH's ability to transfer knowledge between\nsimilar languages makes it particularly useful for low-resource languages. Our\nresults demonstrate that with high-quality dictionaries for generating accurate\npseudo-parallel, PEACH can be valuable for low-resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salemi_A/0/1/0/all/0/1\">Alireza Salemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abaskohi_A/0/1/0/all/0/1\">Amirhossein Abaskohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavakoli_S/0/1/0/all/0/1\">Sara Tavakoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaghoobzadeh_Y/0/1/0/all/0/1\">Yadollah Yaghoobzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakery_A/0/1/0/all/0/1\">Azadeh Shakery</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geotechnical Parrot Tales (GPT): Harnessing Large Language Models in geotechnical engineering. (arXiv:2304.02138v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.02138","description":"<p>The widespread adoption of large language models (LLMs), such as OpenAI's\nChatGPT, could revolutionize various industries, including geotechnical\nengineering. However, GPT models can sometimes generate plausible-sounding but\nfalse outputs, leading to hallucinations. In this article, we discuss the\nimportance of prompt engineering in mitigating these risks and harnessing the\nfull potential of GPT for geotechnical applications. We explore the challenges\nand pitfalls associated with LLMs and highlight the role of context in ensuring\naccurate and valuable responses. Furthermore, we examine the development of\ncontext-specific search engines and the potential of LLMs to become a natural\ninterface for complex tasks, such as data analysis and design. We also develop\na unified interface using natural language to handle complex geotechnical\nengineering tasks and data analysis. By integrating GPT into geotechnical\nengineering workflows, professionals can streamline their work and develop\nsustainable and resilient infrastructure systems for the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_K/0/1/0/all/0/1\">Krishna Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforcement Learning Tutor Better Supported Lower Performers in a Math Task. (arXiv:2304.04933v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2304.04933","description":"<p>Resource limitations make it hard to provide all students with one of the\nmost effective educational interventions: personalized instruction.\nReinforcement learning could be a key tool to reduce the development cost and\nimprove the effectiveness of intelligent tutoring software that aims to provide\nthe right support, at the right time, to a student. Here we illustrate that\ndeep reinforcement learning can be used to provide adaptive pedagogical support\nto students learning about the concept of volume in a narrative storyline\nsoftware. Using explainable artificial intelligence tools, we extracted\ninterpretable insights about the pedagogical policy learned and demonstrated\nthat the resulting policy had similar performance in a different student\npopulation. Most importantly, in both studies, the reinforcement-learning\nnarrative system had the largest benefit for those students with the lowest\ninitial pretest scores, suggesting the opportunity for AI to adapt and provide\nsupport for those most in need.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_S/0/1/0/all/0/1\">Sherry Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_A/0/1/0/all/0/1\">Allen Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steenbergen_W/0/1/0/all/0/1\">William Steenbergen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiayu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">JQ Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Meng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kyle Dang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Catherine Y Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1\">Rui Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landay_J/0/1/0/all/0/1\">James A Landay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1\">Emma Brunskill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sign Language Translation from Instructional Videos. (arXiv:2304.06371v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.06371","description":"<p>The advances in automatic sign language translation (SLT) to spoken languages\nhave been mostly benchmarked with datasets of limited size and restricted\ndomains. Our work advances the state of the art by providing the first baseline\nresults on How2Sign, a large and broad dataset.\n</p>\n<p>We train a Transformer over I3D video features, using the reduced BLEU as a\nreference metric for validation, instead of the widely used BLEU score. We\nreport a result of 8.03 on the BLEU score, and publish the first open-source\nimplementation of its kind to promote further advances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tarres_L/0/1/0/all/0/1\">Laia Tarr&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Gerard I. G&#xe1;llego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duarte_A/0/1/0/all/0/1\">Amanda Duarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torres_J/0/1/0/all/0/1\">Jordi Torres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1\">Xavier Gir&#xf3;-i-Nieto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PDF-VQA: A New Dataset for Real-World VQA on PDF Documents. (arXiv:2304.06447v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2304.06447","description":"<p>Document-based Visual Question Answering examines the document understanding\nof document images in conditions of natural language questions. We proposed a\nnew document-based VQA dataset, PDF-VQA, to comprehensively examine the\ndocument understanding from various aspects, including document element\nrecognition, document layout structural understanding as well as contextual\nunderstanding and key information extraction. Our PDF-VQA dataset extends the\ncurrent scale of document understanding that limits on the single document page\nto the new scale that asks questions over the full document of multiple pages.\nWe also propose a new graph-based VQA model that explicitly integrates the\nspatial and hierarchically structural relationships between different document\nelements to boost the document structural understanding. The performances are\ncompared with several baselines over different question types and\ntasks\\footnote{The full dataset will be released after paper acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yihao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Siwen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyunsuk Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the State of the Art in Legal QA Systems. (arXiv:2304.06623v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.06623","description":"<p>Answering questions related to the legal domain is a complex task, primarily\ndue to the intricate nature and diverse range of legal document systems.\nProviding an accurate answer to a legal query typically necessitates\nspecialized knowledge in the relevant domain, which makes this task all the\nmore challenging, even for human experts. Question answering (QA) systems are\ndesigned to generate answers to questions asked in human languages. QA uses\nnatural language processing to understand questions and search through\ninformation to find relevant answers. QA has various practical applications,\nincluding customer service, education, research, and cross-lingual\ncommunication. However, QA faces challenges such as improving natural language\nunderstanding and handling complex and ambiguous questions. Answering questions\nrelated to the legal domain is a complex task, primarily due to the intricate\nnature and diverse range of legal document systems. Providing an accurate\nanswer to a legal query typically necessitates specialized knowledge in the\nrelevant domain, which makes this task all the more challenging, even for human\nexperts. At this time, there is a lack of surveys that discuss legal question\nanswering. To address this problem, we provide a comprehensive survey that\nreviews 14 benchmark datasets for question-answering in the legal field as well\nas presents a comprehensive review of the state-of-the-art Legal Question\nAnswering deep learning models. We cover the different architectures and\ntechniques used in these studies and the performance and limitations of these\nmodels. Moreover, we have established a public GitHub repository where we\nregularly upload the most recent articles, open data, and source code. The\nrepository is available at:\n\\url{https://github.com/abdoelsayed2016/Legal-Question-Answering-Review}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdallah_A/0/1/0/all/0/1\">Abdelrahman Abdallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piryani_B/0/1/0/all/0/1\">Bhawna Piryani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1\">Adam Jatowt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"G2T: A Simple but Effective Framework for Topic Modeling based on Pretrained Language Model and Community Detection. (arXiv:2304.06653v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.06653","description":"<p>It has been reported that clustering-based topic models, which cluster\nhigh-quality sentence embeddings with an appropriate word selection method, can\ngenerate better topics than generative probabilistic topic models. However,\nthese approaches suffer from the inability to select appropriate parameters and\nincomplete models that overlook the quantitative relation between words with\ntopics and topics with text. To solve these issues, we propose graph to topic\n(G2T), a simple but effective framework for topic modelling. The framework is\ncomposed of four modules. First, document representation is acquired using\npretrained language models. Second, a semantic graph is constructed according\nto the similarity between document representations. Third, communities in\ndocument semantic graphs are identified, and the relationship between topics\nand documents is quantified accordingly. Fourth, the word--topic distribution\nis computed based on a variant of TFIDF. Automatic evaluation suggests that G2T\nachieved state-of-the-art performance on both English and Chinese documents\nwith different lengths. Human judgements demonstrate that G2T can produce\ntopics with better interpretability and coverage than baselines. In addition,\nG2T can not only determine the topic number automatically but also give the\nprobabilistic distribution of words in topics and topics in documents. Finally,\nG2T is publicly available, and the distillation experiments provide instruction\non how it works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Leihang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiapeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qiang Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation. (arXiv:2304.06671v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2304.06671","description":"<p>Spatial control is a core capability in controllable image generation.\nAdvancements in layout-guided image generation have shown promising results on\nin-distribution (ID) datasets with similar spatial configurations. However, it\nis unclear how these models perform when facing out-of-distribution (OOD)\nsamples with arbitrary, unseen layouts. In this paper, we propose LayoutBench,\na diagnostic benchmark for layout-guided image generation that examines four\ncategories of spatial control skills: number, position, size, and shape. We\nbenchmark two recent representative layout-guided image generation methods and\nobserve that the good ID layout control may not generalize well to arbitrary\nlayouts in the wild (e.g., objects at the boundary). Next, we propose\nIterInpaint, a new baseline that generates foreground and background regions in\na step-by-step manner via inpainting, demonstrating stronger generalizability\nthan existing models on OOD layouts in LayoutBench. We perform quantitative and\nqualitative evaluation and fine-grained analysis on the four LayoutBench skills\nto pinpoint the weaknesses of existing models. Lastly, we show comprehensive\nablation studies on IterInpaint, including training task ratio, crop&amp;paste vs.\nrepaint, and generation order. Project website: https://layoutbench.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-04-16T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}