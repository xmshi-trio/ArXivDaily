{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-11-15T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"FinTech for Social Good: A Research Agenda from NLP Perspective. (arXiv:2211.06431v1 [cs.CY])","link":"http://arxiv.org/abs/2211.06431","description":"<p>Making our research results positively impact on society and environment is\none of the goals our community has been pursuing recently. Although financial\ntechnology (FinTech) is one of the popular application fields, we notice that\nthere is no discussion on how NLP can help in FinTech for the social good. When\nmentioning FinTech for social good, people are talking about financial\ninclusion and green finance. However, the role of NLP in these directions only\ngets limited discussions. To fill this gap, this paper shares our idea of how\nwe can use NLP in FinTech for social good. We hope readers can rethink the\nrelationship between finance and NLP based on our sharing, and further join us\nin improving the financial literacy of individual investors and improving the\nsupports for impact investment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chung-Chi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takamura_H/0/1/0/all/0/1\">Hiroya Takamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hsin-Hsi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Platform and Cross-Domain Abusive Language Detection with Supervised Contrastive Learning. (arXiv:2211.06452v1 [cs.CL])","link":"http://arxiv.org/abs/2211.06452","description":"<p>The prevalence of abusive language on different online platforms has been a\nmajor concern that raises the need for automated cross-platform abusive\nlanguage detection. However, prior works focus on concatenating data from\nmultiple platforms, inherently adopting Empirical Risk Minimization (ERM)\nmethod. In this work, we address this challenge from the perspective of domain\ngeneralization objective. We design SCL-Fish, a supervised contrastive learning\nintegrated meta-learning algorithm to detect abusive language on unseen\nplatforms. Our experimental analysis shows that SCL-Fish achieves better\nperformance over ERM and the existing state-of-the-art models. We also show\nthat SCL-Fish is data-efficient and achieves comparable performance with the\nlarge-scale pre-trained models upon finetuning for the abusive language\ndetection task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khondaker_M/0/1/0/all/0/1\">Md Tawkat Islam Khondaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshmanan_L/0/1/0/all/0/1\">Laks V.S. Lakshmanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech-to-Speech Translation For A Real-world Unwritten Language. (arXiv:2211.06474v1 [cs.CL])","link":"http://arxiv.org/abs/2211.06474","description":"<p>We study speech-to-speech translation (S2ST) that translates speech from one\nlanguage into another language and focuses on building systems to support\nlanguages without standard text writing systems. We use English-Taiwanese\nHokkien as a case study, and present an end-to-end solution from training data\ncollection, modeling choices to benchmark dataset release. First, we present\nefforts on creating human annotated data, automatically mining data from large\nunlabeled speech datasets, and adopting pseudo-labeling to produce weakly\nsupervised data. On the modeling, we take advantage of recent advances in\napplying self-supervised discrete representations as target for prediction in\nS2ST and show the effectiveness of leveraging additional text supervision from\nMandarin, a language similar to Hokkien, in model training. Finally, we release\nan S2ST benchmark set to facilitate future research in this field. The demo can\nbe found at https://huggingface.co/spaces/facebook/Hokkien_Translation .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng-Jen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Kevin Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yilin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jingfei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_J/0/1/0/all/0/1\">Justine Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1\">Yu-An Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomasello_P/0/1/0/all/0/1\">Paden Tomasello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duquenne_P/0/1/0/all/0/1\">Paul-Ambroise Duquenne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwenk_H/0/1/0/all/0/1\">Holger Schwenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popuri_S/0/1/0/all/0/1\">Sravya Popuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A unified one-shot prosody and speaker conversion system with self-supervised discrete speech units. (arXiv:2211.06535v1 [eess.AS])","link":"http://arxiv.org/abs/2211.06535","description":"<p>We present a unified system to realize one-shot voice conversion (VC) on the\npitch, rhythm, and speaker attributes. Existing works generally ignore the\ncorrelation between prosody and language content, leading to the degradation of\nnaturalness in converted speech. Additionally, the lack of proper language\nfeatures prevents these systems from accurately preserving language content\nafter conversion. To address these issues, we devise a cascaded modular system\nleveraging self-supervised discrete speech units as language representation.\nThese discrete units provide duration information essential for rhythm\nmodeling. Our system first extracts utterance-level prosody and speaker\nrepresentations from the raw waveform. Given the prosody representation, a\nprosody predictor estimates pitch, energy, and duration for each discrete unit\nin the utterance. A synthesizer further reconstructs speech based on the\npredicted prosody, speaker representation, and discrete units. Experiments show\nthat our system outperforms previous approaches in naturalness,\nintelligibility, speaker transferability, and prosody transferability. Code and\nsamples are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Li-Wei Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rudnicky_A/0/1/0/all/0/1\">Alexander Rudnicky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collecting Interactive Multi-modal Datasets for Grounded Language Understanding. (arXiv:2211.06552v1 [cs.CL])","link":"http://arxiv.org/abs/2211.06552","description":"<p>Human intelligence can remarkably adapt quickly to new tasks and\nenvironments. Starting from a very young age, humans acquire new skills and\nlearn how to solve new tasks either by imitating the behavior of others or by\nfollowing provided natural language instructions. To facilitate research which\ncan enable similar capabilities in machines, we made the following\ncontributions (1) formalized the collaborative embodied agent using natural\nlanguage task; (2) developed a tool for extensive and scalable data collection;\nand (3) collected the first dataset for interactive grounded language\nunderstanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohanty_S/0/1/0/all/0/1\">Shrestha Mohanty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arabzadeh_N/0/1/0/all/0/1\">Negar Arabzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teruel_M/0/1/0/all/0/1\">Milagro Teruel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zholus_A/0/1/0/all/0/1\">Artem Zholus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skrynnik_A/0/1/0/all/0/1\">Alexey Skrynnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burtsev_M/0/1/0/all/0/1\">Mikhail Burtsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinet_K/0/1/0/all/0/1\">Kavya Srinet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panov_A/0/1/0/all/0/1\">Aleksandr Panov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cote_M/0/1/0/all/0/1\">Marc-Alexandre C&#xf4;t&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+kiseleva_J/0/1/0/all/0/1\">Julia kiseleva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lifelong and Continual Learning Dialogue Systems. (arXiv:2211.06553v1 [cs.CL])","link":"http://arxiv.org/abs/2211.06553","description":"<p>Dialogue systems, commonly known as chatbots, have gained escalating\npopularity in recent times due to their wide-spread applications in carrying\nout chit-chat conversations with users and task-oriented dialogues to\naccomplish various user tasks. Existing chatbots are usually trained from\npre-collected and manually-labeled data and/or written with handcrafted rules.\nMany also use manually-compiled knowledge bases (KBs). Their ability to\nunderstand natural language is still limited, and they tend to produce many\nerrors resulting in poor user satisfaction. Typically, they need to be\nconstantly improved by engineers with more labeled data and more manually\ncompiled knowledge. This book introduces the new paradigm of lifelong learning\ndialogue systems to endow chatbots the ability to learn continually by\nthemselves through their own self-initiated interactions with their users and\nworking environments to improve themselves. As the systems chat more and more\nwith users or learn more and more from external sources, they become more and\nmore knowledgeable and better and better at conversing. The book presents the\nlatest developments and techniques for building such continual learning\ndialogue systems that continuously learn new language expressions and lexical\nand factual knowledge during conversation from users and off conversation from\nexternal sources, acquire new training examples during conversation, and learn\nconversational skills. Apart from these general topics, existing works on\ncontinual learning of some specific aspects of dialogue systems are also\nsurveyed. The book concludes with a discussion of open challenges for future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mazumder_S/0/1/0/all/0/1\">Sahisnu Mazumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Robustness of DistilHuBERT to Unseen Noisy Conditions via Data Augmentation, Curriculum Learning, and Multi-Task Enhancement. (arXiv:2211.06562v1 [cs.SD])","link":"http://arxiv.org/abs/2211.06562","description":"<p>Self-supervised speech representation learning aims to extract meaningful\nfactors from the speech signal that can later be used across different\ndownstream tasks, such as speech and/or emotion recognition. Existing models,\nsuch as HuBERT, however, can be fairly large thus may not be suitable for edge\nspeech applications. Moreover, realistic applications typically involve speech\ncorrupted by noise and room reverberation, hence models need to provide\nrepresentations that are robust to such environmental factors. In this study,\nwe build on the so-called DistilHuBERT model, which distils HuBERT to a\nfraction of its original size, with three modifications, namely: (i) augment\nthe training data with noise and reverberation, while the student model needs\nto distill the clean representations from the teacher model; (ii) introduce a\ncurriculum learning approach where increasing levels of noise are introduced as\nthe model trains, thus helping with convergence and with the creation of more\nrobust representations; and (iii) introduce a multi-task learning approach\nwhere the model also reconstructs the clean waveform jointly with the\ndistillation task, thus also acting as an enhancement step to ensure additional\nenvironment robustness to the representation. Experiments on three SUPERB tasks\nshow the advantages of the proposed method not only relative to the original\nDistilHuBERT, but also to the original HuBERT, thus showing the advantages of\nthe proposed method for ``in the wild'' edge speech applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guimaraes_H/0/1/0/all/0/1\">Heitor R. Guimar&#xe3;es</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_A/0/1/0/all/0/1\">Arthur Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avila_A/0/1/0/all/0/1\">Anderson R. Avila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falk_T/0/1/0/all/0/1\">Tiago H. Falk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Textual Adversaries with Minimal Perturbation. (arXiv:2211.06571v1 [cs.CL])","link":"http://arxiv.org/abs/2211.06571","description":"<p>Many word-level adversarial attack approaches for textual data have been\nproposed in recent studies. However, due to the massive search space consisting\nof combinations of candidate words, the existing approaches face the problem of\npreserving the semantics of texts when crafting adversarial counterparts. In\nthis paper, we develop a novel attack strategy to find adversarial texts with\nhigh similarity to the original texts while introducing minimal perturbation.\nThe rationale is that we expect the adversarial texts with small perturbation\ncan better preserve the semantic meaning of original texts. Experiments show\nthat, compared with state-of-the-art attack approaches, our approach achieves\nhigher success rates and lower perturbation rates in four benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xingyi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Depeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Shuhan Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Multimodal Sentiment Analysis based on Multimodal Probabilistic Fusion Prompts. (arXiv:2211.06607v1 [cs.CL])","link":"http://arxiv.org/abs/2211.06607","description":"<p>Multimodal sentiment analysis is a trending topic with the explosion of\nmultimodal content on the web. Present studies in multimodal sentiment analysis\nrely on large-scale supervised data. Collating supervised data is\ntime-consuming and labor-intensive. As such, it is essential to investigate the\nproblem of few-shot multimodal sentiment analysis. Previous works in few-shot\nmodels generally use language model prompts, which can improve performance in\nlow-resource settings. However, the textual prompt ignores the information from\nother modalities. We propose Multimodal Probabilistic Fusion Prompts, which can\nprovide diverse cues for multimodal sentiment detection. We first design a\nunified multimodal prompt to reduce the discrepancy in different modal prompts.\nTo improve the robustness of our model, we then leverage multiple diverse\nprompts for each input and propose a probabilistic method to fuse the output\npredictions. Extensive experiments conducted on three datasets confirm the\neffectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaocui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1\">Pengfei Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConceptX: A Framework for Latent Concept Analysis. (arXiv:2211.06642v1 [cs.CL])","link":"http://arxiv.org/abs/2211.06642","description":"<p>The opacity of deep neural networks remains a challenge in deploying\nsolutions where explanation is as important as precision. We present ConceptX,\na human-in-the-loop framework for interpreting and annotating latent\nrepresentational space in pre-trained Language Models (pLMs). We use an\nunsupervised method to discover concepts learned in these models and enable a\ngraphical interface for humans to generate explanations for the concepts. To\nfacilitate the process, we provide auto-annotations of the concepts (based on\ntraditional linguistic ontologies). Such annotations enable development of a\nlinguistic resource that directly represents latent concepts learned within\ndeep NLP models. These include not just traditional linguistic concepts, but\nalso task-specific or sensitive concepts (words grouped based on gender or\nreligious connotation) that helps the annotators to mark bias in the model. The\nframework consists of two parts (i) concept discovery and (ii) annotation\nplatform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalvi_F/0/1/0/all/0/1\">Fahim Dalvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrani_N/0/1/0/all/0/1\">Nadir Durrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sajjad_H/0/1/0/all/0/1\">Hassan Sajjad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Abdul Rafae Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jia Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLPeer: A Unified Resource for the Computational Study of Peer Review. (arXiv:2211.06651v1 [cs.CL])","link":"http://arxiv.org/abs/2211.06651","description":"<p>Peer review is a core component of scholarly publishing, yet it is\ntime-consuming, requires considerable expertise, and is prone to error. The\napplications of NLP for peer reviewing assistance aim to mitigate those issues,\nbut the lack of clearly licensed datasets and multi-domain corpora prevent the\nsystematic study of NLP for peer review. To remedy this, we introduce NLPeer --\nthe first ethically sourced multidomain corpus of more than 5k papers and 11k\nreview reports from five different venues. In addition to the new datasets of\npaper drafts, camera-ready versions and peer reviews from the NLP community, we\nestablish a unified data representation, and augment previous peer review\ndatasets to include parsed, structured paper representations, rich metadata and\nversioning information. Our work paves the path towards systematic,\nmulti-faceted, evidence-based study of peer review in NLP and beyond. We make\nNLPeer publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dycke_N/0/1/0/all/0/1\">Nils Dycke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_I/0/1/0/all/0/1\">Ilia Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Addressing Segmentation Ambiguity in Neural Linguistic Steganography. (arXiv:2211.06662v1 [cs.CL])","link":"http://arxiv.org/abs/2211.06662","description":"<p>Previous studies on neural linguistic steganography, except Ueoka et al.\n(2021), overlook the fact that the sender must detokenize cover texts to avoid\narousing the eavesdropper's suspicion. In this paper, we demonstrate that\nsegmentation ambiguity indeed causes occasional decoding failures at the\nreceiver's side. With the near-ubiquity of subwords, this problem now affects\nany language. We propose simple tricks to overcome this problem, which are even\napplicable to languages without explicit word boundaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nozaki_J/0/1/0/all/0/1\">Jumon Nozaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murawaki_Y/0/1/0/all/0/1\">Yugo Murawaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities. (arXiv:2211.06679v1 [cs.CL])","link":"http://arxiv.org/abs/2211.06679","description":"<p>In this work, we present a conceptually simple and effective method to train\na strong bilingual multimodal representation model. Starting from the\npretrained multimodal representation model CLIP released by OpenAI, we switched\nits text encoder with a pretrained multilingual text encoder XLM-R, and aligned\nboth languages and image representations by a two-stage training schema\nconsisting of teacher learning and contrastive learning. We validate our method\nthrough evaluations of a wide range of tasks. We set new state-of-the-art\nperformances on a bunch of tasks including ImageNet-CN, Flicker30k- CN, and\nCOCO-CN. Further, we obtain very close performances with CLIP on almost all\ntasks, suggesting that one can simply alter the text encoder in CLIP for\nextended capabilities such as multilingual understanding. Our models and code\nare available at https://github.com/FlagAI-Open/FlagAI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhongzhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo-Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fulong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qinghong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Ledell Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-Scale Bidirectional Training for Zero-Shot Image Captioning. (arXiv:2211.06774v1 [cs.CV])","link":"http://arxiv.org/abs/2211.06774","description":"<p>When trained on large-scale datasets, image captioning models can understand\nthe content of images from a general domain but often fail to generate\naccurate, detailed captions. To improve performance, pretraining-and-finetuning\nhas been a key strategy for image captioning. However, we find that large-scale\nbidirectional training between image and text enables zero-shot image\ncaptioning. In this paper, we introduce Bidirectional Image Text Training in\nlargER Scale, BITTERS, an efficient training and inference framework for\nzero-shot image captioning. We also propose a new evaluation benchmark which\ncomprises of high quality datasets and an extensive set of metrics to properly\nevaluate zero-shot captioning accuracy and societal bias. We additionally\nprovide an efficient finetuning approach for keyword extraction. We show that\ncareful selection of large-scale training set and model architecture is the key\nto achieving zero-shot image captioning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marsden_M/0/1/0/all/0/1\">Mark Marsden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_P/0/1/0/all/0/1\">Pyunghwan Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sihaeng Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sala_A/0/1/0/all/0/1\">Alessandra Sala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seung Hwan Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Textual Data Augmentation for Patient Outcomes Prediction. (arXiv:2211.06778v1 [cs.CL])","link":"http://arxiv.org/abs/2211.06778","description":"<p>Deep learning models have demonstrated superior performance in various\nhealthcare applications. However, the major limitation of these deep models is\nusually the lack of high-quality training data due to the private and sensitive\nnature of this field. In this study, we propose a novel textual data\naugmentation method to generate artificial clinical notes in patients'\nElectronic Health Records (EHRs) that can be used as additional training data\nfor patient outcomes prediction. Essentially, we fine-tune the generative\nlanguage model GPT-2 to synthesize labeled text with the original training\ndata. More specifically, We propose a teacher-student framework where we first\npre-train a teacher model on the original data, and then train a student model\non the GPT-augmented data under the guidance of the teacher. We evaluate our\nmethod on the most common patient outcome, i.e., the 30-day readmission rate.\nThe experimental results show that deep models can improve their predictive\nperformance with the augmented data, indicating the effectiveness of the\nproposed architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qiuhao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thien Huu Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FPT: Improving Prompt Tuning Efficiency via Progressive Training. (arXiv:2211.06840v1 [cs.CL])","link":"http://arxiv.org/abs/2211.06840","description":"<p>Recently, prompt tuning (PT) has gained increasing attention as a\nparameter-efficient way of tuning pre-trained language models (PLMs). Despite\nextensively reducing the number of tunable parameters and achieving satisfying\nperformance, PT is training-inefficient due to its slow convergence. To improve\nPT's training efficiency, we first make some novel observations about the\nprompt transferability of \"partial PLMs\", which are defined by compressing a\nPLM in depth or width. We observe that the soft prompts learned by different\npartial PLMs of various sizes are similar in the parameter space, implying that\nthese soft prompts could potentially be transferred among partial PLMs.\nInspired by these observations, we propose Fast Prompt Tuning (FPT), which\nstarts by conducting PT using a small-scale partial PLM, and then progressively\nexpands its depth and width until the full-model size. After each expansion, we\nrecycle the previously learned soft prompts as initialization for the enlarged\npartial PLM and then proceed PT. We demonstrate the feasibility of FPT on 5\ntasks and show that FPT could save over 30% training computations while\nachieving comparable performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huadong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yichun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CausaLM: Causal Model Explanation Through Counterfactual Language Models. (arXiv:2005.13407v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.13407","description":"<p>Understanding predictions made by deep neural networks is notoriously\ndifficult, but also crucial to their dissemination. As all machine learning\nbased methods, they are as good as their training data, and can also capture\nunwanted biases. While there are tools that can help understand whether such\nbiases exist, they do not distinguish between correlation and causation, and\nmight be ill-suited for text-based models and for reasoning about high level\nlanguage concepts. A key problem of estimating the causal effect of a concept\nof interest on a given model is that this estimation requires the generation of\ncounterfactual examples, which is challenging with existing generation\ntechnology. To bridge that gap, we propose CausaLM, a framework for producing\ncausal model explanations using counterfactual language representation models.\nOur approach is based on fine-tuning of deep contextualized embedding models\nwith auxiliary adversarial tasks derived from the causal graph of the problem.\nConcretely, we show that by carefully choosing auxiliary adversarial\npre-training tasks, language representation models such as BERT can effectively\nlearn a counterfactual representation for a given concept of interest, and be\nused to estimate its true causal effect on model performance. A byproduct of\nour method is a language representation model that is unaffected by the tested\nconcept, which can be useful in mitigating unwanted bias ingrained in the data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1\">Amir Feder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oved_N/0/1/0/all/0/1\">Nadav Oved</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalit_U/0/1/0/all/0/1\">Uri Shalit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SHARE: a System for Hierarchical Assistive Recipe Editing. (arXiv:2105.08185v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.08185","description":"<p>The large population of home cooks with dietary restrictions is under-served\nby existing cooking resources and recipe generation models. To help them, we\npropose the task of controllable recipe editing: adapt a base recipe to satisfy\na user-specified dietary constraint. This task is challenging, and cannot be\nadequately solved with human-written ingredient substitution rules or existing\nend-to-end recipe generation models. We tackle this problem with SHARE: a\nSystem for Hierarchical Assistive Recipe Editing, which performs simultaneous\ningredient substitution before generating natural-language steps using the\nedited ingredients. By decoupling ingredient and step editing, our step\ngenerator can explicitly integrate the available ingredients. Experiments on\nthe novel RecipePairs dataset -- 83K pairs of similar recipes where each recipe\nsatisfies one of seven dietary constraints -- demonstrate that SHARE produces\nconvincing, coherent recipes that are appropriate for a target dietary\nconstraint. We further show through human evaluations and real-world cooking\ntrials that recipes edited by SHARE can be easily followed by home cooks to\ncreate appealing dishes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yufei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jianmo Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Math Word Problems using Pretrained Multilingual Language Models. (arXiv:2105.08928v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.08928","description":"<p>In this paper, we revisit math word problems~(MWPs) from the cross-lingual\nand multilingual perspective. We construct our MWP solvers over pretrained\nmultilingual language models using sequence-to-sequence model with copy\nmechanism. We compare how the MWP solvers perform in cross-lingual and\nmultilingual scenarios. To facilitate the comparison of cross-lingual\nperformance, we first adapt the large-scale English dataset MathQA as a\ncounterpart of the Chinese dataset Math23K. Then we extend several English\ndatasets to bilingual datasets through machine translation plus human\nannotation. Our experiments show that the MWP solvers may not be transferred to\na different language even if the target expressions have the same operator set\nand constants. But for both cross-lingual and multilingual cases, it can be\nbetter generalized if problem types exist on both source language and target\nlanguage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Minghuan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lingxiao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assisting the Human Fact-Checkers: Detecting All Previously Fact-Checked Claims in a Document. (arXiv:2109.07410v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07410","description":"<p>Given the recent proliferation of false claims online, there has been a lot\nof manual fact-checking effort. As this is very time-consuming, human\nfact-checkers can benefit from tools that can support them and make them more\nefficient. Here, we focus on building a system that could provide such support.\nGiven an input document, it aims to detect all sentences that contain a claim\nthat can be verified by some previously fact-checked claims (from a given\ndatabase). The output is a re-ranked list of the document sentences, so that\nthose that can be verified are ranked as high as possible, together with\ncorresponding evidence. Unlike previous work, which has looked into claim\nretrieval, here we take a document-level perspective. We create a new manually\nannotated dataset for this task, and we propose suitable evaluation measures.\nWe further experiment with a learning-to-rank approach, achieving sizable\nperformance gains over several strong baselines. Our analysis demonstrates the\nimportance of modeling text similarity and stance, while also taking into\naccount the veracity of the retrieved previously fact-checked claims. We\nbelieve that this research would be of interest to fact-checkers, journalists,\nmedia, and regulatory authorities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1\">Shaden Shaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TeleMelody: Lyric-to-Melody Generation with a Template-Based Two-Stage Method. (arXiv:2109.09617v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2109.09617","description":"<p>Lyric-to-melody generation is an important task in automatic songwriting.\nPrevious lyric-to-melody generation systems usually adopt end-to-end models\nthat directly generate melodies from lyrics, which suffer from several issues:\n1) lack of paired lyric-melody training data; 2) lack of control on generated\nmelodies. In this paper, we develop TeleMelody, a two-stage lyric-to-melody\ngeneration system with music template (e.g., tonality, chord progression,\nrhythm pattern, and cadence) to bridge the gap between lyrics and melodies\n(i.e., the system consists of a lyric-to-template module and a\ntemplate-to-melody module). TeleMelody has two advantages. First, it is data\nefficient. The template-to-melody module is trained in a self-supervised way\n(i.e., the source template is extracted from the target melody) that does not\nneed any lyric-melody paired data. The lyric-to-template module is made up of\nsome rules and a lyric-to-rhythm model, which is trained with paired\nlyric-rhythm data that is easier to obtain than paired lyric-melody data.\nSecond, it is controllable. The design of template ensures that the generated\nmelodies can be controlled by adjusting the musical elements in template. Both\nsubjective and objective experimental evaluations demonstrate that TeleMelody\ngenerates melodies with higher quality, better controllability, and less\nrequirement on paired lyric-melody data than previous generation systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ju_Z/0/1/0/all/0/1\">Zeqian Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Peiling Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Songruoyao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kejun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Actionable Entities Recognition Benchmark for Interactive Fiction. (arXiv:2109.13855v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13855","description":"<p>This paper presents a new natural language processing task - Actionable\nEntities Recognition (AER) - recognition of entities that protagonists could\ninteract with for further plot development. Though similar to classical Named\nEntity Recognition (NER), it has profound differences. In particular, it is\ncrucial for interactive fiction, where the agent needs to detect entities that\nmight be useful in the future. We also discuss if AER might be further helpful\nfor the systems dealing with narrative processing since actionable entities\nprofoundly impact the causal relationship in a story. We validate the proposed\ntask on two previously available datasets and present a new benchmark dataset\nfor the AER task that includes 5550 descriptions with one or more actionable\nentities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1\">Ivan P. Yamshchikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?. (arXiv:2110.06918v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06918","description":"<p>Despite their recent popularity and well-known advantages, dense retrievers\nstill lag behind sparse methods such as BM25 in their ability to reliably match\nsalient phrases and rare entities in the query and to generalize to\nout-of-domain data. It has been argued that this is an inherent limitation of\ndense models. We rebut this claim by introducing the Salient Phrase Aware\nRetriever (SPAR), a dense retriever with the lexical matching capacity of a\nsparse model. We show that a dense Lexical Model {\\Lambda} can be trained to\nimitate a sparse one, and SPAR is built by augmenting a standard dense\nretriever with {\\Lambda}. Empirically, SPAR shows superior performance on a\nrange of tasks including five question answering datasets, MS MARCO passage\nretrieval, as well as the EntityQuestions and BEIR benchmarks for out-of-domain\nevaluation, exceeding the performance of state-of-the-art dense and sparse\nretrievers. The code and models of SPAR are available at:\nhttps://github.com/facebookresearch/dpr-scale/tree/main/spar\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas O&#x11f;uz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anchit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peshterliev_S/0/1/0/all/0/1\">Stan Peshterliev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sonal Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Embedded Knowledge Graph Multi-hop Question Answering by introducing Relational Chain Reasoning. (arXiv:2110.12679v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.12679","description":"<p>Knowledge Graph Question Answering (KGQA) aims to answer user-questions from\na knowledge graph (KG) by identifying the reasoning relations between topic\nentity and answer. As a complex branch task of KGQA, multi-hop KGQA requires\nreasoning over the multi-hop relational chain preserved in KG to arrive at the\nright answer. Despite recent successes, the existing works on answering\nmulti-hop complex questions still face the following challenges: i) The absence\nof an explicit relational chain order reflected in user-question stems from a\nmisunderstanding of a user's intentions. ii) Incorrectly capturing relational\ntypes on weak supervision of which dataset lacks intermediate reasoning chain\nannotations due to expensive labeling cost. iii) Failing to consider implicit\nrelations between the topic entity and the answer implied in structured KG\nbecause of limited neighborhoods size constraint in subgraph retrieval-based\nalgorithms.To address these issues in multi-hop KGQA, we propose a novel model\nherein, namely Relational Chain based Embedded KGQA (Rce-KGQA), which\nsimultaneously utilizes the explicit relational chain revealed in natural\nlanguage question and the implicit relational chain stored in structured KG.\nOur extensive empirical study on three open-domain benchmarks proves that our\nmethod significantly outperforms the state-of-the-art counterparts like\nGraftNet, PullNet and EmbedKGQA. Comprehensive ablation experiments also verify\nthe effectiveness of our method on the multi-hop KGQA task. We have made our\nmodel's source code available at github:\nhttps://github.com/albert-jin/Rce-KGQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Weiqiang Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Biao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1\">Xi Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_R/0/1/0/all/0/1\">Ruiping Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guizhong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Object Grounding Using Scene Graphs. (arXiv:2201.01901v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.01901","description":"<p>Object grounding tasks aim to locate the target object in an image through\nverbal communications. Understanding human command is an important process\nneeded for effective human-robot communication. However, this is challenging\nbecause human commands can be ambiguous and erroneous. This paper aims to\ndisambiguate the human's referring expressions by allowing the agent to ask\nrelevant questions based on semantic data obtained from scene graphs. We test\nif our agent can use relations between objects from a scene graph to ask\nsemantically relevant questions that can disambiguate the original user\ncommand. In this paper, we present Incremental Grounding using Scene Graphs\n(IGSG), a disambiguation model that uses semantic data from an image scene\ngraph and linguistic structures from a language scene graph to ground objects\nbased on human command. Compared to the baseline, IGSG shows promising results\nin complex real-world scenes where there are multiple identical target objects.\nIGSG can effectively disambiguate ambiguous or wrong referring expressions by\nasking disambiguating questions back to the user.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">John Seon Keun Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoonwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chernova_S/0/1/0/all/0/1\">Sonia Chernova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Black-box Prompt Learning for Pre-trained Language Models. (arXiv:2201.08531v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.08531","description":"<p>The increasing scale of general-purpose Pre-trained Language Models (PLMs)\nnecessitates the study of more efficient adaptation across different downstream\ntasks. In this paper, we establish a Black-box Discrete Prompt Learning (BDPL)\nto resonate with pragmatic interactions between the cloud infrastructure and\nedge devices. Particularly, instead of fine-tuning the model in the cloud, we\nadapt PLMs by prompt learning, which efficiently optimizes only a few\nparameters of the discrete prompts. Moreover, we consider the scenario that we\ndo not have access to the parameters and gradients of the pre-trained models,\nexcept for its outputs given inputs. This black-box setting secures the cloud\ninfrastructure from potential attack and misuse to cause a single-point\nfailure, which is preferable to the white-box counterpart by current\ninfrastructures. Under this black-box constraint, we apply a variance-reduced\npolicy gradient algorithm to estimate the gradients of parameters in the\ncategorical distribution of each discrete prompt. In light of our method, the\nuser devices can efficiently tune their tasks by querying the PLMs bounded by a\nrange of API calls. Our experiments on RoBERTa and GPT-3 demonstrate that the\nproposed algorithm achieves significant improvement on eight benchmarks in a\ncloud-device collaboration manner. Finally, we conduct in-depth case studies to\ncomprehensively analyze our method in terms of various data sizes, prompt\nlengths, training budgets, optimization objectives, prompt transferability, and\nexplanations of the learned prompts. Our code will be available at\nhttps://github.com/shizhediao/Black-Box-Prompt-Learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diao_S/0/1/0/all/0/1\">Shizhe Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhichao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruijia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuechun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Models. (arXiv:2202.04053v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04053","description":"<p>Recently, DALL-E, a multimodal transformer language model, and its variants\n(including diffusion models) have shown high-quality text-to-image generation\ncapabilities. However, despite the interesting image generation results, there\nhas not been a detailed analysis on how to evaluate such models. In this work,\nwe investigate the visual reasoning capabilities and social biases of different\ntext-to-image models, covering both multimodal transformer language models and\ndiffusion models. First, we measure three visual reasoning skills: object\nrecognition, object counting, and spatial relation understanding. For this, we\npropose PaintSkills, a compositional diagnostic dataset and evaluation toolkit\nthat measures these skills. In our experiments, there exists a large gap\nbetween the performance of recent text-to-image models and the upper bound\naccuracy in object counting and spatial relation understanding skills. Second,\nwe assess gender and skin tone biases by measuring the variance of the\ngender/skin tone distribution based on automated and human evaluation. We\ndemonstrate that recent text-to-image models learn specific gender/skin tone\nbiases from web image-text pairs. We hope that our work will help guide future\nprogress in improving text-to-image generation models on visual reasoning\nskills and learning socially unbiased representations. Code and data:\nhttps://github.com/j-min/DallEval\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zala_A/0/1/0/all/0/1\">Abhay Zala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UDAAN: Machine Learning based Post-Editing tool for Document Translation. (arXiv:2203.01644v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.01644","description":"<p>We introduce UDAAN, an open-source post-editing tool that can reduce manual\nediting efforts to quickly produce publishable-standard documents in several\nIndic languages. UDAAN has an end-to-end Machine Translation (MT) plus\npost-editing pipeline wherein users can upload a document to obtain raw MT\noutput. Further, users can edit the raw translations using our tool. UDAAN\noffers several advantages: a) Domain-aware, vocabulary-based lexical\nconstrained MT. b) source-target and target-target lexicon suggestions for\nusers. Replacements are based on the source and target texts lexicon alignment.\nc) Translation suggestions are based on logs created during user interaction.\nd) Source-target sentence alignment visualisation that reduces the cognitive\nload of users during editing. e) Translated outputs from our tool are available\nin multiple formats: docs, latex, and PDF. We also provide the facility to use\naround 100 in-domain dictionaries for lexicon-aware machine translation.\nAlthough we limit our experiments to English-to-Hindi translation, our tool is\nindependent of the source and target languages. Experimental results based on\nthe usage of the tools and users feedback show that our tool speeds up the\ntranslation time by approximately a factor of three compared to the baseline\nmethod of translating documents from scratch. Our tool is available for both\nWindows and Linux platforms. The tool is open-source under MIT license, and the\nsource code can be accessed from our website, https://www.udaanproject.org.\nDemonstration and tutorial videos for various features of our tool can be\naccessed here. Our MT pipeline can be accessed at\nhttps://udaaniitb.aicte-india.org/udaan/translate/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maheshwari_A/0/1/0/all/0/1\">Ayush Maheshwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravindran_A/0/1/0/all/0/1\">Ajay Ravindran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_V/0/1/0/all/0/1\">Venkatapathy Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ScienceWorld: Is your Agent Smarter than a 5th Grader?. (arXiv:2203.07540v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07540","description":"<p>We present ScienceWorld, a benchmark to test agents' scientific reasoning\nabilities in a new interactive text environment at the level of a standard\nelementary school science curriculum. Despite the transformer-based progress\nseen in question-answering and scientific text processing, we find that current\nmodels cannot reason about or explain learned science concepts in novel\ncontexts. For instance, models can easily answer what the conductivity of a\nknown material is but struggle when asked how they would conduct an experiment\nin a grounded environment to find the conductivity of an unknown material. This\nbegs the question of whether current models are simply retrieving answers by\nway of seeing a large number of similar examples or if they have learned to\nreason about concepts in a reusable manner. We hypothesize that agents need to\nbe grounded in interactive environments to achieve such reasoning capabilities.\nOur experiments provide empirical evidence supporting this hypothesis --\nshowing that a 1.5 million parameter agent trained interactively for 100k steps\noutperforms a 11 billion parameter model statically trained for scientific\nquestion-answering and reasoning from millions of expert demonstrations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruoyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jansen_P/0/1/0/all/0/1\">Peter Jansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cote_M/0/1/0/all/0/1\">Marc-Alexandre C&#xf4;t&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computer Science Named Entity Recognition in the Open Research Knowledge Graph. (arXiv:2203.14579v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.14579","description":"<p>Domain-specific named entity recognition (NER) on Computer Science (CS)\nscholarly articles is an information extraction task that is arguably more\nchallenging for the various annotation aims that can beset the task and has\nbeen less studied than NER in the general domain. Given that significant\nprogress has been made on NER, we believe that scholarly domain-specific NER\nwill receive increasing attention in the years to come. Currently, progress on\nCS NER -- the focus of this work -- is hampered in part by its recency and the\nlack of a standardized annotation aim for scientific entities/terms. This work\nproposes a standardized task by defining a set of seven contribution-centric\nscholarly entities for CS NER viz., research problem, solution, resource,\nlanguage, tool, method, and dataset. Following which, its main contributions\nare: combines existing CS NER resources that maintain their annotation focus on\nthe set or subset of contribution-centric scholarly entities we consider;\nfurther, noting the need for big data to train neural NER models, this work\nadditionally supplies thousands of contribution-centric entity annotations from\narticle titles and abstracts, thus releasing a cumulative large novel resource\nfor CS NER; and, finally, trains a sequence labeling CS NER model inspired\nafter state-of-the-art neural architectures from the general domain NER task.\nThroughout the work, several practical considerations are made which can be\nuseful to information technology designers of the digital libraries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DSouza_J/0/1/0/all/0/1\">Jennifer D&#x27;Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auer_S/0/1/0/all/0/1\">S&#xf6;ren Auer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NFLAT: Non-Flat-Lattice Transformer for Chinese Named Entity Recognition. (arXiv:2205.05832v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.05832","description":"<p>Recently, Flat-LAttice Transformer (FLAT) has achieved great success in\nChinese Named Entity Recognition (NER). FLAT performs lexical enhancement by\nconstructing flat lattices, which mitigates the difficulties posed by blurred\nword boundaries and the lack of word semantics. In FLAT, the positions of\nstarting and ending characters are used to connect a matching word. However,\nthis method is likely to match more words when dealing with long texts,\nresulting in long input sequences. Therefore, it significantly increases the\nmemory and computational costs of the self-attention module. To deal with this\nissue, we advocate a novel lexical enhancement method, InterFormer, that\neffectively reduces the amount of computational and memory costs by\nconstructing non-flat lattices. Furthermore, with InterFormer as the backbone,\nwe implement NFLAT for Chinese NER. NFLAT decouples lexicon fusion and context\nfeature encoding. Compared with FLAT, it reduces unnecessary attention\ncalculations in \"word-character\" and \"word-word\". This reduces the memory usage\nby about 50% and can use more extensive lexicons or higher batches for network\ntraining. The experimental results obtained on several well-known benchmarks\ndemonstrate the superiority of the proposed method over the state-of-the-art\nhybrid (character-word) models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiaoning Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhenhua Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Economics of Multilingual Few-shot Learning: Modeling the Cost-Performance Trade-offs of Machine Translated and Manual Data. (arXiv:2205.06350v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.06350","description":"<p>Borrowing ideas from {\\em Production functions} in micro-economics, in this\npaper we introduce a framework to systematically evaluate the performance and\ncost trade-offs between machine-translated and manually-created labelled data\nfor task-specific fine-tuning of massively multilingual language models. We\nillustrate the effectiveness of our framework through a case-study on the\nTyDIQA-GoldP dataset. One of the interesting conclusions of the study is that\nif the cost of machine translation is greater than zero, the optimal\nperformance at least cost is always achieved with at least some or only\nmanually-created data. To our knowledge, this is the first attempt towards\nextending the concept of production functions to study data collection\nstrategies for training multilingual models, and can serve as a valuable tool\nfor other similar cost vs data trade-offs in NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1\">Kabir Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dandapat_S/0/1/0/all/0/1\">Sandipan Dandapat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Static Models and Test Sets: Benchmarking the Potential of Pre-trained Models Across Tasks and Languages. (arXiv:2205.06356v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.06356","description":"<p>Although recent Massively Multilingual Language Models (MMLMs) like mBERT and\nXLMR support around 100 languages, most existing multilingual NLP benchmarks\nprovide evaluation data in only a handful of these languages with little\nlinguistic diversity. We argue that this makes the existing practices in\nmultilingual evaluation unreliable and does not provide a full picture of the\nperformance of MMLMs across the linguistic landscape. We propose that the\nrecent work done in Performance Prediction for NLP tasks can serve as a\npotential solution in fixing benchmarking in Multilingual NLP by utilizing\nfeatures related to data and language typology to estimate the performance of\nan MMLM on different languages. We compare performance prediction with\ntranslating test data with a case study on four different multilingual\ndatasets, and observe that these methods can provide reliable estimates of the\nperformance that are often on-par with the translation based approaches,\nwithout the need for any additional translation as well as evaluation costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1\">Kabir Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dandapat_S/0/1/0/all/0/1\">Sandipan Dandapat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitaram_S/0/1/0/all/0/1\">Sunayana Sitaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Naturalistic Causal Probing for Morpho-Syntax. (arXiv:2205.07043v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.07043","description":"<p>Probing has become a go-to methodology for interpreting and analyzing deep\nneural models in natural language processing. However, there is still a lack of\nunderstanding of the limitations and weaknesses of various types of probes. In\nthis work, we suggest a strategy for input-level intervention on naturalistic\nsentences. Using our approach, we intervene on the morpho-syntactic features of\na sentence, while keeping the rest of the sentence unchanged. Such an\nintervention allows us to causally probe pre-trained models. We apply our\nnaturalistic causal probing framework to analyze the effects of grammatical\ngender and number on contextualized representations extracted from three\npre-trained models in Spanish: the multilingual versions of BERT, RoBERTa, and\nGPT-2. Our experiments suggest that naturalistic interventions lead to stable\nestimates of the causal effects of various linguistic properties. Moreover, our\nexperiments demonstrate the importance of naturalistic causal probing when\nanalyzing pre-trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Afra Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Alignment Bias in Neural Seq2Seq Semantic Parsers. (arXiv:2205.08288v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.08288","description":"<p>Prior to deep learning the semantic parsing community has been interested in\nunderstanding and modeling the range of possible word alignments between\nnatural language sentences and their corresponding meaning representations.\nSequence-to-sequence models changed the research landscape suggesting that we\nno longer need to worry about alignments since they can be learned\nautomatically by means of an attention mechanism. More recently, researchers\nhave started to question such premise. In this work we investigate whether\nseq2seq models can handle both simple and complex alignments. To answer this\nquestion we augment the popular Geo semantic parsing dataset with alignment\nannotations and create Geo-Aligned. We then study the performance of standard\nseq2seq models on the examples that can be aligned monotonically versus\nexamples that require more complex alignments. Our empirical study shows that\nperformance is significantly better over monotonic alignments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Locatelli_D/0/1/0/all/0/1\">Davide Locatelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quattoni_A/0/1/0/all/0/1\">Ariadna Quattoni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aerial Vision-and-Dialog Navigation. (arXiv:2205.12219v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12219","description":"<p>The ability to converse with humans and follow commands in natural language\nis crucial for intelligent unmanned aerial vehicles (a.k.a. drones). It can\nrelieve people's burden of holding a controller all the time, allow\nmultitasking, and make drone control more accessible for people with\ndisabilities or with their hands occupied. To this end, we introduce Aerial\nVision-and-Dialog Navigation (AVDN), to navigate a drone via natural language\nconversation. We build a drone simulator with a continuous photorealistic\nenvironment and collect a new AVDN dataset of over 3k recorded navigation\ntrajectories with asynchronous human-human dialogs between commanders and\nfollowers. The commander provides initial navigation instruction and further\nguidance by request, while the follower navigates the drone in the simulator\nand asks questions when needed. During data collection, followers' attention on\nthe drone's visual observation is also recorded. Based on the AVDN dataset, we\nstudy the tasks of aerial navigation from (full) dialog history and propose an\neffective Human Attention Aided (HAA) baseline model, which learns to predict\nboth navigation waypoints and human attention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yue Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Winson Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tongzhou Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Reinforcement Learning and Distribution Matching for Fine-Tuning Language Models with no Catastrophic Forgetting. (arXiv:2206.00761v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.00761","description":"<p>The availability of large pre-trained models is changing the landscape of\nMachine Learning research and practice, moving from a training-from-scratch to\na fine-tuning paradigm. While in some applications the goal is to \"nudge\" the\npre-trained distribution towards preferred outputs, in others it is to steer it\ntowards a different distribution over the sample space. Two main paradigms have\nemerged to tackle this challenge: Reward Maximization (RM) and, more recently,\nDistribution Matching (DM). RM applies standard Reinforcement Learning (RL)\ntechniques, such as Policy Gradients, to gradually increase the reward signal.\nDM prescribes to first make explicit the target distribution that the model is\nfine-tuned to approximate. Here we explore the theoretical connections between\nthe two paradigms, and show that methods such as KL-control developed for RM\ncan also be construed as belonging to DM. We further observe that while DM\ndiffers from RM, it can suffer from similar training difficulties, such as high\ngradient variance. We leverage connections between the two paradigms to import\nthe concept of baseline into DM methods. We empirically validate the benefits\nof adding a baseline on an array of controllable language generation tasks such\nas constraining topic, sentiment, and gender distributions in texts sampled\nfrom a language model. We observe superior performance in terms of constraint\nsatisfaction, stability and sample efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Korbak_T/0/1/0/all/0/1\">Tomasz Korbak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsahar_H/0/1/0/all/0/1\">Hady Elsahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruszewski_G/0/1/0/all/0/1\">Germ&#xe1;n Kruszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dymetman_M/0/1/0/all/0/1\">Marc Dymetman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Swiss German Speech to Text system evaluation. (arXiv:2207.00412v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.00412","description":"<p>We present an in-depth evaluation of four commercially available\nSpeech-to-Text (STT) systems for Swiss German. The systems are anonymized and\nreferred to as system a-d in this report. We compare the four systems to our\nSTT model, referred to as FHNW from hereon after, and provide details on how we\ntrained our model. To evaluate the models, we use two STT datasets from\ndifferent domains. The Swiss Parliament Corpus (SPC) test set and a private\ndataset in the news domain with an even distribution across seven dialect\nregions. We provide a detailed error analysis to detect the three systems'\nstrengths and weaknesses. This analysis is limited by the characteristics of\nthe two test sets. Our model scored the highest bilingual evaluation understudy\n(BLEU) on both datasets. On the SPC test set, we obtain a BLEU score of 0.607,\nwhereas the best commercial system reaches a BLEU score of 0.509. On our\nprivate test set, we obtain a BLEU score of 0.722 and the best commercial\nsystem a BLEU score of 0.568.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schraner_Y/0/1/0/all/0/1\">Yanick Schraner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheller_C/0/1/0/all/0/1\">Christian Scheller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pluss_M/0/1/0/all/0/1\">Michel Pl&#xfc;ss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogel_M/0/1/0/all/0/1\">Manfred Vogel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Length Generalization in Large Language Models. (arXiv:2207.04901v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.04901","description":"<p>The ability to extrapolate from short problem instances to longer ones is an\nimportant form of out-of-distribution generalization in reasoning tasks, and is\ncrucial when learning from datasets where longer problem instances are rare.\nThese include theorem proving, solving quantitative mathematics problems, and\nreading/summarizing novels. In this paper, we run careful empirical studies\nexploring the length generalization capabilities of transformer-based language\nmodels. We first establish that naively finetuning transformers on length\ngeneralization tasks shows significant generalization deficiencies independent\nof model scale. We then show that combining pretrained large language models'\nin-context learning abilities with scratchpad prompting (asking the model to\noutput solution steps before producing an answer) results in a dramatic\nimprovement in length generalization. We run careful failure analyses on each\nof the learning modalities and identify common sources of mistakes that\nhighlight opportunities in equipping language models with the ability to\ngeneralize to longer problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anil_C/0/1/0/all/0/1\">Cem Anil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuhuai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreassen_A/0/1/0/all/0/1\">Anders Andreassen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewkowycz_A/0/1/0/all/0/1\">Aitor Lewkowycz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_V/0/1/0/all/0/1\">Vedant Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramasesh_V/0/1/0/all/0/1\">Vinay Ramasesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slone_A/0/1/0/all/0/1\">Ambrose Slone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gur_Ari_G/0/1/0/all/0/1\">Guy Gur-Ari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dyer_E/0/1/0/all/0/1\">Ethan Dyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neyshabur_B/0/1/0/all/0/1\">Behnam Neyshabur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Protein Knowledge Graph Construction and Applications. (arXiv:2207.10080v3 [q-bio.QM] UPDATED)","link":"http://arxiv.org/abs/2207.10080","description":"<p>Existing data-centric methods for protein science generally cannot\nsufficiently capture and leverage biology knowledge, which may be crucial for\nmany protein tasks. To facilitate research in this field, we create\nProteinKG65, a knowledge graph for protein science. Using gene ontology and\nUniprot knowledge base as a basis, we transform and integrate various kinds of\nknowledge with aligned descriptions and protein sequences, respectively, to GO\nterms and protein entities. ProteinKG65 is mainly dedicated to providing a\nspecialized protein knowledge graph, bringing the knowledge of Gene Ontology to\nprotein function and structure prediction. We also illustrate the potential\napplications of ProteinKG65 with a prototype. Our dataset can be downloaded at\nhttps://w3id.org/proteinkg65.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Vs. MLP-Mixer: Exponential Expressive Gap For NLP Problems. (arXiv:2208.08191v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.08191","description":"<p>Vision-Transformers are widely used in various vision tasks. Meanwhile, there\nis another line of works starting with the MLP-mixer trying to achieve similar\nperformance using mlp-based architectures. Interestingly, until now those\nmlp-based architectures have not been adapted for NLP tasks. Additionally,\nuntil now, mlp-based architectures have failed to achieve state-of-the-art\nperformance in vision tasks. In this paper, we analyze the expressive power of\nmlp-based architectures in modeling dependencies between multiple different\ninputs simultaneously, and show an exponential gap between the attention and\nthe mlp-based mechanisms. Our results suggest a theoretical explanation for the\nmlp inability to compete with attention-based mechanisms in NLP problems, they\nalso suggest that the performance gap in vision tasks may be due to the mlp\nrelative weakness in modeling dependencies between multiple different\nlocations, and that combining smart input permutations with mlp architectures\nmay not be enough to close the performance gap alone.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Navon_D/0/1/0/all/0/1\">Dan Navon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bronstein_A/0/1/0/all/0/1\">Alex M. Bronstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-stage Information Retrieval for Vietnamese Legal Texts. (arXiv:2209.14494v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.14494","description":"<p>This study deals with the problem of information retrieval (IR) for\nVietnamese legal texts. Despite being well researched in many languages,\ninformation retrieval has still not received much attention from the Vietnamese\nresearch community. This is especially true for the case of legal documents,\nwhich are hard to process. This study proposes a new approach for information\nretrieval for Vietnamese legal documents using sentence-transformer. Besides,\nvarious experiments are conducted to make comparisons between different\ntransformer models, ranking scores, syllable-level, and word-level training.\nThe experiment results show that the proposed model outperforms models used in\ncurrent research on information retrieval for Vietnamese documents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1\">Nhat-Minh Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha-Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Trong-Hop Do</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Anaphora Resolution in Scientific Protocols via Mixtures of In-Context Experts. (arXiv:2210.03690v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.03690","description":"<p>Anaphora resolution is an important task for information extraction across a\nrange of languages, text genres, and domains, motivating the need for methods\nthat do not require large annotated datasets. In-context learning has emerged\nas a promising approach, yet there are a number of challenges in applying\nin-context learning to resolve anaphora. For example, encoding a single\nin-context demonstration that consists of: an anaphor, a paragraph-length\ncontext, and a list of corresponding antecedents, requires conditioning a\nlanguage model on a long sequence of tokens, limiting the number of\ndemonstrations per prompt. In this paper, we present MICE (Mixtures of\nIn-Context Experts), which we demonstrate is effective for few-shot anaphora\nresolution in scientific protocols (Tamari et al., 2021). Given only a handful\nof training examples, MICE combines the predictions of hundreds of in-context\nexperts, yielding a 30% increase in F1 score over a competitive prompt\nretrieval baseline. Furthermore, we show MICE can be used to train compact\nstudent models without sacrificing performance. As far as we are aware, this is\nthe first work to present experimental results demonstrating the effectiveness\nof in-context learning on the task of few-shot anaphora resolution in\nscientific protocols.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Nghia T. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_F/0/1/0/all/0/1\">Fan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dialogic: Controllable Dialogue Simulation with In-Context Learning. (arXiv:2210.04185v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04185","description":"<p>Building dialogue systems requires a large corpus of annotated dialogues.\nSuch datasets are usually created via crowdsourcing, which is expensive and\ntime-consuming. In this paper, we propose \\textsc{Dialogic}, a novel dialogue\nsimulation method based on large language model in-context learning to automate\ndataset creation. Seeded with a few annotated dialogues, \\textsc{Dialogic}\nautomatically selects in-context examples for demonstration and prompts GPT-3\nto generate new dialogues and annotations in a controllable way. Our method can\nrapidly expand a small set of dialogue data with minimum or zero \\textit{human\ninvolvement} and \\textit{parameter update} and is thus much more cost-efficient\nand time-saving than crowdsourcing. Experimental results on the MultiWOZ\ndataset demonstrate that training a model on the simulated dialogues leads to\neven better performance than using the same amount of human-generated dialogues\nunder the challenging low-resource settings, with as few as 85 dialogues as a\nseed. When enough data is available, our method can still serve as an effective\ndata augmentation method. Human evaluation results also show that our simulated\ndialogues have near-human fluency and annotation accuracy. The code and data\nare available at \\textbf{\\url{https://github.com/Leezekun/dialogic}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shiyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jing Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xifeng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Personalization of CTC Speech Recognition Models with Contextual Adapters and Adaptive Boosting. (arXiv:2210.09510v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.09510","description":"<p>End-to-end speech recognition models trained using joint Connectionist\nTemporal Classification (CTC)-Attention loss have gained popularity recently.\nIn these models, a non-autoregressive CTC decoder is often used at inference\ntime due to its speed and simplicity. However, such models are hard to\npersonalize because of their conditional independence assumption that prevents\noutput tokens from previous time steps to influence future predictions. To\ntackle this, we propose a novel two-way approach that first biases the encoder\nwith attention over a predefined list of rare long-tail and out-of-vocabulary\n(OOV) words and then uses dynamic boosting and phone alignment network during\ndecoding to further bias the subword predictions. We evaluate our approach on\nopen-source VoxPopuli and in-house medical datasets to showcase a 60%\nimprovement in F1 score on domain-specific rare words over a strong CTC\nbaseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dingliwal_S/0/1/0/all/0/1\">Saket Dingliwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkara_M/0/1/0/all/0/1\">Monica Sunkara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodapati_S/0/1/0/all/0/1\">Sravan Bodapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ronanki_S/0/1/0/all/0/1\">Srikanth Ronanki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farris_J/0/1/0/all/0/1\">Jeff Farris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1\">Katrin Kirchhoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empathetic Dialogue Generation via Sensitive Emotion Recognition and Sensible Knowledge Selection. (arXiv:2210.11715v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.11715","description":"<p>Empathy, which is widely used in psychological counselling, is a key trait of\neveryday human conversations. Equipped with commonsense knowledge, current\napproaches to empathetic response generation focus on capturing implicit\nemotion within dialogue context, where the emotions are treated as a static\nvariable throughout the conversations. However, emotions change dynamically\nbetween utterances, which makes previous works difficult to perceive the\nemotion flow and predict the correct emotion of the target response, leading to\ninappropriate response. Furthermore, simply importing commonsense knowledge\nwithout harmonization may trigger the conflicts between knowledge and emotion,\nwhich confuse the model to choose incorrect information to guide the generation\nprocess. To address the above problems, we propose a Serial Encoding and\nEmotion-Knowledge interaction (SEEK) method for empathetic dialogue generation.\nWe use a fine-grained encoding strategy which is more sensitive to the emotion\ndynamics (emotion flow) in the conversations to predict the emotion-intent\ncharacteristic of response. Besides, we design a novel framework to model the\ninteraction between knowledge and emotion to generate more sensible response.\nExtensive experiments on EmpatheticDialogues demonstrate that SEEK outperforms\nthe strong baselines in both automatic and manual evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lanrui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chenxu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OTSeq2Set: An Optimal Transport Enhanced Sequence-to-Set Model for Extreme Multi-label Text Classification. (arXiv:2210.14523v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.14523","description":"<p>Extreme multi-label text classification (XMTC) is the task of finding the\nmost relevant subset labels from an extremely large-scale label collection.\nRecently, some deep learning models have achieved state-of-the-art results in\nXMTC tasks. These models commonly predict scores for all labels by a fully\nconnected layer as the last layer of the model. However, such models can't\npredict a relatively complete and variable-length label subset for each\ndocument, because they select positive labels relevant to the document by a\nfixed threshold or take top k labels in descending order of scores. A less\npopular type of deep learning models called sequence-to-sequence (Seq2Seq)\nfocus on predicting variable-length positive labels in sequence style. However,\nthe labels in XMTC tasks are essentially an unordered set rather than an\nordered sequence, the default order of labels restrains Seq2Seq models in\ntraining. To address this limitation in Seq2Seq, we propose an autoregressive\nsequence-to-set model for XMTC tasks named OTSeq2Set. Our model generates\npredictions in student-forcing scheme and is trained by a loss function based\non bipartite matching which enables permutation-invariance. Meanwhile, we use\nthe optimal transport distance as a measurement to force the model to focus on\nthe closest labels in semantic label space. Experiments show that OTSeq2Set\noutperforms other competitive baselines on 4 benchmark datasets. Especially, on\nthe Wikipedia dataset with 31k labels, it outperforms the state-of-the-art\nSeq2Seq method by 16.34% in micro-F1 score. The code is available at\nhttps://github.com/caojie54/OTSeq2Set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jie Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why is Winoground Hard? Investigating Failures in Visuolinguistic Compositionality. (arXiv:2211.00768v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.00768","description":"<p>Recent visuolinguistic pre-trained models show promising progress on various\nend tasks such as image retrieval and video captioning. Yet, they fail\nmiserably on the recently proposed Winoground dataset, which challenges models\nto match paired images and English captions, with items constructed to overlap\nlexically but differ in meaning (e.g., \"there is a mug in some grass\" vs.\n\"there is some grass in a mug\"). By annotating the dataset using new\nfine-grained tags, we show that solving the Winoground task requires not just\ncompositional language understanding, but a host of other abilities like\ncommonsense reasoning or locating small, out-of-focus objects in low-resolution\nimages. In this paper, we identify the dataset's main challenges through a\nsuite of experiments on related tasks (probing task, image retrieval task),\ndata augmentation, and manual inspection of the dataset. Our analysis suggests\nthat a main challenge in visuolinguistic models may lie in fusing visual and\ntextual representations, rather than in compositional language understanding.\nWe release our annotation and code at\nhttps://github.com/ajd12342/why-winoground-hard .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diwan_A/0/1/0/all/0/1\">Anuj Diwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berry_L/0/1/0/all/0/1\">Layne Berry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1\">Kyle Mahowald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers on Multilingual Clause-Level Morphology. (arXiv:2211.01736v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.01736","description":"<p>This paper describes our winning systems in MRL: The 1st Shared Task on\nMultilingual Clause-level Morphology (EMNLP 2022 Workshop) designed by KUIS AI\nNLP team. We present our work for all three parts of the shared task:\ninflection, reinflection, and analysis. We mainly explore transformers with two\napproaches: (i) training models from scratch in combination with data\naugmentation, and (ii) transfer learning with prefix-tuning at multilingual\nmorphological tasks. Data augmentation significantly improves performance for\nmost languages in the inflection and reinflection tasks. On the other hand,\nPrefix-tuning on a pre-trained mGPT model helps us to adapt analysis tasks in\nlow-data and multilingual settings. While transformer architectures with data\naugmentation achieved the most promising results for inflection and\nreinflection tasks, prefix-tuning on mGPT received the highest results for the\nanalysis task. Our systems received 1st place in all three tasks in MRL 2022.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Acikgoz_E/0/1/0/all/0/1\">Emre Can Acikgoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chubakov_T/0/1/0/all/0/1\">Tilek Chubakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kural_M/0/1/0/all/0/1\">M&#xfc;ge Kural</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahin_G/0/1/0/all/0/1\">G&#xf6;zde G&#xfc;l &#x15e;ahin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuret_D/0/1/0/all/0/1\">Deniz Yuret</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inverse scaling can become U-shaped. (arXiv:2211.02011v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.02011","description":"<p>Although scaling language models improves performance on a range of tasks,\nthere are apparently some scenarios where scaling hurts performance. For\ninstance, the Inverse Scaling Prize Round 1 (McKensie et al., 2022) identified\nfour \"inverse scaling\" tasks, for which performance gets worse for larger\nmodels. These tasks were evaluated on models of up to 280B parameters, trained\nup to 500 zettaFLOPs of compute.\n</p>\n<p>This paper takes a closer look at these four tasks. We evaluate models of up\nto 540B parameters, trained on five times more compute than those evaluated in\nthe Inverse Scaling Prize. With this increased range of model sizes and\ntraining compute, two out of the four tasks exhibit what we call \"U-shaped\nscaling\" -- performance decreases up to a certain model size, and then\nincreases again up to the largest model evaluated. One hypothesis is that\nU-shaped scaling occurs when a task comprises a \"true task\" and a \"distractor\ntask\". Medium-size models can do the distractor task, which hurts performance,\nwhile only large-enough models can ignore the distractor task and do the true\ntask. The existence of U-shaped scaling implies that inverse scaling may not\nhold for larger models.\n</p>\n<p>Second, we evaluate the inverse scaling tasks using chain-of-thought (CoT)\nprompting, in addition to basic prompting without CoT. With CoT prompting, all\nfour tasks show either U-shaped scaling or positive scaling, achieving perfect\nsolve rates on two tasks and several sub-tasks. This suggests that the term\n\"inverse scaling task\" is under-specified -- a given task may be inverse\nscaling for one prompt but positive or U-shaped scaling for a different prompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Progress on Scalable Oversight for Large Language Models. (arXiv:2211.03540v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2211.03540","description":"<p>Developing safe and useful general-purpose AI systems will require us to make\nprogress on scalable oversight: the problem of supervising systems that\npotentially outperform us on most skills relevant to the task at hand.\nEmpirical work on this problem is not straightforward, since we do not yet have\nsystems that broadly exceed our abilities. This paper discusses one of the\nmajor ways we think about this problem, with a focus on ways it can be studied\nempirically. We first present an experimental design centered on tasks for\nwhich human specialists succeed but unaided humans and current general AI\nsystems fail. We then present a proof-of-concept experiment meant to\ndemonstrate a key feature of this experimental design and show its viability\nwith two question-answering tasks: MMLU and time-limited QuALITY. On these\ntasks, we find that human participants who interact with an unreliable\nlarge-language-model dialog assistant through chat -- a trivial baseline\nstrategy for scalable oversight -- substantially outperform both the model\nalone and their own unaided performance. These results are an encouraging sign\nthat scalable oversight will be tractable to study with present models and\nbolster recent findings that large language models can productively assist\nhumans with difficult tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyun_J/0/1/0/all/0/1\">Jeeyoon Hyun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Edwin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pettit_C/0/1/0/all/0/1\">Craig Pettit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heiner_S/0/1/0/all/0/1\">Scott Heiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukosiute_K/0/1/0/all/0/1\">Kamil&#x117; Luko&#x161;i&#x16b;t&#x117;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askell_A/0/1/0/all/0/1\">Amanda Askell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_A/0/1/0/all/0/1\">Andy Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anna Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldie_A/0/1/0/all/0/1\">Anna Goldie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirhoseini_A/0/1/0/all/0/1\">Azalia Mirhoseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKinnon_C/0/1/0/all/0/1\">Cameron McKinnon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olah_C/0/1/0/all/0/1\">Christopher Olah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amodei_D/0/1/0/all/0/1\">Daniela Amodei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amodei_D/0/1/0/all/0/1\">Dario Amodei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1\">Dawn Drain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dustin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Johnson_E/0/1/0/all/0/1\">Eli Tran-Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kernion_J/0/1/0/all/0/1\">Jackson Kernion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerr_J/0/1/0/all/0/1\">Jamie Kerr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_J/0/1/0/all/0/1\">Jared Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ladish_J/0/1/0/all/0/1\">Jeffrey Ladish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landau_J/0/1/0/all/0/1\">Joshua Landau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ndousse_K/0/1/0/all/0/1\">Kamal Ndousse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovitt_L/0/1/0/all/0/1\">Liane Lovitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhage_N/0/1/0/all/0/1\">Nelson Elhage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiefer_N/0/1/0/all/0/1\">Nicholas Schiefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_N/0/1/0/all/0/1\">Nicholas Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mercado_N/0/1/0/all/0/1\">Noem&#xed; Mercado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DasSarma_N/0/1/0/all/0/1\">Nova DasSarma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larson_R/0/1/0/all/0/1\">Robin Larson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCandlish_S/0/1/0/all/0/1\">Sam McCandlish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1\">Sandipan Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnston_S/0/1/0/all/0/1\">Scott Johnston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kravec_S/0/1/0/all/0/1\">Shauna Kravec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Showk_S/0/1/0/all/0/1\">Sheer El Showk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1\">Stanislav Fort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Telleen_Lawton_T/0/1/0/all/0/1\">Timothy Telleen-Lawton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_T/0/1/0/all/0/1\">Tom Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henighan_T/0/1/0/all/0/1\">Tom Henighan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hume_T/0/1/0/all/0/1\">Tristan Hume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yuntao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hatfield_Dodds_Z/0/1/0/all/0/1\">Zac Hatfield-Dodds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mann_B/0/1/0/all/0/1\">Ben Mann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1\">Jared Kaplan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Zero-shot Event Extraction with Context-Definition Alignment. (arXiv:2211.05156v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05156","description":"<p>Event extraction (EE) is the task of identifying interested event mentions\nfrom text. Conventional efforts mainly focus on the supervised setting.\nHowever, these supervised models cannot generalize to event types out of the\npre-defined ontology. To fill this gap, many efforts have been devoted to the\nzero-shot EE problem. This paper follows the trend of modeling event-type\nsemantics but moves one step further. We argue that using the static embedding\nof the event type name might not be enough because a single word could be\nambiguous, and we need a sentence to define the type semantics accurately. To\nmodel the definition semantics, we use two separate transformer models to\nproject the contextualized event mentions and corresponding definitions into\nthe same embedding space and then minimize their embedding distance via\ncontrastive learning. On top of that, we also propose a warming phase to help\nthe model learn the minor difference between similar definitions. We name our\napproach Zero-shot Event extraction with Definition (ZED). Experiments on the\nMAVEN dataset show that our model significantly outperforms all previous\nzero-shot EE methods with fast inference speed due to the disjoint design.\nFurther experiments also show that ZED can be easily applied to the few-shot\nsetting when the annotation is available and consistently outperforms baseline\nsupervised methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wenlin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VieCap4H-VLSP 2021: ObjectAoA -- Enhancing performance of Object Relation Transformer with Attention on Attention for Vietnamese image captioning. (arXiv:2211.05405v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2211.05405","description":"<p>Image captioning is currently a challenging task that requires the ability to\nboth understand visual information and use human language to describe this\nvisual information in the image. In this paper, we propose an efficient way to\nimprove the image understanding ability of transformer-based method by\nextending Object Relation Transformer architecture with Attention on Attention\nmechanism. Experiments on the VieCap4H dataset show that our proposed method\nsignificantly outperforms its original structure on both the public test and\nprivate test of the Image Captioning shared task held by VLSP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Nghia Hieu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_D/0/1/0/all/0/1\">Duong T.D. Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_M/0/1/0/all/0/1\">Minh-Quan Ha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-game Toxic Language Detection: Shared Task and Attention Residuals. (arXiv:2211.05995v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.05995","description":"<p>In-game toxic language becomes the hot potato in the gaming industry and\ncommunity. There have been several online game toxicity analysis frameworks and\nmodels proposed. However, it is still challenging to detect toxicity due to the\nnature of in-game chat, which has extremely short length. In this paper, we\ndescribe how the in-game toxic language shared task has been established using\nthe real-world in-game chat data. In addition, we propose and introduce the\nmodel/framework for toxic language token tagging (slot filling) from the\nin-game chat. The data and code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yuanzhe Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weixuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_F/0/1/0/all/0/1\">Feiqi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gradient Imitation Reinforcement Learning for General Low-Resource Information Extraction. (arXiv:2211.06014v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2211.06014","description":"<p>Information Extraction (IE) aims to extract structured information from\nheterogeneous sources. IE from natural language texts include sub-tasks such as\nNamed Entity Recognition (NER), Relation Extraction (RE), and Event Extraction\n(EE). Most IE systems require comprehensive understandings of sentence\nstructure, implied semantics, and domain knowledge to perform well; thus, IE\ntasks always need adequate external resources and annotations. However, it\ntakes time and effort to obtain more human annotations. Low-Resource\nInformation Extraction (LRIE) strives to use unsupervised data, reducing the\nrequired resources and human annotation. In practice, existing systems either\nutilize self-training schemes to generate pseudo labels that will cause the\ngradual drift problem, or leverage consistency regularization methods which\ninevitably possess confirmation bias. To alleviate confirmation bias due to the\nlack of feedback loops in existing LRIE learning paradigms, we develop a\nGradient Imitation Reinforcement Learning (GIRL) method to encourage\npseudo-labeled data to imitate the gradient descent direction on labeled data,\nwhich can force pseudo-labeled data to achieve better optimization capabilities\nsimilar to labeled data. Based on how well the pseudo-labeled data imitates the\ninstructive gradient descent direction obtained from labeled data, we design a\nreward to quantify the imitation process and bootstrap the optimization\ncapability of pseudo-labeled data through trial and error. In addition to\nlearning paradigms, GIRL is not limited to specific sub-tasks, and we leverage\nGIRL to solve all IE sub-tasks (named entity recognition, relation extraction,\nand event extraction) in low-resource settings (semi-supervised IE and few-shot\nIE).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_S/0/1/0/all/0/1\">Shiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiangli Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Lottery Tickets for Pre-trained Language Models. (arXiv:2211.03013v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2211.03013","description":"<p>Recent works on Lottery Ticket Hypothesis have shown that pre-trained\nlanguage models (PLMs) contain smaller matching subnetworks(winning tickets)\nwhich are capable of reaching accuracy comparable to the original models.\nHowever, these tickets are proved to be notrobust to adversarial examples, and\neven worse than their PLM counterparts. To address this problem, we propose a\nnovel method based on learning binary weight masks to identify robust tickets\nhidden in the original PLMs. Since the loss is not differentiable for the\nbinary mask, we assign the hard concrete distribution to the masks and\nencourage their sparsity using a smoothing approximation of L0\nregularization.Furthermore, we design an adversarial loss objective to guide\nthe search for robust tickets and ensure that the tickets perform well bothin\naccuracy and robustness. Experimental results show the significant improvement\nof the proposed method over previous work on adversarial robustness evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Rui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_R/0/1/0/all/0/1\">Rong Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Di Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-11-14T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}