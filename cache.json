{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2023-05-16T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Imitation versus Innovation: What children can do that large language and language-and-vision models cannot (yet)?. (arXiv:2305.07666v1 [cs.AI])","link":"http://arxiv.org/abs/2305.07666","description":"<p>Much discussion about large language models and language-and-vision models\nhas focused on whether these models are intelligent agents. We present an\nalternative perspective. We argue that these artificial intelligence models are\ncultural technologies that enhance cultural transmission in the modern world,\nand are efficient imitation engines. We explore what AI models can tell us\nabout imitation and innovation by evaluating their capacity to design new tools\nand discover novel causal structures, and contrast their responses with those\nof human children. Our work serves as a first step in determining which\nparticular representations and competences, as well as which kinds of knowledge\nor skill, can be derived from particular learning techniques and data.\nCritically, our findings suggest that machines may need more than large scale\nlanguage and images to achieve what a child can do.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yiu_E/0/1/0/all/0/1\">Eunice Yiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosoy_E/0/1/0/all/0/1\">Eliza Kosoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopnik_A/0/1/0/all/0/1\">Alison Gopnik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Audio Text Encoders are Effective Multi-Modal Rescorers. (arXiv:2305.07677v1 [cs.SD])","link":"http://arxiv.org/abs/2305.07677","description":"<p>Masked Language Models (MLMs) have proven to be effective for second-pass\nrescoring in Automatic Speech Recognition (ASR) systems. In this work, we\npropose Masked Audio Text Encoder (MATE), a multi-modal masked language model\nrescorer which incorporates acoustic representations into the input space of\nMLM. We adopt contrastive learning for effectively aligning the modalities by\nlearning shared representations. We show that using a multi-modal rescorer is\nbeneficial for domain generalization of the ASR system when target domain data\nis unavailable. MATE reduces word error rate (WER) by 4%-16% on in-domain, and\n3%-7% on out-of-domain datasets, over the text-only baseline. Additionally,\nwith very limited amount of training data (0.8 hours), MATE achieves a WER\nreduction of 8%-23% over the first-pass baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jinglun Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkara_M/0/1/0/all/0/1\">Monica Sunkara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xilai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_A/0/1/0/all/0/1\">Anshu Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodapati_S/0/1/0/all/0/1\">Sravan Bodapati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Language Models to Detect Alarming Student Responses. (arXiv:2305.07709v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07709","description":"<p>This article details the advances made to a system that uses artificial\nintelligence to identify alarming student responses. This system is built into\nour assessment platform to assess whether a student's response indicates they\nare a threat to themselves or others. Such responses may include details\nconcerning threats of violence, severe depression, suicide risks, and\ndescriptions of abuse. Driven by advances in natural language processing, the\nlatest model is a fine-tuned language model trained on a large corpus\nconsisting of student responses and supplementary texts. We demonstrate that\nthe use of a language model delivers a substantial improvement in accuracy over\nthe previous iterations of this system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ormerod_C/0/1/0/all/0/1\">Christopher M. Ormerod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_M/0/1/0/all/0/1\">Milan Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Harry Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parallel Tree Kernel Computation. (arXiv:2305.07717v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07717","description":"<p>Tree kernels are fundamental tools that have been leveraged in many\napplications, particularly those based on machine learning for Natural Language\nProcessing tasks. In this paper, we devise a parallel implementation of the\nsequential algorithm for the computation of some tree kernels of two finite\nsets of trees (Ouali-Sebti, 2015). Our comparison is narrowed on a sequential\nimplementation of SubTree kernel computation. This latter is mainly reduced to\nan intersection of weighted tree automata. Our approach relies on the nature of\nthe data parallelism source inherent in this computation by deploying the\nMapReduce paradigm. One of the key benefits of our approach is its versatility\nin being adaptable to a wide range of substructure tree kernel-based learning\nmethods. To evaluate the efficacy of our parallel approach, we conducted a\nseries of experiments that compared it against the sequential version using a\ndiverse set of synthetic tree language datasets that were manually crafted for\nour analysis. The reached results clearly demonstrate that the proposed\nparallel algorithm outperforms the sequential one in terms of latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taouti_S/0/1/0/all/0/1\">Souad Taouti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherroun_H/0/1/0/all/0/1\">Hadda Cherroun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziadi_D/0/1/0/all/0/1\">Djelloul Ziadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TinyStories: How Small Can Language Models Be and Still Speak Coherent English?. (arXiv:2305.07759v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07759","description":"<p>Language models (LMs) are powerful tools for natural language processing, but\nthey often struggle to produce coherent and fluent text when they are small.\nModels with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can\nrarely generate coherent and consistent English text beyond a few words even\nafter extensive training. This raises the question of whether the emergence of\nthe ability to produce coherent English text only occurs at larger scales (with\nhundreds of millions of parameters or more) and complex architectures (with\nmany layers of global attention).\n</p>\n<p>In this work, we introduce TinyStories, a synthetic dataset of short stories\nthat only contain words that a typical 3 to 4-year-olds usually understand,\ngenerated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train\nand evaluate LMs that are much smaller than the state-of-the-art models (below\n10 million total parameters), or have much simpler architectures (with only one\ntransformer block), yet still produce fluent and consistent stories with\nseveral paragraphs that are diverse and have almost perfect grammar, and\ndemonstrate reasoning capabilities.\n</p>\n<p>We also introduce a new paradigm for the evaluation of language models: We\nsuggest a framework which uses GPT-4 to grade the content generated by these\nmodels as if those were stories written by students and graded by a (human)\nteacher. This new paradigm overcomes the flaws of standard benchmarks which\noften requires the model's output to be very structures, and moreover provides\na multidimensional score for the model, providing scores for different\ncapabilities such as grammar, creativity and consistency.\n</p>\n<p>We hope that TinyStories can facilitate the development, analysis and\nresearch of LMs, especially for low-resource or specialized domains, and shed\nlight on the emergence of language capabilities in LMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eldan_R/0/1/0/all/0/1\">Ronen Eldan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Melody-Guided Lyrics Generation. (arXiv:2305.07760v1 [cs.AI])","link":"http://arxiv.org/abs/2305.07760","description":"<p>Automatic song writing is a topic of significant practical interest. However,\nits research is largely hindered by the lack of training data due to copyright\nconcerns and challenged by its creative nature. Most noticeably, prior works\noften fall short of modeling the cross-modal correlation between melody and\nlyrics due to limited parallel data, hence generating lyrics that are less\nsingable. Existing works also lack effective mechanisms for content control, a\nmuch desired feature for democratizing song creation for people with limited\nmusic background. In this work, we propose to generate pleasantly listenable\nlyrics without training on melody-lyric aligned data. Instead, we design a\nhierarchical lyric generation framework that disentangles training (based\npurely on text) from inference (melody-guided text generation). At inference\ntime, we leverage the crucial alignments between melody and lyrics and compile\nthe given melody into constraints to guide the generation process. Evaluation\nresults show that our model can generate high-quality lyrics that are more\nsingable, intelligible, coherent, and in rhyme than strong baselines including\nthose supervised on parallel data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yufei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_Chen_A/0/1/0/all/0/1\">Anjali Narayan-Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oraby_S/0/1/0/all/0/1\">Shereen Oraby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cervone_A/0/1/0/all/0/1\">Alessandra Cervone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigurdsson_G/0/1/0/all/0/1\">Gunnar Sigurdsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chenyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1\">Tagyoung Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Authoring for Rules and Actions. (arXiv:2305.07763v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07763","description":"<p>Knowledge representation and reasoning (KRR) systems describe and reason with\ncomplex concepts and relations in the form of facts and rules. Unfortunately,\nwide deployment of KRR systems runs into the problem that domain experts have\ngreat difficulty constructing correct logical representations of their domain\nknowledge. Knowledge engineers can help with this construction process, but\nthere is a deficit of such specialists. The earlier Knowledge Authoring Logic\nMachine (KALM) based on Controlled Natural Language (CNL) was shown to have\nvery high accuracy for authoring facts and questions. More recently, KALMFL, a\nsuccessor of KALM, replaced CNL with factual English, which is much less\nrestrictive and requires very little training from users. However, KALMFL has\nlimitations in representing certain types of knowledge, such as authoring rules\nfor multi-step reasoning or understanding actions with timestamps. To address\nthese limitations, we propose KALMRA to enable authoring of rules and actions.\nOur evaluation using the UTI guidelines benchmark shows that KALMRA achieves a\nhigh level of correctness (100%) on rule authoring. When used for authoring and\nreasoning with actions, KALMRA achieves more than 99.3% correctness on the bAbI\nbenchmark, demonstrating its effectiveness in more sophisticated KRR jobs.\nFinally, we illustrate the logical reasoning capabilities of KALMRA by drawing\nattention to the problems faced by the recently made famous AI, ChatGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fodor_P/0/1/0/all/0/1\">Paul Fodor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kifer_M/0/1/0/all/0/1\">Michael Kifer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models. (arXiv:2305.07766v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07766","description":"<p>Temporal Logic (TL) can be used to rigorously specify complex high-level\nspecification for systems in many engineering applications. The translation\nbetween natural language (NL) and TL has been under-explored due to the lack of\ndataset and generalizable model across different application domains. In this\npaper, we propose an accurate and generalizable transformation framework of\nEnglish instructions from NL to TL, exploring the use of Large Language Models\n(LLMs) at multiple stages. Our contributions are twofold. First, we develop a\nframework to create a dataset of NL-TL pairs combining LLMs and human\nannotation. We publish a dataset with 28K NL-TL pairs. Then, we finetune T5\nmodels on the lifted versions (i.e., the specific Atomic Propositions (AP) are\nhidden) of the NL and TL. The enhanced generalizability originates from two\naspects: 1) Usage of lifted NL-TL characterizes common logical structures,\nwithout constraints of specific domains. 2) Application of LLMs in dataset\ncreation largely enhances corpus richness. We test the generalization of\ntrained models on five varied domains. To achieve full NL-TL transformation, we\neither combine the lifted model with AP recognition task or do the further\nfinetuning on each specific domain. During the further finetuning, our model\nachieves higher accuracy (&gt;95%) using only &lt;10% training data, compared with\nthe baseline sequence to sequence (Seq2Seq) model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yongchao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_R/0/1/0/all/0/1\">Rujul Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chuchu Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Answering Complex Questions over Text by Hybrid Question Parsing and Execution. (arXiv:2305.07789v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07789","description":"<p>The dominant paradigm of textual question answering systems is based on\nend-to-end neural networks, which excels at answering natural language\nquestions but falls short on complex ones. This stands in contrast to the broad\nadaptation of semantic parsing approaches over structured data sources (e.g.,\nrelational database, knowledge graphs), that convert natural language questions\nto logical forms and execute them with query engines. Towards combining the\nstrengths of neural and symbolic methods, we propose a framework of question\nparsing and execution on textual QA. It comprises two central pillars: (1) We\nparse the question of varying complexity into an intermediate representation,\nnamed H-expression, which is composed of simple questions as the primitives and\nsymbolic operations representing the relationships among them; (2) To execute\nthe resulting H-expressions, we design a hybrid executor, which integrates the\ndeterministic rules to translate the symbolic operations with a drop-in neural\nreader network to answer each decomposed simple question. Hence, the proposed\nframework can be viewed as a top-down question parsing followed by a bottom-up\nanswer backtracking. The resulting H-expressions closely guide the execution\nprocess, offering higher precision besides better interpretability while still\npreserving the advantages of the neural readers for resolving its primitive\nelements. Our extensive experiments on MuSiQue, 2WikiQA, HotpotQA, and NQ show\nthat the proposed parsing and hybrid execution framework outperforms existing\napproaches in supervised, few-shot, and zero-shot settings, while also\neffectively exposing its underlying reasoning process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1\">Semih Yavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_R/0/1/0/all/0/1\">Rui Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructing Holistic Measures for Social Biases in Masked Language Models. (arXiv:2305.07795v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07795","description":"<p>Masked Language Models (MLMs) have been successful in many natural language\nprocessing tasks. However, real-world stereotype biases are likely to be\nreflected in MLMs due to their learning from large text corpora. Most of the\nevaluation metrics proposed in the past adopt different masking strategies,\ndesigned with the log-likelihood of MLMs. They lack holistic considerations\nsuch as variance for stereotype bias and anti-stereotype bias samples. In this\npaper, the log-likelihoods of stereotype bias and anti-stereotype bias samples\noutput by MLMs are considered Gaussian distributions. Two evaluation metrics,\nKullback Leibler Divergence Score (KLDivS) and Jensen Shannon Divergence Score\n(JSDivS) are proposed to evaluate social biases in MLMs The experimental\nresults on the public datasets StereoSet and CrowS-Pairs demonstrate that\nKLDivS and JSDivS are more stable and interpretable compared to the metrics\nproposed in the past.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yuexian Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACCENT: An Automatic Event Commonsense Evaluation Metric for Open-Domain Dialogue Systems. (arXiv:2305.07797v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07797","description":"<p>Commonsense reasoning is omnipresent in human communications and thus is an\nimportant feature for open-domain dialogue systems. However, evaluating\ncommonsense in dialogue systems is still an open challenge. We take the first\nstep by focusing on event commonsense that considers events and their\nrelations, and is crucial in both dialogues and general commonsense reasoning.\nWe propose ACCENT, an event commonsense evaluation metric empowered by\ncommonsense knowledge bases (CSKBs). ACCENT first extracts event-relation\ntuples from a dialogue, and then evaluates the response by scoring the tuples\nin terms of their compatibility with the CSKB. To evaluate ACCENT, we construct\nthe first public event commonsense evaluation dataset for open-domain\ndialogues. Our experiments show that ACCENT is an efficient metric for event\ncommonsense evaluation, which achieves higher correlations with human judgments\nthan existing baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghazarian_S/0/1/0/all/0/1\">Sarik Ghazarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yijia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1\">Rujun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dr. LLaMA: Improving Small Language Models in Domain-Specific QA via Generative Data Augmentation. (arXiv:2305.07804v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07804","description":"<p>Large Language Models (LLMs) have made significant strides in natural\nlanguage processing but face challenges in terms of computational expense and\ninefficiency as they grow in size, especially in domain-specific tasks. Small\nLanguage Models (SLMs), on the other hand, often struggle in these tasks due to\nlimited capacity and training data. In this paper, we introduce Dr. LLaMA, a\nmethod for improving SLMs through generative data augmentation using LLMs,\nfocusing on medical question-answering tasks and the PubMedQA dataset. Our\nfindings indicate that LLMs effectively refine and diversify existing\nquestion-answer pairs, resulting in improved performance of a much smaller\nmodel on domain-specific QA datasets after fine-tuning. This study highlights\nthe challenges of using LLMs for domain-specific question answering and\nsuggests potential research directions to address these limitations, ultimately\naiming to create more efficient and capable models for specialized\napplications. We have also made our code available for interested researchers\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shangdi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple and Plug-and-play Method for Unsupervised Sentence Representation Enhancement. (arXiv:2305.07824v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07824","description":"<p>Generating proper embedding of sentences through an unsupervised way is\nbeneficial to semantic matching and retrieval problems in real-world scenarios.\nThis paper presents Representation ALchemy (RepAL), an extremely simple\npost-processing method that enhances sentence representations. The basic idea\nin RepAL is to de-emphasize redundant information of sentence embedding\ngenerated by pre-trained models. Through comprehensive experiments, we show\nthat RepAL is free of training and is a plug-and-play method that can be\ncombined with most existing unsupervised sentence learning models. We also\nconducted in-depth analysis to understand RepAL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lingfeng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frequency-aware Dimension Selection for Static Word Embedding by Mixed Product Distance. (arXiv:2305.07826v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07826","description":"<p>Static word embedding is still useful, particularly for context-unavailable\ntasks, because in the case of no context available, pre-trained language models\noften perform worse than static word embeddings. Although dimension is a key\nfactor determining the quality of static word embeddings, automatic dimension\nselection is rarely discussed. In this paper, we investigate the impact of word\nfrequency on the dimension selection, and empirically find that word frequency\nis so vital that it needs to be taken into account during dimension selection.\nBased on such an empirical finding, this paper proposes a dimension selection\nmethod that uses a metric (Mixed Product Distance, MPD) to select a proper\ndimension for word embedding algorithms without training any word embedding.\nThrough applying a post-processing function to oracle matrices, the MPD-based\nmethod can de-emphasize the impact of word frequency. Experiments on both\ncontext-unavailable and context-available tasks demonstrate the better\nefficiency-performance trade-off of our MPD-based dimension selection method\nover baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lingfeng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Geometry of Multilingual Language Models: An Equality Lens. (arXiv:2305.07839v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07839","description":"<p>Understanding the representations of different languages in multilingual\nlanguage models is essential for comprehending their cross-lingual properties,\npredicting their performance on downstream tasks, and identifying any biases\nacross languages. In our study, we analyze the geometry of three multilingual\nlanguage models in Euclidean space and find that all languages are represented\nby unique geometries. Using a geometric separability index we find that\nalthough languages tend to be closer according to their linguistic family, they\nare almost separable with languages from other families. We also introduce a\nCross-Lingual Similarity Index to measure the distance of languages with each\nother in the semantic space. Our findings indicate that the low-resource\nlanguages are not represented as good as high resource languages in any of the\nmodels\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_C/0/1/0/all/0/1\">Cheril Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandak_Y/0/1/0/all/0/1\">Yashashree Chandak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suri_M/0/1/0/all/0/1\">Manan Suri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging History with AI A Comparative Evaluation of GPT 3.5, GPT4, and GoogleBARD in Predictive Accuracy and Fact Checking. (arXiv:2305.07868v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07868","description":"<p>The rapid proliferation of information in the digital era underscores the\nimportance of accurate historical representation and interpretation. While\nartificial intelligence has shown promise in various fields, its potential for\nhistorical fact-checking and gap-filling remains largely untapped. This study\nevaluates the performance of three large language models LLMs GPT 3.5, GPT 4,\nand GoogleBARD in the context of predicting and verifying historical events\nbased on given data. A novel metric, Distance to Reality (DTR), is introduced\nto assess the models' outputs against established historical facts. The results\nreveal a substantial potential for AI in historical studies, with GPT 4\ndemonstrating superior performance. This paper underscores the need for further\nresearch into AI's role in enriching our understanding of the past and bridging\nhistorical knowledge gaps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tasar_D/0/1/0/all/0/1\">Davut Emre Tasar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tasar_C/0/1/0/all/0/1\">Ceren Ocal Tasar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Use Concerns of Generative AI and Large Language Models. (arXiv:2305.07882v1 [cs.CY])","link":"http://arxiv.org/abs/2305.07882","description":"<p>We suggest the implementation of the Dual Use Research of Concern (DURC)\nframework, originally designed for life sciences, to the domain of generative\nAI, with a specific focus on Large Language Models (LLMs). With its\ndemonstrated advantages and drawbacks in biological research, we believe the\nDURC criteria can be effectively redefined for LLMs, potentially contributing\nto improved AI governance. Acknowledging the balance that must be struck when\nemploying the DURC framework, we highlight its crucial political role in\nenhancing societal awareness of the impact of generative AI. As a final point,\nwe offer a series of specific recommendations for applying the DURC approach to\nLLM research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grinbaum_A/0/1/0/all/0/1\">Alexei Grinbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adomaitis_L/0/1/0/all/0/1\">Laurynas Adomaitis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PESTS: Persian_English Cross Lingual Corpus for Semantic Textual Similarity. (arXiv:2305.07893v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07893","description":"<p>One of the components of natural language processing that has received a lot\nof investigation recently is semantic textual similarity. In computational\nlinguistics and natural language processing, assessing the semantic similarity\nof words, phrases, paragraphs, and texts is crucial. Calculating the degree of\nsemantic resemblance between two textual pieces, paragraphs, or phrases\nprovided in both monolingual and cross-lingual versions is known as semantic\nsimilarity. Cross lingual semantic similarity requires corpora in which there\nare sentence pairs in both the source and target languages with a degree of\nsemantic similarity between them. Many existing cross lingual semantic\nsimilarity models use a machine translation due to the unavailability of cross\nlingual semantic similarity dataset, which the propagation of the machine\ntranslation error reduces the accuracy of the model. On the other hand, when we\nwant to use semantic similarity features for machine translation the same\nmachine translations should not be used for semantic similarity. For Persian,\nwhich is one of the low resource languages, no effort has been made in this\nregard and the need for a model that can understand the context of two\nlanguages is felt more than ever. In this article, the corpus of semantic\ntextual similarity between sentences in Persian and English languages has been\nproduced for the first time by using linguistic experts. We named this dataset\nPESTS (Persian English Semantic Textual Similarity). This corpus contains 5375\nsentence pairs. Also, different models based on transformers have been\nfine-tuned using this dataset. The results show that using the PESTS dataset,\nthe Pearson correlation of the XLM ROBERTa model increases from 85.87% to\n95.62%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdous_M/0/1/0/all/0/1\">Mohammad Abdous</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piroozfar_P/0/1/0/all/0/1\">Poorya Piroozfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bidgoli_B/0/1/0/all/0/1\">Behrouz Minaei Bidgoli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Hidden Mystery of OCR in Large Multimodal Models. (arXiv:2305.07895v1 [cs.CV])","link":"http://arxiv.org/abs/2305.07895","description":"<p>Large models have recently played a dominant role in natural language\nprocessing and multimodal vision-language learning. It remains less explored\nabout their efficacy in text-related visual tasks. We conducted a comprehensive\nstudy of existing publicly available multimodal models, evaluating their\nperformance in text recognition, text-based visual question answering, and key\ninformation extraction. Our findings reveal strengths and weaknesses in these\nmodels, which primarily rely on semantic understanding for word recognition and\nexhibit inferior perception of individual character shapes. They also display\nindifference towards text length and have limited capabilities in detecting\nfine-grained features in images. Consequently, these results demonstrate that\neven the current most powerful large multimodal models cannot match\ndomain-specific methods in traditional text tasks and face greater challenges\nin more complex tasks. Most importantly, the baseline results showcased in this\nstudy could provide a foundational framework for the conception and assessment\nof innovative strategies targeted at enhancing zero-shot multimodal techniques.\nEvaluation pipeline will be available at\nhttps://github.com/Yuliang-Liu/MultimodalOCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenwen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Mingxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Dezhi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingrui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-trained Language Model with Prompts for Temporal Knowledge Graph Completion. (arXiv:2305.07912v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07912","description":"<p>Temporal Knowledge graph completion (TKGC) is a crucial task that involves\nreasoning at known timestamps to complete the missing part of facts and has\nattracted more and more attention in recent years. Most existing methods focus\non learning representations based on graph neural networks while inaccurately\nextracting information from timestamps and insufficiently utilizing the implied\ninformation in relations. To address these problems, we propose a novel TKGC\nmodel, namely Pre-trained Language Model with Prompts for TKGC (PPT). We\nconvert a series of sampled quadruples into pre-trained language model inputs\nand convert intervals between timestamps into different prompts to make\ncoherent sentences with implicit semantic information. We train our model with\na masking strategy to convert TKGC task into a masked token prediction task,\nwhich can leverage the semantic information in pre-trained language models.\nExperiments on three benchmark datasets and extensive analysis demonstrate that\nour model has great competitiveness compared to other models with four metrics.\nOur model can effectively incorporate information from temporal knowledge\ngraphs into the language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenjie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Ben Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1\">Miao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1\">Min Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeT5+: Open Code Large Language Models for Code Understanding and Generation. (arXiv:2305.07922v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07922","description":"<p>Large language models (LLMs) pretrained on vast source code have achieved\nprominent progress in code intelligence. However, existing code LLMs have two\nmain limitations in terms of architecture and pretraining tasks. First, they\noften adopt a specific architecture (encoder-only or decoder-only) or rely on a\nunified encoder-decoder network for different downstream tasks. The former\nparadigm is limited by inflexibility in applications while in the latter, the\nmodel is treated as a single system for all tasks, leading to suboptimal\nperformance on a subset of tasks. Secondly, they often employ a limited set of\npretraining objectives which might not be relevant to some downstream tasks and\nhence result in substantial performance degrade. To address these limitations,\nwe propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which\ncomponent modules can be flexibly combined to suit a wide range of downstream\ncode tasks. Such flexibility is enabled by our proposed mixture of pretraining\nobjectives to mitigate the pretrain-finetune discrepancy. These objectives\ncover span denoising, contrastive learning, text-code matching, and causal LM\npretraining tasks, on both unimodal and bimodal multilingual code corpora.\nFurthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs\nwithout training from scratch to efficiently scale up our models, and explore\ninstruction-tuning to align with natural language instructions. We extensively\nevaluate CodeT5+ on over 20 code-related benchmarks in different settings,\nincluding zero-shot, finetuning, and instruction-tuning. We observe\nstate-of-the-art (SoTA) model performance on various code-related tasks, such\nas code generation and completion, math programming, and text-to-code retrieval\ntasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA\nresults on HumanEval code generation task against other open code LLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gotmare_A/0/1/0/all/0/1\">Akhilesh Deepak Gotmare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_N/0/1/0/all/0/1\">Nghi D.Q. Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RC3: Regularized Contrastive Cross-lingual Cross-modal Pre-training. (arXiv:2305.07927v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07927","description":"<p>Multilingual vision-language (V&amp;L) pre-training has achieved remarkable\nprogress in learning universal representations across different modalities and\nlanguages. In spite of recent success, there still remain challenges limiting\nfurther improvements of V&amp;L pre-trained models in multilingual settings.\nParticularly, current V&amp;L pre-training methods rely heavily on strictly-aligned\nmultilingual image-text pairs generated from English-centric datasets through\nmachine translation. However, the cost of collecting and translating such\nstrictly-aligned datasets is usually unbearable. In this paper, we propose\nRegularized Contrastive Cross-lingual Cross-modal (RC^3) pre-training, which\nfurther exploits more abundant weakly-aligned multilingual image-text pairs.\nSpecifically, we design a regularized cross-lingual visio-textual contrastive\nlearning objective that constrains the representation proximity of\nweakly-aligned visio-textual inputs according to textual relevance. Besides,\nexisting V&amp;L pre-training approaches mainly deal with visual inputs by either\nregion-of-interest (ROI) features or patch embeddings. We flexibly integrate\nthe two forms of visual features into our model for pre-training and downstream\nmulti-modal tasks. Extensive experiments on 5 downstream multi-modal tasks\nacross 6 languages demonstrate the effectiveness of our proposed method over\ncompetitive contrast models with stronger zero-shot capability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chulun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AMTSS: An Adaptive Multi-Teacher Single-Student Knowledge Distillation Framework For Multilingual Language Inference. (arXiv:2305.07928v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07928","description":"<p>Knowledge distillation is of key importance to launching multilingual\npre-trained language models for real applications. To support cost-effective\nlanguage inference in multilingual settings, we propose AMTSS, an adaptive\nmulti-teacher single-student distillation framework, which allows distilling\nknowledge from multiple teachers to a single student. We first introduce an\nadaptive learning strategy and teacher importance weight, which enables a\nstudent to effectively learn from max-margin teachers and easily adapt to new\nlanguages. Moreover, we present a shared student encoder with different\nprojection layers in support of multiple languages, which contributes to\nlargely reducing development and machine cost. Experimental results show that\nAMTSS gains competitive results on the public XNLI dataset and the realistic\nindustrial dataset AliExpress (AE) in the E-commerce scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qianglong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_F/0/1/0/all/0/1\">Feng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Feng-Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guohai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Large Language Models in Conversational Recommender Systems. (arXiv:2305.07961v1 [cs.IR])","link":"http://arxiv.org/abs/2305.07961","description":"<p>A Conversational Recommender System (CRS) offers increased transparency and\ncontrol to users by enabling them to engage with the system through a real-time\nmulti-turn dialogue. Recently, Large Language Models (LLMs) have exhibited an\nunprecedented ability to converse naturally and incorporate world knowledge and\ncommon-sense reasoning into language understanding, unlocking the potential of\nthis paradigm. However, effectively leveraging LLMs within a CRS introduces new\ntechnical challenges, including properly understanding and controlling a\ncomplex conversation and retrieving from external sources of information. These\nissues are exacerbated by a large, evolving item corpus and a lack of\nconversational data for training. In this paper, we provide a roadmap for\nbuilding an end-to-end large-scale CRS using LLMs. In particular, we propose\nnew implementations for user preference understanding, flexible dialogue\nmanagement and explainable recommendations as part of an integrated\narchitecture powered by LLMs. For improved personalization, we describe how an\nLLM can consume interpretable natural language user profiles and use them to\nmodulate session-level context. To overcome conversational data limitations in\nthe absence of an existing production CRS, we propose techniques for building a\ncontrollable LLM-based user simulator to generate synthetic conversations. As a\nproof of concept we introduce RecLLM, a large-scale CRS for YouTube videos\nbuilt on LaMDA, and demonstrate its fluency and diverse functionality through\nsome illustrative example conversations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Friedman_L/0/1/0/all/0/1\">Luke Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_S/0/1/0/all/0/1\">Sameer Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allen_D/0/1/0/all/0/1\">David Allen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Terry Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidahmed_H/0/1/0/all/0/1\">Hakim Sidahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Changbo Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubiner_G/0/1/0/all/0/1\">Gabriel Schubiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Ajay Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lara_H/0/1/0/all/0/1\">Harsh Lara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_B/0/1/0/all/0/1\">Brian Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zexi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_M/0/1/0/all/0/1\">Manoj Tiwari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content. (arXiv:2305.07969v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07969","description":"<p>This paper presents a novel approach for detecting ChatGPT-generated vs.\nhuman-written text using language models. To this end, we first collected and\nreleased a pre-processed dataset named OpenGPTText, which consists of rephrased\ncontent generated using ChatGPT. We then designed, implemented, and trained two\ndifferent models for text classification, using Robustly Optimized BERT\nPretraining Approach (RoBERTa) and Text-to-Text Transfer Transformer (T5),\nrespectively. Our models achieved remarkable results, with an accuracy of over\n97% on the test dataset, as evaluated through various metrics. Furthermore, we\nconducted an interpretability study to showcase our model's ability to extract\nand differentiate key features between human-written and ChatGPT-generated\ntext. Our findings provide important insights into the effective use of\nlanguage models to detect generated text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yutian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hao Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_V/0/1/0/all/0/1\">Vivian Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rita Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_B/0/1/0/all/0/1\">Bhiksha Ramakrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trillion Dollar Words: A New Financial Dataset, Task & Market Analysis. (arXiv:2305.07972v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07972","description":"<p>Monetary policy pronouncements by Federal Open Market Committee (FOMC) are a\nmajor driver of financial market returns. We construct the largest tokenized\nand annotated dataset of FOMC speeches, meeting minutes, and press conference\ntranscripts in order to understand how monetary policy influences financial\nmarkets. In this study, we develop a novel task of hawkish-dovish\nclassification and benchmark various pre-trained language models on the\nproposed dataset. Using the best-performing model (RoBERTa-large), we construct\na measure of monetary policy stance for the FOMC document release days. To\nevaluate the constructed measure, we study its impact on the treasury market,\nstock market, and macroeconomic indicators. Our dataset, models, and code are\npublicly available on Huggingface and GitHub under CC BY-NC 4.0 license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Agam Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paturi_S/0/1/0/all/0/1\">Suvan Paturi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chava_S/0/1/0/all/0/1\">Sudheer Chava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Faithful Factual Error Correction. (arXiv:2305.07982v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07982","description":"<p>Faithfully correcting factual errors is critical for maintaining the\nintegrity of textual knowledge bases and preventing hallucinations in\nsequence-to-sequence models. Drawing on humans' ability to identify and correct\nfactual errors, we present a zero-shot framework that formulates questions\nabout input claims, looks for correct answers in the given evidence, and\nassesses the faithfulness of each correction based on its consistency with the\nevidence. Our zero-shot framework outperforms fully-supervised approaches, as\ndemonstrated by experiments on the FEVER and SciFact datasets, where our\noutputs are shown to be more faithful. More importantly, the decomposability\nnature of our framework inherently provides interpretability. Additionally, to\nreveal the most suitable metrics for evaluating factual error corrections, we\nanalyze the correlation between commonly used metrics with human judgments in\nterms of three different dimensions regarding intelligibility and faithfulness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kung-Hsiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1\">Hou Pong Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCENE: Self-Labeled Counterfactuals for Extrapolating to Negative Examples. (arXiv:2305.07984v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07984","description":"<p>Detecting negatives (such as non-entailment relationships, unanswerable\nquestions, and false claims) is an important and challenging aspect of many\nnatural language understanding tasks. Though manually collecting challenging\nnegative examples can help models detect them, it is both costly and\ndomain-specific. In this work, we propose Self-labeled Counterfactuals for\nExtrapolating to Negative Examples (SCENE), an automatic method for\nsynthesizing training data that greatly improves models' ability to detect\nchallenging negative examples. In contrast with standard data augmentation,\nwhich synthesizes new examples for existing labels, SCENE can synthesize\nnegative examples zero-shot from only positive ones. Given a positive example,\nSCENE perturbs it with a mask infilling model, then determines whether the\nresulting example is negative based on a self-training heuristic. With access\nto only answerable training examples, SCENE can close 69.6% of the performance\ngap on SQuAD 2.0, a dataset where half of the evaluation examples are\nunanswerable, compared to a model trained on SQuAD 2.0. Our method also extends\nto boolean question answering and recognizing textual entailment, and improves\ngeneralization from SQuAD to ACE-whQA, an out-of-domain extractive QA\nbenchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1\">Deqing Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godbole_A/0/1/0/all/0/1\">Ameya Godbole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Sentence Compression for Meeting Summarization. (arXiv:2305.07988v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07988","description":"<p>The conventional summarization model often fails to capture critical\ninformation in meeting transcripts, as meeting corpus usually involves multiple\nparties with lengthy conversations and is stuffed with redundant and trivial\ncontent. To tackle this problem, we present SVB, an effective and efficient\nframework for meeting summarization that `compress' the redundancy while\npreserving important content via three processes: sliding-window dialogue\nrestoration and \\textbf{S}coring, channel-wise importance score\n\\textbf{V}oting, and relative positional \\textbf{B}ucketing. Specifically,\nunder the self-supervised paradigm, the sliding-window scoring aims to rate the\nimportance of each token from multiple views. Then these ratings are aggregated\nby channel-wise voting. Tokens with high ratings will be regarded as salient\ninformation and labeled as \\textit{anchors}. Finally, to tailor the lengthy\ninput to an acceptable length for the language model, the relative positional\nbucketing algorithm is performed to retain the anchors while compressing other\nirrelevant contents in different granularities. Without large-scale\npre-training or expert-grade annotating tools, our proposed method outperforms\nprevious state-of-the-art approaches. A vast amount of evaluations and analyses\nare conducted to prove the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Haochen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_M/0/1/0/all/0/1\">Mingjie Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zhaohui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linqi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Previously Fact-Checked Claim Retrieval. (arXiv:2305.07991v1 [cs.CL])","link":"http://arxiv.org/abs/2305.07991","description":"<p>Fact-checkers are often hampered by the sheer amount of online content that\nneeds to be fact-checked. NLP can help them by retrieving already existing\nfact-checks relevant to the content being investigated. This paper introduces a\nnew multilingual dataset -- MultiClaim -- for previously fact-checked claim\nretrieval. We collected 28k posts in 27 languages from social media, 206k\nfact-checks in 39 languages written by professional fact-checkers, as well as\n31k connections between these two groups. This is the most extensive and the\nmost linguistically diverse dataset of this kind to date. We evaluated how\ndifferent unsupervised methods fare on this dataset and its various dimensions.\nWe show that evaluating such a diverse dataset has its complexities and proper\ncare needs to be taken before interpreting the results. We also evaluated a\nsupervised fine-tuning approach, improving upon the unsupervised method\nsignificantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pikuliak_M/0/1/0/all/0/1\">Mat&#xfa;&#x161; Pikuliak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srba_I/0/1/0/all/0/1\">Ivan Srba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moro_R/0/1/0/all/0/1\">Robert Moro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hromadka_T/0/1/0/all/0/1\">Timo Hromadka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smolen_T/0/1/0/all/0/1\">Timotej Smolen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melisek_M/0/1/0/all/0/1\">Martin Melisek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vykopal_I/0/1/0/all/0/1\">Ivan Vykopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simko_J/0/1/0/all/0/1\">Jakub Simko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Podrouzek_J/0/1/0/all/0/1\">Juraj Podrouzek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielikova_M/0/1/0/all/0/1\">Maria Bielikova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond the Safeguards: Exploring the Security Risks of ChatGPT. (arXiv:2305.08005v1 [cs.CR])","link":"http://arxiv.org/abs/2305.08005","description":"<p>The increasing popularity of large language models (LLMs) such as ChatGPT has\nled to growing concerns about their safety, security risks, and ethical\nimplications. This paper aims to provide an overview of the different types of\nsecurity risks associated with ChatGPT, including malicious text and code\ngeneration, private data disclosure, fraudulent services, information\ngathering, and producing unethical content. We present an empirical study\nexamining the effectiveness of ChatGPT's content filters and explore potential\nways to bypass these safeguards, demonstrating the ethical implications and\nsecurity risks that persist in LLMs even when protections are in place. Based\non a qualitative analysis of the security implications, we discuss potential\nstrategies to mitigate these risks and inform researchers, policymakers, and\nindustry professionals about the complex security challenges posed by LLMs like\nChatGPT. This study contributes to the ongoing discussion on the ethical and\nsecurity implications of LLMs, underscoring the need for continued research in\nthis area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Derner_E/0/1/0/all/0/1\">Erik Derner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batistic_K/0/1/0/all/0/1\">Kristina Batisti&#x10d;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProKnow: Process Knowledge for Safety Constrained and Explainable Question Generation for Mental Health Diagnostic Assistance. (arXiv:2305.08010v1 [cs.CL])","link":"http://arxiv.org/abs/2305.08010","description":"<p>Current Virtual Mental Health Assistants (VMHAs) provide counseling and\nsuggestive care. They refrain from patient diagnostic assistance because they\nlack training in safety-constrained and specialized clinical process knowledge.\nIn this work, we define Proknow as an ordered set of information that maps to\nevidence-based guidelines or categories of conceptual understanding to experts\nin a domain. We also introduce a new dataset of diagnostic conversations guided\nby safety constraints and Proknow that healthcare professionals use. We develop\na method for natural language question generation (NLG) that collects\ndiagnostic information from the patient interactively. We demonstrate the\nlimitations of using state-of-the-art large-scale language models (LMs) on this\ndataset. Our algorithm models the process knowledge through explicitly modeling\nsafety, knowledge capture, and explainability. LMs augmented with ProKnow\nguided method generated 89% safer questions in the depression and anxiety\ndomain. The Explainability of the generated question is assessed by computing\nsimilarity with concepts in depression and anxiety knowledge bases. Overall,\nirrespective of the type of LMs augmented with our ProKnow, we achieved an\naverage 82% improvement over simple pre-trained LMs on safety, explainability,\nand process-guided question generation. We qualitatively and quantitatively\nevaluate the efficacy of the proposed ProKnow-guided methods by introducing\nthree new evaluation metrics for safety, explainability, and process knowledge\nadherence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_M/0/1/0/all/0/1\">Manas Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltani_M/0/1/0/all/0/1\">Misagh Soltani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawte_V/0/1/0/all/0/1\">Vipula Rawte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1\">Ashwin Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-aware Dynamic Retrospective-Prospective Reasoning for Event-level Video Question Answering. (arXiv:2305.08059v1 [cs.CV])","link":"http://arxiv.org/abs/2305.08059","description":"<p>Event-Level Video Question Answering (EVQA) requires complex reasoning across\nvideo events to obtain the visual information needed to provide optimal\nanswers. However, despite significant progress in model performance, few\nstudies have focused on using the explicit semantic connections between the\nquestion and visual information especially at the event level. There is need\nfor using such semantic connections to facilitate complex reasoning across\nvideo frames. Therefore, we propose a semantic-aware dynamic\nretrospective-prospective reasoning approach for video-based question\nanswering. Specifically, we explicitly use the Semantic Role Labeling (SRL)\nstructure of the question in the dynamic reasoning process where we decide to\nmove to the next frame based on which part of the SRL structure (agent, verb,\npatient, etc.) of the question is being focused on. We conduct experiments on a\nbenchmark EVQA dataset - TrafficQA. Results show that our proposed approach\nachieves superior performance compared to previous state-of-the-art models. Our\ncode will be made publicly available for research use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1\">Chenyang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_T/0/1/0/all/0/1\">Tianbo Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graham_Y/0/1/0/all/0/1\">Yvette Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1\">Jennifer Foster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving End-to-End SLU performance with Prosodic Attention and Distillation. (arXiv:2305.08067v1 [cs.CL])","link":"http://arxiv.org/abs/2305.08067","description":"<p>Most End-to-End SLU methods depend on the pretrained ASR or language model\nfeatures for intent prediction. However, other essential information in speech,\nsuch as prosody, is often ignored. Recent research has shown improved results\nin classifying dialogue acts by incorporating prosodic information. The margins\nof improvement in these methods are minimal as the neural models ignore\nprosodic features. In this work, we propose prosody-attention, which uses the\nprosodic features differently to generate attention maps across time frames of\nthe utterance. Then we propose prosody-distillation to explicitly learn the\nprosodic information in the acoustic encoder rather than concatenating the\nimplicit prosodic features. Both the proposed methods improve the baseline\nresults, and the prosody-distillation method gives an intent classification\naccuracy improvement of 8\\% and 2\\% on SLURP and STOP datasets over the prosody\nbaseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajaa_S/0/1/0/all/0/1\">Shangeth Rajaa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Make Prompt-based Black-Box Tuning Colorful: Boosting Model Generalization from Three Orthogonal Perspectives. (arXiv:2305.08088v1 [cs.CL])","link":"http://arxiv.org/abs/2305.08088","description":"<p>Large language models (LLMs) have shown increasing power on various natural\nlanguage processing (NLP) tasks. However, tuning these models for downstream\ntasks usually needs exorbitant costs or is unavailable due to commercial\nconsiderations. Recently, black-box tuning has been proposed to address this\nproblem by optimizing task-specific prompts without accessing the gradients and\nhidden representations. However, most existing works have yet fully exploited\nthe potential of gradient-free optimization under the scenario of few-shot\nlearning. In this paper, we describe BBT-RGB, a suite of straightforward and\ncomplementary techniques for enhancing the efficiency and performance of\nblack-box optimization. Specifically, our method includes three plug-and-play\ncomponents: (1) Two-stage derivative-free optimization strategy that\nfacilitates fast convergence and mitigates overfitting; (2) Automatic\nverbalizer construction with its novel usage under few-shot settings; (3)\nBetter prompt initialization policy based on instruction search and\nauto-selected demonstration. Extensive experiments across various tasks on\nnatural language understanding and inference demonstrate the effectiveness of\nour method. Our codes are publicly available at\nhttps://github.com/QiushiSun/BBT-RGB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qiushi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chengcheng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Renyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_J/0/1/0/all/0/1\">Jingyang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Ming Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Understanding and Improving Knowledge Distillation for Neural Machine Translation. (arXiv:2305.08096v1 [cs.CL])","link":"http://arxiv.org/abs/2305.08096","description":"<p>Knowledge distillation (KD) is a promising technique for model compression in\nneural machine translation. However, where the knowledge hides in KD is still\nnot clear, which may hinder the development of KD. In this work, we first\nunravel this mystery from an empirical perspective and show that the knowledge\ncomes from the top-1 predictions of teachers, which also helps us build a\npotential connection between word- and sequence-level KD. Further, we point out\ntwo inherent issues in vanilla word-level KD based on this finding. Firstly,\nthe current objective of KD spreads its focus to whole distributions to learn\nthe knowledge, yet lacks special treatment on the most crucial top-1\ninformation. Secondly, the knowledge is largely covered by the golden\ninformation due to the fact that most top-1 predictions of teachers overlap\nwith ground-truth tokens, which further restricts the potential of KD. To\naddress these issues, we propose a novel method named \\textbf{T}op-1\n\\textbf{I}nformation \\textbf{E}nhanced \\textbf{K}nowledge \\textbf{D}istillation\n(TIE-KD). Specifically, we design a hierarchical ranking loss to enforce the\nlearning of the top-1 information from the teacher. Additionally, we develop an\niterative KD procedure to infuse more additional knowledge by distilling on the\ndata without ground-truth targets. Experiments on WMT'14 English-German, WMT'14\nEnglish-French and WMT'16 English-Romanian demonstrate that our method can\nrespectively boost Transformer$_{base}$ students by +1.04, +0.60 and +1.11 BLEU\nscores and significantly outperform the vanilla word-level KD baseline.\nBesides, our method shows higher generalizability on different teacher-student\ncapacity gaps than existing KD techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuaibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wenjuan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Neural Factor Analysis for Disentangling Utterance-level Speech Representations. (arXiv:2305.08099v1 [cs.SD])","link":"http://arxiv.org/abs/2305.08099","description":"<p>Self-supervised learning (SSL) speech models such as wav2vec and HuBERT have\ndemonstrated state-of-the-art performance on automatic speech recognition (ASR)\nand proved to be extremely useful in low label-resource settings. However, the\nsuccess of SSL models has yet to transfer to utterance-level tasks such as\nspeaker, emotion, and language recognition, which still require supervised\nfine-tuning of the SSL models to obtain good performance. We argue that the\nproblem is caused by the lack of disentangled representations and an\nutterance-level learning objective for these tasks. Inspired by how HuBERT uses\nclustering to discover hidden acoustic units, we formulate a factor analysis\n(FA) model that uses the discovered hidden acoustic units to align the SSL\nfeatures. The underlying utterance-level representations are disentangled from\nthe content of speech using probabilistic inference on the aligned features.\nFurthermore, the variational lower bound derived from the FA model provides an\nutterance-level objective, allowing error gradients to be backpropagated to the\nTransformer layers to learn highly discriminative acoustic units. When used in\nconjunction with HuBERT's masked prediction training, our models outperform the\ncurrent best model, WavLM, on all utterance-level non-semantic tasks on the\nSUPERB benchmark with only 20% of labeled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiwei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chenhang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mak_M/0/1/0/all/0/1\">Man-Wai Mak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1\">Youzhi Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distinguish Before Answer: Generating Contrastive Explanation as Knowledge for Commonsense Question Answering. (arXiv:2305.08135v1 [cs.CL])","link":"http://arxiv.org/abs/2305.08135","description":"<p>Existing knowledge-enhanced methods have achieved remarkable results in\ncertain QA tasks via obtaining diverse knowledge from different knowledge\nbases. However, limited by the properties of retrieved knowledge, they still\nhave trouble benefiting from both the knowledge relevance and distinguishment\nsimultaneously. To address the challenge, we propose CPACE, a Concept-centric\nPrompt-bAsed Contrastive Explanation Generation model, which aims to convert\nobtained symbolic knowledge into a contrastive explanation for better\ndistinguishing the differences among given candidates. Firstly, following\nprevious works, we retrieve different types of symbolic knowledge with a\nconcept-centric knowledge extraction module. After that, we generate\ncorresponding contrastive explanations using acquired symbolic knowledge and\nexplanation prompts as guidance for better modeling the knowledge\ndistinguishment and interpretability. Finally, we regard the generated\ncontrastive explanation as external knowledge for downstream task enhancement.\nWe conduct a series of experiments on three widely-used question-answering\ndatasets: CSQA, QASC, and OBQA. Experimental results demonstrate that with the\nhelp of generated contrastive explanation, our CPACE model achieves new SOTA on\nCSQA (89.8% on the testing set, 0.9% higher than human performance), and gains\nimpressive improvement on QASC and OBQA (4.2% and 3.5%, respectively).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qianglong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guohai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ParaLS: Lexical Substitution via Pretrained Paraphraser. (arXiv:2305.08146v1 [cs.CL])","link":"http://arxiv.org/abs/2305.08146","description":"<p>Lexical substitution (LS) aims at finding appropriate substitutes for a\ntarget word in a sentence. Recently, LS methods based on pretrained language\nmodels have made remarkable progress, generating potential substitutes for a\ntarget word through analysis of its contextual surroundings. However, these\nmethods tend to overlook the preservation of the sentence's meaning when\ngenerating the substitutes. This study explores how to generate the substitute\ncandidates from a paraphraser, as the generated paraphrases from a paraphraser\ncontain variations in word choice and preserve the sentence's meaning. Since we\ncannot directly generate the substitutes via commonly used decoding strategies,\nwe propose two simple decoding strategies that focus on the variations of the\ntarget word during decoding. Experimental results show that our methods\noutperform state-of-the-art LS methods based on pre-trained language models on\nthree benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiang_J/0/1/0/all/0/1\">Jipeng Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yunhao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STORYWARS: A Dataset and Instruction Tuning Baselines for Collaborative Story Understanding and Generation. (arXiv:2305.08152v1 [cs.CL])","link":"http://arxiv.org/abs/2305.08152","description":"<p>Collaborative stories, which are texts created through the collaborative\nefforts of multiple authors with different writing styles and intentions, pose\nunique challenges for NLP models. Understanding and generating such stories\nremains an underexplored area due to the lack of open-domain corpora. To\naddress this, we introduce STORYWARS, a new dataset of over 40,000\ncollaborative stories written by 9,400 different authors from an online\nplatform. We design 12 task types, comprising 7 understanding and 5 generation\ntask types, on STORYWARS, deriving 101 diverse story-related tasks in total as\na multi-task benchmark covering all fully-supervised, few-shot, and zero-shot\nscenarios. Furthermore, we present our instruction-tuned model, INSTRUCTSTORY,\nfor the story tasks showing that instruction tuning, in addition to achieving\nsuperior results in zero-shot and few-shot scenarios, can also obtain the best\nperformance on the fully-supervised tasks in STORYWARS, establishing strong\nmulti-task benchmark performances on STORYWARS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yulun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chilton_L/0/1/0/all/0/1\">Lydia Chilton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Croatian Film Review Dataset (Cro-FiReDa): A Sentiment Annotated Dataset of Film Reviews. (arXiv:2305.08173v1 [cs.CL])","link":"http://arxiv.org/abs/2305.08173","description":"<p>This paper introduces Cro-FiReDa, a sentiment-annotated dataset for Croatian\nin the domain of movie reviews. The dataset, which contains over 10,000\nsentences, has been annotated at the sentence level. In addition to presenting\nthe overall annotation process, we also present benchmark results based on the\ntransformer-based fine-tuning approach\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_G/0/1/0/all/0/1\">Gaurish Thakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preradovic_N/0/1/0/all/0/1\">Nives Mikelic Preradovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadic_M/0/1/0/all/0/1\">Marko Tadi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CroSentiNews 2.0: A Sentence-Level News Sentiment Corpus. (arXiv:2305.08187v1 [cs.CL])","link":"http://arxiv.org/abs/2305.08187","description":"<p>This article presents a sentence-level sentiment dataset for the Croatian\nnews domain. In addition to the 3K annotated texts already present, our dataset\ncontains 14.5K annotated sentence occurrences that have been tagged with 5\nclasses. We provide baseline scores in addition to the annotation process and\ninter-annotator agreement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_G/0/1/0/all/0/1\">Gaurish Thakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preradovic_N/0/1/0/all/0/1\">Nives Mikelic Preradovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadic_M/0/1/0/all/0/1\">Marko Tadi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Simulate Natural Language Feedback for Interactive Semantic Parsing. (arXiv:2305.08195v1 [cs.CL])","link":"http://arxiv.org/abs/2305.08195","description":"<p>Interactive semantic parsing based on natural language (NL) feedback, where\nusers provide feedback to correct the parser mistakes, has emerged as a more\npractical scenario than the traditional one-shot semantic parsing. However,\nprior work has heavily relied on human-annotated feedback data to train the\ninteractive semantic parser, which is prohibitively expensive and not scalable.\nIn this work, we propose a new task of simulating NL feedback for interactive\nsemantic parsing. We accompany the task with a novel feedback evaluator. The\nevaluator is specifically designed to assess the quality of the simulated\nfeedback, based on which we decide the best feedback simulator from our\nproposed variants. On a text-to-SQL dataset, we show that our feedback\nsimulator can generate high-quality NL feedback to boost the error correction\nability of a specific parser. In low-data settings, our feedback simulator can\nhelp achieve comparable error correction performance as trained using the\ncostly, full set of human annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Saurabh Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Yintao Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sida I. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Ziyu Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Cognitive Stimulation Dialogue System with Multi-source Knowledge Fusion for Elders with Cognitive Impairment. (arXiv:2305.08200v1 [cs.CL])","link":"http://arxiv.org/abs/2305.08200","description":"<p>When communicating with elders with cognitive impairment, cognitive\nstimulation (CS) help to maintain the cognitive health of elders. Data sparsity\nis the main challenge in building CS-based dialogue systems, particularly in\nthe Chinese language. To fill this gap, we construct a Chinese CS conversation\n(CSConv) dataset, which contains about 2.6K groups of dialogues with CS\nprinciples and emotional support strategy labels. Making chit chat while\nproviding emotional support is overlooked by the majority of existing cognitive\ndialogue systems. In this paper, we propose a multi-source knowledge fusion\nmethod for CS dialogue (CSD), to generate open-ended responses guided by the CS\nprinciple and emotional support strategy. We first use a progressive mask\nmethod based on external knowledge to learn encoders as effective classifiers,\nwhich is the prerequisite to predict the CS principle and emotional support\nstrategy of the target response. Then a decoder interacts with the perceived CS\nprinciple and emotional support strategy to generate responses. Extensive\nexperiments conducted on the CSConv dataset demonstrate the effectiveness of\nthe proposed method, while there is still a large space for improvement\ncompared to human performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jiyue Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qintong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Generalize for Cross-domain QA. (arXiv:2305.08208v1 [cs.CL])","link":"http://arxiv.org/abs/2305.08208","description":"<p>There have been growing concerns regarding the out-of-domain generalization\nability of natural language processing (NLP) models, particularly in\nquestion-answering (QA) tasks. Current synthesized data augmentation methods\nfor QA are hampered by increased training costs. To address this issue, we\npropose a novel approach that combines prompting methods and linear probing\nthen fine-tuning strategy, which does not entail additional cost. Our method\nhas been theoretically and empirically shown to be effective in enhancing the\ngeneralization ability of both generative and discriminative models. Our\napproach outperforms state-of-the-art baselines, with an average increase in F1\nscore of 4.5%-7.9%. Furthermore, our method can be easily integrated into any\npre-trained models and offers a promising solution to the under-explored\ncross-domain QA task. We release our source code at GitHub*.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yingjie Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Ruihai Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepFilterNet: Perceptually Motivated Real-Time Speech Enhancement. (arXiv:2305.08227v1 [eess.AS])","link":"http://arxiv.org/abs/2305.08227","description":"<p>Multi-frame algorithms for single-channel speech enhancement are able to take\nadvantage from short-time correlations within the speech signal. Deep Filtering\n(DF) was proposed to directly estimate a complex filter in frequency domain to\ntake advantage of these correlations. In this work, we present a real-time\nspeech enhancement demo using DeepFilterNet. DeepFilterNet's efficiency is\nenabled by exploiting domain knowledge of speech production and psychoacoustic\nperception. Our model is able to match state-of-the-art speech enhancement\nbenchmarks while achieving a real-time-factor of 0.19 on a single threaded\nnotebook CPU. The framework as well as pretrained weights have been published\nunder an open source license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Schroter_H/0/1/0/all/0/1\">Hendrik Schr&#xf6;ter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rosenkranz_T/0/1/0/all/0/1\">Tobias Rosenkranz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Escalante_B%2E_A/0/1/0/all/0/1\">Alberto N. Escalante-B.</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Attentive Residual Networks for Argument Mining. (arXiv:2102.12227v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.12227","description":"<p>We explore the use of residual networks and neural attention for multiple\nargument mining tasks. We propose a residual architecture that exploits\nattention, multi-task learning, and makes use of ensemble, without any\nassumption on document or argument structure. We present an extensive\nexperimental evaluation on five different corpora of user-generated comments,\nscientific publications, and persuasive essays. Our results show that our\napproach is a strong competitor against state-of-the-art architectures with a\nhigher computational footprint or corpus-specific design, representing an\ninteresting compromise between generality, performance accuracy and reduced\nmodel size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galassi_A/0/1/0/all/0/1\">Andrea Galassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippi_M/0/1/0/all/0/1\">Marco Lippi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torroni_P/0/1/0/all/0/1\">Paolo Torroni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quinductor: a multilingual data-driven method for generating reading-comprehension questions using Universal Dependencies. (arXiv:2103.10121v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.10121","description":"<p>We propose a multilingual data-driven method for generating reading\ncomprehension questions using dependency trees. Our method provides a strong,\nmostly deterministic, and inexpensive-to-train baseline for less-resourced\nlanguages. While a language-specific corpus is still required, its size is\nnowhere near those required by modern neural question generation (QG)\narchitectures. Our method surpasses QG baselines previously reported in the\nliterature and shows a good performance in terms of human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalpakchi_D/0/1/0/all/0/1\">Dmytro Kalpakchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boye_J/0/1/0/all/0/1\">Johan Boye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uni-Encoder: A Fast and Accurate Response Selection Paradigm for Generation-Based Dialogue Systems. (arXiv:2106.01263v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.01263","description":"<p>Sample-and-rank is a key decoding strategy for modern generation-based\ndialogue systems. It helps achieve diverse and high-quality responses by\nselecting an answer from a small pool of generated candidates. The current\nstate-of-the-art ranking methods mainly use an encoding paradigm called\nCross-Encoder, which separately encodes each context-candidate pair and ranks\nthe candidates according to their fitness scores. However, Cross-Encoder\nrepeatedly encodes the same lengthy context for each candidate, resulting in\nhigh computational costs. Poly-Encoder addresses the above problems by reducing\nthe interaction between context and candidates, but with a price of performance\ndrop. In this work, we develop a new paradigm called Uni-Encoder, that keeps\nthe full attention over each pair as in Cross-Encoder while only encoding the\ncontext once, as in Poly-Encoder. Uni-Encoder encodes all the candidates with\nthe context in one forward pass. We use the same positional embedding for all\ncandidates to ensure they are treated equally and design a new attention\nmechanism to avoid confusion. Our Uni-Encoder can simulate other ranking\nparadigms using different attention and response concatenation methods.\nExtensive experiments show that our proposed paradigm achieves new\nstate-of-the-art results on four benchmark datasets with high computational\nefficiency. For instance, it improves R10@1 by 2.9% with an approximately 4X\nfaster inference speed on the Ubuntu V2 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chiyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haofei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1\">Pengfei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Leyang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1\">Zhenzhong Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AxoNN: An asynchronous, message-driven parallel framework for extreme-scale deep learning. (arXiv:2110.13005v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.13005","description":"<p>In the last few years, the memory requirements to train state-of-the-art\nneural networks have far exceeded the DRAM capacities of modern hardware\naccelerators. This has necessitated the development of efficient algorithms to\ntrain these neural networks in parallel on large-scale GPU-based clusters.\nSince computation is relatively inexpensive on modern GPUs, designing and\nimplementing extremely efficient communication in these parallel training\nalgorithms is critical for extracting the maximum performance. This paper\npresents AxoNN, a parallel deep learning framework that exploits asynchrony and\nmessage-driven execution to schedule neural network operations on each GPU,\nthereby reducing GPU idle time and maximizing hardware efficiency. By using the\nCPU memory as a scratch space for offloading data periodically during training,\nAxoNN is able to reduce GPU memory consumption by four times. This allows us to\nincrease the number of parameters per GPU by four times, thus reducing the\namount of communication and increasing performance by over 13%. When tested\nagainst large transformer models with 12-100 billion parameters on 48-384\nNVIDIA Tesla V100 GPUs, AxoNN achieves a per-GPU throughput of 49.4-54.78% of\ntheoretical peak and reduces the training time by 22-37 days (15-25% speedup)\nas compared to the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Siddharth Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatele_A/0/1/0/all/0/1\">Abhinav Bhatele</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PADA: Pruning Assisted Domain Adaptation for Self-Supervised Speech Representations. (arXiv:2203.16965v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16965","description":"<p>While self-supervised speech representation learning (SSL) models serve a\nvariety of downstream tasks, these models have been observed to overfit to the\ndomain from which the unlabelled data originates. To alleviate this issue, we\npropose PADA (Pruning Assisted Domain Adaptation) and zero out redundant\nweights from models pre-trained on large amounts of out-of-domain (OOD) data.\nIntuitively, this helps to make space for the target-domain ASR finetuning. The\nredundant weights can be identified through various pruning strategies which\nhave been discussed in detail as a part of this work. Specifically, we\ninvestigate the effect of the recently discovered Task-Agnostic and Task-Aware\npruning on PADA and propose a new pruning paradigm based on the latter, which\nwe call Cross-Domain Task-Aware Pruning (CD-TAW). CD-TAW obtains the initial\npruning mask from a well fine-tuned OOD model, which makes it starkly different\nfrom the rest of the pruning strategies discussed in the paper. Our proposed\nCD-TAW methodology achieves up to 20.6% relative WER improvement over our\nbaseline when fine-tuned on a 2-hour subset of Switchboard data without\nlanguage model (LM) decoding. Furthermore, we conduct a detailed analysis to\nhighlight the key design choices of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prasad_L/0/1/0/all/0/1\">Lodagala V S V Durga Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COOL, a Context Outlooker, and its Application to Question Answering and other Natural Language Processing Tasks. (arXiv:2204.09593v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.09593","description":"<p>Vision outlooker improves the performance of vision transformers, which\nimplements a self-attention mechanism by adding an outlook attention, a form of\nlocal attention.\n</p>\n<p>In natural language processing, as has been the case in computer vision and\nother domains, transformer-based models constitute the state-of-the-art for\nmost processing tasks. In this domain, too, many authors have argued and\ndemonstrated the importance of local context.\n</p>\n<p>We present an outlook attention mechanism, COOL, for natural language\nprocessing. COOL, added on top of the self-attention layers of a\ntransformer-based model, encodes local syntactic context considering word\nproximity and more pair-wise constraints than dynamic convolution used by\nexisting approaches.\n</p>\n<p>A comparative empirical performance evaluation of an implementation of COOL\nwith different transformer-based models confirms the opportunity for\nimprovement over a baseline using the original models alone for various natural\nlanguage processing tasks, including question answering. The proposed approach\nachieves competitive performance with existing state-of-the-art methods on some\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fangyi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1\">See-Kiong Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bressan_S/0/1/0/all/0/1\">St&#xe9;phane Bressan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Hate Intensity of Twitter Conversation Threads. (arXiv:2206.08406v4 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2206.08406","description":"<p>Tweets are the most concise form of communication in online social media,\nwherein a single tweet has the potential to make or break the discourse of the\nconversation. Online hate speech is more accessible than ever, and stifling its\npropagation is of utmost importance for social media companies and users for\ncongenial communication. Most of the research barring a recent few has focused\non classifying an individual tweet regardless of the tweet thread/context\nleading up to that point. One of the classical approaches to curb hate speech\nis to adopt a reactive strategy after the hate speech postage. The ex-post\nfacto strategy results in neglecting subtle posts that do not show the\npotential to instigate hate speech on their own but may portend in the\nsubsequent discussion ensuing in the post's replies. In this paper, we propose\nDRAGNET++, which aims to predict the intensity of hatred that a tweet can bring\nin through its reply chain in the future. It uses the semantic and propagating\nstructure of the tweet threads to maximize the contextual information leading\nup to and the fall of hate intensity at each subsequent tweet. We explore three\npublicly available Twitter datasets -- Anti-Racism contains the reply tweets of\na collection of social media discourse on racist remarks during US political\nand Covid-19 background; Anti-Social presents a dataset of 40 million tweets\namidst the COVID-19 pandemic on anti-social behaviours; and Anti-Asian presents\nTwitter datasets collated based on anti-Asian behaviours during COVID-19\npandemic. All the curated datasets consist of structural graph information of\nthe Tweet threads. We show that DRAGNET++ outperforms all the state-of-the-art\nbaselines significantly. It beats the best baseline by an 11% margin on the\nPerson correlation coefficient and a decrease of 25% on RMSE for the\nAnti-Racism dataset with a similar performance on the other two datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1\">Qing Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_T/0/1/0/all/0/1\">Tharun Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Audio Captioning and Language-Based Audio Retrieval. (arXiv:2207.04156v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2207.04156","description":"<p>This project involved participation in the DCASE 2022 Competition (Task 6)\nwhich had two subtasks: (1) Automated Audio Captioning and (2) Language-Based\nAudio Retrieval. The first subtask involved the generation of a textual\ndescription for audio samples, while the goal of the second was to find audio\nsamples within a fixed dataset that match a given description. For both\nsubtasks, the Clotho dataset was used. The models were evaluated on BLEU1,\nBLEU2, BLEU3, ROUGEL, METEOR, CIDEr, SPICE, and SPIDEr scores for audio\ncaptioning and R1, R5, R10 and mARP10 scores for audio retrieval. We have\nconducted a handful of experiments that modify the baseline models for these\ntasks. Our final architecture for Automated Audio Captioning is close to the\nbaseline performance, while our model for Language-Based Audio Retrieval has\nsurpassed its counterpart.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1\">Clive Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyejin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kollman_P/0/1/0/all/0/1\">Patrick Kollman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houndayi_I/0/1/0/all/0/1\">Iffanice Houndayi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Ankit Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CASE: Aligning Coarse-to-Fine Cognition and Affection for Empathetic Response Generation. (arXiv:2208.08845v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2208.08845","description":"<p>Empathetic conversation is psychologically supposed to be the result of\nconscious alignment and interaction between the cognition and affection of\nempathy. However, existing empathetic dialogue models usually consider only the\naffective aspect or treat cognition and affection in isolation, which limits\nthe capability of empathetic response generation. In this work, we propose the\nCASE model for empathetic dialogue generation. It first builds upon a\ncommonsense cognition graph and an emotional concept graph and then aligns the\nuser's cognition and affection at both the coarse-grained and fine-grained\nlevels. Through automatic and manual evaluation, we demonstrate that CASE\noutperforms state-of-the-art baselines of empathetic dialogues and can generate\nmore empathetic and informative responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jinfeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chujie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Dynamic Contextualised Word Embeddings via Template-based Temporal Adaptation. (arXiv:2208.10734v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.10734","description":"<p>Dynamic contextualised word embeddings (DCWEs) represent the temporal\nsemantic variations of words. We propose a method for learning DCWEs by\ntime-adapting a pretrained Masked Language Model (MLM) using time-sensitive\ntemplates. Given two snapshots $C_1$ and $C_2$ of a corpus taken respectively\nat two distinct timestamps $T_1$ and $T_2$, we first propose an unsupervised\nmethod to select (a) \\emph{pivot} terms related to both $C_1$ and $C_2$, and\n(b) \\emph{anchor} terms that are associated with a specific pivot term in each\nindividual snapshot. We then generate prompts by filling manually compiled\ntemplates using the extracted pivot and anchor terms. Moreover, we propose an\nautomatic method to learn time-sensitive templates from $C_1$ and $C_2$,\nwithout requiring any human supervision. Next, we use the generated prompts to\nadapt a pretrained MLM to $T_2$ by fine-tuning using those prompts. Multiple\nexperiments show that our proposed method reduces the perplexity of test\nsentences in $C_2$, outperforming the current state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaohang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing. (arXiv:2208.13423v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2208.13423","description":"<p>Non-parallel text style transfer is an important task in natural language\ngeneration. However, previous studies concentrate on the token or sentence\nlevel, such as sentence sentiment and formality transfer, but neglect long\nstyle transfer at the discourse level. Long texts usually involve more\ncomplicated author linguistic preferences such as discourse structures than\nsentences. In this paper, we formulate the task of non-parallel story\nauthor-style transfer, which requires transferring an input story into a\nspecified author style while maintaining source semantics. To tackle this\nproblem, we propose a generation model, named StoryTrans, which leverages\ndiscourse representations to capture source content information and transfer\nthem to target styles with learnable style embeddings. We use an additional\ntraining objective to disentangle stylistic features from the learned discourse\nrepresentation to prevent the model from degenerating to an auto-encoder.\nMoreover, to enhance content preservation, we design a mask-and-fill framework\nto explicitly fuse style-specific keywords of source texts into generation.\nFurthermore, we constructed new datasets for this task in Chinese and English,\nrespectively. Extensive experiments show that our model outperforms strong\nbaselines in overall performance of style transfer and content preservation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xuekai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1\">Jian Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Juan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-trained Language Models for the Legal Domain: A Case Study on Indian Law. (arXiv:2209.06049v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.06049","description":"<p>NLP in the legal domain has seen increasing success with the emergence of\nTransformer-based Pre-trained Language Models (PLMs) pre-trained on legal text.\nPLMs trained over European and US legal text are available publicly; however,\nlegal text from other domains (countries), such as India, have a lot of\ndistinguishing characteristics. With the rapidly increasing volume of Legal NLP\napplications in various countries, it has become necessary to pre-train such\nLMs over legal text of other countries as well. In this work, we attempt to\ninvestigate pre-training in the Indian legal domain. We re-train (continue\npre-training) two popular legal PLMs, LegalBERT and CaseLawBERT, on Indian\nlegal data, as well as train a model from scratch with a vocabulary based on\nIndian legal text. We apply these PLMs over three benchmark legal NLP tasks --\nLegal Statute Identification from facts, Semantic Segmentation of Court\nJudgment Documents, and Court Appeal Judgment Prediction -- over both Indian\nand non-Indian (EU, UK) datasets. We observe that our approach not only\nenhances performance on the new domain (Indian texts) but also over the\noriginal domain (European and UK texts). We also conduct explainability\nexperiments for a qualitative comparison of all these different PLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Shounak Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandal_A/0/1/0/all/0/1\">Arpan Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Saptarshi Ghosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Round-Trip Translation for Machine Translation Evaluation. (arXiv:2209.07351v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.07351","description":"<p>Automatic evaluation on low-resource language translation suffers from a\ndeficiency of parallel corpora. Round-trip translation could be served as a\nclever and straightforward technique to alleviate the requirement of the\nparallel evaluation corpus. However, there was an observation of obscure\ncorrelations between the evaluation scores by forward and round-trip\ntranslations in the era of statistical machine translation (SMT). In this\npaper, we report the surprising finding that round-trip translation can be used\nfor automatic evaluation without the references. Firstly, our revisit on the\nround-trip translation in SMT evaluation unveils that its long-standing\nmisunderstanding is essentially caused by copying mechanism. After removing\ncopying mechanism in SMT, round-trip translation scores can appropriately\nreflect the forward translation performance. Then, we demonstrate the\nrectification is overdue as round-trip translation could benefit multiple\nmachine translation evaluation tasks. To be more specific, round-trip\ntranslation could be used i) to predict corresponding forward translation\nscores; ii) to improve the performance of the recently advanced quality\nestimation model; and iii) to identify adversarial competitors in shared tasks\nvia cross-system verification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1\">Terry Yue Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiongkai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuanli He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revamping Multilingual Agreement Bidirectionally via Switched Back-translation for Multilingual Neural Machine Translation. (arXiv:2209.13940v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.13940","description":"<p>Despite the fact that multilingual agreement (MA) has shown its importance\nfor multilingual neural machine translation (MNMT), current methodologies in\nthe field have two shortages: (i) require parallel data between multiple\nlanguage pairs, which is not always realistic and (ii) optimize the agreement\nin an ambiguous direction, which hampers the translation performance. We\npresent \\textbf{B}idirectional \\textbf{M}ultilingual \\textbf{A}greement via\n\\textbf{S}witched \\textbf{B}ack-\\textbf{t}ranslation (\\textbf{BMA-SBT}), a\nnovel and universal multilingual agreement framework for fine-tuning\npre-trained MNMT models, which (i) exempts the need for aforementioned parallel\ndata by using a novel method called switched BT that creates synthetic text\nwritten in another source language using the translation target and (ii)\noptimizes the agreement bidirectionally with the Kullback-Leibler Divergence\nloss. Experiments indicate that BMA-SBT clearly improves the strong baselines\non the task of MNMT with three benchmarks: TED Talks, News, and Europarl.\nIn-depth analyzes indicate that BMA-SBT brings additive improvements to the\nconventional BT method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hongyuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haoyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Makes Pre-trained Language Models Better Zero-shot Learners?. (arXiv:2209.15206v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2209.15206","description":"<p>In this paper, we propose a theoretical framework to explain the efficacy of\nprompt learning in zero/few-shot scenarios. First, we prove that conventional\npre-training and fine-tuning paradigm fails in few-shot scenarios due to\noverfitting the unrepresentative labelled data. We then detail the assumption\nthat prompt learning is more effective because it empowers pre-trained language\nmodel that is built upon massive text corpora, as well as domain-related human\nknowledge to participate more in prediction and thereby reduces the impact of\nlimited label information provided by the small training set. We further\nhypothesize that language discrepancy can measure the quality of prompting.\nComprehensive experiments are performed to verify our assumptions. More\nremarkably, inspired by the theoretical framework, we propose an\nannotation-agnostic template selection method based on perplexity, which\nenables us to ``forecast'' the prompting performance in advance. This approach\nis especially encouraging because existing work still relies on development set\nto post-hoc evaluate templates. Experiments show that this method leads to\nsignificant prediction benefits compared to state-of-the-art zero-shot methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jinghui Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dongsheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Weidong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1\">Brian Mac Namee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1\">Fei Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CCC-wav2vec 2.0: Clustering aided Cross Contrastive Self-supervised learning of speech representations. (arXiv:2210.02592v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.02592","description":"<p>While Self-Supervised Learning has helped reap the benefit of the scale from\nthe available unlabeled data, the learning paradigms are continuously being\nbettered. We present a new pre-training strategy named ccc-wav2vec 2.0, which\nuses clustering and an augmentation-based cross-contrastive loss as its\nself-supervised objective. Through the clustering module, we scale down the\ninfluence of those negative examples that are highly similar to the positive.\nThe Cross-Contrastive loss is computed between the encoder output of the\noriginal sample and the quantizer output of its augmentation and vice-versa,\nbringing robustness to the pre-training strategy. ccc-wav2vec 2.0 achieves up\nto 15.6% and 12.7% relative WER improvement over the baseline wav2vec 2.0 on\nthe test-clean and test-other sets, respectively, of LibriSpeech, without the\nuse of any language model. The proposed method also achieves up to 14.9%\nrelative WER improvement over the baseline wav2vec 2.0 when fine-tuned on\nSwitchboard data. We make all our codes publicly available on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lodagala_V/0/1/0/all/0/1\">Vasista Sai Lodagala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KALM: Knowledge-Aware Integration of Local, Document, and Global Contexts for Long Document Understanding. (arXiv:2210.04105v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.04105","description":"<p>With the advent of pretrained language models (LMs), increasing research\nefforts have been focusing on infusing commonsense and domain-specific\nknowledge to prepare LMs for downstream tasks. These works attempt to leverage\nknowledge graphs, the de facto standard of symbolic knowledge representation,\nalong with pretrained LMs. While existing approaches have leveraged external\nknowledge, it remains an open question how to jointly incorporate knowledge\ngraphs representing varying contexts, from local (e.g., sentence), to\ndocument-level, to global knowledge, to enable knowledge-rich exchange across\nthese contexts. Such rich contextualization can be especially beneficial for\nlong document understanding tasks since standard pretrained LMs are typically\nbounded by the input sequence length. In light of these challenges, we propose\nKALM, a Knowledge-Aware Language Model that jointly leverages knowledge in\nlocal, document-level, and global contexts for long document understanding.\nKALM first encodes long documents and knowledge graphs into the three\nknowledge-aware context representations. It then processes each context with\ncontext-specific layers, followed by a context fusion layer that facilitates\nknowledge exchange to derive an overarching document representation. Extensive\nexperiments demonstrate that KALM achieves state-of-the-art performance on six\nlong document understanding tasks and datasets. Further analyses reveal that\nthe three knowledge-aware contexts are complementary and they all contribute to\nmodel performance, while the importance and information exchange patterns of\ndifferent contexts vary with respect to different tasks and datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shangbin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhaoxuan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1\">Zhenyu Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How do we get there? Evaluating transformer neural networks as cognitive models for English past tense inflection. (arXiv:2210.09167v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2210.09167","description":"<p>There is an ongoing debate on whether neural networks can grasp the\nquasi-regularities in languages like humans. In a typical quasi-regularity\ntask, English past tense inflections, the neural network model has long been\ncriticized that it learns only to generalize the most frequent pattern, but not\nthe regular pattern, thus can not learn the abstract categories of regular and\nirregular and is dissimilar to human performance. In this work, we train a set\nof transformer models with different settings to examine their behavior on this\ntask. The models achieved high accuracy on unseen regular verbs and some\naccuracy on unseen irregular verbs. The models' performance on the regulars is\nheavily affected by type frequency and ratio but not token frequency and ratio,\nand vice versa for the irregulars. The different behaviors on the regulars and\nirregulars suggest that the models have some degree of symbolic learning on the\nregularity of the verbs. In addition, the models are weakly correlated with\nhuman behavior on nonce verbs. Although the transformer model exhibits some\nlevel of learning on the abstract category of verb regularity, its performance\ndoes not fit human data well, suggesting that it might not be a good cognitive\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaomeng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lingyu Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"data2vec-aqc: Search for the right Teaching Assistant in the Teacher-Student training setup. (arXiv:2211.01246v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2211.01246","description":"<p>In this paper, we propose a new Self-Supervised Learning (SSL) algorithm\ncalled data2vec-aqc, for speech representation learning from unlabeled speech\ndata. Our goal is to improve SSL for speech in domains where both unlabeled and\nlabeled data are limited. Building on the recently introduced data2vec, we\nintroduce additional modules to the data2vec framework that leverage the\nbenefit of data augmentations, quantized representations, and clustering. The\ninteraction between these modules helps solve the cross-contrastive loss as an\nadditional self-supervised objective. data2vec-aqc achieves up to 14.1% and\n20.9% relative WER improvement over the existing state-of-the-art data2vec\nsystem over the test-clean and test-other sets, respectively of LibriSpeech,\nwithout the use of any language model (LM). Our proposed model also achieves up\nto 17.8\\% relative WER gains over the baseline data2vec when fine-tuned on a\nsubset of the Switchboard dataset. Code:\nhttps://github.com/Speech-Lab-IITM/data2vec-aqc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lodagala_V/0/1/0/all/0/1\">Vasista Sai Lodagala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A minor extension of the logistic equation for growth of word counts on online media: Parametric description of diversity of growth phenomena in society. (arXiv:2211.16733v2 [physics.soc-ph] UPDATED)","link":"http://arxiv.org/abs/2211.16733","description":"<p>To understand the growing phenomena of new vocabulary on nationwide online\nsocial media, we analyzed monthly word count time series extracted from\napproximately 1 billion Japanese blog articles from 2007 to 2019. In\nparticular, we first introduced the extended logistic equation by adding one\nparameter to the original equation and showed that the model can consistently\nreproduce various patterns of actual growth curves, such as the logistic\nfunction, linear growth, and finite-time divergence. Second, by analyzing the\nmodel parameters, we found that the typical growth pattern is not only a\nlogistic function, which often appears in various complex systems, but also a\nnontrivial growth curve that starts with an exponential function and\nasymptotically approaches a power function without a steady state. Furthermore,\nwe observed a connection between the functional form of growth and the\npeak-out. Finally, we showed that the proposed model and statistical properties\nare also valid for Google Trends data (English, French, Spanish, and Japanese),\nwhich is a time series of the nationwide popularity of search queries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Watanabe_H/0/1/0/all/0/1\">Hayafumi Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logic and Commonsense-Guided Temporal Knowledge Graph Completion. (arXiv:2211.16865v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2211.16865","description":"<p>A temporal knowledge graph (TKG) stores the events derived from the data\ninvolving time. Predicting events is extremely challenging due to the\ntime-sensitive property of events. Besides, the previous TKG completion (TKGC)\napproaches cannot represent both the timeliness and the causality properties of\nevents, simultaneously. To address these challenges, we propose a Logic and\nCommonsense-Guided Embedding model (LCGE) to jointly learn the time-sensitive\nrepresentation involving timeliness and causality of events, together with the\ntime-independent representation of events from the perspective of commonsense.\nSpecifically, we design a temporal rule learning algorithm to construct a\nrule-guided predicate embedding regularization strategy for learning the\ncausality among events. Furthermore, we could accurately evaluate the\nplausibility of events via auxiliary commonsense knowledge. The experimental\nresults of TKGC task illustrate the significant performance improvements of our\nmodel compared with the existing approaches. More interestingly, our model is\nable to provide the explainability of the predicted results in the view of\ncausal inference. The source code and datasets of this paper are available at\nhttps://github.com/ngl567/LCGE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Guanglin Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning. (arXiv:2212.03760v5 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2212.03760","description":"<p>Recent studies have proposed unified user modeling frameworks that leverage\nuser behavior data from various applications. Many of them benefit from\nutilizing users' behavior sequences as plain texts, representing rich\ninformation in any domain or system without losing generality. Hence, a\nquestion arises: Can language modeling for user history corpus help improve\nrecommender systems? While its versatile usability has been widely investigated\nin many domains, its applications to recommender systems still remain\nunderexplored. We show that language modeling applied directly to task-specific\nuser histories achieves excellent results on diverse recommendation tasks.\nAlso, leveraging additional task-agnostic user histories delivers significant\nperformance benefits. We further demonstrate that our approach can provide\npromising transfer learning capabilities for a broad spectrum of real-world\nrecommender systems, even on unseen domains and services.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1\">Kyuyong Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_H/0/1/0/all/0/1\">Hanock Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1\">Wonjae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jisu Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Seungjae Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyung-Min Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causes and Cures for Interference in Multilingual Translation. (arXiv:2212.07530v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.07530","description":"<p>Multilingual machine translation models can benefit from synergy between\ndifferent language pairs, but also suffer from interference. While there is a\ngrowing number of sophisticated methods that aim to eliminate interference, our\nunderstanding of interference as a phenomenon is still limited. This work\nidentifies the main factors that contribute to interference in multilingual\nmachine translation. Through systematic experimentation, we find that\ninterference (or synergy) are primarily determined by model size, data size,\nand the proportion of each language pair within the total dataset. We observe\nthat substantial interference occurs mainly when the model is very small with\nrespect to the available training data, and that using standard transformer\nconfigurations with less than one billion parameters largely alleviates\ninterference and promotes synergy. Moreover, we show that tuning the sampling\ntemperature to control the proportion of each language pair in the data is key\nto balancing the amount of interference between low and high resource language\npairs effectively, and can lead to superior performance overall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaham_U/0/1/0/all/0/1\">Uri Shaham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elbayad_M/0/1/0/all/0/1\">Maha Elbayad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goswami_V/0/1/0/all/0/1\">Vedanuj Goswami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhosale_S/0/1/0/all/0/1\">Shruti Bhosale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advancing Multilingual Pre-training: TRIP Triangular Document-level Pre-training for Multilingual Language Models. (arXiv:2212.07752v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.07752","description":"<p>Despite the success of multilingual sequence-to-sequence pre-training, most\nexisting approaches rely on document-level monolingual corpora in many\ndifferent languages, sentence-level bilingual corpora,\\footnote{In this paper,\nwe use `bilingual corpora' to denote parallel corpora with `bilingual\ntranslation pairs' in many different language pairs, each consisting of two\nsentences/documents with the same meaning written in different languages. We\nuse `trilingual corpora' to denote parallel corpora with `trilingual\ntranslation pairs' in many different language combinations, each consisting of\nthree sentences/documents.} and sometimes synthetic document-level bilingual\ncorpora. This hampers the performance with cross-lingual document-level tasks\nsuch as document-level translation. Therefore, we propose to mine and leverage\ndocument-level trilingual parallel corpora to improve sequence-to-sequence\nmultilingual pre-training. We present \\textbf{Tri}angular Document-level\n\\textbf{P}re-training (\\textbf{TRIP}), which is the first in the field to\naccelerate the conventional monolingual and bilingual objectives into a\ntrilingual objective with a novel method called Grafting. Experiments show that\nTRIP achieves several strong state-of-the-art (SOTA) scores on three\nmultilingual document-level machine translation benchmarks and one\ncross-lingual abstractive summarization benchmark, including consistent\nimprovements by up to 3.11 d-BLEU points and 8.9 ROUGE-L points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hongyuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haoyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PVGRU: Generating Diverse and Relevant Dialogue Responses via Pseudo-Variational Mechanism. (arXiv:2212.09086v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09086","description":"<p>We investigate response generation for multi-turn dialogue in\ngenerative-based chatbots. Existing generative models based on RNNs (Recurrent\nNeural Networks) usually employ the last hidden state to summarize the\nsequences, which makes models unable to capture the subtle variability observed\nin different dialogues and cannot distinguish the differences between dialogues\nthat are similar in composition. In this paper, we propose a Pseudo-Variational\nGated Recurrent Unit (PVGRU) component without posterior knowledge through\nintroducing a recurrent summarizing variable into the GRU, which can aggregate\nthe accumulated distribution variations of subsequences. PVGRU can perceive the\nsubtle semantic variability through summarizing variables that are optimized by\nthe devised distribution consistency and reconstruction objectives. In\naddition, we build a Pseudo-Variational Hierarchical Dialogue (PVHD) model\nbased on PVGRU. Experimental results demonstrate that PVGRU can broadly improve\nthe diversity and relevance of responses on two benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongkang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Summarization Re-ranking. (arXiv:2212.09593v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.09593","description":"<p>With the rise of task-specific pre-training objectives, abstractive\nsummarization models like PEGASUS offer appealing zero-shot performance on\ndownstream summarization tasks. However, the performance of such unsupervised\nmodels still lags significantly behind their supervised counterparts. Similarly\nto the supervised setup, we notice a very high variance in quality among\nsummary candidates from these models while only one candidate is kept as the\nsummary output. In this paper, we propose to re-rank summary candidates in an\nunsupervised manner, aiming to close the performance gap between unsupervised\nand supervised models. Our approach improves the unsupervised PEGASUS by up to\n7.27% and ChatGPT by up to 6.86% relative mean ROUGE across four widely-adopted\nsummarization benchmarks ; and achieves relative gains of 7.51% (up to 23.73%\nfrom XSum to WikiHow) averaged over 30 zero-shot transfer setups (finetuning on\na dataset, evaluating on another).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravaut_M/0/1/0/all/0/1\">Mathieu Ravaut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Defending Against Misinformation Attacks in Open-Domain Question Answering. (arXiv:2212.10002v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10002","description":"<p>Recent work in open-domain question answering (ODQA) has shown that\nadversarial poisoning of the search collection can cause large drops in\naccuracy for production systems. However, little to no work has proposed\nmethods to defend against these attacks. To do so, we rely on the intuition\nthat redundant information often exists in large corpora. To find it, we\nintroduce a method that uses query augmentation to search for a diverse set of\npassages that could answer the original question but are less likely to have\nbeen poisoned. We integrate these new passages into the model through the\ndesign of a novel confidence method, comparing the predicted answer to its\nappearance in the retrieved contexts (what we call \\textit{Confidence from\nAnswer Redundancy}, i.e. CAR). Together these methods allow for a simple but\neffective way to defend against poisoning attacks that provides gains of nearly\n20\\% exact match across varying levels of data poisoning/knowledge conflicts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weller_O/0/1/0/all/0/1\">Orion Weller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Aleem Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weir_N/0/1/0/all/0/1\">Nathaniel Weir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawrie_D/0/1/0/all/0/1\">Dawn Lawrie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers. (arXiv:2212.10559v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10559","description":"<p>Large pretrained language models have shown surprising in-context learning\n(ICL) ability. With a few demonstration input-label pairs, they can predict the\nlabel for an unseen input without parameter updates. Despite the great success\nin performance, its working mechanism still remains an open question. In this\npaper, we explain language models as meta-optimizers and understand in-context\nlearning as implicit finetuning. Theoretically, we figure out that Transformer\nattention has a dual form of gradient descent. On top of it, we understand ICL\nas follows: GPT first produces meta-gradients according to the demonstration\nexamples, and then these meta-gradients are applied to the original GPT to\nbuild an ICL model. We comprehensively compare the behaviors of in-context\nlearning and explicit finetuning on real tasks to provide empirical evidence\nthat supports our understanding. Experimental results show that in-context\nlearning behaves similarly to explicit finetuning from multiple perspectives.\nInspired by the dual form between Transformer attention and gradient descent,\nwe design a momentum-based attention by analogy with gradient descent with\nmomentum. The improved performance over vanilla attention further supports our\nunderstanding from another perspective, and more importantly, shows the\npotential to utilize our understanding for future model design. The code is\navailable at \\url{https://aka.ms/icl}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Damai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yutao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yaru Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ontologically Faithful Generation of Non-Player Character Dialogues. (arXiv:2212.10618v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10618","description":"<p>We introduce a language generation task grounded in a popular video game\nenvironment. KNUDGE (KNowledge Constrained User-NPC Dialogue GEneration)\nrequires models to produce trees of dialogue between video game characters that\naccurately reflect quest and entity specifications stated in natural language.\nKNUDGE is constructed from side quest dialogues drawn directly from game data\nof Obsidian Entertainment's The Outer Worlds, leading to real-world\ncomplexities in generation: (1) dialogues are branching trees as opposed to\nlinear chains of utterances; (2) utterances must remain faithful to the game\nlore -- character personas, backstories, and entity relationships; and (3) a\ndialogue must accurately reveal new quest details to the human player. We\nreport results for a set of neural generation models using supervised and\nin-context learning techniques; we find competent performance but room for\nfuture work addressing the challenges of creating realistic, game-quality\ndialogues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weir_N/0/1/0/all/0/1\">Nathaniel Weir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_R/0/1/0/all/0/1\">Ryan Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAmore_R/0/1/0/all/0/1\">Randolph D&#x27;Amore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_K/0/1/0/all/0/1\">Kellie Hill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jhamtani_H/0/1/0/all/0/1\">Harsh Jhamtani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parallel Context Windows for Large Language Models. (arXiv:2212.10947v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2212.10947","description":"<p>When applied for processing long text, Large Language Models (LLMs) are\nlimited by their context window. Existing efforts to address this limitation\ninvolve training specialized architectures, and cannot be easily applied to\noff-the-shelf LLMs. We present Parallel Context Windows (PCW), a method that\nalleviates the context window restriction for any off-the-shelf LLM without\nfurther training. The key to the approach is to carve a long context into\nchunks (``windows''), restrict the attention mechanism to apply only within\neach window, and re-use the positional embeddings across the windows. Our main\nresults test the PCW approach on in-context learning with models that range in\nsize between 750 million and 178 billion parameters, and show substantial\nimprovements for tasks with diverse input and output spaces. We show additional\nbenefits in other settings where long context windows may be beneficial:\nmulti-hop questions and retrieval-augmented question answering with multiple\nretrieved documents. Our results highlight Parallel Context Windows as a\npromising method for applying off-the-shelf LLMs in a range of settings that\nrequire long text sequences. We make our code publicly available at\nhttps://github.com/ai21labs/parallel-context-windows.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ratner_N/0/1/0/all/0/1\">Nir Ratner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1\">Yoav Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ram_O/0/1/0/all/0/1\">Ori Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magar_I/0/1/0/all/0/1\">Inbal Magar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpas_E/0/1/0/all/0/1\">Ehud Karpas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1\">Amnon Shashua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leyton_Brown_K/0/1/0/all/0/1\">Kevin Leyton-Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoham_Y/0/1/0/all/0/1\">Yoav Shoham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word-Graph2vec: An efficient word embedding approach on word co-occurrence graph using random walk sampling. (arXiv:2301.04312v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2301.04312","description":"<p>Word embedding has become ubiquitous and is widely used in various text\nmining and natural language processing (NLP) tasks, such as information\nretrieval, semantic analysis, and machine translation, among many others.\nUnfortunately, it is prohibitively expensive to train the word embedding in a\nrelatively large corpus. We propose a graph-based word embedding algorithm,\ncalled Word-Graph2vec, which converts the large corpus into a word\nco-occurrence graph, then takes the word sequence samples from this graph by\nrandomly traveling and trains the word embedding on this sampling corpus in the\nend. We posit that because of the stable vocabulary, relative idioms, and fixed\nexpressions in English, the size and density of the word co-occurrence graph\nchange slightly with the increase in the training corpus. So that\nWord-Graph2vec has stable runtime on the large scale data set, and its\nperformance advantage becomes more and more obvious with the growth of the\ntraining corpus. Extensive experiments conducted on real-world datasets show\nthat the proposed algorithm outperforms traditional Skip-Gram by four-five\ntimes in terms of efficiency, while the error generated by the random walk\nsampling is small.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenting Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jiahong Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huacan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuanzhe Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2302.01313","description":"<p>This work provides a formalization of Knowledge Graphs (KGs) as a new class\nof graphs that we denote doubly exchangeable attributed graphs, where node and\npairwise (joint 2-node) representations must be equivariant to permutations of\nboth node ids and edge (&amp; node) attributes (relations &amp; node features).\nDouble-permutation equivariant KG representations open a new research direction\nin KGs. We show that this equivariance imposes a structural representation of\nrelations that allows neural networks to perform complex logical reasoning\ntasks in KGs. Finally, we introduce a general blueprint for such equivariant\nrepresentations and test a simple GNN-based double-permutation equivariant\nneural architecture that achieve state-of-the-art Hits@10 test accuracy in the\nWN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately\nperform logical reasoning tasks that no existing methods can perform, to the\nbest of our knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yangze Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_B/0/1/0/all/0/1\">Bruno Ribeiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffusion Models for Non-autoregressive Text Generation: A Survey. (arXiv:2303.06574v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.06574","description":"<p>Non-autoregressive (NAR) text generation has attracted much attention in the\nfield of natural language processing, which greatly reduces the inference\nlatency but has to sacrifice the generation accuracy. Recently, diffusion\nmodels, a class of latent variable generative models, have been introduced into\nNAR text generation, showing an improved text generation quality. In this\nsurvey, we review the recent progress in diffusion models for NAR text\ngeneration. As the background, we first present the general definition of\ndiffusion models and the text diffusion models, and then discuss their merits\nfor NAR generation. As the core content, we further introduce two mainstream\ndiffusion models in existing work of text diffusion, and review the key designs\nof the diffusion process. Moreover, we discuss the utilization of pre-trained\nlanguage models (PLMs) for text diffusion models and introduce optimization\ntechniques for text data. Finally, we discuss several promising directions and\nconclude this paper. Our survey aims to provide researchers with a systematic\nreference of related research on text diffusion models for NAR generation. We\npresent our collection of text diffusion models at\nhttps://github.com/RUCAIBox/Awesome-Text-Diffusion-Models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Artificial Empathy for Human-Centered Design: A Framework. (arXiv:2303.10583v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2303.10583","description":"<p>In the early stages of the design process, designers explore opportunities by\ndiscovering unmet needs and developing innovative concepts as potential\nsolutions. From a human-centered design perspective, designers must develop\nempathy with people to truly understand their needs. However, developing\nempathy is a complex and subjective process that relies heavily on the\ndesigner's empathic capability. Therefore, the development of empathic\nunderstanding is intuitive, and the discovery of underlying needs is often\nserendipitous. This paper aims to provide insights from artificial intelligence\nresearch to indicate the future direction of AI-driven human-centered design,\ntaking into account the essential role of empathy. Specifically, we conduct an\ninterdisciplinary investigation of research areas such as data-driven user\nstudies, empathic understanding development, and artificial empathy. Based on\nthis foundation, we discuss the role that artificial empathy can play in\nhuman-centered design and propose an artificial empathy framework for\nhuman-centered design. Building on the mechanisms behind empathy and insights\nfrom empathic design research, the framework aims to break down the rather\ncomplex and subjective concept of empathy into components and modules that can\npotentially be modeled computationally. Furthermore, we discuss the expected\nbenefits of developing such systems and identify current research gaps to\nencourage future research efforts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jianxi Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Transformer Models and Human Behaviors on Chinese Character Naming. (arXiv:2303.12294v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.12294","description":"<p>Neural network models have been proposed to explain the grapheme-phoneme\nmapping process in humans for many alphabet languages. These models not only\nsuccessfully learned the correspondence of the letter strings and their\npronunciation, but also captured human behavior in nonce word naming tasks. How\nwould the neural models perform for a non-alphabet language (e.g., Chinese)\nunknown character task? How well would the model capture human behavior? In\nthis study, we first collect human speakers' answers on unknown character\nnaming tasks and then evaluate a set of transformer models by comparing their\nperformances with human behaviors on an unknown Chinese character naming task.\nWe found that the models and humans behaved very similarly, that they had\nsimilar accuracy distribution for each character, and had a substantial overlap\nin answers. In addition, the models' answers are highly correlated with humans'\nanswers. These results suggested that the transformer models can well capture\nhuman's character naming behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaomeng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lingyu Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep RL with Hierarchical Action Exploration for Dialogue Generation. (arXiv:2303.13465v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.13465","description":"<p>Traditionally, approximate dynamic programming is employed in dialogue\ngeneration with greedy policy improvement through action sampling, as the\nnatural language action space is vast. However, this practice is inefficient\nfor reinforcement learning (RL) due to the sparsity of eligible responses with\nhigh action values, which leads to weak improvement sustained by random\nsampling. This paper presents theoretical analysis and experiments that reveal\nthe performance of the dialogue policy is positively correlated with the\nsampling size. To overcome this limitation, we introduce a novel\ndual-granularity Q-function that explores the most promising response category\nto intervene in the sampling process. Our approach extracts actions based on a\ngrained hierarchy, thereby achieving the optimum with fewer policy iterations.\nAdditionally, we use offline RL and learn from multiple reward functions\ndesigned to capture emotional nuances in human interactions. Empirical studies\ndemonstrate that our algorithm outperforms baselines across automatic metrics\nand human evaluations. Further testing reveals that our algorithm exhibits both\nexplainability and controllability and generates responses with higher expected\nrewards.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_I/0/1/0/all/0/1\">Itsugun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takahashi_R/0/1/0/all/0/1\">Ryota Takahashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yanase_Y/0/1/0/all/0/1\">Yusaku Yanase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_H/0/1/0/all/0/1\">Hiroaki Saito</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Reasoning, A Survey. (arXiv:2303.14725v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2303.14725","description":"<p>This survey paper proposes a clearer view of natural language reasoning in\nthe field of Natural Language Processing (NLP), both conceptually and\npractically. Conceptually, we provide a distinct definition for natural\nlanguage reasoning in NLP, based on both philosophy and NLP scenarios, discuss\nwhat types of tasks require reasoning, and introduce a taxonomy of reasoning.\nPractically, we conduct a comprehensive literature review on natural language\nreasoning in NLP, mainly covering classical logical reasoning, natural language\ninference, multi-hop question answering, and commonsense reasoning. The paper\nalso identifies and views backward reasoning, a powerful paradigm for\nmulti-step reasoning, and introduces defeasible reasoning as one of the most\nimportant future directions in natural language reasoning research. We focus on\nsingle-modality unstructured natural language text, excluding neuro-symbolic\ntechniques and mathematical reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_P/0/1/0/all/0/1\">Prayag Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Benyou Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Prompt Cell: A Portable Control Module for Effective Prompt Tuning. (arXiv:2304.05642v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.05642","description":"<p>As a novel approach to tuning pre-trained models, prompt tuning involves\nfreezing the parameters in downstream tasks while inserting trainable\nembeddings into inputs in the first layer. However, previous methods have\nmainly focused on the initialization of prompt embeddings. The strategy of\ntraining and utilizing prompt embeddings in a reasonable way has become a\nlimiting factor in the effectiveness of prompt tuning. To address this issue,\nwe introduce the Global Prompt Cell (GPC), a portable control module for prompt\ntuning that selectively preserves prompt information across all encoder layers.\nOur experimental results demonstrate a 5.8% improvement on SuperGLUE datasets\ncompared to vanilla prompt tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haochun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_N/0/1/0/all/0/1\">Nuwa Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sendong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChatPLUG: Open-Domain Generative Dialogue System with Internet-Augmented Instruction Tuning for Digital Human. (arXiv:2304.07849v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.07849","description":"<p>In this paper, we present ChatPLUG, a Chinese open-domain dialogue system for\ndigital human applications that instruction finetunes on a wide range of\ndialogue tasks in a unified internet-augmented format. Different from other\nopen-domain dialogue models that focus on large-scale pre-training and scaling\nup model size or dialogue corpus, we aim to build a powerful and practical\ndialogue system for digital human with diverse skills and good multi-task\ngeneralization by internet-augmented instruction tuning. To this end, we first\nconduct large-scale pre-training on both common document corpus and dialogue\ndata with curriculum learning, so as to inject various world knowledge and\ndialogue abilities into ChatPLUG. Then, we collect a wide range of dialogue\ntasks spanning diverse features of knowledge, personality, multi-turn memory,\nand empathy, on which we further instruction tune \\modelname via unified\nnatural language instruction templates. External knowledge from an internet\nsearch is also used during instruction finetuning for alleviating the problem\nof knowledge hallucinations. We show that \\modelname outperforms\nstate-of-the-art Chinese dialogue systems on both automatic and human\nevaluation, and demonstrates strong multi-task generalization on a variety of\ntext understanding and generation tasks. In addition, we deploy \\modelname to\nreal-world applications such as Smart Speaker and Instant Message applications\nwith fast inference. Our models and code will be made publicly available on\nModelScope: https://modelscope.cn/models/damo/ChatPLUG-3.7B and Github:\nhttps://github.com/X-PLUG/ChatPLUG .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Junfeng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hehong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guohai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianhai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiayi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenshen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1\">Qi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinghao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiejing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Post-hoc Explanations for Skip-gram-based Node Embeddings by Identifying Important Nodes with Bridgeness. (arXiv:2304.12036v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2304.12036","description":"<p>Node representation learning in a network is an important machine learning\ntechnique for encoding relational information in a continuous vector space\nwhile preserving the inherent properties and structures of the network.\nRecently, unsupervised node embedding methods such as DeepWalk, LINE,\nstruc2vec, PTE, UserItem2vec, and RWJBG have emerged from the Skip-gram model\nand perform better performance in several downstream tasks such as node\nclassification and link prediction than the existing relational models.\nHowever, providing post-hoc explanations of Skip-gram-based embeddings remains\na challenging problem because of the lack of explanation methods and\ntheoretical studies applicable for embeddings. In this paper, we first show\nthat global explanations to the Skip-gram-based embeddings can be found by\ncomputing bridgeness under a spectral cluster-aware local perturbation.\nMoreover, a novel gradient-based explanation method, which we call GRAPH-wGD,\nis proposed that allows the top-q global explanations about learned graph\nembedding vectors more efficiently. Experiments show that the ranking of nodes\nby scores using GRAPH-wGD is highly correlated with true bridgeness scores. We\nalso observe that the top-q node-level explanations selected by GRAPH-wGD have\nhigher importance scores and produce more changes in class label prediction\nwhen perturbed, compared with the nodes selected by recent alternatives, using\nfive real-world graphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hogun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neville_J/0/1/0/all/0/1\">Jennifer Neville</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Massive Multitask Chinese Understanding. (arXiv:2304.12986v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.12986","description":"<p>The development of large-scale Chinese language models is flourishing, yet\nthere is a lack of corresponding capability assessments. Therefore, we propose\na test to measure the multitask accuracy of large Chinese language models. This\ntest encompasses four major domains, including medicine, law, psychology, and\neducation, with 15 subtasks in medicine and 8 subtasks in education. We found\nthat the best-performing models in the zero-shot setting outperformed the\nworst-performing models by nearly 18.6 percentage points on average. Across the\nfour major domains, the highest average zero-shot accuracy of all models is\n0.512. In the subdomains, only the GPT-3.5-turbo model achieved a zero-shot\naccuracy of 0.693 in clinical medicine, which was the highest accuracy among\nall models across all subtasks. All models performed poorly in the legal\ndomain, with the highest zero-shot accuracy reaching only 0.239. By\ncomprehensively evaluating the breadth and depth of knowledge across multiple\ndisciplines, this test can more accurately identify the shortcomings of the\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Hui Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SweCTRL-Mini: a data-transparent Transformer-based large language model for controllable text generation in Swedish. (arXiv:2304.13994v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2304.13994","description":"<p>We present SweCTRL-Mini, a large Swedish language model that can be used for\ninference and fine-tuning on a single consumer-grade GPU. The model is based on\nthe CTRL architecture by Keskar, McCann, Varshney, Xiong, and Socher (2019),\nwhich means that users of the SweCTRL-Mini model can control the genre of the\ngenerated text by inserting special tokens in the generation prompts.\nSweCTRL-Mini is trained on a subset of the Swedish part of the mC4 corpus and a\nset of Swedish novels. In this article, we provide (1) a detailed account of\nthe utilized training data and text pre-processing steps, to the extent that it\nis possible to check whether a specific phrase/source was a part of the\ntraining data, and (2) an evaluation of the model on both discriminative tasks,\nusing automatic evaluation methods, and generative tasks, using human referees.\nWe also compare the generative capabilities of the model with those of GPT-3.\nSweCTRL-Mini is fully open and available for download.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalpakchi_D/0/1/0/all/0/1\">Dmytro Kalpakchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boye_J/0/1/0/all/0/1\">Johan Boye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructing a Knowledge Graph from Textual Descriptions of Software Vulnerabilities in the National Vulnerability Database. (arXiv:2305.00382v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2305.00382","description":"<p>Knowledge graphs have shown promise for several cybersecurity tasks, such as\nvulnerability assessment and threat analysis. In this work, we present a new\nmethod for constructing a vulnerability knowledge graph from information in the\nNational Vulnerability Database (NVD). Our approach combines named entity\nrecognition (NER), relation extraction (RE), and entity prediction using a\ncombination of neural models, heuristic rules, and knowledge graph embeddings.\nWe demonstrate how our method helps to fix missing entities in knowledge graphs\nused for cybersecurity and evaluate the performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Host_A/0/1/0/all/0/1\">Anders M&#xf8;lmen H&#xf8;st</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lison_P/0/1/0/all/0/1\">Pierre Lison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moonen_L/0/1/0/all/0/1\">Leon Moonen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds. (arXiv:2305.00969v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2305.00969","description":"<p>This paper describes the Ubenwa CryCeleb dataset - a labeled collection of\ninfant cries, and the accompanying CryCeleb 2023 task - a public speaker\nverification challenge based on infant cry sounds. We release for academic\nusage more than 6 hours of manually segmented cry sounds from 786 newborns to\nencourage research in infant cry analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Budaghyan_D/0/1/0/all/0/1\">David Budaghyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorin_A/0/1/0/all/0/1\">Arsenii Gorin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subakan_C/0/1/0/all/0/1\">Cem Subakan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onu_C/0/1/0/all/0/1\">Charles C. Onu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HiPool: Modeling Long Documents Using Graph Neural Networks. (arXiv:2305.03319v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03319","description":"<p>Encoding long sequences in Natural Language Processing (NLP) is a challenging\nproblem. Though recent pretraining language models achieve satisfying\nperformances in many NLP tasks, they are still restricted by a pre-defined\nmaximum length, making them challenging to be extended to longer sequences. So\nsome recent works utilize hierarchies to model long sequences. However, most of\nthem apply sequential models for upper hierarchies, suffering from long\ndependency issues. In this paper, we alleviate these issues through a\ngraph-based method. We first chunk the sequence with a fixed length to model\nthe sentence-level information. We then leverage graphs to model intra- and\ncross-sentence correlations with a new attention mechanism. Additionally, due\nto limited standard benchmarks for long document classification (LDC), we\npropose a new challenging benchmark, totaling six datasets with up to 53k\nsamples and 4034 average tokens' length. Evaluation shows our model surpasses\ncompetitive baselines by 2.6% in F1 score, and 4.8% on the longest sequence\ndataset. Our method is shown to outperform hierarchical sequential models with\nbetter performance and scalability, especially for longer sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_A/0/1/0/all/0/1\">Aosong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1\">Rex Ying</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LMs stand their Ground: Investigating the Effect of Embodiment in Figurative Language Interpretation by Language Models. (arXiv:2305.03445v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.03445","description":"<p>Figurative language is a challenge for language models since its\ninterpretation is based on the use of words in a way that deviates from their\nconventional order and meaning. Yet, humans can easily understand and interpret\nmetaphors, similes or idioms as they can be derived from embodied metaphors.\nLanguage is a proxy for embodiment and if a metaphor is conventional and\nlexicalised, it becomes easier for a system without a body to make sense of\nembodied concepts. Yet, the intricate relation between embodiment and features\nsuch as concreteness or age of acquisition has not been studied in the context\nof figurative language interpretation concerning language models. Hence, the\npresented study shows how larger language models perform better at interpreting\nmetaphoric sentences when the action of the metaphorical sentence is more\nembodied. The analysis rules out multicollinearity with other features (e.g.\nword length or concreteness) and provides initial evidence that larger language\nmodels conceptualise embodied concepts to a degree that facilitates figurative\nlanguage understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wicke_P/0/1/0/all/0/1\">Philipp Wicke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment. (arXiv:2305.04476v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2305.04476","description":"<p>The speech-to-singing (STS) voice conversion task aims to generate singing\nsamples corresponding to speech recordings while facing a major challenge: the\nalignment between the target (singing) pitch contour and the source (speech)\ncontent is difficult to learn in a text-free situation. This paper proposes\nAlignSTS, an STS model based on explicit cross-modal alignment, which views\nspeech variance such as pitch and content as different modalities. Inspired by\nthe mechanism of how humans will sing the lyrics to the melody, AlignSTS: 1)\nadopts a novel rhythm adaptor to predict the target rhythm representation to\nbridge the modality gap between content and pitch, where the rhythm\nrepresentation is computed in a simple yet effective way and is quantized into\na discrete space; and 2) uses the predicted rhythm representation to re-align\nthe content based on cross-attention and conducts a cross-modal fusion for\nre-synthesize. Extensive experiments show that AlignSTS achieves superior\nperformance in terms of both objective and subjective metrics. Audio samples\nare available at https://alignsts.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_R/0/1/0/all/0/1\">Ruiqi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_R/0/1/0/all/0/1\">Rongjie Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lichao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jinglin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset. (arXiv:2305.04582v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.04582","description":"<p>Relation extraction (RE) is a fundamental task in information extraction,\nwhose extension to multilingual settings has been hindered by the lack of\nsupervised resources comparable in size to large English datasets such as\nTACRED (Zhang et al., 2017). To address this gap, we introduce the MultiTACRED\ndataset, covering 12 typologically diverse languages from 9 language families,\nwhich is created by machine-translating TACRED instances and automatically\nprojecting their entity annotations. We analyze translation and annotation\nprojection quality, identify error categories, and experimentally evaluate\nfine-tuned pretrained mono- and multilingual language models in common transfer\nlearning scenarios. Our analyses show that machine translation is a viable\nstrategy to transfer RE instances, with native speakers judging more than 83%\nof the translated instances to be linguistically and semantically acceptable.\nWe find monolingual RE model performance to be comparable to the English\noriginal for many of the target languages, and that multilingual models trained\non a combination of English and target language data can outperform their\nmonolingual counterparts. However, we also observe a variety of translation and\nannotation projection errors, both due to the MT systems and linguistic\nfeatures of the target languages, such as pronoun-dropping, compounding and\ninflection, that degrade dataset quality and RE model performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hennig_L/0/1/0/all/0/1\">Leonhard Hennig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_P/0/1/0/all/0/1\">Philippe Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_S/0/1/0/all/0/1\">Sebastian M&#xf6;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A transformer-based method for zero and few-shot biomedical named entity recognition. (arXiv:2305.04928v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.04928","description":"<p>Supervised named entity recognition (NER) in the biomedical domain is\ndependent on large sets of annotated texts with the given named entities, whose\ncreation can be time-consuming and expensive. Furthermore, the extraction of\nnew entities often requires conducting additional annotation tasks and\nretraining the model. To address these challenges, this paper proposes a\ntransformer-based method for zero- and few-shot NER in the biomedical domain.\nThe method is based on transforming the task of multi-class token\nclassification into binary token classification (token contains the searched\nentity or does not contain the searched entity) and pre-training on a larger\namount of datasets and biomedical entities, from where the method can learn\nsemantic relations between the given and potential classes. We have achieved\naverage F1 scores of 35.44% for zero-shot NER, 50.10% for one-shot NER, 69.94%\nfor 10-shot NER, and 79.51% for 100-shot NER on 9 diverse evaluated biomedical\nentities with PubMedBERT fine-tuned model. The results demonstrate the\neffectiveness of the proposed method for recognizing new entities with limited\nexamples, with comparable or better results from the state-of-the-art zero- and\nfew-shot NER methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kosprdic_M/0/1/0/all/0/1\">Milo&#x161; Ko&#x161;prdi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prodanovic_N/0/1/0/all/0/1\">Nikola Prodanovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ljajic_A/0/1/0/all/0/1\">Adela Ljaji&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basaragin_B/0/1/0/all/0/1\">Bojana Ba&#x161;aragin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milosevic_N/0/1/0/all/0/1\">Nikola Milo&#x161;evi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ANALOGICAL -- A New Benchmark for Analogy of Long Text for Large Language Models. (arXiv:2305.05050v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.05050","description":"<p>Over the past decade, analogies, in the form of word-level analogies, have\nplayed a significant role as an intrinsic measure of evaluating the quality of\nword embedding methods such as word2vec. Modern large language models (LLMs),\nhowever, are primarily evaluated on extrinsic measures based on benchmarks such\nas GLUE and SuperGLUE, and there are only a few investigations on whether LLMs\ncan draw analogies between long texts. In this paper, we present ANALOGICAL, a\nnew benchmark to intrinsically evaluate LLMs across a taxonomy of analogies of\nlong text with six levels of complexity -- (i) word, (ii) word vs. sentence,\n(iii) syntactic, (iv) negation, (v) entailment, and (vi) metaphor. Using\nthirteen datasets and three different distance measures, we evaluate the\nabilities of eight LLMs in identifying analogical pairs in the semantic vector\nspace. Our evaluation finds that it is increasingly challenging for LLMs to\nidentify analogies when going up the analogy taxonomy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wijesiriwardene_T/0/1/0/all/0/1\">Thilini Wijesiriwardene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wickramarachchi_R/0/1/0/all/0/1\">Ruwan Wickramarachchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gajera_B/0/1/0/all/0/1\">Bimal G. Gajera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gowaikar_S/0/1/0/all/0/1\">Shreeyash Mukul Gowaikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_C/0/1/0/all/0/1\">Chandan Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1\">Aman Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reganti_A/0/1/0/all/0/1\">Aishwarya Naresh Reganti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Amitava Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VCSUM: A Versatile Chinese Meeting Summarization Dataset. (arXiv:2305.05280v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.05280","description":"<p>Compared to news and chat summarization, the development of meeting\nsummarization is hugely decelerated by the limited data. To this end, we\nintroduce a versatile Chinese meeting summarization dataset, dubbed VCSum,\nconsisting of 239 real-life meetings, with a total duration of over 230 hours.\nWe claim our dataset is versatile because we provide the annotations of topic\nsegmentation, headlines, segmentation summaries, overall meeting summaries, and\nsalient sentences for each meeting transcript. As such, the dataset can adapt\nto various summarization tasks or methods, including segmentation-based\nsummarization, multi-granularity summarization and retrieval-then-generate\nsummarization. Our analysis confirms the effectiveness and robustness of VCSum.\nWe also provide a set of benchmark models regarding different downstream\nsummarization tasks on VCSum to facilitate further research. The dataset and\ncode will be released at https://github.com/hahahawu/VCSum.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_M/0/1/0/all/0/1\">Mingjie Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Haochen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zhaohui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linqi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Good Intentions: Reporting the Research Landscape of NLP for Social Good. (arXiv:2305.05471v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.05471","description":"<p>With the recent advances in natural language processing (NLP), a vast number\nof applications have emerged across various use cases. Among the plethora of\nNLP applications, many academic researchers are motivated to do work that has a\npositive social impact, in line with the recent initiatives of NLP for Social\nGood (NLP4SG). However, it is not always obvious to researchers how their\nresearch efforts are tackling today's big social problems. Thus, in this paper,\nwe introduce NLP4SGPAPERS, a scientific dataset with three associated tasks\nthat can help identify NLP4SG papers and characterize the NLP4SG landscape by:\n(1) identifying the papers that address a social problem, (2) mapping them to\nthe corresponding UN Sustainable Development Goals (SDGs), and (3) identifying\nthe task they are solving and the methods they are using. Using\nstate-of-the-art NLP models, we address each of these tasks and use them on the\nentire ACL Anthology, resulting in a visualization workspace that gives\nresearchers a comprehensive overview of the field of NLP4SG. Our website is\navailable at https://nlp4sg.vercel.app . We released our data at\nhttps://huggingface.co/datasets/feradauto/NLP4SGPapers and code at\nhttps://github.com/feradauto/nlp4sg .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_F/0/1/0/all/0/1\">Fernando Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge. (arXiv:2305.05976v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.05976","description":"<p>Large language models (LLMs) have been widely studied for their ability to\nstore and utilize positive knowledge. However, negative knowledge, such as\n\"lions don't live in the ocean\", is also ubiquitous in the world but rarely\nmentioned explicitly in the text. What do LLMs know about negative knowledge?\nThis work examines the ability of LLMs to negative commonsense knowledge. We\ndesign a constrained keywords-to-sentence generation task (CG) and a Boolean\nquestion-answering task (QA) to probe LLMs. Our experiments reveal that LLMs\nfrequently fail to generate valid sentences grounded in negative commonsense\nknowledge, yet they can correctly answer polar yes-or-no questions. We term\nthis phenomenon the belief conflict of LLMs. Our further analysis shows that\nstatistical shortcuts and negation reporting bias from language modeling\npre-training cause this conflict.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiangjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Wei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Ziquan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Sijie Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2305.06569","description":"<p>Recommendation foundation model utilizes large language models (LLM) for\nrecommendation by converting recommendation tasks into natural language tasks.\nIt enables generative recommendation which directly generates the item(s) to\nrecommend rather than calculating a ranking score for each and every candidate\nitem in traditional recommendation models, simplifying the recommendation\npipeline from multi-stage filtering to single-stage filtering. To avoid\ngenerating excessively long text when deciding which item(s) to recommend,\ncreating LLM-compatible item IDs is essential for recommendation foundation\nmodels. In this study, we systematically examine the item indexing problem for\nrecommendation foundation models, using P5 as the representative backbone model\nand replicating its results with various indexing methods. To emphasize the\nimportance of item indexing, we first discuss the issues of several trivial\nitem indexing methods, such as independent indexing, title indexing, and random\nindexing. We then propose four simple yet effective solutions, including\nsequential indexing, collaborative indexing, semantic (content-based) indexing,\nand hybrid indexing. Our reproducibility study of P5 highlights the significant\ninfluence of item indexing methods on the model performance, and our results on\nreal-world datasets validate the effectiveness of our proposed solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wenyue Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable ELements for explaining neural net classifiers on NLP tasks. (arXiv:2305.06754v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.06754","description":"<p>Transformer architectures are complex and their use in NLP, while it has\nengendered many successes, makes their interpretability or explainability\nchallenging. Recent debates have shown that attention maps and attribution\nmethods are unreliable (Pruthi et al., 2019; Brunner et al., 2019). In this\npaper, we present some of their limitations and introduce COCKATIEL, which\nsuccessfully addresses some of them. COCKATIEL is a novel, post-hoc,\nconcept-based, model-agnostic XAI technique that generates meaningful\nexplanations from the last layer of a neural net model trained on an NLP\nclassification task by using Non-Negative Matrix Factorization (NMF) to\ndiscover the concepts the model leverages to make predictions and by exploiting\na Sensitivity Analysis to estimate accurately the importance of each of these\nconcepts for the model. It does so without compromising the accuracy of the\nunderlying model or requiring a new one to be trained. We conduct experiments\nin single and multi-aspect sentiment analysis tasks and we show COCKATIEL's\nsuperior ability to discover concepts that align with humans' on Transformer\nmodels without any supervision, we objectively verify the faithfulness of its\nexplanations through fidelity metrics, and we showcase its ability to provide\nmeaningful explanations in two different datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jourdan_F/0/1/0/all/0/1\">Fanny Jourdan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picard_A/0/1/0/all/0/1\">Agustin Picard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fel_T/0/1/0/all/0/1\">Thomas Fel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risser_L/0/1/0/all/0/1\">Laurent Risser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loubes_J/0/1/0/all/0/1\">Jean Michel Loubes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asher_N/0/1/0/all/0/1\">Nicholas Asher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Open-Domain Question Answering in the Era of Large Language Models. (arXiv:2305.06984v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.06984","description":"<p>Lexical matching remains the de facto evaluation method for open-domain\nquestion answering (QA). Unfortunately, lexical matching fails completely when\na plausible candidate answer does not appear in the list of gold answers, which\nis increasingly the case as we shift from extractive to generative models. The\nrecent success of large language models (LLMs) for QA aggravates lexical\nmatching failures since candidate answers become longer, thereby making\nmatching with the gold answers even more challenging. Without accurate\nevaluation, the true progress in open-domain QA remains unknown. In this paper,\nwe conduct a thorough analysis of various open-domain QA models, including\nLLMs, by manually evaluating their answers on a subset of NQ-open, a popular\nbenchmark. Our assessments reveal that while the true performance of all models\nis significantly underestimated, the performance of the InstructGPT (zero-shot)\nLLM increases by nearly +60%, making it on par with existing top models, and\nthe InstructGPT (few-shot) model actually achieves a new state-of-the-art on\nNQ-open. We also find that more than 50% of lexical matching failures are\nattributed to semantically equivalent answers. We further demonstrate that\nregex matching ranks QA models consistent with human judgments, although still\nsuffering from unnecessary strictness. Finally, we demonstrate that automated\nevaluation models are a reasonable surrogate for lexical matching in some\ncircumstances, but not for long-form answers generated by LLMs. The automated\nmodels struggle in detecting hallucinations in LLM answers and are thus unable\nto evaluate LLMs. At this time, there appears to be no substitute for human\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamalloo_E/0/1/0/all/0/1\">Ehsan Kamalloo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziri_N/0/1/0/all/0/1\">Nouha Dziri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clarke_C/0/1/0/all/0/1\">Charles L. A. Clarke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafiei_D/0/1/0/all/0/1\">Davood Rafiei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation. (arXiv:2305.07375v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2305.07375","description":"<p>Causal reasoning ability is crucial for numerous NLP applications. Despite\nthe impressive emerging ability of ChatGPT in various NLP tasks, it is unclear\nhow well ChatGPT performs in causal reasoning. In this paper, we conduct the\nfirst comprehensive evaluation of the ChatGPT's causal reasoning capabilities.\nExperiments show that ChatGPT is not a good causal reasoner, but a good causal\ninterpreter. Besides, ChatGPT has a serious hallucination on causal reasoning,\npossibly due to the reporting biases between causal and non-causal\nrelationships in natural language, as well as ChatGPT's upgrading processes,\nsuch as RLHF. The In-Context Learning (ICL) and Chain-of-Though (COT)\ntechniques can further exacerbate such causal hallucination. Additionally, the\ncausal reasoning ability of ChatGPT is sensitive to the words used to express\nthe causal concept in prompts, and close-ended prompts perform better than\nopen-ended prompts. For events in sentences, ChatGPT excels at capturing\nexplicit causality rather than implicit causality, and performs better in\nsentences with lower event density and smaller lexical distance between events.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jinglong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2023-05-15T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}